<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.19543] RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing</title><meta property="og:description" content="Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate the…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.19543">

<!--Generated on Sun May  5 19:04:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yucheng Hu 
<br class="ltx_break">East China University of Science and Technology 
<br class="ltx_break"><span id="id2.2.id1" class="ltx_text ltx_font_typewriter">huyc@mail.ecust.edu.cn</span> 
<br class="ltx_break"><span id="id3.3.id2" class="ltx_ERROR undefined">\And</span>Yuxing Lu<sup id="id4.4.id3" class="ltx_sup">∗</sup> 
<br class="ltx_break">Peking University 
<br class="ltx_break"><span id="id5.5.id4" class="ltx_text ltx_font_typewriter">yxlu0613@gmail.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1">대규모 언어 모델(LLM)은 자연어 처리(NLP)의 상당한 발전을 촉진했지만 환각과 도메인 특정 지식의 필요성과 같은 문제에 직면한다. 이를 완화하기 위해 최근 방법론에서는 외부 리소스에서 검색된 정보를 LLM과 통합하여 NLP 작업 전반에 걸쳐 성능을 크게 향상시켰다. 본 연구는 RALM(Retrieval-Augmented Language Models), RAG(Retrieval-Augmented Generation) 및 RAU(Retrieval-Augmented Understanding)에 대한 포괄적인 개요의 부재를 다루며, 이들의 패러다임, 진화, 분류 및 응용에 대한 심층적인 검토를 제공한다. 본 논문은 검색, 언어 모델, 증강을 포함한 RALM의 필수 구성 요소와 이들의 상호 작용이 다양한 모델 구조와 응용으로 이어지는 방법에 대해 논의한다. RALM은 번역 및 대화 시스템에서 지식 집약적인 응용 프로그램에 이르기까지 다양한 작업 범위에서 유용성을 보여준다. 설문 조사에는 RALM의 여러 평가 방법이 포함되어 있으며 평가에서 견고성, 정확성 및 관련성의 중요성을 강조한다. 또한 RALM의 한계, 특히 검색 품질 및 계산 효율성에 대한 한계를 인정하여 향후 연구에 대한 방향을 제공한다. 결론적으로, 이 조사는 RALM, 그들의 잠재력 및 NLP의 미래 개발을 위한 방법에 대한 구조화된 통찰력을 제공하는 것을 목표로 한다. 본 논문은 <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/2471023025/RALM_Survey" target="_blank" title="">https://github.com/2471023025/RALM_Survey</a>라는 추가 연구를 위해 조사된 작업과 리소스를 포함하는 깃허브 리포지토리로 보완된다.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p class="ltx_p" id="p1.1.2"><span class="ltx_text ltx_font_bold" id="p1.1.2.1">RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.1" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.1.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.1.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.1.2.1.1.1" class="ltx_tr">
<span id="p1.1.1.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Yucheng Hu</span></span></span>
<span id="p1.1.1.2.1.2.2" class="ltx_tr">
<span id="p1.1.1.2.1.2.2.1" class="ltx_td ltx_align_center">East China University of Science and Technology</span></span>
<span id="p1.1.1.2.1.3.3" class="ltx_tr">
<span id="p1.1.1.2.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.1.2.1.3.3.1.1" class="ltx_text ltx_font_typewriter">huyc@mail.ecust.edu.cn</span></span></span>
</span>
</span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="p1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.1.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.1.1.1.1" class="ltx_tr">
<span id="p1.1.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Yuxing Lu<sup id="p1.1.1.1.1.1.1.1.1" class="ltx_sup"><span id="p1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium">∗</span></sup></span></span></span>
<span id="p1.1.1.1.1.2.1" class="ltx_tr">
<span id="p1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center">Peking University</span></span>
<span id="p1.1.1.1.1.3.2" class="ltx_tr">
<span id="p1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="p1.1.1.1.1.3.2.1.1" class="ltx_text ltx_font_typewriter">yxlu0613@gmail.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.19543/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="418" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 1:</span>이 설문 조사의 작업에 대한 일반적인 개요</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">자연 언어 처리(NLP)는 자연 언어를 사용하여 인간과 컴퓨터 간의 효과적인 커뮤니케이션을 가능하게 하는 이론적 및 방법론적 프레임워크의 연구에 전념하여 컴퓨터 과학 및 인공 지능의 영역 내에서 중요한 초점이다. 다학문 분야로서 NLP는 인간의 언어와 컴퓨터 데이터 간의 상호 변환을 실현하기 위한 목적으로 언어학, 컴퓨터 과학, 수학을 통합한다. 컴퓨터에 자연어를 처리하고 ‘이해’할 수 있는 능력을 부여해 자동 번역, 텍스트 분류, 감성 분석 등의 업무를 용이하게 하는 것이 궁극적인 목표다. NLP의 복잡성은 단어 분할, 품사 태깅, 구문 분석, 형태소 분석, 명명된 개체 인식 등을 포함하는 수많은 단계에서 분명하며, 이들 모두는 인공 지능 시스템에서 인간 언어 이해를 복제하는 문제에 기여한다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">전통적인 자연어 처리 작업은 통상적으로 통계 기반 알고리즘 <cite class="ltx_cite ltx_citemacro_cite">Hogenboom et al. (<a class="ltx_ref" href="#bib.bib46" title="">2010</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Serra et al. (<a class="ltx_ref" href="#bib.bib111" title="">2013</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Aussenac-Gilles and Sörgel (<a class="ltx_ref" href="#bib.bib5" title="">2005</a>)</cite>와 CNN(Convolutional Neural Network) <cite class="ltx_cite ltx_citemacro_cite">Yin et al. (<a class="ltx_ref" href="#bib.bib142" title="">2017</a>)</cite>, RNN(Recurrent Neural Network) <cite class="ltx_cite ltx_citemacro_cite">Banerjee et al. (<a class="ltx_ref" href="#bib.bib8" title="">2019</a>)</cite>, LSTM(Long Short-term Memory Network) <cite class="ltx_cite ltx_citemacro_cite">Yao and Guan (<a class="ltx_ref" href="#bib.bib138" title="">2018</a>)</cite> 등의 딥러닝 알고리즘을 사용한다. 최근 자연어 처리를 대표하는 대표적인 트랜스포머 아키텍처 <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="#bib.bib127" title="">2017</a>)</cite>가 등장하면서 그 인기가 크게 높아졌다. 트랜스포머 아키텍처는 자연어 처리 영역에서 두드러진 대형 언어 모델 <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="#bib.bib70" title="">2019</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a class="ltx_ref" href="#bib.bib96" title="">2020</a>)</cite>로서 지속적으로 향상된 성능을 보여주며, 그 성능을 연구하는 연구자들의 주목을 받고 있다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">오늘날 가장 널리 퍼진 LMs은 GPT 패밀리 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="#bib.bib95" title="">2019</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="#bib.bib13" title="">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="#bib.bib1" title="">2023</a>)</cite>와 Bert 패밀리 <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib79" title="">2019</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="#bib.bib24" title="">2018</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a class="ltx_ref" href="#bib.bib107" title="">2019</a>)</cite>로 여러 자연어 처리 작업에서 뛰어난 것으로 입증되었다. 이 중 AutoEncoder 언어 모델은 자연어 이해 작업에 특히 능숙한 반면 AutoRegressive 언어 모델은 자연어 생성 작업에 더 적합하다. 매개 변수 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="#bib.bib126" title="">2023b</a>)</cite> 및 모델 튜닝 <cite class="ltx_cite ltx_citemacro_cite">Han et al. (<a class="ltx_ref" href="#bib.bib38" title="">2023</a>)</cite>가 LLM의 성능을 향상시킬 수 있는 반면, “hallucination” <cite class="ltx_cite ltx_citemacro_cite">Ji et al. (<a class="ltx_ref" href="#bib.bib55" title="">2023</a>)</cite>의 현상은 지속된다. 또한, 지식 집약적인 작업 <cite class="ltx_cite ltx_citemacro_cite">Feng et al. (<a class="ltx_ref" href="#bib.bib28" title="">2023</a>)</cite>를 효과적으로 처리하는 LMs의 한계와 그들의 지식 <cite class="ltx_cite ltx_citemacro_cite">Mousavi et al. (<a class="ltx_ref" href="#bib.bib88" title="">2024</a>)</cite>를 신속하게 업데이트하지 못하는 한계는 일관되게 명백하다. 그 결과, 다수의 연구자 <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="#bib.bib71" title="">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a class="ltx_ref" href="#bib.bib53" title="">2020b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Khandelwal et al. (<a class="ltx_ref" href="#bib.bib63" title="">2019</a>)</cite>는 외부 지식을 얻기 위해 검색 기법을 사용했으며, 이는 언어 모델이 많은 태스크에서 향상된 성능을 달성하는 데 도움을 줄 수 있다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">현재 LLM의 성능을 향상시키기 위한 검색 증강의 사용에 대한 조사가 부족하다. <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. (<a class="ltx_ref" href="#bib.bib153" title="">2023</a>)</cite>는 멀티모달리티를 위한 RAG에 대한 작업의 포괄적인 개요를 제공한다. <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. (<a class="ltx_ref" href="#bib.bib152" title="">2024a</a>)</cite>는 인공지능 생성 콘텐츠(AIGC) 도메인에 대한 검색 증강 생성 기술의 활용에 집중한다. 이 문서에서는 최근 RAG 작업에 대한 포괄적인 개요를 제공하지만 모든 관련 영역을 다루지는 않습니다. 또한, 기사는 전반적인 개발의 포괄적인 타임라인을 제공하기에 충분한 세부 사항이 부족하다. <cite class="ltx_cite ltx_citemacro_citet">Gao et al. (<a class="ltx_ref" href="#bib.bib31" title="">2023</a>)</cite> investigate the enhancement of RAG for large models. 이 문서에서는 최근 RAG 작업 중 일부를 요약하지만 검색기 및 생성기를 독립적으로 도입하여 후속 작업의 구성 요소와의 업그레이드 및 상호 작용에 도움이 되지 않습니다. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="#bib.bib73" title="">2022b</a>)</cite>는 텍스트 생성에만 초점을 맞춘다. 기사는 그림과 표가 적고 내용이 더 추상적이어서 독자의 이해에 도움이 되지 않는다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">또한, RAG에 대한 조사는 NLP에서 검색 강화 방법에서 이야기의 절반만을 보여준다. NLG와 관련된 태스크는 검색 향상 기술을 필요로 할 뿐만 아니라 NLU 태스크는 외부 정보를 필요로 한다. 현재까지 NLP의 스펙트럼 전반에 걸쳐 증강 검색 기술의 적용을 철저히 검토하는 포괄적인 조사는 부족하다. 현재 상황을 개선하기 위해 본 논문에서는 다음과 같은 공헌점을 제시한다.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.19543/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="318" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 2:</span>Retriever가 LM과 상호 작용하는 세 가지 다른 방식</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p" id="S1.p6.1">(1) 이 기사는 단순히 RAG와 관련된 작업에 초점을 맞추는 것이 아니라 RALM에 상당한 중점을 두고 NLP의 개념과 일치한다. 세대와 관련된 작업은 NLG와 일치하고 나머지 작업은 NLU와 일치한다.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p" id="S1.p7.1">(2) RALM의 두 구성요소인 Retriever와 Language Model에 대해 상세히 기술하고, 이 두 구성요소의 서로 다른 인터랙션 모드를 처음으로 정밀하게 정의한다.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p class="ltx_p" id="S1.p8.1">(3) 연관된 제한들의 분석과 함께, 현재 RALM의 공통적이고 신규한 애플리케이션들의 요약과 함께, RALM 작업 스케줄의 포괄적인 개요가 제공된다. 이러한 한계점에 대한 잠재적인 해결책과 향후 연구 방향에 대한 권장 사항이 제안된다.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p class="ltx_p" id="S1.p9.1">그림 <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">1</span></a>는 RALM 메소드의 프레임워크에 대한 일반적인 개요를 제공한다. 다음은 논문의 요약입니다. 섹션 <a class="ltx_ref" href="#S2" title="2 Definition ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">2</span></a>는 RALM을 정의합니다. 섹션 <a class="ltx_ref" href="#S3" title="3 Retriever ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">3</span></a>는 RALM에서 리트리버의 작업에 대한 자세한 분류 및 요약을 제공한다. 섹션 <a class="ltx_ref" href="#S4" title="4 Language Models ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">4</span></a>는 RALM에서 LMs의 작업에 대한 상세한 분류 및 요약을 제공한다. 섹션 <a class="ltx_ref" href="#S5" title="5 RALM Enhancement ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">5</span></a>는 RALM에 대한 특정 개선의 분류 및 요약을 제공한다. RALM의 섹션 <a class="ltx_ref" href="#S6" title="6 Data Sources ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">6</span></a>는 검색된 데이터의 소스의 분류 및 요약이다. 섹션 <a class="ltx_ref" href="#S7" title="7 Applications ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">7</span></a>는 RALM 애플리케이션의 요약이다. 섹션 <a class="ltx_ref" href="#S8" title="8 Evaluation ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">8</span></a>는 RALM 평가 및 벤치마크를 요약한 것이다. 마지막으로 <a class="ltx_ref" href="#S9" title="9 Disscussion ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">9</span></a>절은 기존 RALM의 한계와 향후 작업에 대한 방향에 대한 논의이다.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Definition</h2>

<figure id="S2.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.19543/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="373" height="483" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span>세 가지 유형의 상호 작용의 로드맵. 보라색 영역은 순차적 상호작용 RALM 모델에 대한 작업을 나타내고 빨간색 상자는 순차적 다중 상호작용 RALM 모델에 대한 작업을 나타내며 노란색 영역은 병렬적 상호작용 RALM 모델에 대한 작업을 나타낸다.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.6">검색-증강 언어 모델(Retrieval-Augmented Language Model, RALM)은 검색된 정보로 LM의 출력을 정제하여 사용자에게 만족스러운 결과를 얻는 과정이다. 이 섹션에서는 검색기가 언어 모델과 상호 작용하는 방식을 범주화하여 RALM의 다양한 모드에 대한 자세한 정의를 제공합니다. 상호작용의 구체적인 범주화는 그림 <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">2</span></a>에서 볼 수 있다. 또한, 각 인터랙션 방식의 발전 이력은 <a class="ltx_ref" href="#S2.F3" title="Figure 3 ‣ 2 Definition ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">3</span></a>에서 확인할 수 있다. <math alttext="z" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">z</annotation></semantics></math>가 검색된 메시지이고, <math alttext="x" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">x</annotation></semantics></math>가 입력이고, <math alttext="y" class="ltx_Math" display="inline" id="S2.p1.3.m3.1"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">y</annotation></semantics></math>가 출력이고, <math alttext="F()" class="ltx_Math" display="inline" id="S2.p1.4.m4.1"><semantics id="S2.p1.4.m4.1a"><mrow id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">F</mi><mo id="S2.p1.4.m4.1.1.1" lspace="0em" rspace="0em" xref="S2.p1.4.m4.1.1.1.cmml">​</mo><mrow id="S2.p1.4.m4.1.1.3.2" xref="S2.p1.4.m4.1.1.cmml"><mo id="S2.p1.4.m4.1.1.3.2.1" stretchy="false" xref="S2.p1.4.m4.1.1.3.1.cmml">(</mo><mo id="S2.p1.4.m4.1.1.3.2.2" stretchy="false" xref="S2.p1.4.m4.1.1.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><times id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1"></times><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">𝐹</ci><list id="S2.p1.4.m4.1.1.3.1.cmml" xref="S2.p1.4.m4.1.1.3.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">F()</annotation></semantics></math>가 언어 모델 또는 데이터 처리 함수 중 어느 하나의 함수라고 가정하면, <math alttext="x" class="ltx_Math" display="inline" id="S2.p1.5.m5.1"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">x</annotation></semantics></math> 및 <math alttext="z" class="ltx_Math" display="inline" id="S2.p1.6.m6.1"><semantics id="S2.p1.6.m6.1a"><mi id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">z</annotation></semantics></math>를 독립 변수로 하여 RALM의 기본 아키텍처는 다음과 같이 정의된다:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="y=F(x,z)" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.3" xref="S2.E1.m1.2.3.cmml"><mi id="S2.E1.m1.2.3.2" xref="S2.E1.m1.2.3.2.cmml">y</mi><mo id="S2.E1.m1.2.3.1" xref="S2.E1.m1.2.3.1.cmml">=</mo><mrow id="S2.E1.m1.2.3.3" xref="S2.E1.m1.2.3.3.cmml"><mi id="S2.E1.m1.2.3.3.2" xref="S2.E1.m1.2.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.3.3.1" xref="S2.E1.m1.2.3.3.1.cmml">​</mo><mrow id="S2.E1.m1.2.3.3.3.2" xref="S2.E1.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.3.3.3.2.1" xref="S2.E1.m1.2.3.3.3.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">x</mi><mo id="S2.E1.m1.2.3.3.3.2.2" xref="S2.E1.m1.2.3.3.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">z</mi><mo stretchy="false" id="S2.E1.m1.2.3.3.3.2.3" xref="S2.E1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.3.cmml" xref="S2.E1.m1.2.3"><eq id="S2.E1.m1.2.3.1.cmml" xref="S2.E1.m1.2.3.1"></eq><ci id="S2.E1.m1.2.3.2.cmml" xref="S2.E1.m1.2.3.2">𝑦</ci><apply id="S2.E1.m1.2.3.3.cmml" xref="S2.E1.m1.2.3.3"><times id="S2.E1.m1.2.3.3.1.cmml" xref="S2.E1.m1.2.3.3.1"></times><ci id="S2.E1.m1.2.3.3.2.cmml" xref="S2.E1.m1.2.3.3.2">𝐹</ci><interval closure="open" id="S2.E1.m1.2.3.3.3.1.cmml" xref="S2.E1.m1.2.3.3.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑥</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝑧</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">y=F(x,z)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Sequential Single Interaction</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.10" class="ltx_p">The sequential single interaction process involves finding the Top-K relevant documents <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">z</annotation></semantics></math> to input <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">x</annotation></semantics></math> through a retriever <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="P_{\eta}(z|x)" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><msub id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.3.2" xref="S2.SS1.p1.3.m3.1.1.3.2.cmml">P</mi><mi id="S2.SS1.p1.3.m3.1.1.3.3" xref="S2.SS1.p1.3.m3.1.1.3.3.cmml">η</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">​</mo><mrow id="S2.SS1.p1.3.m3.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.3.m3.1.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.3.m3.1.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS1.p1.3.m3.1.1.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS1.p1.3.m3.1.1.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.SS1.p1.3.m3.1.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><times id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2"></times><apply id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2">𝑃</ci><ci id="S2.SS1.p1.3.m3.1.1.3.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3">𝜂</ci></apply><apply id="S2.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p1.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.2">𝑧</ci><ci id="S2.SS1.p1.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">P_{\eta}(z|x)</annotation></semantics></math>, where <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\eta</annotation></semantics></math> is a parameter of the retriever. Subsequently, the language model <math id="S2.SS1.p1.5.m5.3" class="ltx_Math" alttext="P_{\theta}(y_{i}|x,z,y_{r})" display="inline"><semantics id="S2.SS1.p1.5.m5.3a"><mrow id="S2.SS1.p1.5.m5.3.3" xref="S2.SS1.p1.5.m5.3.3.cmml"><msub id="S2.SS1.p1.5.m5.3.3.3" xref="S2.SS1.p1.5.m5.3.3.3.cmml"><mi id="S2.SS1.p1.5.m5.3.3.3.2" xref="S2.SS1.p1.5.m5.3.3.3.2.cmml">P</mi><mi id="S2.SS1.p1.5.m5.3.3.3.3" xref="S2.SS1.p1.5.m5.3.3.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.5.m5.3.3.2" xref="S2.SS1.p1.5.m5.3.3.2.cmml">​</mo><mrow id="S2.SS1.p1.5.m5.3.3.1.1" xref="S2.SS1.p1.5.m5.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.5.m5.3.3.1.1.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.5.m5.3.3.1.1.1" xref="S2.SS1.p1.5.m5.3.3.1.1.1.cmml"><msub id="S2.SS1.p1.5.m5.3.3.1.1.1.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.cmml"><mi id="S2.SS1.p1.5.m5.3.3.1.1.1.3.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.2.cmml">y</mi><mi id="S2.SS1.p1.5.m5.3.3.1.1.1.3.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.SS1.p1.5.m5.3.3.1.1.1.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.2.cmml">|</mo><mrow id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.2.cmml"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">x</mi><mo id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.2.cmml">,</mo><mi id="S2.SS1.p1.5.m5.2.2" xref="S2.SS1.p1.5.m5.2.2.cmml">z</mi><mo id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.2.cmml">,</mo><msub id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.2.cmml">y</mi><mi id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.3.cmml">r</mi></msub></mrow></mrow><mo stretchy="false" id="S2.SS1.p1.5.m5.3.3.1.1.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.3b"><apply id="S2.SS1.p1.5.m5.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3"><times id="S2.SS1.p1.5.m5.3.3.2.cmml" xref="S2.SS1.p1.5.m5.3.3.2"></times><apply id="S2.SS1.p1.5.m5.3.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.3.3.3.1.cmml" xref="S2.SS1.p1.5.m5.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.5.m5.3.3.3.2.cmml" xref="S2.SS1.p1.5.m5.3.3.3.2">𝑃</ci><ci id="S2.SS1.p1.5.m5.3.3.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3.3.3">𝜃</ci></apply><apply id="S2.SS1.p1.5.m5.3.3.1.1.1.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1"><csymbol cd="latexml" id="S2.SS1.p1.5.m5.3.3.1.1.1.2.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.2">conditional</csymbol><apply id="S2.SS1.p1.5.m5.3.3.1.1.1.3.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.3.3.1.1.1.3.1.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3">subscript</csymbol><ci id="S2.SS1.p1.5.m5.3.3.1.1.1.3.2.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.2">𝑦</ci><ci id="S2.SS1.p1.5.m5.3.3.1.1.1.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.3">𝑖</ci></apply><list id="S2.SS1.p1.5.m5.3.3.1.1.1.1.2.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝑥</ci><ci id="S2.SS1.p1.5.m5.2.2.cmml" xref="S2.SS1.p1.5.m5.2.2">𝑧</ci><apply id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.2">𝑦</ci><ci id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.3">𝑟</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.3c">P_{\theta}(y_{i}|x,z,y_{r})</annotation></semantics></math> receives input <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">x</annotation></semantics></math> along with relevant documents <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mi id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><ci id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">z</annotation></semantics></math> and outputs the i-th token <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">y</mi><mi id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝑦</ci><ci id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">y_{i}</annotation></semantics></math>. Parameter <math id="S2.SS1.p1.9.m9.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><mi id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><ci id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">\theta</annotation></semantics></math> is used, along with relevant output tokens <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="y_{r}" display="inline"><semantics id="S2.SS1.p1.10.m10.1a"><msub id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml"><mi id="S2.SS1.p1.10.m10.1.1.2" xref="S2.SS1.p1.10.m10.1.1.2.cmml">y</mi><mi id="S2.SS1.p1.10.m10.1.1.3" xref="S2.SS1.p1.10.m10.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><apply id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.1.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S2.SS1.p1.10.m10.1.1.2.cmml" xref="S2.SS1.p1.10.m10.1.1.2">𝑦</ci><ci id="S2.SS1.p1.10.m10.1.1.3.cmml" xref="S2.SS1.p1.10.m10.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">y_{r}</annotation></semantics></math>. The number of relevant output tokens is related to the location and type of language model. The RALM for sequential single interaction is defined as follows:</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.3" class="ltx_Math" alttext="y_{i}=LM(z,x,y_{r})" display="block"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml"><msub id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><mi id="S2.E2.m1.3.3.3.2" xref="S2.E2.m1.3.3.3.2.cmml">y</mi><mi id="S2.E2.m1.3.3.3.3" xref="S2.E2.m1.3.3.3.3.cmml">i</mi></msub><mo id="S2.E2.m1.3.3.2" xref="S2.E2.m1.3.3.2.cmml">=</mo><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.cmml"><mi id="S2.E2.m1.3.3.1.3" xref="S2.E2.m1.3.3.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.2" xref="S2.E2.m1.3.3.1.2.cmml">​</mo><mi id="S2.E2.m1.3.3.1.4" xref="S2.E2.m1.3.3.1.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.2a" xref="S2.E2.m1.3.3.1.2.cmml">​</mo><mrow id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">z</mi><mo id="S2.E2.m1.3.3.1.1.1.3" xref="S2.E2.m1.3.3.1.1.2.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">x</mi><mo id="S2.E2.m1.3.3.1.1.1.4" xref="S2.E2.m1.3.3.1.1.2.cmml">,</mo><msub id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml">y</mi><mi id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml">r</mi></msub><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.5" xref="S2.E2.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3"><eq id="S2.E2.m1.3.3.2.cmml" xref="S2.E2.m1.3.3.2"></eq><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3">subscript</csymbol><ci id="S2.E2.m1.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.2">𝑦</ci><ci id="S2.E2.m1.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3">𝑖</ci></apply><apply id="S2.E2.m1.3.3.1.cmml" xref="S2.E2.m1.3.3.1"><times id="S2.E2.m1.3.3.1.2.cmml" xref="S2.E2.m1.3.3.1.2"></times><ci id="S2.E2.m1.3.3.1.3.cmml" xref="S2.E2.m1.3.3.1.3">𝐿</ci><ci id="S2.E2.m1.3.3.1.4.cmml" xref="S2.E2.m1.3.3.1.4">𝑀</ci><vector id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑧</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝑥</ci><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2">𝑦</ci><ci id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3">𝑟</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">y_{i}=LM(z,x,y_{r})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p1.11">RALM이 처음 제안되었을 때 많은 연구자들이 이 방법을 사용했는데, 특히 <cite class="ltx_cite ltx_citemacro_citet">Lewis et al. (<a class="ltx_ref" href="#bib.bib71" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Guu et al. (<a class="ltx_ref" href="#bib.bib36" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Izacard and Grave (<a class="ltx_ref" href="#bib.bib53" title="">2020b</a>)</cite>와 같은 독창적인 아이디어와 일치했기 때문이다.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Sequential Multiple Interactions</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p1.1">RALM 기술이 발전함에 따라 연구자들은 긴 대화 생성과 다중 홉 문제를 해결하기 위해 단일 상호작용이 불충분하다는 것을 발견했다. 따라서, 리트리버와 언어 모델 사이의 다중 상호작용을 갖는 방법이 제안되었다. 이 방법에서, 연구자는 단계 s를 포함하고 전형적으로 언어 모델이 먼저 출력을 생성하도록 한다. 검색 기술이 필요한 경우, 출력된 콘텐츠를 검색에 사용하고 관련 수식은 다음과 같이 표현된다:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.2" class="ltx_Math" alttext="y_{s}=LM(z,x|y_{<s})" display="block"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml"><msub id="S2.E3.m1.2.2.3" xref="S2.E3.m1.2.2.3.cmml"><mi id="S2.E3.m1.2.2.3.2" xref="S2.E3.m1.2.2.3.2.cmml">y</mi><mi id="S2.E3.m1.2.2.3.3" xref="S2.E3.m1.2.2.3.3.cmml">s</mi></msub><mo id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.2.cmml">=</mo><mrow id="S2.E3.m1.2.2.1" xref="S2.E3.m1.2.2.1.cmml"><mi id="S2.E3.m1.2.2.1.3" xref="S2.E3.m1.2.2.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.2" xref="S2.E3.m1.2.2.1.2.cmml">​</mo><mi id="S2.E3.m1.2.2.1.4" xref="S2.E3.m1.2.2.1.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.2a" xref="S2.E3.m1.2.2.1.2.cmml">​</mo><mrow id="S2.E3.m1.2.2.1.1.1" xref="S2.E3.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.1.1.1.2" xref="S2.E3.m1.2.2.1.1.2.cmml">(</mo><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">z</mi><mo id="S2.E3.m1.2.2.1.1.1.3" xref="S2.E3.m1.2.2.1.1.2.cmml">,</mo><mrow id="S2.E3.m1.2.2.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S2.E3.m1.2.2.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml">|</mo><msub id="S2.E3.m1.2.2.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.3.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.3.2" xref="S2.E3.m1.2.2.1.1.1.1.3.2.cmml">y</mi><mrow id="S2.E3.m1.2.2.1.1.1.1.3.3" xref="S2.E3.m1.2.2.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.3.3.2" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2.cmml"></mi><mo id="S2.E3.m1.2.2.1.1.1.1.3.3.1" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S2.E3.m1.2.2.1.1.1.1.3.3.3" xref="S2.E3.m1.2.2.1.1.1.1.3.3.3.cmml">s</mi></mrow></msub></mrow><mo stretchy="false" id="S2.E3.m1.2.2.1.1.1.4" xref="S2.E3.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2"><eq id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2"></eq><apply id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.3.1.cmml" xref="S2.E3.m1.2.2.3">subscript</csymbol><ci id="S2.E3.m1.2.2.3.2.cmml" xref="S2.E3.m1.2.2.3.2">𝑦</ci><ci id="S2.E3.m1.2.2.3.3.cmml" xref="S2.E3.m1.2.2.3.3">𝑠</ci></apply><apply id="S2.E3.m1.2.2.1.cmml" xref="S2.E3.m1.2.2.1"><times id="S2.E3.m1.2.2.1.2.cmml" xref="S2.E3.m1.2.2.1.2"></times><ci id="S2.E3.m1.2.2.1.3.cmml" xref="S2.E3.m1.2.2.1.3">𝐿</ci><ci id="S2.E3.m1.2.2.1.4.cmml" xref="S2.E3.m1.2.2.1.4">𝑀</ci><interval closure="open" id="S2.E3.m1.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1"><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">𝑧</ci><apply id="S2.E3.m1.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S2.E3.m1.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1">conditional</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.2">𝑥</ci><apply id="S2.E3.m1.2.2.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.2">𝑦</ci><apply id="S2.E3.m1.2.2.1.1.1.1.3.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3"><lt id="S2.E3.m1.2.2.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S2.E3.m1.2.2.1.1.1.1.3.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2">absent</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.3">𝑠</ci></apply></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">y_{s}=LM(z,x|y_{&lt;s})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.p2.2">여기서 <math alttext="y_{s}" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><msub id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mi id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">y</mi><mi id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">𝑦</ci><ci id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">y_{s}</annotation></semantics></math>는 현재 단계 <math alttext="s" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m2.1"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">s</annotation></semantics></math>에서 생성된 토큰을 나타낸다. 이 방법을 사용한 연구자 중 가장 유명한 것은 <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a class="ltx_ref" href="#bib.bib58" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">He et al. (<a class="ltx_ref" href="#bib.bib39" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Khattab et al. (<a class="ltx_ref" href="#bib.bib64" title="">2022</a>)</cite>이다.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Parallel Interaction</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p1.3">앞서 언급한 모든 접근법에서 정보의 흐름은 리트리버에서 언어 모델에 이르기까지 또는 언어 모델에서 리트리버에 이르기까지 명확한 순차적 구조를 갖는다. 그러나, 이러한 순차적 구조는 모든 도메인에서 최적이 아닐 수 있고 덜 확장가능할 수 있으므로, 대안적인 접근법을 고려하는 것이 중요하다. 연구자들은 사용자 입력 <math alttext="x" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">x</annotation></semantics></math>에 대해 리트리버와 언어 모델이 독립적으로 작동하는 새로운 병렬 구조를 제안했다. 이어서, 출력 <math alttext="y" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">y</annotation></semantics></math>는 가중 보간법에 의해 결정된다. <math alttext="I()" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.1"><semantics id="S2.SS3.p1.3.m3.1a"><mrow id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml"><mi id="S2.SS3.p1.3.m3.1.1.2" xref="S2.SS3.p1.3.m3.1.1.2.cmml">I</mi><mo id="S2.SS3.p1.3.m3.1.1.1" lspace="0em" rspace="0em" xref="S2.SS3.p1.3.m3.1.1.1.cmml">​</mo><mrow id="S2.SS3.p1.3.m3.1.1.3.2" xref="S2.SS3.p1.3.m3.1.1.cmml"><mo id="S2.SS3.p1.3.m3.1.1.3.2.1" stretchy="false" xref="S2.SS3.p1.3.m3.1.1.3.1.cmml">(</mo><mo id="S2.SS3.p1.3.m3.1.1.3.2.2" stretchy="false" xref="S2.SS3.p1.3.m3.1.1.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1"><times id="S2.SS3.p1.3.m3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1.1"></times><ci id="S2.SS3.p1.3.m3.1.1.2.cmml" xref="S2.SS3.p1.3.m3.1.1.2">𝐼</ci><list id="S2.SS3.p1.3.m3.1.1.3.1.cmml" xref="S2.SS3.p1.3.m3.1.1.3.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">I()</annotation></semantics></math>는 보간 함수이다. 관련 방정식은 다음과 같이 표현된다:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.3" class="ltx_Math" alttext="y=I(LM(x,y_{r}),z)" display="block"><semantics id="S2.E4.m1.3a"><mrow id="S2.E4.m1.3.3" xref="S2.E4.m1.3.3.cmml"><mi id="S2.E4.m1.3.3.3" xref="S2.E4.m1.3.3.3.cmml">y</mi><mo id="S2.E4.m1.3.3.2" xref="S2.E4.m1.3.3.2.cmml">=</mo><mrow id="S2.E4.m1.3.3.1" xref="S2.E4.m1.3.3.1.cmml"><mi id="S2.E4.m1.3.3.1.3" xref="S2.E4.m1.3.3.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.2" xref="S2.E4.m1.3.3.1.2.cmml">​</mo><mrow id="S2.E4.m1.3.3.1.1.1" xref="S2.E4.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.2" xref="S2.E4.m1.3.3.1.1.2.cmml">(</mo><mrow id="S2.E4.m1.3.3.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.2.cmml">​</mo><mi id="S2.E4.m1.3.3.1.1.1.1.4" xref="S2.E4.m1.3.3.1.1.1.1.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.1.1.1.2a" xref="S2.E4.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">x</mi><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">,</mo><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml">r</mi></msub><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.1.1.1.4" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.3.3.1.1.1.3" xref="S2.E4.m1.3.3.1.1.2.cmml">,</mo><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">z</mi><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.4" xref="S2.E4.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.3b"><apply id="S2.E4.m1.3.3.cmml" xref="S2.E4.m1.3.3"><eq id="S2.E4.m1.3.3.2.cmml" xref="S2.E4.m1.3.3.2"></eq><ci id="S2.E4.m1.3.3.3.cmml" xref="S2.E4.m1.3.3.3">𝑦</ci><apply id="S2.E4.m1.3.3.1.cmml" xref="S2.E4.m1.3.3.1"><times id="S2.E4.m1.3.3.1.2.cmml" xref="S2.E4.m1.3.3.1.2"></times><ci id="S2.E4.m1.3.3.1.3.cmml" xref="S2.E4.m1.3.3.1.3">𝐼</ci><interval closure="open" id="S2.E4.m1.3.3.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1"><apply id="S2.E4.m1.3.3.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1"><times id="S2.E4.m1.3.3.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.2"></times><ci id="S2.E4.m1.3.3.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.3">𝐿</ci><ci id="S2.E4.m1.3.3.1.1.1.1.4.cmml" xref="S2.E4.m1.3.3.1.1.1.1.4">𝑀</ci><interval closure="open" id="S2.E4.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1"><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">𝑥</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.3">𝑟</ci></apply></interval></apply><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝑧</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.3c">y=I(LM(x,y_{r}),z)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS3.p1.7">상기 특정 보간 함수는:</p>
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.4" class="ltx_Math" alttext="p(y|x)=\lambda p_{R}(y|x)+(1-\lambda)p_{LM}(y|x)" display="block"><semantics id="S2.E5.m1.4a"><mrow id="S2.E5.m1.4.4" xref="S2.E5.m1.4.4.cmml"><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.3" xref="S2.E5.m1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.2.cmml">​</mo><mrow id="S2.E5.m1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S2.E5.m1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.E5.m1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.4.4.5" xref="S2.E5.m1.4.4.5.cmml">=</mo><mrow id="S2.E5.m1.4.4.4" xref="S2.E5.m1.4.4.4.cmml"><mrow id="S2.E5.m1.2.2.2.1" xref="S2.E5.m1.2.2.2.1.cmml"><mi id="S2.E5.m1.2.2.2.1.3" xref="S2.E5.m1.2.2.2.1.3.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.1.2" xref="S2.E5.m1.2.2.2.1.2.cmml">​</mo><msub id="S2.E5.m1.2.2.2.1.4" xref="S2.E5.m1.2.2.2.1.4.cmml"><mi id="S2.E5.m1.2.2.2.1.4.2" xref="S2.E5.m1.2.2.2.1.4.2.cmml">p</mi><mi id="S2.E5.m1.2.2.2.1.4.3" xref="S2.E5.m1.2.2.2.1.4.3.cmml">R</mi></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.1.2a" xref="S2.E5.m1.2.2.2.1.2.cmml">​</mo><mrow id="S2.E5.m1.2.2.2.1.1.1" xref="S2.E5.m1.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.2.2.2.1.1.1.2" xref="S2.E5.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.2.2.2.1.1.1.1" xref="S2.E5.m1.2.2.2.1.1.1.1.cmml"><mi id="S2.E5.m1.2.2.2.1.1.1.1.2" xref="S2.E5.m1.2.2.2.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S2.E5.m1.2.2.2.1.1.1.1.1" xref="S2.E5.m1.2.2.2.1.1.1.1.1.cmml">|</mo><mi id="S2.E5.m1.2.2.2.1.1.1.1.3" xref="S2.E5.m1.2.2.2.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.E5.m1.2.2.2.1.1.1.3" xref="S2.E5.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.4.4.4.4" xref="S2.E5.m1.4.4.4.4.cmml">+</mo><mrow id="S2.E5.m1.4.4.4.3" xref="S2.E5.m1.4.4.4.3.cmml"><mrow id="S2.E5.m1.3.3.3.2.1.1" xref="S2.E5.m1.3.3.3.2.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.3.3.3.2.1.1.2" xref="S2.E5.m1.3.3.3.2.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.3.3.3.2.1.1.1" xref="S2.E5.m1.3.3.3.2.1.1.1.cmml"><mn id="S2.E5.m1.3.3.3.2.1.1.1.2" xref="S2.E5.m1.3.3.3.2.1.1.1.2.cmml">1</mn><mo id="S2.E5.m1.3.3.3.2.1.1.1.1" xref="S2.E5.m1.3.3.3.2.1.1.1.1.cmml">−</mo><mi id="S2.E5.m1.3.3.3.2.1.1.1.3" xref="S2.E5.m1.3.3.3.2.1.1.1.3.cmml">λ</mi></mrow><mo stretchy="false" id="S2.E5.m1.3.3.3.2.1.1.3" xref="S2.E5.m1.3.3.3.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E5.m1.4.4.4.3.3" xref="S2.E5.m1.4.4.4.3.3.cmml">​</mo><msub id="S2.E5.m1.4.4.4.3.4" xref="S2.E5.m1.4.4.4.3.4.cmml"><mi id="S2.E5.m1.4.4.4.3.4.2" xref="S2.E5.m1.4.4.4.3.4.2.cmml">p</mi><mrow id="S2.E5.m1.4.4.4.3.4.3" xref="S2.E5.m1.4.4.4.3.4.3.cmml"><mi id="S2.E5.m1.4.4.4.3.4.3.2" xref="S2.E5.m1.4.4.4.3.4.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.4.4.4.3.4.3.1" xref="S2.E5.m1.4.4.4.3.4.3.1.cmml">​</mo><mi id="S2.E5.m1.4.4.4.3.4.3.3" xref="S2.E5.m1.4.4.4.3.4.3.3.cmml">M</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.4.4.4.3.3a" xref="S2.E5.m1.4.4.4.3.3.cmml">​</mo><mrow id="S2.E5.m1.4.4.4.3.2.1" xref="S2.E5.m1.4.4.4.3.2.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.4.4.4.3.2.1.2" xref="S2.E5.m1.4.4.4.3.2.1.1.cmml">(</mo><mrow id="S2.E5.m1.4.4.4.3.2.1.1" xref="S2.E5.m1.4.4.4.3.2.1.1.cmml"><mi id="S2.E5.m1.4.4.4.3.2.1.1.2" xref="S2.E5.m1.4.4.4.3.2.1.1.2.cmml">y</mi><mo fence="false" id="S2.E5.m1.4.4.4.3.2.1.1.1" xref="S2.E5.m1.4.4.4.3.2.1.1.1.cmml">|</mo><mi id="S2.E5.m1.4.4.4.3.2.1.1.3" xref="S2.E5.m1.4.4.4.3.2.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.E5.m1.4.4.4.3.2.1.3" xref="S2.E5.m1.4.4.4.3.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.4b"><apply id="S2.E5.m1.4.4.cmml" xref="S2.E5.m1.4.4"><eq id="S2.E5.m1.4.4.5.cmml" xref="S2.E5.m1.4.4.5"></eq><apply id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><times id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.2"></times><ci id="S2.E5.m1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.3">𝑝</ci><apply id="S2.E5.m1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2">𝑦</ci><ci id="S2.E5.m1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.3">𝑥</ci></apply></apply><apply id="S2.E5.m1.4.4.4.cmml" xref="S2.E5.m1.4.4.4"><plus id="S2.E5.m1.4.4.4.4.cmml" xref="S2.E5.m1.4.4.4.4"></plus><apply id="S2.E5.m1.2.2.2.1.cmml" xref="S2.E5.m1.2.2.2.1"><times id="S2.E5.m1.2.2.2.1.2.cmml" xref="S2.E5.m1.2.2.2.1.2"></times><ci id="S2.E5.m1.2.2.2.1.3.cmml" xref="S2.E5.m1.2.2.2.1.3">𝜆</ci><apply id="S2.E5.m1.2.2.2.1.4.cmml" xref="S2.E5.m1.2.2.2.1.4"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.1.4.1.cmml" xref="S2.E5.m1.2.2.2.1.4">subscript</csymbol><ci id="S2.E5.m1.2.2.2.1.4.2.cmml" xref="S2.E5.m1.2.2.2.1.4.2">𝑝</ci><ci id="S2.E5.m1.2.2.2.1.4.3.cmml" xref="S2.E5.m1.2.2.2.1.4.3">𝑅</ci></apply><apply id="S2.E5.m1.2.2.2.1.1.1.1.cmml" xref="S2.E5.m1.2.2.2.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.2.1.1.1.1.1">conditional</csymbol><ci id="S2.E5.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E5.m1.2.2.2.1.1.1.1.2">𝑦</ci><ci id="S2.E5.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E5.m1.2.2.2.1.1.1.1.3">𝑥</ci></apply></apply><apply id="S2.E5.m1.4.4.4.3.cmml" xref="S2.E5.m1.4.4.4.3"><times id="S2.E5.m1.4.4.4.3.3.cmml" xref="S2.E5.m1.4.4.4.3.3"></times><apply id="S2.E5.m1.3.3.3.2.1.1.1.cmml" xref="S2.E5.m1.3.3.3.2.1.1"><minus id="S2.E5.m1.3.3.3.2.1.1.1.1.cmml" xref="S2.E5.m1.3.3.3.2.1.1.1.1"></minus><cn type="integer" id="S2.E5.m1.3.3.3.2.1.1.1.2.cmml" xref="S2.E5.m1.3.3.3.2.1.1.1.2">1</cn><ci id="S2.E5.m1.3.3.3.2.1.1.1.3.cmml" xref="S2.E5.m1.3.3.3.2.1.1.1.3">𝜆</ci></apply><apply id="S2.E5.m1.4.4.4.3.4.cmml" xref="S2.E5.m1.4.4.4.3.4"><csymbol cd="ambiguous" id="S2.E5.m1.4.4.4.3.4.1.cmml" xref="S2.E5.m1.4.4.4.3.4">subscript</csymbol><ci id="S2.E5.m1.4.4.4.3.4.2.cmml" xref="S2.E5.m1.4.4.4.3.4.2">𝑝</ci><apply id="S2.E5.m1.4.4.4.3.4.3.cmml" xref="S2.E5.m1.4.4.4.3.4.3"><times id="S2.E5.m1.4.4.4.3.4.3.1.cmml" xref="S2.E5.m1.4.4.4.3.4.3.1"></times><ci id="S2.E5.m1.4.4.4.3.4.3.2.cmml" xref="S2.E5.m1.4.4.4.3.4.3.2">𝐿</ci><ci id="S2.E5.m1.4.4.4.3.4.3.3.cmml" xref="S2.E5.m1.4.4.4.3.4.3.3">𝑀</ci></apply></apply><apply id="S2.E5.m1.4.4.4.3.2.1.1.cmml" xref="S2.E5.m1.4.4.4.3.2.1"><csymbol cd="latexml" id="S2.E5.m1.4.4.4.3.2.1.1.1.cmml" xref="S2.E5.m1.4.4.4.3.2.1.1.1">conditional</csymbol><ci id="S2.E5.m1.4.4.4.3.2.1.1.2.cmml" xref="S2.E5.m1.4.4.4.3.2.1.1.2">𝑦</ci><ci id="S2.E5.m1.4.4.4.3.2.1.1.3.cmml" xref="S2.E5.m1.4.4.4.3.2.1.1.3">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.4c">p(y|x)=\lambda p_{R}(y|x)+(1-\lambda)p_{LM}(y|x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.SS3.p1.6" class="ltx_p">where the retrieved output tokens are denoted by <math id="S2.SS3.p1.4.m1.1" class="ltx_Math" alttext="p_{R}(y|x)" display="inline"><semantics id="S2.SS3.p1.4.m1.1a"><mrow id="S2.SS3.p1.4.m1.1.1" xref="S2.SS3.p1.4.m1.1.1.cmml"><msub id="S2.SS3.p1.4.m1.1.1.3" xref="S2.SS3.p1.4.m1.1.1.3.cmml"><mi id="S2.SS3.p1.4.m1.1.1.3.2" xref="S2.SS3.p1.4.m1.1.1.3.2.cmml">p</mi><mi id="S2.SS3.p1.4.m1.1.1.3.3" xref="S2.SS3.p1.4.m1.1.1.3.3.cmml">R</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS3.p1.4.m1.1.1.2" xref="S2.SS3.p1.4.m1.1.1.2.cmml">​</mo><mrow id="S2.SS3.p1.4.m1.1.1.1.1" xref="S2.SS3.p1.4.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p1.4.m1.1.1.1.1.2" xref="S2.SS3.p1.4.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.4.m1.1.1.1.1.1" xref="S2.SS3.p1.4.m1.1.1.1.1.1.cmml"><mi id="S2.SS3.p1.4.m1.1.1.1.1.1.2" xref="S2.SS3.p1.4.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S2.SS3.p1.4.m1.1.1.1.1.1.1" xref="S2.SS3.p1.4.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS3.p1.4.m1.1.1.1.1.1.3" xref="S2.SS3.p1.4.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.SS3.p1.4.m1.1.1.1.1.3" xref="S2.SS3.p1.4.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m1.1b"><apply id="S2.SS3.p1.4.m1.1.1.cmml" xref="S2.SS3.p1.4.m1.1.1"><times id="S2.SS3.p1.4.m1.1.1.2.cmml" xref="S2.SS3.p1.4.m1.1.1.2"></times><apply id="S2.SS3.p1.4.m1.1.1.3.cmml" xref="S2.SS3.p1.4.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p1.4.m1.1.1.3.1.cmml" xref="S2.SS3.p1.4.m1.1.1.3">subscript</csymbol><ci id="S2.SS3.p1.4.m1.1.1.3.2.cmml" xref="S2.SS3.p1.4.m1.1.1.3.2">𝑝</ci><ci id="S2.SS3.p1.4.m1.1.1.3.3.cmml" xref="S2.SS3.p1.4.m1.1.1.3.3">𝑅</ci></apply><apply id="S2.SS3.p1.4.m1.1.1.1.1.1.cmml" xref="S2.SS3.p1.4.m1.1.1.1.1"><csymbol cd="latexml" id="S2.SS3.p1.4.m1.1.1.1.1.1.1.cmml" xref="S2.SS3.p1.4.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS3.p1.4.m1.1.1.1.1.1.2.cmml" xref="S2.SS3.p1.4.m1.1.1.1.1.1.2">𝑦</ci><ci id="S2.SS3.p1.4.m1.1.1.1.1.1.3.cmml" xref="S2.SS3.p1.4.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m1.1c">p_{R}(y|x)</annotation></semantics></math>, the language model generated output tokens are denoted by <math id="S2.SS3.p1.5.m2.1" class="ltx_Math" alttext="p_{LM}(y|x)" display="inline"><semantics id="S2.SS3.p1.5.m2.1a"><mrow id="S2.SS3.p1.5.m2.1.1" xref="S2.SS3.p1.5.m2.1.1.cmml"><msub id="S2.SS3.p1.5.m2.1.1.3" xref="S2.SS3.p1.5.m2.1.1.3.cmml"><mi id="S2.SS3.p1.5.m2.1.1.3.2" xref="S2.SS3.p1.5.m2.1.1.3.2.cmml">p</mi><mrow id="S2.SS3.p1.5.m2.1.1.3.3" xref="S2.SS3.p1.5.m2.1.1.3.3.cmml"><mi id="S2.SS3.p1.5.m2.1.1.3.3.2" xref="S2.SS3.p1.5.m2.1.1.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.5.m2.1.1.3.3.1" xref="S2.SS3.p1.5.m2.1.1.3.3.1.cmml">​</mo><mi id="S2.SS3.p1.5.m2.1.1.3.3.3" xref="S2.SS3.p1.5.m2.1.1.3.3.3.cmml">M</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.SS3.p1.5.m2.1.1.2" xref="S2.SS3.p1.5.m2.1.1.2.cmml">​</mo><mrow id="S2.SS3.p1.5.m2.1.1.1.1" xref="S2.SS3.p1.5.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p1.5.m2.1.1.1.1.2" xref="S2.SS3.p1.5.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.5.m2.1.1.1.1.1" xref="S2.SS3.p1.5.m2.1.1.1.1.1.cmml"><mi id="S2.SS3.p1.5.m2.1.1.1.1.1.2" xref="S2.SS3.p1.5.m2.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S2.SS3.p1.5.m2.1.1.1.1.1.1" xref="S2.SS3.p1.5.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS3.p1.5.m2.1.1.1.1.1.3" xref="S2.SS3.p1.5.m2.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.SS3.p1.5.m2.1.1.1.1.3" xref="S2.SS3.p1.5.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.5.m2.1b"><apply id="S2.SS3.p1.5.m2.1.1.cmml" xref="S2.SS3.p1.5.m2.1.1"><times id="S2.SS3.p1.5.m2.1.1.2.cmml" xref="S2.SS3.p1.5.m2.1.1.2"></times><apply id="S2.SS3.p1.5.m2.1.1.3.cmml" xref="S2.SS3.p1.5.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p1.5.m2.1.1.3.1.cmml" xref="S2.SS3.p1.5.m2.1.1.3">subscript</csymbol><ci id="S2.SS3.p1.5.m2.1.1.3.2.cmml" xref="S2.SS3.p1.5.m2.1.1.3.2">𝑝</ci><apply id="S2.SS3.p1.5.m2.1.1.3.3.cmml" xref="S2.SS3.p1.5.m2.1.1.3.3"><times id="S2.SS3.p1.5.m2.1.1.3.3.1.cmml" xref="S2.SS3.p1.5.m2.1.1.3.3.1"></times><ci id="S2.SS3.p1.5.m2.1.1.3.3.2.cmml" xref="S2.SS3.p1.5.m2.1.1.3.3.2">𝐿</ci><ci id="S2.SS3.p1.5.m2.1.1.3.3.3.cmml" xref="S2.SS3.p1.5.m2.1.1.3.3.3">𝑀</ci></apply></apply><apply id="S2.SS3.p1.5.m2.1.1.1.1.1.cmml" xref="S2.SS3.p1.5.m2.1.1.1.1"><csymbol cd="latexml" id="S2.SS3.p1.5.m2.1.1.1.1.1.1.cmml" xref="S2.SS3.p1.5.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS3.p1.5.m2.1.1.1.1.1.2.cmml" xref="S2.SS3.p1.5.m2.1.1.1.1.1.2">𝑦</ci><ci id="S2.SS3.p1.5.m2.1.1.1.1.1.3.cmml" xref="S2.SS3.p1.5.m2.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.5.m2.1c">p_{LM}(y|x)</annotation></semantics></math>, and the weights are denoted by <math id="S2.SS3.p1.6.m3.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS3.p1.6.m3.1a"><mi id="S2.SS3.p1.6.m3.1.1" xref="S2.SS3.p1.6.m3.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.6.m3.1b"><ci id="S2.SS3.p1.6.m3.1.1.cmml" xref="S2.SS3.p1.6.m3.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.6.m3.1c">\lambda</annotation></semantics></math>. Among the researchers who have employed this method, the most renowned are <cite class="ltx_cite ltx_citemacro_citet">Khandelwal et&nbsp;al. (<a href="#bib.bib63" title="" class="ltx_ref">2019</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Alon et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">He et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1:</span>Summary of Retrievers in RALM works.</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:384.6pt;height:590.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.9pt,173.2pt) scale(0.63,0.63) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2">Category</td>
<td id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Technique</td>
<td id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Year</td>
<td id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Reference</td>
</tr>
<tr id="S2.T1.1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.1.2.2.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.2.2.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.2.2.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.1.3.3.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.3.3.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.3.3.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.1.4.4.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.4.4.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.4.4.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Zhong et&nbsp;al. (<a href="#bib.bib156" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.1.5.5.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.5.5.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.5.5.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2024</td>
<td id="S2.T1.1.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.1.6.6.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.6.6.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.6.6.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.6.6.5.1" class="ltx_text" style="color:#882D00;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib53" title="" class="ltx_ref">2020b</a>)</cite></span></td>
</tr>
<tr id="S2.T1.1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.1.7.7.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.7.7.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.7.7.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Mallen et&nbsp;al. (<a href="#bib.bib86" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.1.8.8.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.8.8.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.8.8.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2024</td>
<td id="S2.T1.1.1.8.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Schick et&nbsp;al. (<a href="#bib.bib110" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.9.9" class="ltx_tr">
<td id="S2.T1.1.1.9.9.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.9.9.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.9.9.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Xu et&nbsp;al. (<a href="#bib.bib134" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.10.10" class="ltx_tr">
<td id="S2.T1.1.1.10.10.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.10.10.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.10.10.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.10.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.11.11" class="ltx_tr">
<td id="S2.T1.1.1.11.11.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.11.11.2.1" class="ltx_text">Word Frequency</span></td>
<td id="S2.T1.1.1.11.11.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.11.11.3.1" class="ltx_text">BM25<cite class="ltx_cite ltx_citemacro_cite">Robertson et&nbsp;al. (<a href="#bib.bib102" title="" class="ltx_ref">1995</a>)</cite></span></td>
<td id="S2.T1.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.11.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Yu et&nbsp;al. (<a href="#bib.bib146" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.12.12" class="ltx_tr">
<td id="S2.T1.1.1.12.12.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.12.12.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.12.12.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.12.12.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Alon et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.13.13" class="ltx_tr">
<td id="S2.T1.1.1.13.13.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.13.13.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.13.13.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.13.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Borgeaud et&nbsp;al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.14.14" class="ltx_tr">
<td id="S2.T1.1.1.14.14.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.14.14.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.14.14.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.14.14.3.1" class="ltx_text">KNN search</span></td>
<td id="S2.T1.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2019</td>
<td id="S2.T1.1.1.14.14.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Khandelwal et&nbsp;al. (<a href="#bib.bib63" title="" class="ltx_ref">2019</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.15.15" class="ltx_tr">
<td id="S2.T1.1.1.15.15.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.15.15.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">GUD-IR<cite class="ltx_cite ltx_citemacro_cite">Madaan et&nbsp;al. (<a href="#bib.bib85" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.15.15.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Madaan et&nbsp;al. (<a href="#bib.bib85" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.16.16" class="ltx_tr">
<td id="S2.T1.1.1.16.16.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.16.16.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.16.16.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">GAR<cite class="ltx_cite ltx_citemacro_cite">Mao et&nbsp;al. (<a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S2.T1.1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.16.16.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Mao et&nbsp;al. (<a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.17.17" class="ltx_tr">
<td id="S2.T1.1.1.17.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.17.17.1.1" class="ltx_text">Sparse Retrieval</span></td>
<td id="S2.T1.1.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.17.17.2.1" class="ltx_text">Sparse Vector Representation</span></td>
<td id="S2.T1.1.1.17.17.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Spider<cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.17.17.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.18.18" class="ltx_tr">
<td id="S2.T1.1.1.18.18.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.18.18.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.18.18.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.18.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.18.18.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard et&nbsp;al. (<a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.19.19" class="ltx_tr">
<td id="S2.T1.1.1.19.19.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.19.19.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.19.19.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.19.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.19.19.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.20.20" class="ltx_tr">
<td id="S2.T1.1.1.20.20.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.20.20.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.20.20.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.20.20.3.1" class="ltx_text">COLBERTV2<cite class="ltx_cite ltx_citemacro_cite">Santhanam et&nbsp;al. (<a href="#bib.bib108" title="" class="ltx_ref">2021</a>)</cite></span></td>
<td id="S2.T1.1.1.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.20.20.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Hofstätter et&nbsp;al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.21.21" class="ltx_tr">
<td id="S2.T1.1.1.21.21.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.21.21.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.21.21.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.21.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.21.21.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et&nbsp;al. (<a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.22.22" class="ltx_tr">
<td id="S2.T1.1.1.22.22.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.22.22.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.22.22.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.22.22.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.23.23" class="ltx_tr">
<td id="S2.T1.1.1.23.23.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.23.23.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.23.23.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.23.23.3.1" class="ltx_text">Contriever<cite class="ltx_cite ltx_citemacro_cite">Izacard et&nbsp;al. (<a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite></span></td>
<td id="S2.T1.1.1.23.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.23.23.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Guu et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.24.24" class="ltx_tr">
<td id="S2.T1.1.1.24.24.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.24.24.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.24.24.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">DBE<cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite>
</td>
<td id="S2.T1.1.1.24.24.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.24.24.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.25.25" class="ltx_tr">
<td id="S2.T1.1.1.25.25.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.25.25.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.25.25.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">DKR<cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.25.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.25.25.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.26.26" class="ltx_tr">
<td id="S2.T1.1.1.26.26.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.26.26.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.26.26.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.26.26.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.26.26.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.27.27" class="ltx_tr">
<td id="S2.T1.1.1.27.27.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.27.27.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.27.27.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.27.27.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.27.27.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib53" title="" class="ltx_ref">2020b</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.28.28" class="ltx_tr">
<td id="S2.T1.1.1.28.28.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.28.28.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.28.28.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.28.28.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.28.28.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Karpukhin et&nbsp;al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.29.29" class="ltx_tr">
<td id="S2.T1.1.1.29.29.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.29.29.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.29.29.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.29.29.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.29.29.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Mallen et&nbsp;al. (<a href="#bib.bib86" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.30.30" class="ltx_tr">
<td id="S2.T1.1.1.30.30.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.30.30.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.30.30.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.30.30.3.1" class="ltx_text">DPR<cite class="ltx_cite ltx_citemacro_cite">Karpukhin et&nbsp;al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite></span></td>
<td id="S2.T1.1.1.30.30.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.30.30.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.31.31" class="ltx_tr">
<td id="S2.T1.1.1.31.31.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.31.31.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.31.31.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">GTR<cite class="ltx_cite ltx_citemacro_cite">Ni et&nbsp;al. (<a href="#bib.bib91" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.31.31.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2019</td>
<td id="S2.T1.1.1.31.31.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Lee et&nbsp;al. (<a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.32.32" class="ltx_tr">
<td id="S2.T1.1.1.32.32.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.32.32.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.32.32.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">E2E-NR<cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.32.32.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.32.32.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.33.33" class="ltx_tr">
<td id="S2.T1.1.1.33.33.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.33.33.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.33.33.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">REALM<cite class="ltx_cite ltx_citemacro_cite">Guu et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S2.T1.1.1.33.33.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.33.33.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.34.34" class="ltx_tr">
<td id="S2.T1.1.1.34.34.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.34.34.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.34.34.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">ORQA<cite class="ltx_cite ltx_citemacro_cite">Lee et&nbsp;al. (<a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S2.T1.1.1.34.34.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.34.34.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.35.35" class="ltx_tr">
<td id="S2.T1.1.1.35.35.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.35.35.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.35.35.2.1" class="ltx_text">Word Embedding</span></td>
<td id="S2.T1.1.1.35.35.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">EMDR2<cite class="ltx_cite ltx_citemacro_cite">Singh et&nbsp;al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.35.35.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.35.35.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Santhanam et&nbsp;al. (<a href="#bib.bib108" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.36.36" class="ltx_tr">
<td id="S2.T1.1.1.36.36.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.36.36.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.36.36.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">MuRAG<cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2022b</a>)</cite>
</td>
<td id="S2.T1.1.1.36.36.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.36.36.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2022b</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.37.37" class="ltx_tr">
<td id="S2.T1.1.1.37.37.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.37.37.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.37.37.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">RA-CM3<cite class="ltx_cite ltx_citemacro_cite">Yasunaga et&nbsp;al. (<a href="#bib.bib139" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.37.37.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.37.37.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Yasunaga et&nbsp;al. (<a href="#bib.bib139" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.38.38" class="ltx_tr">
<td id="S2.T1.1.1.38.38.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.38.38.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.38.38.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">RE-IMAGEN<cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2022c</a>)</cite>
</td>
<td id="S2.T1.1.1.38.38.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.38.38.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2022c</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.39.39" class="ltx_tr">
<td id="S2.T1.1.1.39.39.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.39.39.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.39.39.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">MDTIG<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a href="#bib.bib72" title="" class="ltx_ref">2022a</a>)</cite>
</td>
<td id="S2.T1.1.1.39.39.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.39.39.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a href="#bib.bib72" title="" class="ltx_ref">2022a</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.40.40" class="ltx_tr">
<td id="S2.T1.1.1.40.40.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.40.40.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.40.40.2.1" class="ltx_text">Multimodal Retrieval</span></td>
<td id="S2.T1.1.1.40.40.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">RDM<cite class="ltx_cite ltx_citemacro_cite">Blattmann et&nbsp;al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.40.40.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.40.40.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Blattmann et&nbsp;al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.41.41" class="ltx_tr">
<td id="S2.T1.1.1.41.41.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.41.41.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.41.41.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">DRAGON<cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib74" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S2.T1.1.1.41.41.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.41.41.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.42.42" class="ltx_tr">
<td id="S2.T1.1.1.42.42.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.42.42.1.1" class="ltx_text">Dense Retrieval</span></td>
<td id="S2.T1.1.1.42.42.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.42.42.2.1" class="ltx_text">Knowledge Distillation</span></td>
<td id="S2.T1.1.1.42.42.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">REPULG LSR<cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S2.T1.1.1.42.42.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.42.42.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.43.43" class="ltx_tr">
<td id="S2.T1.1.1.43.43.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.43.43.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">FLARE<cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S2.T1.1.1.43.43.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.43.43.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.44.44" class="ltx_tr">
<td id="S2.T1.1.1.44.44.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.44.44.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">RAG-Robust<cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.44.44.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.44.44.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.45.45" class="ltx_tr">
<td id="S2.T1.1.1.45.45.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.45.45.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">IADG<cite class="ltx_cite ltx_citemacro_cite">Komeili et&nbsp;al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.45.45.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.45.45.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Komeili et&nbsp;al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.46.46" class="ltx_tr">
<td id="S2.T1.1.1.46.46.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"><span id="S2.T1.1.1.46.46.1.1" class="ltx_text">Internet Retrieval</span></td>
<td id="S2.T1.1.1.46.46.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Webgpt<cite class="ltx_cite ltx_citemacro_cite">Nakano et&nbsp;al. (<a href="#bib.bib89" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.46.46.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.46.46.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Nakano et&nbsp;al. (<a href="#bib.bib89" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.47.47" class="ltx_tr">
<td id="S2.T1.1.1.47.47.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.47.47.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">DuckDuckGo+BM25<cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S2.T1.1.1.47.47.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.47.47.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.48.48" class="ltx_tr">
<td id="S2.T1.1.1.48.48.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.48.48.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Internet+TF-IDF<cite class="ltx_cite ltx_citemacro_cite">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.48.48.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.48.48.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.49.49" class="ltx_tr">
<td id="S2.T1.1.1.49.49.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.49.49.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">REVEAL <cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.49.49.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.49.49.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.50.50" class="ltx_tr">
<td id="S2.T1.1.1.50.50.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.50.50.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">NAG-ERE<cite class="ltx_cite ltx_citemacro_cite">Hua and Wang (<a href="#bib.bib49" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S2.T1.1.1.50.50.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2018</td>
<td id="S2.T1.1.1.50.50.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Hua and Wang (<a href="#bib.bib49" title="" class="ltx_ref">2018</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.51.51" class="ltx_tr">
<td id="S2.T1.1.1.51.51.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.51.51.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Internet+BM25<cite class="ltx_cite ltx_citemacro_cite">Adolphs et&nbsp;al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.51.51.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.51.51.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Adolphs et&nbsp;al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.52.52" class="ltx_tr">
<td id="S2.T1.1.1.52.52.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"><span id="S2.T1.1.1.52.52.1.1" class="ltx_text">Hybrid Retrieval</span></td>
<td id="S2.T1.1.1.52.52.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">kNN+BM25+translation model<cite class="ltx_cite ltx_citemacro_cite">Boytsov et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S2.T1.1.1.52.52.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2016</td>
<td id="S2.T1.1.1.52.52.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Boytsov et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Retriever</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">리트리버는 RALM 아키텍처에서 중요한 역할을 한다. 리트리버를 통해 얻은 정보는 LM의 정확도를 크게 향상시킬 수 있다. 이 섹션에서는 RALM 아키텍처에서 일반적으로 사용되는 검색 방법에 대한 요약을 제공한다. 검색 방법은 그 방법과 출처에 따라 희소 검색, 밀도 검색, 인터넷 검색, 하이브리드 검색의 네 가지 범주로 분류된다. 표 <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2.3 Parallel Interaction ‣ 2 Definition ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">1</span></a>는 RALM에서 리트리버의 특정 어플리케이션에 대한 정보를 나열한 것이다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Sparse Retriever</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.1">검색 기법의 제안 이후 일정 기간 동안 희소 검색은 문제, 특히 지식을 기반으로 하는 문제를 해결하는 데 간단하고 효과적인 도구임을 증명한다. 희소 검색의 주요 장점 중 하나는 단순하기 때문에 관련된 차원이 적기 때문에 기존 색인 시스템에 쉽게 통합될 수 있다는 것이다. <cite class="ltx_cite ltx_citemacro_cite">Hambarde and Proenca (<a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>이것은 인간의 인지 과정과 일치한다. 또한 희소 검색은 일반화하기 쉽고 효율적이다. RALM에서 사용되는 희소 검색은 단어 빈도와 희소 벡터 표현의 두 가지로 분류할 수 있다. 양자 간의 선택은 기계 학습의 활용 여부에 달려 있다.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Word Frequency</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">초기 단계에서 개인은 종종 고전적이고 효과적인 것으로 간주되는 TF-IDF <cite class="ltx_cite ltx_citemacro_cite">Ramos et al. (<a class="ltx_ref" href="#bib.bib101" title="">2003</a>)</cite> 및 BM25 <cite class="ltx_cite ltx_citemacro_cite">Robertson et al. (<a class="ltx_ref" href="#bib.bib102" title="">1995</a>)</cite> 알고리즘과 같이 관련 콘텐츠의 매칭을 수반하는 검색 방법을 사용한다.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">TF-IDF 알고리즘은 용어 빈도(TF)와 역 문서 빈도(IDF)를 활용하여 관련성을 표현함으로써 단순성과 신속성의 장점을 가지며, 말뭉치가 변하지 않더라도 단어별 TF-IDF 값을 미리 계산할 수 있다. RALM에서 <cite class="ltx_cite ltx_citemacro_citet">Lazaridou et al. (<a class="ltx_ref" href="#bib.bib67" title="">2022</a>)</cite>는 TF-IDF 알고리즘을 사용하여 사용자 쿼리 및 호출에서 얻은 정보를 Google 검색 API에 일치시킨다. <cite class="ltx_cite ltx_citemacro_citet">Hua and Wang (<a class="ltx_ref" href="#bib.bib49" title="">2018</a>)</cite>도 알고리즘을 사용하여 생성된 결과를 채점합니다. BM25는 TF-IDF에 대한 향상을 나타낸다. 사용자의 질의를 고려하여 관련도 점수를 문서에 대한 각 질의어의 관련도의 가중합으로 계산한다. IDF 알고리즘은 각 단어의 가중치를 도출하는데 사용되지만, 특정 요인의 영향력의 강도가 무한하지 않도록 두 가지 조절 요인에 의해 개선된다. 이것은 상식과 일치한다. 우수한 일반화 능력으로 인해, 많은 검색-증강 언어 모델(RaLM) 아키텍처, 특히 오픈 도메인을 지향하는 아키텍처는 BM25를 검색 방법으로서, 예를 들어 <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a class="ltx_ref" href="#bib.bib58" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Ram et al. (<a class="ltx_ref" href="#bib.bib97" title="">2023</a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_citet">Zhong et al. (<a class="ltx_ref" href="#bib.bib156" title="">2022</a>)</cite>를 채용한다.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Sparse Vector Representation</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">단순한 용어 매칭이 더 이상 수요를 충족시키기에 충분하지 않다는 것이 명백해졌다. 수동 라벨링은 동의어 문제와 같은 문제를 해결할 수 있지만 자원 집약적인 방법이다. 기계 학습의 증가에 따라, 희소 벡터는 이제 단어들을 표현하고 그들 사이의 거리를 계산함으로써 단어들을 검색하는데 사용된다. <cite class="ltx_cite ltx_citemacro_cite">Hambarde and Proenca (<a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite> 희소 벡터 표현 기법은 질의 및 문서에 대한 희소 벡터를 구성한다는 점에서 용어 매칭 기법과 차이가 있다. 이러한 표현의 목적은 질의와 문서를 잠재 공간에 배치하는 각 입력 텍스트의 의미적 본질을 포착하는 것이다.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Ram et al. (<a class="ltx_ref" href="#bib.bib98" title="">2021</a>)</cite>는 동일한 반복 스팬을 갖는 두 개의 단락이 주어졌을 때, 하나는 질의를 구성하는 데 사용되고 다른 하나는 검색 대상으로 사용된다는 사실을 활용했다. 반복 스팬이 포함되지 않은 문서의 나머지 단락은 부정적인 예로 사용되었으며 <cite class="ltx_cite ltx_citemacro_citet">Ram et al. (<a class="ltx_ref" href="#bib.bib97" title="">2023</a>)</cite>는 이 리트리버를 RALM 아키텍처에 처음으로 적용했다. 반면, 언어 모델(LM)을 이용하여 제안된 <cite class="ltx_cite ltx_citemacro_citet">Mao et al. (<a class="ltx_ref" href="#bib.bib87" title="">2020</a>)</cite>와 <cite class="ltx_cite ltx_citemacro_citet">Madaan et al. (<a class="ltx_ref" href="#bib.bib85" title="">2022</a>)</cite>는 주어진 질문으로부터 새로운 질의를 생성하고 이를 이용하여 관련 문서를 검색함으로써 희소 벡터 표현의 검색 정확도를 높인다. 그러나 마오의 접근법은 질의 확장을 강조하는 반면, 마단의 접근법은 사용자의 입력에 대한 이해를 강조한다.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dense Retriever</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.1">딥러닝 기술의 등장은 검색 분야를 크게 변화시켰다. 비록 어느 정도의 이해 가능성을 희생하는 것을 의미하더라도, 검색 정확도를 향상시키기 위해 딥 러닝 기술을 사용하는 것에 대한 관심이 증가하고 있다. 듀얼 인코더 구조는 밀집 검색 모델을 위한 일반적인 설계이다. <cite class="ltx_cite ltx_citemacro_cite">Hambarde and Proenca (<a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite> 시스템은 별개의 입력, 즉 쿼리 및 문서를 수신하고, 각각의 입력에 대해 독립적으로 밀집 임베딩을 생성하는 두 개의 별개의 네트워크로 구성된다. RALM에 더 적합한 높은 정확도와 이중 인코더 구조로 인해 대부분의 기사는 검색기를 구축하기 위해 밀집 인덱싱 방법을 사용한다. 본 절에서는 각 검색 방법의 특성에 따라 밀집 검색을 워드 임베딩, 멀티모달 검색, 데이터 증류 세 가지 유형으로 분류한다.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Word Embedding</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">워드 임베딩은 자연어 처리에서 일반적인 접근법이다. 희소 벡터 표현과 유사하게, 그들은 단어들을 고차원 벡터 공간에 투영하기 위해 딥 러닝 기법들을 사용한다. RALM 아키텍처의 몇 가지 기사는 이 기술을 활용하며, 우리는 기술할 대표적인 기사를 선택했다.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Karpukhin et al. (<a class="ltx_ref" href="#bib.bib61" title="">2020</a>)</cite>는 저차원 연속 공간에서 모든 구절을 색인하는 DPR 검색 모델을 제안하였다. 이것은 판독기가 런타임에서 입력 문제와 연관된 제1 k개의 통로들을 효율적으로 검색할 수 있게 한다. 조밀한 인코더는 임의의 텍스트 통로를 d차원 실수값 벡터에 매핑하기 위해 사용되어, 검색에 사용되는 모든 M개의 통로들에 대한 인덱스를 생성한다. RALM 아키텍처에서 리트리버로서 우수한 성능으로 인해, DPR은 <cite class="ltx_cite ltx_citemacro_citet">Lewis et al. (<a class="ltx_ref" href="#bib.bib71" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Izacard and Grave (<a class="ltx_ref" href="#bib.bib53" title="">2020b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Karpukhin et al. (<a class="ltx_ref" href="#bib.bib61" title="">2020</a>)</cite>와 같은 연구자들에 의해 널리 채택되었다. <cite class="ltx_cite ltx_citemacro_citet">Izacard and Grave (<a class="ltx_ref" href="#bib.bib52" title="">2020a</a>)</cite>는 공유 파라미터를 통해 질문과 문단에 대해 동일한 인코딩 기능을 사용한다는 점에서 DPR과 달리 유사한 전술을 취한다. 개입을 더욱 최소화하고 수동 주석 비용을 줄이기 위해 <cite class="ltx_cite ltx_citemacro_citet">Izacard et al. (<a class="ltx_ref" href="#bib.bib51" title="">2021</a>)</cite>는 감독되지 않은 데이터를 사용하여 훈련된 Contriever라는 또 다른 리트리버를 제안했다. 연속적인 고밀도 임베딩을 기반으로 하며 이중 인코더 구조를 가지고 있다. 이전 레이어의 출력에 평균 풀링을 적용하여 각 쿼리 또는 문서에 대해 하나의 벡터 표현을 얻었다. 쿼리와 각 문서 간의 유사성 점수는 해당 임베딩 간의 내적을 계산하여 얻었다. 연구자 <cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et al. (<a class="ltx_ref" href="#bib.bib119" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Sachan et al. (<a class="ltx_ref" href="#bib.bib106" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Guu et al. (<a class="ltx_ref" href="#bib.bib36" title="">2020</a>)</cite>는 감독되지 않은 데이터를 활용하는 능력으로 인해 RALM 아키텍처에서 리트리버로 사용해 왔다.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Multimodal Retrieval</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">멀티모달 태스크에 대한 검색 기법은 텍스트 전용 태스크 <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="#bib.bib153" title="">2023</a>)</cite>에 대한 검색 기법보다 복잡하다. 이는 서로 다른 상태에 대한 정보의 상호 변환을 수반하기 때문이다. 예를 들어, 이미지-텍스트 도메인에서, 조밀한 텍스트 검색을 위한 멀티모달 기법들은 상이한 모달리티들 <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="#bib.bib154" title="">2024b</a>, <a class="ltx_ref" href="#bib.bib155" title="">c</a>)</cite> 사이의 갭을 브릿징하는 수단으로서 관심을 끌었다. 연구자들은 검색 작업을 위해 텍스트 및 시각적 정보를 공유된 잠재 공간으로 인코딩하는 방법을 개발했다.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="#bib.bib72" title="">2022a</a>)</cite>는 멀티모달 태스크를 처리하기 위한 네 가지 매칭 알고리즘, 즉 문장 대 문장, 문장 대 이미지, 단어 대 단어 및 단어 대 이미지를 설계하였다. 그들은 보다 정확한 상관 계산을 위한 재가중치와 코사인 유사성 점수를 각 알고리즘의 효과를 탐구하는 기준으로 사용했다. <cite class="ltx_cite ltx_citemacro_citet">Yasunaga et al. (<a class="ltx_ref" href="#bib.bib139" title="">2022</a>)</cite>는 전자와 달리 CLIP <cite class="ltx_cite ltx_citemacro_cite">Ramesh et al. (<a class="ltx_ref" href="#bib.bib100" title="">2022</a>)</cite>를 단순 확장하여 멀티모달 문서를 텍스트와 이미지 컴포넌트로 분할하여 냉동 인코더로 인코딩하였다. L2 norm은 평균 풀링 기법을 사용하여 1로 스케일링되어 문서의 벡터 표현이 생성되었다. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="#bib.bib72" title="">2022a</a>)</cite>와 유사하게, 그들은 또한 관련성 점수로 최대 내부 제품 검색(MIPS)을 사용하여 궁극적으로 K개의 문서를 선택했다.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Knowledge Distillation</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">지식증류는 대용량 데이터베이스를 사용자의 질의에 보다 적합하도록 점진적으로 필터링하고 간소화하는 기법이다. <cite class="ltx_cite ltx_citemacro_cite">Gou et al. (<a class="ltx_ref" href="#bib.bib34" title="">2021</a>)</cite>는 사전 훈련된 더 큰 모델에서 더 작은 모델로 데이터를 전송하는 것을 포함하며, 종종 임베디드 매칭과 같은 방법을 사용한다. 기술이 발전함에 따라 LMs를 이용한 데이터 증류에도 연구가 진행되었다.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Shi et al. (<a class="ltx_ref" href="#bib.bib117" title="">2023b</a>)</cite>는 지식 증류를 사용하여 검색 프로세스를 네 가지 별개의 단계로 나눈다. 먼저, 문서를 검색하고 검색 가능성을 계산한다. 두 번째로, 검색된 문서들은 언어 모델을 이용하여 점수를 매긴다. 셋째, 검색 확률과 언어 모델 점수의 분포 사이의 KL 불일치를 최소화함으로써 검색 모델의 파라미터를 갱신한다. 마지막으로, 데이터의 인덱스들의 비동기식 업데이트가 수행된다. 이 기술을 기반으로 <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="#bib.bib74" title="">2023a</a>)</cite>는 지식증류의 정확도를 더욱 향상시킨다. 그들은 여러 인핸서를 사용하여 문장 절단 및 쿼리 강화와 증분 관련성 라벨 강화를 결합하는 데이터 증류 스킴을 제시한다.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Internet Retrieval</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.1">인터넷 검색 및 분류 기술의 발전으로 일부 연구자들은 플러그 앤 플레이 접근 방식인 인터넷 검색에 검색 노력을 집중했다. 이 접근법은 비전문가가 RALM의 혜택을 받을 수 있도록 하며 개방형 도메인과 일반화에 더 적합하다. 이 검색 모델의 또 다른 장점은 데이터베이스의 실시간 업데이트를 필요로 하지 않고 상용 검색 엔진으로부터의 업데이트에 의존한다는 것이다. 그러나, 단순성과 편리성의 장점에도 불구하고, RALM의 작업을 방해할 수 있는 인터넷 상의 상당량의 무관하고 심지어 유해한 정보가 존재한다. 효과적인 스크리닝 메커니즘이 구현되지 않으면 RALM의 효과가 크게 감소한다.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p2.1">상업용 검색 엔진 API를 직접 활용하는 대부분의 연구 <cite class="ltx_cite ltx_citemacro_cite">Yoran et al. (<a class="ltx_ref" href="#bib.bib144" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Nakano et al. (<a class="ltx_ref" href="#bib.bib89" title="">2021</a>)</cite>와 달리. <cite class="ltx_cite ltx_citemacro_citet">Komeili et al. (<a class="ltx_ref" href="#bib.bib65" title="">2021</a>)</cite>는 다중 상용 검색 엔진 API를 사용하는 대안적인 접근법을 제안한다. Bing Search API를 사용하여 각 쿼리에 대한 URL 목록을 생성하는 것이 좋습니다. 그런 다음 이러한 URL은 해당 쿼리에 대한 페이지 집합을 채우는 공용 크롤링 스냅숏에서 구성된 조회 테이블에서 페이지 콘텐츠를 검색하는 키로 사용됩니다. 또한, 평가는 URL이 영어 위키피디아로부터의 것인지 여부를 고려한다. 그렇다면 URL에서 페이지 제목을 추출하고 위키피디아 덤프에서 해당 페이지를 검색한다.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Hybrid Retrieval</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p1.1">연구자들이 다양한 검색 기법들의 장단점에 대한 더 나은 이해를 얻음에 따라, 이들은 전술한 바와 같이, 이들을 조합하는 것을 점점 더 선택하고 있다. 이는 RALM 아키텍처의 효과성 및 견고성을 개선하기 위해 이러한 기술들의 장점들을 추가로 이용하기를 희망하여 수행된다.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p2.1">부정확한 인터넷 검색 결과의 문제를 해결하기 위해 TF-IDF 알고리즘을 사용하여 검색 결과의 점수를 매기는 <cite class="ltx_cite ltx_citemacro_citet">Lazaridou et al. (<a class="ltx_ref" href="#bib.bib67" title="">2022</a>)</cite>를 제안하였다. 그들은 각 질문 q를 쿼리로 사용하고 Google Search API를 통해 Google Search에 호출을 발행했다. 각 질문에 대해 상위 20개 URL을 검색하고 HTML 콘텐츠를 파싱하여 클린 텍스트를 추출하여 각 질문 q에 대한 문서 D 세트를 생성했다. 관련 없는 정보가 사용자의 쿼리 해상도를 방해하는 것을 방지하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Hu et al. (<a class="ltx_ref" href="#bib.bib48" title="">2023</a>)</cite>는 게이팅 회로를 설계했다. 이 회로는 유사성을 계산하기 위해 이중 인코더 내적을 사용하고 용어 가중치에 기반한 게이팅 회로를 사용했다. 또한 <cite class="ltx_cite ltx_citemacro_citet">Boytsov et al. (<a class="ltx_ref" href="#bib.bib12" title="">2016</a>)</cite>는 검색 성능을 향상시키기 위해 번역 모델과 BM25를 결합하면서 용어 기반 검색을 k-Nearest Neighbors(kNN) 검색으로 대체하는 접근법을 제시하였다. 이 접근법은 모델이 용어와 전통적인 통계적 가중치 스킴 사이의 의미론적 관계를 고려할 수 있게 하여 보다 효율적인 검색 시스템을 초래했다.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2:</span>RALM methods in LMs의 Summary.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:387.5pt;height:586.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.1pt,103.0pt) scale(0.74,0.74) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2">Category</td>
<td id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Technique</td>
<td id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Year</td>
<td id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Reference</td>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<td id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2" rowspan="3"><span id="S3.T2.1.1.2.2.1.1" class="ltx_text">AutoEncoder Language Model</span></td>
<td id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.2.2.2.1" class="ltx_text">RoBERTa<cite class="ltx_cite ltx_citemacro_cite">Devlin et&nbsp;al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite></span></td>
<td id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.3.3" class="ltx_tr">
<td id="S3.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.4.4" class="ltx_tr">
<td id="S3.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BERT<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib79" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S3.T2.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.5.5" class="ltx_tr">
<td id="S3.T2.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="24"><span id="S3.T2.1.1.5.5.1.1" class="ltx_text">AutoRegressive Language Model</span></td>
<td id="S3.T2.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="12"><span id="S3.T2.1.1.5.5.2.1" class="ltx_text">GPT Family</span></td>
<td id="S3.T2.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.5.5.3.1" class="ltx_text">GPT-3.5</span></td>
<td id="S3.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.6.6" class="ltx_tr">
<td id="S3.T2.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.7.7" class="ltx_tr">
<td id="S3.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">GPT-2<cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a href="#bib.bib95" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S3.T2.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.8.8" class="ltx_tr">
<td id="S3.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">GPT-Neo<cite class="ltx_cite ltx_citemacro_cite">Black et&nbsp;al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T2.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Mallen et&nbsp;al. (<a href="#bib.bib86" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.9.9" class="ltx_tr">
<td id="S3.T2.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">chatGPT</td>
<td id="S3.T2.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Peng et&nbsp;al. (<a href="#bib.bib93" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.10.10" class="ltx_tr">
<td id="S3.T2.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.10.10.1.1" class="ltx_text">GPT-4<cite class="ltx_cite ltx_citemacro_cite">Achiam et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite></span></td>
<td id="S3.T2.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Asai et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.11.11" class="ltx_tr">
<td id="S3.T2.1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.11.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.12.12" class="ltx_tr">
<td id="S3.T2.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.12.12.1.1" class="ltx_text">GPT-3<cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite></span></td>
<td id="S3.T2.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Madaan et&nbsp;al. (<a href="#bib.bib85" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.13.13" class="ltx_tr">
<td id="S3.T2.1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.13.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.14.14" class="ltx_tr">
<td id="S3.T2.1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">GPT<cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a href="#bib.bib94" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S3.T2.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.14.14.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.15.15" class="ltx_tr">
<td id="S3.T2.1.1.15.15.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.15.15.1.1" class="ltx_text">GPT-J</span></td>
<td id="S3.T2.1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Schick et&nbsp;al. (<a href="#bib.bib110" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.16.16" class="ltx_tr">
<td id="S3.T2.1.1.16.16.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.16.16.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib47" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.17.17" class="ltx_tr">
<td id="S3.T2.1.1.17.17.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="6"><span id="S3.T2.1.1.17.17.1.1" class="ltx_text">Llama Family</span></td>
<td id="S3.T2.1.1.17.17.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="4"><span id="S3.T2.1.1.17.17.2.1" class="ltx_text">Llama2<cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib126" title="" class="ltx_ref">2023b</a>)</cite></span></td>
<td id="S3.T2.1.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.17.17.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Yan et&nbsp;al. (<a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.18.18" class="ltx_tr">
<td id="S3.T2.1.1.18.18.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.18.18.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Asai et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.19.19" class="ltx_tr">
<td id="S3.T2.1.1.19.19.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.19.19.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.20.20" class="ltx_tr">
<td id="S3.T2.1.1.20.20.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.20.20.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.21.21" class="ltx_tr">
<td id="S3.T2.1.1.21.21.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.21.21.1.1" class="ltx_text">Llama<cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib125" title="" class="ltx_ref">2023a</a>)</cite></span></td>
<td id="S3.T2.1.1.21.21.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.21.21.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.22.22" class="ltx_tr">
<td id="S3.T2.1.1.22.22.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.22.22.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.23.23" class="ltx_tr">
<td id="S3.T2.1.1.23.23.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="6"><span id="S3.T2.1.1.23.23.1.1" class="ltx_text">Others</span></td>
<td id="S3.T2.1.1.23.23.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Alpaca<cite class="ltx_cite ltx_citemacro_cite">Dubois et&nbsp;al. (<a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S3.T2.1.1.23.23.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.23.23.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Yan et&nbsp;al. (<a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.24.24" class="ltx_tr">
<td id="S3.T2.1.1.24.24.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.24.24.1.1" class="ltx_text">OPT<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a href="#bib.bib151" title="" class="ltx_ref">2022</a>)</cite></span></td>
<td id="S3.T2.1.1.24.24.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.24.24.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.25.25" class="ltx_tr">
<td id="S3.T2.1.1.25.25.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.25.25.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.26.26" class="ltx_tr">
<td id="S3.T2.1.1.26.26.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">XGLM<cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib76" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T2.1.1.26.26.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.26.26.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.27.27" class="ltx_tr">
<td id="S3.T2.1.1.27.27.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BLOOM<cite class="ltx_cite ltx_citemacro_cite">Workshop et&nbsp;al. (<a href="#bib.bib132" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T2.1.1.27.27.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.27.27.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.28.28" class="ltx_tr">
<td id="S3.T2.1.1.28.28.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Mistral<cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib56" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S3.T2.1.1.28.28.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.28.28.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib47" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.29.29" class="ltx_tr">
<td id="S3.T2.1.1.29.29.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2" rowspan="16"><span id="S3.T2.1.1.29.29.1.1" class="ltx_text">Encoder-Decoder Language Model</span></td>
<td id="S3.T2.1.1.29.29.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="9"><span id="S3.T2.1.1.29.29.2.1" class="ltx_text">T5<cite class="ltx_cite ltx_citemacro_cite">Raffel et&nbsp;al. (<a href="#bib.bib96" title="" class="ltx_ref">2020</a>)</cite></span></td>
<td id="S3.T2.1.1.29.29.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.29.29.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard et&nbsp;al. (<a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.30.30" class="ltx_tr">
<td id="S3.T2.1.1.30.30.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.30.30.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Hofstätter et&nbsp;al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.31.31" class="ltx_tr">
<td id="S3.T2.1.1.31.31.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.31.31.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.32.32" class="ltx_tr">
<td id="S3.T2.1.1.32.32.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.32.32.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.33.33" class="ltx_tr">
<td id="S3.T2.1.1.33.33.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.33.33.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2022b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.34.34" class="ltx_tr">
<td id="S3.T2.1.1.34.34.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.34.34.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Singh et&nbsp;al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.35.35" class="ltx_tr">
<td id="S3.T2.1.1.35.35.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2020</td>
<td id="S3.T2.1.1.35.35.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.36.36" class="ltx_tr">
<td id="S3.T2.1.1.36.36.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.36.36.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.37.37" class="ltx_tr">
<td id="S3.T2.1.1.37.37.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.37.37.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.38.38" class="ltx_tr">
<td id="S3.T2.1.1.38.38.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="7"><span id="S3.T2.1.1.38.38.1.1" class="ltx_text">BART<cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite></span></td>
<td id="S3.T2.1.1.38.38.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.38.38.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.39.39" class="ltx_tr">
<td id="S3.T2.1.1.39.39.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.39.39.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et&nbsp;al. (<a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.40.40" class="ltx_tr">
<td id="S3.T2.1.1.40.40.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2019</td>
<td id="S3.T2.1.1.40.40.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.41.41" class="ltx_tr">
<td id="S3.T2.1.1.41.41.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2020</td>
<td id="S3.T2.1.1.41.41.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.42.42" class="ltx_tr">
<td id="S3.T2.1.1.42.42.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.42.42.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.43.43" class="ltx_tr">
<td id="S3.T2.1.1.43.43.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2020</td>
<td id="S3.T2.1.1.43.43.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.44.44" class="ltx_tr">
<td id="S3.T2.1.1.44.44.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.44.44.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Language Models</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">비록 인간이 정보 검색에만 의존하곤 했지만, 언어 모델의 발전은 자연 언어 처리 분야에 혁명을 일으켜 더 활기차고 창의적이었다. 학습에서 도출된 파라미터만을 사용하여 작업을 완료하는 LM과 달리 RALM은 리트리버가 획득한 비모수적 메모리와 LM 자체의 모수적 메모리를 통합하여 반모수적 메모리를 생성함으로써 언어 모델의 성능을 향상시킨다. RALM 아키텍처에서 많은 연구자들은 평가를 위해 기성 언어 모델을 사용한다. 이 절에서는 RALM 아키텍처에서 일반적으로 사용되는 언어 모델을 소개하고 AutoEncoder 언어 모델, AutoRegressive 언어 모델 및 Encoder-Decoder 모델의 세 가지 범주로 분류한다. 표 <a class="ltx_ref" href="#S3.T2" title="Table 2 ‣ 3.4 Hybrid Retrieval ‣ 3 Retriever ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">2</span></a>는 RALM에서 LM의 특정 어플리케이션에 대한 정보를 나열한 것이다.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>AutoEncoder Language Model</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p1.1">AutoEncoder의 논리적 과정은 원래 입력(x로 설정)에 가중치가 부여되고 y에 매핑되며, 그 다음 역 가중치가 부여되고 z에 다시 매핑된다. 반복 훈련을 통해 손실 함수 L(H)가 최소화되고, 즉 z가 가능한 한 x에 가깝고, 즉 x가 완벽하게 재구성된다면, 순방향 가중치는 입력의 주요 특징들을 학습하는 성공적인 방법이라고 말할 수 있다. AutoEncoder 언어 모델은 Denoising AutoEncoder (DAE) <cite class="ltx_cite ltx_citemacro_cite">Vincent et al. (<a class="ltx_ref" href="#bib.bib128" title="">2008</a>)</cite>에서 이름을 따서 문맥 단어에 의해 [가면된] 토큰을 예측하는 데 사용됩니다(이러한 [가면된] 단어는 실제로 입력에 추가되는 노이즈, 사고의 전형). DAE는 데이터의 입력 계층에 랜덤 노이즈를 추가하는 것을 포함하는 기술이다. 이것은 계층적 방식으로 딥 네트워크의 가중치들을 사전 트레이닝하기 위해 감독되지 않은 접근법을 사용할 때 보다 강건한 특징들을 학습하는 것을 돕는다.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p2.1">오토인코더 언어 모델의 대부분은 일반성이 높고 감독되지 않으며 데이터 주석이 필요하지 않다. 그들은 자연스럽게 문맥적 의미 정보를 통합할 수 있다. 다만, Pre-Training 단계에서 도입된 독립성 가정은 예측된 [MASK] 간의 상관관계를 고려하지 않은 것을 의미한다. 또한 원본 토큰을 대체하기 위한 입력에서 [마스크]를 특수 마커로 도입하면 [마스크]가 없는 Pre-Training 단계와 Fine-Tuning 단계의 데이터 간에 불일치가 발생한다. 자체 인코딩 언어 모델은 자연 언어 이해(NLU) 작업을 위해 RALM 아키텍처에서 일반적으로 사용된다.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p3.1">AutoEncoder 언어 모델은 NLU(Natural Language Understanding) 작업에서 탁월하기 때문에 많은 RALM 아키텍처 <cite class="ltx_cite ltx_citemacro_cite">Thulke et al. (<a class="ltx_ref" href="#bib.bib124" title="">2021</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a class="ltx_ref" href="#bib.bib19" title="">2024</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Sachan et al. (<a class="ltx_ref" href="#bib.bib106" title="">2021</a>)</cite> 판단과 같은 특정 작업에 적합합니다. 가장 일반적으로 사용되는 모델 중 하나는 BERT와 개선된 버전이다. <cite class="ltx_cite ltx_citemacro_citet">Devlin et al. (<a class="ltx_ref" href="#bib.bib24" title="">2018</a>)</cite>는 닫힌 태스크<cite class="ltx_cite ltx_citemacro_cite">Taylor (<a class="ltx_ref" href="#bib.bib122" title="">1953</a>)</cite>에서 영감을 받은 BERT 모델을 제안했다. RoBERTa<cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib79" title="">2019</a>)</cite>는 버트 모델의 학습 부족을 해결하기 위해 동적 마스킹, NSP 손실이 없는 풀 문장, 큰 미니 배치 및 더 큰 바이트 수준 BPE를 사용하여 학습된다. <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a class="ltx_ref" href="#bib.bib59" title="">2020</a>)</cite>에 따르면 BERT는 글로벌 셀프 어텐션 블록에 크게 의존하여 메모리 풋프린트와 계산 비용이 크게 발생한다. 모든 어텐션 헤드가 전체 입력 시퀀스를 쿼리하지만, 일부는 로컬 종속성을 학습하기만 하면 되어 계산 중복성이 발생한다. 이 문제를 해결하기 위해 그들은 이러한 자체 주의 헤드를 대체하고 로컬 종속성을 직접 모델링하는 새로운 스팬 기반 동적 컨벌루션을 제안했다. 새로운 콘볼루션 헤드는 다른 자기 주의 헤드들과 함께 하이브리드 주의 블록을 형성한다. 또한 <cite class="ltx_cite ltx_citemacro_citet">Sanh et al. (<a class="ltx_ref" href="#bib.bib107" title="">2019</a>)</cite>는 사전 훈련 단계에서 지식증류를 구현함으로써 언어 이해 능력의 97%를 유지하면서 BERT 모델의 크기를 40% 감소시킬 수 있었고 60%의 속도 증가를 달성할 수 있었다.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>AutoRegressive Language Model</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1">AutoRegressive 언어 모델의 주요 목적은 선행 단어들을 기반으로 다음 단어를 예측하는 것이다. 이것은 일반적으로 좌-우 언어 모델링으로 알려져 있으며, 여기서 현재 시간 t에서의 토큰은 제1 t-1 토큰들에 기초하여 예측된다.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p2.1">이 모델은 좌우라는 장점이 있어 대화 생성 및 기계 번역과 같은 생성적 자연어 처리 작업에 유리하다. 자동 회귀 언어 모델은 이 프로세스에 잘 적합하여 RALM 분야의 NLG 작업에 이 모델이 인기 있는 선택이다. 그러나, 문제의 정보는 선행 또는 후속 텍스트로부터만 활용될 수 있고, 둘 다와 조합될 수 없다. OpenAI는 자기회귀 언어 모델과 관련된 연구 분야에 주목할 만한 영향을 미쳤다. 최근 구글은 이 모델에 대한 연구에도 진전을 이루었다.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p3.1">GPT 계열은 AutoRegressive 언어 모델의 가장 일반적인 예 중 하나이다. 그것은 먼저 <cite class="ltx_cite ltx_citemacro_citet">Radford et al. (<a class="ltx_ref" href="#bib.bib94" title="">2018</a>)</cite>에 의해 제안되었는데, 그는 감독되지 않은 사전 훈련과 미세 조정의 기본 아키텍처를 확인했다. <cite class="ltx_cite ltx_citemacro_citet">Radford et al. (<a class="ltx_ref" href="#bib.bib95" title="">2019</a>)</cite> 이후 GPT 기반의 제로샷 학습을 제안하였다. 이후 <cite class="ltx_cite ltx_citemacro_citet">Brown et al. (<a class="ltx_ref" href="#bib.bib13" title="">2020</a>)</cite>는 <cite class="ltx_cite ltx_citemacro_citet">Radford et al. (<a class="ltx_ref" href="#bib.bib95" title="">2019</a>)</cite>와 유사한 접근법을 사용하여 GPT-3를 제안했으며, 이는 크기 조정 및 미세 조정 중단을 포함했다. 그들은 또한 희박한 변압기 <cite class="ltx_cite ltx_citemacro_cite">Child et al. (<a class="ltx_ref" href="#bib.bib20" title="">2019</a>)</cite>와 유사하게 변압기 층에서 교대로 조밀하고 국부적으로 밴딩된 희박한 주의 패턴을 활용했다. 또한 GPT-NEO<cite class="ltx_cite ltx_citemacro_cite">Black et al. (<a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>와 RRLHF(Reinforcement Learning from Human Feedback)를 사용하는 ChatGPT와 같은 여러 관련 연구가 있다. RLHF는 GPT 모델의 정확도를 크게 향상시켰다. ChatGPT는 오픈 소스가 아니지만 많은 연구자들이 RALM에서 생성 작업을 위해 여전히 자신의 API를 사용한다. 최근 GPT-4의 안전성과 정렬을 개선하는 데 전념하는 예측 가능하고 확장 가능한 딥 러닝 스택을 구축하는 데 중점을 두고 GPT-4<cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="#bib.bib1" title="">2023</a>)</cite>에 대한 보고서가 나타났다. 많은 연구자들 <cite class="ltx_cite ltx_citemacro_cite">Asai et al. (<a class="ltx_ref" href="#bib.bib4" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="#bib.bib82" title="">2023a</a>)</cite>는 최근 GPT-4를 사용하여 RALM에 대한 프롬프트를 생성하였다.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p4.1">라마 패밀리는 자동 회귀 언어 모델의 잘 알려진 클래스이다. Llama<cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="#bib.bib125" title="">2023a</a>)</cite>는 공개된 데이터만을 사용하는 언어 모델로 처음 제안되었다. 훈련 안정성을 향상시키기 위해 출력을 정규화하는 대신 각 변압기 하위 계층의 입력을 정규화한다. 이들은 <cite class="ltx_cite ltx_citemacro_citet">Zhang and Sennrich (<a class="ltx_ref" href="#bib.bib150" title="">2019</a>)</cite>가 도입한 RMSNorm 정규화 함수를 사용하고, ReLU 비선형성을 <cite class="ltx_cite ltx_citemacro_citet">Shazeer (<a class="ltx_ref" href="#bib.bib113" title="">2020</a>)</cite>가 도입한 SwiGLU 활성화 함수로 대체하여 성능을 향상시킨다. 또한, 저자들은 절대 위치 임베딩을 네트워크의 각 계층에서 <cite class="ltx_cite ltx_citemacro_citet">Su et al. (<a class="ltx_ref" href="#bib.bib121" title="">2024</a>)</cite>에 의해 도입된 회전 위치 임베딩(RoPE)으로 대체하였다. Llama2<cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="#bib.bib126" title="">2023b</a>)</cite>는 감독 미세 조정, 초기 및 반복 보상 모델링 및 RLHF를 실험에 사용했다. 그들은 또한 여러 번의 회전으로 대화의 흐름을 제어하는 데 도움이 되는 새로운 기술인 고스트 어텐션(Ghost Attention, GAtt)을 발명했습니다. Qwen<cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="#bib.bib7" title="">2023</a>)</cite>는 Llama를 기반으로 다음과 같이 조정하였다. 1. 메모리 비용을 절약하기 위해 입력 임베딩과 출력 투영의 가중치를 묶는 대신 느슨한 임베딩 방법을 선택했다. 2. 역 주파수 행렬의 정확도가 향상되었다. 3. 대부분의 레이어에 대해 <cite class="ltx_cite ltx_citemacro_citet">Chowdhery et al. (<a class="ltx_ref" href="#bib.bib21" title="">2023</a>)</cite>와 같이 바이어스가 제거되었다. 그러나 모델의 외삽 능력을 향상시키기 위해 QKV 층에 바이어스를 추가했다. <cite class="ltx_cite ltx_citemacro_citet">Ba et al. (<a class="ltx_ref" href="#bib.bib6" title="">2016</a>)</cite>에 기술된 전통적인 계층 정규화 기법은 RMSNorm으로 대체되었다. 그들은 SwiGLU를 활성화 함수로 선택했는데, 이는 Swish <cite class="ltx_cite ltx_citemacro_citet">Ramachandran et al. (<a class="ltx_ref" href="#bib.bib99" title="">2017</a>)</cite>와 게이트 선형 단위 <cite class="ltx_cite ltx_citemacro_cite">Dauphin et al. (<a class="ltx_ref" href="#bib.bib23" title="">2017</a>)</cite>의 조합이다. 피드-포워드 네트워크(FFN)의 차원도 감소된다. 또한, 미스트랄 7b <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="#bib.bib56" title="">2023a</a>)</cite>는 GQA(Grouped Query Attention)를 사용하여 추론 속도를 높이고 SWA(Sliding Window Attention)와 결합하여 추론 비용을 줄인 임의의 길이의 시퀀스를 효율적으로 처리한다. 이러한 기술은 Llama2보다 우수한 성능을 보여준다. Llama 모델은 오픈 소스이며 공개적으로 사용 가능한 데이터를 사용하여 연구자 <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a class="ltx_ref" href="#bib.bib135" title="">2024</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Asai et al. (<a class="ltx_ref" href="#bib.bib4" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite>에 확장 기회가 더 많다. 그 결과 많은 연구자들이 RALM 아키텍처에서 라마 패밀리를 언어 모델로 사용하고 있다.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Encoder-Decoder Language Model</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p1.1">Transformer<cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="#bib.bib127" title="">2017</a>)</cite>는 “인코더-디코더” 아키텍처로, 멀티헤드 셀프 어텐션 모듈에 중첩된 인코더와 디코더로 구성된다. 이 중 입력 시퀀스는 소스 시퀀스와 목적지 시퀀스의 두 부분으로 나뉜다. 전자는 인코더에 입력되고 후자는 디코더에 입력되며, 두 시퀀스 모두 표현을 내장하고 위치 정보를 추가할 필요가 있다. 트랜스포머 구조는 병렬 연산과 전체 텍스트 시퀀스의 처리를 동시에 가능하게 하여 모델 학습과 추론 속도가 크게 증가한다.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Raffel et al. (<a class="ltx_ref" href="#bib.bib96" title="">2020</a>)</cite>는 모든 텍스트 기반 언어 문제를 텍스트 간 형식으로 변환하는 통일된 프레임워크를 소개한다. 목적은 자연어 처리를 위한 전이학습 기법의 가능성을 탐색하는 것이다. 원래 변압기와 대조적으로, 활성화가 추가적인 편향 없이 재스케일링되는 계층 정규화의 단순화된 버전이 사용된다. 레이어 정규화를 적용한 후, 잔차 스킵 연결 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="#bib.bib41" title="">2016</a>)</cite>는 각 하위 컴포넌트의 입력을 자신의 출력에 추가한다. <cite class="ltx_cite ltx_citemacro_citet">Srivastava et al. (<a class="ltx_ref" href="#bib.bib120" title="">2014</a>)</cite>는 피드포워드 네트워크, 스킵 연결, 어텐션 가중치, 전체 스택의 입력과 출력에 적용된다. T5 모델은 <cite class="ltx_cite ltx_citemacro_citet">Hofstätter et al. (<a class="ltx_ref" href="#bib.bib45" title="">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Sachan et al. (<a class="ltx_ref" href="#bib.bib106" title="">2021</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Singh et al. (<a class="ltx_ref" href="#bib.bib118" title="">2021</a>)</cite> 등 많은 연구자들에 의해 언어 모델로 널리 사용되어 왔다. 또한 모델 성능을 향상시키기 위한 방법으로 <cite class="ltx_cite ltx_citemacro_citet">Chung et al. (<a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite> 명령어 튜닝을 제안하였다. 본 연구는 스케일링 작업의 수, 스케일링된 모델의 크기, 연쇄적 사고 데이터의 미세 조정이라는 세 가지 측면에 초점을 맞추었다. 그 결과 모델 크기가 클수록, 미세 조정 작업이 많을수록 모델 성능이 크게 향상되는 것으로 나타났다. 또한, CoT(Chain of Thinking)는 추론 수준을 유의하게 향상시키는 것으로 나타났다. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite>는 이 접근법을 사용하여 T5를 튜닝하고 RALM 아키텍처에 적용했다.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p3.1">BART<cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="#bib.bib70" title="">2019</a>)</cite>는 인코더로의 입력이 디코더의 출력과 정렬할 필요가 없기 때문에 임의의 잡음 변환을 허용하는 인코더-디코더 모델이다. 이 경우 텍스트 범위를 마스크 기호로 대체하여 문서가 손상되었습니다. 사전 훈련을 위해 연구자들은 토큰 마스킹, 토큰 삭제, 텍스트 입력, 문장 순열 및 문서 회전의 5가지 모델을 제안했다. 미세-조정을 위해, 인코더 및 디코더는 중단되지 않은 문서를 공급하고, 디코더로부터의 최종 은닉 상태의 표현이 사용된다. 많은 연구자들 <cite class="ltx_cite ltx_citemacro_cite">Thulke et al. (<a class="ltx_ref" href="#bib.bib124" title="">2021</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et al. (<a class="ltx_ref" href="#bib.bib119" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="#bib.bib70" title="">2019</a>)</cite>는 포괄적이고 새로운 사전 훈련 접근법으로 인해 RALM 아키텍처에서 BART를 언어 모델로 채택하여 모델의 견고성을 크게 향상시켰다.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.19543/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4:</span>Classification of RALM enhancement methods.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>RALM Enhancement</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">이 섹션에서는 RALM 아키텍처의 연구자들이 구성 요소를 향상시킴으로써 출력 품질을 어떻게 향상시켰는지 설명한다. 개선 방법을 Retriever Enhancement, LM Enhancement, Overall Enhancement의 세 부분으로 나누었다. 그림 <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.3 Encoder-Decoder Language Model ‣ 4 Language Models ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">4</span></a>는 향상 방법의 범주화를 보여준다.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Retriever Enhancement</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p1.1">이 절에서는 검색 품질 관리 및 검색 시간 최적화를 포함하는 검색자 측면에서 연구자들의 노력을 제시한다.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Retrieval Quality Control</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Shi et al. (<a class="ltx_ref" href="#bib.bib116" title="">2023a</a>)</cite>는 검색이 유용한 정보를 제공하지 못할 뿐만 아니라 언어 모델 출력의 품질을 손상시킬 수 있는 문서를 생성할 수 있다고 주장한다. 결과적으로 RALM 분야의 많은 학자들은 최종 산출물의 품질을 향상시키기 위해 검색된 콘텐츠와 사용자의 입력 사이의 관련성을 개선하는 데 중점을 둔다.</p>
</div>
<div id="S5.SS1.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="#bib.bib75" title="">2023b</a>)</cite>는 명령어 튜닝을 위한 접근 방식을 제안한다. 그들은 지도 작업과 지도되지 않은 텍스트의 조합을 통해 계산을 완료하는 일반화된 LM 지도 검색(LSR) <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="#bib.bib117" title="">2023b</a>)</cite> 훈련 대상을 사용하여 쿼리 인코더를 업데이트한다. 이를 통해 리트리버는 LLM 선호도와 일치하는 보다 맥락적으로 관련된 결과를 생성할 수 있다. 이 명령어 조정 접근법에서 영감을 받은 <cite class="ltx_cite ltx_citemacro_citet">Asai et al. (<a class="ltx_ref" href="#bib.bib4" title="">2023</a>)</cite>는 명령어 추적 데이터 세트인 SELF-RAG에 대해 훈련된 보다 정교한 모델을 제안했다. SELF-RAG는 세분화된 자기 반성을 통해 요구 시 가능한 최상의 모델 출력을 검색하고 선택할 수 있으므로 광범위하게 적용 가능하고 더 강력하며 제어할 수 있다. 자연어 추론<cite class="ltx_cite ltx_citemacro_cite">Yoran et al. (<a class="ltx_ref" href="#bib.bib144" title="">2023</a>)</cite> 및 요약<cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="#bib.bib134" title="">2023</a>)</cite> 모델과 같은 외부 모델을 사용하여 검색된 문서의 품질을 향상시키는 접근 방식과 달리 SELF-RAG는 완전히 새로운 아이디어를 제안한다. 모델은 검색된 문서를 병렬 세그먼트로 분할하고 관련성을 비교한다. 그런 다음 문서의 가장 유사한 부분을 결합합니다. <cite class="ltx_cite ltx_citemacro_citet">Yan et al. (<a class="ltx_ref" href="#bib.bib135" title="">2024</a>)</cite>는 부정확한 리트리버 결과를 해결하기 위한 수정 전략을 설계함으로써 SELF-RAG에서 개선된다. 그들은 정보를 CORRECT, INCORRECT 및 AMBUOUS의 세 가지 범주로 분류한다. 정보가 CORRECT인 경우, 문서는 정제되고 필터링된다. INCORRECT인 경우, 문서는 폐기되고 웹은 검색을 위해 검색된다. MBIGUOUS라는 용어는 판단의 정확성에 대한 자신감 부족을 나타낸다. 이 경우, 위에서 언급한 두 가지 방법의 조합이 사용될 것이다. 또한 <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite>는 STRINC, 어휘 중복, 조건부 상호 정보(CXMI)의 세 가지 필터를 통해 문장 정밀도로 문서 내용을 검색하는 방법인 FILCO를 제안하였다.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Retrieval Timing Optimization</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">연구자들은 일반적으로 긴 대화 생성 및 다중 홉 문제와 같이 다중 검색을 필요로 하는 작업을 수행할 때 또는 적합하고 관련 문서를 찾을 수 없는 두 가지 상황에서 검색 시기를 고려한다. 관련 없는 문서를 사용하면 출력의 정확도가 저하될 수 있습니다.</p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">검색 시기를 결정하는 간단한 방법은 검색 단계를 조정하는 것이다. <cite class="ltx_cite ltx_citemacro_citet">Ram et al. (<a class="ltx_ref" href="#bib.bib97" title="">2023</a>)</cite>는 런타임 비용을 조정하기 위해 프리픽스 인코딩을 포함하는 접근법을 활용했다. 생성된 콘텐츠의 프리픽스 인코딩은 지속적으로 재계산되었다. 검색 보폭의 선택은 런타임과 성능 사이의 절충이다. 툴포머 <cite class="ltx_cite ltx_citemacro_cite">Schick et al. (<a class="ltx_ref" href="#bib.bib110" title="">2024</a>)</cite>에 따르면, 검색 명령은 모델이 콘텐츠를 생성하는 과정에서 문서 도움말을 검색해야 할 때 유용한 정보를 검색하기 위해 직접 사용될 수 있다. 이 아이디어에 영감을 받아 <cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a class="ltx_ref" href="#bib.bib58" title="">2023c</a>)</cite>는 검색 시기를 결정하는 두 가지 방법을 제안한다. 첫 번째 방법은 검색이 수행될 필요가 있는 장소와 마주쳤을 때 LM 생성을 중단한 다음 검색 작업을 수행하는 것을 포함한다. 두 번째 방법은 그 전체가 임시 문장을 생성하는 것을 포함한다. 문장에 낮은 신뢰 마커가 있는 경우, 마커는 마스킹되고 문장의 나머지는 검색에 사용된다. <cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a class="ltx_ref" href="#bib.bib146" title="">2023</a>)</cite>도 검색 시기를 결정하기 위해 LM을 사용했다. 그러나 LM을 사용하여 저신뢰 마커를 생성하는 대신 검색 전후에 LM 점수를 부여했다. <cite class="ltx_cite ltx_citemacro_citet">Mallen et al. (<a class="ltx_ref" href="#bib.bib86" title="">2022</a>)</cite>의 접근법은 LMs가 낮은 신뢰 마커를 생성하도록 하는 전통적인 방법과 다르다. 대신, 그들은 유병률의 척도로 위키피디아 페이지 뷰를 사용하고 다양한 유병률을 가진 위키 데이터의 지식 3배를 원래 개체 및 관계 유형에 고정된 자연어 질문으로 변환했다. 이러한 접근은 보다 객관적이고 주관적인 평가를 회피한다. 추론이 필요한 태스크의 경우, <cite class="ltx_cite ltx_citemacro_citet">He et al. (<a class="ltx_ref" href="#bib.bib39" title="">2022</a>)</cite>와 <cite class="ltx_cite ltx_citemacro_citet">Khattab et al. (<a class="ltx_ref" href="#bib.bib64" title="">2022</a>)</cite> 모두 검색 수행 시점을 결정하기 위해 CoT(chain of thought)를 사용하였다.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>LM Enhancement</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p1.1">이 절에서는 사전 생성 검색 처리, 구조 모델 최적화 및 사후 생성 출력 향상을 포함한 언어 모델링에 대한 연구자들의 노력을 제시한다.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Pre-Generation Retrieval Processing</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">RALM 아키텍처는 처음에 검색 증강을 위해 단일 문서를 사용했다. 그러나 검색된 단락의 수가 증가하면 RALM의 성능이 크게 향상되는 것으로 나타났다. <cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a class="ltx_ref" href="#bib.bib53" title="">2020b</a>)</cite> 따라서, 그들은 리트리버를 변경시키지 않고, LM 내의 인코더를 사용하여 관련 문서들을 하나씩 인코딩한 다음, 관련 문서들을 연결하여 디코더에 제공하여 출력하는 Fusion-in-Decoder(FiD)라는 새로운 방법을 제안했다. 그런 다음 FiD에서 <cite class="ltx_cite ltx_citemacro_citet">Hofstätter et al. (<a class="ltx_ref" href="#bib.bib45" title="">2023</a>)</cite>가 개선되었습니다. 그들은 인코더에서 디코더로의 정보 흐름을 제한했다. 재순위가 있는FiD-라이트도 텍스트 소스 포인터를 통해 튜닝하여 최상위 소스 정확도를 향상시켰다. <cite class="ltx_cite ltx_citemacro_citet">Izacard and Grave (<a class="ltx_ref" href="#bib.bib52" title="">2020a</a>)</cite>는 리트리버의 합성 표적을 얻기 위해 시퀀스 간 판독기에서 교차 주의 점수를 사용하여 FiD-KD라고도 하는 FiD 모델에 지식 증류를 적용했다. <cite class="ltx_cite ltx_citemacro_citet">Singh et al. (<a class="ltx_ref" href="#bib.bib118" title="">2021</a>)</cite>는 FiD-KD에 비해 적은 문서, 훈련 주기 및 감독된 초기화가 필요하지 않은 종단 간 훈련 접근법을 사용한다는 점에서 지식 증류와 다른 향상 접근법을 제안했다.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Structural Model Optimization</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">언어 모델들이 가속화된 속도로 계속 발전함에 따라, 높은 파라미터 카운트 및 탁월한 성능을 갖는 대형 모델들이 증가하고 있다. 이러한 모델의 매개변수 및 내부 구조를 조정하는 것은 점점 더 어렵고 비효율적이 되어 명령어 튜닝을 어느 때보다 중요하게 만든다.</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">FLAN <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>는 명령어 튜닝에 관한 많은 연구 중 가장 체계적이고 포괄적인 접근법 중 하나이다. 이 접근법은 명령어 최적화 데이터 세트에서 언어 모델을 미세 조정하고, 태스크 수와 모델 크기를 확장하며, 미세 조정에 사상 연쇄 데이터를 통합한다. 저자들은 RALM 아키텍처의 조정 지침에 대한 특정 접근법을 고려하지 않았지만, 그들의 작업은 향후 연구에 귀중한 참조를 제공한다. RALM의 명령어 미세 조정에서는 <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="#bib.bib75" title="">2023b</a>)</cite> 통합 문맥 내 검색 증강을 수행한다. 이는 언어 모델이 관련 없는 검색 콘텐츠에 의해 오도될 가능성을 크게 감소시킨다. SAIL<cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="#bib.bib82" title="">2023a</a>)</cite>는 내부 및 외부 검색 엔진에 의해 생성된 복잡한 검색 결과에 언어 생성 및 명령어 추적 기능을 구축한다. 명령어 튜닝의 코퍼스를 사용하여 서로 다른 검색 API와 도메인에서 각 훈련 사례에 대한 검색 결과를 수집하고, (명령어, 접지 정보, 응답)의 트리플렛을 포함하는 검색 기반 훈련 세트를 구성한다. 명령어 조정 데이터 세트에 대한 훈련과 달리 <cite class="ltx_cite ltx_citemacro_citet">Madaan et al. (<a class="ltx_ref" href="#bib.bib85" title="">2022</a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_citet">Lazaridou et al. (<a class="ltx_ref" href="#bib.bib67" title="">2022</a>)</cite>는 검색된 지식에서 직접 대규모 모델을 프롬프트할 것을 제안한다. <cite class="ltx_cite ltx_citemacro_citet">Madaan et al. (<a class="ltx_ref" href="#bib.bib85" title="">2022</a>)</cite>는 GPT-3를 사용하여 모델이 사용자의 의도뿐만 아니라 사용자 피드백을 잘못 해석한 기록된 사례의 메모리 페어링을 명확히 했다. 이것은 그들의 시스템이 사용자 피드백에 기초하여 각각의 새로운 질의에 대해 향상된 프롬프트들을 생성할 수 있게 한다. 대조적으로, <cite class="ltx_cite ltx_citemacro_citet">Lazaridou et al. (<a class="ltx_ref" href="#bib.bib67" title="">2022</a>)</cite>는 추론 계산을 개선하기 위해 적은 샷 프롬프트와 답변 재정렬을 사용한다.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Post-Generation Output Enhancement</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS3.p1.1">병렬 상호작용에 관한 섹션 2에서 정의된 바와 같이, 이 상호작용은 KNN(K-Nearest Neighbor) LM <cite class="ltx_cite ltx_citemacro_cite">Khandelwal et al. (<a class="ltx_ref" href="#bib.bib63" title="">2019</a>)</cite>에서 영감을 받는다. LM은 결과를 향상시키기 위해만 사용되는 RALM의 패러다임 사례이다. KNN-LM이 제안된 이후 많은 연구자들이 모델을 최적화하기 위해 노력했다. 이 절에서는 랜드마크 작업에 대해 자세히 설명한다.</p>
</div>
<div id="S5.SS2.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS3.p2.1">KNN-LM 접근법은 미리 훈련된 LM 임베딩 공간에서 확장된 신경망 언어 모델을 K-Nearest Neighbours로 선형 보간하는 것을 포함한다. <cite class="ltx_cite ltx_citemacro_citet">Zhong et al. (<a class="ltx_ref" href="#bib.bib156" title="">2022</a>)</cite>는 세 가지 유형의 메모리(로컬, 장기 및 외부)에 대해 서로 다른 처리를 제안했으며 KNN-LM에 배치 내 토큰에 대한 훈련을 추가했다. 제안된 변화들은 모델의 성능을 향상시키는 것을 목표로 한다. 훈련 시 메모리 단위만 사용하는 KNN-LM과 달리 TRIME <cite class="ltx_cite ltx_citemacro_cite">Zhong et al. (<a class="ltx_ref" href="#bib.bib156" title="">2022</a>)</cite>는 테스트와 훈련 시 모두 메모리 단위를 사용한다. <cite class="ltx_cite ltx_citemacro_citet">He et al. (<a class="ltx_ref" href="#bib.bib40" title="">2021</a>)</cite>는 생성된 토큰이 모두 검색될 필요는 없다고 제안했다. 대신에, 경량 신경망은 적응 검색에서 KNN-LM을 돕기 위해 훈련될 수 있다. 또한, 데이터베이스 간소화와 차원 축소를 통해 효율성을 향상시킬 수 있다. <cite class="ltx_cite ltx_citemacro_citet">Alon et al. (<a class="ltx_ref" href="#bib.bib3" title="">2022</a>)</cite> 제안된 RETOMATON은 데이터 저장소 위에 구축된 감독되지 않은 가중 유한 자동화이다. RETOMATON은 연속적인 데이터 저장소 엔트리와 클러스터링 기법 사이의 포인터를 저장하는 것을 기반으로 한다. RETOMATON이 ADAPTRET<cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="#bib.bib40" title="">2021</a>)</cite>보다 KNN 검색 시 남은 포인터를 활용하여 정확도 향상에 더 효과적이다. KNN 검색이 없더라도 언어 모델에만 의존하는 ADAPTRET과 달리 포인터에 저장된 이전 정보를 사용하여 보간 연산을 수행할 수 있다. 또한, RETOMATON은 감독되지 않아 훈련에 추가 데이터가 필요하지 않아 데이터 효율성이 높다. KNN-LM 성능을 향상시키기 위해 연속 캐시를 사용하여 제안된 <cite class="ltx_cite ltx_citemacro_citet">Grave et al. (<a class="ltx_ref" href="#bib.bib35" title="">2016</a>)</cite>이다. 이것은 과거의 숨겨진 활성화를 저장하고 현재 숨겨진 활성화와 함께 내적에 의해 적절한 시간에 액세스하는 것을 포함한다. <cite class="ltx_cite ltx_citemacro_citet">Yogatama et al. (<a class="ltx_ref" href="#bib.bib143" title="">2021</a>)</cite>는 각 시간 단계에서 가장 가까운 이웃 토큰 집합을 검색하여 로컬 히든 상태 및 글로벌 장기 메모리를 캐싱하여 확장된 단기 컨텍스트를 활용한다. 또한 예측을 위해 여러 정보 소스를 적응적으로 결합하는 게이팅 기능을 설계한다. KNN-LM과 비교하여, 이 방법은 동적 가중치들을 사용하며, 메모리 출력이 이미지, 비디오, 또는 사운드인 경우와 같이 보간이 실현 가능하지 않은 경우들을 처리할 수 있다. <cite class="ltx_cite ltx_citemacro_citet">Drozdov et al. (<a class="ltx_ref" href="#bib.bib25" title="">2022</a>)</cite>는 보간 가중치를 조정하는 방법을 제안하였다. 가중치는 검색된 저장 데이터와 평가 세트 사이의 중첩 영역의 크기에 기초하여 동적으로 조정되며, 이는 검색의 품질을 반영한다.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Overall Enhancement</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p1.1">이 절에서는 종단 간 훈련 및 중간 모듈 빌드 등 RALM 아키텍처 전반에 대한 연구자들의 노력을 소개한다.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>End-to-End Training</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">연구자들은 수동 개입을 최소화하고 데이터에만 집중하는 것을 목표로 하는 종단 간 훈련이라는 방법에 대해 연구하기 시작했다. 이 방법은 딥 러닝을 활용하며 사용 가능한 데이터의 양이 증가함에 따라 점점 더 인기를 얻고 있다. RALM 아키텍처에 대한 연구 동안 많은 연구자들은 더 나은 결과를 얻기 위해 종단 간 훈련 방법을 사용하는 경향이 있다.</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS3.SSS1.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Lewis et al. (<a class="ltx_ref" href="#bib.bib71" title="">2020</a>)</cite>와 <cite class="ltx_cite ltx_citemacro_citet">Guu et al. (<a class="ltx_ref" href="#bib.bib36" title="">2020</a>)</cite>는 RALM 분야에 엔드 투 엔드 트레이닝을 적용한 최초의 연구자 중 하나였다. 그러나, 그들은 그들의 접근 방식이 달랐다. REALM <cite class="ltx_cite ltx_citemacro_cite">Guu et al. (<a class="ltx_ref" href="#bib.bib36" title="">2020</a>)</cite>는 사전 훈련 단계에서 마스킹된 언어 훈련을 사용했으며 종단 간 훈련할 수 있는 리트리버를 포함했다. 미세 조정 단계에서는 리트리버를 냉동 상태로 유지하면서 QA 작업만 대상으로 삼았다. 반면에 RAG <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="#bib.bib71" title="">2020</a>)</cite>는 이미 훈련된 리트리버인 DPR을 사용했으며 부분적인 종단 간 훈련에는 BART만 사용했다. REALM과 유사하게 <cite class="ltx_cite ltx_citemacro_citet">Sachan et al. (<a class="ltx_ref" href="#bib.bib106" title="">2021</a>)</cite>는 역 클로즈 작업과 마스킹된 두드러진 스팬을 포함하는 감독되지 않은 사전 훈련 방법을 제시한다. 다음은 질문-컨텍스트 쌍을 사용하여 감독된 미세 조정입니다. 또한, 그들은 엔드 투 엔드 트레이닝된 리트리버들의 사용이 태스크들 전반에 걸쳐 성능에서 상당한 개선을 가져왔다는 것을 발견한다. <cite class="ltx_cite ltx_citemacro_citet">Singh et al. (<a class="ltx_ref" href="#bib.bib118" title="">2021</a>)</cite> end-to-end training to multi-document processing, 이들의 제안 방법에서는 주어진 질문에 대한 관련 문서 집합을 나타내는 잠재 변수의 값을 반복적으로 추정한다. 그런 다음 이 추정치를 사용하여 검색기 및 판독기의 매개변수를 업데이트합니다. <cite class="ltx_cite ltx_citemacro_citet">Siriwardhana et al. (<a class="ltx_ref" href="#bib.bib119" title="">2023</a>)</cite>는 이전 연구의 RAG의 종단 간 최적화를 설명하고 더 많은 도메인 특정 지식을 통합하기 위해 보조 훈련 신호를 도입한다. 이 신호는 RAG-end2end가 외부 지식 베이스에서 관련 정보에 액세스함으로써 주어진 문장을 재구성하도록 강제한다. 이 접근법은 도메인 적응성을 크게 향상시켰다.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.19543/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 5:</span>RALM 데이터 소스의 분류.</figcaption>
</figure>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>Intermediate Modules</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">최근 일부 연구자들은 공간 또는 블랙박스 LLM 제약으로 인해 리트리버와 언어 모델의 활동을 조정하기 위해 중간 모듈을 구성했지만 개선되지 않았다.</p>
</div>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS3.SSS2.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Cheng et al. (<a class="ltx_ref" href="#bib.bib19" title="">2024</a>)</cite>는 낮은 말뭉치 품질 문제를 해결하기 위해 고안된 모델인 Selfmem을 제시한다. 셀프멤은 검색 강화 생성기를 사용하여 무한한 메모리 풀을 만든 다음 메모리 선택기에 의해 후속 세대에 대한 출력을 선택하는 데 사용됩니다. 이 접근 방식을 사용하면 모델이 자체 출력을 사용하여 생성을 향상시킬 수 있습니다. <cite class="ltx_cite ltx_citemacro_citet">Peng et al. (<a class="ltx_ref" href="#bib.bib93" title="">2023</a>)</cite>는 5인칭으로 기술된 마르코프 결정 프로세스(MDP)로서 인간 시스템 대화를 공식화하는 AI 에이전트를 제안한다. 5인조는 무한히 큰 대화 상태 세트, 역사적 행동의 집합, 상태 전이의 확률, 획득된 외부 보상 및 가변 파라미터를 포함한다.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Data Sources</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">이 절에서는 RALM에서 일반적으로 사용되는 데이터 소스 중 일부를 소개하고 정형 데이터와 비정형 데이터로 분류한다. 그림 <a class="ltx_ref" href="#S5.F5" title="Figure 5 ‣ 5.3.1 End-to-End Training ‣ 5.3 Overall Enhancement ‣ 5 RALM Enhancement ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">5</span></a>는 데이터 소스의 범주화를 보여준다.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Structured Data</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS1.p1.1">구조화된 데이터는 표, 지식 그래프 등 다양한 구조를 포함한다. 이러한 유형의 데이터의 이점은 일반적으로 표 형식으로 각 필드가 정확하게 정의된 명확한 구조이다. 숫자, 날짜, 텍스트 및 기타 데이터 유형을 저장하는 데 적합합니다. 구조화된 데이터는 SQL과 같은 구조화된 쿼리 언어를 사용하여 쉽게 쿼리, 분석 및 처리될 수 있다.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS1.p2.1">NQ(Natural Questions) <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et al. (<a class="ltx_ref" href="#bib.bib66" title="">2019</a>)</cite>는 NLU 필드에서 매우 잘 알려진 데이터세트이다. 주어진 텍스트는 구조화된 질문과 그에 대응하는 위키피디아 페이지를 기술한다. 페이지는 긴 답변, 전형적으로 단락, 및 하나 이상의 엔티티들로 구성된 짧은 답변으로 주석이 달린다. 길거나 짧은 답변이 없으면 비어 있는 것으로 표시됩니다. 구글 검색엔진의 신뢰성과 방대한 양의 데이터로 인해 많은 학자들이 이 데이터셋을 사용하여 RALM을 학습시켰는데, 그 예로 <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Izacard et al. (<a class="ltx_ref" href="#bib.bib54" title="">2022</a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_citet">Ram et al. (<a class="ltx_ref" href="#bib.bib97" title="">2023</a>)</cite> 등이 있다. HotpotQA(HQA) <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="#bib.bib137" title="">2018</a>)</cite>는 멀티홉 질문에 대한 정보를 저장하고 추론에 필요한 문장 수준의 지원 사실을 제공한다. 구조는 답변을 지원하는 문단, 질문, 답변, 문장 번호를 포함한다. 이 데이터 세트는 다중 홉 질문 응답을 위한 RALM을 훈련하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Khattab et al. (<a class="ltx_ref" href="#bib.bib64" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Feng et al. (<a class="ltx_ref" href="#bib.bib29" title="">2024</a>)</cite>와 같은 많은 연구자들에 의해 사용되어 왔다. 구조화 데이터의 또 다른 중요한 형태는 지식 그래프입니다. 주로 (개체, 관계, 속성)의 트리플로 구성된 데이터 구조입니다. 가장 자주 사용되는 데이터 세트에는 위키다타5M, 위키KG90Mv2, OpendialKG 및 KOMODIS가 포함된다. 이 모든 모델 <cite class="ltx_cite ltx_citemacro_cite">Kang et al. (<a class="ltx_ref" href="#bib.bib60" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Yu and Yang (<a class="ltx_ref" href="#bib.bib145" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="#bib.bib42" title="">2024</a>)</cite>는 데이터 소스로서 지식 그래프에 의존한다.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Unstructured Data</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS2.p1.1">이와 달리 비정형 데이터는 명확하게 정의된 데이터 구조를 갖지 못하고 텍스트, 이미지, 오디오 등 다양한 형태로 존재한다. 크고 다양한 특성으로 인해 전통적인 표 형식으로 저장 및 관리하기가 어렵다. 가치 있는 정보를 담고 있지만, 이를 파싱하고 이해하기 위해서는 자연어 처리, 이미지 인식 및 기타 기술이 필요하다.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS2.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Khattab et al. (<a class="ltx_ref" href="#bib.bib64" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib129" title="">2023a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a class="ltx_ref" href="#bib.bib136" title="">2023</a>)</cite> 등 여러 RALM 연구자들은 이 데이터셋을 데이터 소스로 사용해 왔다. FEVER <cite class="ltx_cite ltx_citemacro_cite">Thorne et al. (<a class="ltx_ref" href="#bib.bib123" title="">2018</a>)</cite> 데이터셋은 사실 추출 및 검증에 주로 사용된다. <cite class="ltx_cite ltx_citemacro_citet">Lewis et al. (<a class="ltx_ref" href="#bib.bib71" title="">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Izacard et al. (<a class="ltx_ref" href="#bib.bib54" title="">2022</a>)</cite>를 포함한 여러 RALM 연구자들은 이 데이터 세트의 사실 텍스트를 데이터 원본으로 사용했다. 비구조화된 텍스트 외에도, 이미지, 비디오 및 오디오와 같은 본질적으로 덜 구조화된 데이터의 상당한 양이 또한 존재한다. MNIST, CIFAR-10, 파스칼 VOC 및 COCO를 포함하여 연구에 사용할 수 있는 몇 가지 일반적인 이미지 데이터 세트가 있다. RALM 분야의 많은 연구들 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="#bib.bib17" title="">2022b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="#bib.bib48" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Yasunaga et al. (<a class="ltx_ref" href="#bib.bib139" title="">2022</a>)</cite>는 이러한 데이터셋을 활용하였다. 음성 연구에 사용되는 일반적인 오디오 데이터 세트로는 LJ Speech, JSUT, RUSLAN 등이 있다. 이 분야의 많은 연구들 <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a class="ltx_ref" href="#bib.bib147" title="">2024</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="#bib.bib50" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Ghosh et al. (<a class="ltx_ref" href="#bib.bib32" title="">2024</a>)</cite> 역시 오디오 데이터를 주소스로 하고 있다. 연구에 사용되는 일반적인 비디오 데이터 세트에는 HMDB, UCF101 및 ASLAN이 포함된다. RALM 분야의 많은 연구들 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="#bib.bib16" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="#bib.bib43" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Yin et al. (<a class="ltx_ref" href="#bib.bib141" title="">2019</a>)</cite>는 오디오 데이터를 정보의 원천으로 활용하고 있다.</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.19543/assets/x6.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="498" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6:</span>Classification of RALM applications.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Applications</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p" id="S7.p1.1">이 섹션에서는 RALM 아키텍처가 주로 중점을 두는 다운스트림 작업에 대한 요약을 제공합니다. 관련 적용 방향은 모델 생성 또는 이해의 요구 사항에 따라 분류된다. NLG에 대한 RALM은 과제의 성취가 주로 생성 능력에 달려 있음을 나타낸다. 반대로 NLU에 대한 RALM은 과제의 성취가 주로 이해 능력에 달려 있음을 나타낸다. 마지막으로, NLU와 NLG 모두에 대한 RALM은 태스크가 일반적으로 이해 능력에 주로 의존하는 것과 생성 능력에 주로 의존하는 두 가지 방식으로 처리된다는 것을 나타낸다. 그림 <a class="ltx_ref" href="#S6.F6" title="Figure 6 ‣ 6.2 Unstructured Data ‣ 6 Data Sources ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">6</span></a>는 애플리케이션의 범주화를 보여준다.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>RALM on NLG Tasks</h3>

<section id="S7.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1 </span>Machine Translation</h4>

<div id="S7.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS1.SSS1.p1.1">자동 번역이라고도 불리는 기계 번역은 컴퓨터를 이용하여 하나의 자연어(소스 언어)를 다른 자연어(타겟 언어)로 변환하는 과정이다. 인공 지능의 궁극적인 목표 중 하나인 계산 언어학의 한 분야이며, 중요한 과학적 연구 가치를 가지고 있다. 기계 번역 시스템은 규칙 기반과 말뭉치 기반 두 가지로 나눌 수 있다. 전자는 사전과 규칙 베이스로 구성되며, 이는 집합적으로 지식 소스를 구성한다. 대조적으로, 후자는 분할되고 라벨링된 코퍼스로 구성되며 사전이나 규칙이 필요하지 않다. 대신 통계법을 기반으로 하며 대부분의 RALM은 규칙을 기반으로 이 작업을 수행한다.</p>
</div>
<div id="S7.SS1.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS1.SSS1.p2.1">셀프멤 <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a class="ltx_ref" href="#bib.bib19" title="">2024</a>)</cite> 시스템은 기계 번역 작업을 위해 두 가지 별개의 언어 모델을 사용한다. 첫 번째는 훈련 가능한 미니 모델이며, 각각 관절 및 이분 접근법을 사용하여 훈련되었다. 두 번째는 LLM을 자극한 몇 번의 샷이다. 궁극적으로, 셀프멤은 네 가지 번역 방향 모두에 걸쳐 그리고 두 훈련 아키텍처 모두에 걸쳐 성능에서 주목할 만한 향상을 보여주었다. 이 결과는 향상된 메모리 기능이 종종 우수한 생성 결과를 초래한다는 것을 암시한다. 최상의 결과를 얻기 위해 TRIME <cite class="ltx_cite ltx_citemacro_cite">Zhong et al. (<a class="ltx_ref" href="#bib.bib156" title="">2022</a>)</cite>는 IWSLT’14 De-En 기준선을 사용하였다. 과제가 문장 수준임을 감안할 때 연구원들은 반복 토큰이 거의 없기 때문에 로컬 메모리와 장기 메모리를 사용하지 않았다. 대신 외부 메모리만 사용하여 성능에서 KNN-MT <cite class="ltx_cite ltx_citemacro_cite">Khandelwal et al. (<a class="ltx_ref" href="#bib.bib62" title="">2020</a>)</cite>를 이길 수 있었다.</p>
</div>
</section>
<section id="S7.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2 </span>Math Teaching</h4>

<div id="S7.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS1.SSS2.p1.1">RALM 아키텍처가 계속 발전함에 따라, 점점 더 많은 잠재적인 애플리케이션 방향들이 식별되고 있다. 레본 <cite class="ltx_cite ltx_citemacro_cite">Levonian et al. (<a class="ltx_ref" href="#bib.bib69" title="">2023</a>)</cite>는 RALM에서 영감을 받아 이 아키텍처를 수학 교수 학습 영역에 적용했다. LLM에 저장된 지식이 학교에서 가르치는 것과 일치하지 않을 수 있다는 사실을 해결하기 위해, 그들은 검색 향상 생성 시스템을 사용하여 수학 학생 질의에 대한 응답을 생성하기 위해 세 가지 프롬프트된 수업 조건 중 하나를 사용했다. 설문 응답자들은 선호도에 따라 응답 순위를 매기고 검색 코퍼스로 기초 수학 교과서를 평가하였다.</p>
</div>
</section>
<section id="S7.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3 </span>Dialog Generation</h4>

<div id="S7.SS1.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS1.SSS3.p1.1">대화 생성, 특히 긴 대화는 어려운 과제이다. 이는 언어 모델이 자연 언어 처리 능력을 보유하도록 보장할 필요뿐만 아니라, 모델이 대화의 요구 사항을 만족시키기 위해 컨텍스트를 이용할 수 있다는 필요성 때문이다.</p>
</div>
<div id="S7.SS1.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS1.SSS3.p2.1">FILCO <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite>는 “Wikipedia의 마법사” (WoW)로 지칭되는 KILT 벤치마크의 Wikipedia 데이터세트를 사용하여 후속 대화를 생성한다. 이 프로세스는 출력을 위키피디아 기사에 기초하는 것을 포함하며, 입력은 여러 라운드의 대화의 히스토리를 포함한다. RA-DIT <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="#bib.bib75" title="">2023b</a>)</cite>는 또한 미세 조정 단계에서 WoW 데이터 세트를 사용한다. 커맨드 튜닝 동작 결과, 제로샷 조건에서 대화 생성을 위한 파라미터가 동일한 Llama <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="#bib.bib125" title="">2023a</a>)</cite>와 Llama-REPLUG <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="#bib.bib117" title="">2023b</a>)</cite>보다 모델이 우수한 성능을 보였다. 셀프멤 <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a class="ltx_ref" href="#bib.bib19" title="">2024</a>)</cite>를 검색 증강 생성기에 통합하면 뛰어난 유연성으로 인해 대화 생성이 현저하게 향상된다. 이것은 다양하고 정보가 풍부한 대화의 원하는 특성에 대한 메모리의 직접 최적화에 의해 달성된다. 대조적으로, SURGE <cite class="ltx_cite ltx_citemacro_cite">Kang et al. (<a class="ltx_ref" href="#bib.bib60" title="">2023</a>)</cite>는 대화 생성 작업을 위한 데이터 소스로 Knowledge Graph를 사용하며, 각 대화 라운드는 대규모 KG로부터의 사실을 포함한다. 다른 관련 작업 <cite class="ltx_cite ltx_citemacro_cite">Rony et al. (<a class="ltx_ref" href="#bib.bib104" title="">2022</a>)</cite>와 달리 문맥적으로 관련된 하위 그래프만 검색하므로 관련 없는 데이터 검색에서 발생할 수 있는 계산 오버헤드와 오판의 소지가 있는 모델을 피할 수 있다.</p>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>RALM on NLU Tasks</h3>

<section id="S7.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1 </span>Slot Filling</h4>

<div id="S7.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS2.SSS1.p1.1">슬롯 채우기(Slot filling)는 사용자가 제공한 텍스트 또는 음성으로부터 특정 정보를 인식하고 추출하기 위한 목적으로 자연어 처리에 채용되는 기술이다. 슬롯 채움에서, 시스템은 미리 슬롯들의 세트를 정의하며, 각각의 슬롯은 특정 정보 요건을 나타낸다. 이러한 요구 사항은 날짜, 시간, 위치 등을 포함할 수 있지만 이에 제한되지 않는다. 텍스트 또는 스피치의 형태의 사용자 입력을 수신하면, 시스템은 콘텐츠의 분석을 수행하여, 미리 정의된 슬롯들 또는 분류 라벨들 <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a class="ltx_ref" href="#bib.bib81" title="">2023b</a>, <a class="ltx_ref" href="#bib.bib80" title="">a</a>)</cite>와 일치하는 정보를 식별하려고 시도한다. 그런 다음 이 정보는 후속 처리 및 응답을 위해 해당 슬롯에 채워진다.</p>
</div>
<div id="S7.SS2.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS2.SSS1.p2.1">KGI <cite class="ltx_cite ltx_citemacro_cite">Glass et al. (<a class="ltx_ref" href="#bib.bib33" title="">2021</a>)</cite>는 dense 인덱싱에서 하드 네거티브의 활용을 통해 dense 채널 검색을 향상시키며, 검색 향상 생성을 위한 강건한 훈련 과정을 구현한다. 검색 향상은 슬롯 채우기 작업의 효율성을 높이기 위해 사용되어 AI에 의한 고품질 지식 그래프 생성을 촉진한다. 그 결과 TREx 및 zsRE 데이터 세트에서 우수한 성능을 달성했으며 TACRED 데이터 세트에서 현저한 견고성을 보였다.</p>
</div>
</section>
<section id="S7.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2 </span>Image Generation</h4>

<div id="S7.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS2.SSS2.p1.1">텍스트-이미지 생성 과정은 텍스트 데이터의 전형적인 포맷과 달리 높은 수준의 자연어 이해를 입증하고 이미지를 통해 이러한 이해를 전달하기 위한 모델이 요구되는 어려운 과정이다.</p>
</div>
<div id="S7.SS2.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS2.SSS2.p2.1">선구적인 연구에서 <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="#bib.bib72" title="">2022a</a>)</cite>는 텍스트 대 이미지 생성의 품질을 향상시키기 위해 검색 기술의 사용을 제안했다. 그들은 CUB 및 COCO 데이터 세트에 대해 생성된 이미지의 품질과 양을 주류 모델과 비교 분석했다. 그들의 연구 결과는 모든 모델이 동시대보다 우수하다는 것을 보여주었다. 이에 반해 RE-IMAGEN <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="#bib.bib18" title="">2022c</a>)</cite>는 검색을 통해 흔하지 않은 객체의 이미지를 생성하는 모델을 보조하는 데 중점을 두었다. 이 접근법은 궁극적으로 COCO 및 위키이미지 데이터 세트에서 예외적으로 높은 FID 점수를 달성했다. 여러 범주에서 공통 및 희귀 객체의 범위를 포함하는 저자 스스로 제안한 EntityDrawBench 벤치마크에서 훨씬 더 획기적인 결과를 얻었다. RDM <cite class="ltx_cite ltx_citemacro_cite">Blattmann et al. (<a class="ltx_ref" href="#bib.bib10" title="">2022</a>)</cite>는 RE-IMAGEN과 유사한 방식으로 학습되었지만 이미지 특징을 검색의 기반으로 사용하고 추론 과정에서 사용자 예제로 대체한다. 결과적으로, RDM은 기술된 예술 스타일을 생성된 이미지에 효율적으로 전달할 수 있다. 또한, 이미지-텍스트 쌍을 검색에 사용하는 RE-IMAGEN과 달리 KNN-Diffusion <cite class="ltx_cite ltx_citemacro_cite">Sheynin et al. (<a class="ltx_ref" href="#bib.bib114" title="">2022</a>)</cite>는 이미지를 검색에만 사용하여 COCO 데이터 세트에서 결과의 품질이 더 낮다.</p>
</div>
</section>
<section id="S7.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.3 </span>Fact checking</h4>

<div id="S7.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS2.SSS3.p1.1">사실 확인은 증거에 기초하여 진술을 검증하는 것을 포함한다. 당면한 이 작업은 회수 문제와 도전적인 암시적 추론 작업을 포함한다. 또한, 이 작업은 일반적으로 진술을 입력으로 받아들이고 진술을 증명하거나 반증하는 관련 문서 지문을 생성하는 것을 포함한다. 많은 RALM 모델은 자체 리트리버와 함께 제공되기 때문에 우수한 성능을 얻습니다. 그것은 자연 언어 이해의 중요한 측면이다.</p>
</div>
<div id="S7.SS2.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS2.SSS3.p2.1">RAG <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="#bib.bib71" title="">2020</a>)</cite>는 FEVER 데이터셋을 사용하여 레이블(Supported, Refuted, NotEnoughInfo)을 개별 출력 토큰에 매핑한다. 그것은 다른 작품과 달리 검색된 증거에 대해 감독되지 않는 신고 클래스를 사용하여 직접 훈련된다. Atlas <cite class="ltx_cite ltx_citemacro_cite">Izacard et al. (<a class="ltx_ref" href="#bib.bib54" title="">2022</a>)</cite>는 64-shot 조건에서 기존 연구와 유사한 성능을 얻기 위해 적은shot 학습을 사용한다. 또한 전체 데이터셋으로 학습한 후 당시 사용 가능한 최상의 모델 <cite class="ltx_cite ltx_citemacro_cite">Hoffmann et al. (<a class="ltx_ref" href="#bib.bib44" title="">2022</a>)</cite>를 능가했다. FILCO <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite>는 태그 지원 및 반박만 포함하는 KILT 기반 집계로부터 FEVER 데이터셋을 이용하여 검색된 문서의 품질을 향상시키는 작업에 접근하였다. 정확도는 메트릭으로 사용되었다.</p>
</div>
</section>
<section id="S7.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.4 </span>Knowledge Graph Completion</h4>

<div id="S7.SS2.SSS4.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS2.SSS4.p1.1">다수의 이전 작업들은 지식 그래프 형태의 구조화된 데이터를 사용했으며, 지식 그래프 완성은 퍼베이시브 애플리케이션을 나타낸다. 완료에 대한 종래의 방법론 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="#bib.bib14" title="">2022a</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Saxena et al. (<a class="ltx_ref" href="#bib.bib109" title="">2022</a>)</cite>는 작업을 시퀀스 대 시퀀스 프로세스로 정의하는 것을 수반하며, 여기서 불완전한 트리플 및 엔티티는 텍스트 시퀀스로 변환된다. 그러나, 이 접근법은 암묵적 추론에 대한 의존성에 의해 제약되며, 이는 지식 그래프 자체의 효용을 상당히 제약한다. ReSKGC <cite class="ltx_cite ltx_citemacro_cite">Yu and Yang (<a class="ltx_ref" href="#bib.bib145" title="">2023</a>)</cite>는 지식 그래프 완성과 검색 증강 기술의 통합을 제안한다. 이러한 통합은 지식 그래프에서 의미적으로 관련된 트리플의 선택과 명시적 추론을 통해 출력 생성을 알리기 위한 증거로서의 활용을 수반한다. 이 모델은 Wikidata5M 및 WikiKG90Mv2 데이터 세트의 데이터를 사용하여 다양한 조건에서 기존 다른 작업에 비해 우수한 성능을 보여줍니다.</p>
</div>
</section>
<section id="S7.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.5 </span>Commonsense Reasoning</h4>

<div id="S7.SS2.SSS5.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS2.SSS5.p1.1">상식 추론은 언어 모델에게 어려운 작업이다. 인간과 같은 사고와 추론 패턴을 나타내는 것 외에도 이러한 모델은 상당한 양의 상식적인 지식을 저장할 수 있어야 한다. 그러나 RALM의 등장은 검색 기술이 언어 모델에 비모수 메모리에 대한 액세스를 제공하기 때문에 두 번째 요구 사항을 덜 요구하게 만들었다.</p>
</div>
<div id="S7.SS2.SSS5.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS2.SSS5.p2.1">FLARE <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="#bib.bib58" title="">2023c</a>)</cite>는 다양한 소스에서 상당한 수의 예/아니오 질문을 포함하는 StrategyQA를 사용한다. 또한, 저자들은 모델이 정확한 추론 과정과 예/아니오 답변을 결정하는 최종 답변을 제공하여 해답이 금답과 정확하게 일치하는지 확인하도록 요청한다. COPA, HellaSwag 및 PIQA 데이터 세트의 데이터를 사용한 훈련과 함께 검색된 콘텐츠에 컨텍스트 샘플을 통합하면 우수한 성능을 나타내는 LLM-R <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="#bib.bib130" title="">2023b</a>)</cite> 모델이 개발되었다. ITER-RETGEN <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="#bib.bib30" title="">2022</a>)</cite> 모델의 기본 개념은 반복적인 방법론을 사용하여 검색기와 언어 모델을 통합한다. 이 모델은 StrategyQA 데이터 세트를 사용하여 상식 추론 작업에 대해 훈련되었으며 7회 반복에서 최적의 성능을 달성했다. 이에 반해 KG-BART <cite class="ltx_cite ltx_citemacro_cite">Shao et al. (<a class="ltx_ref" href="#bib.bib112" title="">2023</a>)</cite>는 Commonsense Reasoning 태스크의 우선순위를 정하도록 설계되었으며, 지식 그래프를 사용하여 이 영역에서 성능을 향상시켰다. 이 접근법은 상식 추론 작업을 완료하는 모델의 능력을 크게 향상시키는 데 효과적임이 입증되었으며, 성능은 특정 평가 지표에서 인간의 성능에 접근한다.</p>
</div>
</section>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>RALM on both NLU and NLG tasks</h3>

<section id="S7.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.1 </span>Text Summarization</h4>

<div id="S7.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS3.SSS1.p1.1">텍스트 요약은 언어 모델링의 중요한 응용을 나타낸다. 본질적으로 텍스트 요약은 핵심 정보의 내용과 전체적인 의미를 유지하면서 간결하고 유창한 요약을 생성하는 과정이다. 현재 이 작업에는 추출 요약과 추상 요약이라는 두 가지 유형이 있다.</p>
</div>
<div id="S7.SS3.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS3.SSS1.p2.1">RA-DIT <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="#bib.bib75" title="">2023b</a>)</cite>는 CNN/DailyMail 데이터 세트를 사용하여 모델의 언어 모델 구성요소를 정제하며, 이는 명령 미세 조정 작업으로 인한 텍스트 요약 작업에서 놀라운 효능을 보여준다. 대조적으로, Self-Mem<cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a class="ltx_ref" href="#bib.bib19" title="">2024</a>)</cite>는 XSum 및 BigPatent 데이터 세트를 사용하여 훈련되었다. 저자들은 기억력 향상이 XSum보다 BigPatent에 훨씬 더 큰 영향을 미친다는 것을 관찰했다. 그들은 이러한 불일치가 상당한 유사성을 나타내는 빅특허 데이터 세트에 공식 특허 문서가 포함되었기 때문이라고 가정한다. LLM-R <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="#bib.bib130" title="">2023b</a>)</cite> 모델은 in-context 학습 방식을 사용하여 RALM을 통합하고 AESLC, AG News 및 Gigaword 데이터 세트를 텍스트 요약 학습에 활용한다. 결과는 LLM-R이 요약 작업에서 기존 검색기와 밀집 검색기 모두를 상당히 능가한다는 것을 보여준다. RAMKG <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="#bib.bib30" title="">2022</a>)</cite>는 RALM 아키텍처의 반복적인 학습을 다국어 도메인으로 확장하고 요약 작업의 학습을 위해 EcommerceMKP와 AcademicMKP 두 개의 다국어 데이터 세트를 사용하여 그 당시 최상의 결과를 달성했다.</p>
</div>
</section>
<section id="S7.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.2 </span>Question Answering</h4>

<div id="S7.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS3.SSS2.p1.1">질의 응답에는 생성 및 추출 형태도 포함됩니다. 영역별 지식에 의존하는 NLP의 일반적인 작업이다. RALM은 외부에 저장된 지식을 활용하여 전통적인 언어 모델보다 더 나은 결과를 얻을 수 있다. 일반적인 질의 응답 작업에는 도메인별 QA, 개방형 도메인 QA(ODQA) 및 다중 홉 QA가 포함됩니다. 의료 분야에서는 대규모 언어 모델이 일반적으로 사용되며, PKG<cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="#bib.bib83" title="">2023b</a>)</cite>는 MedMC-QA 데이터 세트의 관련 데이터를 사용하여 훈련 세트의 질문을 입력으로 사용하고 의학 설명을 배경 지식으로 사용하며 모델에 의해 생성된 배경 지식의 정확도를 평가 메트릭으로 사용한다. HyKGE<cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="#bib.bib57" title="">2023b</a>)</cite>도 의학 분야의 질문 응답을 대상으로 하지만 지식 그래프 강화 접근법을 사용한다. ODQA 작업을 대상으로 할 때 RAG <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="#bib.bib71" title="">2020</a>)</cite>는 질문과 답변을 입출력 텍스트 쌍으로 간주하고 답변의 부정적인 로그 우도를 최소화하여 학습한다. 이에 반해 ICRALM <cite class="ltx_cite ltx_citemacro_cite">Ram et al. (<a class="ltx_ref" href="#bib.bib97" title="">2023</a>)</cite>는 사전 학습, 미세 조정 또는 검색에 의해 향상되지 않은 냉동 LMs와 관련 지식 문서를 사용하여 ODQA 작업을 독점적으로 수행한다. 다른 모델 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="#bib.bib75" title="">2023b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="#bib.bib117" title="">2023b</a>)</cite>도 ODQA 작업에 대해 훈련되었다. 다중 홉 QA 태스크와 관련하여, FILCO <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="#bib.bib131" title="">2023c</a>)</cite>는 여러 문서를 필터링하기 위해 그들의 제안된 필터링된 검색 방법을 사용했다. 그들은 HotpotQA 데이터 세트를 사용하여 접근 방식을 검증했습니다. 반면에 RR <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="#bib.bib39" title="">2022</a>)</cite>는 다중 홉 문제를 해결하기 위해 CoT(Chain-of-Thought) 접근법을 사용했다. 또한, 다른 많은 모델 <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="#bib.bib58" title="">2023c</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Khattab et al. (<a class="ltx_ref" href="#bib.bib64" title="">2022</a>)</cite>는 멀티 홉 문제를 다루고 있다.</p>
</div>
</section>
<section id="S7.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.3 </span>Code Generation and Summarization</h4>

<div id="S7.SS3.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS3.SSS3.p1.1">코드 생성 <cite class="ltx_cite ltx_citemacro_cite">Romera-Paredes et al. (<a class="ltx_ref" href="#bib.bib103" title="">2024</a>); Ye et al. (<a class="ltx_ref" href="#bib.bib140" title="">2024</a>)</cite> 및 요약 <cite class="ltx_cite ltx_citemacro_cite">Nam et al. (<a class="ltx_ref" href="#bib.bib90" title="">2024</a>)</cite>는 일반 텍스트 생성 및 요약과 대상 청중 및 처리 측면에서 차이가 있다. 코드 생성 및 요약은 언어 모델의 NLU 및 NLG 기능에 대한 더 높은 요구 사항 외에도 도메인별 구문 및 의미론적 이해가 필요할 수 있는 컴퓨터 프로그램 코드를 포함한다.</p>
</div>
<div id="S7.SS3.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS3.SSS3.p2.1">REDCODER <cite class="ltx_cite ltx_citemacro_cite">Parvez et al. (<a class="ltx_ref" href="#bib.bib92" title="">2021</a>)</cite>는 기존 코드 또는 추상 데이터베이스로부터 초기에 잠재적인 후보 코드를 식별하였다. 연구진은 여러 경로를 통해 검색된 데이터로 110만 개의 고유한 코드와 초록을 보유했다. CodeXGLUE 데이터 세트에 대한 최종 평가는 여러 프로그래밍 언어 및 평가 메트릭에 걸쳐 우수한 성능을 보여주었다. 코드 생성의 품질을 향상시키기 위해 사설 라이브러리를 활용한 <cite class="ltx_cite ltx_citemacro_citet">Zan et al. (<a class="ltx_ref" href="#bib.bib149" title="">2022</a>)</cite>가 제안한 또 다른 개선안을 제시하였다. 여기에는 먼저 APIRetriever 구성 요소를 통해 API 설명서를 기반으로 가장 적합한 개인 라이브러리를 식별한 다음 생성을 위해 APIcoder를 활용하는 것이 포함되었다. 이 접근법은 생성된 콘텐츠의 정확도를 현저하게 향상시켰다.</p>
</div>
<div id="S7.SS3.SSS3.p3" class="ltx_para">
<p class="ltx_p" id="S7.SS3.SSS3.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="#bib.bib77" title="">2020</a>)</cite>는 정적 그래프 표현을 위한 소스 코드를 보완하고 로컬 및 글로벌 구조 정보를 캡처하기 위한 하이브리드 메시지 전달 GNN을 설계하기 위한 새로운 주의 기반 동적 그래프를 제안한다. 이 접근법은 코드 요약의 정확도를 향상시키고 궁극적으로 주류 검색기 및 생성기 모두에 비해 우수한 성능을 산출한다. RACE <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="#bib.bib115" title="">2022</a>)</cite> 모델은 MCMD 데이터 세트 내에서 5개의 프로그래밍 언어로 코드를 통합하기 위해 기존의 RALM 프레임워크를 사용하여 모든 기본 모델에 대해 6~38%의 향상을 가져왔다.</p>
</div>
<figure id="S7.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 3:</span>RALM에서의 평가 방법 요약.</figcaption>
<div id="S7.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:432.8pt;height:363.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-101.8pt,85.5pt) scale(0.68,0.68) ;">
<table id="S7.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S7.T3.1.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Reference</td>
<td id="S7.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">RAGAS</td>
</tr>
<tr id="S7.T3.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Es et&nbsp;al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.3.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">RGB</td>
</tr>
<tr id="S7.T3.1.1.1.1.3.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib15" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.4.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">CRUD-RAG</td>
</tr>
<tr id="S7.T3.1.1.1.1.4.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lyu et&nbsp;al. (<a href="#bib.bib84" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.5.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">ARES</td>
</tr>
<tr id="S7.T3.1.1.1.1.5.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Saad-Falcon et&nbsp;al. (<a href="#bib.bib105" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.6.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">MIRAGE</td>
</tr>
<tr id="S7.T3.1.1.1.1.6.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Xiong et&nbsp;al. (<a href="#bib.bib133" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.7.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">RECALL</td>
</tr>
<tr id="S7.T3.1.1.1.1.7.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib78" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
</tbody></table>
</td>
</tr>
<tr id="S7.T3.1.1.2.2" class="ltx_tr">
<td id="S7.T3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Dataset</td>
<td id="S7.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">WikiEval</td>
<td id="S7.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3">LLM-generated</td>
<td id="S7.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.2.2.4.1.1" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">MMLU-Med</td>
</tr>
<tr id="S7.T3.1.1.2.2.4.1.2" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">MedQA-US</td>
</tr>
<tr id="S7.T3.1.1.2.2.4.1.3" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">MedMCQA</td>
</tr>
<tr id="S7.T3.1.1.2.2.4.1.4" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">PubMedQA</td>
</tr>
<tr id="S7.T3.1.1.2.2.4.1.5" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">BioASQ-Y/N</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.2.2.5.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.2.2.5.1.1" class="ltx_tr">
<td id="S7.T3.1.1.2.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">EventKG</td>
</tr>
<tr id="S7.T3.1.1.2.2.5.1.2" class="ltx_tr">
<td id="S7.T3.1.1.2.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">UJ</td>
</tr>
</tbody></table>
</td>
</tr>
<tr id="S7.T3.1.1.3.3" class="ltx_tr">
<td id="S7.T3.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Target</td>
<td id="S7.T3.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="5">
<table id="S7.T3.1.1.3.3.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.3.3.2.1.1" class="ltx_tr">
<td id="S7.T3.1.1.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">Retrieval Quality; Generation Quality</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Generation Quality</td>
</tr>
<tr id="S7.T3.1.1.4.4" class="ltx_tr">
<td id="S7.T3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.4.4.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.4.4.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Context</td>
</tr>
<tr id="S7.T3.1.1.4.4.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.4.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Relevance</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.4.4.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.4.4.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.4.4.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.4.4.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.5.5" class="ltx_tr">
<td id="S7.T3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.5.5.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.5.5.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Faithfulness</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
</tr>
<tr id="S7.T3.1.1.6.6" class="ltx_tr">
<td id="S7.T3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.6.6.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.6.6.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.6.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Answer</td>
</tr>
<tr id="S7.T3.1.1.6.6.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.6.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Relevance</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.6.6.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.6.6.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.6.6.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.6.6.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.7.7" class="ltx_tr">
<td id="S7.T3.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.7.7.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.7.7.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.7.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Noise</td>
</tr>
<tr id="S7.T3.1.1.7.7.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.7.7.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Robustness</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.7.7.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.7.7.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.7.7.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.7.7.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.7.7.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.8.8" class="ltx_tr">
<td id="S7.T3.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.8.8.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.8.8.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.8.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Information</td>
</tr>
<tr id="S7.T3.1.1.8.8.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.8.8.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Integration</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.8.8.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.8.8.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.8.8.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.9.9" class="ltx_tr">
<td id="S7.T3.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.9.9.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.9.9.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.9.9.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Negative</td>
</tr>
<tr id="S7.T3.1.1.9.9.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.9.9.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Rejection</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.9.9.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.9.9.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.9.9.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.9.9.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.9.9.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.10.10" class="ltx_tr">
<td id="S7.T3.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.10.10.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.10.10.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.10.10.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Counterfactual</td>
</tr>
<tr id="S7.T3.1.1.10.10.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.10.10.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Robustness</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.10.10.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.10.10.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.10.10.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
</tr>
<tr id="S7.T3.1.1.11.11" class="ltx_tr">
<td id="S7.T3.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.11.11.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.11.11.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.11.11.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Error</td>
</tr>
<tr id="S7.T3.1.1.11.11.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.11.11.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Correction</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.11.11.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.11.11.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.11.11.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.11.11.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.11.11.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.12.12" class="ltx_tr">
<td id="S7.T3.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.12.12.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.12.12.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.12.12.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Summarization</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.12.12.2" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.12.12.3" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">√</td>
<td id="S7.T3.1.1.12.12.5" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.12.12.6" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.12.12.7" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Evaluation</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p" id="S8.p1.1">이 섹션에서는 RALM에 대한 평가 접근법과 벤치마크의 요약을 제공한다. <a class="ltx_ref" href="#S6" title="6 Data Sources ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">6</span></a> 및 <a class="ltx_ref" href="#S7" title="7 Applications ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">7</span></a> 절에서는 베이스라인뿐만 아니라 대형 언어 모델 작업에 대한 몇 가지 평가 기준을 제시하였다. RALM 아키텍처의 초기 제안 당시 대부분의 연구자들은 일반화된 벤치마크를 사용했다. 그러나 RALM 아키텍처가 발전함에 따라 RALM에 특화된 평가 방법과 기준선이 제안되었다. 표 <a class="ltx_ref" href="#S7.T3" title="Table 3 ‣ 7.3.3 Code Generation and Summarization ‣ 7.3 RALM on both NLU and NLG tasks ‣ 7 Applications ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">3</span></a>는 각 평가 모델의 상세를 설명한다.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p class="ltx_p" id="S8.p2.1">RAGAS <cite class="ltx_cite ltx_citemacro_cite">Es et al. (<a class="ltx_ref" href="#bib.bib27" title="">2023</a>)</cite>는 RALM의 충실성, 답변 관련성 및 컨텍스트 관련성을 평가하기 위해 WikiEval Dataset을 사용한다. 충실성은 응답이 제공된 맥락과 일치하는 정도로 정의된다. 답변 관련성은 생성된 응답이 실제 질문을 해결하는 정도를 나타낸다. 컨텍스트 관련성은 검색된 컨텍스트가 중앙 집중화되고 관련 없는 정보가 없는 정도에 의해 측정된다. 또한, 연구자들은 신속한 gpt-3.5-turbo-16k 모델을 사용하여 평가 프로세스를 자동화한다. RGB <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="#bib.bib15" title="">2024</a>)</cite>는 정확도, 거부율, 오류 검출율의 세 가지 평가 메트릭을 사용하는 이중 언어 중국어 및 영어 평가 시스템을 개발했다. 이러한 메트릭은 LM에 의해 처리되고 구글 API를 통해 검색된 기사인 데이터 소스의 노이즈 견고성, 부정적인 거부, 정보 통합 및 반사실적 견고성을 평가하는 데 활용되었다. CRUD-RAG <cite class="ltx_cite ltx_citemacro_cite">Lyu et al. (<a class="ltx_ref" href="#bib.bib84" title="">2024</a>)</cite>는 검색 컴포넌트의 영향과 기존에 연구자들에 의해 고려되지 않았던 외부 지식 베이스의 구성을 고려한다. ROUGE, BLEU, bertScore 및 RAGQuestEval의 4가지 평가 메트릭을 통해 RALM의 생성, 읽기, 업데이트 및 삭제(요약) 기능을 평가하기 위해 대규모 모델을 사용하여 데이터 세트를 생성했다. 또한 ARES <cite class="ltx_cite ltx_citemacro_cite">Saad-Falcon et al. (<a class="ltx_ref" href="#bib.bib105" title="">2023</a>)</cite>는 LM에 의해 생성된 데이터 세트를 사용하지만 경량 LM을 사용하여 개별 RALM 구성요소의 품질을 결정하고 예측 기반 추론을 위해 인간 라벨링된 데이터 포인트를 활용한다. RALM의 컨텍스트는 KILT 및 SuperGLUE 벤치마크를 사용하여 평가되며, 관련성, 답변 충실성 및 답변 관련성이 관련 기준이다.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p class="ltx_p" id="S8.p3.1">RALM의 일반적인 평가 외에도 특정 세부 사항 및 영역의 평가에 초점을 맞춘 작업이 있었다. RECALL <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib78" title="">2023</a>)</cite>는 EventKG 및 UJ 데이터 세트를 사용하여 부정확한 정보를 기존 데이터 세트에 통합합니다. 그런 다음 질문 응답과 텍스트 생성이라는 두 가지 작업을 통해 RALM이 이러한 부정확한 정보에 의해 오도되기 쉬운지 여부를 결정한다. <cite class="ltx_cite ltx_citemacro_citet">Xiong et al. (<a class="ltx_ref" href="#bib.bib133" title="">2024</a>)</cite>는 의료 영역에 집중되었으며 MMLU-Med를 포함한 5개 데이터 세트의 데이터를 통합하여 의료 RALM의 제로샷 학습, 다중 선택 평가, 검색 강화 생성 및 질문 전용 검색 아이디어 기능을 평가하는 MIRAGE를 제안했다. 궁극적으로 이들은 의료 영역에서 로그 선형 스케일링 특성과 ‘중간 손실’ 효과도 발견했다.</p>
</div>
<figure id="S8.F7" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2404.19543/assets/x7.png" id="S8.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7:</span>Summary of limitations of current RALM models and future prospects.</figcaption>
</figure>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Disscussion</h2>

<div id="S9.p1" class="ltx_para">
<p class="ltx_p" id="S9.p1.1">이 섹션에서는 기존 RALM 아키텍처의 한계에 대한 분석과 향후 개발 가능성에 대한 설명에 전념한다. 그림 <a class="ltx_ref" href="#S8.F7" title="Figure 7 ‣ 8 Evaluation ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">7</span></a>는 기존 RALM과 제안한 솔루션의 한계를 요약한 것이다.</p>
</div>
<section id="S9.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>Limitations</h3>

<div id="S9.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS1.p1.1">이 절에서는 기존 RALM의 몇 가지 제한 사항에 대한 요약 및 분석을 제시한다.</p>
</div>
<section id="S9.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.1 </span>Poor Robustness</h4>

<div id="S9.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS1.SSS1.p1.1">견고성은 모든 시스템에서 고려해야 할 중요한 측면입니다. RALM 시스템은 여러 영역에서 성능상의 이점을 나타내지만, 검색의 통합으로 인해 아키텍처에 많은 불확실성을 도입한다. <cite class="ltx_cite ltx_citemacro_citet">Hu et al. (<a class="ltx_ref" href="#bib.bib47" title="">2024</a>)</cite>에 의해 해명된 바와 같이, 매우 단순한 접두사 공격을 통해 RALM 출력의 관련성과 정확성을 감소시킬 수 있을 뿐만 아니라 리트리버의 검색 전략도 변경될 수 있다. 따라서, LM의 성능을 향상시키기 위해 다양한 검색 향상 기법을 사용하는 것 외에도, 연구자들은 사실적으로 부정확한 데이터, 문제 해결과 관련이 없는 정보, 심지어 일부 해로운 힌트 및 접두사가 LM에 미치는 영향을 최소화하도록 주의해야 한다.</p>
</div>
</section>
<section id="S9.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.2 </span>Poor Quality of Retrieval Results</h4>

<div id="S9.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS1.SSS2.p1.1">검색 효율성을 높이기 위한 노력에 참여한 상당수의 연구자 <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a class="ltx_ref" href="#bib.bib135" title="">2024</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Asai et al. (<a class="ltx_ref" href="#bib.bib4" title="">2023</a>)</cite>는 그들의 제안된 모델이 출력의 품질을 최적화하는 데 분명히 유익하지만 검색 결과가 LM과 완전히 일치할 수 있다는 보장은 아직 없다고 주장했다. 특히 인터넷을 검색 도구로 사용할 때 인터넷 소스의 품질은 매우 다양할 수 있으며 적절한 고려 없이 이 데이터를 병합하면 노이즈 또는 오판의 소지가 있는 정보를 결과 출력에 도입할 수 있다.</p>
</div>
</section>
<section id="S9.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.3 </span>Overspending</h4>

<div id="S9.SS1.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS1.SSS3.p1.1">기존의 RALMs <cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et al. (<a class="ltx_ref" href="#bib.bib119" title="">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Guu et al. (<a class="ltx_ref" href="#bib.bib36" title="">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Borgeaud et al. (<a class="ltx_ref" href="#bib.bib11" title="">2022</a>)</cite>는 다양한 도메인에서 LM의 성능을 크게 향상시킬 수 있지만, 그 중 일부는 복잡한 사전 훈련 및 미세 조정 작업뿐만 아니라 광범위한 모델 변경을 요구하여 시간과 공간 오버헤드를 크게 증가시키고 또한 RALM의 확장성을 감소시킨다. 또한, 검색의 규모가 증가함에 따라, 데이터 소스를 저장하고 액세스하는 복잡성도 증가한다. 결과적으로 연구자들은 모델을 수정하는 것의 이점을 비용과 비교해야 한다.</p>
</div>
</section>
<section id="S9.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.4 </span>Few Applications</h4>

<div id="S9.SS1.SSS4.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS1.SSS4.p1.1">비록 많은 RALM이 다양한 도메인에서 LM의 성능을 크게 향상시켰지만, 애플리케이션 관점에서는 그다지 개선되지 않았으며, RALM은 여전히 LM의 초기 단계에서 수행되었던 일상적인 작업, 예를 들어, <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="#bib.bib83" title="">2023b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="#bib.bib57" title="">2023b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Alon et al. (<a class="ltx_ref" href="#bib.bib3" title="">2022</a>)</cite>를 요약하는 질문 응답 작업을 수행하고 있다. 최근 수학 교수 <cite class="ltx_cite ltx_citemacro_cite">Levonian et al. (<a class="ltx_ref" href="#bib.bib69" title="">2023</a>)</cite>, 슬롯 채우기 <cite class="ltx_cite ltx_citemacro_cite">Glass et al. (<a class="ltx_ref" href="#bib.bib33" title="">2021</a>)</cite> 등과 같이 매우 흥미로운 응용 지침이 있었지만, 이는 충분하지 않다. 기술은 항상 그 가치를 완전히 증명하기 위해 실제로 사용되어야 하며, RALM도 예외는 아니다.</p>
</div>
</section>
</section>
<section id="S9.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>Future Prospects</h3>

<div id="S9.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS2.p1.1">이 섹션에서는 주로 섹션 <a class="ltx_ref" href="#S9.SS1" title="9.1 Limitations ‣ 9 Disscussion ‣ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"><span class="ltx_text ltx_ref_tag">9.1</span></a>에서 언급한 제한 사항을 기반으로 RALM의 향후 개발을 위한 몇 가지 가능한 방향을 제안한다.</p>
</div>
<section id="S9.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.1 </span>Improve Robustness</h4>

<div id="S9.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS2.SSS1.p1.1">일부 학자들은 명시적 자기반성 및 세립귀속 <cite class="ltx_cite ltx_citemacro_cite">Asai et al. (<a class="ltx_ref" href="#bib.bib4" title="">2023</a>)</cite>와 같이 논문의 향후 작업 섹션에서 모델 견고성을 향상시킬 수 있는 가능한 방법을 언급했다. 이러한 연구들과 달리, <cite class="ltx_cite ltx_citemacro_citet">Hu et al. (<a class="ltx_ref" href="#bib.bib47" title="">2024</a>)</cite>는 RALM을 교란시키는 방법인 Gradient Guided Prompt Perturbation (GGPP) 방법을 제안하였는데, 이는 SAT probe <cite class="ltx_cite ltx_citemacro_cite">Yuksekgonul et al. (<a class="ltx_ref" href="#bib.bib148" title="">2023</a>)</cite> and activation (ACT) 분류기를 활용하여 상황 개선에 효과적임을 실험적으로 확인하였다. 섭동된 RALM의 내부 상태를 프롬프트하여 이러한 섭동을 검출하는 방법이 제안된다. 또한 RALM의 평가 방법과 관련 기준선을 제안하고 개선함으로써 모델의 강건성 향상에도 도움을 줄 수 있으며, <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="#bib.bib15" title="">2024</a>)</cite>는 강건성에 초점을 맞추어 RALM에 대한 일련의 평가 시스템을 만들었다.</p>
</div>
</section>
<section id="S9.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.2 </span>Improve Retrieval Quality</h4>

<div id="S9.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS2.SSS2.p1.1">검색의 품질 향상은 검색에 사용되는 데이터세트의 품질 향상과 검색 기법의 성능 향상이라는 두 가지 부분에서 고려될 수 있다. 오늘날에는 관련 콘텐츠를 생성하기 위해 LLM에 많은 데이터 세트가 주어지고, LLM 자체가 “환각”을 가지고 있기 때문에, 정제 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="#bib.bib15" title="">2024</a>)</cite>를 감독하기 위해 인간을 사용하는 것과 같이 데이터의 정확성을 보장하기 위해 특정 수단을 채택해야 한다. 또한, 인터넷 상의 광범위한 정보원들로 인해, 스크리닝을 위해 검색 엔진에만 의존하는 것은 명백히 충분하지 않기 때문에, BM25 <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a class="ltx_ref" href="#bib.bib82" title="">2023a</a>)</cite> 또는 TF-IDF <cite class="ltx_cite ltx_citemacro_cite">Lazaridou et al. (<a class="ltx_ref" href="#bib.bib67" title="">2022</a>)</cite> 알고리즘을 더 재순위화하는 것과 같은 검색 기술의 개선이 필요하다.</p>
</div>
</section>
<section id="S9.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.3 </span>Weigh Expenses and Benefits</h4>

<div id="S9.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS2.SSS3.p1.1">오버헤드를 줄이는 것은 세 가지 관점에서 고려될 수 있다: 먼저, 일부 플러그 앤 플레이 중간 모듈, 예를 들어 CRAG <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a class="ltx_ref" href="#bib.bib135" title="">2024</a>)</cite>, Selfmem <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a class="ltx_ref" href="#bib.bib19" title="">2024</a>)</cite>, AI 에이전트 <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="#bib.bib93" title="">2023</a>)</cite> 또는 일부 배포 솔루션, 예를 들어 LangChain, Llama Index를 설계하여 각 모델에 대해 목표된 개선을 할 필요가 없다. 둘째, 인터넷 검색을 활용하여 리트리버의 오버헤드를 줄일 수 있지만, 앞서 언급한 데이터 관련성에 주의를 기울일 필요가 있다. 마지막으로, ICRALM <cite class="ltx_cite ltx_citemacro_cite">Ram et al. (<a class="ltx_ref" href="#bib.bib97" title="">2023</a>)</cite>와 같은 LMs 개선과 관련된 오버헤드를 줄이기 위해 In-context 학습이 채용될 수 있다.</p>
</div>
</section>
<section id="S9.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.4 </span>Expand Applications</h4>

<div id="S9.SS2.SSS4.p1" class="ltx_para">
<p class="ltx_p" id="S9.SS2.SSS4.p1.1">현대 사회에서 LLM의 적용은 다양한 영역을 포함하도록 확장되었지만 RALM의 적용 방향은 상대적으로 제한적이다. 이러한 한계를 해결하기 위해 연구자들은 LLM의 기존 적용 영역을 고려할 뿐만 아니라 지식 및 경험과 밀접하게 관련된 문제를 해결하는 데 탁월한 RALM의 고유한 장점을 활용해야 한다. 또한 RALM을 다른 첨단 기술과 통합하고 이를 활용하여 관련 문제를 극복해야 한다. 본 논문에서는 의사결정 지원, 검색 엔진, 추천 시스템 등 몇 가지 예시들을 제시한다.</p>
</div>
</section>
</section>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Conclusion</h2>

<div id="S10.p1" class="ltx_para">
<p class="ltx_p" id="S10.p1.1">RALM의 통합은 NLP 시스템의 기능에서 상당한 발전을 나타낸다. 이 조사는 RALM에 대한 광범위한 검토를 제공하여 아키텍처, 애플리케이션 및 직면한 문제를 강조했다. RALM은 외부 지식을 검색하고 통합하여 언어 모델을 향상시키며, 번역, 대화 생성 및 지식 그래프 완성을 포함한 다양한 NLP 작업에 걸쳐 향상된 성능을 제공합니다.</p>
</div>
<div id="S10.p2" class="ltx_para">
<p class="ltx_p" id="S10.p2.1">그들의 성공에도 불구하고, RALM은 몇 가지 한계에 직면한다. 특히, 적대적 입력에 대한 견고성, 검색 결과의 품질, 배치와 관련된 계산 비용, 애플리케이션 도메인의 다양성 부족이 추가 주의가 필요한 영역으로 확인되었다. 이를 해결하기 위해 연구 커뮤니티는 평가 방법 개선, 검색 기술 정제, 성능과 효율성 사이의 균형을 유지하는 비용 효율적인 솔루션 탐색과 같은 몇 가지 전략을 제안했다.</p>
</div>
<div id="S10.p3" class="ltx_para">
<p class="ltx_p" id="S10.p3.1">향후 RALM의 고도화는 RALM의 강건성 향상, 검색 품질의 향상, 적용 범위의 확장에 달려 있을 것이다. 보다 정교한 기술을 통합하고 RALM을 다른 AI 기술과 통합함으로써 이러한 모델을 활용하여 훨씬 광범위한 범위의 문제를 해결할 수 있다. 이 분야에서 진행 중인 연구 개발은 보다 탄력적이고 효율적이며 다재다능한 RALM을 초래하여 NLP 및 그 이상에서 달성할 수 있는 것의 경계를 밀어낼 것으로 예상된다. RALM이 계속 발전함에 따라, 그들은 더 깊은 이해와 더 인간다운 언어 능력을 가진 AI 시스템을 가능하게 하여 광범위한 분야에서 새로운 가능성을 열겠다는 약속을 가지고 있다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia&nbsp;Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adolphs et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Leonard Adolphs, Benjamin Boerschinger, Christian Buck, Michelle&nbsp;Chen Huebscher, Massimiliano Ciaramita, Lasse Espeholt, Thomas Hofmann, Yannic Kilcher, Sascha Rothe, Pier&nbsp;Giuseppe Sessa, et&nbsp;al. 2021.

</span>
<span class="ltx_bibblock">Boosting search engines with interactive agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.00527</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alon et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock">Neuro-symbolic language modeling with automaton-augmented retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 468–485. PMLR.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">Self-rag: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.11511</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aussenac-Gilles and Sörgel (2005)</span>
<span class="ltx_bibblock">
Nathalie Aussenac-Gilles and Dagobert Sörgel. 2005.

</span>
<span class="ltx_bibblock">Text analysis for ontology and terminology engineering.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Applied Ontology</em>, 1(1):35–46.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Jimmy&nbsp;Lei Ba, Jamie&nbsp;Ryan Kiros, and Geoffrey&nbsp;E Hinton. 2016.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1607.06450</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Imon Banerjee, Yuan Ling, Matthew&nbsp;C Chen, Sadid&nbsp;A Hasan, Curtis&nbsp;P Langlotz, Nathaniel Moradzadeh, Brian Chapman, Timothy Amrhein, David Mong, Daniel&nbsp;L Rubin, et&nbsp;al. 2019.

</span>
<span class="ltx_bibblock">Comparative effectiveness of convolutional neural network (cnn) and recurrent neural network (rnn) architectures for radiology text report classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence in medicine</em>, 97:79–88.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Gpt-neox-20b: An open-source autoregressive language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.06745</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blattmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. 2022.

</span>
<span class="ltx_bibblock">Retrieval-augmented diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:15309–15324.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George&nbsp;Bm Van Den&nbsp;Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 2206–2240. PMLR.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boytsov et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Leonid Boytsov, David Novak, Yury Malkov, and Eric Nyberg. 2016.

</span>
<span class="ltx_bibblock">Off the beaten path: Let’s replace term-based retrieval with k-nn search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM international on conference on information and knowledge management</em>, pages 1099–1108.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Chen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam. 2022a.

</span>
<span class="ltx_bibblock">Knowledge is flat: A seq2seq generative framework for various knowledge graph completion.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.07299</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le&nbsp;Sun. 2024.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume&nbsp;38, pages 17754–17762.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Hongyang Chao, and Tao Mei. 2023.

</span>
<span class="ltx_bibblock">Retrieval augmented convolutional encoder-decoder networks for video captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Multimedia Computing, Communications and Applications</em>, 19(1s):1–24.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Xi&nbsp;Chen, Pat Verga, and William&nbsp;W Cohen. 2022b.

</span>
<span class="ltx_bibblock">Murag: Multimodal retrieval-augmented generator for open question answering over images and text.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.02928</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022c)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William&nbsp;W Cohen. 2022c.

</span>
<span class="ltx_bibblock">Re-imagen: Retrieval-augmented text-to-image generator.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.14491</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Xin Cheng, Di&nbsp;Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2024.

</span>
<span class="ltx_bibblock">Lift yourself up: Retrieval-augmented text generation with self-memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Child et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock">Generating long sequences with sparse transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.10509</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 24(240):1–113.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, Shayne Longpre, Barret Zoph, Yi&nbsp;Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.11416</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dauphin et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Yann&nbsp;N Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017.

</span>
<span class="ltx_bibblock">Language modeling with gated convolutional networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 933–941. PMLR.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drozdov et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, and Mohit Iyyer. 2022.

</span>
<span class="ltx_bibblock">You can’t pick your neighbors, or can you? when and how to rely on retrieval in the <math id="bib.bib25.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="bib.bib25.1.m1.1a"><mi id="bib.bib25.1.m1.1.1" xref="bib.bib25.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="bib.bib25.1.m1.1b"><ci id="bib.bib25.1.m1.1.1.cmml" xref="bib.bib25.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib25.1.m1.1c">k</annotation></semantics></math> nn-lm.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.15859</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Yann Dubois, Chen&nbsp;Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy&nbsp;S Liang, and Tatsunori&nbsp;B Hashimoto. 2024.

</span>
<span class="ltx_bibblock">Alpacafarm: A simulation framework for methods that learn from human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Es et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023.

</span>
<span class="ltx_bibblock">Ragas: Automated evaluation of retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15217</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Chao Feng, Xinyu Zhang, and Zichu Fei. 2023.

</span>
<span class="ltx_bibblock">Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.03118</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024.

</span>
<span class="ltx_bibblock">Retrieval-generation synergy augmented large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 11661–11665. IEEE.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yifan Gao, Qingyu Yin, Zheng Li, Rui Meng, Tong Zhao, Bing Yin, Irwin King, and Michael&nbsp;R Lyu. 2022.

</span>
<span class="ltx_bibblock">Retrieval-augmented multilingual keyphrase generation with retriever-generator iterative training.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.10471</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi&nbsp;Dai, Jiawei Sun, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10997</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Sreyan Ghosh, Sonal Kumar, Chandra Kiran&nbsp;Reddy Evuru, Ramani Duraiswami, and Dinesh Manocha. 2024.

</span>
<span class="ltx_bibblock">Recap: retrieval-augmented audio captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1161–1165. IEEE.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glass et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Michael Glass, Gaetano Rossiello, Md&nbsp;Faisal&nbsp;Mahbub Chowdhury, and Alfio Gliozzo. 2021.

</span>
<span class="ltx_bibblock">Robust retrieval augmented generation for zero-shot slot filling.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.13934</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gou et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianping Gou, Baosheng Yu, Stephen&nbsp;J Maybank, and Dacheng Tao. 2021.

</span>
<span class="ltx_bibblock">Knowledge distillation: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 129(6):1789–1819.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grave et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016.

</span>
<span class="ltx_bibblock">Improving neural language models with a continuous cache.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1612.04426</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 3929–3938. PMLR.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hambarde and Proenca (2023)</span>
<span class="ltx_bibblock">
Kailash&nbsp;A Hambarde and Hugo Proenca. 2023.

</span>
<span class="ltx_bibblock">Information retrieval: recent advances and beyond.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Imagebind-llm: Multi-modality instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.03905</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hangfeng He, Hongming Zhang, and Dan Roth. 2022.

</span>
<span class="ltx_bibblock">Rethinking with retrieval: Faithful large language model inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.00303</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021.

</span>
<span class="ltx_bibblock">Efficient nearest neighbor language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.04212</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 770–778.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh&nbsp;V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024.

</span>
<span class="ltx_bibblock">G-retriever: Retrieval-augmented generation for textual graph understanding and question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.07630</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Animate-a-story: Storytelling with retrieval-augmented video generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.06940</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15556</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hofstätter et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">Fid-light: Efficient and effective retrieval-augmented text generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pages 1437–1447.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hogenboom et&nbsp;al. (2010)</span>
<span class="ltx_bibblock">
Frederik Hogenboom, Flavius Frasincar, and Uzay Kaymak. 2010.

</span>
<span class="ltx_bibblock">An overview of approaches to extract information from natural language corpora.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Information Foraging Lab</em>, 69.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Zhibo Hu, Chen Wang, Yanfeng Shu, Liming Zhu, et&nbsp;al. 2024.

</span>
<span class="ltx_bibblock">Prompt perturbation in retrieval-augmented generation based large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.07179</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David&nbsp;A Ross, and Alireza Fathi. 2023.

</span>
<span class="ltx_bibblock">Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 23369–23379.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hua and Wang (2018)</span>
<span class="ltx_bibblock">
Xinyu Hua and Lu&nbsp;Wang. 2018.

</span>
<span class="ltx_bibblock">Neural argument generation augmented with externally retrieved evidence.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.10254</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi&nbsp;Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. 2023.

</span>
<span class="ltx_bibblock">Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 13916–13932. PMLR.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.09118</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2020a)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2020a.

</span>
<span class="ltx_bibblock">Distilling knowledge from reader to retriever for question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.04584</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2020b)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2020b.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.01282</em>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.03299</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye&nbsp;Jin Bang, Andrea Madotto, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 55(12):1–38.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding, Xu&nbsp;Chu, Junfeng Zhao, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Think and retrieval: A hypothesis knowledge graph enhanced medical large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15883</em>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank&nbsp;F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023c.

</span>
<span class="ltx_bibblock">Active retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.06983</em>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. 2020.

</span>
<span class="ltx_bibblock">Convbert: Improving bert with span-based dynamic convolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:12837–12848.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Minki Kang, Jin&nbsp;Myung Kwak, Jinheon Baek, and Sung&nbsp;Ju Hwang. 2023.

</span>
<span class="ltx_bibblock">Knowledge graph-augmented language models for knowledge-grounded dialogue generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18846</em>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.04906</em>.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020.

</span>
<span class="ltx_bibblock">Nearest neighbor machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.00710</em>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019.

</span>
<span class="ltx_bibblock">Generalization through memorization: Nearest neighbor language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00172</em>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Omar Khattab, Keshav Santhanam, Xiang&nbsp;Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022.

</span>
<span class="ltx_bibblock">Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.14024</em>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Komeili et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2021.

</span>
<span class="ltx_bibblock">Internet-augmented dialogue generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.07566</em>.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et&nbsp;al. 2019.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 7:453–466.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazaridou et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022.

</span>
<span class="ltx_bibblock">Internet-augmented language models through few-shot prompting for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.05115</em>.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Latent retrieval for weakly supervised open domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.00300</em>.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levonian et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel, Millie-Ellen Postle, and Wanli Xing. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation to improve math question-answering: Trade-offs between groundedness and human preference.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03184</em>.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.13461</em>.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:9459–9474.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Bowen Li, Philip&nbsp;HS Torr, and Thomas Lukasiewicz. 2022a.

</span>
<span class="ltx_bibblock">Memory-driven text-to-image generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.07022</em>.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022b.

</span>
<span class="ltx_bibblock">A survey on retrieval-augmented text generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.01110</em>.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023a.

</span>
<span class="ltx_bibblock">How to train your dragon: Diverse augmentation towards generalizable dense retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.07452</em>.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01352</em>.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual generative language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 9019–9052.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Shangqing Liu, Yu&nbsp;Chen, Xiaofei Xie, Jingkai Siow, and Yang Liu. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for code summarization via hybrid gnn.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.05405</em>.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yi&nbsp;Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu&nbsp;Sun. 2023.

</span>
<span class="ltx_bibblock">Recall: A benchmark for llms robustness against external counterfactual knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.08147</em>.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Yuxing Lu, Xiaohong Liu, Zongxin Du, Yuanxu Gao, and Guangyu Wang. 2023a.

</span>
<span class="ltx_bibblock">Medkpl: a heterogeneous knowledge enhanced prompt learning framework for transferable diagnosis.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Journal of Biomedical Informatics</em>, 143:104417.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Yuxing Lu, Xukai Zhao, and Jinzhuo Wang. 2023b.

</span>
<span class="ltx_bibblock">Medical knowledge-enhanced prompt learning for diagnosis classification from clinical text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Clinical Natural Language Processing Workshop</em>, pages 278–288.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass. 2023a.

</span>
<span class="ltx_bibblock">Sail: Search-augmented instruction learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15225</em>.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Ziyang Luo, Can Xu, Pu&nbsp;Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023b.

</span>
<span class="ltx_bibblock">Augmented large language models with parametric knowledge guiding.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.04757</em>.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo&nbsp;Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. 2024.

</span>
<span class="ltx_bibblock">Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.17043</em>.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022.

</span>
<span class="ltx_bibblock">Memory-assisted prompt editing to improve gpt-3 after deployment.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.06009</em>.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10511</em>.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020.

</span>
<span class="ltx_bibblock">Generation-augmented retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.08553</em>.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mousavi et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Seyed&nbsp;Mahed Mousavi, Simone Alghisi, and Giuseppe Riccardi. 2024.

</span>
<span class="ltx_bibblock">Is your llm outdated? benchmarking llms &amp; alignment algorithms for time-sensitive knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.08700</em>.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et&nbsp;al. 2021.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback, 2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">URL https://arxiv. org/abs/2112.09332</em>.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nam et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024.

</span>
<span class="ltx_bibblock">Using an llm to help with code understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/ACM 46th International Conference on Software Engineering</em>, pages 1–13.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo&nbsp;Hernández Ábrego, Ji&nbsp;Ma, Vincent&nbsp;Y Zhao, Yi&nbsp;Luan, Keith&nbsp;B Hall, Ming-Wei Chang, et&nbsp;al. 2021.

</span>
<span class="ltx_bibblock">Large dual encoders are generalizable retrievers.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.07899</em>.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parvez et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Md&nbsp;Rizwan Parvez, Wasi&nbsp;Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021.

</span>
<span class="ltx_bibblock">Retrieval augmented code generation and summarization.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.11601</em>.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu&nbsp;Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Check your facts and try again: Improving large language models with external knowledge and automated feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.12813</em>.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et&nbsp;al. 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et&nbsp;al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Journal of machine learning research</em>, 21(140):1–67.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:1316–1331.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2021.

</span>
<span class="ltx_bibblock">Learning to retrieve passages without supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.07708</em>.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramachandran et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Prajit Ramachandran, Barret Zoph, and Quoc&nbsp;V Le. 2017.

</span>
<span class="ltx_bibblock">Searching for activation functions.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.05941</em>.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.06125</em>, 1(2):3.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramos et&nbsp;al. (2003)</span>
<span class="ltx_bibblock">
Juan Ramos et&nbsp;al. 2003.

</span>
<span class="ltx_bibblock">Using tf-idf to determine word relevance in document queries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Proceedings of the first instructional conference on machine learning</em>, volume 242, pages 29–48. Citeseer.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et&nbsp;al. (1995)</span>
<span class="ltx_bibblock">
Stephen&nbsp;E Robertson, Steve Walker, Susan Jones, Micheline&nbsp;M Hancock-Beaulieu, Mike Gatford, et&nbsp;al. 1995.

</span>
<span class="ltx_bibblock">Okapi at trec-3.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Nist Special Publication Sp</em>, 109:109.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romera-Paredes et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M&nbsp;Pawan Kumar, Emilien Dupont, Francisco&nbsp;JR Ruiz, Jordan&nbsp;S Ellenberg, Pengming Wang, Omar Fawzi, et&nbsp;al. 2024.

</span>
<span class="ltx_bibblock">Mathematical discoveries from program search with large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Nature</em>, 625(7995):468–475.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rony et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Md&nbsp;Rashad Al&nbsp;Hasan Rony, Ricardo Usbeck, and Jens Lehmann. 2022.

</span>
<span class="ltx_bibblock">Dialokg: Knowledge-structure aware task-oriented dialogue generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.09149</em>.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023.

</span>
<span class="ltx_bibblock">Ares: An automated evaluation framework for retrieval-augmented generation systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.09476</em>.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachan et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Devendra&nbsp;Singh Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William&nbsp;L Hamilton, and Bryan Catanzaro. 2021.

</span>
<span class="ltx_bibblock">End-to-end training of neural retrievers for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00408</em>.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.

</span>
<span class="ltx_bibblock">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.01108</em>.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santhanam et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021.

</span>
<span class="ltx_bibblock">Colbertv2: Effective and efficient retrieval via lightweight late interaction.

</span>
<span class="ltx_bibblock"><em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.01488</em>.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxena et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. 2022.

</span>
<span class="ltx_bibblock">Sequence-to-sequence knowledge graph completion and question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.10321</em>.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Serra et&nbsp;al. (2013)</span>
<span class="ltx_bibblock">
Ivo Serra, Rosario Girardi, and Paulo Novais. 2013.

</span>
<span class="ltx_bibblock">Parnt: a statistic based approach to extract non-taxonomic relationships of ontologies from text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">2013 10th International Conference on Information Technology: New Generations</em>, pages 561–566. IEEE.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock">Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.

</span>
<span class="ltx_bibblock"><em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15294</em>.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)</span>
<span class="ltx_bibblock">
Noam Shazeer. 2020.

</span>
<span class="ltx_bibblock">Glu variants improve transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.05202</em>.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheynin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman. 2022.

</span>
<span class="ltx_bibblock">Knn-diffusion: Image generation via large-scale retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02849</em>.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun. 2022.

</span>
<span class="ltx_bibblock">Race: Retrieval-augmented commit message generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.02700</em>.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed&nbsp;H Chi, Nathanael Schärli, and Denny Zhou. 2023a.

</span>
<span class="ltx_bibblock">Large language models can be easily distracted by irrelevant context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 31210–31227. PMLR.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023b.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12652</em>.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. 2021.

</span>
<span class="ltx_bibblock">End-to-end training of multi-document reader and retriever for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34:25968–25981.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siriwardhana et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023.

</span>
<span class="ltx_bibblock">Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:1–17.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et&nbsp;al. (2014)</span>
<span class="ltx_bibblock">
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014.

</span>
<span class="ltx_bibblock">Dropout: a simple way to prevent neural networks from overfitting.

</span>
<span class="ltx_bibblock"><em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">The journal of machine learning research</em>, 15(1):1929–1958.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha Ahmed, Yu&nbsp;Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, 568:127063.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor (1953)</span>
<span class="ltx_bibblock">
Wilson&nbsp;L Taylor. 1953.

</span>
<span class="ltx_bibblock">“cloze procedure”: A new tool for measuring readability.

</span>
<span class="ltx_bibblock"><em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">Journalism quarterly</em>, 30(4):415–433.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018.

</span>
<span class="ltx_bibblock">Fever: a large-scale dataset for fact extraction and verification.

</span>
<span class="ltx_bibblock"><em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05355</em>.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thulke et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
David Thulke, Nico Daheim, Christian Dugast, and Hermann Ney. 2021.

</span>
<span class="ltx_bibblock">Efficient retrieval augmented generation from unstructured knowledge for task-oriented dialog.

</span>
<span class="ltx_bibblock"><em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.04643</em>.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent et&nbsp;al. (2008)</span>
<span class="ltx_bibblock">
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008.

</span>
<span class="ltx_bibblock">Extracting and composing robust features with denoising autoencoders.

</span>
<span class="ltx_bibblock">In <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th international conference on Machine learning</em>, pages 1096–1103.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo&nbsp;Li, Mohammad Shoeybi, and Bryan Catanzaro. 2023a.

</span>
<span class="ltx_bibblock">Instructretro: Instruction tuning post retrieval-augmented pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.07713</em>.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, and Furu Wei. 2023b.

</span>
<span class="ltx_bibblock">Learning to retrieve in-context examples for large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.07164</em>.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md&nbsp;Rizwan Parvez, and Graham Neubig. 2023c.

</span>
<span class="ltx_bibblock">Learning to filter context for retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.08377</em>.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
BigScience Workshop, Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni, François Yvon, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em>.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024.

</span>
<span class="ltx_bibblock">Benchmarking retrieval-augmented generation for medicine.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.13178</em>.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.

</span>
<span class="ltx_bibblock">Recomp: Improving retrieval-augmented lms with compression and selective augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.04408</em>.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024.

</span>
<span class="ltx_bibblock">Corrective retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.15884</em>.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. 2023.

</span>
<span class="ltx_bibblock">Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter.

</span>
<span class="ltx_bibblock"><em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.18347</em>.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William&nbsp;W Cohen, Ruslan Salakhutdinov, and Christopher&nbsp;D Manning. 2018.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.09600</em>.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao and Guan (2018)</span>
<span class="ltx_bibblock">
Lirong Yao and Yazhuo Guan. 2018.

</span>
<span class="ltx_bibblock">An improved lstm structure for natural language processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference of safety produce informatization (IICSPI)</em>, pages 565–569. IEEE.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2022.

</span>
<span class="ltx_bibblock">Retrieval-augmented multimodal language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.12561</em>.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Haoran Ye, Jiarui Wang, Zhiguang Cao, and Guojie Song. 2024.

</span>
<span class="ltx_bibblock">Reevo: Large language models as hyper-heuristics with reflective evolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.01145</em>.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. 2019.

</span>
<span class="ltx_bibblock">Memory augmented deep recurrent neural network for video question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>, 31(9):3159–3167.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Wenpeng Yin, Katharina Kann, Mo&nbsp;Yu, and Hinrich Schütze. 2017.

</span>
<span class="ltx_bibblock">Comparative study of cnn and rnn for natural language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1702.01923</em>.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yogatama et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Dani Yogatama, Cyprien de&nbsp;Masson&nbsp;d’Autume, and Lingpeng Kong. 2021.

</span>
<span class="ltx_bibblock">Adaptive semiparametric language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 9:362–373.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023.

</span>
<span class="ltx_bibblock">Making retrieval-augmented language models robust to irrelevant context.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01558</em>.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu and Yang (2023)</span>
<span class="ltx_bibblock">
Donghan Yu and Yiming Yang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-enhanced generative model for large-scale knowledge graph completion.

</span>
<span class="ltx_bibblock">In <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pages 2334–2338.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023.

</span>
<span class="ltx_bibblock">Improving language models via plug-and-play retrieval feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14002</em>.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Yi&nbsp;Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark&nbsp;D Plumbley, and Wenwu Wang. 2024.

</span>
<span class="ltx_bibblock">Retrieval-augmented text-to-audio generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 581–585. IEEE.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuksekgonul et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. 2023.

</span>
<span class="ltx_bibblock">Attention satisfies: A constraint-satisfaction lens on factual errors of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15098</em>.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2022.

</span>
<span class="ltx_bibblock">When language model meets private library.

</span>
<span class="ltx_bibblock"><em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.17236</em>.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Sennrich (2019)</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich. 2019.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 32.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01068</em>.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024a)</span>
<span class="ltx_bibblock">
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024a.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for ai-generated content: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.19473</em>.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan&nbsp;Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Retrieving multimodal information for augmented generation: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.10868</em>.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024b)</span>
<span class="ltx_bibblock">
Weichen Zhao, Yuxing Lu, Ge&nbsp;Jiao, and Yuan Yang. 2024b.

</span>
<span class="ltx_bibblock">Concentrated reasoning and unified reconstruction for multi-modal media manipulation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 8190–8194. IEEE.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024c)</span>
<span class="ltx_bibblock">
Weichen Zhao, Yuxing Lu, Ge&nbsp;Jiao, and Yuan Yang. 2024c.

</span>
<span class="ltx_bibblock">Dual-color granularity alignment for text-based person search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 8075–8079. IEEE.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zexuan Zhong, Tao Lei, and Danqi Chen. 2022.

</span>
<span class="ltx_bibblock">Training language models with memory augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.12674</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2404.19542" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2404.19543" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.19543">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.19543" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2404.19544" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 19:04:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
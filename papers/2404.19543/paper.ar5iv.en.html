<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.19543] RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing</title><meta property="og:description" content="Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate theâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.19543">

<!--Generated on Sun May  5 19:04:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yucheng Hu 
<br class="ltx_break">East China University of Science and Technology 
<br class="ltx_break"><span id="id2.2.id1" class="ltx_text ltx_font_typewriter">huyc@mail.ecust.edu.cn</span> 
<br class="ltx_break"><span id="id3.3.id2" class="ltx_ERROR undefined">\And</span>Yuxing Lu<sup id="id4.4.id3" class="ltx_sup">âˆ—</sup> 
<br class="ltx_break">Peking University 
<br class="ltx_break"><span id="id5.5.id4" class="ltx_text ltx_font_typewriter">yxlu0613@gmail.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: <a target="_blank" href="https://github.com/2471023025/RALM_Survey" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/2471023025/RALM_Survey</a>.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.2" class="ltx_p"><span id="p1.1.2.1" class="ltx_text ltx_font_bold">RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.1" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.1.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.1.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.1.2.1.1.1" class="ltx_tr">
<span id="p1.1.1.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Yucheng Hu</span></span></span>
<span id="p1.1.1.2.1.2.2" class="ltx_tr">
<span id="p1.1.1.2.1.2.2.1" class="ltx_td ltx_align_center">East China University of Science and Technology</span></span>
<span id="p1.1.1.2.1.3.3" class="ltx_tr">
<span id="p1.1.1.2.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.1.2.1.3.3.1.1" class="ltx_text ltx_font_typewriter">huyc@mail.ecust.edu.cn</span></span></span>
</span>
</span></span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="p1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.1.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.1.1.1.1" class="ltx_tr">
<span id="p1.1.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Yuxing Lu<sup id="p1.1.1.1.1.1.1.1.1" class="ltx_sup"><span id="p1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium">âˆ—</span></sup></span></span></span>
<span id="p1.1.1.1.1.2.1" class="ltx_tr">
<span id="p1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center">Peking University</span></span>
<span id="p1.1.1.1.1.3.2" class="ltx_tr">
<span id="p1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="p1.1.1.1.1.3.2.1.1" class="ltx_text ltx_font_typewriter">yxlu0613@gmail.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.19543/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="418" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A general overview of this surveyâ€˜s work</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Natural Language Processing (NLP) is a significant focus within the realms of computer science and artificial intelligence, dedicated to the study of theoretical and methodological frameworks that enable effective communication between humans and computers using natural language. As a multidisciplinary field, NLP integrates linguistics, computer science, and mathematics with the aim of realizing the mutual transformation between human language and computer data. Its ultimate objective is to empower computers with the capability to process and â€understandâ€ natural language, thereby facilitating tasks such as automatic translation, text categorization, and sentiment analysis. The complexity of NLP is evident in the numerous steps it encompasses, including word segmentation, part-of-speech tagging, parsing, stemming, named entity recognition, and more, all of which contribute to the challenge of replicating human language understanding in artificial intelligence systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Traditional natural language processing tasks typically employ statistic-based algorithms <cite class="ltx_cite ltx_citemacro_cite">Hogenboom et&nbsp;al. (<a href="#bib.bib46" title="" class="ltx_ref">2010</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Serra et&nbsp;al. (<a href="#bib.bib111" title="" class="ltx_ref">2013</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Aussenac-Gilles and SÃ¶rgel (<a href="#bib.bib5" title="" class="ltx_ref">2005</a>)</cite> and deep learning algorithms such as convolutional neural network (CNN) <cite class="ltx_cite ltx_citemacro_cite">Yin et&nbsp;al. (<a href="#bib.bib142" title="" class="ltx_ref">2017</a>)</cite>, recurrent neural network (RNN) <cite class="ltx_cite ltx_citemacro_cite">Banerjee et&nbsp;al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>, long short-term memory network (LSTM) <cite class="ltx_cite ltx_citemacro_cite">Yao and Guan (<a href="#bib.bib138" title="" class="ltx_ref">2018</a>)</cite>, and others. Recently, with the advent of the transformer architecture <cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a href="#bib.bib127" title="" class="ltx_ref">2017</a>)</cite> as a leading representative of natural language processing, its popularity has grown significantly. The transformer architecture, as a prominent large language model <cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Raffel et&nbsp;al. (<a href="#bib.bib96" title="" class="ltx_ref">2020</a>)</cite> in the natural language processing domain, has consistently demonstrated enhanced performance, attracting the attention of an increasing number of researchers who are engaged in studying its capabilities.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The most prevalent LMs nowadays are the GPT families <cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a href="#bib.bib95" title="" class="ltx_ref">2019</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Achiam et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> and Bert families <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib79" title="" class="ltx_ref">2019</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Devlin et&nbsp;al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Sanh et&nbsp;al. (<a href="#bib.bib107" title="" class="ltx_ref">2019</a>)</cite>, which have been demonstrated to excel in a multitude of natural language processing tasks. Among these, the AutoEncoder language model is particularly adept at natural language understanding tasks, while the AutoRegressive language model is more suited to natural language generation tasks. While increasing parameters <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib126" title="" class="ltx_ref">2023b</a>)</cite> and model tuning <cite class="ltx_cite ltx_citemacro_cite">Han et&nbsp;al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> can enhance the performance of LLMs, the phenomenon of â€hallucinationâ€ <cite class="ltx_cite ltx_citemacro_cite">Ji et&nbsp;al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite> persists. Furthermore, the limitations of LMs in effectively handling knowledge-intensive work <cite class="ltx_cite ltx_citemacro_cite">Feng et&nbsp;al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite> and their inability to promptly update their knowledge <cite class="ltx_cite ltx_citemacro_cite">Mousavi et&nbsp;al. (<a href="#bib.bib88" title="" class="ltx_ref">2024</a>)</cite> are consistently apparent. Consequently, numerous researchers <cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib53" title="" class="ltx_ref">2020b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Khandelwal et&nbsp;al. (<a href="#bib.bib63" title="" class="ltx_ref">2019</a>)</cite> have employed the technique of retrieval to obtain external knowledge, which can assist the language model in attaining enhanced performance in a multitude of tasks.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Currently, there is a paucity of surveys on the use of retrieval augmentation to enhance the performance of LLMs. <cite class="ltx_cite ltx_citemacro_citet">Zhao et&nbsp;al. (<a href="#bib.bib153" title="" class="ltx_ref">2023</a>)</cite> provide a comprehensive overview of work on RAG for multimodality. <cite class="ltx_cite ltx_citemacro_citet">Zhao et&nbsp;al. (<a href="#bib.bib152" title="" class="ltx_ref">2024a</a>)</cite> concentrate on the utilisation of retrieval augmentation generation techniques for the Artificial Intelligence Generated Content (AIGC) domain. This article provides a comprehensive overview of recent RAG work, but it does not cover all relevant domains. Additionally, the article lacks sufficient detail to provide a comprehensive timeline of the overall development. <cite class="ltx_cite ltx_citemacro_citet">Gao et&nbsp;al. (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite> investigate the enhancement of RAG for large models. This article summarizes some of the recent RAG work, but it introduces the retrievers and generators independently, which is not conducive to the upgrading and interactions with the components of subsequent work. <cite class="ltx_cite ltx_citemacro_citet">Li et&nbsp;al. (<a href="#bib.bib73" title="" class="ltx_ref">2022b</a>)</cite> focus on text generation only. The article has fewer figures and tables, and the content is more abstract, which is not conducive to the readerâ€™s understanding.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Also, surveys on RAG only tells half of the story in retrieval-augmented methods in NLP. Not only do tasks associated with NLG require retrieval enhancement techniques, but NLU tasks also necessitate external information. To date, there is a scarcity of comprehensive surveys that thoroughly review the application of augmented retrieval techniques across the spectrum of NLP. In order to improve the current situation, this paper presents the following contributions:</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2404.19543/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="318" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Three different ways the Retriever interacts with the LM</figcaption>
</figure>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">(1) The article does not merely focus on the work related to RAG; it also places significant emphasis on RALM and aligns with the concept of NLP. The work related to generation aligns with NLG, while the rest of the work aligns with NLU.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">(2) The two components of RALM, the Retriever and the Language Model, are described in detail, and the different interaction modes of these two components are precisely defined for the first time.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">(3) A comprehensive overview of the RALM work schedule is provided, along with a summary of the common and novel applications of current RALM, accompanied by an analysis of the associated limitations. Potential solutions to these limitations are proposed, along with recommendations for future research directions.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides a general overview of the framework of RALM methods. The following is a summary of the paper:
Section <a href="#S2" title="2 Definition â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> defines RALM.
Section <a href="#S3" title="3 Retriever â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides a detailed classification and summary of the work of retrievers in RALM.
Section <a href="#S4" title="4 Language Models â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides a detailed classification and summary of the work of LMs in RALM.
Section <a href="#S5" title="5 RALM Enhancement â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> provides a classification and summary of specific enhancements to RALM.
Section <a href="#S6" title="6 Data Sources â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> of RALM is a classification and summary of the sources of retrieved data.
Section <a href="#S7" title="7 Applications â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> is a summary of RALM applications.
Section <a href="#S8" title="8 Evaluation â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> is a summary of RALM evaluations and benchmarks.
Finally, Section <a href="#S9" title="9 Disscussion â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> is a discussion of the limitations of existing RALM and directions for future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Definition</h2>

<figure id="S2.F3" class="ltx_figure"><img src="/html/2404.19543/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="373" height="483" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A roadmap of the three types of interactions. The purple areas represent work on Sequential Interaction RALM models, the red boxes signify work on Sequential Multiple Interactions RALMs models, and the yellow areas indicate work on Parallel Interaction RALM models.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.6" class="ltx_p">Retrieval-Augmented Language Model (RALM) is the process of refining the output of the LM with retrieved information to obtain a satisfactory result for the user. This section provides a detailed definition of the different modes of RALM by categorising the ways in which the retriever interacts with the language model. The specific categorization of interactions can be seen in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In addition, the development history of each interaction method can be seen in Figure <a href="#S2.F3" title="Figure 3 â€£ 2 Definition â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Assuming that <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">z</annotation></semantics></math> is the retrieved message, <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">x</annotation></semantics></math> is the input, <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">y</annotation></semantics></math> is the output, and <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="F()" display="inline"><semantics id="S2.p1.4.m4.1a"><mrow id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.1" xref="S2.p1.4.m4.1.1.1.cmml">â€‹</mo><mrow id="S2.p1.4.m4.1.1.3.2" xref="S2.p1.4.m4.1.1.cmml"><mo stretchy="false" id="S2.p1.4.m4.1.1.3.2.1" xref="S2.p1.4.m4.1.1.3.1.cmml">(</mo><mo stretchy="false" id="S2.p1.4.m4.1.1.3.2.2" xref="S2.p1.4.m4.1.1.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><times id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1"></times><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">ğ¹</ci><list id="S2.p1.4.m4.1.1.3.1.cmml" xref="S2.p1.4.m4.1.1.3.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">F()</annotation></semantics></math> is a function, either a language model or a data processing function, with <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">x</annotation></semantics></math> and <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.p1.6.m6.1a"><mi id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">z</annotation></semantics></math> as independent variables, the basic architecture of RALM is defined as follows:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="y=F(x,z)" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.3" xref="S2.E1.m1.2.3.cmml"><mi id="S2.E1.m1.2.3.2" xref="S2.E1.m1.2.3.2.cmml">y</mi><mo id="S2.E1.m1.2.3.1" xref="S2.E1.m1.2.3.1.cmml">=</mo><mrow id="S2.E1.m1.2.3.3" xref="S2.E1.m1.2.3.3.cmml"><mi id="S2.E1.m1.2.3.3.2" xref="S2.E1.m1.2.3.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.3.3.1" xref="S2.E1.m1.2.3.3.1.cmml">â€‹</mo><mrow id="S2.E1.m1.2.3.3.3.2" xref="S2.E1.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.3.3.3.2.1" xref="S2.E1.m1.2.3.3.3.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">x</mi><mo id="S2.E1.m1.2.3.3.3.2.2" xref="S2.E1.m1.2.3.3.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">z</mi><mo stretchy="false" id="S2.E1.m1.2.3.3.3.2.3" xref="S2.E1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.3.cmml" xref="S2.E1.m1.2.3"><eq id="S2.E1.m1.2.3.1.cmml" xref="S2.E1.m1.2.3.1"></eq><ci id="S2.E1.m1.2.3.2.cmml" xref="S2.E1.m1.2.3.2">ğ‘¦</ci><apply id="S2.E1.m1.2.3.3.cmml" xref="S2.E1.m1.2.3.3"><times id="S2.E1.m1.2.3.3.1.cmml" xref="S2.E1.m1.2.3.3.1"></times><ci id="S2.E1.m1.2.3.3.2.cmml" xref="S2.E1.m1.2.3.3.2">ğ¹</ci><interval closure="open" id="S2.E1.m1.2.3.3.3.1.cmml" xref="S2.E1.m1.2.3.3.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğ‘¥</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğ‘§</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">y=F(x,z)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Sequential Single Interaction</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.10" class="ltx_p">The sequential single interaction process involves finding the Top-K relevant documents <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">z</annotation></semantics></math> to input <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">x</annotation></semantics></math> through a retriever <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="P_{\eta}(z|x)" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><msub id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml"><mi id="S2.SS1.p1.3.m3.1.1.3.2" xref="S2.SS1.p1.3.m3.1.1.3.2.cmml">P</mi><mi id="S2.SS1.p1.3.m3.1.1.3.3" xref="S2.SS1.p1.3.m3.1.1.3.3.cmml">Î·</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p1.3.m3.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.3.m3.1.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.3.m3.1.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S2.SS1.p1.3.m3.1.1.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS1.p1.3.m3.1.1.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.SS1.p1.3.m3.1.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><times id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2"></times><apply id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2">ğ‘ƒ</ci><ci id="S2.SS1.p1.3.m3.1.1.3.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3">ğœ‚</ci></apply><apply id="S2.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p1.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.2">ğ‘§</ci><ci id="S2.SS1.p1.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.1.3">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">P_{\eta}(z|x)</annotation></semantics></math>, where <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\eta</annotation></semantics></math> is a parameter of the retriever. Subsequently, the language model <math id="S2.SS1.p1.5.m5.3" class="ltx_Math" alttext="P_{\theta}(y_{i}|x,z,y_{r})" display="inline"><semantics id="S2.SS1.p1.5.m5.3a"><mrow id="S2.SS1.p1.5.m5.3.3" xref="S2.SS1.p1.5.m5.3.3.cmml"><msub id="S2.SS1.p1.5.m5.3.3.3" xref="S2.SS1.p1.5.m5.3.3.3.cmml"><mi id="S2.SS1.p1.5.m5.3.3.3.2" xref="S2.SS1.p1.5.m5.3.3.3.2.cmml">P</mi><mi id="S2.SS1.p1.5.m5.3.3.3.3" xref="S2.SS1.p1.5.m5.3.3.3.3.cmml">Î¸</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.5.m5.3.3.2" xref="S2.SS1.p1.5.m5.3.3.2.cmml">â€‹</mo><mrow id="S2.SS1.p1.5.m5.3.3.1.1" xref="S2.SS1.p1.5.m5.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.5.m5.3.3.1.1.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.5.m5.3.3.1.1.1" xref="S2.SS1.p1.5.m5.3.3.1.1.1.cmml"><msub id="S2.SS1.p1.5.m5.3.3.1.1.1.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.cmml"><mi id="S2.SS1.p1.5.m5.3.3.1.1.1.3.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.2.cmml">y</mi><mi id="S2.SS1.p1.5.m5.3.3.1.1.1.3.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S2.SS1.p1.5.m5.3.3.1.1.1.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.2.cmml">|</mo><mrow id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.2.cmml"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">x</mi><mo id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.2.cmml">,</mo><mi id="S2.SS1.p1.5.m5.2.2" xref="S2.SS1.p1.5.m5.2.2.cmml">z</mi><mo id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.2.cmml">,</mo><msub id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.2" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.2.cmml">y</mi><mi id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.3.cmml">r</mi></msub></mrow></mrow><mo stretchy="false" id="S2.SS1.p1.5.m5.3.3.1.1.3" xref="S2.SS1.p1.5.m5.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.3b"><apply id="S2.SS1.p1.5.m5.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3"><times id="S2.SS1.p1.5.m5.3.3.2.cmml" xref="S2.SS1.p1.5.m5.3.3.2"></times><apply id="S2.SS1.p1.5.m5.3.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.3.3.3.1.cmml" xref="S2.SS1.p1.5.m5.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.5.m5.3.3.3.2.cmml" xref="S2.SS1.p1.5.m5.3.3.3.2">ğ‘ƒ</ci><ci id="S2.SS1.p1.5.m5.3.3.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3.3.3">ğœƒ</ci></apply><apply id="S2.SS1.p1.5.m5.3.3.1.1.1.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1"><csymbol cd="latexml" id="S2.SS1.p1.5.m5.3.3.1.1.1.2.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.2">conditional</csymbol><apply id="S2.SS1.p1.5.m5.3.3.1.1.1.3.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.3.3.1.1.1.3.1.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3">subscript</csymbol><ci id="S2.SS1.p1.5.m5.3.3.1.1.1.3.2.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.2">ğ‘¦</ci><ci id="S2.SS1.p1.5.m5.3.3.1.1.1.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.3.3">ğ‘–</ci></apply><list id="S2.SS1.p1.5.m5.3.3.1.1.1.1.2.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">ğ‘¥</ci><ci id="S2.SS1.p1.5.m5.2.2.cmml" xref="S2.SS1.p1.5.m5.2.2">ğ‘§</ci><apply id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.5.m5.3.3.1.1.1.1.1.1.3">ğ‘Ÿ</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.3c">P_{\theta}(y_{i}|x,z,y_{r})</annotation></semantics></math> receives input <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">x</annotation></semantics></math> along with relevant documents <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mi id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><ci id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">z</annotation></semantics></math> and outputs the i-th token <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">y</mi><mi id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">ğ‘¦</ci><ci id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">y_{i}</annotation></semantics></math>. Parameter <math id="S2.SS1.p1.9.m9.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><mi id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><ci id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">\theta</annotation></semantics></math> is used, along with relevant output tokens <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="y_{r}" display="inline"><semantics id="S2.SS1.p1.10.m10.1a"><msub id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml"><mi id="S2.SS1.p1.10.m10.1.1.2" xref="S2.SS1.p1.10.m10.1.1.2.cmml">y</mi><mi id="S2.SS1.p1.10.m10.1.1.3" xref="S2.SS1.p1.10.m10.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><apply id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.1.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S2.SS1.p1.10.m10.1.1.2.cmml" xref="S2.SS1.p1.10.m10.1.1.2">ğ‘¦</ci><ci id="S2.SS1.p1.10.m10.1.1.3.cmml" xref="S2.SS1.p1.10.m10.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">y_{r}</annotation></semantics></math>. The number of relevant output tokens is related to the location and type of language model. The RALM for sequential single interaction is defined as follows:</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.3" class="ltx_Math" alttext="y_{i}=LM(z,x,y_{r})" display="block"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml"><msub id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><mi id="S2.E2.m1.3.3.3.2" xref="S2.E2.m1.3.3.3.2.cmml">y</mi><mi id="S2.E2.m1.3.3.3.3" xref="S2.E2.m1.3.3.3.3.cmml">i</mi></msub><mo id="S2.E2.m1.3.3.2" xref="S2.E2.m1.3.3.2.cmml">=</mo><mrow id="S2.E2.m1.3.3.1" xref="S2.E2.m1.3.3.1.cmml"><mi id="S2.E2.m1.3.3.1.3" xref="S2.E2.m1.3.3.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.2" xref="S2.E2.m1.3.3.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.3.3.1.4" xref="S2.E2.m1.3.3.1.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.1.2a" xref="S2.E2.m1.3.3.1.2.cmml">â€‹</mo><mrow id="S2.E2.m1.3.3.1.1.1" xref="S2.E2.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.2" xref="S2.E2.m1.3.3.1.1.2.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">z</mi><mo id="S2.E2.m1.3.3.1.1.1.3" xref="S2.E2.m1.3.3.1.1.2.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">x</mi><mo id="S2.E2.m1.3.3.1.1.1.4" xref="S2.E2.m1.3.3.1.1.2.cmml">,</mo><msub id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml">y</mi><mi id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml">r</mi></msub><mo stretchy="false" id="S2.E2.m1.3.3.1.1.1.5" xref="S2.E2.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3"><eq id="S2.E2.m1.3.3.2.cmml" xref="S2.E2.m1.3.3.2"></eq><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3">subscript</csymbol><ci id="S2.E2.m1.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.2">ğ‘¦</ci><ci id="S2.E2.m1.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3">ğ‘–</ci></apply><apply id="S2.E2.m1.3.3.1.cmml" xref="S2.E2.m1.3.3.1"><times id="S2.E2.m1.3.3.1.2.cmml" xref="S2.E2.m1.3.3.1.2"></times><ci id="S2.E2.m1.3.3.1.3.cmml" xref="S2.E2.m1.3.3.1.3">ğ¿</ci><ci id="S2.E2.m1.3.3.1.4.cmml" xref="S2.E2.m1.3.3.1.4">ğ‘€</ci><vector id="S2.E2.m1.3.3.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ğ‘§</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ğ‘¥</ci><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2">ğ‘¦</ci><ci id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3">ğ‘Ÿ</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">y_{i}=LM(z,x,y_{r})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.11" class="ltx_p">When RALM was first proposed, many researchers used this method because it aligned with their original ideas, particularly those of <cite class="ltx_cite ltx_citemacro_citet">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Guu et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Izacard and Grave (<a href="#bib.bib53" title="" class="ltx_ref">2020b</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Sequential Multiple Interactions</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As RALM technology develops, researchers have discovered that a single interaction is insufficient for long dialogue generation and solving multi-hop problems. Therefore, a method with multiple interactions between a retriever and a language model has been proposed. In this method, the researcher includes step s and typically has the language model generate the output first. When a retrieval technique is necessary, the outputted content is used for retrieval and the relevant formulas are expressed as follows:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.2" class="ltx_Math" alttext="y_{s}=LM(z,x|y_{<s})" display="block"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml"><msub id="S2.E3.m1.2.2.3" xref="S2.E3.m1.2.2.3.cmml"><mi id="S2.E3.m1.2.2.3.2" xref="S2.E3.m1.2.2.3.2.cmml">y</mi><mi id="S2.E3.m1.2.2.3.3" xref="S2.E3.m1.2.2.3.3.cmml">s</mi></msub><mo id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.2.cmml">=</mo><mrow id="S2.E3.m1.2.2.1" xref="S2.E3.m1.2.2.1.cmml"><mi id="S2.E3.m1.2.2.1.3" xref="S2.E3.m1.2.2.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.2" xref="S2.E3.m1.2.2.1.2.cmml">â€‹</mo><mi id="S2.E3.m1.2.2.1.4" xref="S2.E3.m1.2.2.1.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.2.2.1.2a" xref="S2.E3.m1.2.2.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.2.2.1.1.1" xref="S2.E3.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.1.1.1.2" xref="S2.E3.m1.2.2.1.1.2.cmml">(</mo><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">z</mi><mo id="S2.E3.m1.2.2.1.1.1.3" xref="S2.E3.m1.2.2.1.1.2.cmml">,</mo><mrow id="S2.E3.m1.2.2.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S2.E3.m1.2.2.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml">|</mo><msub id="S2.E3.m1.2.2.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.3.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.3.2" xref="S2.E3.m1.2.2.1.1.1.1.3.2.cmml">y</mi><mrow id="S2.E3.m1.2.2.1.1.1.1.3.3" xref="S2.E3.m1.2.2.1.1.1.1.3.3.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.3.3.2" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2.cmml"></mi><mo id="S2.E3.m1.2.2.1.1.1.1.3.3.1" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S2.E3.m1.2.2.1.1.1.1.3.3.3" xref="S2.E3.m1.2.2.1.1.1.1.3.3.3.cmml">s</mi></mrow></msub></mrow><mo stretchy="false" id="S2.E3.m1.2.2.1.1.1.4" xref="S2.E3.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2"><eq id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2"></eq><apply id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.3.1.cmml" xref="S2.E3.m1.2.2.3">subscript</csymbol><ci id="S2.E3.m1.2.2.3.2.cmml" xref="S2.E3.m1.2.2.3.2">ğ‘¦</ci><ci id="S2.E3.m1.2.2.3.3.cmml" xref="S2.E3.m1.2.2.3.3">ğ‘ </ci></apply><apply id="S2.E3.m1.2.2.1.cmml" xref="S2.E3.m1.2.2.1"><times id="S2.E3.m1.2.2.1.2.cmml" xref="S2.E3.m1.2.2.1.2"></times><ci id="S2.E3.m1.2.2.1.3.cmml" xref="S2.E3.m1.2.2.1.3">ğ¿</ci><ci id="S2.E3.m1.2.2.1.4.cmml" xref="S2.E3.m1.2.2.1.4">ğ‘€</ci><interval closure="open" id="S2.E3.m1.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1"><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">ğ‘§</ci><apply id="S2.E3.m1.2.2.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S2.E3.m1.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1">conditional</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.2">ğ‘¥</ci><apply id="S2.E3.m1.2.2.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.2">ğ‘¦</ci><apply id="S2.E3.m1.2.2.1.1.1.1.3.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3"><lt id="S2.E3.m1.2.2.1.1.1.1.3.3.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S2.E3.m1.2.2.1.1.1.1.3.3.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.2">absent</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.3.3.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.3.3.3">ğ‘ </ci></apply></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">y_{s}=LM(z,x|y_{&lt;s})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p2.2" class="ltx_p">where <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="y_{s}" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><msub id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mi id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">y</mi><mi id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">ğ‘¦</ci><ci id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">y_{s}</annotation></semantics></math> represents the generated tokens at the current step <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">s</annotation></semantics></math>. Among the researchers who have employed this method, the most renowned are <cite class="ltx_cite ltx_citemacro_citet">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">He et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Parallel Interaction</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.3" class="ltx_p">In all of the previously mentioned approaches, the flow of information has a clear sequential structure, whether from the retriever to the language model or from the language model to the retriever. However, this sequential structure may not be optimal in all domains and may be less extensible, it is important to consider alternative approaches.
Researchers have proposed a novel parallel structure in which the retriever and the language model work independently for the user input <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">x</annotation></semantics></math>. The output <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">y</annotation></semantics></math> is then determined by weighted interpolation. <math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="I()" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><mrow id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml"><mi id="S2.SS3.p1.3.m3.1.1.2" xref="S2.SS3.p1.3.m3.1.1.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.3.m3.1.1.1" xref="S2.SS3.p1.3.m3.1.1.1.cmml">â€‹</mo><mrow id="S2.SS3.p1.3.m3.1.1.3.2" xref="S2.SS3.p1.3.m3.1.1.cmml"><mo stretchy="false" id="S2.SS3.p1.3.m3.1.1.3.2.1" xref="S2.SS3.p1.3.m3.1.1.3.1.cmml">(</mo><mo stretchy="false" id="S2.SS3.p1.3.m3.1.1.3.2.2" xref="S2.SS3.p1.3.m3.1.1.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1"><times id="S2.SS3.p1.3.m3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1.1"></times><ci id="S2.SS3.p1.3.m3.1.1.2.cmml" xref="S2.SS3.p1.3.m3.1.1.2">ğ¼</ci><list id="S2.SS3.p1.3.m3.1.1.3.1.cmml" xref="S2.SS3.p1.3.m3.1.1.3.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">I()</annotation></semantics></math> is the interpolation function. The relevant equations are expressed as follows:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.3" class="ltx_Math" alttext="y=I(LM(x,y_{r}),z)" display="block"><semantics id="S2.E4.m1.3a"><mrow id="S2.E4.m1.3.3" xref="S2.E4.m1.3.3.cmml"><mi id="S2.E4.m1.3.3.3" xref="S2.E4.m1.3.3.3.cmml">y</mi><mo id="S2.E4.m1.3.3.2" xref="S2.E4.m1.3.3.2.cmml">=</mo><mrow id="S2.E4.m1.3.3.1" xref="S2.E4.m1.3.3.1.cmml"><mi id="S2.E4.m1.3.3.1.3" xref="S2.E4.m1.3.3.1.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.2" xref="S2.E4.m1.3.3.1.2.cmml">â€‹</mo><mrow id="S2.E4.m1.3.3.1.1.1" xref="S2.E4.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.2" xref="S2.E4.m1.3.3.1.1.2.cmml">(</mo><mrow id="S2.E4.m1.3.3.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E4.m1.3.3.1.1.1.1.4" xref="S2.E4.m1.3.3.1.1.1.1.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.1.1.1.2a" xref="S2.E4.m1.3.3.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E4.m1.3.3.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">x</mi><mo id="S2.E4.m1.3.3.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">,</mo><msub id="S2.E4.m1.3.3.1.1.1.1.1.1.1" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S2.E4.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml">r</mi></msub><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.1.1.1.4" xref="S2.E4.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.3.3.1.1.1.3" xref="S2.E4.m1.3.3.1.1.2.cmml">,</mo><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">z</mi><mo stretchy="false" id="S2.E4.m1.3.3.1.1.1.4" xref="S2.E4.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.3b"><apply id="S2.E4.m1.3.3.cmml" xref="S2.E4.m1.3.3"><eq id="S2.E4.m1.3.3.2.cmml" xref="S2.E4.m1.3.3.2"></eq><ci id="S2.E4.m1.3.3.3.cmml" xref="S2.E4.m1.3.3.3">ğ‘¦</ci><apply id="S2.E4.m1.3.3.1.cmml" xref="S2.E4.m1.3.3.1"><times id="S2.E4.m1.3.3.1.2.cmml" xref="S2.E4.m1.3.3.1.2"></times><ci id="S2.E4.m1.3.3.1.3.cmml" xref="S2.E4.m1.3.3.1.3">ğ¼</ci><interval closure="open" id="S2.E4.m1.3.3.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1"><apply id="S2.E4.m1.3.3.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1"><times id="S2.E4.m1.3.3.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.2"></times><ci id="S2.E4.m1.3.3.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.3">ğ¿</ci><ci id="S2.E4.m1.3.3.1.1.1.1.4.cmml" xref="S2.E4.m1.3.3.1.1.1.1.4">ğ‘€</ci><interval closure="open" id="S2.E4.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1"><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">ğ‘¥</ci><apply id="S2.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S2.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.3.3.1.1.1.1.1.1.1.3">ğ‘Ÿ</ci></apply></interval></apply><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">ğ‘§</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.3c">y=I(LM(x,y_{r}),z)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS3.p1.7" class="ltx_p">the specific interpolation function is:</p>
<table id="S2.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E5.m1.4" class="ltx_Math" alttext="p(y|x)=\lambda p_{R}(y|x)+(1-\lambda)p_{LM}(y|x)" display="block"><semantics id="S2.E5.m1.4a"><mrow id="S2.E5.m1.4.4" xref="S2.E5.m1.4.4.cmml"><mrow id="S2.E5.m1.1.1.1" xref="S2.E5.m1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.3" xref="S2.E5.m1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.1.1.1.2" xref="S2.E5.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E5.m1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.cmml"><mi id="S2.E5.m1.1.1.1.1.1.1.2" xref="S2.E5.m1.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S2.E5.m1.1.1.1.1.1.1.1" xref="S2.E5.m1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.E5.m1.1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.E5.m1.1.1.1.1.1.3" xref="S2.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.4.4.5" xref="S2.E5.m1.4.4.5.cmml">=</mo><mrow id="S2.E5.m1.4.4.4" xref="S2.E5.m1.4.4.4.cmml"><mrow id="S2.E5.m1.2.2.2.1" xref="S2.E5.m1.2.2.2.1.cmml"><mi id="S2.E5.m1.2.2.2.1.3" xref="S2.E5.m1.2.2.2.1.3.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.1.2" xref="S2.E5.m1.2.2.2.1.2.cmml">â€‹</mo><msub id="S2.E5.m1.2.2.2.1.4" xref="S2.E5.m1.2.2.2.1.4.cmml"><mi id="S2.E5.m1.2.2.2.1.4.2" xref="S2.E5.m1.2.2.2.1.4.2.cmml">p</mi><mi id="S2.E5.m1.2.2.2.1.4.3" xref="S2.E5.m1.2.2.2.1.4.3.cmml">R</mi></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.2.2.1.2a" xref="S2.E5.m1.2.2.2.1.2.cmml">â€‹</mo><mrow id="S2.E5.m1.2.2.2.1.1.1" xref="S2.E5.m1.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.2.2.2.1.1.1.2" xref="S2.E5.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.2.2.2.1.1.1.1" xref="S2.E5.m1.2.2.2.1.1.1.1.cmml"><mi id="S2.E5.m1.2.2.2.1.1.1.1.2" xref="S2.E5.m1.2.2.2.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S2.E5.m1.2.2.2.1.1.1.1.1" xref="S2.E5.m1.2.2.2.1.1.1.1.1.cmml">|</mo><mi id="S2.E5.m1.2.2.2.1.1.1.1.3" xref="S2.E5.m1.2.2.2.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.E5.m1.2.2.2.1.1.1.3" xref="S2.E5.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E5.m1.4.4.4.4" xref="S2.E5.m1.4.4.4.4.cmml">+</mo><mrow id="S2.E5.m1.4.4.4.3" xref="S2.E5.m1.4.4.4.3.cmml"><mrow id="S2.E5.m1.3.3.3.2.1.1" xref="S2.E5.m1.3.3.3.2.1.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.3.3.3.2.1.1.2" xref="S2.E5.m1.3.3.3.2.1.1.1.cmml">(</mo><mrow id="S2.E5.m1.3.3.3.2.1.1.1" xref="S2.E5.m1.3.3.3.2.1.1.1.cmml"><mn id="S2.E5.m1.3.3.3.2.1.1.1.2" xref="S2.E5.m1.3.3.3.2.1.1.1.2.cmml">1</mn><mo id="S2.E5.m1.3.3.3.2.1.1.1.1" xref="S2.E5.m1.3.3.3.2.1.1.1.1.cmml">âˆ’</mo><mi id="S2.E5.m1.3.3.3.2.1.1.1.3" xref="S2.E5.m1.3.3.3.2.1.1.1.3.cmml">Î»</mi></mrow><mo stretchy="false" id="S2.E5.m1.3.3.3.2.1.1.3" xref="S2.E5.m1.3.3.3.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E5.m1.4.4.4.3.3" xref="S2.E5.m1.4.4.4.3.3.cmml">â€‹</mo><msub id="S2.E5.m1.4.4.4.3.4" xref="S2.E5.m1.4.4.4.3.4.cmml"><mi id="S2.E5.m1.4.4.4.3.4.2" xref="S2.E5.m1.4.4.4.3.4.2.cmml">p</mi><mrow id="S2.E5.m1.4.4.4.3.4.3" xref="S2.E5.m1.4.4.4.3.4.3.cmml"><mi id="S2.E5.m1.4.4.4.3.4.3.2" xref="S2.E5.m1.4.4.4.3.4.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.4.4.4.3.4.3.1" xref="S2.E5.m1.4.4.4.3.4.3.1.cmml">â€‹</mo><mi id="S2.E5.m1.4.4.4.3.4.3.3" xref="S2.E5.m1.4.4.4.3.4.3.3.cmml">M</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E5.m1.4.4.4.3.3a" xref="S2.E5.m1.4.4.4.3.3.cmml">â€‹</mo><mrow id="S2.E5.m1.4.4.4.3.2.1" xref="S2.E5.m1.4.4.4.3.2.1.1.cmml"><mo stretchy="false" id="S2.E5.m1.4.4.4.3.2.1.2" xref="S2.E5.m1.4.4.4.3.2.1.1.cmml">(</mo><mrow id="S2.E5.m1.4.4.4.3.2.1.1" xref="S2.E5.m1.4.4.4.3.2.1.1.cmml"><mi id="S2.E5.m1.4.4.4.3.2.1.1.2" xref="S2.E5.m1.4.4.4.3.2.1.1.2.cmml">y</mi><mo fence="false" id="S2.E5.m1.4.4.4.3.2.1.1.1" xref="S2.E5.m1.4.4.4.3.2.1.1.1.cmml">|</mo><mi id="S2.E5.m1.4.4.4.3.2.1.1.3" xref="S2.E5.m1.4.4.4.3.2.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.E5.m1.4.4.4.3.2.1.3" xref="S2.E5.m1.4.4.4.3.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.4b"><apply id="S2.E5.m1.4.4.cmml" xref="S2.E5.m1.4.4"><eq id="S2.E5.m1.4.4.5.cmml" xref="S2.E5.m1.4.4.5"></eq><apply id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1.1"><times id="S2.E5.m1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.2"></times><ci id="S2.E5.m1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.3">ğ‘</ci><apply id="S2.E5.m1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.1.1.1.1.1.1.1.cmml" xref="S2.E5.m1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.E5.m1.1.1.1.1.1.1.2.cmml" xref="S2.E5.m1.1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S2.E5.m1.1.1.1.1.1.1.3.cmml" xref="S2.E5.m1.1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply><apply id="S2.E5.m1.4.4.4.cmml" xref="S2.E5.m1.4.4.4"><plus id="S2.E5.m1.4.4.4.4.cmml" xref="S2.E5.m1.4.4.4.4"></plus><apply id="S2.E5.m1.2.2.2.1.cmml" xref="S2.E5.m1.2.2.2.1"><times id="S2.E5.m1.2.2.2.1.2.cmml" xref="S2.E5.m1.2.2.2.1.2"></times><ci id="S2.E5.m1.2.2.2.1.3.cmml" xref="S2.E5.m1.2.2.2.1.3">ğœ†</ci><apply id="S2.E5.m1.2.2.2.1.4.cmml" xref="S2.E5.m1.2.2.2.1.4"><csymbol cd="ambiguous" id="S2.E5.m1.2.2.2.1.4.1.cmml" xref="S2.E5.m1.2.2.2.1.4">subscript</csymbol><ci id="S2.E5.m1.2.2.2.1.4.2.cmml" xref="S2.E5.m1.2.2.2.1.4.2">ğ‘</ci><ci id="S2.E5.m1.2.2.2.1.4.3.cmml" xref="S2.E5.m1.2.2.2.1.4.3">ğ‘…</ci></apply><apply id="S2.E5.m1.2.2.2.1.1.1.1.cmml" xref="S2.E5.m1.2.2.2.1.1.1"><csymbol cd="latexml" id="S2.E5.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E5.m1.2.2.2.1.1.1.1.1">conditional</csymbol><ci id="S2.E5.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E5.m1.2.2.2.1.1.1.1.2">ğ‘¦</ci><ci id="S2.E5.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E5.m1.2.2.2.1.1.1.1.3">ğ‘¥</ci></apply></apply><apply id="S2.E5.m1.4.4.4.3.cmml" xref="S2.E5.m1.4.4.4.3"><times id="S2.E5.m1.4.4.4.3.3.cmml" xref="S2.E5.m1.4.4.4.3.3"></times><apply id="S2.E5.m1.3.3.3.2.1.1.1.cmml" xref="S2.E5.m1.3.3.3.2.1.1"><minus id="S2.E5.m1.3.3.3.2.1.1.1.1.cmml" xref="S2.E5.m1.3.3.3.2.1.1.1.1"></minus><cn type="integer" id="S2.E5.m1.3.3.3.2.1.1.1.2.cmml" xref="S2.E5.m1.3.3.3.2.1.1.1.2">1</cn><ci id="S2.E5.m1.3.3.3.2.1.1.1.3.cmml" xref="S2.E5.m1.3.3.3.2.1.1.1.3">ğœ†</ci></apply><apply id="S2.E5.m1.4.4.4.3.4.cmml" xref="S2.E5.m1.4.4.4.3.4"><csymbol cd="ambiguous" id="S2.E5.m1.4.4.4.3.4.1.cmml" xref="S2.E5.m1.4.4.4.3.4">subscript</csymbol><ci id="S2.E5.m1.4.4.4.3.4.2.cmml" xref="S2.E5.m1.4.4.4.3.4.2">ğ‘</ci><apply id="S2.E5.m1.4.4.4.3.4.3.cmml" xref="S2.E5.m1.4.4.4.3.4.3"><times id="S2.E5.m1.4.4.4.3.4.3.1.cmml" xref="S2.E5.m1.4.4.4.3.4.3.1"></times><ci id="S2.E5.m1.4.4.4.3.4.3.2.cmml" xref="S2.E5.m1.4.4.4.3.4.3.2">ğ¿</ci><ci id="S2.E5.m1.4.4.4.3.4.3.3.cmml" xref="S2.E5.m1.4.4.4.3.4.3.3">ğ‘€</ci></apply></apply><apply id="S2.E5.m1.4.4.4.3.2.1.1.cmml" xref="S2.E5.m1.4.4.4.3.2.1"><csymbol cd="latexml" id="S2.E5.m1.4.4.4.3.2.1.1.1.cmml" xref="S2.E5.m1.4.4.4.3.2.1.1.1">conditional</csymbol><ci id="S2.E5.m1.4.4.4.3.2.1.1.2.cmml" xref="S2.E5.m1.4.4.4.3.2.1.1.2">ğ‘¦</ci><ci id="S2.E5.m1.4.4.4.3.2.1.1.3.cmml" xref="S2.E5.m1.4.4.4.3.2.1.1.3">ğ‘¥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.4c">p(y|x)=\lambda p_{R}(y|x)+(1-\lambda)p_{LM}(y|x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S2.SS3.p1.6" class="ltx_p">where the retrieved output tokens are denoted by <math id="S2.SS3.p1.4.m1.1" class="ltx_Math" alttext="p_{R}(y|x)" display="inline"><semantics id="S2.SS3.p1.4.m1.1a"><mrow id="S2.SS3.p1.4.m1.1.1" xref="S2.SS3.p1.4.m1.1.1.cmml"><msub id="S2.SS3.p1.4.m1.1.1.3" xref="S2.SS3.p1.4.m1.1.1.3.cmml"><mi id="S2.SS3.p1.4.m1.1.1.3.2" xref="S2.SS3.p1.4.m1.1.1.3.2.cmml">p</mi><mi id="S2.SS3.p1.4.m1.1.1.3.3" xref="S2.SS3.p1.4.m1.1.1.3.3.cmml">R</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS3.p1.4.m1.1.1.2" xref="S2.SS3.p1.4.m1.1.1.2.cmml">â€‹</mo><mrow id="S2.SS3.p1.4.m1.1.1.1.1" xref="S2.SS3.p1.4.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p1.4.m1.1.1.1.1.2" xref="S2.SS3.p1.4.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.4.m1.1.1.1.1.1" xref="S2.SS3.p1.4.m1.1.1.1.1.1.cmml"><mi id="S2.SS3.p1.4.m1.1.1.1.1.1.2" xref="S2.SS3.p1.4.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S2.SS3.p1.4.m1.1.1.1.1.1.1" xref="S2.SS3.p1.4.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS3.p1.4.m1.1.1.1.1.1.3" xref="S2.SS3.p1.4.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.SS3.p1.4.m1.1.1.1.1.3" xref="S2.SS3.p1.4.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m1.1b"><apply id="S2.SS3.p1.4.m1.1.1.cmml" xref="S2.SS3.p1.4.m1.1.1"><times id="S2.SS3.p1.4.m1.1.1.2.cmml" xref="S2.SS3.p1.4.m1.1.1.2"></times><apply id="S2.SS3.p1.4.m1.1.1.3.cmml" xref="S2.SS3.p1.4.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p1.4.m1.1.1.3.1.cmml" xref="S2.SS3.p1.4.m1.1.1.3">subscript</csymbol><ci id="S2.SS3.p1.4.m1.1.1.3.2.cmml" xref="S2.SS3.p1.4.m1.1.1.3.2">ğ‘</ci><ci id="S2.SS3.p1.4.m1.1.1.3.3.cmml" xref="S2.SS3.p1.4.m1.1.1.3.3">ğ‘…</ci></apply><apply id="S2.SS3.p1.4.m1.1.1.1.1.1.cmml" xref="S2.SS3.p1.4.m1.1.1.1.1"><csymbol cd="latexml" id="S2.SS3.p1.4.m1.1.1.1.1.1.1.cmml" xref="S2.SS3.p1.4.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS3.p1.4.m1.1.1.1.1.1.2.cmml" xref="S2.SS3.p1.4.m1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S2.SS3.p1.4.m1.1.1.1.1.1.3.cmml" xref="S2.SS3.p1.4.m1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m1.1c">p_{R}(y|x)</annotation></semantics></math>, the language model generated output tokens are denoted by <math id="S2.SS3.p1.5.m2.1" class="ltx_Math" alttext="p_{LM}(y|x)" display="inline"><semantics id="S2.SS3.p1.5.m2.1a"><mrow id="S2.SS3.p1.5.m2.1.1" xref="S2.SS3.p1.5.m2.1.1.cmml"><msub id="S2.SS3.p1.5.m2.1.1.3" xref="S2.SS3.p1.5.m2.1.1.3.cmml"><mi id="S2.SS3.p1.5.m2.1.1.3.2" xref="S2.SS3.p1.5.m2.1.1.3.2.cmml">p</mi><mrow id="S2.SS3.p1.5.m2.1.1.3.3" xref="S2.SS3.p1.5.m2.1.1.3.3.cmml"><mi id="S2.SS3.p1.5.m2.1.1.3.3.2" xref="S2.SS3.p1.5.m2.1.1.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.5.m2.1.1.3.3.1" xref="S2.SS3.p1.5.m2.1.1.3.3.1.cmml">â€‹</mo><mi id="S2.SS3.p1.5.m2.1.1.3.3.3" xref="S2.SS3.p1.5.m2.1.1.3.3.3.cmml">M</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.SS3.p1.5.m2.1.1.2" xref="S2.SS3.p1.5.m2.1.1.2.cmml">â€‹</mo><mrow id="S2.SS3.p1.5.m2.1.1.1.1" xref="S2.SS3.p1.5.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p1.5.m2.1.1.1.1.2" xref="S2.SS3.p1.5.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.5.m2.1.1.1.1.1" xref="S2.SS3.p1.5.m2.1.1.1.1.1.cmml"><mi id="S2.SS3.p1.5.m2.1.1.1.1.1.2" xref="S2.SS3.p1.5.m2.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S2.SS3.p1.5.m2.1.1.1.1.1.1" xref="S2.SS3.p1.5.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS3.p1.5.m2.1.1.1.1.1.3" xref="S2.SS3.p1.5.m2.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.SS3.p1.5.m2.1.1.1.1.3" xref="S2.SS3.p1.5.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.5.m2.1b"><apply id="S2.SS3.p1.5.m2.1.1.cmml" xref="S2.SS3.p1.5.m2.1.1"><times id="S2.SS3.p1.5.m2.1.1.2.cmml" xref="S2.SS3.p1.5.m2.1.1.2"></times><apply id="S2.SS3.p1.5.m2.1.1.3.cmml" xref="S2.SS3.p1.5.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p1.5.m2.1.1.3.1.cmml" xref="S2.SS3.p1.5.m2.1.1.3">subscript</csymbol><ci id="S2.SS3.p1.5.m2.1.1.3.2.cmml" xref="S2.SS3.p1.5.m2.1.1.3.2">ğ‘</ci><apply id="S2.SS3.p1.5.m2.1.1.3.3.cmml" xref="S2.SS3.p1.5.m2.1.1.3.3"><times id="S2.SS3.p1.5.m2.1.1.3.3.1.cmml" xref="S2.SS3.p1.5.m2.1.1.3.3.1"></times><ci id="S2.SS3.p1.5.m2.1.1.3.3.2.cmml" xref="S2.SS3.p1.5.m2.1.1.3.3.2">ğ¿</ci><ci id="S2.SS3.p1.5.m2.1.1.3.3.3.cmml" xref="S2.SS3.p1.5.m2.1.1.3.3.3">ğ‘€</ci></apply></apply><apply id="S2.SS3.p1.5.m2.1.1.1.1.1.cmml" xref="S2.SS3.p1.5.m2.1.1.1.1"><csymbol cd="latexml" id="S2.SS3.p1.5.m2.1.1.1.1.1.1.cmml" xref="S2.SS3.p1.5.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS3.p1.5.m2.1.1.1.1.1.2.cmml" xref="S2.SS3.p1.5.m2.1.1.1.1.1.2">ğ‘¦</ci><ci id="S2.SS3.p1.5.m2.1.1.1.1.1.3.cmml" xref="S2.SS3.p1.5.m2.1.1.1.1.1.3">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.5.m2.1c">p_{LM}(y|x)</annotation></semantics></math>, and the weights are denoted by <math id="S2.SS3.p1.6.m3.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS3.p1.6.m3.1a"><mi id="S2.SS3.p1.6.m3.1.1" xref="S2.SS3.p1.6.m3.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.6.m3.1b"><ci id="S2.SS3.p1.6.m3.1.1.cmml" xref="S2.SS3.p1.6.m3.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.6.m3.1c">\lambda</annotation></semantics></math>. Among the researchers who have employed this method, the most renowned are <cite class="ltx_cite ltx_citemacro_citet">Khandelwal et&nbsp;al. (<a href="#bib.bib63" title="" class="ltx_ref">2019</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Alon et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">He et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of Retrievers in RALM works.</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:384.6pt;height:590.3pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.9pt,173.2pt) scale(0.63,0.63) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2">Category</td>
<td id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Technique</td>
<td id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Year</td>
<td id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Reference</td>
</tr>
<tr id="S2.T1.1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.1.2.2.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.2.2.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.2.2.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.1.3.3.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.3.3.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.3.3.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.1.4.4.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.4.4.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.4.4.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Zhong et&nbsp;al. (<a href="#bib.bib156" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.1.5.5.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.5.5.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.5.5.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2024</td>
<td id="S2.T1.1.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.1.6.6.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.6.6.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.6.6.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.6.6.5.1" class="ltx_text" style="color:#882D00;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib53" title="" class="ltx_ref">2020b</a>)</cite></span></td>
</tr>
<tr id="S2.T1.1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.1.7.7.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.7.7.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.7.7.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Mallen et&nbsp;al. (<a href="#bib.bib86" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.1.8.8.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.8.8.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.8.8.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2024</td>
<td id="S2.T1.1.1.8.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Schick et&nbsp;al. (<a href="#bib.bib110" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.9.9" class="ltx_tr">
<td id="S2.T1.1.1.9.9.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.9.9.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.9.9.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Xu et&nbsp;al. (<a href="#bib.bib134" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.10.10" class="ltx_tr">
<td id="S2.T1.1.1.10.10.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.10.10.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.10.10.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.10.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.11.11" class="ltx_tr">
<td id="S2.T1.1.1.11.11.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.11.11.2.1" class="ltx_text">Word Frequency</span></td>
<td id="S2.T1.1.1.11.11.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.11.11.3.1" class="ltx_text">BM25<cite class="ltx_cite ltx_citemacro_cite">Robertson et&nbsp;al. (<a href="#bib.bib102" title="" class="ltx_ref">1995</a>)</cite></span></td>
<td id="S2.T1.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.11.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Yu et&nbsp;al. (<a href="#bib.bib146" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.12.12" class="ltx_tr">
<td id="S2.T1.1.1.12.12.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.12.12.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.12.12.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.12.12.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Alon et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.13.13" class="ltx_tr">
<td id="S2.T1.1.1.13.13.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.13.13.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.13.13.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.13.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Borgeaud et&nbsp;al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.14.14" class="ltx_tr">
<td id="S2.T1.1.1.14.14.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.14.14.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.14.14.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.14.14.3.1" class="ltx_text">KNN search</span></td>
<td id="S2.T1.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2019</td>
<td id="S2.T1.1.1.14.14.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Khandelwal et&nbsp;al. (<a href="#bib.bib63" title="" class="ltx_ref">2019</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.15.15" class="ltx_tr">
<td id="S2.T1.1.1.15.15.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.15.15.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">GUD-IR<cite class="ltx_cite ltx_citemacro_cite">Madaan et&nbsp;al. (<a href="#bib.bib85" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.15.15.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Madaan et&nbsp;al. (<a href="#bib.bib85" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.16.16" class="ltx_tr">
<td id="S2.T1.1.1.16.16.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.16.16.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.16.16.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">GAR<cite class="ltx_cite ltx_citemacro_cite">Mao et&nbsp;al. (<a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S2.T1.1.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.16.16.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Mao et&nbsp;al. (<a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.17.17" class="ltx_tr">
<td id="S2.T1.1.1.17.17.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.17.17.1.1" class="ltx_text">Sparse Retrieval</span></td>
<td id="S2.T1.1.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.17.17.2.1" class="ltx_text">Sparse Vector Representation</span></td>
<td id="S2.T1.1.1.17.17.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Spider<cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.17.17.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.18.18" class="ltx_tr">
<td id="S2.T1.1.1.18.18.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.18.18.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.18.18.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.18.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.18.18.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard et&nbsp;al. (<a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.19.19" class="ltx_tr">
<td id="S2.T1.1.1.19.19.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.19.19.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.19.19.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.19.19.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.19.19.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.20.20" class="ltx_tr">
<td id="S2.T1.1.1.20.20.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.20.20.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.20.20.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.20.20.3.1" class="ltx_text">COLBERTV2<cite class="ltx_cite ltx_citemacro_cite">Santhanam et&nbsp;al. (<a href="#bib.bib108" title="" class="ltx_ref">2021</a>)</cite></span></td>
<td id="S2.T1.1.1.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.20.20.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">HofstÃ¤tter et&nbsp;al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.21.21" class="ltx_tr">
<td id="S2.T1.1.1.21.21.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.21.21.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.21.21.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.21.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.21.21.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et&nbsp;al. (<a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.22.22" class="ltx_tr">
<td id="S2.T1.1.1.22.22.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.22.22.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.22.22.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.22.22.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.23.23" class="ltx_tr">
<td id="S2.T1.1.1.23.23.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.23.23.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.23.23.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.23.23.3.1" class="ltx_text">Contriever<cite class="ltx_cite ltx_citemacro_cite">Izacard et&nbsp;al. (<a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite></span></td>
<td id="S2.T1.1.1.23.23.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.23.23.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Guu et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.24.24" class="ltx_tr">
<td id="S2.T1.1.1.24.24.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.24.24.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.24.24.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">DBE<cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite>
</td>
<td id="S2.T1.1.1.24.24.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.24.24.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.25.25" class="ltx_tr">
<td id="S2.T1.1.1.25.25.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.25.25.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.25.25.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">DKR<cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.25.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.25.25.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.26.26" class="ltx_tr">
<td id="S2.T1.1.1.26.26.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.26.26.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.26.26.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.26.26.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.26.26.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.27.27" class="ltx_tr">
<td id="S2.T1.1.1.27.27.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.27.27.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.27.27.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.27.27.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.27.27.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib53" title="" class="ltx_ref">2020b</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.28.28" class="ltx_tr">
<td id="S2.T1.1.1.28.28.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.28.28.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.28.28.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.28.28.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.28.28.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Karpukhin et&nbsp;al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.29.29" class="ltx_tr">
<td id="S2.T1.1.1.29.29.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.29.29.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.29.29.3" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.29.29.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.29.29.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Mallen et&nbsp;al. (<a href="#bib.bib86" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.30.30" class="ltx_tr">
<td id="S2.T1.1.1.30.30.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.30.30.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.30.30.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.30.30.3.1" class="ltx_text">DPR<cite class="ltx_cite ltx_citemacro_cite">Karpukhin et&nbsp;al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite></span></td>
<td id="S2.T1.1.1.30.30.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2020</td>
<td id="S2.T1.1.1.30.30.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.31.31" class="ltx_tr">
<td id="S2.T1.1.1.31.31.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.31.31.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.31.31.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">GTR<cite class="ltx_cite ltx_citemacro_cite">Ni et&nbsp;al. (<a href="#bib.bib91" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.31.31.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2019</td>
<td id="S2.T1.1.1.31.31.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Lee et&nbsp;al. (<a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.32.32" class="ltx_tr">
<td id="S2.T1.1.1.32.32.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.32.32.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.32.32.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">E2E-NR<cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.32.32.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.32.32.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.33.33" class="ltx_tr">
<td id="S2.T1.1.1.33.33.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.33.33.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.33.33.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">REALM<cite class="ltx_cite ltx_citemacro_cite">Guu et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S2.T1.1.1.33.33.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.33.33.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.34.34" class="ltx_tr">
<td id="S2.T1.1.1.34.34.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.34.34.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.34.34.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">ORQA<cite class="ltx_cite ltx_citemacro_cite">Lee et&nbsp;al. (<a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S2.T1.1.1.34.34.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.34.34.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.35.35" class="ltx_tr">
<td id="S2.T1.1.1.35.35.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.35.35.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.35.35.2.1" class="ltx_text">Word Embedding</span></td>
<td id="S2.T1.1.1.35.35.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">EMDR2<cite class="ltx_cite ltx_citemacro_cite">Singh et&nbsp;al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.35.35.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.35.35.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Santhanam et&nbsp;al. (<a href="#bib.bib108" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.36.36" class="ltx_tr">
<td id="S2.T1.1.1.36.36.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.36.36.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.36.36.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">MuRAG<cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2022b</a>)</cite>
</td>
<td id="S2.T1.1.1.36.36.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.36.36.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2022b</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.37.37" class="ltx_tr">
<td id="S2.T1.1.1.37.37.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.37.37.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.37.37.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">RA-CM3<cite class="ltx_cite ltx_citemacro_cite">Yasunaga et&nbsp;al. (<a href="#bib.bib139" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.37.37.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.37.37.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Yasunaga et&nbsp;al. (<a href="#bib.bib139" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.38.38" class="ltx_tr">
<td id="S2.T1.1.1.38.38.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.38.38.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.38.38.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">RE-IMAGEN<cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2022c</a>)</cite>
</td>
<td id="S2.T1.1.1.38.38.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.38.38.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2022c</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.39.39" class="ltx_tr">
<td id="S2.T1.1.1.39.39.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.39.39.2" class="ltx_td ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.39.39.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">MDTIG<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a href="#bib.bib72" title="" class="ltx_ref">2022a</a>)</cite>
</td>
<td id="S2.T1.1.1.39.39.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.39.39.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a href="#bib.bib72" title="" class="ltx_ref">2022a</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.40.40" class="ltx_tr">
<td id="S2.T1.1.1.40.40.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.40.40.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.40.40.2.1" class="ltx_text">Multimodal Retrieval</span></td>
<td id="S2.T1.1.1.40.40.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">RDM<cite class="ltx_cite ltx_citemacro_cite">Blattmann et&nbsp;al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.40.40.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.40.40.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Blattmann et&nbsp;al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.41.41" class="ltx_tr">
<td id="S2.T1.1.1.41.41.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.41.41.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"></td>
<td id="S2.T1.1.1.41.41.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">DRAGON<cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib74" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S2.T1.1.1.41.41.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.41.41.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.42.42" class="ltx_tr">
<td id="S2.T1.1.1.42.42.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.42.42.1.1" class="ltx_text">Dense Retrieval</span></td>
<td id="S2.T1.1.1.42.42.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;"><span id="S2.T1.1.1.42.42.2.1" class="ltx_text">Knowledge Distillation</span></td>
<td id="S2.T1.1.1.42.42.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">REPULG LSR<cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="S2.T1.1.1.42.42.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.42.42.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.43.43" class="ltx_tr">
<td id="S2.T1.1.1.43.43.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.43.43.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">FLARE<cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite>
</td>
<td id="S2.T1.1.1.43.43.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.43.43.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.44.44" class="ltx_tr">
<td id="S2.T1.1.1.44.44.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.44.44.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">RAG-Robust<cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.44.44.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.44.44.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.45.45" class="ltx_tr">
<td id="S2.T1.1.1.45.45.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.45.45.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">IADG<cite class="ltx_cite ltx_citemacro_cite">Komeili et&nbsp;al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.45.45.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.45.45.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Komeili et&nbsp;al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.46.46" class="ltx_tr">
<td id="S2.T1.1.1.46.46.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"><span id="S2.T1.1.1.46.46.1.1" class="ltx_text">Internet Retrieval</span></td>
<td id="S2.T1.1.1.46.46.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Webgpt<cite class="ltx_cite ltx_citemacro_cite">Nakano et&nbsp;al. (<a href="#bib.bib89" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.46.46.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.46.46.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Nakano et&nbsp;al. (<a href="#bib.bib89" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.47.47" class="ltx_tr">
<td id="S2.T1.1.1.47.47.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.47.47.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">DuckDuckGo+BM25<cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S2.T1.1.1.47.47.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.47.47.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.48.48" class="ltx_tr">
<td id="S2.T1.1.1.48.48.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.48.48.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Internet+TF-IDF<cite class="ltx_cite ltx_citemacro_cite">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S2.T1.1.1.48.48.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2022</td>
<td id="S2.T1.1.1.48.48.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.49.49" class="ltx_tr">
<td id="S2.T1.1.1.49.49.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.49.49.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">REVEAL <cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S2.T1.1.1.49.49.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2023</td>
<td id="S2.T1.1.1.49.49.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.50.50" class="ltx_tr">
<td id="S2.T1.1.1.50.50.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.50.50.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">NAG-ERE<cite class="ltx_cite ltx_citemacro_cite">Hua and Wang (<a href="#bib.bib49" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S2.T1.1.1.50.50.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2018</td>
<td id="S2.T1.1.1.50.50.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Hua and Wang (<a href="#bib.bib49" title="" class="ltx_ref">2018</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.51.51" class="ltx_tr">
<td id="S2.T1.1.1.51.51.1" class="ltx_td ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"></td>
<td id="S2.T1.1.1.51.51.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">Internet+BM25<cite class="ltx_cite ltx_citemacro_cite">Adolphs et&nbsp;al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S2.T1.1.1.51.51.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2021</td>
<td id="S2.T1.1.1.51.51.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Adolphs et&nbsp;al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S2.T1.1.1.52.52" class="ltx_tr">
<td id="S2.T1.1.1.52.52.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" style="padding-top:0.55pt;padding-bottom:0.55pt;" colspan="2"><span id="S2.T1.1.1.52.52.1.1" class="ltx_text">Hybrid Retrieval</span></td>
<td id="S2.T1.1.1.52.52.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">kNN+BM25+translation model<cite class="ltx_cite ltx_citemacro_cite">Boytsov et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>
</td>
<td id="S2.T1.1.1.52.52.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;">2016</td>
<td id="S2.T1.1.1.52.52.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.55pt;padding-bottom:0.55pt;"><cite class="ltx_cite ltx_citemacro_cite">Boytsov et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Retriever</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Retrievers play a crucial role in the RALM architecture. The information obtained through retrievers can significantly improve the accuracy of the LM. This section provides a summary of the retrieval methods commonly used in the RALM architecture. The retrieval methods are classified into four categories based on their methods and sources: Sparse Retrieval, Dense Retrieval, Internet Retrieval, and Hybrid Retrieval. Table <a href="#S2.T1" title="Table 1 â€£ 2.3 Parallel Interaction â€£ 2 Definition â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> lists information about specific applications of retrievers in the RALM.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Sparse Retriever</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">For a period of time following the proposal of the retrieval technique, sparse retrieval proves to be a straightforward and effective tool in solving problems, particularly those based on knowledge. One of the main advantages of sparse retrieval is its simplicity, which can be easily integrated into existing indexing systems due to the fewer dimensions involved. <cite class="ltx_cite ltx_citemacro_cite">Hambarde and Proenca (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>This is consistent with human cognitive processes. Additionally, sparse retrieval is easier to generalise and more efficient. Sparse retrieval used in RALM can be classified into two categories: Word Frequency and Sparse Vector Representation. The choice between the two depends on whether machine learning is used.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Word Frequency</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">In the initial stage, individuals often use methods for retrieval that involve matching of relevant content, such as the TF-IDF <cite class="ltx_cite ltx_citemacro_cite">Ramos et&nbsp;al. (<a href="#bib.bib101" title="" class="ltx_ref">2003</a>)</cite> and BM25 <cite class="ltx_cite ltx_citemacro_cite">Robertson et&nbsp;al. (<a href="#bib.bib102" title="" class="ltx_ref">1995</a>)</cite> algorithms, which are considered classic and effective.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">The TF-IDF algorithm utilises term frequency (TF) and inverse document frequency (IDF) to represent relevance, which has the advantages of simplicity and speed, and even if the corpus is unchanged, the TF-IDF value for each word can be computed in advance. In RALM, <cite class="ltx_cite ltx_citemacro_citet">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> utilise the TF-IDF algorithm to match information obtained from user queries and calls to the Google search API. <cite class="ltx_cite ltx_citemacro_citet">Hua and Wang (<a href="#bib.bib49" title="" class="ltx_ref">2018</a>)</cite> also employ the algorithm to score the generated results. The BM25 represents an enhancement over the TF-IDF. It considers the userâ€™s query and calculates the relevance score as the weighted sum of the relevance of each query word to the document. The IDF algorithm is used to derive the weight of each word, but it is improved by two moderating factors to prevent the strength of the influence of a certain factor from being infinite. This is consistent with common sense. Due to its excellent generalisation capabilities, many Retrieval-Augmented Language Model (RaLM) architectures, particularly those oriented towards open domains, employ BM25 as a retrieval method, such as <cite class="ltx_cite ltx_citemacro_citet">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zhong et&nbsp;al. (<a href="#bib.bib156" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Sparse Vector Representation</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">It has become evident that simple term matching is no longer sufficient to meet the demand. Manual labelling can solve problems such as the synonym issue, but it is a resource-intensive method. With the rise of machine learning, sparse vectors are now used to represent words and retrieve them by calculating the distance between them. <cite class="ltx_cite ltx_citemacro_cite">Hambarde and Proenca (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> Sparse vector representation techniques differ from term matching methods in that they construct sparse vectors for queries and documents. The purpose of these representations is to capture the semantic essence of each input text, which places queries and documents in a latent space.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Ram et&nbsp;al. (<a href="#bib.bib98" title="" class="ltx_ref">2021</a>)</cite> utilised the fact that when given two paragraphs with the same repeat span, one was used to construct a query and the other as the retrieval target. The remaining paragraph in the document, which did not contain a repeat span, was used as a negative example and <cite class="ltx_cite ltx_citemacro_citet">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite> applied this retriever for the first time to the RALM architecture. On the other hand, both <cite class="ltx_cite ltx_citemacro_citet">Mao et&nbsp;al. (<a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Madaan et&nbsp;al. (<a href="#bib.bib85" title="" class="ltx_ref">2022</a>)</cite> proposed using language models (LM) to enhance retrieval accuracy of sparse vector representation by generating new queries from a given question and using them to retrieve relevant documents. However, Maoâ€™s approach emphasizes query expansion, while Maddanâ€™s approach emphasizes understanding the userâ€™s input.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dense Retriever</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The emergence of deep learning techniques has significantly transformed the field of retrieval. There is a growing interest in using deep learning techniques to enhance retrieval accuracy, even if it means sacrificing some level of comprehensibility. The dual encoder architecture is a common design for dense retrieval models. <cite class="ltx_cite ltx_citemacro_cite">Hambarde and Proenca (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> The system comprises of two distinct networks that receive separate inputs, namely queries and documents, and independently generate dense embeddings for each input. Due to its high accuracy and dual-encoder structure, which is more suitable for RALM, most articles choose to use the dense indexing method to build their retrievers. This section classifies dense retrieval into three types: Word Embedding, Multimodal Retrieval, and Data Distillation, based on the characteristics of each retrieval method.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Word Embedding</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Word embeddings are a common approach in natural language processing. Similar to sparse vector representations, they use deep learning techniques to project words into a higher-dimensional vector space. Several articles in the RALM architecture utilize this technique, and we have selected representative ones to describe.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Karpukhin et&nbsp;al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite> proposed the DPR retrieval model, which indexes all passages in a low-dimensional and continuous space. This allows the reader to efficiently retrieve the first k passages associated with the input problem at runtime. A dense encoder is used to map any text passage to a d-dimensional real-valued vector, creating an index for all M passages used for retrieval. Due to its excellent performance as a retriever in RALM architectures, DPR has been widely adopted by researchers such as <cite class="ltx_cite ltx_citemacro_citet">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Izacard and Grave (<a href="#bib.bib53" title="" class="ltx_ref">2020b</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Karpukhin et&nbsp;al. (<a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite> takes a similar tactic, unlike DPR, in that he uses the same encoding function for questions and paragraphs through shared parameters. In order to further minimise the intervention and reduce the cost of manual annotation, <cite class="ltx_cite ltx_citemacro_citet">Izacard et&nbsp;al. (<a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite> proposed another retriever called Contriever, which was trained using unsupervised data. It is based on successive dense embeddings and has a dual-encoder architecture. Average pooling was applied on the output of the previous layer to obtain one vector representation for each query or document. The similarity score between the query and each document was obtained by computing the dot product between their corresponding embeddings. Researchers <cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et&nbsp;al. (<a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_cite">Guu et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> have used it as a retriever in the RALM architecture due to its ability to utilize unsupervised data.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Multimodal Retrieval</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Retrieval techniques for multimodal tasks are more complex than those for text-only tasks <cite class="ltx_cite ltx_citemacro_cite">Zhao et&nbsp;al. (<a href="#bib.bib153" title="" class="ltx_ref">2023</a>)</cite>. This is because they involve the inter-transformation of information about different states. For example, in the image-text domain, multimodal techniques for dense text retrieval have attracted interest as a means of bridging the gap between different modalities <cite class="ltx_cite ltx_citemacro_cite">Zhao et&nbsp;al. (<a href="#bib.bib154" title="" class="ltx_ref">2024b</a>, <a href="#bib.bib155" title="" class="ltx_ref">c</a>)</cite>. Researchers have developed methods to encode textual and visual information into a shared latent space for retrieval tasks.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Li et&nbsp;al. (<a href="#bib.bib72" title="" class="ltx_ref">2022a</a>)</cite> designed four matching algorithms for handling multimodal tasks, namely sentence-to-sentence, sentence-to-image, word-to-word and word-to-image. They used reweighting for more accurate correlation calculations and cosine similarity scores as a criterion to explore the effectiveness of each algorithm. Unlike the former, <cite class="ltx_cite ltx_citemacro_citet">Yasunaga et&nbsp;al. (<a href="#bib.bib139" title="" class="ltx_ref">2022</a>)</cite> utilised a straightforward extension of CLIP <cite class="ltx_cite ltx_citemacro_cite">Ramesh et&nbsp;al. (<a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite> to divide a multimodal document into text and image components, which were encoded with frozen encoders. The L2 norm was scaled to 1 using an average pooling technique, resulting in a vector representation of the document. Similarly to <cite class="ltx_cite ltx_citemacro_citet">Li et&nbsp;al. (<a href="#bib.bib72" title="" class="ltx_ref">2022a</a>)</cite>, they also employed a Maximum Inner Product Search (MIPS) as relevance scores, ultimately selecting K documents.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Knowledge Distillation</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Knowledge distillation is a technique for gradually filtering and streamlining a large database to make it more suitable for a userâ€™s query. <cite class="ltx_cite ltx_citemacro_cite">Gou et&nbsp;al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> involves transferring data from a pre-trained, larger model to a smaller one, often using methods such as embedded matching. Research has even been conducted on data distillation using LMs as the technology has evolved.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite> utilises knowledge distillation to divide the retrieval process into four distinct steps. Firstly, documents are retrieved and retrieval likelihoods are computed. Secondly, the retrieved documents are scored using a language model. Thirdly, the parameters of the retrieval model are updated by minimising the KL discrepancy between the retrieval likelihoods and the distribution of the language model scores. Finally, asynchronously updating of the indexes of the data is performed. Based on this technique, <cite class="ltx_cite ltx_citemacro_citet">Lin et&nbsp;al. (<a href="#bib.bib74" title="" class="ltx_ref">2023a</a>)</cite> further improve the accuracy of knowledge distillation. They present a data distillation scheme that combines sentence truncation and query enhancement with incremental relevance label enhancement using multiple enhancers.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Internet Retrieval</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">With the advancement of Internet search and sorting technology, some researchers have focused their search efforts on Internet retrieval, which is a plug-and-play approach. This approach allows non-specialists to benefit from RALM and is better suited to the open domain and generalisation. Another advantage of this retrieval model is that it does not require real-time updating of the database, but relies on updates from commercial search engines. However, despite the advantages of simplicity and convenience, there is a significant amount of irrelevant and even harmful information on the Internet that can hinder the work of RALM. If an effective screening mechanism is not implemented, the effectiveness of RALM will be significantly reduced.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Unlike most studies <cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Nakano et&nbsp;al. (<a href="#bib.bib89" title="" class="ltx_ref">2021</a>)</cite> that directly utilise commercial search engine APIs. <cite class="ltx_cite ltx_citemacro_citet">Komeili et&nbsp;al. (<a href="#bib.bib65" title="" class="ltx_ref">2021</a>)</cite> propose an alternative approach to using multiple commercial search engine APIs. They suggest using the Bing Search API to generate a list of URLs for each query. These URLs are then used as keys to look up their page content in a lookup table constructed from public crawl snapshots, which populates a set of pages for that query. In addition, the evaluation takes into account whether the URL is from the English Wikipedia. If so, the page title is extracted from the URL and the corresponding page is searched for in the Wikipedia dump.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Hybrid Retrieval</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">As researchers gain a better understanding of the strengths and weaknesses of various retrieval techniques, they are increasingly opting to combine them, as described above. This is done in the hope of further exploiting the advantages of these techniques to improve the effectiveness and robustness of the RALM architecture.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">To tackle the issue of inaccurate Internet retrieval results, <cite class="ltx_cite ltx_citemacro_citet">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> proposed using the TF-IDF algorithm to score the retrieval results. They used each question q verbatim as a query and issued a call to Google Search via the Google Search API. For each question, they retrieved the top 20 URLs and parsed their HTML content to extract clean text, generating a set of documents D for each question q. To prevent irrelevant information from hindering the resolution of a userâ€™s query, <cite class="ltx_cite ltx_citemacro_citet">Hu et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite> designed a gating circuit. This circuit utilised a dual-encoder dot product to calculate similarity and a gating circuit based on term weights. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Boytsov et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> presented an approach that replaced term-based retrieval with k-Nearest Neighbors(kNN) search while combining a translation model and BM25 to improve retrieval performance. This approach enabled the model to take into account the semantic relationships between terms and traditional statistical weighting schemes, resulting in a more efficient retrieval system.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Summary of LMs in RALM methods.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:387.5pt;height:586.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.1pt,103.0pt) scale(0.74,0.74) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2">Category</td>
<td id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Technique</td>
<td id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Year</td>
<td id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Reference</td>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<td id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2" rowspan="3"><span id="S3.T2.1.1.2.2.1.1" class="ltx_text">AutoEncoder Language Model</span></td>
<td id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.2.2.2.1" class="ltx_text">RoBERTa<cite class="ltx_cite ltx_citemacro_cite">Devlin et&nbsp;al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite></span></td>
<td id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.3.3" class="ltx_tr">
<td id="S3.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.4.4" class="ltx_tr">
<td id="S3.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BERT<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib79" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S3.T2.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.5.5" class="ltx_tr">
<td id="S3.T2.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="24"><span id="S3.T2.1.1.5.5.1.1" class="ltx_text">AutoRegressive Language Model</span></td>
<td id="S3.T2.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="12"><span id="S3.T2.1.1.5.5.2.1" class="ltx_text">GPT Family</span></td>
<td id="S3.T2.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.5.5.3.1" class="ltx_text">GPT-3.5</span></td>
<td id="S3.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.6.6" class="ltx_tr">
<td id="S3.T2.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.7.7" class="ltx_tr">
<td id="S3.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">GPT-2<cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a href="#bib.bib95" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S3.T2.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.8.8" class="ltx_tr">
<td id="S3.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">GPT-Neo<cite class="ltx_cite ltx_citemacro_cite">Black et&nbsp;al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T2.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Mallen et&nbsp;al. (<a href="#bib.bib86" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.9.9" class="ltx_tr">
<td id="S3.T2.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">chatGPT</td>
<td id="S3.T2.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Peng et&nbsp;al. (<a href="#bib.bib93" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.10.10" class="ltx_tr">
<td id="S3.T2.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.10.10.1.1" class="ltx_text">GPT-4<cite class="ltx_cite ltx_citemacro_cite">Achiam et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite></span></td>
<td id="S3.T2.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Asai et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.11.11" class="ltx_tr">
<td id="S3.T2.1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.11.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.12.12" class="ltx_tr">
<td id="S3.T2.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.12.12.1.1" class="ltx_text">GPT-3<cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite></span></td>
<td id="S3.T2.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.12.12.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Madaan et&nbsp;al. (<a href="#bib.bib85" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.13.13" class="ltx_tr">
<td id="S3.T2.1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.13.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.14.14" class="ltx_tr">
<td id="S3.T2.1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">GPT<cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a href="#bib.bib94" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S3.T2.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.14.14.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.15.15" class="ltx_tr">
<td id="S3.T2.1.1.15.15.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.15.15.1.1" class="ltx_text">GPT-J</span></td>
<td id="S3.T2.1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.15.15.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Schick et&nbsp;al. (<a href="#bib.bib110" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.16.16" class="ltx_tr">
<td id="S3.T2.1.1.16.16.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.16.16.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib47" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.17.17" class="ltx_tr">
<td id="S3.T2.1.1.17.17.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="6"><span id="S3.T2.1.1.17.17.1.1" class="ltx_text">Llama Family</span></td>
<td id="S3.T2.1.1.17.17.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="4"><span id="S3.T2.1.1.17.17.2.1" class="ltx_text">Llama2<cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib126" title="" class="ltx_ref">2023b</a>)</cite></span></td>
<td id="S3.T2.1.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.17.17.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Yan et&nbsp;al. (<a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.18.18" class="ltx_tr">
<td id="S3.T2.1.1.18.18.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.18.18.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Asai et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.19.19" class="ltx_tr">
<td id="S3.T2.1.1.19.19.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.19.19.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.20.20" class="ltx_tr">
<td id="S3.T2.1.1.20.20.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.20.20.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.21.21" class="ltx_tr">
<td id="S3.T2.1.1.21.21.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.21.21.1.1" class="ltx_text">Llama<cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib125" title="" class="ltx_ref">2023a</a>)</cite></span></td>
<td id="S3.T2.1.1.21.21.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.21.21.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.22.22" class="ltx_tr">
<td id="S3.T2.1.1.22.22.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.22.22.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.23.23" class="ltx_tr">
<td id="S3.T2.1.1.23.23.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="6"><span id="S3.T2.1.1.23.23.1.1" class="ltx_text">Others</span></td>
<td id="S3.T2.1.1.23.23.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Alpaca<cite class="ltx_cite ltx_citemacro_cite">Dubois et&nbsp;al. (<a href="#bib.bib26" title="" class="ltx_ref">2024</a>)</cite>
</td>
<td id="S3.T2.1.1.23.23.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.23.23.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Yan et&nbsp;al. (<a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.24.24" class="ltx_tr">
<td id="S3.T2.1.1.24.24.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="2"><span id="S3.T2.1.1.24.24.1.1" class="ltx_text">OPT<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a href="#bib.bib151" title="" class="ltx_ref">2022</a>)</cite></span></td>
<td id="S3.T2.1.1.24.24.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.24.24.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.25.25" class="ltx_tr">
<td id="S3.T2.1.1.25.25.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.25.25.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.26.26" class="ltx_tr">
<td id="S3.T2.1.1.26.26.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">XGLM<cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib76" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T2.1.1.26.26.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.26.26.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.27.27" class="ltx_tr">
<td id="S3.T2.1.1.27.27.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BLOOM<cite class="ltx_cite ltx_citemacro_cite">Workshop et&nbsp;al. (<a href="#bib.bib132" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T2.1.1.27.27.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.27.27.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.28.28" class="ltx_tr">
<td id="S3.T2.1.1.28.28.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Mistral<cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib56" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S3.T2.1.1.28.28.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2024</td>
<td id="S3.T2.1.1.28.28.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib47" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.29.29" class="ltx_tr">
<td id="S3.T2.1.1.29.29.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2" rowspan="16"><span id="S3.T2.1.1.29.29.1.1" class="ltx_text">Encoder-Decoder Language Model</span></td>
<td id="S3.T2.1.1.29.29.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="9"><span id="S3.T2.1.1.29.29.2.1" class="ltx_text">T5<cite class="ltx_cite ltx_citemacro_cite">Raffel et&nbsp;al. (<a href="#bib.bib96" title="" class="ltx_ref">2020</a>)</cite></span></td>
<td id="S3.T2.1.1.29.29.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.29.29.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard et&nbsp;al. (<a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.30.30" class="ltx_tr">
<td id="S3.T2.1.1.30.30.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.30.30.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">HofstÃ¤tter et&nbsp;al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.31.31" class="ltx_tr">
<td id="S3.T2.1.1.31.31.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.31.31.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.32.32" class="ltx_tr">
<td id="S3.T2.1.1.32.32.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.32.32.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.33.33" class="ltx_tr">
<td id="S3.T2.1.1.33.33.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.33.33.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2022b</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.34.34" class="ltx_tr">
<td id="S3.T2.1.1.34.34.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.34.34.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Singh et&nbsp;al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.35.35" class="ltx_tr">
<td id="S3.T2.1.1.35.35.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2020</td>
<td id="S3.T2.1.1.35.35.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.36.36" class="ltx_tr">
<td id="S3.T2.1.1.36.36.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.36.36.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.37.37" class="ltx_tr">
<td id="S3.T2.1.1.37.37.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.37.37.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.38.38" class="ltx_tr">
<td id="S3.T2.1.1.38.38.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;" rowspan="7"><span id="S3.T2.1.1.38.38.1.1" class="ltx_text">BART<cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite></span></td>
<td id="S3.T2.1.1.38.38.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2021</td>
<td id="S3.T2.1.1.38.38.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.39.39" class="ltx_tr">
<td id="S3.T2.1.1.39.39.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.39.39.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et&nbsp;al. (<a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.40.40" class="ltx_tr">
<td id="S3.T2.1.1.40.40.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2019</td>
<td id="S3.T2.1.1.40.40.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.41.41" class="ltx_tr">
<td id="S3.T2.1.1.41.41.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2020</td>
<td id="S3.T2.1.1.41.41.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.42.42" class="ltx_tr">
<td id="S3.T2.1.1.42.42.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2023</td>
<td id="S3.T2.1.1.42.42.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.43.43" class="ltx_tr">
<td id="S3.T2.1.1.43.43.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2020</td>
<td id="S3.T2.1.1.43.43.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite></td>
</tr>
<tr id="S3.T2.1.1.44.44" class="ltx_tr">
<td id="S3.T2.1.1.44.44.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2022</td>
<td id="S3.T2.1.1.44.44.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Language Models</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Although humans used to rely solely on searching for information, the development of language models has revolutionised the field of natural language processing, making it more vibrant and creative. In contrast to LM, which employs solely the parameters derived from training to complete the task, RALM integrates the nonparametric memory acquired by the retriever with the parametric memory of LM itself to create a semiparametric memory, thereby enhancing the performance of the language model. In the RALM architecture, many researchers utilise off-the-shelf language models for evaluation. This section introduces the language models commonly used in RALM architectures and classifies them into three categories: AutoEncoderlanguage model, AutoRegressive language model and Encoder-Decoder model. Table <a href="#S3.T2" title="Table 2 â€£ 3.4 Hybrid Retrieval â€£ 3 Retriever â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lists information about specific applications of LM in the RALM.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>AutoEncoder Language Model</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The logical process of an AutoEncoder is that the original input (set to x) is weighted and mapped to y, which is then inversely weighted and mapped back to z. If through iterative training the loss function L(H) is minimised, i.e. z is as close to x as possible, i.e. x is perfectly reconstructed, then it can be said that forward weighting is a successful way of learning the key features of the input. AutoEncoder language models take their name from the Denoising AutoEncoder (DAE) <cite class="ltx_cite ltx_citemacro_cite">Vincent et&nbsp;al. (<a href="#bib.bib128" title="" class="ltx_ref">2008</a>)</cite>, which is used to predict tokens that are [masked] by contextual words (these [masked] words are actually noise added at the input, typical of thinking). DAE is a technique that involves adding random noise to the input layer of data. This helps to learn more robust features when using an unsupervised approach to pre-train the weights of a deep network in a hierarchical manner.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Most of AutoEncoder language models are highly generalisable, unsupervised, and do not require data annotation. They can naturally incorporate contextual semantic information. However, the independence assumption introduced in the Pre-Training stage means that the correlation between predicted [MASK] is not considered. Additionally, the introduction of [Mask] as a special marker in the input to replace the original Token creates inconsistency between the data in the Pre-Training stage and the Fine-Tuning stage, where [Mask] is not present. Self-encoding language models are commonly used in RALM architectures for natural language understanding (NLU) tasks.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">As AutoEncoder language models excel at Natural Language Understanding (NLU) tasks, many RALM architectures<cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite>utilise them for specific tasks, such as judgement. One of the most commonly used models is BERT and its improved versions.<cite class="ltx_cite ltx_citemacro_citet">Devlin et&nbsp;al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite> proposed the BERT model, which was inspired by closed tasks<cite class="ltx_cite ltx_citemacro_cite">Taylor (<a href="#bib.bib122" title="" class="ltx_ref">1953</a>)</cite>. RoBERTa<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib79" title="" class="ltx_ref">2019</a>)</cite> is trained using dynamic masking, full sentences without NSP loss, large mini-batches, and a larger byte-level BPE to address the lack of training of Bertâ€™s model. According to <cite class="ltx_cite ltx_citemacro_citet">Jiang et&nbsp;al. (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite>, BERT heavily relies on global self-attention blocks, resulting in a large memory footprint and computational cost. Although all attention heads query the entire input sequence, some only need to learn local dependencies, leading to computational redundancy. To address this issue, They proposed a new span-based dynamic convolution to replace these self-attention heads and directly model local dependencies. The new convolutional head, along with other self-attentive heads, forms a hybrid attention block. Furthermore, <cite class="ltx_cite ltx_citemacro_citet">Sanh et&nbsp;al. (<a href="#bib.bib107" title="" class="ltx_ref">2019</a>)</cite> was able to decrease the size of the BERT model by 40% while maintaining 97% of its language comprehension abilities and achieving a 60% increase in speed by implementing knowledge distillation during the pre-training phase.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>AutoRegressive Language Model</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The primary purpose of an AutoRegressive language model is to predict the next word based on the preceding words. This is commonly known as left-to-right language modelling, where the token at the current time t is predicted based on the first t-1 tokens.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">This model has the advantage of being left-to-right, which is beneficial for generative natural language processing tasks like dialog generation and machine translation. AutoRegressive language models are well-suited to this process, making this model a popular choice for NLG tasks in the field of RALM. However, The information in question can be utilized only from the preceding or following text, and not in combination with both. OpenAI has made a notable impact on the field of research pertaining to autoregressive language models. Recently, Google has also made advancements in research on the model.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The GPT family is one of the most common examples of AutoRegressive language models. It was first proposed by <cite class="ltx_cite ltx_citemacro_citet">Radford et&nbsp;al. (<a href="#bib.bib94" title="" class="ltx_ref">2018</a>)</cite>, who identified a basic architecture of unsupervised pre-training followed by fine-tuning. <cite class="ltx_cite ltx_citemacro_citet">Radford et&nbsp;al. (<a href="#bib.bib95" title="" class="ltx_ref">2019</a>)</cite> later proposed zero-shot learning based on GPT. Later, <cite class="ltx_cite ltx_citemacro_citet">Brown et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> proposed GPT-3 using an approach similar to <cite class="ltx_cite ltx_citemacro_citet">Radford et&nbsp;al. (<a href="#bib.bib95" title="" class="ltx_ref">2019</a>)</cite>, which involved scaling up and abandoning fine-tuning. They also utilized alternately dense and locally banded sparse attentional patterns in the transformer layer, similar to the sparse transformer <cite class="ltx_cite ltx_citemacro_cite">Child et&nbsp;al. (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>. There are also several related studies, such as GPT-NEO<cite class="ltx_cite ltx_citemacro_cite">Black et&nbsp;al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> and ChatGPT, which use Reinforcement Learning from Human Feedback (RLHF). RLHF has significantly enhanced the accuracy of GPT models. Although ChatGPT is not open source, many researchers <cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite> still use its API for generative tasks in RALM. Recently, report on GPT-4<cite class="ltx_cite ltx_citemacro_cite">Achiam et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> have appeared, with the main focus on building a predictable and scalable deep learning stack dedicated to improving GPT-4â€™s safety and alignment. Many researchers <cite class="ltx_cite ltx_citemacro_cite">Asai et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite> have recently used GPT-4 to generate prompts for RALM.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">The Llama family is a well-known class of AutoRegressive language models. Llama<cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib125" title="" class="ltx_ref">2023a</a>)</cite> was first proposed as a language model that uses only publicly available data. To improve training stability, they normalise the input of each transformer sub-layer instead of normalising the output. They use the RMSNorm normalisation function introduced by <cite class="ltx_cite ltx_citemacro_citet">Zhang and Sennrich (<a href="#bib.bib150" title="" class="ltx_ref">2019</a>)</cite> and replace the ReLU non-linearity with the SwiGLU activation function introduced by <cite class="ltx_cite ltx_citemacro_citet">Shazeer (<a href="#bib.bib113" title="" class="ltx_ref">2020</a>)</cite> to improve performance. Furthermore, the authors replaced the absolute position embedding with the rotational position embedding (RoPE) introduced by <cite class="ltx_cite ltx_citemacro_citet">Su et&nbsp;al. (<a href="#bib.bib121" title="" class="ltx_ref">2024</a>)</cite> at each layer of the network. Llama2<cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib126" title="" class="ltx_ref">2023b</a>)</cite> used supervised fine-tuning, initial and iterative reward modelling and RLHF in their experiments. they also invented a new technique, Ghost Attention (GAtt), which helps to control the flow of dialogue in multiple turns. Qwen<cite class="ltx_cite ltx_citemacro_cite">Bai et&nbsp;al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, based on Llama, the following adjustments were made: 1. The method of loose embedding was chosen instead of bundling the weights of input embedding and output projection to save memory cost. 2. The accuracy of the inverse frequency matrix was improved. 3. For most of the layers, bias was eliminated as per <cite class="ltx_cite ltx_citemacro_citet">Chowdhery et&nbsp;al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. However, bias was added in the QKV layer to enhance the modelâ€™s extrapolation capability. The traditional layer normalization technique described in <cite class="ltx_cite ltx_citemacro_citet">Ba et&nbsp;al. (<a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite> was replaced with RMSNorm. They chose SwiGLU as their activation function, which is a combination of Swish <cite class="ltx_cite ltx_citemacro_citet">Ramachandran et&nbsp;al. (<a href="#bib.bib99" title="" class="ltx_ref">2017</a>)</cite> and gated linear units <cite class="ltx_cite ltx_citemacro_cite">Dauphin et&nbsp;al. (<a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>. The dimension of the feed-forward network (FFN) is also reduced. Furthermore, Mistral 7b <cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib56" title="" class="ltx_ref">2023a</a>)</cite> utilises Grouped Query Attention (GQA) to enhance inference speed and combines it with Sliding Window Attention (SWA) to efficiently process sequences of any length with reduced inference cost. These techniques demonstrate superior performance over Llama2.The Llama model is open source and uses publicly available data, providing researchers <cite class="ltx_cite ltx_citemacro_cite">Yan et&nbsp;al. (<a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Asai et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite> with more opportunities to expand. As a result, many researchers use the Llama family as language models in the RALM architecture.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Encoder-Decoder Language Model</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Transformer<cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a href="#bib.bib127" title="" class="ltx_ref">2017</a>)</cite> is an â€encoder-decoderâ€ architecture, which consists of encoders and decoders superimposed on multi-head self-attention modules. Among them, the input sequence is divided into two parts, the source sequence and the destination sequence. The former is input to the encoder and the latter is input to the decoder, and both sequences need to embed representation and add position information. The Transformer architecture enables parallel computation and the processing of entire text sequences simultaneously, resulting in a significant increase in model training and inference speed.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Raffel et&nbsp;al. (<a href="#bib.bib96" title="" class="ltx_ref">2020</a>)</cite> introduces a unified framework for converting all text-based language problems into text-to-text format. The aim is to explore the potential of transfer learning techniques for natural language processing. In contrast to the original transformer, a simplified version of layer normalization is used, where activations are rescaled without additional biases. After applying layer normalization, a residual skip connection <cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib41" title="" class="ltx_ref">2016</a>)</cite> adds the input of each subcomponent to its output. <cite class="ltx_cite ltx_citemacro_citet">Srivastava et&nbsp;al. (<a href="#bib.bib120" title="" class="ltx_ref">2014</a>)</cite> is applied to the feed-forward network, the skip connections, the attentional weights, and the inputs and outputs of the entire stack. The T5 model has been widely used as a language model by many researchers, such as <cite class="ltx_cite ltx_citemacro_citet">HofstÃ¤tter et&nbsp;al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Singh et&nbsp;al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>)</cite>. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Chung et&nbsp;al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite> proposed instruction tuning as an approach to improve model performance. The study focused on three aspects: the number of scaling tasks, the size of the scaled model, and the fine-tuning of chain of thought data. The results showed that larger model sizes and more fine-tuning tasks significantly improved model performance. Additionally, the study found that chain of thought(CoT) significantly improves inference level. <cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite> used this approach to tune T5 and apply it to the RALM architecture.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">BART<cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite> is an Encoder-Decoder model that allows for arbitrary noise transformations, as the input to the encoder does not need to align with the output of the decoder. In this case, the document is corrupted by replacing the text span with mask symbols. For pre-training, the researchers proposed five models: Token Masking, Token Deletion, Text Infilling, Sentence Permutation, and Document Rotation. For fine-tuning, the encoder and decoder are fed an uncorrupted document, and the representation of the final hidden state from the decoder is used. Many researchers<cite class="ltx_cite ltx_citemacro_cite">Thulke et&nbsp;al. (<a href="#bib.bib124" title="" class="ltx_ref">2021</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et&nbsp;al. (<a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite> have adopted BART as the language model in the RALM architecture due to its comprehensive and novel pre-training approach, which greatly enhances the modelâ€™s robustness.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2404.19543/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="144" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Classification of RALM enhancement methods.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>RALM Enhancement</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section describes how researchers in the RALM architecture improved the output quality by enhancing its components. We divided the improvement method into three parts: Retriever Enhancement, LM Enhancement, and Overall Enhancement. Figure <a href="#S4.F4" title="Figure 4 â€£ 4.3 Encoder-Decoder Language Model â€£ 4 Language Models â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the categorization of enhancement methods.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Retriever Enhancement</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">This section presents the researchersâ€™ efforts on the retriever side, which include Retrieval Quality Control and Retrieval Timing Optimization.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Retrieval Quality Control</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Shi et&nbsp;al. (<a href="#bib.bib116" title="" class="ltx_ref">2023a</a>)</cite> argue that retrieval can produce documents that not only fail to provide helpful information but can also compromise the quality of the language model output. As a result, many scholars in the field of RALM focus on improving the relevance between the retrieved content and the userâ€™s input to enhance the final outputâ€™s quality.</p>
</div>
<div id="S5.SS1.SSS1.p2" class="ltx_para">
<p id="S5.SS1.SSS1.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite> propose an approach for instruction-tuning. They update the query encoder using a generalised LM supervised retrieval (LSR) <cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite>training target that completes the computation through a combination of supervised tasks and unsupervised text. This enables the retriever to produce more contextually relevant results that are consistent with LLM preferences. Inspired by this instruction-tuning approach, <cite class="ltx_cite ltx_citemacro_citet">Asai et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> proposed a more sophisticated model trained on an instruction-tracking dataset: SELF-RAG. As a result of their refinements, SELF-RAG can retrieve and select the best possible model outputs on demand through fine-grained self-reflection, making it broadly applicable, more robust, and controllable. In contrast to approaches that aim to enhance the quality of retrieved documents through the use of external models, such as natural language inference <cite class="ltx_cite ltx_citemacro_cite">Yoran et&nbsp;al. (<a href="#bib.bib144" title="" class="ltx_ref">2023</a>)</cite> and summarization<cite class="ltx_cite ltx_citemacro_cite">Xu et&nbsp;al. (<a href="#bib.bib134" title="" class="ltx_ref">2023</a>)</cite> models, SELF-RAG proposes entirely novel ideas. The model divides the retrieved document into parallel segments and compares their relevance. It then combines the most similar parts of the document. <cite class="ltx_cite ltx_citemacro_citet">Yan et&nbsp;al. (<a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite> improves on SELF-RAG by designing a correction strategy to address inaccurate retriever results. They classify the information into three categories: CORRECT, INCORRECT, and AMBIGUOUS. If the information is CORRECT, the document is refined and filtered. If it is INCORRECT, the document is discarded and the web is searched for retrieval. The term AMBIGUOUS indicates a lack of confidence in the accuracy of a judgement. In this case, a combination of the two methods mentioned above will be used. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite> proposed FILCO, a method for retrieving document content with sentence precision through three filters: STRINC, lexical overlap, and conditional cross-mutual information (CXMI).</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Retrieval Timing Optimization</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">Researchers typically consider the timing of retrieval in two situations: when working on tasks that require multiple retrievals, such as long dialogue generation and multi-hop problems, or when it is impossible to find a suitable and relevant document. Using irrelevant documents can harm the accuracy of the output.</p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.p2.1" class="ltx_p">A simple way to determine the timing of a retrieval is to adjust the retrieval steps. <cite class="ltx_cite ltx_citemacro_citet">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite> utilised an approach that involved a prefix encoding to adjust the runtime cost. The prefix encoding of the generated content was constantly recalculated. The choice of retrieval stride is a trade-off between runtime and performance. According to Toolformer <cite class="ltx_cite ltx_citemacro_cite">Schick et&nbsp;al. (<a href="#bib.bib110" title="" class="ltx_ref">2024</a>)</cite>, the search command can be used directly to retrieve useful information when the model needs to retrieve documentation help in the process of generating content. Inspired by this idea, <cite class="ltx_cite ltx_citemacro_citet">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite> propose two methods for determining the timing of retrieval. The first method involves interrupting the generation of the LM when it encounters a place where a retrieval needs to be performed and then performing the retrieval operation. The second method involves generating a temporary sentence in its entirety. If there is a low confidence marker in the sentence, the marker is masked and the rest of the sentence is used for retrieval. <cite class="ltx_cite ltx_citemacro_citet">Yu et&nbsp;al. (<a href="#bib.bib146" title="" class="ltx_ref">2023</a>)</cite> also employed LM to determine the timing of retrieval. However, instead of generating low-confidence markers using LM, they had LM score the output before and after retrieval.<cite class="ltx_cite ltx_citemacro_citet">Mallen et&nbsp;al. (<a href="#bib.bib86" title="" class="ltx_ref">2022</a>)</cite>â€™s approach differed from the traditional method of having LMs generate low-confidence markers. Instead, they used Wikipedia page views as a measure of prevalence and converted knowledge triples of wiki data with varying levels of prevalence into natural language questions anchored to the original entity and relation types. This approach is more objective and avoids subjective evaluations. For tasks that required reasoning, both <cite class="ltx_cite ltx_citemacro_citet">He et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite> used chain of thought(CoT) to determine when to perform a retrieval.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>LM Enhancement</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">This section presents the researchersâ€™ efforts in language modelling, including Pre-Generation Retrieval Processing, Structural Model Optimization, and Post-Generation Output Enhancement.</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Pre-Generation Retrieval Processing</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">The RALM architecture initially used a single document for retrieval augmentation. However, it was discovered that RALMâ€™s performance significantly improved when the number of retrieved paragraphs was increased. <cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib53" title="" class="ltx_ref">2020b</a>)</cite> Therefore, they proposed a new method called Fusion-in-Decoder (FiD) which involves keeping the retriever unchanged, using the encoder in LM to encode the related documents one by one, and then connecting the related documents and giving them to the decoder for output. Then <cite class="ltx_cite ltx_citemacro_citet">HofstÃ¤tter et&nbsp;al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite> improved on the FiD. They constrained the information flow from encoder to decoder. FiD-Light with reranking was also tuned via text source pointers to improve the topmost source accuracy. <cite class="ltx_cite ltx_citemacro_citet">Izacard and Grave (<a href="#bib.bib52" title="" class="ltx_ref">2020a</a>)</cite> applied knowledge distillation to the FiD model, also known as FiD-KD, using cross-attention scores from a sequence-to-sequence reader to obtain synthetic targets for the retriever. <cite class="ltx_cite ltx_citemacro_citet">Singh et&nbsp;al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>)</cite> proposed an enhancement approach that differs from knowledge distillation in that it uses an end-to-end training approach requiring fewer documents, training cycles, and no supervised initialization compared to FiD-KD.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Structural Model Optimization</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">As language models continue to evolve at an accelerated pace, an increasing number of large models with high parameter counts and exceptional performance are emerging. Tuning the parameters and internal structure of these models has become increasingly difficult and inefficient, making instruction tuning more important than ever.</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p">FLAN<cite class="ltx_cite ltx_citemacro_cite">Chung et&nbsp;al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite> is one of the most systematic and comprehensive approaches among the many studies on instruction tuning. This approach fine-tunes the language model on the instruction-optimised dataset, scales the number of tasks and model size, and incorporates chain-of-thought data in the fine-tuning. Although the authors did not consider a specific approach to tuning instructions in RALM architecture, their work provides a valuable reference for future research. In the instruction fine-tuning of RALM, <cite class="ltx_cite ltx_citemacro_citet">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite> integrated in-context retrieval augmentation. This greatly reduces the likelihood of the language model being misled by irrelevant retrieval content. SAIL<cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite> builds language generation and instruction tracking capabilities on complex search results generated by internal and external search engines. Using a corpus of instruction tuning, they collect search results for each training case from different search APIs and domains, and construct a search-based training set containing a triplet of (instruction, grounding information, response). In contrast to training on instruction-tuned datasets, <cite class="ltx_cite ltx_citemacro_citet">Madaan et&nbsp;al. (<a href="#bib.bib85" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> propose to prompt large models directly from retrieved knowledge. <cite class="ltx_cite ltx_citemacro_citet">Madaan et&nbsp;al. (<a href="#bib.bib85" title="" class="ltx_ref">2022</a>)</cite> used GPT-3 to clarify memory pairings of recorded cases where the model misinterpreted the userâ€™s intention, as well as user feedback. This ensures that their system can generate enhanced prompts for each new query based on user feedback. In contrast, <cite class="ltx_cite ltx_citemacro_citet">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> uses few-shot prompts and answer reordering to improve inference computation.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>Post-Generation Output Enhancement</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">As defined in Section 2 on Parallel Interaction, this interaction is inspired by the K-Nearest Neighbor (KNN) LM <cite class="ltx_cite ltx_citemacro_cite">Khandelwal et&nbsp;al. (<a href="#bib.bib63" title="" class="ltx_ref">2019</a>)</cite>. It is a paradigmatic instance of RALM, wherein the LM is employed solely to enhance the outcomes. Since the proposal of KNN-LM, many researchers have worked to optimize the model. In this section, we will describe the landmark work in detail.</p>
</div>
<div id="S5.SS2.SSS3.p2" class="ltx_para">
<p id="S5.SS2.SSS3.p2.1" class="ltx_p">The KNN-LM approach involves linearly interpolating the extended neural language model with the K-Nearest Neighbours in the pre-trained LM embedding space. <cite class="ltx_cite ltx_citemacro_citet">Zhong et&nbsp;al. (<a href="#bib.bib156" title="" class="ltx_ref">2022</a>)</cite> proposed different processing for three types of memories (local, long-term and external) and added training for in-batch tokens to KNN-LM. The proposed changes aim to improve the performance of the model. Unlike KNN-LM, which only uses memory units during training, TRIME <cite class="ltx_cite ltx_citemacro_cite">Zhong et&nbsp;al. (<a href="#bib.bib156" title="" class="ltx_ref">2022</a>)</cite> uses memory units during both testing and training. <cite class="ltx_cite ltx_citemacro_citet">He et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> suggested that not all generated tokens need to be retrieved. Instead, a lightweight neural network can be trained to aid the KNN-LM in adaptive retrieval. Additionally, efficiency can be improved through database streamlining and dimension reduction. <cite class="ltx_cite ltx_citemacro_citet">Alon et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite> proposed RETOMATON, an unsupervised, weighted finite automaton built on top of the data store. RETOMATON is based on saving pointers between successive data store entries and clustering techniques. RETOMATON is more effective than ADAPTRET<cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> in improving accuracy by utilizing remaining pointers during KNN retrieval. Even without KNN retrieval, interpolation operations can still be performed using the stored previous information in the pointers, unlike ADAPTRET which solely relies on the language model. Furthermore, RETOMATON is unsupervised, requiring no additional data for training, making it more data-efficient. <cite class="ltx_cite ltx_citemacro_citet">Grave et&nbsp;al. (<a href="#bib.bib35" title="" class="ltx_ref">2016</a>)</cite> proposed using continuous cache to improve the performance of KNN-LM. This involves storing past hidden activations and accessing them at the appropriate time by dot product with present hidden activation. <cite class="ltx_cite ltx_citemacro_citet">Yogatama et&nbsp;al. (<a href="#bib.bib143" title="" class="ltx_ref">2021</a>)</cite> utilise an extended short-term context by caching local hidden states and global long-term memory by retrieving a set of nearest-neighbour tokens at each time step. They also design a gating function to adaptively combine multiple sources of information for prediction. Compared to KNN-LM, this method uses dynamic weights and can handle cases where interpolation is not feasible, such as when the memory output is an image, video, or sound. <cite class="ltx_cite ltx_citemacro_citet">Drozdov et&nbsp;al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> proposed a method for adjusting the interpolation weights. The weights are dynamically adjusted based on the size of the region of overlap between the retrieved stored data and the assessment set, which reflects the quality of the retrieval.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Overall Enhancement</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">This section presents the researchersâ€™ efforts on the RALM architecture as a whole, including End-to-End Training and Build intermediate modules.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>End-to-End Training</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">Researchers have begun working on a method called end-to-end training, which aims to minimise manual intervention and focus solely on data. This method utilises deep learning and is becoming increasingly popular due to the growing amount of available data. During research on RALM architectures, many researchers tend to use end-to-end training methods to achieve better results.</p>
</div>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Guu et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> were among the first researchers to apply end-to-end training to the field of RALM. However, they differed in their approach. REALM <cite class="ltx_cite ltx_citemacro_cite">Guu et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> used masked language training in the pre-training phase and included a retriever that can be trained end-to-end. In the fine-tuning phase, only the QA task was targeted while keeping the retriever frozen. On the other hand, RAG <cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite> used an already trained retriever, DPR, and only employed BART for partial end-to-end training. Similar to REALM, <cite class="ltx_cite ltx_citemacro_citet">Sachan et&nbsp;al. (<a href="#bib.bib106" title="" class="ltx_ref">2021</a>)</cite> present an unsupervised pre-training method that involves an inverse cloze task and masked salient spans. This is followed by supervised fine-tuning using question-context pairs. In addition, they find that the use of end-to-end trained retrievers resulted in a significant improvement in performance across tasks. <cite class="ltx_cite ltx_citemacro_citet">Singh et&nbsp;al. (<a href="#bib.bib118" title="" class="ltx_ref">2021</a>)</cite> apply end-to-end training to multi-document processing, in their proposed approach, the value of the latent variable, which represents the set of relevant documents for a given question, is estimated iteratively. This estimate is then used to update the parameters of the retriever and reader. <cite class="ltx_cite ltx_citemacro_citet">Siriwardhana et&nbsp;al. (<a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite> describe the end-to-end optimization of RAG from previous studies and introduces an auxiliary training signal to incorporate more domain-specific knowledge. This signal forces RAG-end2end to reconstruct a given sentence by accessing relevant information in an external knowledge base. This approach has greatly improved domain adaptability.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2404.19543/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Classification of RALM data sources.</figcaption>
</figure>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>Intermediate Modules</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">Recently, some researchers have constructed an intermediate module to coordinate the activities of both the retriever and the language model due to space or black-box LLM constraints, without improving either.</p>
</div>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p id="S5.SS3.SSS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite> present Selfmem, a model designed to tackle the issue of low corpus quality. Selfmem utilises a retrieval-enhanced generator to create an infinite pool of memory, which is then used by a memory selector to choose an output for subsequent generations. This approach enables the model to use its own output to enhance generation. <cite class="ltx_cite ltx_citemacro_citet">Peng et&nbsp;al. (<a href="#bib.bib93" title="" class="ltx_ref">2023</a>)</cite> propose an AI agent that formulates human system dialogue as a Markov Decision Process (MDP) described by a quintuple. The quintuple includes an infinitely large set of dialogue states, a collection of historical behaviours, a probability of state transfer, external rewards obtained, and a variable parameter.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Data Sources</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This section will introduce some of the data sources commonly used in RALM and categorise them into structured and unstructured data. Figure <a href="#S5.F5" title="Figure 5 â€£ 5.3.1 End-to-End Training â€£ 5.3 Overall Enhancement â€£ 5 RALM Enhancement â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the categorization of data sources.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Structured Data</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">Structured data includes various structures, such as tables and knowledge graphs. The benefit of this type of data is its clear structure, typically in tabular form, with each field precisely defined. It is appropriate for storing numbers, dates, text, and other data types. Structured data can be easily queried, analysed, and processed using a structured query language like SQL.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Natural Questions(NQ) <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et&nbsp;al. (<a href="#bib.bib66" title="" class="ltx_ref">2019</a>)</cite> is a very well-known dataset in the NLU field.The given text describes a structured question and a corresponding Wikipedia page. The page is annotated with a long answer, typically a paragraph, and a short answer consisting of one or more entities. If there is no long or short answer, it is labelled as empty. Due to the reliability of the Google search engine and its vast amount of data, many scholars have used this dataset to train RALM, such as <cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Izacard et&nbsp;al. (<a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite>. HotpotQA(HQA) <cite class="ltx_cite ltx_citemacro_cite">Yang et&nbsp;al. (<a href="#bib.bib137" title="" class="ltx_ref">2018</a>)</cite> stores information about multi-hop questions and provides sentence-level supporting facts needed for inference. The structure includes the paragraph, question, answer, and sentence number that supports the answer. This dataset has been used by many researchers, such as <cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Feng et&nbsp;al. (<a href="#bib.bib29" title="" class="ltx_ref">2024</a>)</cite>, to train RALM for multi-hop question answering. Another significant form of structured data is the Knowledge Graph. It is a data structure that primarily consists of triples of (entities, relationships, attributes). Some of the most frequently used datasets include Wikidata5M, WikiKG90Mv2, OpendialKG, and KOMODIS. All of these models <cite class="ltx_cite ltx_citemacro_cite">Kang et&nbsp;al. (<a href="#bib.bib60" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Yu and Yang (<a href="#bib.bib145" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib42" title="" class="ltx_ref">2024</a>)</cite> rely on knowledge graphs as a data source.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Unstructured Data</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Unstructured data, in contrast, does not have a clearly defined data structure and exists in various forms, including text, images, and audio. Due to its large and diverse nature, it is challenging to store and manage in traditional tabular form. Although it contains valuable information, it requires natural language processing, image recognition, and other technologies to parse and comprehend.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Several RALM researchers, including <cite class="ltx_cite ltx_citemacro_citet">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a href="#bib.bib129" title="" class="ltx_ref">2023a</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Yang et&nbsp;al. (<a href="#bib.bib136" title="" class="ltx_ref">2023</a>)</cite>, have used this dataset as a source of data. The FEVER <cite class="ltx_cite ltx_citemacro_cite">Thorne et&nbsp;al. (<a href="#bib.bib123" title="" class="ltx_ref">2018</a>)</cite> dataset is mainly used for fact extraction and validation. Several RALM researchers, including <cite class="ltx_cite ltx_citemacro_citet">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Izacard et&nbsp;al. (<a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite>, have used the factual text in this dataset as a source of data. In addition to unstructured text, there is also a significant amount of inherently less structured data, such as images, videos, and audio. Several common image datasets are available for use in research, including MNIST, CIFAR-10, Pascal VOC, and COCO. Many studies <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib17" title="" class="ltx_ref">2022b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Yasunaga et&nbsp;al. (<a href="#bib.bib139" title="" class="ltx_ref">2022</a>)</cite> in the field of RALM have utilized these datasets. Common audio datasets used in speech research include LJ Speech, JSUT, and RUSLAN. Many studies <cite class="ltx_cite ltx_citemacro_cite">Yuan et&nbsp;al. (<a href="#bib.bib147" title="" class="ltx_ref">2024</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Huang et&nbsp;al. (<a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Ghosh et&nbsp;al. (<a href="#bib.bib32" title="" class="ltx_ref">2024</a>)</cite> in the field also rely on audio data as a primary source. Common video datasets used in research include HMDB, UCF101, and ASLAN. Many studies <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Yin et&nbsp;al. (<a href="#bib.bib141" title="" class="ltx_ref">2019</a>)</cite> in the field of RALM utilize audio data as a source of information.</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="/html/2404.19543/assets/x6.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="498" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Classification of RALM applications.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Applications</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This section provides a summary of the downstream tasks that the RALM architecture primarily focuses on. The relevant application directions are categorized according to the requirements for model generation or comprehension. RALM on NLG indicates that the accomplishment of the task primarily depends on the generative capabilities. Conversely, RALM on NLU indicates that the accomplishment of the task primarily depends on the comprehension capabilities. Finally, RALM on both NLU and NLG indicates that the task is generally handled in two ways, one that relies primarily on comprehension capabilities and one that relies primarily on generative capabilities. Figure <a href="#S6.F6" title="Figure 6 â€£ 6.2 Unstructured Data â€£ 6 Data Sources â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the categorization of applications.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>RALM on NLG Tasks</h3>

<section id="S7.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1 </span>Machine Translation</h4>

<div id="S7.SS1.SSS1.p1" class="ltx_para">
<p id="S7.SS1.SSS1.p1.1" class="ltx_p">Machine translation, also known as automatic translation, is the process of converting one natural language (source language) into another natural language (target language) using a computer. It is a branch of computational linguistics, one of the ultimate goals of artificial intelligence, and has important scientific research value. Machine translation systems can be divided into two categories: rule-based and corpus-based. The former comprises a dictionary and a rule base, which collectively constitute the knowledge source. In contrast, the latter comprises a corpus that has been divided and labeled, and which does not require a dictionary or rules. Instead, it is based on statistical laws and most RALMs accomplish this task based on rules.</p>
</div>
<div id="S7.SS1.SSS1.p2" class="ltx_para">
<p id="S7.SS1.SSS1.p2.1" class="ltx_p">The Selfmem <cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite> system employs two distinct language models for the machine translation task. The first is a trainable mini-model, which has been trained using a joint and bipartite approach, respectively. The second is a few-shot prompted LLM. Ultimately, Selfmem has demonstrated a notable enhancement in its performance across all four translation directions and for both training architectures. This outcome suggests that enhanced memory capabilities often result in superior generation outcomes. In order to achieve the best results, TRIME <cite class="ltx_cite ltx_citemacro_cite">Zhong et&nbsp;al. (<a href="#bib.bib156" title="" class="ltx_ref">2022</a>)</cite> used the IWSLTâ€™14 De-En baseline. Given that the task is sentence-level, the researchers did not use local memory and long-term memory, as there are few repetitive tokens in them. Instead, they used only external memory, which enabled them to beat the KNN-MT <cite class="ltx_cite ltx_citemacro_cite">Khandelwal et&nbsp;al. (<a href="#bib.bib62" title="" class="ltx_ref">2020</a>)</cite> in performance.</p>
</div>
</section>
<section id="S7.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2 </span>Math Teaching</h4>

<div id="S7.SS1.SSS2.p1" class="ltx_para">
<p id="S7.SS1.SSS2.p1.1" class="ltx_p">As the RALM architecture continues to evolve, an increasing number of potential application directions are being identified. Levonian <cite class="ltx_cite ltx_citemacro_cite">Levonian et&nbsp;al. (<a href="#bib.bib69" title="" class="ltx_ref">2023</a>)</cite> were inspired by RALM to apply this architecture to the domain of mathematics teaching and learning. To address the fact that the knowledge stored in the LLM may not match what is taught in schools, they used one of three prompted instructional conditions to generate responses to math student queries using a retrieval enhancement generation system. Survey respondents ranked the responses according to preference and evaluated basic math textbooks as a retrieval corpus.</p>
</div>
</section>
<section id="S7.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3 </span>Dialog Generation</h4>

<div id="S7.SS1.SSS3.p1" class="ltx_para">
<p id="S7.SS1.SSS3.p1.1" class="ltx_p">Dialog generation, in particular lengthy dialogue, is a challenging task. This is due to the necessity of not only ensuring that the language model possesses natural language processing capabilities, but also that the model is able to utilise context in order to satisfy the requirements of the dialogue.</p>
</div>
<div id="S7.SS1.SSS3.p2" class="ltx_para">
<p id="S7.SS1.SSS3.p2.1" class="ltx_p">FILCO <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite> employs the Wikipedia dataset from the KILT benchmark, referred to as the â€Wizard of Wikipediaâ€ (WoW), to generate subsequent dialogue. This process involves basing the output on a Wikipedia article, with the input comprising the history of multiple rounds of dialogue. RA-DIT <cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite> also employs the WoW dataset in the fine-tuning phase. As a result of the command tuning operation, the model outperforms Llama <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib125" title="" class="ltx_ref">2023a</a>)</cite> and Llama-REPLUG <cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite> with the same parameters for dialogue generation in the zero-shot condition. The incorporation of Selfmem <cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite> into the retrieval-augmented generator markedly enhances the generation of dialogue, as a consequence of its remarkable flexibility. This is achieved by the direct optimisation of memory for the desired properties of diverse and information-rich dialogues. In contrast, SURGE <cite class="ltx_cite ltx_citemacro_cite">Kang et&nbsp;al. (<a href="#bib.bib60" title="" class="ltx_ref">2023</a>)</cite> employs the Knowledge Graph as a data source for the dialogue generation task, wherein each dialogue round comprises facts from a large-scale KG. In contrast to other related work <cite class="ltx_cite ltx_citemacro_cite">Rony et&nbsp;al. (<a href="#bib.bib104" title="" class="ltx_ref">2022</a>)</cite>, they retrieve only contextually relevant subgraphs, thus avoiding the computational overheads and misleading models that can result from retrieving irrelevant data.</p>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>RALM on NLU Tasks</h3>

<section id="S7.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1 </span>Slot Filling</h4>

<div id="S7.SS2.SSS1.p1" class="ltx_para">
<p id="S7.SS2.SSS1.p1.1" class="ltx_p">Slot filling is a technique employed in natural language processing for the purpose of recognizing and extracting specific information from user-supplied text or speech. In slot filling, the system defines a set of slots in advance, with each slot representing a specific information requirement. These requirements may include, but are not limited to, date, time, location, and so forth. Upon receipt of a user input in the form of text or speech, the system performs an analysis of the content, attempting to identify information that matches the predefined slots or classification labels <cite class="ltx_cite ltx_citemacro_cite">Lu et&nbsp;al. (<a href="#bib.bib81" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib80" title="" class="ltx_ref">a</a>)</cite>. This information is then populated into the corresponding slots for subsequent processing and response.</p>
</div>
<div id="S7.SS2.SSS1.p2" class="ltx_para">
<p id="S7.SS2.SSS1.p2.1" class="ltx_p">KGI <cite class="ltx_cite ltx_citemacro_cite">Glass et&nbsp;al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> enhances dense channel retrieval through the utilization of hard negatives in dense indexing and implements a robust training process for retrieval enhancement generation. The retrieval enhancement is employed to enhance the effectiveness of the slot-filling task, thereby facilitating the generation of high-quality knowledge graphs by AI. The results demonstrate that the method achieves excellent performance in TREx and zsRE datasets and exhibits remarkable robustness in TACRED dataset.</p>
</div>
</section>
<section id="S7.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2 </span>Image Generation</h4>

<div id="S7.SS2.SSS2.p1" class="ltx_para">
<p id="S7.SS2.SSS2.p1.1" class="ltx_p">The process of text-to-image generation is a challenging one, requiring a model to demonstrate a high degree of natural language understanding and to convey this understanding through an image, in contrast to the typical format of textual data.</p>
</div>
<div id="S7.SS2.SSS2.p2" class="ltx_para">
<p id="S7.SS2.SSS2.p2.1" class="ltx_p">In a pioneering study, <cite class="ltx_cite ltx_citemacro_citet">Li et&nbsp;al. (<a href="#bib.bib72" title="" class="ltx_ref">2022a</a>)</cite> proposed the use of retrieval techniques to enhance the quality of text-to-image generation. They conducted a comparative analysis of the quality and quantity of the generated images with mainstream models on the CUB and COCO datasets. Their findings demonstrated that all models outperformed their contemporaries. In contrast, RE-IMAGEN <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2022c</a>)</cite> focused on assisting the model in generating images of uncommon objects through retrieval. This approach ultimately led to the achievement of exceptionally high FID scores on the COCO and WikiImage datasets. Even more groundbreaking results were obtained on the authorsâ€™ own proposed EntityDrawBench benchmark, which encompasses a range of common and rare objects across multiple categories. RDM <cite class="ltx_cite ltx_citemacro_cite">Blattmann et&nbsp;al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>, although trained in a similar manner to RE-IMAGEN, employs image features as the foundation for retrieval and is supplanted by user examples during the inference process. Consequently, RDM is capable of efficiently transferring the described artistic style to the generated images. Furthermore, in contrast to RE-IMAGEN, which employs image-text pairs for retrieval, KNN-Diffusion <cite class="ltx_cite ltx_citemacro_cite">Sheynin et&nbsp;al. (<a href="#bib.bib114" title="" class="ltx_ref">2022</a>)</cite> solely utilizes images for retrieval, resulting in a lower quality of results on the COCO dataset.</p>
</div>
</section>
<section id="S7.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.3 </span>Fact checking</h4>

<div id="S7.SS2.SSS3.p1" class="ltx_para">
<p id="S7.SS2.SSS3.p1.1" class="ltx_p">Fact checking involves verifying a statement based on evidence. This task at hand involves a retrieval problem and a challenging implicit reasoning task. Furthermore, This task typically involves taking the statement as input and producing relevant document passages that prove or disprove the statement. Many RALM models get excellent performance because they come with their own retrievers. It is an important aspect of natural language understanding.</p>
</div>
<div id="S7.SS2.SSS3.p2" class="ltx_para">
<p id="S7.SS2.SSS3.p2.1" class="ltx_p">RAG <cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite> uses the FEVER dataset to map labels (Supported, Refuted, NotEnoughInfo) to individual output tokens. It is trained directly using the declaration class, which is not supervised over the retrieved evidence, unlike other works. Atlas <cite class="ltx_cite ltx_citemacro_cite">Izacard et&nbsp;al. (<a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite> employs few-shot learning to achieve performance comparable to previous studies in just 64-shot conditions. Furthermore, after training with the full dataset, it outperformed the best model <cite class="ltx_cite ltx_citemacro_cite">Hoffmann et&nbsp;al. (<a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite> available at the time. FILCO <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite> approached the task of improving the quality of retrieved documents by using the FEVER dataset from the KILT base aggregation, which only included the supports and refutes tags. Accuracy was used as a metric.</p>
</div>
</section>
<section id="S7.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.4 </span>Knowledge Graph Completion</h4>

<div id="S7.SS2.SSS4.p1" class="ltx_para">
<p id="S7.SS2.SSS4.p1.1" class="ltx_p">A multitude of previous tasks have employed structured data in the form of knowledge graphs, with knowledge graph completion representing a pervasive application. The conventional methodology <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib14" title="" class="ltx_ref">2022a</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Saxena et&nbsp;al. (<a href="#bib.bib109" title="" class="ltx_ref">2022</a>)</cite> for completion entails defining the task as a sequence-to-sequence process, wherein incomplete triples and entities are transformed into text sequences. However, this approach is constrained by its reliance on implicit reasoning, which significantly constrains the utility of the knowledge graph itself. ReSKGC <cite class="ltx_cite ltx_citemacro_cite">Yu and Yang (<a href="#bib.bib145" title="" class="ltx_ref">2023</a>)</cite> proposes the integration of retrieval augmentation techniques with knowledge graph completion. This integration entails the selection of semantically relevant triples from the knowledge graph and their utilization as evidence to inform the generation of output through explicit reasoning. This model employs data from the Wikidata5M and WikiKG90Mv2 datasets, demonstrating superior performance compared to other existing work in a range of conditions.</p>
</div>
</section>
<section id="S7.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.5 </span>Commonsense Reasoning</h4>

<div id="S7.SS2.SSS5.p1" class="ltx_para">
<p id="S7.SS2.SSS5.p1.1" class="ltx_p">Commonsense Reasoning is a challenging task for language models. In addition to exhibiting human-like thinking and reasoning patterns, these models must also be able to store a substantial amount of commonsense knowledge. However, the advent of RALM has made the second requirement less demanding, as retrieval techniques provide language models with access to nonparametric memories.</p>
</div>
<div id="S7.SS2.SSS5.p2" class="ltx_para">
<p id="S7.SS2.SSS5.p2.1" class="ltx_p">FLARE <cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite> uses StrategyQA, which contains a significant number of yes/no questions from a diverse range of sources. In addition, the authors request that the model provide the exact reasoning process and the final answer that determines the yes/no answer, ensuring that the answer matches the gold answer exactly. The incorporation of in-context samples into the retrieved content, along with training using data from the COPA, HellaSwag, and PIQA datasets, has resulted in the development of LLM-R <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib130" title="" class="ltx_ref">2023b</a>)</cite> model that exhibits excellent performance. The fundamental concept of the ITER-RETGEN <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite> model employs an iterative methodology to integrate retrievers and language models. The model was trained for the Commonsense Reasoning task using the StrategyQA dataset and achieved its optimal performance at seven iterations. In contrast, KG-BART <cite class="ltx_cite ltx_citemacro_cite">Shao et&nbsp;al. (<a href="#bib.bib112" title="" class="ltx_ref">2023</a>)</cite> is designed to prioritize the Commonsense Reasoning task and employs knowledge graphs to enhance its performance in this area. This approach has proven effective in significantly improving the modelâ€™s ability to complete the Commonsense Reasoning task, with performance approaching that of human beings under certain evaluation metrics.</p>
</div>
</section>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>RALM on both NLU and NLG tasks</h3>

<section id="S7.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.1 </span>Text Summarization</h4>

<div id="S7.SS3.SSS1.p1" class="ltx_para">
<p id="S7.SS3.SSS1.p1.1" class="ltx_p">Text summarization represents a crucial application of language modelling. In essence, text summarization is the process of generating concise and fluent summaries while maintaining the content and overall meaning of key information. Currently, two distinct types of this task exist: extractive summarization and abstractive summarization.</p>
</div>
<div id="S7.SS3.SSS1.p2" class="ltx_para">
<p id="S7.SS3.SSS1.p2.1" class="ltx_p">RA-DIT <cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite> employs the CNN/DailyMail dataset to refine the language model component of the model, which demonstrates remarkable efficacy in the text summarization task due to the operation of command fine-tuning. In contrast, Self-Mem<cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite> was trained using the XSum and BigPatent datasets. The authors observed that memory enhancement had a significantly larger effect on BigPatent than XSum. They hypothesize that this discrepancy is due to the inclusion of official patent documents in the BigPatent dataset, which exhibit considerable similarity. The LLM-R <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib130" title="" class="ltx_ref">2023b</a>)</cite> model employs an in-context learning approach, integrating RALM, and utilizes the AESLC, AG News, and Gigaword datasets for text summarization training. The results demonstrate that LLM-R significantly outperforms both traditional and dense retrievers in the summarization task. RAMKG <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite> extended the iterative training of the RALM architecture to the multilingual domain and employed two multilingual datasets, EcommerceMKP and AcademicMKP, for the training of summarization work, achieving the best results at that time.</p>
</div>
</section>
<section id="S7.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.2 </span>Question Answering</h4>

<div id="S7.SS3.SSS2.p1" class="ltx_para">
<p id="S7.SS3.SSS2.p1.1" class="ltx_p">Question answering also includes generative and extractive forms. It is a common task in NLP that relies on domain-specific knowledge. RALMs can achieve better results than traditional language models by utilizing externally stored knowledge. Common question answering tasks include domain-specific QA, open-domain QA(ODQA), and multi-hop QA. In the medical field, large language models are commonly used, PKG<cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib83" title="" class="ltx_ref">2023b</a>)</cite> uses the relevant data from the MedMC-QA dataset, using the questions in the training set as input and the medical explanations as background knowledge, and the accuracy of the background knowledge generated by the model as an evaluation metric. HyKGE<cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib57" title="" class="ltx_ref">2023b</a>)</cite> also targets question answering in the medical field, but uses a knowledge graph-enhanced approach. When targeting ODQA tasks, RAG <cite class="ltx_cite ltx_citemacro_cite">Lewis et&nbsp;al. (<a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite> considers questions and answers as input-output text pairs and trains by minimizing the negative log-likelihood of the answers. In contrast, ICRALM <cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite> exclusively performs the ODQA task using frozen LMs, which have not been enhanced by pre-training, fine-tuning, or retrieval, as well as the associated knowledge documents. Other models <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib75" title="" class="ltx_ref">2023b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib117" title="" class="ltx_ref">2023b</a>)</cite> were also trained for the ODQA task. In relation to the multi-hop QA task, FILCO <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib131" title="" class="ltx_ref">2023c</a>)</cite> used their proposed filtered retrieval method to filter multiple documents. They validated their approach using the HotpotQA dataset. RR <cite class="ltx_cite ltx_citemacro_cite">He et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>, on the other hand, used the Chain-of-Thought (CoT) approach to address the multi-hop problem. In addition, many other models <cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib58" title="" class="ltx_ref">2023c</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Khattab et&nbsp;al. (<a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite> deal with multi-hop problems.</p>
</div>
</section>
<section id="S7.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.3 </span>Code Generation and Summarization</h4>

<div id="S7.SS3.SSS3.p1" class="ltx_para">
<p id="S7.SS3.SSS3.p1.1" class="ltx_p">Code generation <cite class="ltx_cite ltx_citemacro_cite">Romera-Paredes et&nbsp;al. (<a href="#bib.bib103" title="" class="ltx_ref">2024</a>); Ye et&nbsp;al. (<a href="#bib.bib140" title="" class="ltx_ref">2024</a>)</cite> and summarization <cite class="ltx_cite ltx_citemacro_cite">Nam et&nbsp;al. (<a href="#bib.bib90" title="" class="ltx_ref">2024</a>)</cite> differ from ordinary text generation and summarization in terms of the target audience and processing. Code generation and summarization involves computer program code, which may require domain-specific syntactic and semantic understanding, in addition to higher requirements for NLU and NLG capabilities of language models.</p>
</div>
<div id="S7.SS3.SSS3.p2" class="ltx_para">
<p id="S7.SS3.SSS3.p2.1" class="ltx_p">REDCODER <cite class="ltx_cite ltx_citemacro_cite">Parvez et&nbsp;al. (<a href="#bib.bib92" title="" class="ltx_ref">2021</a>)</cite> initially identified potential candidate codes from existing code or abstract databases. The researchers retained 1.1 million unique lines of codes and abstracts as retrieved data through multiple channels. The final evaluation on the CodeXGLUE dataset demonstrated excellent performance across multiple programming languages and evaluation metrics. Another proposed enhancement was put forth by <cite class="ltx_cite ltx_citemacro_citet">Zan et&nbsp;al. (<a href="#bib.bib149" title="" class="ltx_ref">2022</a>)</cite>, who utilized private libraries to enhance the quality of code generation. This involved first identifying the most suitable private libraries based on the API documentation through the APIRetriever component, and then utilizing the APIcoder for generation. This approach led to a notable improvement in the accuracy of the generated content.</p>
</div>
<div id="S7.SS3.SSS3.p3" class="ltx_para">
<p id="S7.SS3.SSS3.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Liu et&nbsp;al. (<a href="#bib.bib77" title="" class="ltx_ref">2020</a>)</cite> propose a novel attention-based dynamic graph to complement the source code for static graph representations and design a hybrid message-passing GNN to capture local and global structural information. This approach improves the accuracy of code summarization and ultimately yields superior performance over both mainstream retrievers and generators. The RACE <cite class="ltx_cite ltx_citemacro_cite">Shi et&nbsp;al. (<a href="#bib.bib115" title="" class="ltx_ref">2022</a>)</cite> model employ a conventional RALM framework to consolidate code in five programming languages within the MCMD dataset, resulting in a 6 to 38 percent enhancement over all baseline models.</p>
</div>
<figure id="S7.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Summary of evaluation methods in RALM.</figcaption>
<div id="S7.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:432.8pt;height:363.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-101.8pt,85.5pt) scale(0.68,0.68) ;">
<table id="S7.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S7.T3.1.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Reference</td>
<td id="S7.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">RAGAS</td>
</tr>
<tr id="S7.T3.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Es et&nbsp;al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.3.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">RGB</td>
</tr>
<tr id="S7.T3.1.1.1.1.3.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib15" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.4.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">CRUD-RAG</td>
</tr>
<tr id="S7.T3.1.1.1.1.4.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Lyu et&nbsp;al. (<a href="#bib.bib84" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.5.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">ARES</td>
</tr>
<tr id="S7.T3.1.1.1.1.5.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Saad-Falcon et&nbsp;al. (<a href="#bib.bib105" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.6.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">MIRAGE</td>
</tr>
<tr id="S7.T3.1.1.1.1.6.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Xiong et&nbsp;al. (<a href="#bib.bib133" title="" class="ltx_ref">2024</a>)</cite></td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.1.1.7.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.1.1.7.1.1" class="ltx_tr">
<td id="S7.T3.1.1.1.1.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">RECALL</td>
</tr>
<tr id="S7.T3.1.1.1.1.7.1.2" class="ltx_tr">
<td id="S7.T3.1.1.1.1.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib78" title="" class="ltx_ref">2023</a>)</cite></td>
</tr>
</tbody></table>
</td>
</tr>
<tr id="S7.T3.1.1.2.2" class="ltx_tr">
<td id="S7.T3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Dataset</td>
<td id="S7.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">WikiEval</td>
<td id="S7.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3">LLM-generated</td>
<td id="S7.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.2.2.4.1.1" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">MMLU-Med</td>
</tr>
<tr id="S7.T3.1.1.2.2.4.1.2" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">MedQA-US</td>
</tr>
<tr id="S7.T3.1.1.2.2.4.1.3" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">MedMCQA</td>
</tr>
<tr id="S7.T3.1.1.2.2.4.1.4" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">PubMedQA</td>
</tr>
<tr id="S7.T3.1.1.2.2.4.1.5" class="ltx_tr">
<td id="S7.T3.1.1.2.2.4.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">BioASQ-Y/N</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.2.2.5.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.2.2.5.1.1" class="ltx_tr">
<td id="S7.T3.1.1.2.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">EventKG</td>
</tr>
<tr id="S7.T3.1.1.2.2.5.1.2" class="ltx_tr">
<td id="S7.T3.1.1.2.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">UJ</td>
</tr>
</tbody></table>
</td>
</tr>
<tr id="S7.T3.1.1.3.3" class="ltx_tr">
<td id="S7.T3.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Target</td>
<td id="S7.T3.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="5">
<table id="S7.T3.1.1.3.3.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.3.3.2.1.1" class="ltx_tr">
<td id="S7.T3.1.1.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">Retrieval Quality;â€ƒGeneration Quality</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Generation Quality</td>
</tr>
<tr id="S7.T3.1.1.4.4" class="ltx_tr">
<td id="S7.T3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.4.4.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.4.4.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Context</td>
</tr>
<tr id="S7.T3.1.1.4.4.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.4.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Relevance</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.4.4.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.4.4.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.4.4.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.4.4.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.5.5" class="ltx_tr">
<td id="S7.T3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.5.5.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.5.5.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Faithfulness</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
</tr>
<tr id="S7.T3.1.1.6.6" class="ltx_tr">
<td id="S7.T3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.6.6.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.6.6.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.6.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Answer</td>
</tr>
<tr id="S7.T3.1.1.6.6.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.6.6.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Relevance</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.6.6.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.6.6.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.6.6.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.6.6.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.7.7" class="ltx_tr">
<td id="S7.T3.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.7.7.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.7.7.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.7.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Noise</td>
</tr>
<tr id="S7.T3.1.1.7.7.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.7.7.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Robustness</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.7.7.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.7.7.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.7.7.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.7.7.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.7.7.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.8.8" class="ltx_tr">
<td id="S7.T3.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.8.8.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.8.8.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.8.8.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Information</td>
</tr>
<tr id="S7.T3.1.1.8.8.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.8.8.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Integration</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.8.8.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.8.8.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.8.8.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.9.9" class="ltx_tr">
<td id="S7.T3.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.9.9.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.9.9.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.9.9.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Negative</td>
</tr>
<tr id="S7.T3.1.1.9.9.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.9.9.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Rejection</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.9.9.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.9.9.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.9.9.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.9.9.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.9.9.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.10.10" class="ltx_tr">
<td id="S7.T3.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.10.10.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.10.10.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.10.10.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Counterfactual</td>
</tr>
<tr id="S7.T3.1.1.10.10.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.10.10.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Robustness</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.10.10.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.10.10.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.10.10.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
</tr>
<tr id="S7.T3.1.1.11.11" class="ltx_tr">
<td id="S7.T3.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.11.11.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.11.11.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.11.11.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Error</td>
</tr>
<tr id="S7.T3.1.1.11.11.1.1.2" class="ltx_tr">
<td id="S7.T3.1.1.11.11.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Correction</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.11.11.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.11.11.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.11.11.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.11.11.6" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.11.11.7" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="S7.T3.1.1.12.12" class="ltx_tr">
<td id="S7.T3.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<table id="S7.T3.1.1.12.12.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T3.1.1.12.12.1.1.1" class="ltx_tr">
<td id="S7.T3.1.1.12.12.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Summarization</td>
</tr>
</tbody></table>
</td>
<td id="S7.T3.1.1.12.12.2" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.12.12.3" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">âˆš</td>
<td id="S7.T3.1.1.12.12.5" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.12.12.6" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S7.T3.1.1.12.12.7" class="ltx_td ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Evaluation</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">This section provides a summary of the evaluation approach and benchmarks for RALM. In Sections <a href="#S6" title="6 Data Sources â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#S7" title="7 Applications â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we presented some evaluation criteria for large language model tasks as well as baselines. At the time of the initial proposal of the RALM architecture, the majority of researchers employed generalized benchmarks. However, as the RALM architecture evolved, there was a growing number of RALM-specific evaluation methods and baselines proposed. Table <a href="#S7.T3" title="Table 3 â€£ 7.3.3 Code Generation and Summarization â€£ 7.3 RALM on both NLU and NLG tasks â€£ 7 Applications â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrates the details of each evaluation model.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">RAGAS <cite class="ltx_cite ltx_citemacro_cite">Es et&nbsp;al. (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> employs the WikiEval Dataset to assess the faithfulness, answer relevance, and context relevance of RALMs. Faithfulness is defined as the degree to which responses align with the provided context. Answer relevance refers to the extent to which generated responses address the actual question posed. Context relevance is gauged by the degree to which retrieved context is centralized and devoid of irrelevant information. Additionally, the researchers utilize the prompt gpt-3.5-turbo-16k model to automate the evaluation process. RGB <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib15" title="" class="ltx_ref">2024</a>)</cite> developed a bilingual Chinese and English evaluation system employing three evaluation metrics: accuracy, rejection rate, and error detection rate. These metrics were utilized to assess the noise robustness, negative rejection, information integration, and counterfactual robustness of the data sources, which were articles processed by LM and retrieved through Googleâ€™s API. CRUD-RAG <cite class="ltx_cite ltx_citemacro_cite">Lyu et&nbsp;al. (<a href="#bib.bib84" title="" class="ltx_ref">2024</a>)</cite> considers the impact of retrieval components and the construction of external knowledge bases that have not been previously considered by researchers. A dataset was generated using a large model to evaluate the Create, Read, Update, and Delete (summarization) capabilities of RALM through four evaluation metrics: ROUGE, BLEU, bertScore, and RAGQuestEval. In addition, ARES <cite class="ltx_cite ltx_citemacro_cite">Saad-Falcon et&nbsp;al. (<a href="#bib.bib105" title="" class="ltx_ref">2023</a>)</cite> employs datasets generated by the LM, but utilizes a lightweight LM to determine the quality of individual RALM components and utilizes human-labeled data points for prediction-powered inference. The RALMâ€™s context is evaluated using the KILT and SuperGLUE benchmarks, with Relevance, Answer Faithfulness, and Answer Relevance being the relevant criteria.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">In addition to the general assessment of RALM, there has been some work focusing on the assessment of specific details and domains. RECALL <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib78" title="" class="ltx_ref">2023</a>)</cite> employs the EventKG and UJ datasets to incorporate inaccurate information into its existing data set. It then determines whether RALM is susceptible to being misled by such inaccurate information through two tasks: question answering and text generation. <cite class="ltx_cite ltx_citemacro_citet">Xiong et&nbsp;al. (<a href="#bib.bib133" title="" class="ltx_ref">2024</a>)</cite> concentrated on the medical domain and proposed MIRAGE, which integrates data from five datasets, including MMLU-Med, to evaluate the zero-shot learning, multi-choice evaluation, retrieval-augmented generation, and question-only retrieval ideation capabilities of medical RALMs. Ultimately, they also discovered the log-linear scaling property and the â€lost-in-the-middleâ€ effect in the medical domain.</p>
</div>
<figure id="S8.F7" class="ltx_figure"><img src="/html/2404.19543/assets/x7.png" id="S8.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Summary of limitations of current RALM models and future prospects.</figcaption>
</figure>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Disscussion</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">This section is devoted to an analysis of the limitations of existing RALM architectures and a description of potential future developments. Figure <a href="#S8.F7" title="Figure 7 â€£ 8 Evaluation â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> summarizes the limitations of existing RALMs and our proposed solutions.</p>
</div>
<section id="S9.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>Limitations</h3>

<div id="S9.SS1.p1" class="ltx_para">
<p id="S9.SS1.p1.1" class="ltx_p">This section presents a summary and analysis of some of the limitations of existing RALMs.</p>
</div>
<section id="S9.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.1 </span>Poor Robustness</h4>

<div id="S9.SS1.SSS1.p1" class="ltx_para">
<p id="S9.SS1.SSS1.p1.1" class="ltx_p">Robustness is a crucial aspect to be considered in all systems. RALM systems, despite exhibiting performance benefits in several domains, introduce a multitude of uncertainties to the architecture due to the incorporation of retrieval as a technique. As elucidated by <cite class="ltx_cite ltx_citemacro_citet">Hu et&nbsp;al. (<a href="#bib.bib47" title="" class="ltx_ref">2024</a>)</cite>, through exceedingly simple prefix attacks, not only can the relevance and accuracy of RALM output be diminished, but even the retrieval strategy of the retriever can be altered. Consequently, in addition to utilising various retrieval enhancement techniques to enhance the performance of LMs, researchers should also take care to minimise the effects of factually inaccurate data, information that is not relevant to problem solving and even some harmful hints and prefixes on LMs.</p>
</div>
</section>
<section id="S9.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.2 </span>Poor Quality of Retrieval Results</h4>

<div id="S9.SS1.SSS2.p1" class="ltx_para">
<p id="S9.SS1.SSS2.p1.1" class="ltx_p">A significant number of researchers <cite class="ltx_cite ltx_citemacro_cite">Yan et&nbsp;al. (<a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Asai et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite> engaged in the endeavour of enhancing retrieval efficacy have asserted that although their proposed models are demonstrably beneficial in optimising the quality of the output, there is as yet no assurance that the retrieval outcomes can be entirely aligned with the LM. Particularly when using the Internet as a retrieval tool, the quality of Internet sources can vary widely, and merging this data without proper consideration can introduce noise or misleading information into the resulting output.</p>
</div>
</section>
<section id="S9.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.3 </span>Overspending</h4>

<div id="S9.SS1.SSS3.p1" class="ltx_para">
<p id="S9.SS1.SSS3.p1.1" class="ltx_p">While existing RALMs <cite class="ltx_cite ltx_citemacro_cite">Siriwardhana et&nbsp;al. (<a href="#bib.bib119" title="" class="ltx_ref">2023</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Guu et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Borgeaud et&nbsp;al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite> can greatly improve the performance of LMs in various domains, some of them require extensive model changes as well as complex pre-training and fine-tuning operations, which greatly increases the time and space overhead and also reduces the scalability of RALMs. In addition, as the scale of retrieval increases, so does the complexity of storing and accessing the data sources. As a result, researchers must weigh the benefits of modifying the model against the costs.</p>
</div>
</section>
<section id="S9.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.4 </span>Few Applications</h4>

<div id="S9.SS1.SSS4.p1" class="ltx_para">
<p id="S9.SS1.SSS4.p1.1" class="ltx_p">Although many RALMs have greatly improved the performance of LMs in various domains, there has not been much improvement from an application perspective, and RALMs are still doing some of the routine work that was done in the early days of LMs, e.g., question answering, summarizing <cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib83" title="" class="ltx_ref">2023b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a href="#bib.bib57" title="" class="ltx_ref">2023b</a>)</cite> <cite class="ltx_cite ltx_citemacro_cite">Alon et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>. Although there have been some very interesting application directions recently, such as math teaching <cite class="ltx_cite ltx_citemacro_cite">Levonian et&nbsp;al. (<a href="#bib.bib69" title="" class="ltx_ref">2023</a>)</cite>, slot filling <cite class="ltx_cite ltx_citemacro_cite">Glass et&nbsp;al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, etc., this is not enough. A technology always needs to be actually used to fully prove its value, and RALM is no exception.</p>
</div>
</section>
</section>
<section id="S9.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>Future Prospects</h3>

<div id="S9.SS2.p1" class="ltx_para">
<p id="S9.SS2.p1.1" class="ltx_p">This section suggests some possible directions for the future development of RALM, based mainly on the limitations mentioned in Section <a href="#S9.SS1" title="9.1 Limitations â€£ 9 Disscussion â€£ RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9.1</span></a>.</p>
</div>
<section id="S9.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.1 </span>Improve Robustness</h4>

<div id="S9.SS2.SSS1.p1" class="ltx_para">
<p id="S9.SS2.SSS1.p1.1" class="ltx_p">Some scholars have mentioned possible ways to improve model robustness in the future work section of their papers, such as explicit self-reflection and fine-grained attribution <cite class="ltx_cite ltx_citemacro_cite">Asai et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>. In contrast to these works, <cite class="ltx_cite ltx_citemacro_citet">Hu et&nbsp;al. (<a href="#bib.bib47" title="" class="ltx_ref">2024</a>)</cite> proposed a method called Gradient Guided Prompt Perturbation (GGPP), a way to perturb the RALM, which was experimentally found to be effective in improving the situation by utilizing the SAT probe <cite class="ltx_cite ltx_citemacro_cite">Yuksekgonul et&nbsp;al. (<a href="#bib.bib148" title="" class="ltx_ref">2023</a>)</cite> and activation (ACT) classifier. A method is proposed to detect this perturbation by prompting the internal state of the perturbed RALM. In addition, by proposing and improving the evaluation method of RALM and the related baseline can also help improve the robustness of the model, <cite class="ltx_cite ltx_citemacro_citet">Chen et&nbsp;al. (<a href="#bib.bib15" title="" class="ltx_ref">2024</a>)</cite> made a series of evaluation system for RALM by focusing on the robustness.</p>
</div>
</section>
<section id="S9.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.2 </span>Improve Retrieval Quality</h4>

<div id="S9.SS2.SSS2.p1" class="ltx_para">
<p id="S9.SS2.SSS2.p1.1" class="ltx_p">Improving the quality of retrieval can be considered in two parts: improving the quality of the dataset used for retrieval, and improving the performance of the retrieval technique. Nowadays, many data sets are given to LLM to generate relevant content, and since LLM itself has â€hallucinationâ€, certain means must be adopted to ensure the accuracy of the data, such as using human beings to supervise the refinement <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a href="#bib.bib15" title="" class="ltx_ref">2024</a>)</cite>. In addition, due to the wide range of information sources on the Internet, it is obviously not enough to rely solely on search engines for screening, so it is necessary to improve the retrieval technology, such as the use of BM25 <cite class="ltx_cite ltx_citemacro_cite">Luo et&nbsp;al. (<a href="#bib.bib82" title="" class="ltx_ref">2023a</a>)</cite> or TF-IDF <cite class="ltx_cite ltx_citemacro_cite">Lazaridou et&nbsp;al. (<a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> algorithms for further re-ranking.</p>
</div>
</section>
<section id="S9.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.3 </span>Weigh Expenses and Benefits</h4>

<div id="S9.SS2.SSS3.p1" class="ltx_para">
<p id="S9.SS2.SSS3.p1.1" class="ltx_p">Reducing the overhead can be considered from three perspectives: first, some plug-and-play intermediate modules can be designed, e.g., CRAG <cite class="ltx_cite ltx_citemacro_cite">Yan et&nbsp;al. (<a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite>, Selfmem <cite class="ltx_cite ltx_citemacro_cite">Cheng et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2024</a>)</cite>, AI agent <cite class="ltx_cite ltx_citemacro_cite">Peng et&nbsp;al. (<a href="#bib.bib93" title="" class="ltx_ref">2023</a>)</cite>, or some deployment solutions, e.g., LangChain, Llama Index, so that there is no need to make targeted improvements for each model. Second, Internet retrieval can be utilized to reduce the overhead of the retriever, but attention needs to be paid to the data relevance mentioned earlier. Finally, In-context learning can be employed to reduce the overhead associated with improving LMs, e.g., ICRALM <cite class="ltx_cite ltx_citemacro_cite">Ram et&nbsp;al. (<a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S9.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.4 </span>Expand Applications</h4>

<div id="S9.SS2.SSS4.p1" class="ltx_para">
<p id="S9.SS2.SSS4.p1.1" class="ltx_p">In the contemporary era, the application of LLM has been expanded to encompass a multitude of domains, whereas the application direction of RALM remains relatively limited. To address this limitation, researchers must not only consider the existing application areas of LLM but also leverage the distinctive strengths of RALM, which excels in addressing problems closely related to knowledge and experience. Additionally, they should integrate RALM with other advanced technologies and utilize it to overcome the challenges associated with them. This paper presents several illustrative examples, including decision support, search engine, and recommendation system.</p>
</div>
</section>
</section>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Conclusion</h2>

<div id="S10.p1" class="ltx_para">
<p id="S10.p1.1" class="ltx_p">The integration of RALMs represents a significant advance in the capabilities of NLP systems. This survey has provided an extensive review of RALMs, highlighting their architecture, applications, and the challenges they face. RALMs enhance language models by retrieving and integrating external knowledge, leading to improved performance across a variety of NLP tasks, including translation, dialogue generation, and knowledge graph completion.</p>
</div>
<div id="S10.p2" class="ltx_para">
<p id="S10.p2.1" class="ltx_p">Despite their successes, RALMs encounter several limitations. Notably, their robustness against adversarial inputs, the quality of retrieval results, the computational costs associated with their deployment, and a lack of diversity in application domains have been identified as areas requiring further attention. To address these, the research community has proposed several strategies, such as improving the evaluation methods, refining retrieval techniques, and exploring cost-effective solutions that maintain a balance between performance and efficiency.</p>
</div>
<div id="S10.p3" class="ltx_para">
<p id="S10.p3.1" class="ltx_p">In the future, the advancement of RALMs will depend on the enhancement of their robustness, the improvement of retrieval quality, and the expansion of their application scope. By incorporating more sophisticated techniques and integrating RALMs with other AI technologies, these models can be leveraged to address an even broader spectrum of challenges. The ongoing research and development in this field are expected to result in more resilient, efficient, and versatile RALMs, thereby pushing the boundaries of what is achievable in NLP and beyond. As RALMs continue to evolve, they hold the promise of enabling AI systems with deeper understanding and more human-like language capabilities, thereby opening up new possibilities in a wide range of fields.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia&nbsp;Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adolphs et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Leonard Adolphs, Benjamin Boerschinger, Christian Buck, Michelle&nbsp;Chen Huebscher, Massimiliano Ciaramita, Lasse Espeholt, Thomas Hofmann, Yannic Kilcher, Sascha Rothe, Pier&nbsp;Giuseppe Sessa, et&nbsp;al. 2021.

</span>
<span class="ltx_bibblock">Boosting search engines with interactive agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.00527</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alon et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022.

</span>
<span class="ltx_bibblock">Neuro-symbolic language modeling with automaton-augmented retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 468â€“485. PMLR.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">Self-rag: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.11511</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aussenac-Gilles and SÃ¶rgel (2005)</span>
<span class="ltx_bibblock">
Nathalie Aussenac-Gilles and Dagobert SÃ¶rgel. 2005.

</span>
<span class="ltx_bibblock">Text analysis for ontology and terminology engineering.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Applied Ontology</em>, 1(1):35â€“46.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Jimmy&nbsp;Lei Ba, Jamie&nbsp;Ryan Kiros, and Geoffrey&nbsp;E Hinton. 2016.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1607.06450</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Imon Banerjee, Yuan Ling, Matthew&nbsp;C Chen, Sadid&nbsp;A Hasan, Curtis&nbsp;P Langlotz, Nathaniel Moradzadeh, Brian Chapman, Timothy Amrhein, David Mong, Daniel&nbsp;L Rubin, et&nbsp;al. 2019.

</span>
<span class="ltx_bibblock">Comparative effectiveness of convolutional neural network (cnn) and recurrent neural network (rnn) architectures for radiology text report classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence in medicine</em>, 97:79â€“88.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Gpt-neox-20b: An open-source autoregressive language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.06745</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blattmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas MÃ¼ller, and BjÃ¶rn Ommer. 2022.

</span>
<span class="ltx_bibblock">Retrieval-augmented diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:15309â€“15324.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George&nbsp;Bm Van Den&nbsp;Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 2206â€“2240. PMLR.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boytsov et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Leonid Boytsov, David Novak, Yury Malkov, and Eric Nyberg. 2016.

</span>
<span class="ltx_bibblock">Off the beaten path: Letâ€™s replace term-based retrieval with k-nn search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM international on conference on information and knowledge management</em>, pages 1099â€“1108.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877â€“1901.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Chen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam. 2022a.

</span>
<span class="ltx_bibblock">Knowledge is flat: A seq2seq generative framework for various knowledge graph completion.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.07299</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le&nbsp;Sun. 2024.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume&nbsp;38, pages 17754â€“17762.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Hongyang Chao, and Tao Mei. 2023.

</span>
<span class="ltx_bibblock">Retrieval augmented convolutional encoder-decoder networks for video captioning.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Multimedia Computing, Communications and Applications</em>, 19(1s):1â€“24.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Xi&nbsp;Chen, Pat Verga, and William&nbsp;W Cohen. 2022b.

</span>
<span class="ltx_bibblock">Murag: Multimodal retrieval-augmented generator for open question answering over images and text.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.02928</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022c)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William&nbsp;W Cohen. 2022c.

</span>
<span class="ltx_bibblock">Re-imagen: Retrieval-augmented text-to-image generator.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.14491</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Xin Cheng, Di&nbsp;Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2024.

</span>
<span class="ltx_bibblock">Lift yourself up: Retrieval-augmented text generation with self-memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Child et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock">Generating long sequences with sparse transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.10509</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 24(240):1â€“113.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, Shayne Longpre, Barret Zoph, Yi&nbsp;Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.11416</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dauphin et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Yann&nbsp;N Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017.

</span>
<span class="ltx_bibblock">Language modeling with gated convolutional networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 933â€“941. PMLR.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drozdov et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, and Mohit Iyyer. 2022.

</span>
<span class="ltx_bibblock">You canâ€™t pick your neighbors, or can you? when and how to rely on retrieval in the <math id="bib.bib25.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="bib.bib25.1.m1.1a"><mi id="bib.bib25.1.m1.1.1" xref="bib.bib25.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="bib.bib25.1.m1.1b"><ci id="bib.bib25.1.m1.1.1.cmml" xref="bib.bib25.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib25.1.m1.1c">k</annotation></semantics></math> nn-lm.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.15859</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Yann Dubois, Chen&nbsp;Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy&nbsp;S Liang, and Tatsunori&nbsp;B Hashimoto. 2024.

</span>
<span class="ltx_bibblock">Alpacafarm: A simulation framework for methods that learn from human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Es et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023.

</span>
<span class="ltx_bibblock">Ragas: Automated evaluation of retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15217</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Chao Feng, Xinyu Zhang, and Zichu Fei. 2023.

</span>
<span class="ltx_bibblock">Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.03118</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024.

</span>
<span class="ltx_bibblock">Retrieval-generation synergy augmented large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 11661â€“11665. IEEE.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yifan Gao, Qingyu Yin, Zheng Li, Rui Meng, Tong Zhao, Bing Yin, Irwin King, and Michael&nbsp;R Lyu. 2022.

</span>
<span class="ltx_bibblock">Retrieval-augmented multilingual keyphrase generation with retriever-generator iterative training.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.10471</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi&nbsp;Dai, Jiawei Sun, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10997</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Sreyan Ghosh, Sonal Kumar, Chandra Kiran&nbsp;Reddy Evuru, Ramani Duraiswami, and Dinesh Manocha. 2024.

</span>
<span class="ltx_bibblock">Recap: retrieval-augmented audio captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1161â€“1165. IEEE.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glass et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Michael Glass, Gaetano Rossiello, Md&nbsp;Faisal&nbsp;Mahbub Chowdhury, and Alfio Gliozzo. 2021.

</span>
<span class="ltx_bibblock">Robust retrieval augmented generation for zero-shot slot filling.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.13934</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gou et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianping Gou, Baosheng Yu, Stephen&nbsp;J Maybank, and Dacheng Tao. 2021.

</span>
<span class="ltx_bibblock">Knowledge distillation: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 129(6):1789â€“1819.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grave et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016.

</span>
<span class="ltx_bibblock">Improving neural language models with a continuous cache.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1612.04426</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 3929â€“3938. PMLR.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hambarde and Proenca (2023)</span>
<span class="ltx_bibblock">
Kailash&nbsp;A Hambarde and Hugo Proenca. 2023.

</span>
<span class="ltx_bibblock">Information retrieval: recent advances and beyond.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Imagebind-llm: Multi-modality instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.03905</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hangfeng He, Hongming Zhang, and Dan Roth. 2022.

</span>
<span class="ltx_bibblock">Rethinking with retrieval: Faithful large language model inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.00303</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021.

</span>
<span class="ltx_bibblock">Efficient nearest neighbor language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.04212</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 770â€“778.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh&nbsp;V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024.

</span>
<span class="ltx_bibblock">G-retriever: Retrieval-augmented generation for textual graph understanding and question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.07630</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Animate-a-story: Storytelling with retrieval-augmented video generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.06940</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15556</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HofstÃ¤tter et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Sebastian HofstÃ¤tter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">Fid-light: Efficient and effective retrieval-augmented text generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pages 1437â€“1447.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hogenboom et&nbsp;al. (2010)</span>
<span class="ltx_bibblock">
Frederik Hogenboom, Flavius Frasincar, and Uzay Kaymak. 2010.

</span>
<span class="ltx_bibblock">An overview of approaches to extract information from natural language corpora.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Information Foraging Lab</em>, 69.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Zhibo Hu, Chen Wang, Yanfeng Shu, Liming Zhu, et&nbsp;al. 2024.

</span>
<span class="ltx_bibblock">Prompt perturbation in retrieval-augmented generation based large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.07179</em>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David&nbsp;A Ross, and Alireza Fathi. 2023.

</span>
<span class="ltx_bibblock">Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 23369â€“23379.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hua and Wang (2018)</span>
<span class="ltx_bibblock">
Xinyu Hua and Lu&nbsp;Wang. 2018.

</span>
<span class="ltx_bibblock">Neural argument generation augmented with externally retrieved evidence.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.10254</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi&nbsp;Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. 2023.

</span>
<span class="ltx_bibblock">Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 13916â€“13932. PMLR.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.09118</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2020a)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2020a.

</span>
<span class="ltx_bibblock">Distilling knowledge from reader to retriever for question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.04584</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2020b)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2020b.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.01282</em>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.03299</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye&nbsp;Jin Bang, Andrea Madotto, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 55(12):1â€“38.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu, Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding, Xu&nbsp;Chu, Junfeng Zhao, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Think and retrieval: A hypothesis knowledge graph enhanced medical large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15883</em>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank&nbsp;F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023c.

</span>
<span class="ltx_bibblock">Active retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.06983</em>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. 2020.

</span>
<span class="ltx_bibblock">Convbert: Improving bert with span-based dynamic convolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:12837â€“12848.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Minki Kang, Jin&nbsp;Myung Kwak, Jinheon Baek, and Sung&nbsp;Ju Hwang. 2023.

</span>
<span class="ltx_bibblock">Knowledge graph-augmented language models for knowledge-grounded dialogue generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18846</em>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.04906</em>.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020.

</span>
<span class="ltx_bibblock">Nearest neighbor machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.00710</em>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019.

</span>
<span class="ltx_bibblock">Generalization through memorization: Nearest neighbor language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00172</em>.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Omar Khattab, Keshav Santhanam, Xiang&nbsp;Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022.

</span>
<span class="ltx_bibblock">Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.14024</em>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Komeili et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2021.

</span>
<span class="ltx_bibblock">Internet-augmented dialogue generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.07566</em>.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et&nbsp;al. 2019.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 7:453â€“466.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazaridou et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022.

</span>
<span class="ltx_bibblock">Internet-augmented language models through few-shot prompting for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.05115</em>.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Latent retrieval for weakly supervised open domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.00300</em>.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levonian et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel, Millie-Ellen Postle, and Wanli Xing. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation to improve math question-answering: Trade-offs between groundedness and human preference.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03184</em>.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.13461</em>.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 33:9459â€“9474.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Bowen Li, Philip&nbsp;HS Torr, and Thomas Lukasiewicz. 2022a.

</span>
<span class="ltx_bibblock">Memory-driven text-to-image generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.07022</em>.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022b.

</span>
<span class="ltx_bibblock">A survey on retrieval-augmented text generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.01110</em>.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023a.

</span>
<span class="ltx_bibblock">How to train your dragon: Diverse augmentation towards generalizable dense retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.07452</em>.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01352</em>.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Xi&nbsp;Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual generative language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 9019â€“9052.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Shangqing Liu, Yu&nbsp;Chen, Xiaofei Xie, Jingkai Siow, and Yang Liu. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for code summarization via hybrid gnn.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.05405</em>.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yi&nbsp;Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu&nbsp;Sun. 2023.

</span>
<span class="ltx_bibblock">Recall: A benchmark for llms robustness against external counterfactual knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.08147</em>.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Yuxing Lu, Xiaohong Liu, Zongxin Du, Yuanxu Gao, and Guangyu Wang. 2023a.

</span>
<span class="ltx_bibblock">Medkpl: a heterogeneous knowledge enhanced prompt learning framework for transferable diagnosis.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Journal of Biomedical Informatics</em>, 143:104417.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Yuxing Lu, Xukai Zhao, and Jinzhuo Wang. 2023b.

</span>
<span class="ltx_bibblock">Medical knowledge-enhanced prompt learning for diagnosis classification from clinical text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 5th Clinical Natural Language Processing Workshop</em>, pages 278â€“288.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass. 2023a.

</span>
<span class="ltx_bibblock">Sail: Search-augmented instruction learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15225</em>.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Ziyang Luo, Can Xu, Pu&nbsp;Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023b.

</span>
<span class="ltx_bibblock">Augmented large language models with parametric knowledge guiding.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.04757</em>.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo&nbsp;Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. 2024.

</span>
<span class="ltx_bibblock">Crud-rag: A comprehensive chinese benchmark for retrieval-augmented generation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.17043</em>.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022.

</span>
<span class="ltx_bibblock">Memory-assisted prompt editing to improve gpt-3 after deployment.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.06009</em>.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mallen et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock">When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10511</em>.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020.

</span>
<span class="ltx_bibblock">Generation-augmented retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.08553</em>.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mousavi et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Seyed&nbsp;Mahed Mousavi, Simone Alghisi, and Giuseppe Riccardi. 2024.

</span>
<span class="ltx_bibblock">Is your llm outdated? benchmarking llms &amp; alignment algorithms for time-sensitive knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.08700</em>.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et&nbsp;al. 2021.

</span>
<span class="ltx_bibblock">Webgpt: Browser-assisted question-answering with human feedback, 2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">URL https://arxiv. org/abs/2112.09332</em>.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nam et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024.

</span>
<span class="ltx_bibblock">Using an llm to help with code understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/ACM 46th International Conference on Software Engineering</em>, pages 1â€“13.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo&nbsp;HernÃ¡ndez Ãbrego, Ji&nbsp;Ma, Vincent&nbsp;Y Zhao, Yi&nbsp;Luan, Keith&nbsp;B Hall, Ming-Wei Chang, et&nbsp;al. 2021.

</span>
<span class="ltx_bibblock">Large dual encoders are generalizable retrievers.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.07899</em>.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parvez et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Md&nbsp;Rizwan Parvez, Wasi&nbsp;Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021.

</span>
<span class="ltx_bibblock">Retrieval augmented code generation and summarization.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.11601</em>.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu&nbsp;Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Check your facts and try again: Improving large language models with external knowledge and automated feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.12813</em>.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et&nbsp;al. 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et&nbsp;al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Journal of machine learning research</em>, 21(140):1â€“67.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:1316â€“1331.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2021.

</span>
<span class="ltx_bibblock">Learning to retrieve passages without supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.07708</em>.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramachandran et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Prajit Ramachandran, Barret Zoph, and Quoc&nbsp;V Le. 2017.

</span>
<span class="ltx_bibblock">Searching for activation functions.

</span>
<span class="ltx_bibblock"><em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.05941</em>.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.06125</em>, 1(2):3.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramos et&nbsp;al. (2003)</span>
<span class="ltx_bibblock">
Juan Ramos et&nbsp;al. 2003.

</span>
<span class="ltx_bibblock">Using tf-idf to determine word relevance in document queries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Proceedings of the first instructional conference on machine learning</em>, volume 242, pages 29â€“48. Citeseer.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et&nbsp;al. (1995)</span>
<span class="ltx_bibblock">
Stephen&nbsp;E Robertson, Steve Walker, Susan Jones, Micheline&nbsp;M Hancock-Beaulieu, Mike Gatford, et&nbsp;al. 1995.

</span>
<span class="ltx_bibblock">Okapi at trec-3.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">Nist Special Publication Sp</em>, 109:109.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romera-Paredes et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M&nbsp;Pawan Kumar, Emilien Dupont, Francisco&nbsp;JR Ruiz, Jordan&nbsp;S Ellenberg, Pengming Wang, Omar Fawzi, et&nbsp;al. 2024.

</span>
<span class="ltx_bibblock">Mathematical discoveries from program search with large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Nature</em>, 625(7995):468â€“475.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rony et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Md&nbsp;Rashad Al&nbsp;Hasan Rony, Ricardo Usbeck, and Jens Lehmann. 2022.

</span>
<span class="ltx_bibblock">Dialokg: Knowledge-structure aware task-oriented dialogue generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.09149</em>.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023.

</span>
<span class="ltx_bibblock">Ares: An automated evaluation framework for retrieval-augmented generation systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.09476</em>.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachan et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Devendra&nbsp;Singh Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William&nbsp;L Hamilton, and Bryan Catanzaro. 2021.

</span>
<span class="ltx_bibblock">End-to-end training of neural retrievers for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00408</em>.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.

</span>
<span class="ltx_bibblock">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.01108</em>.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santhanam et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021.

</span>
<span class="ltx_bibblock">Colbertv2: Effective and efficient retrieval via lightweight late interaction.

</span>
<span class="ltx_bibblock"><em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.01488</em>.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxena et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. 2022.

</span>
<span class="ltx_bibblock">Sequence-to-sequence knowledge graph completion and question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.10321</em>.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools.

</span>
<span class="ltx_bibblock"><em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Serra et&nbsp;al. (2013)</span>
<span class="ltx_bibblock">
Ivo Serra, Rosario Girardi, and Paulo Novais. 2013.

</span>
<span class="ltx_bibblock">Parnt: a statistic based approach to extract non-taxonomic relationships of ontologies from text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">2013 10th International Conference on Information Technology: New Generations</em>, pages 561â€“566. IEEE.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock">Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.

</span>
<span class="ltx_bibblock"><em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15294</em>.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)</span>
<span class="ltx_bibblock">
Noam Shazeer. 2020.

</span>
<span class="ltx_bibblock">Glu variants improve transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.05202</em>.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheynin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman. 2022.

</span>
<span class="ltx_bibblock">Knn-diffusion: Image generation via large-scale retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02849</em>.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun. 2022.

</span>
<span class="ltx_bibblock">Race: Retrieval-augmented commit message generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.02700</em>.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed&nbsp;H Chi, Nathanael SchÃ¤rli, and Denny Zhou. 2023a.

</span>
<span class="ltx_bibblock">Large language models can be easily distracted by irrelevant context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 31210â€“31227. PMLR.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023b.

</span>
<span class="ltx_bibblock">Replug: Retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12652</em>.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. 2021.

</span>
<span class="ltx_bibblock">End-to-end training of multi-document reader and retriever for open-domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34:25968â€“25981.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siriwardhana et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023.

</span>
<span class="ltx_bibblock">Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:1â€“17.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et&nbsp;al. (2014)</span>
<span class="ltx_bibblock">
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014.

</span>
<span class="ltx_bibblock">Dropout: a simple way to prevent neural networks from overfitting.

</span>
<span class="ltx_bibblock"><em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">The journal of machine learning research</em>, 15(1):1929â€“1958.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha Ahmed, Yu&nbsp;Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, 568:127063.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor (1953)</span>
<span class="ltx_bibblock">
Wilson&nbsp;L Taylor. 1953.

</span>
<span class="ltx_bibblock">â€œcloze procedureâ€: A new tool for measuring readability.

</span>
<span class="ltx_bibblock"><em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">Journalism quarterly</em>, 30(4):415â€“433.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018.

</span>
<span class="ltx_bibblock">Fever: a large-scale dataset for fact extraction and verification.

</span>
<span class="ltx_bibblock"><em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05355</em>.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thulke et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
David Thulke, Nico Daheim, Christian Dugast, and Hermann Ney. 2021.

</span>
<span class="ltx_bibblock">Efficient retrieval augmented generation from unstructured knowledge for task-oriented dialog.

</span>
<span class="ltx_bibblock"><em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.04643</em>.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vincent et&nbsp;al. (2008)</span>
<span class="ltx_bibblock">
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008.

</span>
<span class="ltx_bibblock">Extracting and composing robust features with denoising autoencoders.

</span>
<span class="ltx_bibblock">In <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th international conference on Machine learning</em>, pages 1096â€“1103.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo&nbsp;Li, Mohammad Shoeybi, and Bryan Catanzaro. 2023a.

</span>
<span class="ltx_bibblock">Instructretro: Instruction tuning post retrieval-augmented pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.07713</em>.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, and Furu Wei. 2023b.

</span>
<span class="ltx_bibblock">Learning to retrieve in-context examples for large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.07164</em>.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023c)</span>
<span class="ltx_bibblock">
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md&nbsp;Rizwan Parvez, and Graham Neubig. 2023c.

</span>
<span class="ltx_bibblock">Learning to filter context for retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.08377</em>.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
BigScience Workshop, Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana IliÄ‡, Daniel Hesslow, Roman CastagnÃ©, Alexandra&nbsp;Sasha Luccioni, FranÃ§ois Yvon, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em>.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024.

</span>
<span class="ltx_bibblock">Benchmarking retrieval-augmented generation for medicine.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.13178</em>.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.

</span>
<span class="ltx_bibblock">Recomp: Improving retrieval-augmented lms with compression and selective augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.04408</em>.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024.

</span>
<span class="ltx_bibblock">Corrective retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.15884</em>.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. 2023.

</span>
<span class="ltx_bibblock">Prca: Fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter.

</span>
<span class="ltx_bibblock"><em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.18347</em>.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William&nbsp;W Cohen, Ruslan Salakhutdinov, and Christopher&nbsp;D Manning. 2018.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.09600</em>.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao and Guan (2018)</span>
<span class="ltx_bibblock">
Lirong Yao and Yazhuo Guan. 2018.

</span>
<span class="ltx_bibblock">An improved lstm structure for natural language processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference of safety produce informatization (IICSPI)</em>, pages 565â€“569. IEEE.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2022.

</span>
<span class="ltx_bibblock">Retrieval-augmented multimodal language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.12561</em>.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Haoran Ye, Jiarui Wang, Zhiguang Cao, and Guojie Song. 2024.

</span>
<span class="ltx_bibblock">Reevo: Large language models as hyper-heuristics with reflective evolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.01145</em>.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. 2019.

</span>
<span class="ltx_bibblock">Memory augmented deep recurrent neural network for video question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>, 31(9):3159â€“3167.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Wenpeng Yin, Katharina Kann, Mo&nbsp;Yu, and Hinrich SchÃ¼tze. 2017.

</span>
<span class="ltx_bibblock">Comparative study of cnn and rnn for natural language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1702.01923</em>.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yogatama et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Dani Yogatama, Cyprien de&nbsp;Masson&nbsp;dâ€™Autume, and Lingpeng Kong. 2021.

</span>
<span class="ltx_bibblock">Adaptive semiparametric language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 9:362â€“373.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023.

</span>
<span class="ltx_bibblock">Making retrieval-augmented language models robust to irrelevant context.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01558</em>.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu and Yang (2023)</span>
<span class="ltx_bibblock">
Donghan Yu and Yiming Yang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-enhanced generative model for large-scale knowledge graph completion.

</span>
<span class="ltx_bibblock">In <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pages 2334â€“2338.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023.

</span>
<span class="ltx_bibblock">Improving language models via plug-and-play retrieval feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14002</em>.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Yi&nbsp;Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark&nbsp;D Plumbley, and Wenwu Wang. 2024.

</span>
<span class="ltx_bibblock">Retrieval-augmented text-to-audio generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 581â€“585. IEEE.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuksekgonul et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. 2023.

</span>
<span class="ltx_bibblock">Attention satisfies: A constraint-satisfaction lens on factual errors of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15098</em>.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zan et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2022.

</span>
<span class="ltx_bibblock">When language model meets private library.

</span>
<span class="ltx_bibblock"><em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.17236</em>.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Sennrich (2019)</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich. 2019.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 32.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, et&nbsp;al. 2022.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01068</em>.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024a)</span>
<span class="ltx_bibblock">
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024a.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for ai-generated content: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.19473</em>.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan&nbsp;Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Retrieving multimodal information for augmented generation: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.10868</em>.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024b)</span>
<span class="ltx_bibblock">
Weichen Zhao, Yuxing Lu, Ge&nbsp;Jiao, and Yuan Yang. 2024b.

</span>
<span class="ltx_bibblock">Concentrated reasoning and unified reconstruction for multi-modal media manipulation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 8190â€“8194. IEEE.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2024c)</span>
<span class="ltx_bibblock">
Weichen Zhao, Yuxing Lu, Ge&nbsp;Jiao, and Yuan Yang. 2024c.

</span>
<span class="ltx_bibblock">Dual-color granularity alignment for text-based person search.

</span>
<span class="ltx_bibblock">In <em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 8075â€“8079. IEEE.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zexuan Zhong, Tao Lei, and Danqi Chen. 2022.

</span>
<span class="ltx_bibblock">Training language models with memory augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.12674</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.19542" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.19543" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2404.19543">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.19543" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.19544" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 19:04:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
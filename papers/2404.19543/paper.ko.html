<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# RAG와 RAU: 자연어 처리에서의 검색-증강 언어 모델 연구\n' +
      '\n' +
      'Yucheng Hu\n' +
      '\n' +
      '동중국과학기술대학교\n' +
      '\n' +
      'huyc@mail.ecust.edu.cn\n' +
      '\n' +
      '&Yuxing Lu\n' +
      '\n' +
      'Peking University\n' +
      '\n' +
      'yxlu@613@gmail.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대규모 언어 모델(LLM)은 자연어 처리(NLP)의 상당한 발전을 촉진했지만 환각과 도메인 특정 지식의 필요성과 같은 문제에 직면한다. 이를 완화하기 위해 최근 방법론에서는 외부 리소스에서 검색된 정보를 LLM과 통합하여 NLP 작업 전반에 걸쳐 성능을 크게 향상시켰다. 본 연구는 RALM(Retrieval-Augmented Language Models), RAG(Retrieval-Augmented Generation) 및 RAU(Retrieval-Augmented Understanding)에 대한 포괄적인 개요의 부재를 다루며, 이들의 패러다임, 진화, 분류 및 응용에 대한 심층적인 검토를 제공한다. 본 논문은 검색, 언어 모델, 증강을 포함한 RALM의 필수 구성 요소와 이들의 상호 작용이 다양한 모델 구조와 응용으로 이어지는 방법에 대해 논의한다. RALM은 번역 및 대화 시스템에서 지식 집약적인 응용 프로그램에 이르기까지 다양한 작업 범위에서 유용성을 보여준다. 설문 조사에는 RALM의 여러 평가 방법이 포함되어 있으며 평가에서 견고성, 정확성 및 관련성의 중요성을 강조한다. 또한 RALM의 한계, 특히 검색 품질 및 계산 효율성에 대한 한계를 인정하여 향후 연구에 대한 방향을 제공한다. 결론적으로, 이 조사는 RALM, 그들의 잠재력 및 NLP의 미래 개발을 위한 방법에 대한 구조화된 통찰력을 제공하는 것을 목표로 한다. 이 문서는 [https://github.com/2471023025/RALM_Survey](https://github.com/2471023025/RALM_Survey)와 같은 추가 연구를 위해 조사된 작업 및 리소스가 포함된 Github 리포지토리로 보완됩니다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '자연 언어 처리(NLP)는 자연 언어를 사용하여 인간과 컴퓨터 간의 효과적인 커뮤니케이션을 가능하게 하는 이론적 및 방법론적 프레임워크의 연구에 전념하여 컴퓨터 과학 및 인공 지능의 영역 내에서 중요한 초점이다. 다학문 분야로서 NLP는 인간의 언어와 컴퓨터 데이터 간의 상호 변환을 실현하기 위한 목적으로 언어학, 컴퓨터 과학, 수학을 통합한다. 그것의 궁극적인 목표는 컴퓨터에 자연어를 처리하고 "이해"할 수 있는 능력을 부여하여 자동 번역, 텍스트 분류 및 감정 분석과 같은 작업을 촉진하는 것이다. NLP의 복잡성은 단어 분할, 품사 태깅, 구문 분석, 형태소 분석, 명명된 개체 인식 등을 포함하는 수많은 단계에서 분명하며, 이들 모두는 인공 지능 시스템에서 인간 언어 이해를 복제하는 문제에 기여한다.\n' +
      '\n' +
      '전통적인 자연어 처리 태스크들은 통상적으로 통계 기반 알고리즘들(Hogenboom et al., 2010)Serra et al.(2013)Aussenac-Gilles and Sorgel (2005) 및 딥 러닝 알고리즘들, 예컨대 컨볼루션 뉴럴 네트워크(CNN)(Yin et al., 2017), 순환 뉴럴 네트워크(RNN)(Banerjee et al., 2019), 롱 숏-텀 메모리 네트워크(LSTM)(Yao and Guan, 2018) 등을 포함한다. 최근 자연어 처리를 대표하는 대표적인 트랜스포머 아키텍처(Vaswani et al., 2017)의 등장으로 그 인기가 크게 성장하였다. 트랜스포머 아키텍처는 자연어 처리 영역에서 두드러진 대형 언어 모델(Lewis et al., 2019)Raffel et al.(2020)로서 지속적으로 향상된 성능을 보여 왔으며, 그 성능을 연구하는 연구자들의 관심이 증가하고 있다.\n' +
      '\n' +
      '오늘날 가장 널리 퍼진 LMs는 GPT 패밀리(Radford et al., 2019)(Brown et al., 2020)(Achiam et al., 2023) 및 Bert 패밀리(Liu et al., 2019)(Devlin et al., 2018)(Sanh et al., 2019)이며, 이는 다수의 자연어 처리 작업에서 탁월한 것으로 입증되었다. 이 중 AutoEncoder 언어 모델은 자연어 이해 작업에 특히 능숙하고, AutoRegressive 언어 모델은 자연어 생성 작업에 더 적합하다. 매개변수를 증가시키는 것(Touvron et al., 2023b) 및 모델 튜닝(Han et al., 2023)은 LLM의 성능을 향상시킬 수 있지만, "환각"(Ji et al., 2023)의 현상은 지속된다. 또한, 지식 집약적인 작업을 효과적으로 처리하는 LMs의 한계(Feng 등, 2023) 및 그들의 지식을 신속하게 업데이트하지 못하는 것(Mousavi 등, 2024)은 일관되게 명백하다. 그 결과, 많은 연구자들(Lewis et al., 2020)(Izacard and Grave, 2020b)(Khandelwal et al., 2019)은 외부 지식을 얻기 위해 검색 기술을 사용했으며, 이는 언어 모델이 다수의 태스크에서 향상된 성능을 달성하는 데 도움을 줄 수 있다.\n' +
      '\n' +
      '현재 LLM의 성능을 향상시키기 위한 검색 증강의 사용에 대한 조사가 부족하다. Zhao et al. (2023)은 멀티모달리티를 위한 RAG에 대한 작업에 대한 포괄적인 개요를 제공한다. Zhao et al.(2024a)은 인공지능 생성 콘텐츠(AIGC) 도메인에 대한 검색 증강 생성 기술의 활용에 초점을 맞추고 있다. 이 문서에서는 최근 RAG 작업에 대한 포괄적인 개요를 제공하지만 모든 관련 영역을 다루지는 않습니다. 또한, 기사는 전반적인 개발의 포괄적인 타임라인을 제공하기에 충분한 세부 사항이 부족하다. Gao et al. (2023)은 대형 모델에 대한 RAG의 향상을 조사한다. 이 문서에서는 최근 RAG 작업 중 일부를 요약하지만 검색기 및 생성기를 독립적으로 도입하여 후속 작업의 구성 요소와의 업그레이드 및 상호 작용에 도움이 되지 않습니다. Li 등(2022b)은 텍스트 생성에만 초점을 맞춘다. 그 기사는 숫자와 표가 적고, 내용도 적다.\n' +
      '\n' +
      '"그림 1" : 이 조사 작업의 일반적인 개요\n' +
      '\n' +
      '보다 추상적이어서 독자의 이해에 도움이 되지 않는다.\n' +
      '\n' +
      '또한, RAG에 대한 조사는 NLP에서 검색 강화 방법에서 이야기의 절반만을 보여준다. NLG와 관련된 태스크는 검색 향상 기술을 필요로 할 뿐만 아니라 NLU 태스크는 외부 정보를 필요로 한다. 현재까지 NLP의 스펙트럼 전반에 걸쳐 증강 검색 기술의 적용을 철저히 검토하는 포괄적인 조사는 부족하다. 현재 상황을 개선하기 위해 본 논문에서는 다음과 같은 공헌점을 제시한다.\n' +
      '\n' +
      '(1) 이 기사는 단순히 RAG와 관련된 작업에 초점을 맞추는 것이 아니라 RALM에 상당한 중점을 두고 NLP의 개념과 일치한다. 세대와 관련된 작업은 NLG와 일치하고 나머지 작업은 NLU와 일치한다.\n' +
      '\n' +
      '(2) RALM의 두 구성요소인 Retriever와 Language Model에 대해 상세히 기술하고, 이 두 구성요소의 서로 다른 인터랙션 모드를 처음으로 정밀하게 정의한다.\n' +
      '\n' +
      '(3) 연관된 제한들의 분석과 함께, 현재 RALM의 공통적이고 신규한 애플리케이션들의 요약과 함께, RALM 작업 스케줄의 포괄적인 개요가 제공된다. 이러한 한계점에 대한 잠재적인 해결책과 향후 연구 방향에 대한 권장 사항이 제안된다.\n' +
      '\n' +
      '그림 1은 RALM 방법의 프레임워크에 대한 일반적인 개요를 제공한다. 다음은 논문의 요약입니다. 섹션 2는 RALM을 정의합니다. 섹션 3은 RALM에서 리트리버의 작업에 대한 자세한 분류 및 요약을 제공한다. 섹션 4는 RALM에서 LM의 작업에 대한 자세한 분류 및 요약을 제공한다. 섹션 5는 RALM에 대한 특정 개선 사항의 분류 및 요약을 제공한다. RALM의 섹션 6은 검색된 데이터의 출처에 대한 분류 및 요약이다. 섹션 7은 RALM 응용 프로그램의 요약이다. 섹션 8은 RALM 평가 및 벤치마크를 요약한 것이다. 마지막으로 제9절은 기존 RALM의 한계와 향후 업무에 대한 방향에 대한 논의이다.\n' +
      '\n' +
      '## 2 Definition\n' +
      '\n' +
      '검색-증강 언어 모델(Retrieval-Augmented Language Model, RALM)은 검색된 정보로 LM의 출력을 정제하여 사용자에게 만족스러운 결과를 얻는 과정이다. 이 섹션에서는 검색기가 언어 모델과 상호 작용하는 방식을 범주화하여 RALM의 다양한 모드에 대한 자세한 정의를 제공합니다. 상호 작용의 구체적인 범주화는 그림 2에서 확인할 수 있다. 또한 각 상호 작용 방법의 발전 이력은 그림 3에서 확인할 수 있다. \\(z\\)가 검색된 메시지, \\(x\\)가 입력, \\(y\\)가 출력, \\(F()\\)가 언어 모델 또는 데이터 처리 함수 중 하나이며, \\(x\\)와 \\(z\\)를 독립 변수로 한다고 가정하면 RALM의 기본 아키텍처는 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[y=F(x,z) \\tag{1}\\]\n' +
      '\n' +
      '### 순차적 단일 상호 작용\n' +
      '\n' +
      '순차적인 단일 상호작용 과정은 검색기 \\(P_{\\eta}(z|x)\\)를 통해 입력 \\(x\\)에 대한 Top-K 관련 문서 \\(z\\)를 찾는 것을 포함하며, 여기서 \\(\\eta\\)는 검색기의 매개변수이다. 이어서, 언어 모델 \\(P_{\\theta}(y_{i}|x,z,y_{r})\\)은 관련 문서 \\(z\\)와 함께 입력 \\(x\\)을 입력받아 i번째 토큰 \\(y_{i}\\)을 출력한다. 매개변수 \\(\\theta\\)는 관련 출력 토큰 \\(y_{r}\\)과 함께 사용됩니다. 관련 출력 토큰의 수는 언어 모델의 위치 및 유형과 관련이 있다. 순차적인 단일 상호작용을 위한 RALM은 다음과 같이 정의된다:\n' +
      '\n' +
      '\\[y_{i}=LM(z,x,y_{r}) \\tag{2}\\]\n' +
      '\n' +
      'RALM이 처음 제안되었을 때 많은 연구자들이 이 방법을 사용했는데, 특히 Lewis et al.(2020), Guu et al.(2020), Izacard and Grave(2020)의 아이디어와 일치했기 때문이다.\n' +
      '\n' +
      '### 순차적 다중 상호 작용\n' +
      '\n' +
      'RALM 기술이 발전함에 따라 연구자들은 긴 대화 생성과 다중 홉 문제를 해결하기 위해 단일 상호작용이 불충분하다는 것을 발견했다. 따라서, 리트리버와 언어 모델 사이의 다중 상호작용을 갖는 방법이 제안되었다. 이 방법에서, 연구자는 단계 s를 포함하고 전형적으로 언어 모델이 먼저 출력을 생성하도록 한다. 검색 기술이 필요한 경우, 출력된 콘텐츠를 검색에 사용하고 관련 수식은 다음과 같이 표현된다:\n' +
      '\n' +
      '\\[y_{s}=LM(z,x|y_{<s}) \\tag{3}\\]\n' +
      '\n' +
      '여기서, \\(y_{s}\\)는 현재 단계 \\(s\\)에서 생성된 토큰을 나타낸다. 이 방법을 사용한 연구자들 중 가장 유명한 것은 Jiang 등(2023), He 등(2022), Khattab 등(2022)이다.\n' +
      '\n' +
      '### Parallel Interaction\n' +
      '\n' +
      '앞서 언급한 모든 접근법에서 정보의 흐름은 리트리버에서 언어 모델에 이르기까지 또는 언어 모델에서 리트리버에 이르기까지 명확한 순차적 구조를 갖는다. 그러나, 이러한 순차적 구조는 모든 도메인에서 최적이 아닐 수 있고 덜 확장가능할 수 있으므로, 대안적인 접근법을 고려하는 것이 중요하다. 연구자들은 사용자 입력에 대해 리트리버와 언어 모델이 독립적으로 동작하는 새로운 병렬 구조를 제안하였다 \\(x\\). 그런 다음 출력 \\(y\\)은 가중 보간법에 의해 결정된다 \\ (I()\\)는 보간 함수이다. 관련 방정식은 다음과 같이 표현된다:\n' +
      '\n' +
      '\\[y=I(LM(x,y_{r}),z) \\tag{4}\\]\n' +
      '\n' +
      '상기 특정 보간 함수는:\n' +
      '\n' +
      '\\[p(y|x)=\\lambda p_{R}(y|x)+(1-\\lambda)p_{LM}(y|x) \\tag{5}\\]\n' +
      '\n' +
      '여기서 검색된 출력 토큰은 \\(p_{R}(y|x)\\), 언어 모델 생성된 출력 토큰은 \\(p_{LM}(y|x)\\) 및 가중치는 \\(\\lambda\\)로 표시된다. 이 방법을 사용한 연구자 중 가장 유명한 것은 Khandelwal et al.(2019), Alon et al.(2022), He et al.(2021)이다.\n' +
      '\n' +
      '## 3 Retriever\n' +
      '\n' +
      '리트리버는 RALM 아키텍처에서 중요한 역할을 한다. 리트리버를 통해 얻은 정보는 LM의 정확도를 크게 향상시킬 수 있다. 이 섹션에서는 RALM 아키텍처에서 일반적으로 사용되는 검색 방법에 대한 요약을 제공한다. 검색 방법은 그 방법과 출처에 따라 희소 검색, 밀도 검색, 인터넷 검색, 하이브리드 검색의 네 가지 범주로 분류된다. 표 1은 RALM에서 리트리버의 특정 애플리케이션에 대한 정보를 나열한다.\n' +
      '\n' +
      '### Sparse Retriever\n' +
      '\n' +
      '검색 기법의 제안 이후 일정 기간 동안 희소 검색은 문제, 특히 지식을 기반으로 하는 문제를 해결하는 데 간단하고 효과적인 도구임을 증명한다. 희소 검색의 주요 장점 중 하나는 단순하기 때문에 관련된 차원이 적기 때문에 기존 색인 시스템에 쉽게 통합될 수 있다는 것이다. (Hambarde and Proenca, 2023) 이것은 인간의 인지 과정과 일치합니다. 또한 희소 검색은 일반화하기 쉽고 효율적이다. RALM에서 사용되는 희소 검색은 단어 빈도와\n' +
      '\n' +
      '그림 2: 리트리버가 LMSparse 벡터 표현과 상호 작용하는 세 가지 다른 방식. 양자 간의 선택은 기계 학습의 활용 여부에 달려 있다.\n' +
      '\n' +
      '#### 3.1.1 단어 빈도\n' +
      '\n' +
      '초기 단계에서, 개인들은 종종 고전적이고 효과적인 것으로 간주되는 TF-IDF Ramos 등(2003) 및 BM25 Robertson 등(1995) 알고리즘과 같은 관련 콘텐츠의 매칭을 수반하는 검색 방법을 사용한다.\n' +
      '\n' +
      'TF-IDF 알고리즘은 용어 빈도(TF)와 역 문서 빈도(IDF)를 활용하여 관련성을 표현함으로써 단순성과 신속성의 장점을 가지며, 말뭉치가 변하지 않더라도 단어별 TF-IDF 값을 미리 계산할 수 있다. RALM에서 Lazaridou 등(2022)은 TF-IDF 알고리즘을 사용하여 사용자 쿼리 및 호출에서 얻은 정보를 Google 검색 API에 일치시킨다. Hua and Wang (2018)은 또한 생성된 결과들을 스코어링하기 위해 알고리즘을 채용한다. BM25는 TF-IDF에 대한 향상을 나타낸다. 사용자의 질의를 고려하여 관련도 점수를 문서에 대한 각 질의어의 관련도의 가중합으로 계산한다. IDF 알고리즘은 각 단어의 가중치를 도출하는데 사용되지만, 특정 요인의 영향력의 강도가 무한하지 않도록 두 가지 조절 요인에 의해 개선된다. 이것은 …과 일치한다.\n' +
      '\n' +
      '그림 3: 세 가지 유형의 상호 작용에 대한 로드맵입니다. 보라색 영역은 순차적 상호작용 RALM 모델에 대한 작업을 나타내고 빨간색 상자는 순차적 다중 상호작용 RALM 모델에 대한 작업을 나타내며 노란색 영역은 병렬적 상호작용 RALM 모델에 대한 작업을 나타낸다.\n' +
      '\n' +
      '상식. 우수한 일반화 능력으로 인해 많은 검색-증강 언어 모델(RaLM: Retrieval-Augmented Language Model) 아키텍처, 특히 오픈 도메인을 지향하는 아키텍처는 BM25를 검색 방법으로 사용하고 있는데, Jiang et al. (2023c), Ram et al. (2023) 및 Zhong et al. (2022).\n' +
      '\n' +
      '#### 3.1.2 Sparse Vector Representation\n' +
      '\n' +
      '단순한 용어 매칭이 더 이상 수요를 충족시키기에 충분하지 않다는 것이 명백해졌다. 수동 라벨링은 동의어 문제와 같은 문제를 해결할 수 있지만 자원 집약적인 방법이다. 기계 학습의 증가에 따라, 희소 벡터는 이제 단어들을 표현하고 그들 사이의 거리를 계산함으로써 단어들을 검색하는데 사용된다. (Hambarde and Proenca, 2023) 희소 벡터 표현 기법은 질의 및 문서에 대한 희소 벡터를 구성한다는 점에서 용어 매칭 방법과 다르다. 이러한 표현의 목적은 질의와 문서를 잠재 공간에 배치하는 각 입력 텍스트의 의미적 본질을 포착하는 것이다.\n' +
      '\n' +
      'Ram et al.(2021)은 동일한 반복 스팬을 갖는 두 개의 단락이 주어졌을 때, 하나는 질의를 구성하는 데 사용되고 다른 하나는 검색 대상으로 사용된다는 사실을 활용했다. 반복 스팬이 포함되지 않은 문서의 나머지 단락은 부정적인 예로 사용되었으며 Ram\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|l|l|l|l|} \\hline \\multicolumn{1}{|c|}{ Category} & Technique & Year & Reference \\\\ \\hline \\multirow{10}{*}{Sparse Retrieval} & \\multirow{8}{*}{Word Frequency} & \\multirow{8}{*}{BM25(Robertson et al., 1995)} & 2023 & (Jiang et al., 2023c) \\\\ \\cline{3-5}  & & & 2023 & (Ram et al., 2023) \\\\ \\cline{3-5}  & & & 2022 & (Zhong et al., 2022) \\\\ \\cline{3-5}  & & & 2024 & (Cheng et al., 2024) \\\\ \\cline{3-5}  & & & 2020 & (Lazard and Ghare, 2020b) \\\\ \\cline{3-5}  & & & 2022 & (Mallen et al., 2022) \\\\ \\cline{3-5}  & & & 2024 & (Schield et al., 2024) \\\\ \\cline{3-5}  & & & 2023 & (Xu et al., 2023) \\\\ \\cline{3-5}  & & & 2022 & (Hle et al., 2022) \\\\ \\cline{3-5}  & & & 2023 & (Yu et al., 2023) \\\\ \\cline{3-5}  & & & 2022 & (Ado et al., 2022) \\\\ \\cline{3-5}  & & KNN search & 2022 & (Borgnod et al., 2022) \\\\ \\cline{3-5}  & & & 2019 & (Khandelwal et al., 2019) \\\\ \\cline{3-5}  & & GUD-IR(Madan et al., 2022) & 2022 & (Madan et al., 2022) \\\\ \\cline{3-5}  & & GAR(Mao et al., 2020) & 2020 & (Mao et al., 2020) \\\\ \\cline{3-5}  & & Spider(Ram et al., 2021) & 2023 & (Ram et al., 2023) \\\\ \\hline \\multirow{10}{*}{Dense Retrieval} & \\multirow{8}{*}{COLBERTY2(Santhanam et al., 2021)} & 2022 & (Izacard et al., 2022) \\\\ \\cline{3-5}  & & & 2021 & (Thulley et al., 2021) \\\\ \\cline{3-5}  & & & 2023 & (Holkatner et al., 2023) \\\\ \\cline{3-5}  & & & 2023 & (Sriwardhama et al., 2023) \\\\ \\cline{3-5}  & & & 2021 & (Sachan et al., 2021) \\\\ \\cline{3-5}  & & & 2020 & (Guan et al., 2020) \\\\ \\cline{3-5}  & & & 2020 & (Ram et al., 2023) \\\\ \\cline{3-5}  & & & 2020 & (Lazard and Ghare, 2020b) \\\\ \\cline{3-5}  & & & 2020 & (Karpukhin et al., 2020) \\\\ \\cline{3-5}  & & & 2020 & (Karpukhin et al., 2020) \\\\ \\cline{3-5}  & & & 2022 & (Mallen et al., 2022) \\\\ \\cline{3-5}  & & & 2020 & (Izacard and Ghare, 2020a) \\\\ \\cline{3-5}  & & & 2021 & (Izacard and Ghare, 2020b) \\\\ \\cline{3-5}  & & & 2021 & (Izacard and Ghare, 2020a) \\\\ \\cline{3-5}  & & & 2021 & (Izacard et al., 2019) \\\\ \\cline{3-5}  & & & 2021 & (He et al., 2021) \\\\ \\cline{3-5}  & & & 2023 & (Yoran et al., 2023) \\\\ \\cline{3-5}  & & & 2022 & (Mallen et al., 2022) \\\\ \\cline{3-5}  & & & 2021 & (Santharam et al., 2021) \\\\ \\hline \\multirow{10}{*}{Multimodal Retrieval} & MultiMedia(Chen et al., 2022b) & 2022 & (Chen et al., 2022b) \\\\ \\cline{3-5}  & & & 2022 & (Yasumga et al., 2022) \\\\ \\cline{3-5}  & & & 2022 & (Chen et al., 2022) \\\\ \\cline{3-5}  & & & 2022 & (Chen et al., 2022) \\\\ \\cline{3-5}  & & & 2022 & (Chen et al., 2022) \\\\ \\cline{3-5}  & & & 2022 & (Li et al., 2022a) \\\\ \\cline{3-5}  & & & 2022 & (Bittmann et al., 2022) \\\\ \\cline{3-5}  & & & 2023 & (Lin et al., 2023b) \\\\ \\cline{3-5}  & & & 2021 & (Shi et al., 2023b) \\\\ \\hline \\multirow{10}{*}{Internet Retrieval} & \\multirow{8}{*}{Hybrid Retrieval} & FLARE(Jiang et al., 2023c) & 2023 & (Jiang et al., 2023c) \\\\ \\cline{3-5}  & & & 2023 & (Yoran et al., 2023) \\\\ \\cline{3-5}  & & & 2021 & (Konelli et al., 2021) \\\\ \\cline{1-1} \\cline{3-5}  & & & 2021 & (Nakano et al., 2021) \\\\ \\hline \\multirow{10}{*}{Hybrid Retrieval} & \\multirow{8}{*}{Hybrid Retrieval} & Doc\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      '2021)은 종종 임베디드 매칭과 같은 방법을 사용하여 미리 훈련된 더 큰 모델에서 더 작은 모델로 데이터를 전송하는 것을 포함한다. 기술이 발전함에 따라 LMs를 이용한 데이터 증류에도 연구가 진행되었다.\n' +
      '\n' +
      'Shi et al. (2023)은 지식증류를 사용하여 검색 프로세스를 4개의 별개의 단계로 나눈다. 먼저, 문서를 검색하고 검색 가능성을 계산한다. 두 번째로, 검색된 문서들은 언어 모델을 이용하여 점수를 매긴다. 셋째, 검색 확률과 언어 모델 점수의 분포 사이의 KL 불일치를 최소화함으로써 검색 모델의 파라미터를 갱신한다. 마지막으로, 데이터의 인덱스들의 비동기식 업데이트가 수행된다. 이 기술을 기반으로 Lin et al.(2023)은 지식증류의 정확도를 더욱 향상시킨다. 그들은 여러 인핸서를 사용하여 문장 절단 및 쿼리 강화와 증분 관련성 라벨 강화를 결합하는 데이터 증류 스킴을 제시한다.\n' +
      '\n' +
      '### Internet Retrieval\n' +
      '\n' +
      '인터넷 검색 및 분류 기술의 발전으로 일부 연구자들은 플러그 앤 플레이 접근 방식인 인터넷 검색에 검색 노력을 집중했다. 이 접근법은 비전문가가 RALM의 혜택을 받을 수 있도록 하며 개방형 도메인과 일반화에 더 적합하다. 이 검색 모델의 또 다른 장점은 데이터베이스의 실시간 업데이트를 필요로 하지 않고 상용 검색 엔진으로부터의 업데이트에 의존한다는 것이다. 그러나, 단순성과 편리성의 장점에도 불구하고, RALM의 작업을 방해할 수 있는 인터넷 상의 상당량의 무관하고 심지어 유해한 정보가 존재한다. 효과적인 스크리닝 메커니즘이 구현되지 않으면 RALM의 효과가 크게 감소한다.\n' +
      '\n' +
      '상업 검색 엔진 API를 직접 사용하는 대부분의 연구(Yoran 등, 2023)(Nakano 등, 2021)와 달리. Komeili 등(2021)은 다중 상용 검색 엔진 API를 사용하는 대안적인 접근법을 제안한다. Bing Search API를 사용하여 각 쿼리에 대한 URL 목록을 생성하는 것이 좋습니다. 그런 다음 이러한 URL은 해당 쿼리에 대한 페이지 집합을 채우는 공용 크롤링 스냅숏에서 구성된 조회 테이블에서 페이지 콘텐츠를 검색하는 키로 사용됩니다. 또한, 평가는 URL이 영어 위키피디아로부터의 것인지 여부를 고려한다. 그렇다면 URL에서 페이지 제목을 추출하고 위키피디아 덤프에서 해당 페이지를 검색한다.\n' +
      '\n' +
      '### Hybrid Retrieval\n' +
      '\n' +
      '연구자들이 다양한 검색 기법들의 장단점에 대한 더 나은 이해를 얻음에 따라, 이들은 전술한 바와 같이, 이들을 조합하는 것을 점점 더 선택하고 있다. 이는 RALM 아키텍처의 효과성 및 견고성을 개선하기 위해 이러한 기술들의 장점들을 추가로 이용하기를 희망하여 수행된다.\n' +
      '\n' +
      '부정확한 인터넷 검색 결과의 문제를 해결하기 위해, Lazaridou 등(2022)은 검색 결과를 점수화하기 위해 TF-IDF 알고리즘을 사용하는 것을 제안했다. 그들은 각 질문 q를 쿼리로 사용하고 Google Search API를 통해 Google Search에 호출을 발행했다. 각 질문에 대해 상위 20개 URL을 검색하고 HTML 콘텐츠를 파싱하여 클린 텍스트를 추출하여 각 질문 q에 대한 문서 D 세트를 생성했다. 관련없는 정보가 사용자의 질의의 해상도를 저해하지 않도록 Hu et al.(2023)은 게이팅 회로를 설계하였다. 이 회로는 유사성을 계산하기 위해 이중 인코더 내적을 사용하고 용어 가중치에 기반한 게이팅 회로를 사용했다. 추가적으로 Boytsov et al.(2016)은 검색 성능을 향상시키기 위해 번역 모델과 BM25를 결합하면서 용어 기반 검색을 k-Nearest Neighbors(kNN) 검색으로 대체하는 접근법을 제시하였다. 이 접근법은 모델이 용어와 전통적인 통계적 가중치 스킴 사이의 의미론적 관계를 고려할 수 있게 하여 보다 효율적인 검색 시스템을 초래했다.\n' +
      '\n' +
      '## 4 언어 모델\n' +
      '\n' +
      '비록 인간이 정보 검색에만 의존하곤 했지만, 언어 모델의 발전은 자연 언어 처리 분야에 혁명을 일으켜 더 활기차고 창의적이었다. 학습에서 도출된 파라미터만을 사용하여 작업을 완료하는 LM과 달리 RALM은 리트리버가 획득한 비모수적 메모리와 LM 자체의 모수적 메모리를 통합하여 반모수적 메모리를 생성함으로써 언어 모델의 성능을 향상시킨다. RALM 아키텍처에서 많은 연구자들은 평가를 위해 기성 언어 모델을 사용한다. 이 절에서는 RALM 아키텍처에서 일반적으로 사용되는 언어 모델을 소개하고 AutoEncoder-language 모델, AutoRegressive language 모델 및 Encoder-Decoder 모델의 세 가지 범주로 분류한다. 표 2는 RALM에서 LM의 특정 응용에 대한 정보를 나열한 것이다.\n' +
      '\n' +
      '### AutoEncoder 언어 모델\n' +
      '\n' +
      'AutoEncoder의 논리적 과정은 원래 입력(x로 설정)에 가중치가 부여되고 y에 매핑되며, 그 다음 역 가중치가 부여되고 z에 다시 매핑된다. 반복 훈련을 통해 손실 함수 L(H)가 최소화되고, 즉 z가 가능한 한 x에 가깝고, 즉 x가 완벽하게 재구성된다면, 순방향 가중치는 입력의 주요 특징들을 학습하는 성공적인 방법이라고 말할 수 있다. AutoEncoder 언어 모델들은 그들의 이름을 Denoising AutoEncoder(DAE)(Vincent et al., 2008)로부터 가져오는데, 이는 문맥 단어들에 의해 [마스크된] 토큰들을 예측하는 데 사용된다(이러한 [마스크된] 단어들은 실제로 입력에서 추가된 노이즈, 사고의 전형이다). DAE는 데이터의 입력 계층에 랜덤 노이즈를 추가하는 것을 포함하는 기술이다. 이것은 계층적 방식으로 딥 네트워크의 가중치들을 사전 트레이닝하기 위해 감독되지 않은 접근법을 사용할 때 보다 강건한 특징들을 학습하는 것을 돕는다.\n' +
      '\n' +
      '오토인코더 언어 모델의 대부분은 일반성이 높고 감독되지 않으며 데이터 주석이 필요하지 않다. 그들은 자연스럽게 문맥적 의미 정보를 통합할 수 있다. 그러나 사전 훈련 단계에서 도입된 독립성 가정은 상호 간의 상관 관계를 의미한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|c|l|l|l|} \\hline Category & Technique & Year & Reference \\\\ \\hline \\multirow{3}{*}{AutoEncoder Language Model} & RoBERTaDevlin et al. (2018) & 2021 & Thulke et al. (2021) \\\\ \\cline{2-4}  & BERTLiu et al. (2019) & 2021 & Sachan et al. (2021) \\\\ \\hline \\multirow{9}{*}{AutoRegressive Language Model} & GPT-3.5 & 2023 & Jiang et al. (2023c) \\\\ \\cline{2-4}  & GPT-2Rafford et al. (2019) & 2023 & Khattab et al. (2022) \\\\ \\cline{2-4}  & GPT-NeoBlack et al. (2022) & 2022 & Mallen et al. (2022) \\\\ \\cline{2-4}  & chatGPT & 2023 & Feng et al. (2023) \\\\ \\cline{2-4}  & GPT Family & GPT-4Achiam et al. (2023) & 2023 & Asai et al. (2023) \\\\ \\cline{2-4}  & GPT-3Brown et al. (2020) & 2022 & Maoan et al. (2022) \\\\ \\cline{2-4}  & GPT-3Brown et al. (2020) & 2022 & He et al. (2022) \\\\ \\cline{2-4}  & GPTRafford et al. (2018) & 2023 & Shi et al. (2023b) \\\\ \\cline{2-4}  & GPT-J & 2024 & Schick et al. (2024) \\\\ \\cline{2-4}  & GPT-J & 2024 & Hu et al. (2024) \\\\ \\cline{2-4}  & \\multirow{3}{*}{Llama Family} & 2024 & Yan et al. (2024) \\\\ \\cline{2-4}  & & 2023 & Asai et al. (2023) \\\\ \\cline{2-4}  & & 2023 & Wang et al. (2023c) \\\\ \\cline{2-4}  & & 2023 & Yoran et al. (2023) \\\\ \\cline{2-4}  & & 2023 & Lin et al. (2023b) \\\\ \\cline{2-4}  & & 2023 & Luo et al. (2023a) \\\\ \\hline \\multirow{9}{*}{Others} & AlpacaDubois et al. (2024) & 2024 & Yan et al. (2024) \\\\ \\cline{2-4}  & OPTZhang et al. (2022) & 2023 & Ram et al. (2023) \\\\ \\cline{2-4}  & XGHMLin et al. (2022) & 2024 & Cheng et al. (2024) \\\\ \\cline{2-4}  & BLOOMWorkshop et al. (2022) & 2023 & Shi et al. (2023b) \\\\ \\cline{2-4}  & MistralJiang et al. (2023a) & 2024 & Hu et al. (2024) \\\\ \\hline \\multirow{9}{*}{Encoder-Decoder Language Model} & & 2022 & Izaard et al. (2022) \\\\ \\cline{2-4}  & & 2023 & Hofstatter et al. (2023) \\\\ \\cline{2-4}  & & 2021 & Sachan et al. (2021) \\\\ \\cline{2-4}  & & 2023 & Wang et al. (2023c) \\\\ \\cline{2-4}  & T5Raffel et al. (2020) & 2022 & Chen et al. (2022b) \\\\ \\cline{2-4}  & & 2021 & Singh et al. (2021) \\\\ \\cline{2-4}  & & 2020 & Izaard and Grave (2020a) \\\\ \\cline{2-4}  & & 2022 & Lazaridou et al. (2022) \\\\ \\cline{2-4}  & & 2023 & Hu et al. (2023) \\\\ \\cline{2-4}  & & 2021 & Thulke et al. (2021) \\\\ \\cline{2-4}  & & 2023 & Siwirandana et al. (2023) \\\\ \\cline{2-4}  & & 2019 & Lewis et al. (2019) \\\\ \\cline{2-4}  & BARTlLewis et al. (2019) & 2020 & Lewis et al. (2020) \\\\ \\cline{2-4}  & & 2023 & Yoran et al. (2023) \\\\ \\cline{2-4}  & & 2020 & Izaard and Grave (2020a) \\\\ \\cline{2-4}  & & 2022 & Lazaridou et al. (2022) \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: RALM 방법에서의 LM의 요약.\n' +
      '\n' +
      'predicted [MASK]는 고려되지 않습니다. 또한 원본 토큰을 대체하기 위한 입력에서 [마스크]를 특수 마커로 도입하면 [마스크]가 없는 Pre-Training 단계와 Fine-Tuning 단계의 데이터 간에 불일치가 발생한다. 자체 인코딩 언어 모델은 자연 언어 이해(NLU) 작업을 위해 RALM 아키텍처에서 일반적으로 사용된다.\n' +
      '\n' +
      'AutoEncoder 언어 모델은 자연어 이해(Natural Language Understanding, NLU) 작업에서 우수하기 때문에 많은 RALM 아키텍처(Thulke et al., 2021)(Cheng et al., 2024)(Sachan et al., 2021)가 판단과 같은 특정 작업에 대해 이를 활용한다. 가장 일반적으로 사용되는 모델 중 하나는 BERT 및 개선된 버전이다. Devlin 등(2018)은 폐쇄 태스크(Taylor, 1953)에서 영감을 받은 BERT 모델을 제안했다. RoBERTa(Liu et al., 2019)는 버트 모델의 학습 부족을 해결하기 위해 동적 마스킹, NSP 손실이 없는 풀 문장, 큰 미니 배치 및 더 큰 바이트 수준 BPE를 사용하여 학습된다. Jiang et al.(2020)에 따르면 BERT는 전역적 자기 주의 블록에 크게 의존하여 메모리 풋프린트와 계산 비용이 크게 발생한다. 모든 어텐션 헤드가 전체 입력 시퀀스를 쿼리하지만, 일부는 로컬 종속성을 학습하기만 하면 되어 계산 중복성이 발생한다. 이 문제를 해결하기 위해 그들은 이러한 자체 주의 헤드를 대체하고 로컬 종속성을 직접 모델링하는 새로운 스팬 기반 동적 컨벌루션을 제안했다. 새로운 콘볼루션 헤드는 다른 자기 주의 헤드들과 함께 하이브리드 주의 블록을 형성한다. 또한, Sanh et al.(2019)은 사전 훈련 단계에서 지식 증류를 구현하여 언어 이해 능력의 97%를 유지하면서 BERT 모델의 크기를 40% 줄일 수 있었고 속도의 60% 증가를 달성할 수 있었다.\n' +
      '\n' +
      '### AutoRegressive Language Model\n' +
      '\n' +
      'AutoRegressive 언어 모델의 주요 목적은 선행 단어들을 기반으로 다음 단어를 예측하는 것이다. 이것은 일반적으로 좌-우 언어 모델링으로 알려져 있으며, 여기서 현재 시간 t에서의 토큰은 제1 t-1 토큰들에 기초하여 예측된다.\n' +
      '\n' +
      '이 모델은 좌우라는 장점이 있어 대화 생성 및 기계 번역과 같은 생성적 자연어 처리 작업에 유리하다. 자동 회귀 언어 모델은 이 프로세스에 잘 적합하여 RALM 분야의 NLG 작업에 이 모델이 인기 있는 선택이다. 그러나, 문제의 정보는 선행 또는 후속 텍스트로부터만 활용될 수 있고, 둘 다와 조합될 수 없다. OpenAI는 자기회귀 언어 모델과 관련된 연구 분야에 주목할 만한 영향을 미쳤다. 최근 구글은 이 모델에 대한 연구에도 진전을 이루었다.\n' +
      '\n' +
      'GPT 계열은 AutoRegressive 언어 모델의 가장 일반적인 예 중 하나이다. 그것은 Radford et al.(2018)에 의해 처음 제안되었으며, 그는 감독되지 않은 사전 훈련에 이어 미세 조정의 기본 아키텍처를 확인했다. Radford et al.(2019)은 이후 GPT에 기반한 제로 샷 학습을 제안하였다. 나중에 Brown 등(2020)은 Radford 등(2019)과 유사한 접근법을 사용하여 GPT-3를 제안했는데, 이는 스케일링 업 및 미세 조정 포기를 포함했다. 그들은 또한 희소 변압기 Child 등(2019)과 유사하게 변압기 층에서 교대로 조밀하고 국부적으로 밴딩된 희소 주의 패턴을 활용했다. 또한 GPT-NEO(Black et al., 2022)와 ChatGPT(Reinforcement Learning from Human Feedback,RLHF)와 같은 여러 관련 연구가 있다. RLHF는 GPT 모델의 정확도를 크게 향상시켰다. ChatGPT는 오픈 소스가 아니지만, 많은 연구자 Jiang et al.(2023)(Khattab et al., 2022)은 여전히 RALM에서 생성 작업을 위해 API를 사용한다. 최근 GPT-4(Achiam et al., 2023)에 대한 보고서가 등장하여 GPT-4의 안전성과 정렬을 개선하기 위한 예측 가능하고 확장 가능한 딥 러닝 스택을 구축하는 데 중점을 두고 있다. 많은 연구자 Asai et al.(2023)(Luo et al., 2023)은 최근 GPT-4를 사용하여 RALM에 대한 프롬프트를 생성하였다.\n' +
      '\n' +
      '라마 패밀리는 자동 회귀 언어 모델의 잘 알려진 클래스이다. Llama(Touvron et al., 2023)는 공개된 데이터만을 사용하는 언어 모델로 처음 제안되었다. 훈련 안정성을 향상시키기 위해 출력을 정규화하는 대신 각 변압기 하위 계층의 입력을 정규화한다. 이들은 Zhang과 Sennrich(2019)가 도입한 RMSNorm 정규화 함수를 사용하고, 성능 향상을 위해 ReLU 비선형성을 Shazeer(2020)가 도입한 SwiGLU 활성화 함수로 대체한다. 또한, 저자들은 절대 위치 임베딩을 네트워크의 각 계층에서 Su 등(2024)이 도입한 회전 위치 임베딩(RoPE)으로 대체하였다. Llama2(Touvron et al., 2023)는 감독 미세 조정, 초기 및 반복 보상 모델링 및 RLHF를 실험에 사용했다. 그들은 또한 여러 번의 회전으로 대화의 흐름을 제어하는 데 도움이 되는 새로운 기술인 고스트 어텐션(Ghost Attention, GAtt)을 발명했습니다. Qwen(Bai et al., 2023)은 Llama를 기반으로 다음과 같이 조정하였다. 1. 메모리 비용을 절약하기 위해 입력 임베딩 및 출력 투영의 가중치를 묶는 대신 느슨한 임베딩 방법을 선택했다. 2. 역 주파수 매트릭스의 정확도가 향상되었다. 3. 대부분의 레이어에 대해 Chowdhery et al.(2023)에 따라 바이어스가 제거되었다. 그러나 모델의 외삽 능력을 향상시키기 위해 QKV 층에 바이어스를 추가했다. Ba 등(2016)에 기술된 전통적인 층 정규화 기법은 RMSNorm으로 대체되었다. 그들은 SwiGLU를 활성화 함수로 선택했는데, 이는 Swish Ramachandran et al.(2017)과 게이트 선형 단위(Dauphin et al., 2017)의 조합이다. 피드-포워드 네트워크(FFN)의 차원도 감소된다. 또한, 미스트랄 7b(Jiang et al., 2023)는 GQA(Grouped Query Attention)를 사용하여 추론 속도를 높이고 SWA(Sliding Window Attention)와 결합하여 추론 비용을 줄인 임의의 길이의 시퀀스를 효율적으로 처리한다. 이러한 기술은 Llama2보다 우수한 성능을 보여준다. Llama 모델은 오픈 소스이고 공개적으로 이용 가능한 데이터를 사용하여 연구자(Yan 등, 2024)(Asai 등, 2023)(Wang 등, 2023)에게 더 많은 확장 기회를 제공한다. 그 결과 많은 연구자들이 RALM 아키텍처에서 라마 패밀리를 언어 모델로 사용하고 있다.\n' +
      '\n' +
      '### Encoder-Decoder 언어 모델\n' +
      '\n' +
      'Transformer(Vaswani et al., 2017)는 "인코더-디코더" 구조로서, 멀티헤드 셀프-어텐션 모듈에 중첩된 인코더와 디코더로 구성된다. 이 중 입력 시퀀스는 소스 시퀀스와 목적지 시퀀스의 두 부분으로 나뉜다. 전자는 인코더에 입력되고 후자는 디코더에 입력되며, 두 시퀀스 모두 표현을 내장하고 위치 정보를 추가할 필요가 있다. 트랜스포머 구조는 병렬 연산과 전체 텍스트 시퀀스의 처리를 동시에 가능하게 하여 모델 학습과 추론 속도가 크게 증가한다.\n' +
      '\n' +
      'Raffel et al.(2020)은 모든 텍스트 기반 언어 문제를 텍스트 대 텍스트 형식으로 변환하는 통일된 프레임워크를 소개한다. 목적은 자연어 처리를 위한 전이학습 기법의 가능성을 탐색하는 것이다. 원래 변압기와 대조적으로, 활성화가 추가적인 편향 없이 재스케일링되는 계층 정규화의 단순화된 버전이 사용된다. 레이어 정규화를 적용한 후, 잔여 스킵 연결(He et al., 2016)은 각각의 서브컴포넌트의 입력을 그의 출력에 추가한다. Srivastava et al.(2014)는 피드포워드 네트워크, 스킵 연결, 어텐션 가중치, 및 전체 스택의 입력 및 출력에 적용된다. T5 모델은 Hofstatter et al.(2023), Sachan et al.(2021), Singh et al.(2021) 등 많은 연구자들에 의해 언어 모델로 널리 사용되어 왔다. 추가적으로 Chung et al.(2022)은 모델 성능 향상을 위한 접근 방법으로 명령어 튜닝을 제안하였다. 본 연구는 스케일링 작업의 수, 스케일링된 모델의 크기, 연쇄적 사고 데이터의 미세 조정이라는 세 가지 측면에 초점을 맞추었다. 그 결과 모델 크기가 클수록, 미세 조정 작업이 많을수록 모델 성능이 크게 향상되는 것으로 나타났다. 또한, CoT(Chain of Thinking)는 추론 수준을 유의하게 향상시키는 것으로 나타났다. Wang et al.(2023)은 T5를 튜닝하고 RALM 아키텍처에 적용하기 위해 이 접근법을 사용했다.\n' +
      '\n' +
      'BART(Lewis et al., 2019)는 인코더로의 입력이 디코더의 출력과 정렬할 필요가 없기 때문에 임의의 잡음 변환을 허용하는 인코더-디코더 모델이다. 이 경우 텍스트 범위를 마스크 기호로 대체하여 문서가 손상되었습니다. 사전 훈련을 위해 연구자들은 토큰 마스킹, 토큰 삭제, 텍스트 입력, 문장 순열 및 문서 회전의 5가지 모델을 제안했다. 미세-조정을 위해, 인코더 및 디코더는 중단되지 않은 문서를 공급하고, 디코더로부터의 최종 은닉 상태의 표현이 사용된다. 많은 연구자(Thulke et al., 2021)(Siriwardhana et al., 2023)(Lewis et al., 2019)는 포괄적이고 새로운 사전 훈련 접근법으로 인해 RALM 아키텍처에서 BART를 언어 모델로 채택하여 모델의 견고성을 크게 향상시켰다.\n' +
      '\n' +
      '## 5 RALM Enhancement\n' +
      '\n' +
      '이 섹션에서는 RALM 아키텍처의 연구자들이 구성 요소를 향상시킴으로써 출력 품질을 어떻게 향상시켰는지 설명한다. 개선 방법을 Retriever Enhancement, LM Enhancement, Overall Enhancement의 세 부분으로 나누었다. 그림 4는 향상 방법의 범주화를 보여준다.\n' +
      '\n' +
      '### Retriever Enhancement\n' +
      '\n' +
      '본 절에서는 검색 품질 관리 및 검색 시간 최적화를 포함하는 검색자 측면에서 연구자들의 노력을 제시한다.\n' +
      '\n' +
      '#### 5.1.1 검색 품질 관리\n' +
      '\n' +
      'Shi et al.(2023a)은 검색이 유용한 정보를 제공하지 못할 뿐만 아니라 언어 모델 출력의 품질을 손상시킬 수 있는 문서를 생성할 수 있다고 주장한다. 결과적으로, RALM 분야의 많은 학자들은 최종 출력의 품질을 향상시키기 위해 검색된 콘텐츠와 사용자의 입력 사이의 관련성을 개선하는데 초점을 맞추고 있다.\n' +
      '\n' +
      'Lin et al.(2023b)은 명령어-튜닝을 위한 접근법을 제안한다. 그들은 지도 작업과 비지도 텍스트의 조합을 통해 계산을 완료하는 일반화된 LM 지도 검색(LSR)(Shi et al., 2023b) 훈련 타겟을 사용하여 쿼리 인코더를 업데이트한다. 이를 통해 리트리버는 LLM 선호도와 일치하는 보다 맥락적으로 관련된 결과를 생성할 수 있다. 이 명령어-튜닝 접근법에 영감을 받은 Asai et al.(2023)은 명령어-추적 데이터세트: SELF-RAG에 대해 훈련된 보다 정교한 모델을 제안했다. SELF-RAG는 세분화된 자기 반성을 통해 요구 시 가능한 최상의 모델 출력을 검색하고 선택할 수 있으므로 광범위하게 적용 가능하고 더 강력하며 제어할 수 있다. 자연어 추론(Yoran et al., 2023) 및 요약(Xu et al., 2023) 모델과 같은 외부 모델을 사용하여 검색된 문서의 품질을 향상시키는 접근법과 달리 SELF-RAG는 완전히 새로운 아이디어를 제안한다. 모델은 검색된 문서를 병렬 세그먼트로 분할하고 관련성을 비교한다. 그런 다음 문서의 가장 유사한 부분을 결합합니다. Yan 등(2024)은 부정확한 리트리버 결과를 해결하기 위한 정정 전략을 설계함으로써 SELF-RAG를 개선한다. 그들은 정보를 CORRECT, INCORRECT 및 AMBUOUS의 세 가지 범주로 분류한다. 정보가 CORRECT인 경우, 문서는 정제되고 필터링된다. INCORRECT인 경우, 문서는 폐기되고 웹은 검색을 위해 검색된다. MBIGUOUS라는 용어는 판단의 정확성에 대한 자신감 부족을 나타낸다. 이 경우, 위에서 언급한 두 가지 방법의 조합이 사용될 것이다. 또한 Wang et al.(2023c)은 STRING, 어휘 중복, 조건부 상호 정보(CXMI)의 세 가지 필터를 통해 문장 정밀도로 문서 내용을 검색하는 방법인 FILCO를 제안하였다.\n' +
      '\n' +
      '#### 5.1.2 검색 타이밍 최적화\n' +
      '\n' +
      '연구자들은 일반적으로 긴 대화 생성 및 다중 홉 문제와 같이 다중 검색을 필요로 하는 작업을 수행할 때 또는 적합하고 관련 문서를 찾을 수 없는 두 가지 상황에서 검색 시기를 고려한다. 관련 없는 문서를 사용하면 출력의 정확도가 저하될 수 있습니다.\n' +
      '\n' +
      '검색 시기를 결정하는 간단한 방법은 검색 단계를 조정하는 것이다. Ram et al. (2023)은 런타임 비용을 조정하기 위해 프리픽스 인코딩을 포함하는 접근법을 사용했다. 생성된 콘텐츠의 프리픽스 인코딩은 지속적으로 재계산되었다. 검색 보폭의 선택은 런타임과 성능 사이의 절충이다. 툴포머(Schick et al., 2024)에 따르면, 검색 명령은 모델이 콘텐츠를 생성하는 과정에서 문서 도움말을 검색해야 할 때 유용한 정보를 검색하기 위해 직접 사용될 수 있다. 이 아이디어에 영감을 받은 장 등(2023c)은 검색 시기를 결정하는 두 가지 방법을 제안한다. 첫 번째 방법은 검색이 수행될 필요가 있는 장소와 마주쳤을 때 LM 생성을 중단한 다음 검색 작업을 수행하는 것을 포함한다. 두 번째 방법은 그 전체가 임시 문장을 생성하는 것을 포함한다. 낮음이 있는 경우\n' +
      '\n' +
      '그림 4: RALM 향상 방법의 분류.\n' +
      '\n' +
      '문장의 신뢰 마커, 마커는 마스킹되고 문장의 나머지는 검색에 사용된다. 유 등(2023)은 또한 검색 시기를 결정하기 위해 LM을 사용했다. 그러나 LM을 이용하여 저자신감 마커를 생성하는 대신 검색 전후에 LM 점수를 얻었다. Mallen et al.(2022)의 접근법은 LMs가 저자신감 마커를 생성하도록 하는 전통적인 방법과 다르다. 대신, 그들은 유병률의 척도로 위키피디아 페이지 뷰를 사용하고 다양한 유병률을 가진 위키 데이터의 지식 3배를 원래 개체 및 관계 유형에 고정된 자연어 질문으로 변환했다. 이러한 접근은 보다 객관적이고 주관적인 평가를 회피한다. 추론이 필요한 작업의 경우 He et al.(2022)과 Khattab et al.(2022) 모두 검색 수행 시기를 결정하기 위해 CoT(chain of thought)를 사용하였다.\n' +
      '\n' +
      '### LM Enhancement\n' +
      '\n' +
      '이 절에서는 사전 생성 검색 처리, 구조 모델 최적화 및 사후 생성 출력 향상을 포함한 언어 모델링에 대한 연구자들의 노력을 제시한다.\n' +
      '\n' +
      '#### 5.2.1 Pre-Generation Retrieval Processing\n' +
      '\n' +
      'RALM 아키텍처는 처음에 검색 증강을 위해 단일 문서를 사용했다. 그러나 검색된 단락의 수가 증가하면 RALM의 성능이 크게 향상되는 것으로 나타났다. (Izacard and Grave, 2020) 따라서, 그들은 리트리버를 변경하지 않고 유지하는 Fusion-in-Decoder(FiD)라는 새로운 방법을 제안했는데, 이는 LM에서 인코더를 사용하여 관련 문서를 하나씩 인코딩한 다음 관련 문서를 연결하여 디코더에 제공하여 출력한다. 그 후, Hofstatter et al. (2023)은 FiD 상에서 개선되었다. 그들은 인코더에서 디코더로의 정보 흐름을 제한했다. 재순위가 있는FiD-라이트도 텍스트 소스 포인터를 통해 튜닝하여 최상위 소스 정확도를 향상시켰다. 이자카드와 그레이브(2020)는 리트리버의 합성 표적을 얻기 위해 시퀀스 간 판독기의 교차 주의 점수를 사용하여 FiD-KD라고도 하는 FiD 모델에 지식 증류를 적용했다. Singh et al.(2021)은 FiD-KD에 비해 적은 문서, 훈련 주기 및 감독된 초기화가 필요하지 않은 종단 간 훈련 접근법을 사용한다는 점에서 지식 증류와 다른 개선 접근법을 제안했다.\n' +
      '\n' +
      '#### 5.2.2 구조 모델 최적화\n' +
      '\n' +
      '언어 모델들이 가속화된 속도로 계속 발전함에 따라, 높은 파라미터 카운트 및 탁월한 성능을 갖는 대형 모델들이 증가하고 있다. 이러한 모델의 매개변수 및 내부 구조를 조정하는 것은 점점 더 어렵고 비효율적이 되어 명령어 튜닝을 어느 때보다 중요하게 만든다.\n' +
      '\n' +
      'FLAN(Chung et al., 2022)은 수업 튜닝을 위한 많은 연구 중 가장 체계적이고 포괄적인 접근법 중 하나이다. 이 접근법은 명령어 최적화 데이터 세트에서 언어 모델을 미세 조정하고, 태스크 수와 모델 크기를 확장하며, 미세 조정에 사상 연쇄 데이터를 통합한다. 저자들은 RALM 아키텍처의 조정 지침에 대한 특정 접근법을 고려하지 않았지만, 그들의 작업은 향후 연구에 귀중한 참조를 제공한다. RALM의 instruction fine-tuning에서 Lin et al.(2023)은 in-context retrieval augmentation을 통합하였다. 이는 언어 모델이 관련 없는 검색 콘텐츠에 의해 오도될 가능성을 크게 감소시킨다. SAIL(Luo et al., 2023)은 내부 및 외부 검색 엔진에 의해 생성된 복잡한 검색 결과에 언어 생성 및 명령어 추적 기능을 구축한다. 명령어 튜닝의 코퍼스를 사용하여 서로 다른 검색 API와 도메인에서 각 훈련 사례에 대한 검색 결과를 수집하고, (명령어, 접지 정보, 응답)의 트리플렛을 포함하는 검색 기반 훈련 세트를 구성한다. 명령 조정 데이터 세트에 대한 훈련과 대조적으로 Madaan 등(2022)과 Lazaridou 등(2022)은 검색된 지식에서 직접 대규모 모델을 촉구할 것을 제안한다. Madaan et al.(2022)은 GPT-3를 사용하여 모델이 사용자의 의도를 잘못 해석한 기록 사례의 메모리 페어링과 사용자 피드백을 명확히 했다. 이것은 그들의 시스템이 사용자 피드백에 기초하여 각각의 새로운 질의에 대해 향상된 프롬프트들을 생성할 수 있게 한다. 대조적으로, Lazaridou 등(2022)은 추론 계산을 개선하기 위해 소수의 샷 프롬프트와 답변 재정렬을 사용한다.\n' +
      '\n' +
      '#### 5.2.3 Post-Generation 출력 향상\n' +
      '\n' +
      '병렬 상호작용에 관한 섹션 2에서 정의된 바와 같이, 이러한 상호작용은 K-Nearest Neighbor(KNN) LM(Khandelwal et al., 2019)에 의해 영감을 받는다. LM은 결과를 향상시키기 위해만 사용되는 RALM의 패러다임 사례이다. KNN-LM이 제안된 이후 많은 연구자들이 모델을 최적화하기 위해 노력했다. 이 절에서는 랜드마크 작업에 대해 자세히 설명한다.\n' +
      '\n' +
      'KNN-LM 접근법은 미리 훈련된 LM 임베딩 공간에서 확장된 신경망 언어 모델을 K-Nearest Neighbours로 선형 보간하는 것을 포함한다. Zhong et al.(2022)은 세 가지 유형의 메모리(로컬, 장기 및 외부)에 대해 서로 다른 처리를 제안하고 KNN-LM에 배치 내 토큰에 대한 훈련을 추가했다. 제안된 변화들은 모델의 성능을 향상시키는 것을 목표로 한다. 훈련 시 메모리 단위만을 사용하는 KNN-LM과 달리 TRIME Zhong 등(2022)은 테스트와 훈련 시 모두 메모리 단위를 사용한다. He et al.(2021)은 생성된 토큰이 모두 검색될 필요는 없다고 제안하였다. 대신에, 경량 신경망은 적응 검색에서 KNN-LM을 돕기 위해 훈련될 수 있다. 또한, 데이터베이스 간소화와 차원 축소를 통해 효율성을 향상시킬 수 있다. Alon 등(2022)은 데이터 저장소 위에 구축된 감독되지 않은 가중 유한 오토마톤인 RETOMATON을 제안했다. RETOMATON은 연속적인 데이터 저장소 엔트리와 클러스터링 기법 사이의 포인터를 저장하는 것을 기반으로 한다. RETOMATON은 ADAPTRETHe 등(2021)보다 KNN 검색 시 남은 포인터를 활용하여 정확도를 향상시키는데 더 효과적이다. KNN 검색이 없더라도 언어 모델에만 의존하는 ADAPTRET과 달리 포인터에 저장된 이전 정보를 사용하여 보간 연산을 수행할 수 있다. 또한, RETOMATON은 감독되지 않아 훈련에 추가 데이터가 필요하지 않아 데이터 효율성이 높다. Grave et al.(2016)은 KNN-LM의 성능 향상을 위해 연속 캐시를 사용하여 제안하였다. 이것은 과거의 숨겨진 활성화를 저장하고 현재 숨겨진 활성화와 함께 내적에 의해 적절한 시간에 액세스하는 것을 포함한다. 요가타마 등(2021)은 각 시간 단계에서 가장 가까운 이웃 토큰 집합을 검색하여 로컬 히든 상태 및 글로벌 장기 메모리를 캐싱하여 확장된 단기 컨텍스트를 활용한다. 또한 예측을 위해 여러 정보 소스를 적응적으로 결합하는 게이팅 기능을 설계한다. KNN-LM과 비교하여, 이 방법은 동적 가중치들을 사용하며, 메모리 출력이 이미지, 비디오, 또는 사운드인 경우와 같이 보간이 실현 가능하지 않은 경우들을 처리할 수 있다. Drozdov et al.(2022)은 보간 가중치를 조정하는 방법을 제안하였다. 가중치는 검색된 저장 데이터와 평가 세트 사이의 중첩 영역의 크기에 기초하여 동적으로 조정되며, 이는 검색의 품질을 반영한다.\n' +
      '\n' +
      '### Overall Enhancement\n' +
      '\n' +
      '이 절에서는 종단 간 훈련 및 중간 모듈 빌드 등 RALM 아키텍처 전반에 대한 연구자들의 노력을 제시한다.\n' +
      '\n' +
      '#### 5.3.1 End-to-End Training\n' +
      '\n' +
      '연구자들은 수동 개입을 최소화하고 데이터에만 집중하는 것을 목표로 하는 종단 간 훈련이라는 방법에 대해 연구하기 시작했다. 이 방법은 딥 러닝을 활용하며 사용 가능한 데이터의 양이 증가함에 따라 점점 더 인기를 얻고 있다. RALM 아키텍처에 대한 연구 동안 많은 연구자들은 더 나은 결과를 얻기 위해 종단 간 훈련 방법을 사용하는 경향이 있다.\n' +
      '\n' +
      'Lewis et al.(2020)과 Guu et al.(2020)은 RALM 분야에 종단간 훈련을 적용한 최초의 연구자 중 하나였다. 그러나, 그들은 그들의 접근 방식이 달랐다. REALM Guu et al.(2020)은 사전 트레이닝 단계에서 마스킹된 언어 트레이닝을 사용하였고, 엔드 투 엔드 트레이닝될 수 있는 리트리버를 포함하였다. 미세 조정 단계에서는 리트리버를 냉동 상태로 유지하면서 QA 작업만 대상으로 삼았다. 반면, RAG Lewis et al.(2020)은 이미 훈련된 리트리버인 DPR을 사용하였고, 부분적인 종단간 훈련에는 BART만을 사용하였다. REALM과 유사하게, Sachan 등(2021)은 역 클로즈 태스크 및 마스킹된 두드러진 스팬을 포함하는 비감독 사전 트레이닝 방법을 제시한다. 다음은 질문-컨텍스트 쌍을 사용하여 감독된 미세 조정입니다. 또한, 그들은 엔드 투 엔드 트레이닝된 리트리버들의 사용이 태스크들 전반에 걸쳐 성능에서 상당한 개선을 가져왔다는 것을 발견한다. Singh et al. (2021)은 다중 문서 처리에 종단 간 학습을 적용하며, 제안된 접근법에서는 주어진 질문에 대한 관련 문서 집합을 나타내는 잠재 변수의 값을 반복적으로 추정한다. 그런 다음 이 추정치를 사용하여 검색기 및 판독기의 매개변수를 업데이트합니다. Siriwardhana et al. (2023)은 이전 연구에서 RAG의 종단 간 최적화를 설명하고 더 많은 도메인 특정 지식을 통합하기 위한 보조 훈련 신호를 소개한다. 이 신호는 RAG-end2end가 외부 지식 베이스에서 관련 정보에 액세스함으로써 주어진 문장을 재구성하도록 강제한다. 이 접근법은 도메인 적응성을 크게 향상시켰다.\n' +
      '\n' +
      '#### 5.3.2 Intermediate Modules\n' +
      '\n' +
      '최근 일부 연구자들은 공간 또는 블랙박스 LLM 제약으로 인해 리트리버와 언어 모델의 활동을 조정하기 위해 중간 모듈을 구성했지만 개선되지 않았다.\n' +
      '\n' +
      'Cheng et al.(2024)은 낮은 말뭉치 품질 문제를 해결하기 위해 고안된 모델인 Selfmem을 제시한다. 셀프멤은 검색 강화 생성기를 사용하여 무한한 메모리 풀을 만든 다음 메모리 선택기에 의해 후속 세대에 대한 출력을 선택하는 데 사용됩니다. 이 접근 방식을 사용하면 모델이 자체 출력을 사용하여 생성을 향상시킬 수 있습니다. Peng 등(2023)은 5인칭으로 기술된 마르코프 결정 프로세스(MDP: Markov Decision Process)로서 인간 시스템 대화를 공식화하는 AI 에이전트를 제안한다. 5인조는 무한히 큰 대화 상태 세트, 역사적 행동의 집합, 상태 전이의 확률, 획득된 외부 보상 및 가변 파라미터를 포함한다.\n' +
      '\n' +
      '## 6 Data Source\n' +
      '\n' +
      '이 절에서는 RALM에서 일반적으로 사용되는 데이터 소스 중 일부를 소개하고 정형 데이터와 비정형 데이터로 분류한다. 그림 5는 데이터 원본의 범주화를 보여줍니다.\n' +
      '\n' +
      '### Structured Data\n' +
      '\n' +
      '구조화된 데이터는 표, 지식 그래프 등 다양한 구조를 포함한다. 이러한 유형의 데이터의 이점은 일반적으로 표 형식으로 각 필드가 정확하게 정의된 명확한 구조이다. 숫자, 날짜, 텍스트 및 기타 데이터 유형을 저장하는 데 적합합니다. 구조화된 데이터는 SQL과 같은 구조화된 쿼리 언어를 사용하여 쉽게 쿼리, 분석 및 처리될 수 있다.\n' +
      '\n' +
      'NQ(Natural Questions)(Kwiatkowski et al., 2019)는 NLU 분야에서 매우 잘 알려진 데이터세트이다. 주어진 텍스트는 구조화된 질문 및 대응하는 위키피디아 페이지를 기술한다. 페이지는 긴 답변, 전형적으로 단락, 및 하나 이상의 엔티티들로 구성된 짧은 답변으로 주석이 달린다. 길거나 짧은 답변이 없으면 비어 있는 것으로 표시됩니다. 구글 검색엔진의 신뢰성과 방대한 데이터로 인해 Wang et al.(2023c), Izacard et al.(2022), Ram et al.(2023) 등 많은 학자들이 RALM을 학습하기 위해 이 데이터셋을 사용해 왔다. HotpotQA(HQA)(Yang et al., 2018)는 멀티홉 질문에 대한 정보를 저장하고 추론에 필요한 문장 수준의 지원 사실을 제공한다. 구조는 답변을 지원하는 문단, 질문, 답변, 문장 번호를 포함한다. 이 데이터 세트는 Wang 등(2023c), Khattab 등(2022) 및 Feng 등(2024)과 같은 많은 연구자들에 의해 다중 홉 질의 응답을 위한 RALM을 훈련시키기 위해 사용되었다. 구조화 데이터의 또 다른 중요한 형태는 지식 그래프입니다. 주로 (개체, 관계, 속성)의 트리플로 구성된 데이터 구조입니다. 가장 자주 사용되는 데이터 세트 중 일부는 Wikidata5M, WikiKG90Mv2를 포함하며,\n' +
      '\n' +
      '그림 5: RALM 데이터 소스의 분류.\n' +
      '\n' +
      '- OpendialKG, KOMODIS. 이 모든 모델 Kang et al.(2023)Yu and Yang et al.(2023)He et al.(2024)은 데이터 소스로서 지식 그래프에 의존한다.\n' +
      '\n' +
      '### Unstructured Data\n' +
      '\n' +
      '이와 달리 비정형 데이터는 명확하게 정의된 데이터 구조를 갖지 못하고 텍스트, 이미지, 오디오 등 다양한 형태로 존재한다. 크고 다양한 특성으로 인해 전통적인 표 형식으로 저장 및 관리하기가 어렵다. 가치 있는 정보를 담고 있지만, 이를 파싱하고 이해하기 위해서는 자연어 처리, 이미지 인식 및 기타 기술이 필요하다.\n' +
      '\n' +
      'Khattab et al.(2022), Wang et al.(2023), Yang et al.(2023) 등 여러 RALM 연구자들이 이 데이터셋을 데이터 소스로 사용하였다. FEVER Thorne et al.(2018) 데이터셋은 사실 추출 및 검증에 주로 사용된다. Lewis et al.(2020), Wang et al.(2023) 및 Izacard et al.(2022)을 포함한 여러 RALM 연구자들은 이 데이터 세트에서 사실 텍스트를 데이터 소스로 사용했다. 비구조화된 텍스트 외에도, 이미지, 비디오 및 오디오와 같은 본질적으로 덜 구조화된 데이터의 상당한 양이 또한 존재한다. MNIST, CIFAR-10, 파스칼 VOC 및 COCO를 포함하여 연구에 사용할 수 있는 몇 가지 일반적인 이미지 데이터 세트가 있다. RALM 분야에서 Chen et al. (2022)Hu et al. (2023)Yasunaga et al. (2022)은 이러한 데이터 세트를 활용했다. 음성 연구에 사용되는 일반적인 오디오 데이터 세트로는 LJ Speech, JSUT, RUSLAN 등이 있다. 이 분야의 많은 연구들 Yuan et al. (2024)Huang et al. (2023)Ghosh et al. (2024)도 오디오 데이터를 주요 소스로서 사용한다. 연구에 사용되는 일반적인 비디오 데이터 세트에는 HMDB, UCF101 및 ASLAN이 포함된다. RALM 분야의 많은 연구 Chen et al.(2023)He et al.(2023)Yin et al.(2019)은 오디오 데이터를 정보의 원천으로 활용하고 있다.\n' +
      '\n' +
      '## 7 Applications\n' +
      '\n' +
      '이 섹션에서는 RALM 아키텍처가 주로 중점을 두는 다운스트림 작업에 대한 요약을 제공합니다. 관련 적용 방향은 모델 생성 또는 이해의 요구 사항에 따라 분류된다. NLG에 대한 RALM은 과제의 성취가 주로 생성 능력에 달려 있음을 나타낸다. 반대로 NLU에 대한 RALM은 과제의 성취가 주로 이해 능력에 달려 있음을 나타낸다. 마지막으로, NLU와 NLG 모두에 대한 RALM은 태스크가 일반적으로 이해 능력에 주로 의존하는 것과 생성 능력에 주로 의존하는 두 가지 방식으로 처리된다는 것을 나타낸다. 그림 6은 애플리케이션의 범주화를 보여준다.\n' +
      '\n' +
      '### NLG 태스크의 RALM\n' +
      '\n' +
      '#### 7.1.1 Machine Translation\n' +
      '\n' +
      '자동 번역이라고도 불리는 기계 번역은 컴퓨터를 이용하여 하나의 자연어(소스 언어)를 다른 자연어(타겟 언어)로 변환하는 과정이다. 인공 지능의 궁극적인 목표 중 하나인 계산 언어학의 한 분야이며, 중요한 과학적 연구 가치를 가지고 있다. 기계 번역 시스템은 규칙 기반과 말뭉치 기반 두 가지로 나눌 수 있다. 전자는 사전과 규칙 베이스로 구성되며, 이는 집합적으로 지식 소스를 구성한다. 대조적으로, 후자는 분할되고 라벨링된 코퍼스로 구성되며 사전이나 규칙이 필요하지 않다. 대신 통계법을 기반으로 하며 대부분의 RALM은 규칙을 기반으로 이 작업을 수행한다.\n' +
      '\n' +
      '셀프멤 쳉 등(2024) 시스템은 기계 번역 작업을 위해 두 개의 별개의 언어 모델을 사용한다. 첫 번째는 훈련 가능한 미니 모델이며, 각각 관절 및 이분 접근법을 사용하여 훈련되었다. 두 번째는 LLM을 자극한 몇 번의 샷이다. 궁극적으로, 셀프멤은 네 가지 번역 방향 모두에 걸쳐 그리고 두 훈련 아키텍처 모두에 걸쳐 성능에서 주목할 만한 향상을 보여주었다. 이 결과는 향상된 메모리 기능이 종종 우수한 생성 결과를 초래한다는 것을 암시한다. 최상의 결과를 얻기 위해 TRIME Zhong et al.(2022)은 IWSLT\'14 De-En 기준선을 사용하였다. 과제가 문장 수준임을 감안할 때 연구원들은 반복 토큰이 거의 없기 때문에 로컬 메모리와 장기 메모리를 사용하지 않았다. 대신 외부 메모리만 사용했기 때문에 성능에서 KNN-MT Khandelwal 등(2020)을 이길 수 있었다.\n' +
      '\n' +
      '#### 7.1.2 수학 수업\n' +
      '\n' +
      'RALM 아키텍처가 계속 발전함에 따라, 점점 더 많은 잠재적인 애플리케이션 방향들이 식별되고 있다. Levonian Levonian et al. (2023)은 이 아키텍처를 수학 교수 학습 영역에 적용하기 위해 RALM에서 영감을 받았다. LLM에 저장된 지식이 학교에서 가르치는 것과 일치하지 않을 수 있다는 사실을 해결하기 위해, 그들은 세 가지 프롬프트된 수업 조건 중 하나를 사용하여 기초 수학 교과서에 대한 응답을 생성하고 검색 코퍼스로 평가했다.\n' +
      '\n' +
      '#### 7.1.3 다이얼로그 생성\n' +
      '\n' +
      '대화 생성, 특히 긴 대화는 어려운 과제이다. 이는 언어 모델이 자연 언어 처리 능력을 보유하도록 보장할 필요뿐만 아니라, 모델이 대화의 요구 사항을 만족시키기 위해 컨텍스트를 이용할 수 있다는 필요성 때문이다.\n' +
      '\n' +
      'FILCO(Wang et al., 2023c)는 후속 대화를 생성하기 위해 "Wikipedia의 Wizard"(WoW)로 지칭되는 KILT 벤치마크로부터의 Wikipedia 데이터세트를 채용한다. 이 프로세스는 출력을 위키피디아 기사에 기초하는 것을 포함하며, 입력은 여러 라운드의 대화의 히스토리를 포함한다. RA-DIT(Lin 등, 2023b)는 또한 미세 조정 단계에서 WoW 데이터세트를 채용한다. 커맨드 튜닝 동작의 결과로서, 모델은 제로-샷 조건에서 대화 생성을 위한 동일한 파라미터로 Llama(Touvron et al., 2023a) 및 Llama-REPLUG(Shi et al., 2023b)보다 우수하다. 셀프멤(Cheng et al., 2024)의 검색-증강 생성기에의 통합은 현저한 유연성의 결과로서 대화의 생성을 현저하게 향상시킨다. 이것은 직접 최적화에 의해 달성된다.\n' +
      '\n' +
      '그림 6: RALM 애플리케이션의 분류.\n' +
      '\n' +
      '다양하고 정보가 풍부한 대화의 원하는 속성에 대한 메모리입니다. 이와는 대조적으로, SURGE Kang et al. (2023)은 대화 생성 작업을 위한 데이터 소스로서 Knowledge Graph를 사용하며, 각 대화 라운드는 대규모 KG의 사실들로 구성된다. 다른 관련 작업 Rony 등(2022)과 달리 문맥적으로 관련된 서브그래프만 검색하므로 관련 없는 데이터를 검색함으로써 발생할 수 있는 계산 오버헤드와 오판의 소지가 있는 모델을 피할 수 있다.\n' +
      '\n' +
      '### NLU 태스크의 RALM\n' +
      '\n' +
      '#### 7.2.1 슬롯 채우기\n' +
      '\n' +
      '슬롯 채우기(Slot filling)는 사용자가 제공한 텍스트 또는 음성으로부터 특정 정보를 인식하고 추출하기 위한 목적으로 자연어 처리에 채용되는 기술이다. 슬롯 채움에서, 시스템은 미리 슬롯들의 세트를 정의하며, 각각의 슬롯은 특정 정보 요건을 나타낸다. 이러한 요구 사항은 날짜, 시간, 위치 등을 포함할 수 있지만 이에 제한되지 않는다. 텍스트 또는 스피치의 형태의 사용자 입력을 수신하면, 시스템은 콘텐츠의 분석을 수행하여, 미리 정의된 슬롯들 또는 분류 라벨들 Lu 등과 매칭되는 정보를 식별하려고 시도한다(2023, 2023). 그런 다음 이 정보는 후속 처리 및 응답을 위해 해당 슬롯에 채워진다.\n' +
      '\n' +
      'KGI Glass et al.(2021)은 dense 인덱싱에서 하드 네거티브의 활용을 통해 dense 채널 검색을 강화하고, 검색 향상 생성을 위한 강건한 훈련 과정을 구현한다. 검색 향상은 슬롯 채우기 작업의 효율성을 높이기 위해 사용되어 AI에 의한 고품질 지식 그래프 생성을 촉진한다. 그 결과 TREx 및 zsRE 데이터 세트에서 우수한 성능을 달성했으며 TACRED 데이터 세트에서 현저한 견고성을 보였다.\n' +
      '\n' +
      '#### 7.2.2 이미지 생성\n' +
      '\n' +
      '텍스트-이미지 생성 과정은 텍스트 데이터의 전형적인 포맷과 달리 높은 수준의 자연어 이해를 입증하고 이미지를 통해 이러한 이해를 전달하기 위한 모델이 요구되는 어려운 과정이다.\n' +
      '\n' +
      '선구적인 연구에서 Li et al.(2022)은 텍스트-이미지 생성의 품질을 향상시키기 위해 검색 기술의 사용을 제안했다. 그들은 CUB 및 COCO 데이터 세트에 대해 생성된 이미지의 품질과 양을 주류 모델과 비교 분석했다. 그들의 연구 결과는 모든 모델이 동시대보다 우수하다는 것을 보여주었다. 이와는 대조적으로 RE-IMAGEN Chen et al.(2022)은 검색을 통해 흔하지 않은 객체의 이미지를 생성하는 모델을 보조하는 데 중점을 두었다. 이 접근법은 궁극적으로 COCO 및 위킬 이미지 데이터 세트에서 예외적으로 높은 FID 점수를 달성했다. 여러 범주에서 공통 및 희귀 객체의 범위를 포함하는 저자 자체 제안된 EntityDrawBench 벤치마크에서 훨씬 더 획기적인 결과를 얻었다. RDM Blattmann et al. (2022)은 RE-IMAGEN과 유사한 방식으로 학습되었지만, 이미지 특징을 검색의 기초로 사용하고 추론 과정에서 사용자 예제로 대체한다. 결과적으로, RDM은 기술된 예술 스타일을 생성된 이미지에 효율적으로 전달할 수 있다. 또한, 이미지-텍스트 쌍을 검색에 사용하는 RE-IMAGEN과 달리 KNN-Diffusion Sheynin et al.(2022)은 이미지를 검색에만 사용하므로 COCO 데이터 세트에서 결과의 품질이 더 낮다.\n' +
      '\n' +
      '#### 7.2.3 Fact checking\n' +
      '\n' +
      '사실 확인은 증거에 기초하여 진술을 검증하는 것을 포함한다. 당면한 이 작업은 회수 문제와 도전적인 암시적 추론 작업을 포함한다. 또한, 이 작업은 일반적으로 진술을 입력으로 받아들이고 진술을 증명하거나 반증하는 관련 문서 지문을 생성하는 것을 포함한다. 많은 RALM 모델은 자체 리트리버와 함께 제공되기 때문에 우수한 성능을 얻습니다. 그것은 자연 언어 이해의 중요한 측면이다.\n' +
      '\n' +
      'RAG Lewis 등(2020)은 FEVER 데이터셋을 사용하여 레이블(Supported, Refuted, NotE-noughInfo)을 개별 출력 토큰에 매핑한다. 그것은 다른 작품과 달리 검색된 증거에 대해 감독되지 않는 신고 클래스를 사용하여 직접 훈련된다. Atlas Izacard et al.(2022)은 64-shot 조건에서 기존 연구와 유사한 성능을 얻기 위해 적은 수의shot 학습을 사용한다. 또한, 전체 데이터 세트로 학습한 후, 당시 사용 가능한 최상의 모델인 Hoffmann 등(2022)을 능가했다. FILCO Wang et al. (2023)은 KILT 기반 집합의 FEVER 데이터 세트를 사용하여 검색된 문서의 품질을 향상시키는 작업에 접근했으며, 이는 지지 및 반박 태그만 포함했다. 정확도는 메트릭으로 사용되었다.\n' +
      '\n' +
      '#### 7.2.4 Knowledge Graph 완료\n' +
      '\n' +
      '다수의 이전 작업들은 지식 그래프 형태의 구조화된 데이터를 사용했으며, 지식 그래프 완성은 퍼베이시브 애플리케이션을 나타낸다. 완성을 위한 종래의 방법론(Chen 등, 2022)(Saxena 등, 2022)은 작업을 시퀀스-대-시퀀스 프로세스로서 정의하는 것을 수반하며, 여기서 불완전한 트리플들 및 엔티티들은 텍스트 시퀀스들로 변환된다. 그러나, 이 접근법은 암묵적 추론에 대한 의존성에 의해 제약되며, 이는 지식 그래프 자체의 효용을 상당히 제약한다. ReSKGC (Yu and Yang, 2023)는 지식 그래프 완성과 검색 증강 기술의 통합을 제안한다. 이러한 통합은 지식 그래프에서 의미적으로 관련된 트리플의 선택과 명시적 추론을 통해 출력 생성을 알리기 위한 증거로서의 활용을 수반한다. 이 모델은 Wikidata5M 및 WikiKG90Mv2 데이터 세트의 데이터를 사용하여 다양한 조건에서 기존 다른 작업에 비해 우수한 성능을 보여줍니다.\n' +
      '\n' +
      '#### 7.2.5 Commonsense Reasoning\n' +
      '\n' +
      '상식 추론은 언어 모델에게 어려운 작업이다. 인간과 같은 사고와 추론 패턴을 나타내는 것 외에도 이러한 모델은 상당한 양의 상식적인 지식을 저장할 수 있어야 한다. 그러나 RALM의 등장은 검색 기술이 언어 모델에 비모수 메모리에 대한 액세스를 제공하기 때문에 두 번째 요구 사항을 덜 요구하게 만들었다.\n' +
      '\n' +
      'FLARE(Jiang et al., 2023)는 다양한 출처의 상당한 수의 예/아니오 질문을 포함하는 StrategyQA를 사용한다. 또한, 저자들은 모델이 정확한 추론 과정과 예/아니오 답변을 결정하는 최종 답변을 제공하여 해답이 금답과 정확하게 일치하는지 확인하도록 요청한다. COPA, HellaSwag 및 PIQA 데이터 세트의 데이터를 사용한 훈련과 함께 검색된 콘텐츠에 컨텍스트 샘플을 통합한 결과 우수한 성능을 나타내는 LLM-R(Wang 등, 2023) 모델이 개발되었다. ITER-RETGEN(Gao et al., 2022) 모델의 기본 개념은 검색기와 언어 모델을 통합하기 위해 반복 방법론을 사용한다. 이 모델은 StrategyQA 데이터 세트를 사용하여 상식 추론 작업에 대해 훈련되었으며 7회 반복에서 최적의 성능을 달성했다. 대조적으로, KG-BART(Shao et al., 2023)는 Commonsense Reasoning 태스크의 우선순위를 지정하도록 설계되고, 지식 그래프를 사용하여 이 영역에서 그 성능을 향상시킨다. 이 접근법은 특정 평가 메트릭에서 인간의 성능에 접근하여 상식 추론 작업을 완료하는 모델의 능력을 크게 향상시키는 데 효과적인 것으로 입증되었다.\n' +
      '\n' +
      '### NLU 및 NLG 작업 모두에서 RALM\n' +
      '\n' +
      '#### 7.3.1 Text Summarization\n' +
      '\n' +
      '텍스트 요약은 언어 모델링의 중요한 응용을 나타낸다. 본질적으로 텍스트 요약은 핵심 정보의 내용과 전체적인 의미를 유지하면서 간결하고 유창한 요약을 생성하는 과정이다. 현재 이 작업에는 추출 요약과 추상 요약이라는 두 가지 유형이 있다.\n' +
      '\n' +
      'RA-DIT(Lin et al., 2023)는 모델의 언어 모델 컴포넌트를 정제하기 위해 CNN/DailyMail 데이터세트를 채용하는데, 이는 명령 미세조정의 동작으로 인한 텍스트 요약 태스크에서 현저한 효능을 입증한다. 대조적으로, Self-Mem(Cheng et al., 2024)은 XSum 및 BigPatent 데이터 세트를 사용하여 훈련되었다. 저자들은 기억력 향상이 XSum보다 BigPatent에 훨씬 더 큰 영향을 미친다는 것을 관찰했다. 그들은 이러한 불일치가 상당한 유사성을 나타내는 빅특허 데이터 세트에 공식 특허 문서가 포함되었기 때문이라고 가정한다. LLM-R(Wang et al., 2023) 모델은 인컨텍스트 학습 접근법을 채택하여 RALM을 통합하고, 텍스트 요약 훈련을 위해 AESLC, AG News 및 Gigaword 데이터 세트를 활용한다. 결과는 LLM-R이 요약 작업에서 기존 검색기와 밀집 검색기 모두를 상당히 능가한다는 것을 보여준다. RAMKG (Gao et al., 2022)는 RALM 아키텍처의 반복적인 학습을 다국어 영역으로 확장하고 요약 작업의 훈련을 위해 EcommerceMKP와 AcademicMKP 두 개의 다국어 데이터 세트를 사용하여 그 당시 최상의 결과를 달성했다.\n' +
      '\n' +
      '#### 7.3.2 Question Answering\n' +
      '\n' +
      '질의 응답에는 생성 및 추출 형태도 포함됩니다. 영역별 지식에 의존하는 NLP의 일반적인 작업이다. RALM은 외부에 저장된 지식을 활용하여 전통적인 언어 모델보다 더 나은 결과를 얻을 수 있다. 일반적인 질의 응답 작업에는 도메인별 QA, 개방형 도메인 QA(ODQA) 및 다중 홉 QA가 포함됩니다. 의료 분야에서는 대규모 언어 모델이 일반적으로 사용되며, PKG(Luo et al., 2023)는 MedMC-QA 데이터 세트의 관련 데이터를 사용하여 훈련 세트의 질문을 입력으로 사용하고 의학 설명을 배경 지식으로 사용하며 모델에 의해 생성된 배경 지식의 정확도를 평가 메트릭으로 사용한다. HyKGE(Jiang et al., 2023)는 또한 의학 분야에서 질의 응답을 대상으로 하지만, 지식 그래프 강화 접근법을 사용한다. ODQA 태스크를 타겟팅할 때, RAG(Lewis et al., 2020)는 질문 및 답변을 입력-출력 텍스트 쌍으로 간주하고 답변의 음의 로그-우도를 최소화함으로써 트레이닝한다. 대조적으로, ICRALM(Ram 등, 2023)은 관련 지식 문서뿐만 아니라 사전 훈련, 미세 조정 또는 검색에 의해 향상되지 않은 동결된 LMs를 사용하여 ODQA 작업을 독점적으로 수행한다. 다른 모델들(Wang et al., 2023)(Lin et al., 2023)(Shi et al., 2023)도 ODQA 태스크에 대해 트레이닝되었다. 다중 홉 QA 태스크와 관련하여, FILCO(Wang et al., 2023)는 다수의 문서를 필터링하기 위해 그들의 제안된 필터링된 검색 방법을 사용하였다. 그들은 HotpotQA 데이터 세트를 사용하여 접근 방식을 검증했습니다. 반면에 RR(He et al., 2022)은 다중 홉 문제를 해결하기 위해 CoT(Chain-of-Think) 접근법을 사용하였다. 또한, 많은 다른 모델들(Jiang et al., 2023)(Khattab et al., 2022)은 멀티 홉 문제를 다룬다.\n' +
      '\n' +
      '#### 7.3.3 코드 생성 및 요약\n' +
      '\n' +
      '코드 생성(Romera-Paredes et al., 2024; Ye et al., 2024) 및 요약(Nam et al., 2024)은 타겟 오디언스 및 프로세싱의 관점에서 통상적인 텍스트 생성 및 요약과 상이하다. 코드 생성 및 요약은 언어 모델의 NLU 및 NLG 기능에 대한 더 높은 요구 사항 외에도 도메인별 구문 및 의미론적 이해가 필요할 수 있는 컴퓨터 프로그램 코드를 포함한다.\n' +
      '\n' +
      'REDCODER(Parvez et al., 2021)는 초기에 기존의 코드 또는 추상 데이터베이스로부터 잠재적인 후보 코드를 식별하였다. 연구진은 여러 경로를 통해 검색된 데이터로 110만 개의 고유한 코드와 초록을 보유했다. CodeXGLUE 데이터 세트에 대한 최종 평가는 여러 프로그래밍 언어 및 평가 메트릭에 걸쳐 우수한 성능을 보여주었다. 또 다른 제안된 개선안은 Zan et al.(2022)에 의해 제안되었는데, 그는 코드 생성의 품질을 향상시키기 위해 사설 라이브러리를 활용했다. 여기에는 먼저 APIRetriever 구성 요소를 통해 API 설명서를 기반으로 가장 적합한 개인 라이브러리를 식별한 다음 생성을 위해 APIcoder를 활용하는 것이 포함되었다. 이 접근법은 생성된 콘텐츠의 정확도를 현저하게 향상시켰다.\n' +
      '\n' +
      'Liu et al.(2020)은 정적 그래프 표현을 위한 소스 코드를 보완하고 로컬 및 글로벌 구조 정보를 캡처하기 위한 하이브리드 메시지 전달 GNN을 설계하기 위해 새로운 주의 기반 동적 그래프를 제안한다. 이 접근법은 코드 요약의 정확도를 향상시키고 궁극적으로 주류 검색기 및 생성기 모두에 비해 우수한 성능을 산출한다. RACE (Shi et al., 2022) 모델은 MCMD 데이터 세트 내에서 5개의 프로그래밍 언어로 코드를 통합하기 위해 기존의 RALM 프레임워크를 채택하여 모든 기준 모델에 대해 6~38%의 향상을 가져왔다.\n' +
      '\n' +
      '## 8 Evaluation\n' +
      '\n' +
      '이 섹션에서는 RALM에 대한 평가 접근법과 벤치마크의 요약을 제공한다. 6절과 7절에서는 기준선뿐만 아니라 대형 언어 모델 작업에 대한 몇 가지 평가 기준을 제시하였다. RALM 아키텍처의 초기 제안 당시 대부분의 연구자들은 일반화된 벤치마크를 사용했다. 그러나 RALM 아키텍처가 발전함에 따라 RALM에 특화된 평가 방법과 기준선이 제안되었다. 표 3은 각 평가 모델의 세부 사항을 보여준다.\n' +
      '\n' +
      'RAGAS(Es et al., 2023)는 RALM들의 충실성, 답변 관련성, 및 컨텍스트 관련성을 평가하기 위해 WikiEval Dataset을 채용한다. 충실성은 응답이 제공된 맥락과 일치하는 정도로 정의된다. 답변 관련성은 생성된 응답이 실제 질문을 해결하는 정도를 나타낸다. 컨텍스트 관련성은 검색된 컨텍스트가 중앙 집중화되고 관련 없는 정보가 없는 정도에 의해 측정된다. 또한, 연구자들은 신속한 gpt-3.5-turbo-16k 모델을 사용하여 평가 프로세스를 자동화한다. RGB(Chen et al., 2024)는 정확도, 거부율 및 오류 검출율의 세 가지 평가 메트릭을 사용하는 이중 언어 중국어 및 영어 평가 시스템을 개발했다. 이러한 메트릭은 LM에 의해 처리되고 구글 API를 통해 검색된 기사인 데이터 소스의 노이즈 견고성, 부정적인 거부, 정보 통합 및 반사실적 견고성을 평가하는 데 활용되었다. CRUD-RAG (Lyu et al., 2024)는 검색 컴포넌트의 영향과 이전에 연구자들에 의해 고려되지 않았던 외부 지식 베이스의 구성을 고려한다. ROUGE, BLEU, bertScore 및 RAGQuestEval의 4가지 평가 메트릭을 통해 RALM의 생성, 읽기, 업데이트 및 삭제(요약) 기능을 평가하기 위해 대규모 모델을 사용하여 데이터 세트를 생성했다. 또한, ARES(Saad-Falcon et al., 2023)는 LM에 의해 생성된 데이터 세트를 채용하지만, 개별 RALM 컴포넌트의 품질을 결정하기 위해 경량 LM을 이용하고 예측-동력 추론을 위해 인간-라벨링된 데이터 포인트를 이용한다. RALM의 컨텍스트는 KILT 및 SuperGLUE 벤치마크를 사용하여 평가되며, 관련성, 답변 충실성 및 답변 관련성이 관련 기준이다.\n' +
      '\n' +
      'RALM의 일반적인 평가 외에도 특정 세부 사항 및 영역의 평가에 초점을 맞춘 작업이 있었다. RECALL(Liu et al., 2023)은 부정확한 정보를 기존 데이터 세트에 통합하기 위해 EventKG 및 UJ 데이터 세트를 사용한다. 그런 다음 질문 응답과 텍스트 생성이라는 두 가지 작업을 통해 RALM이 이러한 부정확한 정보에 의해 오도되기 쉬운지 여부를 결정한다. Xiong et al.(2024)은 의료 영역에 초점을 맞추고 MMLU-Med를 포함한 5개의 데이터 세트의 데이터를 통합하여 의료 RALM의 제로샷 학습, 다중 선택 평가, 검색 강화 생성 및 질문 전용 검색 아이디어 기능을 평가하는 MIRAGE를 제안했다. 궁극적으로, 그들은 또한 의료 영역에서 로그 선형 스케일링 특성과 "중간 손실" 효과를 발견했다.\n' +
      '\n' +
      '## 9 Disscussion\n' +
      '\n' +
      '이 섹션에서는 기존 RALM 아키텍처의 한계에 대한 분석과 향후 개발 가능성에 대한 설명에 전념한다. 그림 7은 기존 RALM의 한계와 제안된 솔루션을 요약한 것이다.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      '이 절에서는 기존 RALM의 몇 가지 제한 사항에 대한 요약 및 분석을 제시한다.\n' +
      '\n' +
      '#### 9.1.1 Poor Robustness\n' +
      '\n' +
      '견고성은 모든 시스템에서 고려해야 할 중요한 측면입니다. RALM 시스템은 여러 영역에서 성능상의 이점을 나타내지만, 검색의 통합으로 인해 아키텍처에 많은 불확실성을 도입한다. Hu et al. (2024)에 의해 밝혀진 바와 같이, 매우 단순한 접두사 공격을 통해, RALM 출력의 관련성 및 정확성을 감소시킬 수 있을 뿐만 아니라, 리트리버의 검색 전략조차도 변경될 수 있다. 그 결과, 다양한 것을 활용하는 것 외에\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|c|c|c|c|c|c|} \\hline Reference & \\begin{tabular}{c} RAGAS \\\\ Es et al., 2023) \\\\ \\end{tabular} & \\begin{tabular}{c} RGB \\\\ Chen et al., 2024) \\\\ Chen et al., 2024) \\\\ \\end{tabular} & \\begin{tabular}{c} CRUD-RAG \\\\ Lyu et al., 2024) \\\\ \\end{tabular} & \\begin{tabular}{c} ARES \\\\ Saad-Falcon et al., 2023) \\\\ \\end{tabular} & \\begin{tabular}{c} MIRAGE \\\\ Xiao et al., 2024) \\\\ \\end{tabular} & \\begin{tabular}{c} RECALL \\\\ Liu et al., 2023) \\\\ \\end{tabular} \\\\ \\hline Dataset & WikiEval & \\multicolumn{4}{c|}{LLM-generated} & \\begin{tabular}{c} MMLU-Med \\\\ MedQA-US \\\\ MedMCQA \\\\ PubMedQA \\\\ BioASQ-Y/N \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} EventKG \\\\ UJ \\\\ \\end{tabular} \\\\ \\hline Target & \\multicolumn{4}{c|}{Retrieval Quality: Generation Quality} & Generation Quality \\\\ \\hline Context & \\(\\checkmark\\) & & & \\(\\checkmark\\) & & \\\\ Relevance & \\(\\checkmark\\) & & & \\(\\checkmark\\) & & \\\\ \\hline Faithfulness & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) \\\\ \\hline Answer & \\(\\checkmark\\) & & & \\(\\checkmark\\) & & \\\\ Relevance & \\(\\checkmark\\) & & & \\(\\checkmark\\) & & \\\\ \\hline Noise & & \\(\\checkmark\\) & & & & \\\\ Robustness & & & & & & \\\\ \\hline Information & & \\(\\checkmark\\) & \\(\\checkmark\\) & & \\(\\checkmark\\) & \\\\ Integration & & & & & & \\\\ \\hline Negative & & \\(\\checkmark\\) & & & & \\\\ Rejection & & & & & & \\\\ \\hline Counterfactual & & \\(\\checkmark\\) & \\(\\checkmark\\) & & & \\(\\checkmark\\) \\\\ Robustness & & & & & & \\\\ \\hline Error & & & \\(\\checkmark\\) & & & \\\\ Correction & & & & & \\\\ \\hline Summarization & & & \\(\\checkmark\\) & & & \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: RALM에서의 평가 방법의 요약.\n' +
      '\n' +
      'LMs의 성능을 향상시키기 위한 검색 향상 기술, 연구자들은 또한 사실적으로 부정확한 데이터, 문제 해결과 관련이 없는 정보, 심지어 LMs에 대한 일부 유해한 힌트 및 접두사의 영향을 최소화하도록 주의해야 한다.\n' +
      '\n' +
      '#### 9.1.2 Poor Quality of Retrieval Results\n' +
      '\n' +
      '검색 효율을 향상시키는 노력에 참여한 상당수의 연구자(Yan et al., 2024)(Asai et al., 2023)는 그들의 제안된 모델이 출력의 품질을 최적화하는 데 분명히 유익하지만, 검색 결과가 LM과 완전히 정렬될 수 있다는 보장은 아직 없다고 주장했다. 특히 인터넷을 검색 도구로 사용할 때 인터넷 소스의 품질은 매우 다양할 수 있으며 적절한 고려 없이 이 데이터를 병합하면 노이즈 또는 오판의 소지가 있는 정보를 결과 출력에 도입할 수 있다.\n' +
      '\n' +
      '#### 9.1.3 Overspending\n' +
      '\n' +
      '기존의 RALM(Siriwardhana et al., 2023)(Guu et al., 2020)(Borgeaud et al., 2022)은 다양한 도메인에서 LM의 성능을 크게 향상시킬 수 있지만, 그 중 일부는 복잡한 사전 훈련 및 미세 조정 작업뿐만 아니라 광범위한 모델 변경을 요구하여 시간 및 공간 오버헤드를 크게 증가시키고 또한 RALM의 확장성을 감소시킨다. 또한, 검색의 규모가 증가함에 따라, 데이터 소스를 저장하고 액세스하는 복잡성도 증가한다. 결과적으로 연구자들은 모델을 수정하는 것의 이점을 비용과 비교해야 한다.\n' +
      '\n' +
      '#### 9.1.4 Few Applications\n' +
      '\n' +
      '비록 많은 RALM들이 다양한 도메인들에서 LMs들의 성능을 크게 향상시켰지만, 애플리케이션 관점에서는 그다지 개선되지 않았고, RALM들은 여전히 LMs들의 초기에 행해진 루틴 작업들 중 일부, 예를 들어, 질문 응답, 요약(Luo 등, 2023)(Jiang 등, 2023)(Alon 등, 2022)을 수행하고 있다. 최근에 수학 교수법(Levonian et al., 2023), 슬롯 채우기(Glass et al., 2021) 등과 같은 매우 흥미로운 응용 방향이 있었지만, 이는 충분하지 않다. 기술은 항상 그 가치를 완전히 증명하기 위해 실제로 사용되어야 하며, RALM도 예외는 아니다.\n' +
      '\n' +
      '### Future Prospects\n' +
      '\n' +
      '이 섹션은 주로 섹션 9.1에서 언급한 제한 사항을 기반으로 RALM의 향후 개발을 위한 몇 가지 가능한 방향을 제안한다.\n' +
      '\n' +
      '#### 9.2.1 Improve Robustness\n' +
      '\n' +
      '일부 학자들은 명시적 자기반성 및 세립귀속(Asai et al., 2023)과 같은 논문의 향후 작업 섹션에서 모델 견고성을 향상시키는 가능한 방법을 언급했다. In\n' +
      '\n' +
      '그림 7: 현재 RALM 모델의 한계와 향후 전망의 요약이다.\n' +
      '\n' +
      '이와 대조적으로 Hu et al. (2024)은 RALM을 교란시키는 방법인 Gradient Guided Prompt Perturbation (GGPP)이라는 방법을 제안했는데, 이는 SAT probe (Yuksekgonul et al., 2023) 및 활성화 (ACT) 분류기를 활용하여 상황을 개선하는 데 실험적으로 효과적인 것으로 밝혀졌다. 섭동된 RALM의 내부 상태를 프롬프트하여 이러한 섭동을 검출하는 방법이 제안된다. 또한, RALM의 평가 방법 및 관련 기준선을 제안하고 개선함으로써 모델의 강건성 향상에도 도움을 줄 수 있으며, Chen 등(2024)은 강건성에 초점을 맞추어 RALM에 대한 일련의 평가 시스템을 만들었다.\n' +
      '\n' +
      '#### 9.2.2 Improve Retrieval Quality\n' +
      '\n' +
      '검색의 품질 향상은 검색에 사용되는 데이터세트의 품질 향상과 검색 기법의 성능 향상이라는 두 가지 부분에서 고려될 수 있다. 오늘날, 관련 콘텐츠를 생성하기 위해 LLM에 많은 데이터 세트가 주어지고, LLM 자체가 "환각"을 갖기 때문에, 정제를 감독하기 위해 인간을 이용하는 것과 같은, 데이터의 정확성을 보장하기 위해 특정 수단이 채택되어야 한다(Chen 등, 2024). 또한, 인터넷 상의 광범위한 정보 출처로 인해, 스크리닝을 위해 검색 엔진에만 의존하는 것은 명백히 충분하지 않으므로, 추가 재순위를 위한 BM25(Luo 등, 2023) 또는 TF-IDF(Lazaridou 등, 2022) 알고리즘의 사용과 같은 검색 기술의 개선이 필요하다.\n' +
      '\n' +
      '#### 9.2.3 가중치 비용 및 혜택\n' +
      '\n' +
      '오버헤드를 감소시키는 것은 세 가지 관점에서 고려될 수 있다: 먼저, 일부 플러그-앤-플레이 중간 모듈들이 설계될 수 있고, 예를 들어, CRAG(Yan 등, 2024), Selfmem(Cheng 등, 2024), AI 에이전트(Peng 등, 2023), 또는 일부 배포 솔루션들, 예를 들어, LangChain, Llama Index, 따라서, 각각의 모델에 대한 타겟화된 개선들을 행할 필요가 없다. 둘째, 인터넷 검색을 활용하여 리트리버의 오버헤드를 줄일 수 있지만, 앞서 언급한 데이터 관련성에 주의를 기울일 필요가 있다. 마지막으로, In-컨텍스트 학습은 LMs의 개선과 관련된 오버헤드, 예를 들어 ICRALM(Ram 등, 2023)을 감소시키기 위해 채용될 수 있다.\n' +
      '\n' +
      '#### 9.2.4 Expand Applications\n' +
      '\n' +
      '현대 사회에서 LLM의 적용은 다양한 영역을 포함하도록 확장되었지만 RALM의 적용 방향은 상대적으로 제한적이다. 이러한 한계를 해결하기 위해 연구자들은 LLM의 기존 적용 영역을 고려할 뿐만 아니라 지식 및 경험과 밀접하게 관련된 문제를 해결하는 데 탁월한 RALM의 고유한 장점을 활용해야 한다. 또한 RALM을 다른 첨단 기술과 통합하고 이를 활용하여 관련 문제를 극복해야 한다. 본 논문에서는 의사결정 지원, 검색 엔진, 추천 시스템 등 몇 가지 예시들을 제시한다.\n' +
      '\n' +
      '## 10 Conclusion\n' +
      '\n' +
      'RALM의 통합은 NLP 시스템의 기능에서 상당한 발전을 나타낸다. 이 조사는 RALM에 대한 광범위한 검토를 제공하여 아키텍처, 애플리케이션 및 직면한 문제를 강조했다. RALM은 외부 지식을 검색하고 통합하여 언어 모델을 향상시키며, 번역, 대화 생성 및 지식 그래프 완성을 포함한 다양한 NLP 작업에 걸쳐 향상된 성능을 제공합니다.\n' +
      '\n' +
      '그들의 성공에도 불구하고, RALM은 몇 가지 한계에 직면한다. 특히, 적대적 입력에 대한 견고성, 검색 결과의 품질, 배치와 관련된 계산 비용, 애플리케이션 도메인의 다양성 부족이 추가 주의가 필요한 영역으로 확인되었다. 이를 해결하기 위해 연구 커뮤니티는 평가 방법 개선, 검색 기술 정제, 성능과 효율성 사이의 균형을 유지하는 비용 효율적인 솔루션 탐색과 같은 몇 가지 전략을 제안했다.\n' +
      '\n' +
      '향후 RALM의 고도화는 RALM의 강건성 향상, 검색 품질의 향상, 적용 범위의 확장에 달려 있을 것이다. 보다 정교한 기술을 통합하고 RALM을 다른 AI 기술과 통합함으로써 이러한 모델을 활용하여 훨씬 광범위한 범위의 문제를 해결할 수 있다. 이 분야에서 진행 중인 연구 개발은 보다 탄력적이고 효율적이며 다재다능한 RALM을 초래하여 NLP 및 그 이상에서 달성할 수 있는 것의 경계를 밀어낼 것으로 예상된다. RALM이 계속 발전함에 따라, 그들은 더 깊은 이해와 더 인간다운 언어 능력을 가진 AI 시스템을 가능하게 하여 광범위한 분야에서 새로운 가능성을 열겠다는 약속을 가지고 있다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam 등(2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. _ arXiv preprint arXiv:2303.08774_.\n' +
      '* Adolphs et al.(2021) Leonard Adolphs, Benjamin Boerschinger, Christian Buck, Michelle Chen Huebscher, Massimiliano Ciaramita, Lasse Espeholt, Thomas Hofmann, Yannic Kilcher, Sascha Rothe, Pier Giuseppe Sessa, et al. 2021. Boosting search engines with interactive agent. _ arXiv preprint arXiv:2109.00527_.\n' +
      '* Alon 등 (2022) Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. 2022. Neuro-symbolic language modeling with automaton-augmented retrieval. [Machine Learning에 대한 국제 회의]에서 페이지 468-485. PMLR.\n' +
      '* Asai 등(2023) Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: 자기 성찰을 통해 검색, 생성, 비평하는 학습. _ arXiv preprint arXiv:2310.11511_.\n' +
      '* Aussenac-Gilles and Sorgel (2005) Nathalie Aussenac-Gilles and Dagobert Sorgel. 2005. Text analysis for ontology and terminology engineering. _ Applied Ontology_, 1(1):35-46.\n' +
      '* Ba 등(2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. _ arXiv preprint arXiv:1607.06450_.\n' +
      '* Bai 등(2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. _ arXiv preprint arXiv:2309.16609_.\n' +
      '* Banerjee 등(2019) Imon Banerjee, Yuan Ling, Matthew C Chen, Sadid A Hasan, Curtis P Langlotz, Nathaniel Moradzadeh, Brian Chapman, Timothy Amrhein, David Mong, Daniel L Rubin, et al. 2019. Comparative effectiveness of convolutional neural network (cnn) and recurrent neural network (rnn) architecture for radiology text report classification. _ 인공지능 in medicine_, 97:79-88.\n' +
      '* Black 등(2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: Open-source autoregressive language model. _ arXiv preprint arXiv:2204.06745_.\n' +
      '* Blattmann et al.(2022) Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Muller, and Bjorn Ommer. 2022. Retrieval-augmented diffusion models. _ Advances in Neural Information Processing Systems_, 35:15309-15324.\n' +
      '* Borgeaud 등 (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from trilions of tokens. [기계 학습에 관한 국제 회의]에서 페이지 2206-2240. PMLR.\n' +
      '* Boytsov 등(2016) Leonid Boytsov, David Novak, Yury Malkov, and Eric Nyberg. 2016. Off the beaten path: The term-based retrieval to replace k-nn search. 정보 및 지식 관리에 관한 회의에 관한 제25차 ACM 국제회의의 1099-1108 페이지입니다.\n' +
      '* Brown 등(2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models is few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901.\n' +
      '* Chen et al.(2022a) Chen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam. 2022a. 지식은 평평합니다. 다양한 지식 그래프 완성을 위한 seq2seq 생성 프레임워크입니다. _ arXiv preprint arXiv:2209.07299_.\n' +
      '* Chen et al.(2024) Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. 검색-증강 생성에서의 대형 언어 모델 벤치마킹. 《인공지능에 관한 AAAI 회의록》 제38권 17754-17762쪽이다.\n' +
      '* Chen et al. (2023) Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Hongyang Chao, and Tao Mei. 2023. Retrieval augmented convolutional encoder-decoder networks for video captioning. _ ACM Transactions on Multimedia Computing, Communications and Applications_, 19(1s):1-24.\n' +
      '* Chen et al.(2022b) Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W Cohen. 2022b. 무라그: 이미지 및 텍스트에 대한 개방형 질문 응답을 위한 다중 모드 검색 증강 생성기. _ arXiv preprint arXiv:2210.02928_.\n' +
      '* Chen et al.(2022c) Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. 2022c. Re-imagen: 검색 증강 텍스트-이미지 생성기. _ arXiv preprint arXiv:2209.14491_.\n' +
      '*Cheng et al.(2024) Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2024. 자신을 들어 올립니다. 자체 메모리가 있는 검색 강화 텍스트 생성입니다. _ Neural Information Processing Systems_, 36에서의 진보.\n' +
      '* Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. 희소 변압기를 사용하여 긴 시퀀스를 생성합니다. _ arXiv preprint arXiv:1904.10509_.\n' +
      '* Chowdhery 등(2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. _ Journal of Machine Learning Research_, 24(240):1-113.\n' +
      '* Chung et al.(2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. _ arXiv preprint arXiv:2210.11416_.\n' +
      '* Chen et al.(2020)Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. 게이티드 컨볼루션 네트워크를 갖는 언어 모델링. Machine Learning에 대한 국제 회의 933-941 페이지 PMLR입니다.\n' +
      '* Devlin 등(2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: 언어 이해를 위한 딥 양방향 트랜스포머의 사전 훈련. _ arXiv preprint arXiv:1810.04805_.\n' +
      '* Drozdov et al. (2022) Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, and Mohit Iyyer. 2022년 이웃을 고를 순 없죠? \\(k\\) nn-lm에서 검색에 의존하는 경우 및 방법. _ arXiv preprint arXiv:2210.15859_.\n' +
      '* Dubois 등(2024) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. 2024. Alpacaafarm: 인간의 피드백으로부터 학습하는 방법을 위한 시뮬레이션 프레임워크. _ Neural Information Processing Systems_, 36에서의 진보.\n' +
      '* Es et al. (2023) Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023. Ragas: Automated evaluation of retrieval augmented generation. _ arXiv preprint arXiv:2309.15217_.\n' +
      '* Feng et al. (2023) Chao Feng, Xinyu Zhang, and Zichu Fei. 2023. 지식 풀이기: 지식 그래프에서 도메인 지식을 검색하는 llms를 가르칩니다. _ arXiv preprint arXiv:2309.03118_.\n' +
      '* Feng 등(2024) Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024. 검색-생성 시너지 증강 대형 언어 모델. _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_에서, 페이지 11661-11665. IEEE.\n' +
      '* Gao et al.(2022) Yifan Gao, Qingyu Yin, Zheng Li, Rui Meng, Tong Zhao, Bing Yin, Irwin King, and Michael R Lyu. 2022. Retrieval-augmented multilingual keyphase generation with retriever-generator iterative training. _ arXiv preprint arXiv:2205.10471_.\n' +
      '* Gao et al.(2023) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. _ arXiv preprint arXiv:2312.10997_.\n' +
      '* Ghosh et al.(2024) Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy Evuru, Ramani Duraiswami, and Dinesh Manocha. 2024. 리캡: 검색-증강 오디오 캡션. _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_에서, 페이지 1161-1165. IEEE.\n' +
      '* Glass et al.(2021) Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, and Alfio Gliozzo. 2021. 제로 샷 슬롯 채우기에 대한 강력한 검색 증강 생성. _ arXiv preprint arXiv:2108.13934_.\n' +
      '* Gou et al.(2021) Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. _ International Journal of Computer Vision_, 129(6):1789-1819.\n' +
      '* Grave et al.(2016) Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016. 연속 캐시를 사용하여 신경망 모델을 개선합니다. _ arXiv preprint arXiv:1612.04426_.\n' +
      '* Guu et al.(2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. 증강 언어 모델 사전 학습 검색. [기계 학습에 관한 국제 회의]에서 3929-3938 페이지, PMLR.\n' +
      '* Hambarde and Proenca (2023) Kailash A Hambarde and Hugo Proenca. 2023. 정보 검색: 최근 발전 및 그 이상. _ IEEE Access_.\n' +
      '* Han 등 (2023) Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. 2023. Imagebind-llm: Multi-modality instruction tuning _ arXiv preprint arXiv:2309.03905_.\n' +
      '* He et al.(2022) Hangfeng He, Hongming Zhang, and Dan Roth. 2022. Rethinking with retrieval: Faithful large language model inference. _ arXiv preprint arXiv:2301.00303_.\n' +
      '* He et al.(2021) Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021. 효율적인 최근접 언어 모델. _ arXiv preprint arXiv:2109.04212_.\n' +
      '* He et al.(2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. 이미지 인식을 위한 딥 레지듀얼 학습. IEEE 회의의 컴퓨터 비전 및 패턴 인식에서 770-778 페이지입니다.\n' +
      '* He et al. (2024) Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-retriever: textual graph understanding and question answering를 위한 Retrieval-augmented generation. _ arXiv preprint arXiv:2402.07630_.\n' +
      '* He et al.(2023) Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. 2023. Animate-a-story: storytelling with retrieval-augmented video generation. _ arXiv preprint arXiv:2307.06940_.\n' +
      '* Hoffmann 등 (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_.\n' +
      '*Hofstatter et al.(2023) Sebastian Hofstatter, Jiccao Chen, Karthik Raman, and Hamed Zamani. 2023. Fid-light: 효율적이고 효과적인 검색 증강 텍스트 생성. 제46회 국제 ACM SIGIR 정보 검색 연구 개발 회의 회보에서 1437-1447페이지입니다.\n' +
      '* H. H. et al.(2016)Frederik Hogenboom, Flavius Frasincar, and Uzay Kaymak. 2010. 자연어 말뭉치에서 정보를 추출하는 방법에 대한 개요 _ 정보 수집 Lab_, 69.\n' +
      '* Hu et al.(2024) Zhibo Hu, Chen Wang, Yanfeng Shu, Liming Zhu, et al. 2024. Prompt perturbation in retrieval-augmented generation based large language models. _ arXiv preprint arXiv:2402.07179_.\n' +
      '* Hu et al. (2023) Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A Ross, and Alireza Fathi. 2023. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory. *프로ceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 23369-23379.\n' +
      '* 화앤왕(2018) 신유화앤루왕. 2018. Neural argument generation augmented with externally retrieved evidence. _ arXiv preprint arXiv:1805.10254_.\n' +
      '*황 등(2023) 룽제황, 자웨이황, 동차오양, 이렌, 루핑류, 밍제리, 진후이예, 징린류, 상음, 주자오. 2023. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. [국제 기계 학습 회의]에서 페이지 13916-13932. PMLR.\n' +
      '* Izacard 등 (2021) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. 대조적 학습을 통한 비지도 밀집 정보 검색. _ arXiv preprint arXiv:2112.09118_.\n' +
      '* Izacard and Grave (2020a) Gautier Izacard and Edouard Grave. 2020a. 질문에 응답하기 위해 판독기에서 검색기로 지식을 증류합니다. _ arXiv preprint arXiv:2012.04584_.\n' +
      '* Izacard and Grave (2020b) Gautier Izacard and Edouard Grave. 2020b. 개방형 도메인 질문 응답에 대해 생성 모델을 사용하여 통로 검색을 활용합니다. _ arXiv preprint arXiv:2007.01282_.\n' +
      '* Izacard 등 (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: 검색 증강 언어 모델을 사용한 소수의 샷 학습입니다. _ arXiv preprint arXiv:2208.03299_.\n' +
      '* Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of 환각 in natural language generation. _ ACM Computing Surveys_, 55(12):1-38.\n' +
      '* 장 등(2023a) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023a. 미스트랄 7b. _ arXiv preprint arXiv:2310.06825_.\n' +
      '*장 등(2023b) 신케 장, 루이제 장, 융신 쉬, 리홍 치우, 위에 팡, 지위안 왕, 진이 탕, 홍신 딩, 쉬추, 준펑 자오, 등 2023b. 생각과 검색: 가설 지식 그래프는 의료 대형 언어 모델을 향상시켰습니다. _ arXiv preprint arXiv:2312.15883_.\n' +
      '* Jiang et al. (2020) Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023c. 활성 검색으로 생성이 증가했습니다. _ arXiv preprint arXiv:2305.06983_.\n' +
      '* Jiang et al.(2020) Zi-hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. 2020. Convbert: Span-based dynamic convolution을 이용한 개선된 bert. _ Neural Information Processing Systems_, 33:12837-12848에서의 진보.\n' +
      '* Kang et al.(2023) Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. 2023. 지식 기반 대화 생성을 위한 지식 그래프 증강 언어 모델. _ arXiv preprint arXiv:2305.18846_.\n' +
      '* Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. 오픈 도메인 질의 응답에 대한 밀도 있는 통과 검색입니다. _ arXiv preprint arXiv:2004.04906_.\n' +
      '* Khandelwal 등(2020) Urvash Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. 최근접 이웃 기계 번역입니다. _ arXiv preprint arXiv:2010.00710_.\n' +
      '* Khandelwal 등(2019) Urvash Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. 암기를 통한 일반화: 최근접 이웃 언어 모델. _ arXiv preprint arXiv:1911.00172_.\n' +
      '* Khattab 등(2022) Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. 시연-검색-예측: 지식 집약적 nlp를 위한 검색 및 언어 모델을 구성합니다. _ arXiv preprint arXiv:2212.14024_.\n' +
      '* Komeili 등 (2021) Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2021. 인터넷 증강 대화 생성. _ arXiv preprint arXiv:2107.07566_.\n' +
      '* Kwiatkowski 등(2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. _ 계산 언어학 협회의 트랜잭션_, 7:453-466.\n' +
      '* Lazaridou 등(2022) Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internet-augmented language models through few-shot prompting for open-domain question answering. _ arXiv preprint arXiv:2203.05115_.\n' +
      '* Lee et al.(2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. 약한 감독 개방형 도메인 질문 응답에 대한 잠재 검색입니다. _ arXiv preprint arXiv:1906.00300_.\n' +
      '* Liu et al. (2019)Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel, Mille-Ellen Postle, and Wanli Xing. 2023. Retrieval-augmented generation to improve mau question-answering: Trade-offs between groundedness and human preference. _ arXiv preprint arXiv:2310.03184_.\n' +
      '* Lewis 등(2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: 자연어 생성, 번역 및 이해를 위한 시퀀스-투-시퀀스 사전 트레이닝을 노이즈 제거합니다. _ arXiv preprint arXiv:1910.13461_.\n' +
      '* Lewis 등(2019) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. _ Advances in Neural Information Processing Systems_, 33:9459-9474.\n' +
      '* Li et al.(2022a) Bowen Li, Philip HS Torr, and Thomas Lukasiewicz. 2022a. 메모리 기반 텍스트 대 이미지 생성 _ arXiv preprint arXiv:2208.07022_.\n' +
      '* Li et al.(2022b) 화양리, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022b. 검색 강화 텍스트 생성에 대한 조사입니다. _ arXiv preprint arXiv:2202.01110_.\n' +
      '* Lin et al.(2023a) Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023a. 드래곤을 훈련시키는 방법: 일반화할 수 있는 고밀도 검색을 위한 다양한 확대. _ arXiv preprint arXiv:2302.07452_.\n' +
      '* Lin et al.(2023b) Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvay, Mike Lewis, et al. 2023b. Ra-dit: 검색 강화 이중 명령어 튜닝. _ arXiv preprint arXiv:2310.01352_.\n' +
      '* Lin et al.(2022) Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. 2022. Few-shot learning with multilingual generative language models. 《자연어 처리의 경험적 방법에 관한 2022년 회의 회보》에서 9019-9052쪽이다.\n' +
      '* Liu et al.(2020) Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and Yang Liu. 2020. Hybrid gnn을 통한 코드 요약에 대한 검색 증강 생성. _ arXiv preprint arXiv:2006.05405_.\n' +
      '* Liu et al. (2023) Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. 상기: 외부 반사실적 지식에 대한 l1ms 견고성에 대한 벤치마크. _ arXiv preprint arXiv:2311.08147_.\n' +
      '* Liu 등(2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: Robustly optimized bert pretraining approach. _ arXiv preprint arXiv:1907.11692_.\n' +
      '* Lu et al.(2023a) Yuxing Lu, Xiaohong Liu, Zongxin Du, Yuanxu Gao, and Guangyu Wang. 2023a. Medkpl: 이질적인 지식은 전이 가능한 진단을 위한 신속한 학습 프레임워크를 향상시킵니다. _ Journal of Biomedical Informatics_, 143:104417.\n' +
      '* Lu et al.(2023b) Yuxing Lu, Xukai Zhao, and Jinzhuo Wang. 2023b. 임상 텍스트로부터 진단 분류를 위한 의료 지식 강화 신속한 학습. 제5회 임상 자연어 처리 워크샵의 278-288 페이지입니다.\n' +
      '* Luo et al.(2023a) Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass. 2023a. 세일: 검색 강화 교육 학습입니다. _ arXiv preprint arXiv:2305.15225_.\n' +
      '* Luo et al.(2023b) Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023b. 매개 변수 지식 가이드가 포함된 대형 언어 모델을 확장합니다. _ arXiv preprint arXiv:2305.04757_.\n' +
      '* Lyu et al. (2024) Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. 2024. Crud-rag: 대형 언어 모델의 검색 증강 생성을 위한 포괄적인 중국 벤치마크. _ arXiv preprint arXiv:2401.17043_.\n' +
      '* Madaan 등(2022) Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang. 2022. 배포 후 gpt-3를 개선하기 위한 메모리 지원 프롬프트 편집 _ arXiv preprint arXiv:2201.06009_.\n' +
      '* Mallen et al. (2022) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. 언어 모델을 신뢰하지 않을 때: 매개 변수 및 비매개 변수 메모리의 유효성을 조사합니다. _ arXiv preprint arXiv:2212.10511_.\n' +
      '* Mao et al.(2020) Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020. 오픈 도메인 질문 답변에 대한 생성 강화 검색입니다. _ arXiv preprint arXiv:2009.08553_.\n' +
      '* Mousavi 등 (2024) Seyed M. 무사비, 시몬 알기시 주세페 리카르디 2024년요, 구식인가요? 시간에 민감한 지식에 대한 l1ms 및 정렬 알고리즘을 벤치마킹합니다. _ arXiv preprint arXiv:2404.08700_.\n' +
      '* Nakano 등(2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback, 2021. _URL https://arxiv. org/abs/2112.09332_.\n' +
      '* Nam et al.(2024) Daye Nam, Andrew Macevan, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. llm을 사용하여 코드 이해를 돕는다. _Proceedings of the IEEE/ACM 46th International Conference on Software Engineering_ 1-13 페이지.\n' +
      '* Nemi et al. (2019)Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021. Large Dual Encoder is generalizable Retrievers. _ arXiv preprint arXiv:2112.07899_.\n' +
      '* Parvez et al.(2021) Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. 증강 코드 생성 및 요약 검색 _ arXiv preprint arXiv:2108.11601_.\n' +
      '* Peng 등(2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Fact를 확인하고 다시 시도: 외부 지식과 자동화된 피드백을 가진 대형 언어 모델 개선_ arXiv preprint arXiv:2302.12813_.\n' +
      '* Radford et al.(2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\n' +
      '* Radford et al.(2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비감독 멀티태스킹 학습자이다. _ OpenAI blog_, 1(8):9.\n' +
      '* Raffel 등(2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. 통합 텍스트 대 텍스트 변환기를 사용 하 여 전이 학습의 한계를 탐색 합니다. _ Journal of machine learning research_, 21(140):1-67.\n' +
      '* Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. _ 계산 언어학 협회의 트랜잭션_, 11:1316-1331.\n' +
      '* Ram et al.(2021) Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2021. 감독 없이 지문을 검색하는 학습입니다. _ arXiv preprint arXiv:2112.07708_.\n' +
      '* Ramachandran et al.(2017) Prajit Ramachandran, Barret Zoph, and Quoc V Le. 2017. 활성화 함수를 검색하는 중입니다. _ arXiv preprint arXiv:1710.05941_.\n' +
      '* Ramesh 등(2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. _ arXiv preprint arXiv:2204.06125_, 1(2):3.\n' +
      '* Ramos et al.(2003) Juan Ramos et al. 2003. tf-idf를 사용하여 문서 질의에서 단어 관련성을 결정한다. [기계 학습에 대한 첫 번째 교육 회의의 진행]에서, 242쪽, 29-48쪽, 시티저.\n' +
      '* Robertson 등(1995) Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at trec-3. _Nist Special Publication Sp_, 109:109.\n' +
      '* Romera-Paredes 등(2021) Bernardino Romera-Paredes, Mohammadam Barrekata, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. 2024. Mathematical discoveries from program search with large language models. _ Nature_, 625(7995):468-475.\n' +
      '* Rony 등 (2022) Md Rashad Al Hasan Rony, Ricardo Usbeck, and Jens Lehmann. 2022. Dialokg: Knowledge-structure aware task-oriented dialogue generation. _ arXiv preprint arXiv:2204.09149_.\n' +
      '* Saad-Falcon et al. (2023) Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023. Ares: 검색 증강 생성 시스템을 위한 자동화된 평가 프레임워크. _ arXiv preprint arXiv:2311.09476_.\n' +
      '* Sachan 등(2021) Devendra Singh Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L Hamilton, and Bryan Catanzaro. 2021. End-to-end training of neural retrieve for open-domain question answering. _ arXiv preprint arXiv:2101.00408_.\n' +
      '* Sanh et al.(2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. _ arXiv preprint arXiv:1910.01108_.\n' +
      '* Santhanam 등(2021) Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021. Colbertv2: 경량 후기 상호 작용을 통한 효과적이고 효율적인 검색 _ arXiv preprint arXiv:2112.01488_.\n' +
      '* Saxena et al.(2022) Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. 2022. Sequence-to-Sequence 지식 그래프 완성 및 질의 응답. _ arXiv preprint arXiv:2203.10321_.\n' +
      '* Schick 등(2024) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Railenau, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. 툴포머: 언어 모델은 스스로 도구를 사용하도록 가르칠 수 있습니다. _ Neural Information Processing Systems_, 36에서의 진보.\n' +
      '* Serra et al.(2013) Ivo Serra, Rosario Girardi, and Paulo Novais. 2013. Parnt: 텍스트로부터 온톨로지들의 비-분류학적 관계들을 추출하기 위한 통계 기반 접근법. *2013 10th International Conference on Information Technology: New Generations_, pages 561-566. IEEE.\n' +
      '* Shao et al. (2023) Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. _ arXiv preprint arXiv:2305.15294_.\n' +
      '* Shazeer (2020) Noam Shazeer. 2020. Glu 변형은 변압기를 개선합니다. _ arXiv preprint arXiv:2002.05202_.\n' +
      '* Sheynin et al.(2021) Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and YanivTaigman. 2022. Knn-diffusion: large-scale retrieval을 통한 이미지 생성. _ arXiv preprint arXiv:2204.02849_.\n' +
      '* Shi et al.(2022) Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun. 2022. Race: Retrieval-augmented commit message generation. _ arXiv preprint arXiv:2203.02700_.\n' +
      '* Shi et al.(2023a) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Scharli, and Denny Zhou. 2023a. 큰 언어 모델은 관련 없는 맥락에 의해 쉽게 산만해질 수 있다. [Machine Learning에 대한 국제 회의]에서 31210-31227 페이지. PMLR.\n' +
      '* Shi et al.(2023b) Weijia Shi, Sewon Min, Michihiro Yasunaga, Min준 Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023b. 플러그: 검색 기능이 강화된 블랙박스 언어 모델입니다. _ arXiv preprint arXiv:2301.12652_.\n' +
      '* Singh et al.(2021) Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. 2021. End-to-end training of multi-document reader and retriever for open-domain question answering. _ Advances in Neural Information Processing Systems_, 34:25968-25981.\n' +
      '* Siriwardhana et al. (2023) Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering. _ 계산 언어학 협회의 트랜잭션_, 11:1-17.\n' +
      '* Srivastava 등(2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: 신경망이 과적합되는 것을 방지하는 간단한 방법입니다. _ The journal of machine learning research_, 15(1):1929-1958.\n' +
      '* Su et al.(2024) Jianlin Su, Murtafha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. 로포머: 회전식 위치 임베딩을 갖는 향상된 변압기. _ Neurocomputing_, 568:127063.\n' +
      '* Taylor (1953) Wilson L Taylor. 1953. "cloze procedure": 가독성을 측정하기 위한 새로운 도구 _ Journalism 분기_, 30(4):415-433.\n' +
      '* Thorne et al.(2018) James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: 사실 추출 및 확인을 위한 대규모 데이터 세트입니다. _ arXiv preprint arXiv:1803.05355_.\n' +
      '* Thulke et al.(2021) David Thulke, Nico Dhaheim, Christian Dugast, and Hermann Ney. 2021. 태스크 지향 대화 상자를 위한 비정형 지식에서 증강 생성을 효율적으로 검색합니다. _ arXiv preprint arXiv:2102.04643_.\n' +
      '* Touvron 등(2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothele Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. 라마: 개방적이고 효율적인 기초 언어 모델입니다. _ arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron 등(2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델입니다. _ arXiv preprint arXiv:2307.09288_.\n' +
      '* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. 주의력만 있으면 됩니다. _ 신경 정보 처리 시스템의 진보_, 30.\n' +
      '* Vincent 등(2008) Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoder. [Machine Learning에 관한 제25회 국제 회의의 진행]에서 1096-1103 페이지입니다.\n' +
      '* Wang et al.(2023a) Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan Catanzaro. 2023a. 지침: 검색 후 강화 사전 교육을 조정하는 지침입니다. _ arXiv preprint arXiv:2310.07713_.\n' +
      '* Wang et al.(2023b) Liang Wang, Nan Yang, and Furu Wei. 2023b. 대용량 언어 모델에 대한 컨텍스트 내 예제 검색을 배우는 중입니다. _ arXiv preprint arXiv:2307.07164_.\n' +
      '* Wang et al.(2023c) Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023c. 검색 강화 생성을 위해 컨텍스트를 필터링하는 방법을 학습합니다. _ arXiv preprint arXiv:2311.08377_.\n' +
      '* Workshop 등(2022) BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. _ arXiv preprint arXiv:2211.05100_.\n' +
      '*Xiong et al. (2024) Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. 의학에 대한 검색-증강 생성 벤치마킹. _ arXiv preprint arXiv:2402.13178_.\n' +
      '*Xu et al.(2023) Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Enhancement retrieval-augmented lms with compression and selective augmentation. _ arXiv preprint arXiv:2310.04408_.\n' +
      '* Yan et al.(2024) Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. 정정 검색 증강 생성. _ arXiv preprint arXiv:2401.15884_.\n' +
      '* Yang et al. (2023) Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. 2023. Prca: 플러그식 보상 기반 컨텍스트 어댑터를 통해 검색 질문 응답을 위한 블랙박스 대형 언어 모델에 적합 합니다. _ arXiv preprint arXiv:2310.18347_.\n' +
      '* Yang et al.(2018) Zilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: 다양하고 설명 가능한 다중 홉 질문 응답을 위한 데이터 세트입니다. _ arXiv preprint arXiv:1809.09600_.\n' +
      '* Zhang et al.(2020)* Yao and Guan(2018) Lirong Yao and Yazhuo Guan. 2018. A improved lstm structure for natural language processing. _2018 IEEE 국제 안전 회의 IICSPI(International Conference of Safety Production Information)_에서 페이지 565-569. IEEE.\n' +
      '* 야수나가 외(2022) 미치히로 야수나가, 아르멘 아가잔얀, 웨이자 시, 리치 제임스, 주레 레스코벡, 퍼시 량, 마이크 루이스, 루크 제틀모이어, 및 원-타우 이. 2022. Retrieval-augmented multimodal language modeling. _ arXiv preprint arXiv:2211.12561_.\n' +
      '* Ye et al.(2024) Haoran Ye, Jiarui Wang, Ziguang Cao, and Guojie Song. 2024. Reevo: Large language models as hyper-heuristics with reflective evolution. _ arXiv preprint arXiv:2402.01145_.\n' +
      '* Yin et al. (2019) Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi Wang. 2019. Memory augmented deep recurrent neural network for video question answering. _ IEEE transactions on neural networks and learning systems_, 31(9):3159-3167.\n' +
      '* Yin et al.(2017) Wenpeng Yin, Katharina Kann, Mo Yu, and Hinrich Schutze. 2017. 자연어 처리를 위한 cnn과 rnn의 비교 연구 _ arXiv preprint arXiv:1702.01923_.\n' +
      '* Yogatama et al. (2021) Dani Yogatama, Cyprien de Masson d\'Autume, and Lingpeng Kong. 2021. Adaptive semiparametric language models. _ 계산 언어학 협회의 트랜잭션_, 9:362-373.\n' +
      '* Yoran et al.(2023) Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. 관련 없는 컨텍스트에 강력한 검색 강화 언어 모델을 만듭니다. _ arXiv preprint arXiv:2310.01558_.\n' +
      '* Yu and Yang (2023) Donghan Yu and Yiming Yang. 2023. Retrieval-enhanced generative model for large-scale knowledge graph completion. 제46회 국제 ACM SIGIR 정보 검색 연구 개발 회의 회보에서 2334-2338페이지입니다.\n' +
      '* Yu et al. (2023) Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023. 플러그 앤 플레이 검색 피드백을 통해 언어 모델을 개선합니다. _ arXiv preprint arXiv:2305.14002_.\n' +
      '* Yuan et al.(2024) Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D Plumbley, and Wenwu Wang. 2024. 검색-증강 텍스트-오디오 생성. _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_에서, 페이지 581-585. IEEE.\n' +
      '* Yukeskgonul et al. (2023) Mert Yukeskgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. 2023. 주의 만족: 언어 모델의 사실적 오류에 대한 제약-만족 렌즈. _ arXiv preprint arXiv:2309.15098_.\n' +
      '* Zan et al.(2022) Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2022. 언어 모델이 개인 라이브러리를 충족할 때 _ arXiv preprint arXiv:2210.17236_.\n' +
      '* Zhang and Sennrich (2019) Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. _ Neural Information Processing Systems_, 32에서의 진보.\n' +
      '* Zhang 등(2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _ arXiv preprint arXiv:2205.01068_.\n' +
      '*Zhao et al.(2024a) Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024a. ai 생성 콘텐츠에 대한 검색 강화 생성: 설문 조사 _ arXiv preprint arXiv:2402.19473_.\n' +
      '* Zhao et al.(2023) Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. 2023. Retrieving multimodal information for augmented generation: A survey. _ arXiv preprint arXiv:2303.10868_.\n' +
      '*Zhao et al.(2024b) Weichen Zhao, Yuxing Lu, Ge Jiao, and Yuan Yang. 2024b. 다중 모드 미디어 조작을 위한 집중 추론 및 통합 재구성 _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_에서, 페이지 8190-8194. IEEE.\n' +
      '*Zhao et al.(2024c) Weichen Zhao, Yuxing Lu, Ge Jiao, and Yuan Yang. 2024c. 텍스트 기반 사용자 검색을 위한 이중 색상 세분성 정렬 _ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_에서, 페이지 8075-8079. IEEE.\n' +
      '* Zhong et al.(2022) Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. _ arXiv preprint arXiv:2205.12674_.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
# LQ-LORA: 효율적인 언어 모델 파인튜닝을 위한 저순위 플러스 양자화 행렬 분해

 한궈\({}^{\star}\)

Carnegie Mellon University, \({}^{\ddagger}\)Columbia University

\({}^{\diamond}\)Mohamed bin Zayed University of Artificial Intelligence, Petuum Inc.

매사추세츠공과대학교

hanguo@cs.cmu.edu, pg211@columbia.edu, epxing@cs.cmu.edu, yoonkim@mit.edu

Philip Greengard\({}^{\ddagger}\)

Carnegie Mellon University, \({}^{\ddagger}\)Columbia University

\({}^{\diamond}\)Mohamed bin Zayed University of Artificial Intelligence, Petuum Inc.

매사추세츠공과대학교

hanguo@cs.cmu.edu, pg211@columbia.edu, epxing@cs.cmu.edu, yoonkim@mit.edu

에릭 P. Xing\({}^{\dagger\diamond}\)

Carnegie Mellon University, \({}^{\ddagger}\)Columbia University

\({}^{\diamond}\)Mohamed bin Zayed University of Artificial Intelligence, Petuum Inc.

매사추세츠공과대학교

hanguo@cs.cmu.edu, pg211@columbia.edu, epxing@cs.cmu.edu, yoonkim@mit.edu

###### Abstract

본 논문에서는 사전 학습된 언어 모델의 메모리 효율적인 적응을 위한 간단한 방법을 제안한다. 제안된 방법은 반복 알고리즘을 사용하여 사전 학습된 각 행렬을 고정밀 저순위 성분과 메모리 효율적인 양자화 성분으로 분해한다. 파인튜닝 동안, 양자화된 컴포넌트는 고정된 채로 유지되고 저순위 컴포넌트만이 업데이트된다. 전체 목표 메모리 버짓이 주어진 각 행렬에 대한 양자화 매개 변수(예: 비트 폭, 블록 크기)의 동적 구성을 가능하게 하는 양자화 성분의 정수 선형 프로그래밍 공식을 제시한다. 또한 피셔 정보 행렬의 근사치를 사용하여 행렬 분해 동안 재구성 목표에 가중치를 부여하는 알고리즘의 데이터 인식 버전을 추가로 탐구한다. RoBERTa와 LLaMA-2 (7B와 70B)의 미세조정에 대한 실험은 저순위 플러스 양자화된 행렬 분해 접근법 (LQ-LoRA)이 강한 QLoRA와 GPTQ-LoRA 기준선을 능가하고 단지 작은 성능 저하로 3비트 이하의 비트로 공격적인 양자화를 가능하게 한다는 것을 보여준다. 언어 모델링 교정 데이터 세트에서 미세 조정될 때 LQ-LoRA는 모델 압축에도 사용될 수 있으며, 이 설정에서 우리의 2.75비트 LLaMA-2-70B 모델(낮은 순위 구성 요소를 포함할 때 평균 2.85비트를 가지며 27GB의 GPU 메모리가 필요함)은 16비트 기준선에 비해 상당히 우수하다.

각주 1: 코드 및 모델은 [https://github.com/HanGuo97/lq-lora](https://github.com/HanGuo97/lq-lora)에서 사용할 수 있습니다. 이 작품은 한궈가 MIT 방문 학생일 때 완성됐다.

## 1 Introduction

대규모 언어 모델(LLM) 및 이들의 사전 트레이닝된 파라미터(Zhang 등, 2022; Scao 등, 2022; Touvron 등, 2023; 20)의 증가된 가용성에도 불구하고, 이들의 순전한 사이즈는 이들을 완전한 미세조정을 통해 새로운 데이터세트에 적응시키기 위해 비싸게 만든다. 이는 특히 비이상적인데, 이는 데이터에 후속하는 명령에 대한 소량의 감독된 미세조정이 일반적인 명령을 따를 수 있는 대화형 에이전트를 학습하기 위한 효과적인 접근법인 것으로 나타났기 때문이다(Wang 등, 2023; Taori 등, 2023; Team, 2023; Zhou 등, 2023), 더욱이, 인간 피드백을 갖는 강화 학습을 통해 미세조정된 LLMs는 오늘날 존재하는 가장 능력 있는 AI 시스템 중 일부를 나타낸다(OpenAI, 2023; Bubeck 등, 2023). 따라서 LLM 미세조정의 메모리 효율성을 개선하는 것은 LLM이 실질적으로 적용될 수 있는 문제의 범위를 넓히는 핵심 단계로 남아 있다.

메모리 효율적인 LLM 적응을 위한 한 가지 유망한 프레임워크는 매개변수 효율적인 미세 조정 방법을 통한 것으로, 일반적으로 기본 사전 훈련 모델에 대한 더 작은 미세 조정 가능한 _확장_ 을 학습한다(측량은 딩 등(2023) 참조). 이러한 방법은 사전 훈련된 파라미터가 고정된 상태로 유지됨에 따라 미세 조정에 필요한 메모리의 양을 줄일 수 있으며, 따라서 이러한 파라미터에 대한 구배 및 최적화기 상태를 저장하기 위한 메모리를 할당할 필요성을 줄이는 반면, 최적화될 새로운 파라미터의 수는 고정된 파라미터의 일부이다. 기존의 많은 파라미터 효율적인 미세 조정 방법 중 낮은 순위 적응(LoRA; Hu et al., 2022)이 효율적인 LLM 적응을 위한 인기 있는 기술로 부상했다. LoRA에서는 미리 학습된 모델의 가중치 행렬 \(\mathbf{W}\)을 \(\mathbf{W}+\mathbf{L}_{1}\mathbf{L}_{2}\)로 재매개하고, \(\mathbf{L}_{1}\)와 \(\mathbf{L}_{2}\)만을 미세조정한다. 최근의 연구들은 LoRA의 메모리 효율을 양자화된 사전 훈련 모델에 적용함으로써, 즉 재매개변수화 \(q(\mathbf{W})+\mathbf{L}_{1}\mathbf{L}_{2}\)를 이용하여 더욱 향상시켰다. 여기서 \(q(\cdot)\)는 양자화 함수이다(Dettmers et al., 2023; Chai et al., 2023).

LoRA에서, \(\mathbf{L}_{2}\)는 \(\mathbf{0}\)로 초기화되어 모델 출력이 핀튜닝 초기에 미리 학습된 모델과 동일한지 확인한다(즉, \(\mathbf{X}(\mathbf{W}+\mathbf{L}_{1}\mathbf{L}_{2})=\mathbf{X}\mathbf{W}\). 그러나, 사전 훈련된 행렬들이 상당한 양자화 에러가 있는 범위(서브-4 비트 체제에서 경험적으로 발생하는 것으로 밝혀짐)까지 양자화된 경우, 제로 초기화는 \(q(\mathbf{W})+\mathbf{L}_{1}\mathbf{L}_{2}\neq\mathbf{W}\)이기 때문에 최적이 아닐 수 있다. 본 논문에서는 LoRA가 양자화된 모델에 대한 저순위 업데이트만을 수행한다는 점을 이용하여 양자화 오차를 고려한 초기화 기법을 유도한다. 우리는 강력한 PCA 문헌(Wright et al., 2009; Candes et al., 2011; Zhou and Tao, 2011)에서 사용된 것과 유사한 반복 알고리즘을 사용하여 \(\mathbf{W}\approx\mathbf{Q}+\mathbf{L}_{1}\mathbf{L}_{2}\)을 분해한다. 여기서 \(\mathbf{Q}\)는 고정된 상태로 유지되는 양자화된 성분이고 \(\mathbf{L}_{1}\mathbf{L}_{2}\)는 저순위 성분이다. 적응 동안 \(\mathbf{L}_{1}\) 및 \(\mathbf{L}_{2}\)( \(\mathbf{W}\)의 고분산 부분 공간을 캡처함)만 미세 조정됩니다. 모든 레이어에 동일한 양자화 구성을 적용하는 대신 정수 선형 프로그래밍을 사용하여 전체 목표 비트율이 주어진 각 매트릭스에 다른 구성(비트, 블록 크기 등)을 할당할 수 있는 혼합 양자화 전략을 찾는다. 마지막으로, 우리는 보정 샘플로부터 얻은 피셔 정보 행렬의 근사치로 분해 목적을 수정하는 알고리즘의 데이터 인식 버전을 탐색한다.

LQ-LoRA를 RoBERTa (Liu et al., 2019)와 LLaMA-2 (Touvron et al., 2023b) 모델에 적용하여, 사용자가 목표 메모리 예산을 유연하게 설정할 수 있도록 하면서 강한 QLoRA (Dettmers et al., 2023a)와 GPTQ-LoRA (Frantar et al., 2022; Chai et al., 2023) 기준선에서 의미 있게 개선할 수 있음을 확인하였다. LQ-LoRA는 또한 표준 언어 모델링 데이터 세트에 적용되어 가중치 전용 PTQ(post-training quantization) 방법의 역할을 할 수 있다. 이 환경에서 LLaMA-2-70B를 2.85비트로 압축할 수 있음을 알 수 있었다.

## 2 Background

### 대용량 언어 모델의 저순위 적응

대규모 언어 모델(LoRA; Hu et al., 2022)의 저순위 적응은 LLM 미세 조정 동안 메모리 풋프린트를 줄이기 위한 간단하지만 효과적인 접근법으로 부상했다. 사전 훈련된 선형 층의 행렬 \(\mathbf{W}\in\mathbb{R}^{d\times k}\)이 주어지면 LoRA는 두 행렬 \(\mathbf{L}_{1}\in\mathbb{R}^{d\times r},\mathbf{L}_{2}\in\mathbb{R}^{r\times k}\)을 \(r<\min(d,k)\)로 초기화한다. 여기서 \(\mathbf{L}_{1}\)는 가우시안 노이즈로 초기화되고 \(\mathbf{L}_{2}\)는 \(\mathbf{L}_{1}\mathbf{L}_{2}=\mathbf{0}\)로 초기화된다(훈련 시작 시 \(\mathbf{L}_{1}\mathbf{L}_{2}=\mathbf{0}\). 그런 다음 LoRA는 선형 레이어를 \(\mathbf{X}(\mathbf{W}+\mathbf{L}_{1}\mathbf{L}_{2})\) (여기서 \(\mathbf{X}\)는 이전 레이어의 활성화이며, 언어 모델 적응 동안 \(\mathbf{L}_{1}\) 및 \(\mathbf{L}_{2}\)만 재매개 변수화한다. (바이어스 벡터는 간결함을 위해 생략됨.)

LoRA는 \(\mathbf{W}\)에 대한 기울기 및 관련 최적화 상태(예를 들어 Adam(Kingma and Ba, 2015))에 GPU 메모리를 할당할 필요가 없기 때문에 완전한 미세 조정보다 메모리 효율이 높다. 아마도 사전 학습된 모델(예: Adapters(Houlsby et al., 2019) 및 Prompt Tuning(Li and Liang, 2021; Lester et al., 2021)) 위에서 소수의 매개변수를 학습하는 메모리 효율적인 미세 조정을 위한 다른 전략이 더 많이 사용될 수 있을 것이다. LoRA는 LLM, 특히 명령 후속 벤치마크에서 감독된 미세 조정을 위해 적응하는 데 널리 보급되었다.

### 대용량 언어 모델의 가중치 양자화

Round-to-nearest (RTN) 양자화는 가중치 블록을 스케일링 팩터 \(s=\frac{\max(\mathbf{u}|\mathbf{u})}{2^{b-1}-1}\)와 비트 크기 \(b\)를 이용하여 \(\mathbf{u}\approx s\times\mathrm{clamp}\left(\left\lfloor\frac{1}{2}\mathbf{u}\right\rceil;-2^{b-1},2^{b-1}-1\right)\)로 양자화/역양자화하는 것으로, 사전 훈련된 LLM의 가중치를 8비트로 양자화하는데 효과적인 것으로 나타났다 (Yao et al., 2022). 그러나, (sub) \(4\)-비트 양자화는 RTN에서는 어렵다는 것이 경험적으로 밝혀졌고, 최근의 방법들은 일반적으로 더 나은 가중치 양자화를 얻기 위해 교정 샘플들을 사용하는 데이터 인식 전략을 채용한다(Frantar et al., 2022; Dettmers et al., 2022; Xiao et al., 2022; Kim et al., 2023; Lin et al., 2023; Dettmers et al., 2023; Shao et al., 2023, _inter alia_).

우리의 접근법은 최근에 제안된 NormalFloat(NF) 양자화 스킴(Dettmers et al., 2023a)에 의존하며, 이는 훈련된 모델의 가중치의 분포가 대략 가우시안이라는 사실을 이용한다. Yoshida (2023)의 발표에 이어, NF 양자화는 \([\delta,\frac{1}{2}]\)로부터 \(2^{b-1}\) 균등 간격 값을 계산하고, \([\frac{1}{2},1-\delta]\)로부터 \(2^{b-1}+1\) 균등 간격 값을 계산하며, 여기서 \(\delta=\frac{1}{2}\big{(}\frac{1}{30}+\frac{1}{32}\big{)}\). 결과적으로 \(2^{b}\) 확률 값 \([p_{1},\ldots,p_{2^{b}}]\)이 생성되며, 여기서 \(p_{1}=\delta,p_{2^{b-1}}=\frac{1}{2}\) 및 \(p_{2^{b}}=1-\delta\). 이 확률들은 \([q_{1},\ldots,q_{2^{b}}]\) 분위수로 변환되며, 여기서 \(q_{i}=\Phi^{-1}(p_{i})\)은 \(p_{i}\)에 대한 가우시안 분위수이고, 이들 분위수는 \([-1,1]\)로 정규화된다. \(\tilde{q}_{i}=\frac{q_{i}}{q_{2^{b}}}}\). 그런 다음, 가중치 \(\mathbf{u}=[u_{1},\ldots,u_{B}]\)와 그 블록에 대한 absmax 값 \(s=\max(|\mathbf{u}|)\)이 주어지면, 이 블록 내의 가중치 \(u_{j}\)는 가장 가까운 분위수 \(c_{j}\), 즉 \(c_{j}=\arg\min_{i\in\{1,\ldots,2^{k}\}}\big{|}\tilde{q}_{i}-\frac{u_{i}}{s} \big{|}\로 양자화된다.

\(d\times k\) 행렬의 경우 \(\frac{dk}{B}\) 블록이 있으므로 각 블록에 대한 absmax 값 \(s\)은 작은 블록 크기로 상당할 수 있다. Dettmers et al. (2023)은 주어진 행렬에 대한 absmax 값 \([s_{1},\ldots,s_{\frac{dk}{B}}]\)의 집합을 RTN을 통해 다시 양자화하는 이중 양자화 전략을 사용한다. 이러한 양자화 기법을 기반으로 Dettmers 등(2023)은 사전 훈련된 LLM에 대해 4비트로 NF 양자화를 수행하고, 저순위 업데이트를 학습하는 QLoRA를 제안한다. QLoRA는 여러 벤치마크에 걸쳐 완전한 미세 조정으로 경쟁력이 있는 것으로 밝혀졌으며 따라서 현재 작업의 주요 기준선 역할을 한다.

## 3 Method: LQ-LoRA

이 방법은 사전 학습된 각 행렬을 저순위 행렬과 양자화된 행렬(SS3.1)로 분해하는 간단한 인수분해 기법에 의존하며, 여기서 미세조정 동안 저순위 성분만 적응된다. SS3.2에서는 목표 평균 비트율이 주어진 계층에 걸쳐 동적 양자화를 허용하기 위해 정수 선형 프로그래밍을 통한 혼합 양자화 전략을 탐구한다. 또한 행렬 인수분해(SS3.3) 과정에서 재구성 목적에 가중치를 부여하기 위해 경험적 Fisher 정보 행렬을 사용하여 LQ-LoRA의 데이터 인식 버전을 고려한다.

### 하위 순위 플러스 양자화 매트릭스 분해

SS2.1에서 알 수 있듯이 LoRA는 사전 학습된 행렬을 \(\mathbf{W}\)로 재매개변수화하고, \(\mathbf{L}_{1}\mathbf{L}_{2}\)를 가우시안 행렬에서 \(\mathbf{L}_{1}\)로 초기화하고, 미세 조정 전에 \(\mathbf{L}_{2}\)에서 \(\mathbf{0}\)로 초기화한다. 이는 모델 출력이 미세 조정 시작 시 재매개 변수화 전과 정확히 동일함을 보장하지만, 낮은 비트로 양자화할 때 \(\|\mathbf{W}-\mathrm{Quantize}(\mathbf{W})\|_{F}\gg 0\)를 가질 수 있기 때문에 양자화된 버전의 \(\mathbf{W}\)로 작업할 때 문제가 발생할 수 있다. 이 초기화는 적응할 부분 공간을 결정할 때 \(\mathbf{W}\)'의 구조를 고려하지 않습니다. 이 문제를 행렬 인수분해의 관점에서 접근하여 원래 행렬을 쉽게 양자화할 수 있는 성분과 고분산 방향을 포착하는 저순위 성분으로 인수분해하고,

\[\operatorname*{arg\,min}_{\mathbff{Q},\mathbf{L}_{1},\mathbf{L}_{2}}\|\mathbf{ W}-(\mathbf{Q}+\mathbf{L}_{1}\mathbf{L}_{2})\|_{F},\quad\text{where}\ \mathbf{Q}\in\mathbb{Q}_{b}^{d\times k},\mathbf{L}_{1}\in\mathbb{R}^{d\times r},\mathbf{L}_{2}\in\mathbb{R}^{r\times k}. \tag{1}\]

여기서 \(\mathbb{Q}_{b}^{d\times k}\subset\mathbb{R}^{d\times k}\)는 \(b\)-비트로 무손실 NF 양자화가 가능한 행렬들의 집합이다. 이 최적화 문제는 강력한 주성분 분석(RPCA; Wright et al., 2009; Candes et al., 2011)에서 직면한 문제와 유사하며, 행렬 \(\mathbf{W}\)을 \(\mathbf{L}+\mathbf{S}\)로 분해하는 것을 목표로 하며, 여기서 \(\mathbf{L}\)는 낮은 순위이고 \(\mathbf{S}\)는 _sparse_이다. RCPA(Lin et al., 2010; Zhou and Tao, 2011)에 대해 효과적인 것으로 나타난 반복 알고리즘에 따라, 우리는 대략적으로 Eq. 1 : 최적화 \(\mathbf{L}_{1}\mathbf{L}_{2}\)와 \(\mathbf{Q}\):2

각주 2: 실제로 우리는 완전한 SVD 대신에 무작위화된 SVD를 사용하는데, 이는 성능의 큰 열화 없이 알고리즘의 SVD 부분에 대한 런타임을 상당히 감소시켰다.

\mathrm{W}(\mathrm{L})\leq r}\| \mathrm{W}-(\mathrm{Q}^{(t-1)}+\mathrm{L})\|_{F},\\mathbf{L}_{2}}&\leftarrow\mathrm{SVD}(\mathrm{W}-(\mathrm{Q}^{(t)})\|_{F},\\mathbf{Q}^{(t)}&\leftarrow\mathrm{Quantize}(\mathbf{ W}-\mathbf{L}_{1}^{(t)}\mathbf{L}_{2}^{(t)}),&\approx \operatorname*{arg\,min}_{\mathbf{Q}\in\mathbb{Q}_{b}^{d\times k}}\|\mathrm{W}-(\mathrm{Q}+\mathrm{L}_{1}^{(t)})\|_{F},\end{split} \tag{2}

여기서 \(\mathbf{Q}^{(0)}\)는 \(\mathbf{0}\)로 초기화된다. 이론적인 수렴 보장을 얻을 수 있는 (일부) RPCA 알고리즘(Ma and Aybat, 2018)과 달리 위의 알고리즘은 휴리스틱이다. 따라서 오류 \(\|\mathbf{W}-(\mathbf{Q}^{(t)}+\mathbf{L}_{1}^{(t)}\mathbf{L}_{2}^{(t)})\|_{F}\)를 추적하고 오류가 증가하면 알고리즘을 종료하는 간단한 정지 기준을 사용한다. 반복 분해 알고리즘은 알고리즘 2.3에 나와 있다. 알고리즘의 각 단계(즉, 무작위화된 SVD 후 양자화)는 \(4096\times 4096\) 행렬에 대해 현대 GPU에서 몇 초가 걸린다.

**예비 실험.** 그림 1(왼쪽)에서 우리는 단계 수의 함수로 LLaMA-2-7B의 몇 개 층에 대한 분해 오류 \(\|\mathbf{W}-(\mathbf{Q}+\mathbf{L}_{1}\mathbf{L}_{2})\|_{F}\)를 보여준다. 제안된 알고리즘은 휴리스틱(heuristic)이지만 경험적으로 효과적임을 알 수 있었다. 그림 1(중앙)에서는 모든 행렬에 대해 3비트 NF 양자화에 대한 양자화 오차를 보여주는 반면, 그림 1(오른쪽)에서는 LQ 분해에 대한 해당 오차를 보여준다. 두 접근법 모두 값 및 출력 투영 행렬은 더 깊은 층에서 양자화가 어려워지고 키 및 쿼리 행렬은 더 쉬워지지만, LQ 분해는 모든 층에서 바닐라 양자화에 따라 개선될 수 있음을 발견했다.

### 정수 선형 프로그램을 통한 혼합 구성 양자화

LQ-LoRA는 Dettmers et al.(2023)의 NormalFloat (NF) 양자화 기법을 사용하여 각 시간 단계에서 잔차 \(\mathbf{Q}\)를 양자화한다. NF-양자화는 이중 양자화를 위한 분위 빈 수, 블록 수, 비트 수와 같은 전체 압축률에 영향을 미치는 여러 파라미터를 갖는다. 본 논문에서는 행렬 \(\mathbf{A}\)을 양자화하는 약간 다른 변형으로 다음과 같이 작업한다.

\[\widehat{\mathbf{A}},\mathbf{s}=\mathrm{Quantize-NF}\left(\mathbf{A},b_{0},B_ {0}\right),\quad\widehat{\mathbf{s}},\mathbf{v}=\mathrm{Quantize-INT}\left( \mathbf{s},b_{1},B_{1}\right),\quad\widehat{\mathbf{v}}=\mathrm{cast}\left( \mathbf{v},b_{2}\right).\]

구체적으로 비트 크기 \(b_{0}\)와 버킷 크기 \(B_{0}\)로 NF 양자화를 적용하여 양자화된 행렬 \(\widehat{\mathbf{A}}\)과 각 블록 \(\mathbf{s}=[s_{1},\dots,s_{\frac{d_{k}}{B_{0}}}]\)에 대한 absmax 값을 구한다(SS2.2 참조). 이러한 absmax 값은 버킷 크기 \(B_{1}\)를 갖는 균일한 정수 양자화를 통해 \(b_{1}\) 비트로 더 양자화되어 \(\widehat{\mathbf{s}}\)에 대한 absmax 값과 함께 \(\mathbf{s}\), 즉 \(\mathbf{v}=[v_{1},\dots v_{\frac{d_{k}}{B_{0}}\mathbf{T}_{1}}]\).4 마지막으로 \(\mathbf{v}\)를 \(b_{2}\) 비트로 주조하여 \(\widehat{\mathbf{s}_{1}}}\).5

각주 4: 크기 \(B_{1}\)에 대해 \(v_{1}=\mathrm{absmax}([s_{1},\dots,s_{B_{1}}])\)가 주어지면 \(\hat{s}_{i}=\mathrm{clamp}\left([\frac{s_{i}}{v_{1}}];0,2^{b_{i}-1}\right)\).

각주 5: 이 접근법은 FP8(결과에 영향을 주지 않음)과 대조적으로 \(\mathbf{s}\)에 정수 양자화를 사용하고 \(\mathbf{v}\)를 더 낮은 정밀도(오차가 무시할 정도로 증가함)로 주조한다는 점에서 원래 접근법에서 벗어난다.

이 양자화 기법은 \(\mathbf{A}\)을 표현하기 위해 \(\widehat{\mathbf{A}},\widehat{\mathbf{s}},\widehat{\mathbf{v}}\)를 저장해야 한다. 따라서 우리는 \(c=(b_{0},b_{1},b_{2},B_{0},B_{1})이 주어진 \(\mathbf{A}\)를 저장하기 위한 저장 비용(비트 수)을 정량화할 수 있다. \)

\[\mathrm{storage}(\mathbf{A},c)=\mathrm{sizeof}(\mathbf{A})\cdot\left(b_{0}+ \frac{b_{1}}{B_{0}}+\frac{b_{2}}{B_{0}\cdot B_{1}}}\right). \tag{3}\

원래의 NF-4 이중 양자화는 \(c_{\mathrm{NF4}}=(4,8,\mathsf{fp32},64,256)\)와 \(\mathrm{storage}(\mathbf{A},c_{\mathrm{NF4}})=4.127\cdot\mathrm{sizeof}(\mathbf{A})\인 특수한 경우로, 즉 NF-4는 파라미터당 평균 4.127비트가 필요하다.

**동적 양자화 구성** LLM 양자화에 대한 이전 작업은 일반적으로 사용자의 다양한 리소스 제약에 적응할 수 없고 일부 행렬이 다른 행렬보다 양자화하기 어려울 수 있으므로 각 행렬에 동일한 양자화 전략을 적용하는 데 중점을 두었습니다. 우리는 정수 선형 프로그래밍(Yao et al., 2021; Tang et al., 2022; Kundu et al., 2022)에 기초한 혼합-정밀 양자화 전략을 탐색하며, 이는 사용자-정의된 타겟 타겟 비트 레이트가 주어진 각각의 매트릭스에 상이한 구성들의 할당을 허용한다.

\(c=(b_{0},b_{1},b_{2},B_{0},B_{1})\)를 구성 매개 변수로 하고 \(\mathcal{C}\)를 사용자가 지정할 수 있는 구성 집합으로 합니다 (이 작업에서 고려하는 설정은 표 1 참조).

그림 1: (왼쪽) LLaMA-2-7B의 서로 다른 레이어에 대한 쿼리 투영 행렬에 대한 분해 오류 \(\|\mathbf{W}-(\mathbf{Q}+\mathbf{L}_{1}\mathbf{L}_{2})\|_{F}\)를 LQ 단계 수의 함수로 나타낸다. (Center) 모든 레이어에 대한 NF-3 양자화에 대한 양자화 오차. (오른쪽) 랭크 = 64인 3비트 양자화에 대한 LQ 분해 에러. LQ 분해는 양자화 에러를 적게 초래한다.

[MISSING_PAGE_FAIL:5]

토치, 텐서 이를 통해 행렬 곱셈과 같은 PyTorch 연산에 과부하를 주어 정시간 역양자화를 수행할 수 있다. 그런 다음 PyTorch의 (전체 그래프) 컴파일러를 사용하여 비트 풀 패킹, 역양자화, 기타 선형 대수 연산을 컴파일한다. 배치 크기 \(>1\)의 경우 이 PyTorch 기반 구현(컴파일에 따라 수행됨)은 비트 및 바이트와 같은 일부 사용자 지정 CUDA 구현만큼 빨랐다. 8 추가 세부 정보 및 속도 비교는 부록 A에 나와 있다.

각주 8: [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

### 피셔 가중치 SVD를 통한 데이터 인식 매트릭스 분해

SS3.1에서 고려된 분해 목표는 \(\mathbf{W}\)의 각 항목을 인수분해 동안 재구성에 동등하게 중요한 것으로 처리하는 한 데이터에 영향을 미치지 않는다. LLMs을 정량화하기 위해 교정 데이터를 사용하는 것의 중요성을 입증하는 최근의 연구들(Frantar et al., 2022; Lin et al., 2023; Kim et al., 2023b)에 이어서, 우리는 재구성 목표에 가중치를 부여하기 위해 Fisher 정보 매트릭스의 대각 근사를 사용함으로써 접근법의 데이터 인식 버전을 고려한다. \(\mathbf{W}\)에 대한 경험적 피셔 정보 행렬의 대각선은 \(\mathbf{F}\in\mathbb{R}^{d\times k}\)로 주어지며, 여기서 행렬의 각 엔트리는 \(\mathbf{F}_{ij}=\frac{1}{D}\sum_{d=1}^{D}\left(\frac{\partial}{\partial\mathbf{ W}_{ij}}\log p_{\text{LM}}\left(\mathbf{x}^{(d)}\right)\right)^{2}로 주어진다.\ 직관적으로, 이 메트릭은 모델의 출력이 각 파라미터의 섭동에 얼마나 민감한지를 측정하고, 이전에 사전 트레이닝된 언어 모델의 저순위 압축(Hsu 등, 2022) 및 양자화(Kim 등, 2023b)를 개선하기 위해 이용되었다. 우리는 유사하게 \(\mathbf{F}\)를 사용하여 분해 목적을 가중화하고,

\[\left\|\sqrt{\mathbf{F}}\odot\left(\mathbf{W}-(\mathbf{Q}+\mathbf{L}_{1} \mathbf{L}_{2})\right)\right\|_{F}^{2}, \tag{4}\]

여기서 \(\odot\)는 Hadamard product이다. SS3.1에서 LQ 분해 알고리즘에 적용했을 때, 이것은 다음과 같은 가중된 SVD 문제를 초래하는데, 주어진 \(\mathbf{E}:=\mathbf{W}-\mathbf{Q}\) 및 가중 매트릭스 \(\mathbf{F}\)에서, 우리는 최상의 랭크-\(r\) 근사를 형성하는 매트릭스 \(\mathbf{L}_{1}\in\mathbb{R}^{d\times r},\mathbf{L}_{2}\in\mathbb{R}^{r\times k}\)를 찾아야 하고,

\[\mathbf{L}_{1},\mathbf{L}_{2}=\operatorname*{arg\,min}_{\mathbf{L}_{1}, \mathbf{L}_{2}}\\left\|\sqrt{\mathbf{F}}\odot(\mathbf{E}-\mathbf{L}_{1}\mathbf{L}_{2}) \right\|_{F}^{2}}.\]

가중되지 않은 대응물을 좋아하지 않는 이 문제는 일반적으로 난치성이며(그리고 사실 NP-hard; Razenshteyn et al., 2016) 일반적으로 근사적 방법을 통해 해결된다(Srebro and Jaakkola, 2003; Li et al., 2016; Tuzhilina and Hastie, 2021). 그러나 가중치 행렬 \(\mathbf{F}\)의 행 또는 열이 동일한 값을 갖는다고 가정하면 다음과 같은 동일성을 가지며,

\[\mathbf{L}_{1},\mathbf{L}_{2}=\operatorname*{arg\,min}_{\mathbff{L}_{1}\mathbff{L}_{2}}\left\|\sqrt{\mathbff{F}}\odot(\mathbf{E}-\mathbf{L}_{1}\mathbff{L}_{2}) \right\|_{F}^{2}\ \ \ =\operatorname*{arg\,min}_{\mathbff{L}_{1},\mathbf{L}_{2}}\ \left\| \mathbf{D}_{\text{row}}\left(\mathbf{E}-\mathbf{L}_{1}\mathbf{L}_{2}\right) \mathbf{D}_{\text{col}}\right\|_{F}^{2},\

여기서, \(\mathbf{D}_{\text{row}}\)는 대각 행렬이 \(\sqrt{\mathbf{F}}\)의 행-수단으로 구성되고, \(\mathbf{D}_{\text{col}}\)는 \(\sqrt{\mathbf{F}}\)의 열-수단으로 구성된 대각 행렬, 즉,

\[\mathbf{D}_{\text{row}}}\!\!\!\!\!\!\!\!\! operatorname{diag}\left(\left[\operatorname{avg }(\sqrt{\mathbf{F}_{1,\cdot}}),\ldots,\operatorname{avg}(\sqrt{\mathbf{F}_{d,\cdot}})\right]\right),\mathbf{D}_{\text{col}}}\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!=\!= operatorname{diag} \left(\left[\operatorname{avg}(\sqrt{\mathbf{F}_{\cdot,1}}),\ldots, \operatorname{avg}(\sqrt{\mathbf{F}_{\cdot,k}})\right]\right.\]

이 경우, 상기 문제는 표준 SVD에 의해 정확하게 해결될 수 있고,

\[\mathbf{U},\mathbf{\Sigma},\mathbf{V}^{\top}\leftarrow\mathrm{SVD}(\mathbf{D}_{ \text{row}}\mathbf{A}\mathbf{D}_{\text{col}}),\ \ \mathbf{L}_{1}\leftarrow\mathbf{D}_{\text{row}}^{-1}\mathbf{U}\sqrt{\mathbff{ \Sigma}},\ \ \ \mathbf{L}_{2}\leftarrow\sqrt{\mathbf{\Sigma}}\mathbf{V}^{\top}\mathbf{D}_{ \text{col}}^{-1}. \tag{5}\\

(알고리즘 4 참조.) 동일한 행/열 가정이 \(\mathbf{F}\)에 대해 명백히 성립하지 않지만, 우리는 이 접근법이 실제로 잘 작동한다는 것을 발견했다. 9 이 근사치는 가중 SVD에서 \(\mathbf{D}_{\text{row}}\)을 사용하지만 \(\mathbf{D}_{\text{col}}\)을 사용하지 않는 Hsu 등(2022)의 단순한 확장이라는 점에 주목한다(행 평균과 열 평균을 모두 사용하는 것이 약간 더 잘 수행된다는 것을 발견했다).

각주 9: 예비 실험에서 우리는 피셔 정보 행렬 대신 교정 데이터에서 활성화 \(\mathbf{X}) \(\|\mathbf{X}(\mathbf{W}-(\mathbf{Q}+\mathbf{L}_{1}\mathbf{L}_{2}))\|_{F}\)를 거의 최소화한 데이터 인식 LQ-LoRA 버전을 탐색했다. 그러나 우리는 이것이 피셔 접근법의 성능을 저하시킨다는 것을 발견했다.

논의.이 LQ-LoRA의 데이터 인식 버전은 피셔 행렬 \(\{\mathbf{F}^{(i)}\}_{i\in[N]}\)을 얻기 위해 사전 훈련된 LM을 통해 역 전파할 수 있어야 하며, 이는 어떤 의미에서 완전한 미세 조정이 불가능하다고 간주되는 메모리 효율적인 적응 방법이 목표로 하는 설정에 위배된다. 이것은 유효한 점이며, 따라서 우리는 경험적 연구에서 LQ-LoRA의 두 버전을 모두 연구한다. 그러나 일부 일반 텍스트 데이터를 기반으로 \(\{\mathbf{F}^{(i)}\}_{i\in[N]}\)을 계산하여 LQ-LoRA 초기화 \(\{\mathbf{Q}^{(i)},\mathbf{L}_{1}^{(i)},\mathbf{L}_{2}^{(i)}\}_{i\in[N]}\)을 구하고, 서로 다른 다운스트림 작업에 대해 _same_ 초기화를 사용한다는 점에 유의한다. 이것은 피셔 계산과 행렬 분해가 (비데이터 인식 버전에서와 같이) 한 번만 수행되어야 하기 때문에 데이터 인식 접근법을 실용적으로 만든다.

## 4 Empirical Study

LQ-LoRA는 C4 학습 데이터에 대한 지속적인 언어 모델링, OpenAssistant 데이터셋에 대한 명령어 튜닝(Kopf et al., 2023), GLUE에 대한 finetuning(Wang et al., 2018)의 세 가지 설정에 걸쳐 실험을 수행한다. (1) 및 (2)의 경우 LLaMA-2 모델(Touvron et al., 2023b)을 사용하는 반면, (3)의 경우 RoBERTa-Large 모델(Liu et al., 2019)을 사용한다. 우리의 설정은 Dettmers et al.(2023a)의 설정을 밀접하게 따른다. LQ-LoRA의 피셔 가중치 버전은 C4 훈련 세트에서 무작위로 샘플링된 서열을 사용하며, RoBERTa-Large의 경우 피셔 매트릭스를 얻기 위해 마스킹 언어 모델링 목적(C4에서도)을 사용한다.

기준.우리의 주요 기준으로는 QLoRA(Dettmers et al., 2023a) 및 GPTQ-LoRA가 있다. 두 접근법 모두 적응을 위해 양자화된 모델에 대한 저순위 업데이트를 학습하기 전에 사전 학습된 모델에 대해 PTQ를 수행하고; QLoRA는 NF 양자화를 사용하고, GPTQ-LoRA는 \(\arg\min_{\mathbf{W}\in\mathbb{Q}_{0}^{d\times k}}\|\mathbf{X}\mathbf{W}-\mathbf{X}\\hat{\mathbf{W}}\|_{F}\)(Frantz and Alistarh, 2022; Frantz et al., 2022). 본 논문에서는 LLaMA-1(Touvron et al., 2023a) 위에 원논문을 적용하였으며, 공정비교를 위해 LLaMA-2 위에 이 기준선들을 재구현하였다. 본 연구에서는 Dettmers et al.(2023a)의 주요 실험을 위해 rank = 64를 사용하고, 분석 섹션의 rank를 삭제하였다.

평가.C4에서 훈련된 모델을 평가하기 위해 C4 검증에 대한 복잡성, WikiText-2에 대한 복잡성(Merity et al., 2016), 5-shot MMLU 정확도(Hendrycks et al., 2021)의 세 가지 메트릭을 사용한다. 명령 조정 모델의 경우 10 Vicuna 스타일의 자동 평가(팀, 2023)를 사용합니다. 여기에는 GPT-\(4\)에게 \(80\) 선별된 질문에 대한 GPT-\(3.5\)(동점 가능성 있음)의 출력과 쌍별 비교를 요청하는 것이 포함된다. 우리는 Dettmers et al.(2023a)의 권장 설정에 따라 \(10\)-포인트 등급 체계보다 이 평가 체계를 선택했다. 11 GLUE 벤치마크의 경우 모든 작업에 대한 평균 메트릭을 보여준다.

각주 10: 훈련이 불안정하기 때문에 수업 조정 실험에 GPTQ-LoRA를 포함하지 않았다.

각주 11: 그러나 우리는 관련된 많은 수의 모델 때문에 (가능한 모든 쌍에 걸쳐 평가가 필요한) ELO-스타일 등급 시스템을 사용하지 않는다.

훈련 세부 정보. 달리 명시되지 않는 한 몇 가지 예외를 제외하고 순위 \(64\), LoRA 드롭아웃 없음 및 기본 학습률 \(2\times 10^{-5}\)을 사용한다. 연속적인 언어 모델링을 위해, 우리는 훈련과 평가 모두에 대해 \(1024\)의 시퀀스 길이를 사용하여 절반 에포크 동안 C4 데이터의 한 파티션에서 훈련한다. 피셔를 추정하기 위해 시퀀스 길이가 \(1024\)인 C4의 \(10000\) 샘플을 사용한다. GLUE 작업의 경우 유사한 설정을 사용하지만 C4에서 마스킹된 언어 모델링 목표를 사용한다. 명령어 튜닝을 위해 Dettmers 등(2023a)에서 제안한 하이퍼파라미터(LoRA dropout 제외)를 사용한다. GLUE 미세 조정을 위해 학습률과 에포크 수를 따른다.

그림 2: 순위 = 64인 LQ-LoRA LLaMA-2 모델. C4/Wikipedia/MMLU 결과는 C4에 대한 미세 조정을 기반으로 한다. Vicuna evi는 OpenAssistant 데이터 세트에 대한 미세 조정을 기반으로 한다. QLoRA(Dettmers et al., 2023a) 및 GPTQ-LoRA(Chai et al., 2023)는 자체 재구현을 기반으로 한다. 치밀도는 명령어 튜닝 실험을 제외하고 양자화되지 않은 모델(훈련 없음)을 의미한다. 후자의 경우, 밀집도는 완전 미세 조정(7B 모델만, 70B를 위한 OOM)을 지칭한다.

recommended by Hu et al. (2022) for the QLoRA baseline. 그러나 MNLI 및 QQP의 크기 때문에 \(5\) 에폭에 대한 모델만 미세 조정한다.

### Results

그림 2는 다양한 모델 크기와 메트릭에 걸쳐 LLaMA-2에 대한 언어 모델링 및 명령어 튜닝 결과를 보여준다. 부록 B의 표 6의 전체 숫자 결과는 일반적으로 유사한 비트 예산에서 LQ-LoRA가 거의 항상 QLoRA 및 GPTQ-LoRA보다 우수하다는 것을 발견한다. 예를 들어, 3.5 비트(피셔) LQ-LoRA는 일반적으로 NF-4-비트 QLoRA(4.127 비트/파라미를 필요로 함); 유사하게, 2.75-비트 LQ-LoRA는 NF-3-비트 QLoRA(3.127 비트/파라미를 필요로 함)와 경쟁적이다. 이러한 비교는 이러한 혼합 전략이 ILP 없이 발견되지 않았을 것이기 때문에 혼합 양자화 계획의 유용성을 강조한다. 그러나 \(2.5\)-비트 범위에 접근하면 성능이 크게 저하되기 시작한다는 점에 유의해야 한다. 더 작은 7B 규모에서 LQ-LoRA의 피셔 가중 버전은 모든 목표 비트 폭에서 상당한 마진만큼 비가중 버전을 능가한다. 그러나 이러한 불일치는 70B 척도에서 축소된다.

표 2는 RoBERTa-Large를 사용한 GLUE 벤치마크를 보여주며, 여기에서 유사한 경향을 관찰한다: LQ-LoRA는 유사한 비트 폭에서 QLoRA보다 우수하고 피셔 가중 LQ-LoRA는 2.5비트에서 특히 효과적이다.

### 모델 압축을 위한 LQ-LoRA

결과.표 3은 C4 및 WikiText에 대한 결과를 보여주며, 여기서 우리는 이전의 PTQ 작업(Frantar et al., 2022; Shao et al., 2023)을 따르고 데이터의 특정 서브세트에 대한 C4 및 WikiText-2 perplexity를 통해 성능을 측정한다. 2.75 비트를 갖는 LQ-LoRA는 LoRA 컴포넌트들을 고려할 때, 7B 및 70B 모델들에 대해 각각 2.95 비트 및 2.85 비트의 평균 비트/파라미를 초래한다("표 3의 유효 비트들") 우리는 이것이 일반적으로 사전 훈련된 모델을 양자화하기 위해 교정 데이터를 사용하는 다른 하위 4비트 PTQ 방법보다 우수하다는 것을 발견했다.

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & Bits & GLUE \\ \hline Full FT & 16 & 88.5 \\ QLoRA 3-bit & 3.127 & 86.1 \\ QLoRA 4-bit & 4.127 & 88.8 \\ \hline QLoRA & 2.5 & 75.4 \\ (ILP) & 2.75 & 80.7 \\  & 3.0 & 85.5 \\  & 3.25 & 86.1 \\ \hline LQ-LoRA & 2.5 & 85.7 \\  & 2.75 & 87.1 \\  & 3.0 & 87.3 \\  & 3.25 & 88.1 \\ \hline \hline LQ-LoRA & 2.5 & 87.3 \\ (Fisher) & 2.75 & 86.4 \\  & 3.0 & 87.3 \\  & 3.25 & 88.3 \\ \hline \hline \end{tabular}
\end{table}
표 2: RoBERTa-Large를 갖는 GLUE에 대한 성능.

\begin{table}
\begin{tabular}{l c|c c|c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c|}{**Effective Bits**} & \multicolumn{2}{c|}{**C4**} & \multicolumn{2}{c}{**WikiText**} \\  & (7B, 65B/70B) & 7B & 65B/70B & 7B & 65B/70B \\ \hline _LLaMA-1 Uncompressed\({}^{\dagger}\)_ & _16_ & _7.08_ & _5.62_ & _5.68_ & _3.53_ \\ SpqR (Dettmers et al., 2023b) & 3.94, 3.90 & 7.28 & 5.70 & 5.87 & 3.68 \\ RTN (3-bits, 6128) & 3.15 & 8.62 & 6.10 & 7.01 & 4.24 \\ GPTQ (3-bits, g128) (Frantar et al., 2022)\({}^{\dagger}\) & 3.15 & 7.85 & 6.00 & 6.55 & 4.17 \\ AWQ (3-bits, g128) (Lin et al., 2023)\({}^{\dagger}\) & 3.15 & 7.92 & 5.94 & 6.46 & 3.99 \\ PEQA (3-bits, g128) (Kim et al., 2023a) & 3.15 & - & - & 5.91 & - \\ OWQ (3-bits) (Lee et al., 2023)\({}^{\dagger}\) & 3.1 & 8.15 & 6.16 & 6.39 & 4.08 \\ SeqexL2(3-bits, 4.5x) (Kim et al., 2023b) & 3.24 & 7.56 & - & 6.13 & - \\ SqueezeL1M (3-bits) (Kim et al., 2023b) & 3.02 & 7.75 & - & 6.32 & - \\ OmiQuant (3-bits, g128) (Shao et al., 2023)\({}^{\dagger}\) & 3.15 & 7.75 & 5.93 & 6.15 & 3.94 \\ OmiQuant (2-bits, g48) (Shao et al., 2023)\({}^{\dagger}\) & 2.28 & 11.78 & 7.60 & 8.90 & 5.65 \\ LREC (3-bits, g128) (Chi et al., 2023) & 3.35 & 8.24 & - & 5.52 & - \\ LREC (2-bits, g128) (Chi et al., 2023) & 2.24 & 12.52 & - & 8.74 & - \\ \hline _LLaMA-2 Uncompressed\({}^{\dagger}\)_ & _16_ & _6.97_ & _5.52_ & _5.47_ & _3.31_ \\ RTN (3-bits, g128) & 3.15 & 8.40 & 6.02 & 6.66 & 3.97 \\ GPTQ (3-bits, g128) (Frantar et al., 2022)\({}^{\dagger}\) & 3.15 & 7.89 & **5.85** & 6.29 & 3.85 \\ AWQ (3-bits, g128) (Lin et al., 2023)\({}^{\dagger}\) & 3.15 & 7.84 & - & 6.24 & - \\ OmiQuant (3-bits, g128) (Shao et al., 2023)\({}^{\dagger}\) & 3.15 & 7.75 & **5.85** & 6.03 & 3.78 \\ OmiQuant (2-bits, g64) (Shao et al., 2023)\({}^{\dagger}\) & 2.28 & 12.72 & 7.88 & 9.62 & 6.11 \\ LQ-LoRA (2.75-bits, 64-rank, Fisher) & 2.95, 2.85 & **7.60** & 5.88 & **5.67** & **3.65** \\ \hline \hline \end{tabular}
\end{table}
표 3: 다른 서브-4 비트 PTQ 방법에 대한 LQ-LoRA 비교. 우리는 LLaMA-2(하단)에서 LQ-LoRA만을 실험하지만, 대부분의 이전 작업들이 LLaMA-1에 초점을 맞추었기 때문에 결과를 보정하기 위해 LLaMA-1(상단)에서 다른 PTQ 결과들을 보여준다. "효과적인 비트들"은 양자화 파라미터들(예를 들어, 스케일링 팩터들)을 저장하는 데 필요한 여분의 저장소를 고려한다. LQ-LoRA에서 이것은 LoRA 컴포넌트들을 포함하며, 이 컴포넌트들은 그 자체로 8비트로 양자화된다. 다른 방법의 경우 3비트 양자화와 그룹 크기 128(가능하면 가장 가까운 것)이 있는 설정에 해당하는 결과를 취한다. SpQR과 LQ-LoRA의 유효 비트는 모델 크기에 의존하므로 두 설정 모두에 대한 유효 비트를 보여준다. \ ({}^{\dagger}\)Results from Shao et al. (2023). \ ({}^{\dagger}\) 그룹 크기 128에서 3.01-비트가 아닌 3.1-비트를 보여주는데, 이는 후자가 더 나쁜 성능을 나타내기 때문이다.

LQ-LoRA의 지속적인 언어 모델링에 대한 유망한 결과를 감안할 때, 우리는 다음으로 대규모 언어 모델링이 결과를 더욱 개선하고 모델 압축을 위한 실행 가능한 기술로 LQ-LoRA를 사용할 수 있는지 여부를 실험한다. 특히, LQ-LoRA (Fisher, \(2.75\)-bits, \(64\)-rank)를 사용하여 두 개의 C4 파티션과 WikiText-2의 더 큰 보정 데이터 세트에서 \(2048\)의 시퀀스 길이를 사용하여 미세 조정한다. 우리는 훈련 12 후에 NF-\(8\) 구성을 사용하여 저순위 성분 자체를 추가로 양자화한다.

각주 12: NF-8은 원래 NF-4의 첫 번째 레벨 양자화를 \(8\)-비트로 대체한다.

표 4에서, 우리는 다수의 상이한 평가 과제들에 대한 생성 언어 모델들을 테스트하기 위한 통일된 프레임워크인 Eleuther AI 언어 모델 평가 Harness(Gao et al., 2023)를 사용하여 제로/퓨-샷 능력들을 평가한다. 구체적으로, HuggingFace's the Open LLM Leaderboard13을 따르고 6개의 주요 벤치마크에 대한 모델을 평가한다: ARC(Clark et al., 2018), HellaSwag(Zellers et al., 2019), MMLU(Hendrycks et al., 2020), TruthfulQA(Lin et al., 2022), Winogrande(Sakaguchi et al., 2021), 및 GSM8k(Cobbe et al., 2021). 우리는 일부 벤치마크(GSM8K, ARC)에서 사소한 열화가 있음을 관찰하며, 이는 복잡도 열화가 다운스트림 제로/페우 샷 성능에 항상 상응하는 것은 아님을 나타낸다.

각주 13: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)

### Analysis

혼합 구성 양자화.우리는 주어진 행렬에 대한 매개변수당 평균 비트로 측정된 양자화 구성의 할당을 그림 4에서 보여준다. 각 플롯은 \(2.75\) 목표 비트율에 대한 ILP의 결정을 표시한다. ILP는 서로 다른 구성을 서로 다른 매트릭스에 할당할 수 있으며 이 결정은 실제로 피셔 가중 변형과 비피셔 가중 변형 간에 다르다.

LoRA 랭크.우리는 양자화 구성을 NF-3(즉, 3.127 비트/파람)으로 고정한 표 5의 LLaMA-2-7b 모델로 LoRA 랭크의 영향을 조사한다. QLoRA는 LoRA 순위에 둔감하다. 그러나 LQ-LoRA는 초기화의 오류를 최소화하기 위해 추가 랭크를 "더 나은"으로 사용할 수 있어 성능이 향상된다.

메모리 요구 사항 그림 3에서 우리는 비양자화 성분, 양자화 성분 및 LoRA 매개변수에 의해 분해된 서로 다른 비트율에 대한 모델을 저장하는 데 필요한 메모리를 보여준다. 서브-3-비트로의 양자화는 모델을 실행하는 데 필요한 메모리를 크게 감소시킨다. 서브-3 비트에서, 40GB를 갖는 단일 GPU 상에서 70B 모델을 실행하는 것이 가능해진다. 파인튜닝은 활성화 및 LoRA 구배/최적화 상태에 필요한 메모리로 인해 더 많은 메모리를 필요로 한다. 그러나 배치 크기 2 및 시퀀스 길이 2048의 단일 80GB GPU에서 하위 3비트 70B 모델에서 전체 순방향/후방 패스를 실행할 수 있다.

\begin{table}
\begin{tabular}{l c|c c c c c c|c} \hline \hline Method & Size & ARC & HellaSwag & MMLU & TruthfulQA & Winogrande & GSM8K & **Average** \\ \hline Uncompressed (16 bits) & 7B & 53.2 & 78.6 & 39.0 & 46.6 & 73.6 & 14.9 & 51.0 \\ LQ-LoRA (2.95 bits) & 7B & 49.8 & 75.9 & 39.3 & 43.0 & 72.4 & 7.4 & 48.0 \\ \hline Uncompressed (16 bits) & 70B & 67.2 & 87.3 & 44.8 & 69.6 & 83.7 & 53.7 & 67.7 \\ LQ-LoRA (2.85 bits) & 70B & 65.8 & 86.2 & 44.5 & 66.9 & 83.2 & 45.6 & 65.3 \\ \hline \hline \end{tabular}
\end{table}
표 4: LLaMA-2를 사용한 HuggingFace의 Open LLM 벤치마크에 대한 성능. 표 3과 동일한 LQ-LoRA 설정을 사용한다(2.75-bits, 64-rank, Fisher).

그림 4: ILP에 의해 할당된 비트/파라미터를 행렬 유형별로 분해한 시각화. y축은 비트/파라미터이고, x축은 레이어 번호를 나타낸다. 목표 비트율 2.75비트에 대한 QLoRA, LQ-LoRA 및 Fisher-weighted LQ-LoRA의 할당을 보인다.

그림 3: GB(y축) 대 Storage. 비트(x축), 양자화된 파라미터, LoRA 파라미터 및 기타(예를 들어, 임베딩)에 의해 분해된다. 16비트의 LLAMA-2 7B와 70B는 각각 14GB와 139GB가 필요하다.

## 5. 토론 및 제한 사항

우리의 간단한 반복 알고리즘은 경험적으로 효과적이나 궁극적으로 휴리스틱한 것으로 밝혀졌으며, 더 이론적으로 원칙화된 최적화 알고리즘이 도출될 수 있는지 보는 것은 흥미로울 것이다. 그리고 QLoRA와 비교를 가능하게 하기 위해 NF 양자화에 초점을 맞추었지만, 다른 양자화 접근법에 LQ 분해를 적용하면 추가 이득이 발생할 수 있다. 또한 ILP 기반 혼합 정밀도 접근법을 혼합 정밀도 양자화 _and_ 혼합 순위 분해에 확장하여 각 행렬에 서로 다른 랭크를 할당할 수 있다.14

각주 14: 그러나, 이는 ILP가 최종 다운스트림 성능이 아닌 분해 오차만을 최소화하기 때문에 차선책일 수 있다. 이것은 (미세 조정 가능한 매개변수가 ILP에서 다운스트림 성능에 더 많이 기여하기 때문에) LoRA 매개변수에 비용이 덜 드는 것으로 가중치를 부여하여 해결할 수 있다.

또한 LQ-LoRA의 한계뿐만 아니라 몇 가지 부정적인 결과에 대해서도 논의한다. 우리는 주기적으로 (예를 들어, 모든 \(K\) 구배 단계 후에) 매트릭스를 다시 팩터라이징하는 것이 개선되지 않는다는 것을 발견했다. 본 논문에서 제안한 \(\mathbf{L}_{1}\)와 \(\mathbf{L}_{2}\)의 초기화는 주어진 작업에 최적이 아닐 수 있는 방식으로 모델을 스스로 적응하도록 방향을 설정할 수 있는 반면, 적응 가능한 저순위 컴포넌트의 절반은 LQ-LoRA에서, 나머지 절반은 표준 LoRA 초기화에서 오는 하이브리드 접근 방식을 시도했지만 결과를 개선하지는 못했다. 우리의 접근법은 또한 적응이 낮은 순위 업데이트를 통해 발생할 것이라는 사실에 크게 의존하므로 일반적으로 다른 매개변수 효율적인 미세 조정 방법에는 적용할 수 없다.

## 6 관련 작업

매개 변수 효율적인 미세 조정입니다. 우리의 작업은 매개변수 효율적인 미세 조정과 밀접하게 관련되어 있습니다. 인기 있는 방법에는 훈련 가능한 레이어를 삽입하는 Adapters(Houlsby et al., 2019; Mahabadi et al., 2021), 연속 프롬프트를 최적화하는 프롬프트 튜닝(Li and Liang, 2021; Lester et al., 2021), 파라미터 벡터의 하위 부분을 업데이트하는 다른 방법(Guo et al., 2021; Zaken et al., 2022; Sung et al., 2021; Hu et al., 2022)이 있다. (일부) 파라미터-효율적인 미세조정 방법들은 고정된 파라미터들과 연관된 최적화기 상태들을 저장할 필요가 없기 때문에 미세조정에 필요한 GPU 메모리를 감소시킬 수 있다. 최근의 연구는 파라미터-효율적 미세조정 방법들을 양자화와 결합시켰다(Kwon et al., 2022; Dettmers et al., 2023; Chai et al., 2023).

저순위 플러스 희소/양자화된 행렬 분해.데이터 행렬을 저순위 플러스 희소 행렬(robust PCA라고도 함)로 분해하는 것은 이론적인 관점 및 응용적인 관점 모두에서 잘 연구된다(Lin et al., 2010; Zhou and Tao, 2011, 2013; Liu et al., 2013; Aravkin et al., 2014; Hintermuller and Wu, 2014; Yi et al., 2016; Zhang and Yang, 2017, _inter alia_). 딥러닝 내에서 강력한 PCA는 이전에 100M 미만의 파라미터를 갖는 더 작은 모델을 압축하는 데 적용되었다(Chen and Ranftl, 2018; Cai et al., 2021). citesaha2023matrix는 사전 훈련된 행렬의 양자화된 저순위 근사치를 얻기 위해 스케치 기법을 사용한다. 최근의 동시대 작업(Li 등, 2023)은 또한 LLM 적응을 위해 낮은 순위 플러스 양자화된 분해를 수행한다.

LLM 압축.1B 미만의 파라미터를 갖는 더 작은 LLMs의 저순위 압축에 대한 많은 연구가 있었지만(Chen et al., 2021; Tukan et al., 2021; Tahaei et al., 2021), 1B+ LLMs에 대한 저순위 접근법들은 미응답 상태로 남아 있는데, 이는 아마도 LLMs의 사전 트레이닝된 행렬들의 특이값들이 천천히 감쇠하는 것으로 발견되었기 때문일 수 있다(Chen et al., 2021). 따라서 LLM 압축을 위한 기존의 접근법들은 일반적으로 양자화에 초점을 맞추었다. 많은 최근의 작업이 데이터 인식 양자화 전략에 초점을 맞추고 있다(Dettmers et al., 2022; Xiao et al., 2022; Dettmers et al., 2023; Frantar et al., 2022; Kim et al., 2023; Lin et al., 2023).

\begin{table}
\begin{tabular}{l l|l|l|l} \hline \hline
**Method** & **LoRA rank** & **C4** & **WikiText** & **Error** \\ \hline QLoRA 3-bit & 32 & 8.21 & 6.75 & \\ (3.127 bits/param) & 64 & 8.21 & 6.76 & \(9.83\times 10^{4}\) \\  & 128 & 8.21 & 6.76 & \\ \hline LQ-LoRA 3-bit & 32 & 8.02 & 6.61 & \(7.99\times 10^{4}\) \\ (3.127 bits/param) & 64 & 7.93 & 6.51 & \(7.12\times 10^{4}\) \\  & 128 & 7.84 & 6.46 & \(5.98\times 10^{4}\) \\ \hline \hline \end{tabular}
\end{table}
표 5: LoRA 순위의 함수로서 C4 및 WikiText 복잡성. QLoRA와 LQ-LoRA 모두에 대해 우리는 모든 레이어에 고정된 NF-3 구성을 사용했으며, 이는 평균 3.127비트/파라미터의 비용을 발생시킨다. 가장 오른쪽 열은 QLoRA의 경우 \(\|\mathbf{W}-\text{Quantize}(\mathbf{W})\|_{F}^{2}\)와 LQ-LoRA의 경우 \(\|\mathbf{W}-(\mathbf{Q}+\mathbf{L}_{1}\mathbf{L}_{2})\|_{F}^{2}\)에 해당하는 LLaMA-2-7b의 모든 행렬에 대한 오차의 합을 보여준다.

## 7 Conclusion

본 논문에서는 미리 훈련된 행렬들을 낮은 계층과 양자화된 성분으로 분해하는 LoRA의 간단한 확장을 제안한다. 여기서 양자화 성분은 동적 구성 전략을 사용할 수 있다. 우리는 강한 기준선에 비해 의미 있는 개선을 산출하기 위해 이 낮은 순위 플러스 양자화된 분해 접근법을 관찰했다.

## Acknowledgements

허민영, 왕홍이, 이샤푸리, 현대42닷 연구팀원들이 도움이 되는 댓글과 토론에 감사드린다. 우리는 또한 옴니퀀트에 관한 명확한 질문들에 대해 멍자오 첸에게 감사한다. 에릭 싱과 한궈는 마이크로소프트 박사 펠로우십, NGA HM04762010002, NSF IIS1955532, NSF CNS2008248, NIGMS R01GM140467, NSF IIS2123952, NSF BCS2040381, 아마존 연구상, NSF IIS2311990, DARPA ECOLE HR00112390063의 지원을 인정한다. 이 연구는 현대 자동차 그룹, MIT-IBM 왓슨 AI, MLA@CSAIL 이니셔티브의 자금으로 지원되었다.

## References

* Aravkin et al.(2014) Aravkin, S. 베커 세버와 P. 올슨 안정적인 주성분 추구에 대한 변분 접근법. Uncertainty on Artificial Intelligence (UAI)에서, SS1에 의해 인용되었다.
*Bubeck et al. (2023)Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of Artificial General Intelligence: Early experiments with GPT-4. arXiv:2303.12712. 인용: SS1.
* Cai et al.(2021)HanQin Cai, Jialin Liu, and Wotao Yin. 강인한 PCA 학습: 고차원 이상치 탐지를 위한 확장 가능한 딥 언폴딩 접근법. arXiv:2110.05649. 인용부호:SS1.
* Emmanuel Candes et al. (2011)Externalized gradient descent. 외부 링크: 1106.03715 인용: SS1.
* Yuji Chai et al. (2023)Joint discountours, Glenn G Ko, David Brooks, and Gu-Yeon Wei. Int2. 1: 하위 순위 적응을 통한 오류 정정이 있는 미세 조정 가능한 양자화된 대형 언어 모델을 지향한다. arXiv preprint arXiv:2306.08162. 인용: SS1.
* J. Chen and R. Ranftl(2018)Deep robust pca using convolutional autoencoder. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2836-2840. Cited by: SS1.
* P. Chen 등 (2021)DRONE: data-aware Low-rank Compression for Large NLP Models. In Advances in Neural Information Processing Systems, J. M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan(Eds.), pp. 29321-29334. Cited by: SS1.
* P. Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문을 풀었다고 생각해? ai2 추론 챌린지를 시도합니다. arXiv preprint arXiv:1803.05457. 인용: SS1.
* K. 코베 고사라주 바이에른 천현준 카이저 플래퍼트, J. 투렉, J. 힐튼, R. Nakano, et al.(2021)Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. 인용: SS1.
*T. M. Dettmers 루이스 벨카다, L. Zettlemoyer (2022)LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. arXiv:2208.07339. 인용부호:SS1.
*T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer (2023)Qlora: 양자화된 llms의 효율적인 미세조정. arXiv preprint arXiv:2305.14314. 인용: SS1.

* Dettmers 등(2023b) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantz, Saleh Ashkbos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: 거의 손실 없는 llm 가중치 압축을 위한 희소 양자화된 표현입니다. _ arXiv preprint arXiv:2306.03078_, 2023b.
*딩 등 (2023) 닝딩, 유자친, 광양, 후초웨이, 종한양, 유성수, 성딩후, 유린천, 치민찬, 위즈천, 징이, 바일린자오, 샤오즈왕, 지위안류, 하이타오정, 지안페이천, 양류, 제탕, 후안지리, 마오송순. 대규모 사전 훈련 언어 모델의 매개변수 효율적인 미세 조정 _ 네이처 머신 인텔리전스_, 2023.
* Frantar & Alistarh (2022) Elias Frantar and Dan Alistarh. 최적의 뇌 압축: 정확한 훈련 후 양자화 및 가지치기를 위한 프레임워크입니다. _ Advances in Neural Information Processing Systems_, 35:4475-4488, 2022.
* Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: 생성 미리 훈련된 변압기에 대한 정확한 훈련 후 압축입니다. _ arXiv preprint arXiv:2210.17323_, 2022.
* Gao 등(2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 소샷 언어 모델 평가를 위한 프레임워크, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
* Guo et al.(2021) Demi Guo, Alexander M. 러쉬, 윤김 Diff 프루닝을 사용한 매개변수 효율적인 전이 학습. 2021년 ACL 진행률입니다.
* Hendrycks 등(2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 멀티태스킹 언어 이해도 측정 2020년 _International Conference on Learning Representations_ 에서.
* Hendrycks 등(2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 멀티태스킹 언어 이해도 측정 2021년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=d7KbjmI3GmQ](https://openreview.net/forum?id=d7KbjmI3GmQ).
* Hintermuller & Wu (2014) Michael Hintermuller and Tao Wu. 매트릭스 매니폴드의 부정확한 대체 최소화를 통한 강력한 주성분 추적 _ Journal of Mathematical Imaging_, 51:361-377, 2014.
* Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. nlp에 대한 매개변수 효율적인 전이 학습입니다. In _International Conference on Machine Learning_, pp. 2790-2799. PMLR, 2019.
*Hsu et al. (2022) Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. 가중 저순위 인수분해를 이용한 언어 모델 압축 2022년 _International Conference on Learning Representations_ 에서.
* Hu et al.(2022) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: 대형 언어 모델의 저순위 적응 *Proceedings of ICLR_, 2022.
*김 등(2023a) 김정훈, 이정현, 김성동, 박준석, 유강민, 세정권, 이동수. 4비트 이하 정수 양자화를 통해 압축된 대용량 언어 모델의 메모리 효율적인 미세 조정 _ arXiv preprint arXiv:2305.14152_, 2023a.
* Kim et al.(2023b) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: 조밀하고 희박하게 양자화합니다. _ arXiv preprint arXiv:2306.07629_, 2023b.
* Kingma & Ba (2015) Diederik P. Kingma and Jimmy Ba. 애덤: 확률적 최적화를 위한 방법. <Proceedings of ICLR>, 2015.
* Kundu 등 (2022) Souvik Kundu, Shikai Wang, Qirui Sun, Peter A Beerel, and Massoud Pedram. Bmpq: bit-gradient sensitivity-driven mixed-precision quantization of dnns from scratch. In _2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)_, pp. 588-591. IEEE, 2022.

* 권 등(2022) 세정권, 김정훈, 정인배, 강민유, 김진화, 박배송, 김병욱, 하정우, 나코성, 이동수. 알파튜닝: 양자화 인식 파라미터-대규모 사전 훈련된 언어 모델들의 효율적인 적응. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pp. 3288-3305, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL [https://aclanthology.org/2022.findings-emnlp.240](https://aclanthology.org/2022.findings-emnlp.240).
* 대규모 언어 모델 정렬을 민주화하는 중입니다. _ arXiv:2304.07327_, 2023.
* Lee et al.(2023) 이창훈, 이정유진, 태수김, 김형준, 박은혁 Owq: 대용량 언어 모델에서 가중치 양자화를 위한 활성화 이상치에서 배운 교훈입니다. _ arXiv preprint arXiv:2306.02272_, 2023.
* Lester et al.(2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 매개 변수 효율적인 프롬프트 조정을 위한 축척의 힘입니다. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 3045-3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL [https://aclanthology.org/2021.emnlp-main.243](https://aclanthology.org/2021.emnlp-main.243).
* Li & Liang (2021) Xiang Lisa Li and Percy Liang. 접두사 조정: 생성을 위한 연속 프롬프트를 최적화합니다. 2021년 8월 ACL 회차입니다.
* Li et al.(2023) Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo Zhao. Loftq: 대형 언어 모델에 대한 Lora-fine-tuning 인식 양자화. _ arXiv preprint arXiv:2310.08659_, 2023.
* Li et al.(2016) Yuanzhi Li, Yingyu Liang, and Andrej Risteski. 교대 최소화를 통한 가중 저순위 근사의 복구 보장 In _International Conference on Machine Learning_, pp. 2358-2367. PMLR, 2016.
* Lin et al.(2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: llm 압축 및 가속을 위한 활성화 인식 가중치 양자화. _ arXiv preprint arXiv:2306.00978_, 2023.
* Lin et al.(2022) Stephanie Lin, Jacob Hilton, and Owain Evans. 진실성: 모델들이 인간의 거짓을 어떻게 모방하는지 측정하는 것. <프로시빙스 of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp.3214-3252, 2022>
* Lin et al.(2010) Zhouchen Lin, Minming Chen, and Yi Ma. 손상된 저순위 행렬의 정확한 복구를 위한 증강 라그랑주 승산기 방법 _ arXiv:1009.5055_, 2010.
* Liu et al.(2013) Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma. 하위 계층 표현에 의한 하위 공간 구조의 강력한 복구 _ IEEE Transactions on Pattern Analysis and Machine Intelligence_, 35(1):171-184, 2013.
* Liu 등(2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 로베르타: 강력하게 최적화된 버트 사전 훈련 접근법입니다. _ arXiv preprint arXiv:1907.11692_, 2019.
* Ma & Aybat (2018) Shiqian Ma and Necdet Serhat Aybat. 강인한 주성분 분석과 그 변이를 위한 효율적인 최적화 알고리즘_ arXiv:1806.03430_, 2018.
* Mahabadi 등(2021) Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. 공유 하이퍼 네트워크를 통한 변압기의 매개변수 효율적인 다중 작업 미세 조정 <계산 언어학 협회의 제59차 연차 회의 및 제11차 자연어 처리에 관한 국제 공동 회의(제1권: 장문) >에서, pp. 565-576, 2021.
* Merity 등(2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 포인터 센티넬 혼합물 모델, 2016년
* Merity et al.(2017)
* 오이팡 등(2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, et al. Training language models to follow instructions with human feedback. _ arXiv:2203.02155_, 2022.
* Razenshteyn et al.(2016) Ilya Razenshteyn, Zhao Song, and David P Woodruff. 증명할 수 있는 보장과 함께 가중치가 낮은 순위 근사치. "Proceedings of the 40-8 annual ACM symposium on Theory of Computing"에서, pp. 250-263, 2016.
* Ren et al.(2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. {ZeRO-Offload}: 민주화 {Billion-Scale} 모델 훈련. <2021 USENIX Annual Technical Conference (USENIX ATC 21)_에서, pp. 551-564, 2021.
* Sakaguchi et al.(2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 위노그란데: 규모의 적대적인 위노그라드 스키마 도전입니다. _ Communications of the ACM_, 64(9):99-106, 2021.
* Le Scao 등(2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, et al. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. _ arXiv:2211.05100_, 2022.
* Shao et al. (2023) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 옴니콴트: 대형 언어 모델에 대해 옴니방향 보정된 양자화. _ arXiv preprint arXiv:2308.13137_, 2023.
* Srebro and Jaakkola (2003) Nathan Srebro and Tommi Jaakkola. 가중된 저순위 근사치 In _Proceedings of the 20th international conference on machine learning (ICML-03)_, pp. 720-727, 2003.
*Sung et al.(2021) Yi-Lin Sung, Varun Nair, and Colin A Raffel. 고정 희소 마스크로 신경망을 훈련합니다. In M. 란자토 A. 베이겔지머 Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems_, volume 34, pp. 24193-24205. Curran Associates, Inc., 2021. URL [https://proceedings.neurips.cc/paper/2021/file/cb2653f548f7809598e8b5156738cc51-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/cb2653f548f7809598e8b5156738cc51-Paper.pdf)
* Tahaei et al.(2021) Marzieh S. 타해이, 엘라 샤를릭스, 바히드 파르토비치 니아, 알리 곤드시, 메흐디 레재그홀리자데 KroneckerBERT: Knowledge Distillation을 통해 미리 훈련된 언어 모델에 대한 Kronecker 분해를 학습합니다. _ arXiv:2109.06243_, 2021.
* Tang et al.(2022) Chen Tang, Kai Ouyang, Zhi Wang, Yifei Zhu, Wen Ji, Yaowei Wang, and Wenwu Zhu. 학습된 계층별 중요도를 통한 혼합 정밀 신경망 양자화 In _European Conference on Computer Vision_, pp. 259-275. Springer, 2022.
* Taori 등(2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: 명령어 후속 llama 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca), 2023.
* 팀 (2023) 비쿠나 팀. Vicuna: 90%* ChatGPT 품질을 가진 GPT-4를 표현하는 오픈 소스 챗봇. 2023. [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)
* Touvron 등(2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and Efficient Foundation Language Models. _ arXiv:2302.13971_, 2023a.
* Touvron 등(2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almhahiri, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023b.
* Tukan et al.(2021) Murad Tukan, Alaa Maalouf, Matan Weksler, and Dan Feldman. 미세 조정, 울림 없음: 심층 네트워크를 압축하는 데 강력한 svd입니다. _ Sensors_, 21(16):5599, August 2021. ISSN 1424-3210. doi: 10.3390/s21165599.
* Tuzhilina & Hastie (2021) Elena Tuzhilina and Trevor Hastie. 가중치가 낮은 순위 행렬 근사 및 가속입니다. _ arXiv preprint arXiv:2109.11057_, 2021.
* Tuzhilina et al.(2021)* Wang et al.(2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 접착제: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼입니다. 2018년 _International Conference on Learning Representations_ 에서.
* Wang et al. (2023) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 자체 지침: 언어 모델을 자체 생성 지침과 정렬합니다. <Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 13484-13508, Toronto, Canada, July 2023.
* Wright 등(2009) John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. 강인한 주성분 분석: 볼록 최적화를 통해 손상된 하위 순위 행렬을 정확하게 복구합니다. In _Advances in Neural Information Processing Systems_, volume 22, pp. 2080-2088, 2009.
*Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: 대용량 언어 모델에 대한 정확하고 효율적인 훈련 후 양자화 _ arXiv:2211.10438_, 2022.
* Yao et al.(2021) Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3: Dyadic neural network quantization. In _International Conference on Machine Learning_, pp. 11875-11886. PMLR, 2021.
* Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 제로퀀트: 대규모 변압기를 위한 효율적이고 경제적인 훈련 후 양자화입니다. _ arXiv:2206.01861_, 2022.
* Yao et al. (2023) Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: 포괄적인 연구에서 낮은 순위 보상에 이르기까지 학습 후 양자화를 탐색합니다. _ arXiv preprint arXiv:2303.08302_, 2023.
* Yi et al.(2016) Xinyang Yi, Dohyung Park, Yudong Chen, and Constantine Caramanis. Gradient Descent를 통한 Robust PCA의 빠른 알고리즘 _ arXiv:1605.07784_, 2016. v1, 마지막으로 수정된 v2.
* 요시다(2023) 데이비스 요시다. Nf4는 이론적으로 최적의 정보가 아니다(그리고 그것은 좋다). _ arXiv preprint arXiv:2306.06965_, 2023.
* Zaken 등(2022) Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 비트핏: 변압기 기반 마스킹 언어 모델에 대한 간단한 매개변수 효율적인 미세 조정. 《제60회 컴퓨터 언어학 협회 연례 회의(제2권: 짧은 논문)》에서, pp. 1-9, 2022.
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yoatan Bisk, Ali Farhadi, and Yejin Choi. 헬라스와그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? <Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics>, pp. 4791-4800, 2019.
* Zhang 등(2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, et al. Opt: Open pre-trained transformer language models. _ arXiv:2205.01068_, 2022.
* Zhang and Yang (2017) Teng Zhang and Yi Yang. 매니폴드 최적화를 통한 강력한 PCA입니다. _ arXiv:1708.00257_, 2017.
* Zhou 등(2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment _ arXiv preprint arXiv:2305.11206_, 2023.
* Zhou and Tao (2011) Tianyi Zhou and Dacheng Tao. 고덱: 잡음이 있는 경우 랜덤화된 저순위 및 희소 행렬 분해. _ Proceedings of the 28th International Conference on Machine Learning (ICML-11)_, 2011.
* Zhou and Tao (2013) Tianyi Zhou and Dacheng Tao. 탐욕스러운 쌍방향 스케치, 완성 및 평활화 카를로스 M. Carvalho and Pradeep Ravikumar (eds.), _Proceedings of the 16th International Conference on Artificial Intelligence and Statistics_, Volume 31, 2013).

## 부록 A 구현 세부 정보

여기서 우리는 LQ-LoRA를 효율적으로 구현하기 위한 몇 가지 구현 세부 사항에 대해 논의한다.

PyTorch 기반 혼합 양자화.가중치 전용 양자화 기술은 일반적으로 서브-8 비트 행렬을 기본적으로 지원되는 데이터 유형(예: int8)으로 패킹한 다음, 역양자화 동안 플로팅 포인트 형식으로 패킹 해제해야 한다. 이와 같이, 기존의 구현들은 종종 특정 양자화 구성에 의존하는 커스텀 CUDA 확장을 요구하여, 혼합 양자화 전략들로 확장하는 것을 어렵게 한다. 우리의 구현은 빠른 실험과 동적 양자화 전략의 구현을 위해 전적으로 PyTorch를 기반으로 한다. PyTorch의 _torch_dispatch_ 기능을 사용하여 더하기 및 행렬 곱셈과 같은 PyTorch 연산에서 동작을 재정의하는 duck-type torch.Tensor,15를 사용한다. 그런 다음 PyTorch의 (전체 그래프) 컴파일러를 사용하여 다음과 같은 연산을 컴파일한다: (1) 비트-언패킹, (2) 역양자화, (3) 추가 및 마트물과 같은 선형 대수 연산, (4) (bf16 트레이닝을 위해) 전치 및 캐스팅. LoRA 미세조정을 위해, 우리는 이 PyTorch 기반 구현(컴파일에 따라 수행됨)이 4비트 NF 양자화에 맞춰진 CUDA 확장에 크게 의존하는 QLoRA의 비트 및 바이트 구현만큼 빠른 것을 관찰했다.

각주 15: [https://dev-discuss.pytorch.org/t/what-and-why-is-torch-dispatch/557](https://dev-discuss.pytorch.org/t/what-and-why-is-torch-dispatch/557)

각주 16: [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)

LoRA 옵티마이저 오프로딩.우리는 또한 선택적으로 CPU 기반 옵티마이저(Ren et al., 2021)와 함께 작업하며, 이는 Dettmers et al.(2023a)에서 제안된 페이지 가능 옵티마이저를 확장한다. 이 구현은 LoRA에서 파라미터의 작은 부분만이 훈련될 필요가 있고, 이는 CPU에 대한 계산뿐만 아니라 CPU와 GPU 사이의 데이터 이동을 비교적 관리할 수 있게 한다는 사실을 이용한다. 우리는 CPU에서 훈련 가능한 파라미터의 복사본을 유지하고, CPU에서 파라미터 복사본을 최적화 단계를 실행하기 전에 GPU에서 CPU로 기울기를 오프로드하고, 이를 다시 GPU로 복사한다. 매트릭스당 최적화 단계와 CPU를 비동기 복사를 통해 GPU 이동과 겹친다. 가장 큰 700억 개의 파라미터 모델에서, 우리는 이 전략을 사용한 훈련 속도의 한계(\(<\)2%) 증가만으로 \(14\%\) 메모리 절약을 발견했다.

그림 5: 입력 데이터와 양자화된 행렬 사이의 fb32에서 \(100\) 행렬-행렬 곱셈을 수행하기 위한 A100/A6000 GPU 런타임(이는 플라이 역양자화를 포함한다). bitsandbytes(bnb)(Dettmers et al., 2023a)는 훈련 및 추론(matrix-vector multiplications, Leftmost figure)을 위한 별도의 구현들을 갖는다. 우리는 NF-4와 동일한 양자화 구성을 사용하고 일관된 비교를 위해 첫 번째 레벨 비트(\(2,3,4,8\))를 변경한다.

[MISSING_PAGE_FAIL:17]

{
    "2311.12023v2": {
        "paper_id": "2311.12023v2",
        "abs_url": "https://arxiv.org/abs/2311.12023v2",
        "pdf_url": "https://arxiv.org/pdf/2311.12023v2.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2311.12023v2_LQ-LoRA_Low-rank_Plus_Quantized_Matrix_Decomposition_for_Efficient_Language_Model_Finetuning.pdf",
        "title": "LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Han Guo",
            "Philip Greengard",
            "Eric P. Xing",
            "Yoon Kim"
        ],
        "abstract": "We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.",
        "comments": "",
        "official_code_urls": [
            "https://github.com/hanguo97/lq-lora"
        ],
        "pwc_page_url": "https://paperswithcode.com/paper/lq-lora-low-rank-plus-quantized-matrix",
        "bibtex": "@misc{guo2024lqlora,\n      title={LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning}, \n      author={Han Guo and Philip Greengard and Eric P. Xing and Yoon Kim},\n      year={2024},\n      eprint={2311.12023},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
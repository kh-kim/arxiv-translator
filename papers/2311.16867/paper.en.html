<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# The Falcon Series of Open Language Models\n' +
      '\n' +
      'The Falcon LLM Team1\n' +
      '\n' +
      'Ebtesam Almazrouei Hamza Alobeidli Abdulaziz Alshamsi Alessandro Cappelli Ruxandra Cojocaru Merouane Debbah Etienne Goffinet Daniel Hesslow Julien Launay Quentin Malartic Daniele Mazzotta Badreddine Noune Baptiste Pannier Guilherme Penedo\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text-the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.\n' +
      '\n' +
      'Footnote 1: Authors listed alphabetically, contributions detailed in Appendix A. Correspondence to falconllm@tii.ae\n' +
      '\n' +
      '[https://huggingface.co/tiiuae/](https://huggingface.co/tiiuae/)\n' +
      '\n' +
      'Figure 1: **The Falcon series of models achieves competitive performance, with Falcon-180B nearly matching the 1-shot performance of PaLM-2 Large. 1-shot performance of PaLM (Chowdhery et al., 2022), PaLM-2 (Anil et al., 2023), and Falcon-180B, on a set of tasks from Brown et al. (2020). These evaluation results are only a small snapshot of our evaluations, see Section 6 for details and comparisons with GPT-3.5/4, LLaMA-1/2, and Inflection-1.**Introduction\n' +
      '\n' +
      'The on-going Cambrian explosion of language models has been primarily fueled by the unique _scalability_ of popular Transformer-based recipes. This scalability manifests over multiple axes:\n' +
      '\n' +
      '* **Performance scalability** (and predictability). Increase in pretraining compute budgets systematically yield improvements in language modeling capabilities, in a consistent and predictable way (Kaplan et al., 2020). Falcon-180B is the first publicly documented GPT-3-sized model to follow the updated scaling law recommendations of Hoffmann et al. (2022), with a total pretraining length of 3,500 billion tokens, without any upsampling.\n' +
      '* Data scalability. To scale-up pretraining efficiently, and to decouple pretraining and inference compute, increasingly large models should be trained for longer, on larger corpora. To sustain the Falcon series, we developed RefinedWeb (Penedo et al., 2023), a 5 trillion tokens high-quality filtered and deduplicated web dataset-the largest publicly documented.\n' +
      '* **Hardware scalability.** Transformer models (Vaswani et al., 2017) are naturally suited for modern GEMM optimized hardware, allowing their training and inference to be efficiently distributed over a large number of accelerators (Narayanan et al., 2021; Pope et al., 2023). With Falcon-180B, we demonstrate scaling-up pretraining to 4,096 A100 40GB with only 50 Gbps interconnect per accelerator on cost-efficient AWS cloud infrastructure.\n' +
      '\n' +
      'Building upon these fundamentals, increasingly large language models give rise to so-called emergent capabilities (Wei et al., 2022). These capabilities can be further tailored to human preference, to build instruction-following or chatty models (Ouyang et al., 2022). All together these methods have lead to the widespread deployment of large language models in customer-facing applications, such as ChatGPT (GPT-3.5/4, OpenAI (2023)), Claude, Bard (PaLM-2, Anil et al. (2023)), or Pi (Inflection-1, Inflection (2023)). In this paper, we primarily report on the pretraining alone of the Falcon series of models, and leave further downstream finetuning and alignment to future works.\n' +
      '\n' +
      'The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al., 2023)-a massive filtered and deduplicated web dataset. The architecture of the models is based on PaLM (Chowdhery et al., 2022), although we independently validated each decision, ultimately resulting in some minor tweaks-see Section 4 for details. The Falcon series leverages extensive custom tooling (i.e., pretraining codebase, data pipeline), of which development started in August 2022, with training of the models kicked-off in December 2022. In-depth evaluation shows that the Falcon series is competitive across scale, and that Falcon-180B nears the performance of PaLM-2 Large, positioning it as the best open model and in the top-3 of the best language models.\n' +
      '\n' +
      'Contributions.With this paper and the Falcon series, we make the following contributions:\n' +
      '\n' +
      '* **Public documentation of the pretraining of a large-scale model.** Recent state-of-the-art models have been scarcely documented, hindering further research and progress in the field. At variance with these works, we extensively document the pretraining of the Falcon series.\n' +
      '* **Open data and models.** To accelerate research, and to enable community-driven improvements of large language models, we openly release Falcon-7/40/180B, and a 600 billion tokens extract of the RefinedWeb dataset: [https://huggingface.co/tiiuae/](https://huggingface.co/tiiuae/).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & **Falcon-7B** & **Falcon-40B** & **Falcon-180B** \\\\ \\hline\n' +
      '**Pretraining** [tokens] & 1,500B & 1,000B & 3,500B \\\\\n' +
      '**Compute** [PF-days] & 730 & 2,800 & 43,500 \\\\\n' +
      '**Training** [A100s] & 384 & 384 & 4,096 \\\\\n' +
      '**Availability** & Apache 2.0 & Apache 2.0 & Responsible use license \\\\\n' +
      '**Agg. performance** (Section 6.5) & 60.8 & 67.1 & 70.3 \\\\\n' +
      '**Closest model** & \\textless{}GPT-3 & Chinchilla & PaLM-2 Large \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 1: **The Falcon series of models covers a wide range of capabilities and inference requirements, enabled by large-scale web data. Falcon-7B can efficiently run on consumer hardware (e.g., Apple M2), while Falcon-180B typically requires dedicated inference infrastructure (e.g., \\(8\\times\\text{A100}\\) 80GB ). We report steady zero-shot performance gains across the entire Falcon series.**\n' +
      '\n' +
      '###### Contents\n' +
      '\n' +
      '* 1 Introduction\n' +
      '* 2 State-of-the-art: from language modeling to frontier models\n' +
      '* 3 Design philosophy\n' +
      '* 4 Experiments and motivations for data, architecture, and hyperparameters\n' +
      '	* 4.1 Setup for small-scale experiments\n' +
      '	* 4.2 Data: web vs curated, code and multilinguality impact on English performance\n' +
      '		* 4.2.1 Web data alone can outperform curated corpora\n' +
      '		* 4.2.2 Against a strong web baseline, curated data can even be detrimental\n' +
      '		* 4.2.3 Limited code and multilingual data do not strongly degrade English performance\n' +
      '	* 4.3 Architecture and pretraining: validating popular recipes, and inference optimizations\n' +
      '		* 4.3.1 Extending multiquery into multigroup for tensor parallel training and inference\n' +
      '		* 4.3.2 Rotary positionnal embeddings may only offer a limited edge over ALBi\n' +
      '		* 4.3.3 The extra memory cost of GLU may not be worth it for cost-efficient training\n' +
      '		* 4.3.4 Small tweaks help scalability: parallel layers and no biases in linear layers\n' +
      '		* 4.3.5 Validating best practices for hyperparameters: z-loss, weight decay, LR search\n' +
      '	* 4.4 Further experimentation required: ideas that did not make the cut\n' +
      '	* 4.5 Wrapping-it up: validating overall dataset and architecture recipes\n' +
      '* 5 Implementation\n' +
      '	* 5.1 The Falcon dataset: predominantly web, with added curated and conversational data\n' +
      '		* 5.1.1 The Macrodata Refinement pipeline and the RefinedWeb dataset\n' +
      '		* 5.1.2 The Microdata curated corpora and conversational masking\n' +
      '	* 5.2 The Falcon architecture and recipe for efficient inference and (stable) training\n' +
      '		* 5.2.1 Architectural nitpicks: separate layer norms, tied embeddings, and scaling-up\n' +
      '		* 5.2.2 Large language model alchemy: hyperparameters for pretraining\n' +
      '	* 5.3 Large-scale distributed training on cloud infrastructure with Gigatron\n' +
      '		* 5.3.1 Combining 3D parallelism for fine-grained control, and ZeRO for scalability\n' +
      '		* 5.3.2 State-of-the-art throughput with dedicated Triton kernels\n' +
      '		* 5.3.3 Efficient memory use via selective recomputation implemented as a monolayer\n' +
      '		* 5.3.4 Numerical precision: all you need is bfloat16?\n' +
      '		* 5.3.5 Quality-of-life features for improved flexibility and reliability\n' +
      '	* 5.4 Run management: keeping large-scale infrastructure running\n' +
      '* 6 Results\n' +
      '	* 6.1 To prompt or not to prompt: comparing evaluations across codebases\n' +
      '	* 6.2 Comparisons with PaLM on a natural language tasks aggregate\n' +
      '	* 6.3 Comparisons with GPT-3.5 and GPT-4 on a limited set of tasks\n' +
      'State-of-the-art comparisons on common sense, question answering, and code tasks 6.5 Comparison with other models using the EleutherAI Evaluation Harness\n' +
      '* 7 Limitations\n' +
      '	* 7.1 Limitations of our findings and ablations\n' +
      '	* 7.2 Limitations of the Falcon models\n' +
      '* 8 Conclusion\n' +
      '* A Contributions\n' +
      '* B Acknowledgements\n' +
      '* C Model card\n' +
      '* Datasheet\n' +
      '* E Comparisons with undocumented models\n' +
      '* F Pseudocode samples\n' +
      '* F.1 Measurement plan to measure all to all bandwidths/latencies efficiently\n' +
      '* F.2 Converting tree token depth into an attention mask:\n' +
      '* F.3 Zero-1 pseudo-code\n' +
      '* G Prompts\n' +
      '\n' +
      'State-of-the-art: from language modeling to frontier models\n' +
      '\n' +
      'We provide in this section an overview of general trends and works adjacent to the Falcon series. For an in-depth literature review of individual technical components, see the relevant sections.\n' +
      '\n' +
      '**Language modeling.** Beyond corpus/task-specific approaches, the first large-scale vector-based word embeddings methods (Mikolov et al., 2013; Pennington et al., 2014) pioneered unsupervised learning from massive unstructured text corpora. The integration of deep recurrent neural architectures enabled models to deal with polysemy and to integrate contextual information (Peters et al., 2018); up to the emergence of the transfer learning paradigm, leveraging universal models specialized to downstream tasks through finetuning (Howard and Ruder, 2018). Despite the existence of many of the first principles currently used, early scaling attempts (Jozefowicz et al., 2016) only had mixed success, partly due to the fastidiousness and poor scalability on common hardware of recurrent approaches.\n' +
      '\n' +
      '**Transfomer models.** The introduction of the attention-based Transformer architecture Vaswani et al. (2017) sparked an explosion in the number of recipes to produce efficient, generalist models: from embedding and classification focused encoder-only BERTs (Kenton and Toutanova, 2019), to causal decoder-only GPTs (Radford et al., 2018). Specifically, GPT-2 (Radford et al., 2019) was the first series of models to popularize emergent few-shot generalization abilities, allowing a model to understand and perform arbitrary tasks simply from in-context instructions and demonstrations.\n' +
      '\n' +
      '**Large language models.** The aforementioned works laid out the key components to current models; the last ingredient, scaling, was demonstrated by GPT-3 (Brown et al., 2020), and consecerated by the outlining of scaling laws (Kaplan et al., 2020). As increasingly large amounts of compute are spent to pretrain models, commensurate gains are made in language modeling performance. The tantalizing prospect of a systematic and direct path to more capable language models lead to a "scaling frenzy": first with reproductions of GPT-3 with Jurassic-1 (Lieber et al., 2021) or PanGu-Alpha (Zeng et al., 2021), and with open efforts such as GPT-J (Wang and Komatsuzaki, 2021), OPT (Zhang et al., 2022), or BLOOM (Scao et al., 2022); second, with works pushing further the limits of scaling with Gopher (Rae et al., 2021), MT-NLG (Smith et al., 2022), or PaLM (Chowdhery et al., 2022). Increased development and adoption of large language models also lead to improvements of pretraining methods. Notably, Hoffmann et al. (2022) demonstrated with Chinchilla that optimal scaling should actually jointly increase model size and pretraining dataset. For deployment in the real-world, it may even be desirable to train far past so-called optimality, to decouple training and inference compute and reduce serving costs. This is illustrated with the LLaMA models (Touvron et al., 2023, 2023), with 7B/13B/30B/70B parameters models trained on up to 2 trillion tokens.\n' +
      '\n' +
      '**Frontier models.** Concurrently to this work, so-called frontier models have emerged, under the shifting definition of "large-scale machine-learning models that exceed the capabilities currently present in the most advanced existing models, and can perform a wide variety of tasks". Although a moving goal, we attribute recent works on GPT-4 (OpenAI, 2023) and PaLM-2 (Anil et al., 2023) as early contributions to this category. These stand out by their significantly increased compute budget, and improved capabilities. See Appendix E for details on our approach to undocumented models.\n' +
      '\n' +
      'Figure 2: Although open models lag behind closed models in pretraining compute (by \\(\\sim\\)18 months), the gap is not widening. After a 1 year lag, GPT-3 sparked a “scaling frenzy”, with the emergence of numerous LLMs. Models in the \\(>\\)10,000 PF-days range remain rare for open-source.\n' +
      '\n' +
      'Design philosophy\n' +
      '\n' +
      'Inspired by the bitter lesson (Sutton, 2019), we believe that scalable methods that best leverage compute are ultimately the most effective. Accordingly, in designing the Falcon series of models, we focused primarily on **scalability**, across three axes: performance, data, and hardware.\n' +
      '\n' +
      '**Performance scalability.** The sustained increase in the scale of large language models (Fig. 2) has been primarily motivated by so-called scaling laws: with increased pretraining compute comes commensurate improvements in language modeling capabilities (Hestness et al., 2017; Kaplan et al., 2020). This systematic path to model improvement has proved far more reliable than waiting for the occasional research epiphany to manifest a paradigm-shifting method. But upstream and downstream performance are not simply motivators of scaling, they are also a powerful driver. Indeed, quantifying the impact of modeling interventions (e.g., architecture tweaks, data sourcing, hyperparameters selection) is critical to sustaining the feedback loop provided by small-scale ablations. However, the question of _what_ to quantify is not trivial: upstream performance alone can be at odds with downstream tasks (Tay et al., 2021), and even downstream metrics may not be aligned with human preferences (Stiennon et al., 2020). This is made even more challenging by the fundamental contrast between pretraining objective (i.e., predict the next word on a predominantly web-based corpora) and common downstream use (i.e., follow users\' instructions in a helpful, harmless, and honest way) (Bai et al., 2022). As we focus on the pretraining stage of the Falcon models, we choose to center our evaluations on measuring **zero/few-shot generalization on large aggregates of natural language tasks** with the EleutherAI Harness (Gao et al., 2021)-similar to the setup of Le Scao et al. (2022); Wang et al. (2022). We build aggregates enabling comparisons with other state-of-the-art models, but note the difficulty in executing principled comparisons: practices diverge widely across papers in terms of task selection, prompting, and even mode of evaluation-standardized benchmarks remain only scarcely adopted. We discuss in Section 7 limitations of our evaluation setup.\n' +
      '\n' +
      'Data scalability. An increase in pretraining compute budget can be either spent towards a larger model, or towards longer training. While Kaplan et al. (2020) had first found optimal scaling to be mostly model size driven, Hoffmann et al. (2022) revised this finding and found that joint scaling is preferable-see Fig. 3 to see impact. Furthermore, growing model size also increases inference burden; where as increasing the pretraining dataset size is decoupled from inference costs. Recent open models have been trained on datasets of up to two trillions tokens (Touvron et al., 2023); because naive repetition risks degrading the model (Hernandez et al., 2022; Muennighoff et al., 2023), this has led to concerns about the sustainability of data scaling (Villalobos et al., 2022). These concerns are exacerbated by the widely belief that curated corpora are necessary to build state-of-the-art models, requiring manual addition of individual sources such as arXiv papers, books, and more (Brown et al., 2020; Gao et al., 2020). For the Falcon series of models, we choose to focus on **scaling high-quality web data through stringent filtering and deduplication**, enabling us to collect an English web dataset of 5,000 billion tokens and to not repeat any data during training. We extensively report on this work in Penedo et al. (2023), but also provide some key elements in this paper.\n' +
      '\n' +
      '**Hardware scalability.** Large-scale training requires thousands of hardware accelerators to work efficiently in unison; making the best use of these accelerators requires in turn principled distributed training methods (Shoeybi et al., 2019). Methods that are able to best run efficiently and leverage large-scale compute are often the ones that gain the most traction in the community (Hooker, 2021), as best evidenced by the Transformer architecture itself (Vaswani et al., 2017). Furthermore, it is difficult to find architectural improvements that significantly improve the task performance of models, compared to the impact of data for instance (Le Scao et al., 2022). Accordingly, **we focus architectural decisions not on improving task performance, but on improving hardware scalability and throughput**. We are also concerned with inference scalability (Pope et al., 2023), leading us to the adoption of tweaks such as a (revised) multiquery attention scheme (Shazeer, 2019).\n' +
      '\n' +
      'Finally, beyond scalability, we are also concerced with **cost-efficiency** and relying on **proven approaches**. We reimplement a 3D parallelism strategy (Narayanan et al., 2021) combined with optimizer sharding (Rajbhandari et al., 2020), enabling us to run on a more cost-efficient cloud AWS infrastructure with limited interconnect. We also put a strong focus on memory-saving methods, enabling us to run on cheaper 40GB A100. Reimplementing our data preprocessing and pretraining pipelines from scratch allow us to extensively verify them, rather than relying on an unknown external codebase. We also do not explore radical departures from the classic designs of large language models, such as state-space models (Fu et al., 2022), as these have usually not been proven at scale.\n' +
      '\n' +
      'Experiments and motivations for data, architecture, and hyperparameters\n' +
      '\n' +
      'We first focus on a series of small-scale experiments with models in the 1B-3B parameters range to validate recommended practices, as well as to identify interesting tweaks. We also conducted extensive experiments to validate our web data pipeline, reported in Penedo et al. (2023). With each subject of interest, we also outline common practices and our motivations for exploring this direction.\n' +
      '\n' +
      '### Setup for small-scale experiments\n' +
      '\n' +
      '**Small models.** For these ablations, we seek to be able to rapidly iterate with limited compute costs. This leads us to train 1/3 billion parameters models, for 30/60 billion parameters respectively. We choose these short training lengths to be illustrative of the optimality regime from Hoffmann et al. (2022) under which our larger models are likely to be trained. We base our reference architecture and hyperparameters on the one described for GPT-3 (Brown et al., 2020), with the caveat of using ALiBi positional embeddings as our baseline (Press et al., 2022). With reasonable resources (32-64 A100), these ablation models can be trained overnight or in a few days, enabling rapid iterations. We note that a caveat of using smaller models is that they may not be illustrative of some of the behaviours of larger models: for instance, Dettmers et al. (2022) found that outlier features emerge at the 6B scale, impacting quantization; concerns around data duplication and memorization have also shown to disprotionately affect larger models (Carlini et al., 2022; Hernandez et al., 2022)\n' +
      '\n' +
      '**Dedicated aggregates.** Although small models enable rapid iterations, they also have limited zero-shot capabilities; naive bulk evaluation would result in most tasks being close to the random baseline, and in significant noise in the evaluations. Using models trained on subsets of The Pile, we identified tasks that showed both reasonable performance at small-scale, and limited variability across runs. We independently quantified variance and average performance on over 50 tasks across 5 runs with a different seed (we use this \\(\\sigma\\) for architecture experiments) and across 10 runs with different data subsets and shuffling (for data experiments). Based on this analysis, we source 11 tasks from the evaluation setup of Brown et al. (2020); Le Scao et al. (2022); Srivastava et al. (2023) for our ablations (zs-main/data/web). Note that differences between these three subsets are mostly due to differing practices across time and between teams. zs-comp is a subset of main for comparisons with other models, based on commonly reported tasks. We also report perplexities on The Pile (Gao et al., 2020) (ppl-pile) for architectures (for data experiments, we found perplexities on The Pile to mostly illustrate differences in formatting rather than content), and on a restricted subset of 3 NLP tasks with low variance (zs-small). For our small-scale evaluations, we use both the EleutherAI harness (Gao et al., 2021) and BigBench (Srivastava et al., 2023); note that our inference and evaluation codebase differ significantly between this setup and the final results we report in Section 6, so results are not directly comparable. We present an outline of the aggregates in Table 2.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l} \\hline \\hline\n' +
      '**Tasks** & **Type** & **Random** & main & comp & data & web & core & pile \\\\ \\hline LAMBADA (Papemo et al., 2016) & Reading Comprehension & 0.0 & \\(\\checkmark\\) & & & & \\(\\checkmark\\) & \\\\ RACE (Lai et al., 2017) & Reading Comprehension & 25.0 & \\(\\checkmark\\) & & & & & \\(\\checkmark\\) & \\\\ HellaSwag (Zellers et al., 2019) & Common Sense & 25.0 & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\\\ Winogomez (Sakaguchi et al., 2019) & Common Sense & 50.0 & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\\\ PIQA (Risk et al., 2020) & Common Sense & 50.0 & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\\\ BooIQ (Clark et al., 2019) & Common Sense & 50.0 & \\(\\checkmark\\) & & \\(\\checkmark\\) & & \\\\ COPA (Gordon et al., 2012) & Common Sense & 50.0 & \\(\\checkmark\\) & & \\(\\checkmark\\) & & \\\\ Date (Srivastava et al., 2023) & Common Sense & 25.0 & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\\\ ARC (Clark et al., 2018) & Question Answering & 25.0 & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\(\\checkmark\\) & \\\\ OpenBookQA (Milwolov et al., 2018) & Question Answering & 25.0 & \\(\\checkmark\\) & & \\(\\checkmark\\) & & \\\\ SciQ (Johannes Wbelt, 2017) & Question Answering & 25.0 & \\(\\checkmark\\) & & \\(\\checkmark\\) & \\(\\checkmark\\) & \\\\ The Pile (Gao et al., 2020) & Language Modeling & & & & & & \\(\\checkmark\\) \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 2: **To evaluate small models used in ablations (1/3B models trained on 30/60B tokens), we build four aggregates across 11 tasks on which to measure zero-shot performance and perplexity.** We trained 15 reference models on subsets of The Pile and with random seeds to identify tasks with performance better than random and low variablity at this scale. All evaluations leverage the EAI Harness (Gao et al., 2021) except date which is taken from BigBench (Srivastava et al., 2023). main was built for architecture and hyperparameters ablations; data for data mixtures experiments; web for small-scale ablations on web data; and core for its low variance on a reduced number of tasks. This setup only covers natural language abilities. For main, data, and web, differences are mostly due to individual preferences at the time of the experiments.\n' +
      '\n' +
      '### Data: web vs curated, code and multilinguality impact on English performance\n' +
      '\n' +
      '#### 4.2.1 Web data alone can outperform curated corpora\n' +
      '\n' +
      'Our motivations for predominantly training on web data, details of the processing pipeline, and extensive evaluations are detailed in our dedicated RefinedWeb paper (Penedo et al., 2023). In this section, we only highlight key ablations guiding our decision to focus our efforts on web data.\n' +
      '\n' +
      '**Background.** Since its origins with simpler and shallower statistical language models (Shannon, 1951; Mikolov et al., 2013), natural language processing has long leveraged unstructured massive text corpora. If these corpora were at first built "sentence-wise" (Chelba et al., 2013), the emergence of more advanced architectures enabled models to best use long context information present in unified documents (Devlin et al., 2018; Radford et al., 2018). Starting with single-domain sources, such as Wikipedia or BookCorpus (Zhu et al., 2015), datasets scaled along with models, and massive web-scrapes gained prevalence (Ortiz Suarez et al., 2019; Raffel et al., 2019). However, it is widely believed that web data alone is insufficient to build performant models (Brown et al., 2020; Gao et al., 2020). Accordingly, large language models train on mixed corpora, combining both large-scale web data, and curated so-called "high-quality" individual sources (e.g., books, technical papers, social media conversations)- see Table 3 for an overview of common pretraining mixes.\n' +
      '\n' +
      'However, as we outlined in the data scalability discussion in Section 3, sourcing the trillions of tokens required for pretraining a modern language model may be challenging. This leads us to challenge the idea that curated data is fundamentally better than web data. Notably, building upon the work of Lee et al. (2022), we study how stringent deduplication and extensive filtering inspired by Rae et al. (2021) may enable web data alone to train performant models.\n' +
      '\n' +
      '**Question.** Can web data alone (with filtering and deduplication) be used to train models outperforming models trained on curated data, as measured by natural language zero-shot performance?\n' +
      '\n' +
      '**Methods.** We train 1/3B parameters models on 27/60B tokens, on a number of datasets of interest and on intermediary artefacts of our data pipeline. For state-of-the-art web datasets, we consider two versions of OSCAR (Ortiz Suarez et al., 2019; Abadji et al., 2022) and C4 (Raffel et al., 2019). For curated datasets, we consider The Pile (Gao et al., 2020), the most popular pre-aggregated dataset-many models have also elected to base their pretraining data on specific components of The Pile (Smith et al., 2022; Zhang et al., 2022). RW-Raw corresponds to the output of our pipeline with the least amount of filtering, immediately after text extraction-but still with English language identification applied as well as URL blocklist for known adult content; RW-Filtered applies a first round of heuristics, similar to the ones used by Rae et al. (2021); finally, RefinedWeb corresponds to our final web dataset, with deduplication applied in two stages. We evaluate all models on the zs-web aggregate-see Table 2 for details of its composition.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline  & **Web** & **Curated web** & **Curated** & & & & & **Total** \\\\  & & & & Books & Tech. & Code & Conv. & Epochs & \\\\ \\hline\n' +
      '**GPT-3** & 60 \\% & 22 \\% & 18 \\% & 16 \\% & 2 \\% & 0 \\% & 0 \\% & 2.4 & 300B \\\\\n' +
      '**The Pile** & 18 \\% & 10 \\% & 72 \\% & 15 \\% & 40 \\% & 7 \\% & 10 \\% & 1.8 & 340B \\\\\n' +
      '**MT-NLG** & 38 \\% & 29 \\% & 33 \\% & 16 \\% & 9 \\% & 2 \\% & 6 \\% & 2 & 270B \\\\\n' +
      '**Gopher** & 58 \\% & 10 \\% & 32 \\% & 27 \\% & 2 \\% & 3 \\% & 0 \\% & \\(\\sim\\)1 & 300B \\\\\n' +
      '**LaMDA** & 25 \\% & 0 \\% & 75 \\% & 0 \\% & 25 \\% & 0 \\% & 50 \\% & & 340B \\\\\n' +
      '**PaLM** & 27 \\% & 1 \\% & 72 \\% & 13 \\% & 4 \\% & 5 \\% & 50 \\% & & 780B \\\\ \\hline\n' +
      '**Chinchilla** & 55 \\% & 10 \\% & 35 \\% & 30 \\% & 1 \\% & 4 \\% & 0 \\% & 1.1 & 1400B \\\\\n' +
      '**LLaMA** & 82 \\% & 0 \\% & 18 \\% & 5 \\% & 7 \\% & 4 \\% & 2 \\% & 1.7 & 1400B \\\\\n' +
      '**Falcon** & 84 \\% & 0 \\% & 16 \\% & 6 \\% & 2 \\% & 3 \\% & 5 \\% & 1 & 3500B \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Following the recommendations of Hoffmann et al. (2022), pretraining datasets have increased in size, causing an increase in the prevalence of web data. Web data sources are massive web scrapes such as C4 (Raffel et al., 2019), sourced from CommonCrawl. Curated web data undergoes a targeted domain filtering: this includes CC-News (Hamborg et al., 2017) for instance. We consider sources such as arXiv, Wikipedia, or PubMed as technical curated data, and sources such as Reddit, HackerNews, or StackOverflow as conversational. We report not the size of the overall dataset, but the amount of tokens used for pretraining. For LaMDA (Thoppilan et al., 2022), numbers are roughly estimated as only rough counts are provided in the paper.\n' +
      '\n' +
      '**Results.** Results for this round of experiments are presented in Table 4. In line with expectations, we find that raw web data (RW-Raw) performs poorly; similarly OSCAR-22.01 offers the worst performance of all datasets. This is likely because its creators have opted to distribute it by default without any deduplication applied. Conversely, OSCAR-21.09 and C4 are both strong baselines. We notably find that The Pile very likely does not deliver better performance than web data.\n' +
      '\n' +
      'Subsequent stages in our pipeline significantly uplift dataset quality and the performance of models trained on it. Filtering alone enables us to close the gap with The Pile, while the addition of stringent deduplication allows for RefinedWeb to be the best dataset among the ones we benchmarked.\n' +
      '\n' +
      'We note two limitations of these experiments. First, the models are small, and trained on a limited amount of data. However, it is likely that gains from deduplication actually increase with model scale, as larger models are more sensitive to duplicates (Hernandez et al., 2022) and more likely to memorize individual samples (Carlini et al., 2022). Second, our evaluation focuses on natural language tasks. It is unlikely that models trained on web data alone would compare favorably to models trained on The Pile on code tasks for instance, as The Pile explicitely include code, while massive web scrapes are likely to be mostly devoid of it except for some incidental occurrences.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{|l|} \\hline\n' +
      '**Finding.** Challenging beliefs on data quality, filtered and deduplicated web data _alone_ allows \\\\ models to match the natural language tasks performance of models trained on curated data. \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Curation is not a silver bullet for zero-shot generalization: small-scale models trained on RefinedWeb outperform models trained on web data (C4, OSCAR), and on curated corpora (The Pile). Zero-shot accuracy on zs-web aggregate (\\(\\sigma\\) = 0.69, _likely_\\(\\pm\\)0.69%, _very likely_\\(\\pm\\)1.38% differences in scores). All models trained with identical architectures and hyperparameters, for the same amount of tokens. We find that OSCAR-22.01 underperforms other datasets significantly, perhaps because deduplication is only optional. C4 is a strong baseline, with OSCAR-21.09 lagging slightly behind, but we find that RefinedWeb outperforms both web datasets and the curated dataset, The Pile-performance gap with C4 is insufficient to be conclusive, but C4 would be too small for our models. Both filtering and deduplication contribute significantly to improving zero-shot performance.\n' +
      '\n' +
      'Figure 3: Two eras of pretraining practices, from predominant model scaling \\(\\blacksquare\\) to joint model and data scaling \\(\\blackphi\\). Before Hoffmann et al. (2022), models (\\(\\blacksquare\\)) predominantly scaled their parameter count at a fixed dataset size (around 300 billion tokens), in line with the recommendations of Kaplan et al. (2020). Afterward (\\(\\blacklozenge\\)), models started scaling both model size and dataset size jointly, sharply increasing the need for scalable data pipelines. Estimate of clean data available in CommonCrawl from our work in Penedo et al. (2023), considering English only–would double if allowing multilinguality.\n' +
      '\n' +
      '#### 4.2.2 Against a strong web baseline, curated data can even be detrimental\n' +
      '\n' +
      '**Background.** In Section 4.2.1 and Table 3, we have noted that large language models use pretraining datasets combining both massive web crawl data and individual curated sources. Such sources were first employed to build domain-specific models (Beltagy et al., 2019); they have also been proposed to broaden the expressiveness of models, for instance for conversational modalities (Adiwardana et al., 2020; Thoppilan et al., 2022). Some of these sources can also exist at the intersection of strongly curated data and crawls: Laurenson et al. (2022) has for instance proposed to seed the first links in a crawl using human-selected URLs. However, these tailored sources raise challenges for practitioners. First, individual corpora are much less scalable than massive web crawls, as they require scattered work instead of a centralized pipeline. Second, the providers of some of these sources have begun taking steps to forbid LLMs from being trained on their data Paresh (2023), and adequate licensing may be costly to obtain. Based on our findings from the previous section, we may wonder what happens when we combine curated data with a strong web baseline like RefinedWeb.\n' +
      '\n' +
      '**Question.** When added in substitution of a strong web baseline, is curated data from individual corpora still beneficial to the natural language zero-shot performance of a model?\n' +
      '\n' +
      '**Methods.** We train small 1B models on 30B tokens, with the pretraining data split between web data and a specific curated category. We sample training on 1, 10, 25, 50, 75, and 100% of the targeted category. We only consider a one-dimensional approach, and mix web data with a single category of curated data. We split our categories in books, conversations, and technical data as outlined in Table 5. For the individual corpora making these categories, we draw inspiration from The Pile (Gao et al., 2020) which we enhance with data from Reddit (Baumgartner et al., 2020) for the conversational category. Our web data is taken from RefinedWeb (Penedo et al., 2023) and we process curated sources through a similar pipeline, applying filtering and deduplication to make for a fair comparison. We evaluate performance on the zs-data aggregate-see Table 2 for detailed make-up.\n' +
      '\n' +
      '**Results.** We plot the zero-shot accuracy as we increase the fraction of curated data in Fig. 4. When combined with a strong web baseline, we find that the addition of curated data never significantly uplifts performance. In fact, excess of curated data even worsens performance: for books and technical, past 50%, we start observing meaningful degradation of accuracy. We believe this is likely caused by "mode collapse" compared to the high diversity of web data.\n' +
      '\n' +
      'Interestingly, conversations perform decently throughout, with the smallest degradation at 100%. We posit this could either be due to our conversation category being the most diverse of the three, or to conversations being closer to the distribution of tasks. Indeed, this category likely includes people interacting, answering questions, and giving instructions to one another-a style which is less prevalent in books or technical-driven content like papers and patents.\n' +
      '\n' +
      'Once again, this ablation is limited by the scope of the tasks considered. It is likely that highly technical tasks may benefit from domain specific data, and that web data would be a rather poor baseline for code tasks. Furthermore, our zero-shot evaluations are all done under a short context length: books could for instance be beneficial in helping the model learn long-range correlations.\n' +
      '\n' +
      '**Finding.** When added in substitution of a strong web baseline, curated categories of data do not systematically result in an improvement in natural language zero-shot performance.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Conversations** & Reddit (Baumgartner et al., 2020), HackerNews, OpenSubtitles (Tiedemann, 2016), Ubuntu IRC, Youtube Subtitles, StackOverflow \\\\\n' +
      '**Books** & Project Gutenberg Rae et al. (2019) \\\\\n' +
      '**Technical** & ArXiv, PubMed Central, PubMed Abstracts, USPTO Patents \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: **We split curated data in three broad categories: conversations, books, and technical. Individual components inspired by Gao et al. (2020), but and processed through our own data pipeline.**\n' +
      '\n' +
      '#### 4.2.3 Limited code and multilingual data do not strongly degrade English performance\n' +
      '\n' +
      '**Question.** Can limited amounts (5-10%) of code and multilingual data be substituted in pretraining data added without compromising the English performance of the model?\n' +
      '\n' +
      '**Multilinguality.** Some of the first large-scale deployment of RNNs and Transformers were for machine translation (Wu et al., 2016); with the attention mechanism originally introduced for such use cases (Bahdanau et al., 2014). Accordingly, it\'s no surprise that multilingual language models rapidly flourished along their monolingual counterparts (Xue et al., 2021). However, multilingual _generative_ large language models have remained more elusive. Indeed, both Lin et al. (2021) and Scao et al. (2022b) have reported that massive multilinguality comes at the expense of English performance, resulting in multilingual models that underperform their monolingual counterparts (Scao et al., 2022a). This so-called curse of multilinguality (Conneau et al., 2020) has lead practitioners to be weary of naively adding other languages in bulk - even PaLM (Chowdhery et al., 2022), which explicitly targets multilingual capabilities, only trains on 20% non-English data. Furthermore, multilingual data is scarcely available (Costa-jussa et al., 2022): nearly 60% of the documents in CommonCrawl are English, and the distribution of top languages is skewed towards European ones. Notably, Mandarin Chinese is only the 6th top language in CommonCrawl, despite being 2nd worldwide, and Hindi does not show-up even in the top-20 despite being 3rd worldwide (Eberhard et al., 2023).\n' +
      '\n' +
      'We choose to experiment with a restricted multilingual setup: we consider languages with a Latin alphabet, and focus on those for which we can collect a non-trivial amount out of CommonCrawl (over 10 billion tokens) using our data pipeline. Our splits for experiments are in Table 6. We train models with a fixed 10% multilingual data (weighing individual languages according to their prevalence in CommonCrawl), and evaluate on English tasks. Each data split uses a dedicated tokenizer.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Set** & **Languages** \\\\ \\hline\n' +
      '**English** & English \\\\\n' +
      '**Restricted** & English, German, Spanish, French \\\\\n' +
      '**European** & English, German, Spanish, French, Italian, Dutch, Polish, Portuguese, Czech, \\\\  & Swedish, Romanian, Danish, Norwegian, Catalan, Slovene, Bulgarian \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: **We split languages in three categories: English-only, Restricted (adding the 3 most spoken languages in Europe), and European.** We only consider languages with a Latin alphabet, and sufficient presence in CommonCrawl to collect at least 10 billion tokens.\n' +
      '\n' +
      'Figure 4: When compared to a strong web data baseline, high-quality curated data does not improve zero-shot performance. Over-reliance on a single type of curated data is even detrimental to performance. The make-up of the conversations, books, and tech data corpora is outlined in Table 5. We report the zero-shot performance using different data-mixtures and compare with a baseline model trained on RefinedWeb only, our deduplicated and filtered web-only dataset Penedo et al. (2023). The shaded green-area indicates the \\(\\pm 3\\sigma\\) confidence interval based on 10 experiments across data splits.\n' +
      '\n' +
      'We present results in Table 7. We find that the performance degradation from 10% multilinguality is very limited, and that the addition of other European languages over German, Spanish, and French do not drive additional degradation. We saw most of the reduction in performance on HellaSwag, while other tasks are not strongly impacted. We note that these experiments apply in a very restricted setting, and may not be illustrative of other multilingual setups (e.g., languages without a latin alphabet, using other languages to compensate for running out of English tokens, etc.)\n' +
      '\n' +
      '**Code.** Large language models have demonstrated strong coding abilities, either through finetuning after general pretraining (Chen et al., 2021; Chowdhery et al., 2022; Roziere et al., 2023), or through dedicated pretraining recipes (Li et al., 2023a). Furthermore, at variance with multilingual data, code data is plentiful (Kocetkov et al., 2022), with trillions of tokens available from crawing public repositories. Code tasks are a predominant application of LLM-powered assistants (Luo et al., 2023), and so allowing for such use cases with Falcon is important. However, we do not want to risk compromising language capabilities: we thus validate here the common practice of adding limited code data to pretraining, in line with other models (see Table 3 for common fractions in pretraining).\n' +
      '\n' +
      'We select the top-30 programming languages from GitHub, and substitute 5% of our pretraining data for code. Note that we apply deduplication to the code dataset, but adjust our heuristics to avoid removing too much of the data. We consider only performance on English tasks. Results are outlined in Table 7. We find the addition of code data to have a similar effect to multilinguality (if only weaker, perhaps because of the smaller proportion), with only little to no task degradation.\n' +
      '\n' +
      '**Finding.** Small fractions of code and multilingual data (5-10%), in line with common recipes for large language models, do not broadly impact zero-shot performance on English tasks.\n' +
      '\n' +
      'We note that the small scale of our ablations is here a stronger limitation, likely leading us to a more conservative choice on multilinguality and code. It has been argued that larger models, thanks to their increased capacity, can better deal with multilinguality (Shaham et al., 2022). Code data has also been shown for larger models to boost commonsense abilities (Madan et al., 2022). More broadly, a similar effect has been observed for multilingual models (Aghajanyan et al., 2023).\n' +
      '\n' +
      '### Architecture and pretraining: validating popular recipes, and inference optimizations\n' +
      '\n' +
      '#### 4.3.1 Extending multiquery into multigroup for tensor parallel training and inference\n' +
      '\n' +
      '**Background.** Unanimously, large language models first adopted the multihead attention scheme described in Vaswani et al. (2017). Each token produces \\(n_{\\text{head}}\\) triplets of (query, keys, and values), and the result of each head is then summed to produce the final output of the attention module. However, this scheme can be altered. Shazeer (2019) found that one can share the same keys and values between all attention heads with only a small degradation in performance. In this so-called multiquery attention, the number of heads for the queries remains \\(n_{q}=n_{\\text{head}}\\) but there is only one head for the keys and values, \\(n_{kv}=1\\). This significantly reduces inference memory consumption: during autoregressive generation, the keys and values are cached to accelerate generation-with multiquery, the K,V-cache size is divided by \\(n_{\\text{head}}\\) compared to vanilla attention, resulting in a 10-100x reduction in memory consumption for common models-see Table 8. Multiquery improves the scalability of inference for large models (Pope et al., 2023). Chowdhery et al. (2022) has recently popularized this architectural modification, which has notably been adopted by LLaMA-2 (Touvron et al., 2023b).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline\n' +
      '**Pretraining data** & \\multicolumn{2}{c}{**Zero-shot accuracy**} \\\\  & zs-main\\(\\uparrow\\) & zs-small\\(\\uparrow\\) \\\\ _Likely_ threshold (1-\\(\\sigma\\)) & \\(\\pm 1.0\\) & \\(\\pm 0.5\\) \\\\ \\hline English-only & **53.7** & **49.2** \\\\\n' +
      '10\\% Restricted & 53.4 & 48.3 \\\\\n' +
      '10\\% European & 53.6 & 48.2 \\\\ \\hline\n' +
      '5\\% Code & 53.6 & 48.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Including in the pretraining data a small fraction of code or multilingual data broadly does not degrade performance significantly, except on specific tasks. Overall, the degradation observed is barely measurable on our larger aggregate, and mostly driven by HellaSwag on the zs-small one. Underlined values have crossed the _likely_ 1-\\(\\sigma\\) degradation threshold.\n' +
      '\n' +
      '**Scaling.** Interestingly, multiquery is disproportionately effective for larger models. With \\(N\\) the total number of parameters, \\(d_{\\text{model}}\\) the model size, \\(n_{\\text{layer}}\\) the number of layers, and assuming a fixed \\(d_{\\text{head}}=d_{\\text{model}}/n_{\\text{head}}\\)-which is typically the case when using FlashAttention (Dao et al., 2022). For efficient scaling, it is recommended that \\(n_{\\text{layer}}\\sim\\mathcal{O}(\\log(N))\\)(Levine et al., 2020); since we can approximate \\(N\\simeq n_{\\text{layer}}(d_{\\text{model}})^{2}\\)(Kaplan et al., 2020), it follows that the size of the K,V-cache with multihead attention scales in \\(\\mathcal{O}(\\sqrt{N}\\log(n))\\). Conversely, for multiquery the K,V-cache only stores a fixed \\(2d_{\\text{head}}\\) per layer, which does not increase with width; this results in overall more efficient scaling, in \\(\\mathcal{O}(\\log(N))\\) instead.\n' +
      '\n' +
      '**Multigroup.** One caveat of multiquery attention is that it is difficult to efficiently parallelize when relying on tensor parallelism, as is common with GPU-based infrastructure (Shoeybi et al., 2019). Either each GPU keeps a copy of the shared key/value, recomputing them individually and then sharing gradients to keep them in sync, or they are computed on a single GPU and then communicated as necessary. We propose to introduce separate key/value pairs for each tensor parallel rank, simplifying the required communications. As in Shazeer (2019), we keep \\(n_{q}=n_{\\text{head}}\\), but now have \\(n_{kv}=\\text{TP}\\). This scheme doesn\'t change the scaling of the K,V-cache, as it only applies a fixed TP factor. Concurrently to the development of the Falcon series, Ainslie et al. (2023) also proposed this modification; we refer to this variant of attention as grouped query attention or multigroup. We note that the communication reduction applies not just during inference, but also during training.\n' +
      '\n' +
      '**Results.** We train 1B/3B models on 30/60B tokens from The Pile (Gao et al., 2020), with multiquery and varying degrees of multigroup attention. Importantly, we do not control for the reduction in parameters caused by the loss of additional keys and values-some degradation is thus expected. Results are presented in Table 9. Both multiquery and multigroup do not result in any large reduction in zero-shot performance, even without compensating for the reduction in trainable parameters.\n' +
      '\n' +
      '**Recipe decision.** To improve performance scalability for the largest models, the Falcon series implement multigroup with KV=TP for all models (respectively 1/8/8 for Falcon-7/40/180B).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline\n' +
      '**Attention scheme** & \\(n_{q}\\) & \\(n_{kv}\\) & **K,V-cache for a 2,048 sequence** & & \\\\  & & & & 7B & 40B & 180B \\\\ \\hline Vanilla & \\(n_{head}\\) & \\(n_{head}\\) & \\(\\mathcal{O}(\\sqrt{N}\\log(n))\\) & 1GB & 4GB & 10GB \\\\ Multiquery (Shazeer, 2019) & \\(n_{head}\\) & 1 & \\(\\mathcal{O}(\\log(N))\\) & 20MB & 30MB & 40MB \\\\ Multigroup (Ainslie et al., 2023) & \\(n_{head}\\) & TP & \\(\\mathcal{O}(\\log(N))\\) & N/A & 250MB & 335MB \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 8: Multiquery/group schemes significantly reduces the size of the K,V-cache for inference. Assuming TP=1 for Falcon-7B and TP=8 for Falcon-40/180B, sequence length 2,048.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline\n' +
      '**Model size** & **KV** & **Performance** & & \\\\  & & zs-main\\(\\uparrow\\) & zs-small\\(\\uparrow\\) & ppl-pile\\(\\downarrow\\) \\\\ _Very likely_ threshold (2-\\(\\sigma\\)) & \\(\\pm 2.2\\) & \\(\\pm 0.8\\) & \\(\\pm 0.005\\) \\\\ \\hline\n' +
      '1B & 1 & 48.5 & 42.6 & 0.908 \\\\  & 2 & 48.2 & 42.1 & 0.899 \\\\  & 4 & 48.9 & 42.4 & 0.908 \\\\  & 8 & 48.6 & 42.8 & 0.903 \\\\  & Vanilla & **49.2** & **43.1** & **0.895** \\\\ \\hline\n' +
      '3B & 1 & 52.7 & 48.6 & 0.825 \\\\  & 8 & **54.6** & **50.1** & 0.819 \\\\  & Vanilla & 54.4 & 49.8 & **0.807** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 9: Even without controlling for the reduction in parameters, multiquery only comes at a limited zero-shot performance cost. Impact on perplexity is more directly measurable, whereas impact on zero-shot performance is less consistent. Multigroup with KV=8 consistently performs close to the vanilla baseline. Underlined values have crossed the _very likely_ 2-\\(\\sigma\\) degradation threshold.\n' +
      '\n' +
      '#### 4.3.2 Rotary positionnal embeddings may only offer a limited edge over ALiBi\n' +
      '\n' +
      '**Background.** By default, attention does not provide positional information to the model: it only sees the sequence as a bag-of-word. Accordingly, the original Transformer architecture adopted absolute sinusoidal embeddings to encode positional information Vaswani et al. (2017). However, absolute embeddings have since declined in popularity, the community shifting to relative embeddings instead. While this shift is well motivated empirically (Shaw et al., 2018; Scao et al., 2022), practitioners have yet to crystallize on a single relative positional embedding: BLOOM and MPT (Scao et al., 2022; MosaicML, 2023) use ALiBi (Press et al., 2022), while GPT-J, PaLM, and LLaMA (Wang and Komatsuzaki, 2021; Chowdhery et al., 2022; Touvron et al., 2023, 2023) use Rotary Positional Embeddings (RoPE) (Su et al., 2021). RoPE are often cited as delivering better upstream performance in the works above, while ALiBi benefits from built-in extrapolation abilities. Another recently introduced alternative are Universal Relative Positonal Embeddings, URPE (Luo et al., 2022), which address shortcomings in the expressivity of typical relative positional embeddings. As a curiosity, we note that the autoregressive mask of a causal model also provides some positionnal information to the model (Scao et al., 2022; Haviv et al., 2022), enabling training without any positional embeddings to be comparable to absolute sinusoidal ones for zero-shot performance.\n' +
      '\n' +
      'Concurrently to this work, recipes have emerged to enable zero-shot or finetuned length extrapolation with RoPE (Chen et al., 2023), briding the gap with ALiBi for extrapolation.\n' +
      '\n' +
      '**Results.** We train 1/3B models on 30/60B tokens on The Pile (Gao et al., 2020). We report results in Table 10. We find no evidence for URPE outperforming RoPE-since it would require significant modifications to the fused-attention kernels to deliver acceptable performance, we do not pursue it further. At the 1B scale, we find a likely advantage to using RoPE over ALiBi; however, that advantage diminishes at the 3B scale, and is insufficient to conclude clearly. One remaining advantage of ALiBi is its compute overhead: it is significantly cheaper to compute than RoPE; however, with custom Triton kernels (Section 5.3.2) we are able to mitigate that overhead.\n' +
      '\n' +
      '**Recipe decision.** In-line with other popular large-scale models, we adopt rotary positionnal embeddings, and use custom kernels to mitigate the overhead.\n' +
      '\n' +
      '#### 4.3.3 The extra memory cost of GLU may not be worth it for cost-efficient training\n' +
      '\n' +
      '**Background.** Activations based on gated linear units (Shazeer, 2020) are widely believed to outperform traditional activation functions such as GeLU (Scao et al., 2022). They have seen adoption in models such as PaLM and LLaMA (Chowdhery et al., 2022; Touvron et al., 2023, 2023).\n' +
      '\n' +
      '**Scaling.** Scaling-wise, GLU activations have been preferred as well, as they increase the size of the MLP (doubling its first layer), shifting more compute towards simple matrix multiplications. However, this does come at a cost: the memory required to store the intermediary activations in the MLP is higher. Remember that, typically, the inputs to the activation function are saved for the backward (as recomputing the function itself is negligible). For gated units, this input is now twice large. Overall, SwiGLU doubles intermediary activations, and increases by 50% the number of parameters in the MLP: the activation memory per parameter for the MLP is thus increased by 33%.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline\n' +
      '**Model size** & **Pos. Emb.** & \\multicolumn{2}{c}{**Performance**} \\\\  & & zs-main\\(\\uparrow\\) & zs-small \\(\\uparrow\\) & ppl-pile \\(\\downarrow\\) \\\\ _Likely_ threshold (1-\\(\\sigma\\)) & \\(\\pm\\)1.1 & \\(\\pm\\)0.4 & \\(\\pm\\)0.002 \\\\ \\hline\n' +
      '1B & ALiBi & 49.2 & 43.1 & 0.895 \\\\  & URPE & 49.6 & 43.1 & 0.885 \\\\  & RoPE & **50.0** & **44.2** & **0.883** \\\\ \\hline\n' +
      '3B & ALiBi & **54.4** & 49.8 & 0.807 \\\\  & RoPE & **54.4** & **50.5** & **0.799** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 10: Although at small-scale URPE and RoPE may likely be better than ALiBi, that advantage isn’t as clear at increased size. We find ALiBi to be likely worst than rotary on two of our three aggregates for 1B models, but to be closer to the performance of rotary at the 3B scale. Underlined values have crossed the _likely_ 1-\\(\\sigma\\) degradation threshold over RoPE.\n' +
      '\n' +
      '**Results.** In Table 11, training 1B models on 30B tokens of The Pile, we find no clear benefits from adopting SwiGLU for zero-shot performance-and the improvement on perplexity is just at the threshold of being unlikely to characterize an actual improvement due to variance in the evaluation setup. We note that this could be due to the fact SwiGLU may require dedicated hyperparameters tuning-as for all architecture ablations, we simply adopted the ones of GPT-3 (Brown et al., 2020). Furthermore, we saw little additional throughput gains from SwiGLU (as we already use parallel attention/MLP layers). Since we train the Falcon series on 40GB A100 to optimize costs, we were concerned early on about memory consumption-accordingly, SwiGLU, with its increased memory intensity, and similar performance to GeLU, would likely be a net negative for us.\n' +
      '\n' +
      '**Recipe decision.** Out of concern for the memory footprint of our trainings on A100-40GB, and because of no clear uplift in zero-shot, we choose not to adopt SwiGLU.\n' +
      '\n' +
      '#### 4.3.4 Small tweaks help scalability: parallel layers and no biases in linear layers\n' +
      '\n' +
      '**Parallel attention and MLP blocks.**Wang and Komatsuzaki (2021) first introduced parallel attention and MLP layers while training GPT-J. This augmentation is important to reduce the communication costs associated with tensor parallelism: this simple modification cuts the number of all_reduce necessary from two to one per layer. We found no measurable degradation in zero-shot performance or perplexity, in line with Chowdhery et al. (2022), and adopt this practice. See Fig. 5 for an illustration.\n' +
      '\n' +
      '**No biases.**Chowdhery et al. (2022) found that removing the biases in the linear layers and layer norms improves stability. We validate that removing the biases in the linear layer does not result in worse performance (see Table 11): neither in terms of language modeling loss nor in terms of the final zero-shot performance. Accordingly, we remove biases from the linear layers in the Falcon series.\n' +
      '\n' +
      '**Recipe decision.** We adopt parallel attention and MLP, and remove biases from linear layers.\n' +
      '\n' +
      '#### 4.3.5 Validating best practices for hyperparameters: z-loss, weight decay, LR search\n' +
      '\n' +
      '**Z-loss.** First introduced in the mesh-tensorflow codebase2 Shazeer et al. (2018), z-loss aims at increasing the stability of training, by encouraging the logits to stay close to zero. It can be implemented as an auxiliary loss: z_loss=\\(10^{-4}\\) log\\({}^{2}\\)(\\(\\Sigma_{i}e^{z_{i}}\\)), where \\(z_{i}\\) is the output logits of the model. Note that z-loss does not have a significant impact on task performance at small scale (Table 11).\n' +
      '\n' +
      'Footnote 2: As a fun exercise for the reader, we encourage trying to dig up that citation, starting from PaLM (Chowdhery et al., 2022), which popularized this practice outside of Google by outlining it in its main text.\n' +
      '\n' +
      '**Recipe decision.** We adopt z-loss, as it is claimed to improve large-scale training stability and does not impact zero-shot performance in our ablations.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline\n' +
      '**Model size** & **Pos. Emb.** & \\multicolumn{2}{c}{**Performance**} \\\\  & & zs-main\\(\\uparrow\\) & zs-small\\(\\uparrow\\) & ppl-pile\\(\\downarrow\\) \\\\ _Unlikely_ threshold (0.4-\\(\\sigma\\)) & \\(\\pm 0.4\\) & \\(\\pm 0.2\\) & \\(\\pm 0.001\\) \\\\ \\hline\n' +
      '1B & & **49.2** & 43.1 & 0.895 \\\\  & SwiGLU & **49.2** & 43.1 & **0.891** \\\\  & z-loss & 49.0 & **43.6** & 0.895 \\\\ \\hline\n' +
      '3B & & **54.5** & **49.8** & **0.807** \\\\  & No biases & 54.4 & **49.8** & **0.807** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 11: Small architectural tweaks like GLU, z-loss, and removing biases are unlikely to improve zero-shot performance. However, in some scenarios, these have been proposed to improve scalability and/or stability, which may warrant their adoption. Underlined values have crossed above the _unlikely_ 0.4-\\(\\sigma\\) improvement threshold over our baseline.\n' +
      '\n' +
      '**Weight decay.** We attempted to reproduce the weight decay schedule from Chowdhery et al. (2022), but failed to obtain an improvement-we suspect this is due to differences in initialization. We found that weight decay has a disproportionate effect on datasets which have not been deduplicated/are of lower quality (Table 12). We use AdamW for weight decay (Loshchilov and Hutter, 2018).\n' +
      '\n' +
      '**Recipe decision.** We use a fixed weight decay of 0.1 with AdamW for all Falcon models.\n' +
      '\n' +
      '**Optimal learning rate.** Practices for setting the learning rate of a run differs, from naive grid search to more principled approaches (Yang et al., 2022; Dinan et al., 2023). We further discuss in Section 4.4 our failures to implement and reproduce some of the later; in this short section, we focus on the naive approach and our validation of it. Broadly speaking, setting too high of a learning rate risks to cause divergence of the run and instabilities during training; too low, on the opposite, will leave some upstream and downstream performance on the table, leading to inefficient training.\n' +
      '\n' +
      'We propose to search through possible learning rate in the following away: (1) we select 4-6 roughly logarithmically-spaced candidate learning rates, anchoring around the ones used in GPT-3 (Brown et al., 2020), and favoring higher learning rates; (2) we run through a long 500 million tokens warmup for all candidate learning rates; (3) we pick the learning rate which has achieved the lowest loss at this point, and discard any learning rate which has already caused spikes.\n' +
      '\n' +
      'We test this method at small scale, picking 7 learning rates for a 1B model, and then comparing the ranking obtained after warmup against the actual final ranking achieved. Results are presented in Table 13. We find that this naive method succeeds at finding the best learning rate from the rankings at the end of warm-up. More broadly, ranks at the end of warm-up and at the end of training are relatively stable, with only the 2nd and 3rd best switching places.\n' +
      '\n' +
      '**Recipe decision.** From candidates LRs, we pick the one with the lowest loss after warm-up.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c} \\hline \\hline\n' +
      '**Dataset** & **Weight decay** & \\begin{tabular}{c} **Performance** \\\\ **zs-main** \\(\\uparrow\\) \\\\ \\end{tabular} & \\begin{tabular}{c} **zs-small** \\(\\uparrow\\) \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{c} **ppl-pile** \\(\\downarrow\\) \\\\ \\end{tabular} \\\\ _Likely_ threshold (1-\\(\\sigma\\)) & \\(\\pm\\)1.1 & \\(\\pm\\)0.4 & \\(\\pm\\)0.002 \\\\ \\hline RefinedWeb & 0. & **52.1** & 47.9 & 1.07 \\\\  & 1. & 52.0 & **48.4** & **1.06** \\\\ \\hline The Pile & 0. & 50.3 & 43.7 & 0.877 \\\\  & 1. & **51.7** & **45.0** & **0.868** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 12: Weight decay is likely to improve performance, especially so for datasets such as The Pile, which may not have been adequately deduplicated. Surprisingly, the effect of weight decay is disproportionately strong depending on the underlying dataset.\n' +
      '\n' +
      'Figure 5: **Parallelizing the attention and MLP blocks allows us to remove one sync point during tensor parallel training. This was first proposed by Wang and Komatsuzaki (2021) for GPT-J.**\n' +
      '\n' +
      '### Further experimentation required: ideas that did not make the cut\n' +
      '\n' +
      'In this section, we briefly mention some practices and ideas we experimented with, but for which we were unable to reproduce results or obtain a meaningful outcome. We note that this is not be viewed as an indictment of these practices: many have been adopted in popular models successfully.\n' +
      '\n' +
      '**Alternative training objectives.** The largest of language models have been typically trained with a causal decoder-only architecture and objective (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022). Wang et al. (2022) found that such models exhibit better zero-shot abilities than masked encoder-decoders such as T5 (Raffel et al., 2019); however, they also found that after multitask finetuning (Sanh et al., 2021), masked encoder-decoder performed better, highlighting that different regimes and use cases may favor different architectures and objective. Throughout, a non-causal decoder-only (so-called prefix language model) performed competitively as a close second. With UL2, Tay et al. (2022) found that these paradigms could be unified, by training on a mixture of objectives instead. We experimented with UL2, but were unable to obtain an uplift in zero-shot performance, even after adapting tasks to the various paradigms when relevant. Due to time and ressource constraints, we did not end-up pushing our experiments further, as Tay et al. (2022) showed that a posteriori adaptation to the UL2 objective was not only possible but efficient.\n' +
      '\n' +
      'For code models, so-called fill-in-the-middle (FIM) training (Bavarian et al., 2022) has been popular, as it addresses a common use cases for such models. FIM is claimed to come at little to no expense of autoregressive modeling capabilities; we were broadly able to confirm these results, showcasing likely degradation only for a handful of tasks for intermediary infilling rates (0.25-0.5). Low and high infilling rates had the lowest effect on zero-shot performance. Nevertheless, due to lack of wide adoption at the time, we choose to skip FIM, and to instead consider it as an adaptation step for a Falcon-Coder model-this was concurrently demonstrated for Code-LLaMA (Roziere et al., 2023).\n' +
      '\n' +
      '**Principled hyperparameters.** We experimented with \\(\\mu\\)-parametrization (Yang et al., 2022) as a way to scale our hyperparameters in a principled way from smaller to larger runs. Unfortunately, we were unable to demonstrate an improvement over our naive strategy of hyperparameters estimation.\n' +
      '\n' +
      '**Alternative optimizers.** Encouraged by its strong reported results on BERT and T5, we experimented with Amos (Tian and Parikh, 2022), an alternative to Adam with adaptive learning rate and weight decay. We were unable to obtain an improvement for causal decoder-only models, even at small scale.\n' +
      '\n' +
      '**Conversation mining.** In developing RefinedWeb, we experimented with the idea of mining specific types of data from the web. Training dedicated classifiers based on BERT models often resulted in over-fitting, so we instead used simple heuristics (e.g., identifying arguments by the density of transition words, conversations by finding turns of users). We were able to obtain significant zero-shot performance uplift, however this data was very scarce. Since training of the models was started before we had processed all of CommonCrawl, we could not effectively surface all of that data and use it in priority over standard web data, leading us to leave that idea for future models.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline \\multicolumn{2}{c}{**Learning rate**} & \\multicolumn{2}{c}{**End LR warm-up [0.5GT]**} & \\multicolumn{2}{c}{**End of run [27GT]**} & \\multicolumn{1}{c}{**Run stability**} \\\\ Factor & LR & Improv. \\(\\downarrow\\) & Rank & Improv. \\(\\downarrow\\) & Rank & \\\\ \\hline x1 & \\(2\\times 10^{-4}\\) & \\(\\pm 0.0\\)\\% & (4) & \\(\\pm 0.0\\)\\% & (4) & No spikes \\\\ x2 & \\(4\\times 10^{-4}\\) & \\(-2.0\\)\\% & (2) & \\(-1.7\\)\\% & (3) & No spikes \\\\ x5 & \\(1\\times 10^{-3}\\) & \\(\\mathbf{-2.6}\\)\\% & (1) & \\(\\mathbf{-2.5}\\)\\% & (1) & No spikes \\\\ x10 & \\(2\\times 10^{-3}\\) & \\(-1.4\\)\\% & (3) & \\(-1.9\\)\\% & (2) & One small spike \\\\ x20 & \\(4\\times 10^{-3}\\) & \\(+1.3\\)\\% & (5) & \\(+1.9\\)\\% & (5) & Multiple spikes \\\\ x50 & \\(1\\times 10^{-2}\\) & \\(+7.6\\)\\% & (6) & \\(+6.8\\)\\% & (6) & Multiple large spikes \\\\ x100 & \\(2\\times 10^{-2}\\) & \\multicolumn{4}{c}{Diverging after 0.2GT} & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 13: Loss rankings at the end of learning rate warm-up broadly reflects rankings at the end of training, enabling us to search for optimal learning rates efficiently This simple heuristic is easy to use, and consume only a fraction of resources for a larger run.\n' +
      '\n' +
      '### Wrapping-it up: validating overall dataset and architecture recipes\n' +
      '\n' +
      'For convenience, we refer readers to Section 7 for a full overview of the Falcon recipe. We will now validate the performance of the recipe at larger scale, with a longer training run and comparisons with models from the state-of-the-art. We independently verify: (1) the pretraining dataset, in particular its web component; (2) the architecture, by training a model adopting it on The Pile.\n' +
      '\n' +
      '**Dataset validation.** We train 1B and 7B parameters models for 27B tokens and 350B tokens, to reproduce common practices from previous models. We use our baseline architecture, which is based on GPT-3 (Brown et al., 2020) with ALBi (Press et al., 2022). We train on The Pile (Gao et al., 2020), RefinedWeb (our web dataset, Penedo et al. (2023)), and the Falcon data mixture which combines RefinedWeb and curated sources without any upsampling. This last mixture is designed for Falcon-180B, and targets a total of 3,500B tokens. See Section 5.1 for details.\n' +
      '\n' +
      'In Table 14, we find our data mixture significantly uplifts performance. The large majority of these gains are also achieved by RefinedWeb alone, without the curated data. This highlights our previous finding that web data alone, when adequately filtered and deduplicated, can train performant models. We find our 1/7B model compares favorably with other models from the state-of-the-art, however we note our setup put it at a small advantage by training for slightly longer.\n' +
      '\n' +
      '**Architecture validation.** We follow the set-up of our dataset validation, and train the architecture validation 1B models on The Pile for 27B and 350B tokens. Note that we do not include in this experiment hyperparameters tweaks: all models use weight decay, and the same learning rate from Brown et al. (2020)-this experiment concerns only the architecture of the models itself.\n' +
      '\n' +
      'We find our architecture comes with a small performance degradation (Table 14), which we mostly attribute to the reduction in parameters caused by multiquery. We suspect that growing up the multiquery model would likely close that gap. Nevertheless, it\'s interesting to note that while data improvements have very significant effects, architecture improvements are mostly focused on improving training and inference scalability; they do not result in an uplift in task performance.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c} \\hline \\hline\n' +
      '**Scale** & **Dataset** & **Architecture** & \\multicolumn{2}{c}{**Performance**} & & \\\\  & & & zs-main \\(\\uparrow\\) & zs-comp \\(\\uparrow\\) & zs-small \\(\\uparrow\\) & pp1-pile \\(\\downarrow\\) \\\\ \\hline \\multirow{4}{*}{1B@27GT} & The Pile & Baseline & 51.7 & 40.3 & 45.0 & 0.868 \\\\ \\cline{2-7}  & Falcon & Baseline & **53.5** & 42.3 & **48.8** & \\\\  & RefinedWeb & Baseline & 53.2 & **43.4** & 48.4 & \\\\ \\cline{2-7}  & The Pile & Falcon & _51.1_ & _40.0_ & **45.1** & _0.870_ \\\\ \\hline \\hline \\multirow{4}{*}{1B@350GT} & The Pile & Baseline & 57.8 & 47.1 & 54.0 & 0.763 \\\\ \\cline{2-7}  & RefinedWeb & Baseline & **59.8** & **50.1** & **55.7** & \\\\ \\cline{2-7}  & The Pile & Falcon & _56.6_ & _46.1_ & _52.7_ & _0.775_ \\\\ \\hline\n' +
      '1B@300GT & OpenAI & babbage\\({}^{\\dagger}\\) & & 47.8d & & \\\\\n' +
      '1B@380GT & The Pile & GPT-Neo\\({}^{\\dagger}\\) & & 44.3d\\({}^{\\text{a,d}}\\) & & \\\\\n' +
      '1B@300GT & The Pile & BS-A\\&S\\({}^{\\dagger}\\) & & 46.1d\\({}^{\\text{d}}\\) & & \\\\\n' +
      '1B@300GT & The Pile & Pythia\\({}^{\\dagger}\\) & & 45.2d\\({}^{\\text{a,d}}\\) & & \\\\ \\hline \\hline\n' +
      '7B@350GT & RefinedWeb & Baseline & **55.3** & & \\\\\n' +
      '6B@300GT & OpenAI & curie\\({}^{\\dagger}\\) & & 53.7d\\({}^{\\text{d}}\\) & & \\\\\n' +
      '6B@400GT & The Pile & GPT-J\\({}^{\\dagger}\\) & & 53.5d\\({}^{\\text{d}}\\) & & \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 14: Our data recipe, predominantly based on our work on RefinedWeb (Penedo et al., 2023), significantly improves upon The Pile and other models from the state-of-the-art. Because multiquery makes models smaller (and we do not control for that effect), our architecture comes with a small zero-shot performance degradation. We suspect controlling for parameter count would bring our architecture to be on-par or better than the baseline. Nevertheless, improvements to the architecture mostly deliver improvements in hardware scalability for inference and training, while improvements to the data recipe significantly uplift the downstream performance of models. Underline for relevant changes, **bold** for improvement over baseline, _italics_ for degradation over baseline. \\({}^{\\dagger}\\) flags independent evaluations with the EleutherAI Harness (Gao et al., 2021), and \\({}^{\\text{a}}\\) indicates our architecture run is better, while \\({}^{\\text{d}}\\) shows our data run is better.\n' +
      '\n' +
      'Implementation\n' +
      '\n' +
      'Based on our findings from the previous ablations Section 4, and further tests and best practices from the literature, we now describe the codebases and methods used to train the Falcon series of models.\n' +
      '\n' +
      '### The Falcon dataset: predominantly web, with added curated and conversational data\n' +
      '\n' +
      'Based on estimates of our compute budget of 30,000-50,000 PF-days, we target a pretraining dataset size in the range of **3,000-5,000 billion tokens**-we use Hoffmann et al. (2022) as an upper boundary for model size and lower boundary for pretraining length. This is more than 2x the size of the dataset for Chinchilla, and 10x the one for GPT-3; although this range of size is recently becoming more common in concurrent models like LLaMA-2 or OLMo (Touvron et al., 2023; Soldaini et al., 2023). Out of concerns for memorization and degradation caused by repeating data (Carlini et al., 2022; Hernandez et al., 2022) we choose to **not upsample any sources**.\n' +
      '\n' +
      '**High-level overview.** In Section 4.2.1, we have shown that sufficiently filtered and deduplicated web data can deliver performant models: this leads to focus on scaling-up web data to achieve the scale necessary. Because improvements to data quality translate to significant improvements to downstream performance, and because data processing is tremendously cheaper than model training, we do not concern ourselves too much with optimizing for costs with data processing-it is likely that a 90% cheaper recipe with less than a 10% relative performance degradation could be found.\n' +
      '\n' +
      'We still include a small amount of curated data, inspired by The Pile (Gao et al., 2020) with the addition of conversations from Reddit (Baumgartner et al., 2020), as it is unlikely to degrade performance if adequately processed (Section 4.2.2) and as it may broaden the downstream applicability of the model. However, these sources are bound to remain a minority, given that we do not allow any upsampling-they end-up accounting for 13% of our final dataset. Regarding code and multilinguality, we take a conservative approach: based on our results in Section 4.2.3, we include 8% multilingual data and 3% code. These lower fractions than the ones experimented with are due to stock constraint; specifically for code, further improvements to our pipeline enabled us to significantly scale availability, but this was after the models had started training, so we did not revise the mix.\n' +
      '\n' +
      'The final Falcon mixture is presented in Table 15. We designed the mixture based on a 3,500B tokens pretraining dataset, not allowing any upsampling of the curated sources. Despite differing training lengths, the same mixture (in %) is used for Falcon-7B, 40B, and 180B.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c} \\hline \\hline\n' +
      '**Corpora** & & \\multicolumn{2}{c}{**Pretraining**} \\\\ Name & Source & Stock & Fraction & Used \\\\ \\hline\n' +
      '**RefinedWeb-English** & Filtered and deduplicated Common- & \\(\\sim\\)5,000B & 76\\% & 2,700B \\\\  & Crawl, see Penedo et al. (2023) & \\(\\sim\\)2,000B & 8\\% & 400B \\\\\n' +
      '**RefinedWeb-Euro** & Filtered and deduplicated multilingual (Europe-focused) Common- & 215B & 6\\% & 214B \\\\\n' +
      '**Books** & Project Gutenberg & 170B & 5\\% & 168B \\\\\n' +
      '**Conversations** & Reddit, StackOverflow, HackerNews, IRC, YouTube Subtitles & \\(\\sim\\)1,000B & 3\\% & 115B \\\\\n' +
      '**Code** & GitHub & 60B & 2\\% & 57B \\\\\n' +
      '**Technical** & arXiv, PubMed, USPTO, Wikipedia & 100B & 100B & 100B \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 15: The final Falcon mixture is predominantly web-based (nearly 85%), but includes other curated corpora (without any upsampling) to broaden the expressiveness of the model. Individual curated corpora are inspired from The Pile (Gao et al., 2020), but rebuilt from scratch to ensure high-quality and compatibility with our data pipeline. Code stock is a rough estimate based on an updated pipeline; code data used in Falcon was sourced from permissively licensed GitHub repositories. Mixture was designed to avoid upsampling; note that total stocks for RefinedWeb were not known at the beginning of training, as processing was still in-progress. Quantities in tokens.\n' +
      '\n' +
      '#### 5.1.1 The Macrodata Refinement pipeline and the RefinedWeb dataset\n' +
      '\n' +
      'Our web data processing pipeline is extensively described in the dedicated paper RefinedWeb paper (Penedo et al., 2023). In this section, we only highlight key components and decisions.\n' +
      '\n' +
      'To scale-up pretraining data, two approaches are possible:\n' +
      '\n' +
      '* **Repeat data.** This is the easiest option, and was the norm in computer vision originally. However, most large language models have been far more conservative, usually only up-sampling specific corpora for 2-6 times (see Table 3). This is largely due to concerns with memorization (Carlini et al., 2022) and with deduplicates disproportionately degrading the quality of models (Lee et al., 2022; Hernandez et al., 2022). Recently, Muennighoff et al. (2023) has argued that while up to 4 epochs on the same data may be acceptable, further repetition will cause degradation-this leads us to eschew this strategy.\n' +
      '* **Scale-up web data processing.** While scaling curated sources is cumbersome and requires extensive manual work, web data is a massive, plentiful source. Improvements to web data have high leverage, as they impact a large amount of tokens at once: public crawls such as CommonCrawl may contain in excess of 50-100 trillion tokens, such that even a 90% rejection rate would result in a trillion-scale dataset. However, raw web data is also of extremely poor quality (Trinh and Le, 2018; Kreutzer et al., 2022), containing large amounts of undesirable adult content and machine generated spam. We choose to focus our work on improving the quality of web data, through large-scale filtering and deduplication.\n' +
      '\n' +
      'These approaches are orthogonal, but not antagonist; scaling to frontier models, with pretraining datasets of 10-100 trillion tokens, will likely require repeating massive web datasets for a few epochs.\n' +
      '\n' +
      '**Philosophy.** RefinedWeb differentiates itself from previous web datasets in the following ways:\n' +
      '\n' +
      '* **Extreme-scale.** Our Macrodata Refinement pipeline focuses on scalability: we used up to 20,000 CPU cores to produce RefinedWeb. With nearly five trillion deduplicated tokens, the RefinedWeb dataset is the largest documented pretraining dataset, supporting the training of larger models than previously though possible without relying on multiple epochs.\n' +
      '* **Stringent deduplication and filtering.** Inspired by Lee et al. (2022), RefinedWeb is fully deduplicated. Fuzzy deduplication with MinHash is used to first massively shrink the dataset, and then extract substring deduplication is applied. Filtering heuristics are also first used to reduce text extraction artefacts, and to remove machine-generated content. We find in Section 4.2.1 that this allows web data to match curated corpora.\n' +
      '* **Neutral filtering.** With the exception of language identification, the Macrodata Refinement pipeline does not rely on ML-based filtering strategies. Indeed, such filters can easily introduce or amplify biases into the data Dodge et al. (2021); Welbl et al. (2021).\n' +
      '\n' +
      'Figure 6: Subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl. Notably, filtering and deduplication each result in a halving of the data available: around 50% of documents are discarded for not being English, 24% of remaining for being of insufficient quality, and 12% for being duplicates. We report removal rate (grey) with respect to each previous stage, and kept rate (shade) overall. Figure from Penedo et al. (2023).\n' +
      '\n' +
      '**Overview.** The Macrodata Refinement pipeline is split into three subsequent stages (see also Fig. 6 for detailed removal rates): (1) document preparation; (2) filtering; (3) deduplication.\n' +
      '\n' +
      'For the document preparation, before undertaking any compute-heavy processing, we first filter documents based on the URL alone, using a blocklist of adult sites and scoring URLs based on their name. We found that the preprocessed.NET files offered by CommonCrawl still contain undesirable content (e.g., navigation menus) so we instead process raw.WARC files (HTML response) with trafilatura to extract natural text. Finally, we use the fastText classifier from CCNet (Wenzek et al., 2020) to identify the top language of documents. For English, about 48% of documents remain.\n' +
      '\n' +
      'In the filtering stage, we apply a number of heuristics to remove repeated text (which may be an artefact of crawling/text extraction), and documents which are outliers in terms of length, symbol-to-word ratio, etc. These heuristics are inspired by Rae et al. (2021). We also introduce so-called line-wise corrections, which remove lingering artefacts such as likes counter or navigation buttons. The size of the suitable data is against halved, resulting in about 23% of CommonCrawl being kept.\n' +
      '\n' +
      'Finally, we apply large-scale deduplication in two steps: first, we remove approximate duplicates at the document-level with MinHash (Broder, 1997), before removing exact substring matches with a suffix array (Manber and Myers, 1993). With the deduplication settings of Lee et al. (2022), this results in a final halving of the usable data, down to only about 12% of the total data in CommonCrawl.\n' +
      '\n' +
      '#### 5.1.2 The Microdata curated corpora and conversational masking\n' +
      '\n' +
      'In Section 4.2.2, we found that adding curated data from conversations, books, or technical sources did not further improve performance on top of a strong web baseline like RefinedWeb. However, we believe this sort of data, along with code, can broaden the expressiveness of the model, and its applicability to different kinds of downstream tasks not captured by our evaluation setup. Accordingly, we also add a small fraction of curated data, with individual sources inspired from (Gao et al., 2020). We also add data from Reddit (Baumgartner et al., 2020), and introduce a new attention masking strategy for formatting tree-like conversations efficiently. On top of the Microdata curated corpora, we apply the Macrodata Refinement pipeline with adjusted filter settings (e.g., tuning document length thresholds for books) and deduplicate individual corpora.\n' +
      '\n' +
      '**Components from The Pile.** We reuse individual components of The Pile (Gao et al., 2020), except where there are significant quality or licensing concerns. In all cases, we reimplement them from scratch, to ensure the formats match our data pipeline. We introduced a number of special tokens to reproduce structured information in curated corpora and better control generation: *ITITLE*, *ABSTRACT*, *INTRODUCTION*, and *COMMENT*. When IETEX files are available, we convert them to markdown, similar to Lewkowycz et al. (2022). Heuristics from Macrodata Refinement where hand-tuned for each corpora, manually analysing rejected and accepted samples; for books, we also introduce new rules to remove irrelevant content such as indexes, disclaimers, or tables of content.\n' +
      '\n' +
      '**Conversational data.** Increasingly, large language models are deployed in "chatty" use cases, with back and forth interactions between users and models (Adiwardana et al., 2020; Zheng et al., 2023). Altough models are adapted downstream to this use case, we put a focus on enhancing pretraining with conversational data; we notably add data from Reddit (Baumgartner et al., 2020).\n' +
      '\n' +
      '**Conversation trees and attention masking.** One issue with using data from online forums such as Reddit or HackerNews is that this data is formatted in trees, with diverging turns of conversation between users. Past models have sampled trajectories from these trees (Thoppilan et al., 2022; Chowdhery et al., 2022), but this either means that data has to be repeated, or some trajectories left out. Instead, we find that we can use the attention mask to encode a tree-like structure, allowing later comments to only attend to comments from their trajectory, ignoring "side" comments. Conversations are serialized into sequences in depth-first order, and then comments/turns are masked if they are not relevant to the current one (i.e., in a different branch or deeper). For positionnal embeddings, we use the depth of the tree, which naturally serializes the conversation. We illustrate with pseudocode for converting depth first positions to an attention mask in Appendix F.2. We also employ this strategy to mask documents from one another, instead of relying on the <EOD> token alone. Note that cursory experiments did not find a benefit for zero-shot performance.\n' +
      '\n' +
      '### The Falcon architecture and recipe for efficient inference and (stable) training\n' +
      '\n' +
      'Our goals with the Falcon architecture are to maximize training and inference efficiency, while minimizing impact to downstream performance and risks for the models. In Section 4.3, we outlined a number of decisions we made based on the ablations:\n' +
      '\n' +
      '* **Architecture.** We use multigroup attention (Section 4.3.1) to improve the scalability of inference, an extension of multiquery (Shazeer, 2019); we use rotary embeddings (Su et al., 2021); we do not use GLU (Shazeer, 2020) because of the increased memory footprint, and use vanilla GLU instead; we use parallel attention and MLP blocks (Wang and Komatsuzaki, 2021) and remove biases from linear layers (Chowdhery et al., 2022).\n' +
      '* **Hyperparameters.** We use z-loss to help with stability (Shazeer et al., 2018); we use a fixed 0.1 weight decay; we perform a learning rate logarithmic grid search during warm-up and pick the learning rate with the lowest loss at the end of warm-up.\n' +
      '\n' +
      '#### 5.2.1 Architectural nitpicks: separate layer norms, tied embeddings, and scaling-up\n' +
      '\n' +
      '**Layer norms.** When using parallel attention, it is possible to either have separate layer norms for the MLP and the attention block (closer to the vanilla Transformer architecture), or to use a unified layer norm for both. Since the gradient computation with regards to the input of the layer norm is linear, it is possible to maintain the favourable communication volume of parallel attention and MLP while using two layer norms. Furthermore, after training, it is possible to merge the two layer norms back into one, by multiplying in the weights and biases of the layer norm into the subsequent linear layer. This leads us to stay close to the vanilla architecture, and to use two separate layer norms; however, we note this introduces unnecessary additional complexity for downstream conversion to other popular formats. For Falcon-7B (later trained), we switched to a single layer norm.\n' +
      '\n' +
      '**Tied embeddings.** Tying embeddings is a ubiquitous practice for Transformer models: the embedding weights converting the tokens \\(x\\) into \\(z^{0}=xW\\) are the same that converts the embedding back into the predicted logits \\(p=z^{u}W^{T}\\). Although it is still used by most recent LLMs (GPT:3 Brown et al. (2020), PaLM Chowdhery et al. (2022), LLaMA Touvron et al. (2023a)), the original motivations (Press and Wolf, 2016; Inan et al., 2016) may not entirely be relevant. Notably, weight sharing was at the time used to reduce the size of models; but for models with over 100 billion parameters, embeddings are not a significant fraction of the parameters. Furthermore, weight sharing poses challenges in distributed training, as it requires additionnal communications. The semantic argument remains, but cursory experiments showed no strong impact at the 1B parameters scale. Still, in the interest of not adding additional risks to the training of the Falcon series, we keep our embeddings tied.\n' +
      '\n' +
      'Figure 7: Tree-like attention masking enables us to realize all conversation trajectories without sampling or repeating data. The tree is serialized depth-first, where the position of the token is the depth in the tree. This allows us to train on all conversations in the tree at the same time, without repeating the early turns of the conversation. For instance, the passage _Yes! It’s lovely_, in pink, has visibility on _Do you like Paris?_ in green and causal attention on itself, but cannot see other passages.\n' +
      '\n' +
      '**Vocabulary size.** The size of the vocabulary in language models can differ widely from character level models with 256 entries (Xue et al., 2022) to massively multilingual models with millions (Liang et al., 2023). For generative models, practices have collapsed around two modes: vocabularies in the 30-60k range for monolingual models (Brown et al., 2020; Rae et al., 2021; Touvron et al., 2023a), or in the +100k range for models more inclined towards multilinguality (Scao et al., 2022; Chowdhery et al., 2022). Although larger vocabularies may have better fertility, and hence yield faster inference per byte of text, they also come with some caveats: from a scalability perspective, they can lead to unbalanced pipeline stages, and may require more storage space; also, it is unclear whether models optimally use them (Lieber et al., 2021). We train our tokenizer on a vocabulary size of 65,024, and store vocabulary information in a 16bit unsigned integer-this leaves about 500 extra values to use for downstream adaptations (e.g., paradigm tokens for UL2 Tay et al. (2022b)).\n' +
      '\n' +
      '**Model scaling.** We outline the shape and hyperparameters of the Falcon models in Table 16. When increasing compute budget, resources can be either spent towards a larger model (increased parameter count) or towards longer training (increased token count). Hoffmann et al. (2022) recommends a joint (i.e., equal) increase for optimal scaling. However, this finding should be nuanced in two ways: (1) increasing parameter count also increases downstream inference costs, which can be significant if a model is widely deployed; (2) on the other hand, if data-constrained, increasing the size may be a way to trade compute for increased downstream performance. For Falcon, we choose to use Hoffmann et al. (2022) as a lower bound for pretraining length and as an upper bound for model size.\n' +
      '\n' +
      'Beyond model size, there is also the question of how to shape these parameters: e.g., should they be allocated to make a deeper or shallower model, to widen attention heads or to increase their count. Scao et al. (2022b) conducted a short review, highlighting that model depth is typically only scaled logarithmically with total parameter count (Levine et al., 2020). Practices around attention head size however varied. We broadly follow the logarithmic depth scaling recommendation, and uses a fixed attention head size of 64 to optimize for performance with FlashAttention (Dao et al., 2022). We also fix our number of queries in multigroup to be equal to the number of tensor parallel degrees used during training, and train for a fixed sequence length of 2,048. We note that this context length can be efficiently increased with an a posteriori adaptation (Chen et al., 2023).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & **Falcon-7B** & **Falcon-40B** & **Falcon-180B** \\\\ \\hline\n' +
      '**Data** & 1,500B & 1,000B & 3,500B \\\\ \\hline\n' +
      '**Shape** & & & \\\\ \\(n_{\\text{layer}}\\) & 32 & 60 & 80 \\\\ \\(d_{\\text{model}}\\) & 4,544 & 8,192 & 14,848 \\\\ \\(d_{\\text{head}}\\) & & 64 & \\\\ \\(n_{\\text{q}}\\) & 71 & 128 & 232 \\\\ \\(n_{\\text{kv}}\\) & 1 & 8 & 8 \\\\ \\(d_{\\text{vocab}}\\) & & 65,024 & \\\\ \\(n_{\\text{tokens}}\\) & & 2,048 & \\\\ \\hline\n' +
      '**Pretraining** & & & \\\\ Learning rate & \\(6\\times 10^{-4}\\) & \\(1.85\\times 10^{-4}\\) & \\(1.25\\times 10^{-4}\\) \\\\ Decay & & Cosine, divides by 10 & \\\\ Ramp-up & 4B & 4B & 4B \\\\ Batch-size & 2,304 & 1,152 & 2,048 \\\\ Warm-up & 30B & 100B & 100B \\\\ Weight decay & & 0.1 & \\\\ Gradient clipping & 1. & 0.6 & 0.4 \\\\ Z-loss & & \\(1\\times 10^{-4}\\) & \\\\ \\hline\n' +
      '**Parallelism** & & & \\\\ TP & 1 & 8 & 8 \\\\ PP & 2 & 4 & 8 \\\\ DP & 192 & 12 & 64 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 16: **Summary of the shape, hyperparameters, and distribution strategy of the Falcon models.** Falcon-7B was trained after Falcon-40/180B, with an experimental increased batch size.\n' +
      '\n' +
      '#### 5.2.2 Large language model alchemy: hyperparameters for pretraining\n' +
      '\n' +
      'We report hyperparameters used during pretraining in Table 16, and highlight some decisions below.\n' +
      '\n' +
      '**Learning rate search.** The procedure we describe in Section 4.3.5 results in a learning rate of \\(6\\times 10^{-4}\\), \\(1.85\\times 10^{-4}\\), and \\(1.15\\times 10^{-4}\\) for Falcon-7, 40, and 180B respectively. This is significantly higher than learning rates reported by previous models, but we found our training runs to be (mostly) stable. In hindsight we believe our search procedure may result in higher learning rates than optimal-an acceptable tradeoff, since recovering from spikes is relatively easy (Section 5.4).\n' +
      '\n' +
      '**Learning rate ramp-up.** We perform a long ramp-up over 4 billion tokens for all models.\n' +
      '\n' +
      '**Batch size warm-up.** Practices around batch-size warm-up have surprisingly diverged widely depending on the underlying hardware: models trained on GPUs, such as GPT-3 (Brown et al., 2020), BLOOM (Scao et al., 2022), or MT-NLG (Smith et al., 2022), have typically performed a fine-grained warm-up for 10-20 billion tokens; meanwhile, models trained on TPU, such as Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), or PaLM (Chowdhery et al., 2022) often double the batch size mid-training or take larger batch size steps at 25/50% through training. Some recent models have also elected to skip batch size warm-up entirely (Zhang et al., 2022; Touvron et al., 2023, 20). At small scale, we found longer warm-ups to never hurt downstream performance, and in fact to generally deliver better models; this leads us to adopt a long warm-up strategy, over 100 billion tokens-note that we are able to scale the data parallelism degree and overall size of the cluster during training, which means this comes at limited throughput cost. For Falcon-7B, wall-clock time constraints lead us to opt for an accelerated schedule over only 30 billion tokens.\n' +
      '\n' +
      '**Gradient clipping.** We set the treshold for gradient clipping to 0.6 for Falcon-40B. For Falcon-180B, we initially started with 0.6 but later reduced it to 0.4 to improve training stability. In both cases, we tuned the gradient clipping threshold to only affects outlier events and not the vast majority of the training steps. For Falcon-7B, we set it at 1. We find that training at this scale is anyway not really prone to instabilities, even with the large batch size we adopted.\n' +
      '\n' +
      '**Optimizer.** We use AdamW (Loshchilov and Hutter, 2017): it is the most commonly used optimizer, and has been proven time and time again to perform well, both in general and for large language model training in particular. To increase performance during training we use the fused optimizer kernel from Megatron Shoeybi et al. (2019). However, we note that fused kernels for optimizers are less important in large scale training, especially when when optimizer sharding is used.\n' +
      '\n' +
      '**No dropout.** As large language models are typically trained for a single epoch of relatively unique data, they typically do not use dropout (Srivastava et al., 2014). Shortage of data and recent papers suggesting a few epochs may be tolerable (Xue et al., 2023) will surely challenge this practice as future model require tens of trillions of tokens, but for the Falcon series we did not use dropout.\n' +
      '\n' +
      '### Large-scale distributed training on cloud infrastructure with Gigatron\n' +
      '\n' +
      'Rather than training on expensive dedicated HPC resources, we elected to train the Falcon series on cloud infrastructure to improve cost-efficiency. The Falcon series was trained on clusters of p4d on AWS-with up to 4,096 A100s for Falcon-180B. Key metrics for our training infrastructure include:\n' +
      '\n' +
      '* **Nodes with \\(8\\times\\mathbf{A100}\\) 40GB.** We found that configurations with 4x A100 40/80GB, popular in some datacenters with power or granularity constraints, resulted in lower throughputs because of the reduction in available degrees of tensor parallelism (4 instead of 8). However, we found the 40GB version of the A100 to offer increased availability and cost-efficiency.\n' +
      '* **50Gbps interconnect per GPU.** State-of-the-art infrastructure will come with 200Gbps interconnect per A100, powered by low-latency InfiniBand; however, these configurations can be prohibitively expensive. We found that for models up to size of Falcon-180B, bandwidth had only a limited impact on overall throughput (mostly linked to the size of the all_reduce across data parallel degrees). Conversely, the higher latency of EFA on p4d was the main bottleneck for pipeline communications, especially for small scale models.\n' +
      '* **No distributed filesystem.** We stream data directly from S3, instead of relying on a dedicated filesystem. Distributed filesystems are expensive and difficult to maintain, and the small data I/O volumes incurred by large language model training do not justify their use.\n' +
      '\n' +
      'Our infrastructure accordingly exists as an in-between between a true HPC system and a flexible cloud environment-notably, we found that the GPUs still had to share a single spine in the datacenter, as multispine configurations were unreliable. Although it is more cost-efficient, it also requires us to be mindful of its limitations. We found that the popular (and simple) recipe of training with fully sharded data parallelism (Rajbhandari et al., 2020) did not scale well to this infrastructure. Instead, we required the finer control of 3D parallelism (Narayanan et al., 2021) to achieve optimal performance.\n' +
      '\n' +
      'Because of limitations in open-source frameworks at the time, we elected to build our own proprietary distributed training framework. Gigatron is based on pytorch, and at its core implements a 3D distributed parallelism strategy (Shoeybi et al., 2019; Narayanan et al., 2021) combined with ZeRO optimizer sharding (Rajbhandari et al., 2020) to reduce memory consumption and improve scalability.\n' +
      '\n' +
      '#### 5.3.1 Combining 3D parallelism for fine-grained control, and ZeRO for scalability\n' +
      '\n' +
      '**Data Parallelism (DP).** Data parallelism (Fig. 8) is by far the most commonly used form of parallelism. All machine learning frameworks now provide ways to easily parallelize model training across data samples: pytorch with Distributed Data Parallel (DDP), jax with pmap, and tensorflow with MirroredStrategy (Paszke et al., 2017; Bradbury et al., 2021; Abadi et al., 2015). Data parallelism is appealing thanks to its simplicity: the distributed machinery can be hidden away inside the framework easily, without any interactions with the user-defined architecture. In its simplest implementation, data parallelism only requires two modifications to the training procedure: (1) each device needs to operate on unique data samples; (2) the gradients needs to be reduced across devices to keep the weights in sync. We note that this all_reduce will grow with the batch size, eventually making data parallelism bandwidth-bound. Furthermore, data parallelism is however no cure-all: since the model is not sharded, memory footprint per device is constant, even as we add more devices. Accordingly, data parallelism alone is constrained to models which fit on a single device.\n' +
      '\n' +
      '**Tensor Parallelism (TP).** To share the model across devices, we need to turn to model parallelism. Introduced by Shoeybi et al. (2019) in its most popular form, tensor parallelism splits linear layers in the attention block and MLP in a principled way to reduce communication volume. This can be viewed as an instance of width-wise model parallelism. Specifically, for the simple case of a two-layer MLP, by making the first layer column parallel and the second row parallel, only a single all reduce is necessary to produce the final result-no communication of the intermediary result is required. To propagate the gradient backward, the matrices are transposed, and hence the row parallel layer turn into a column parallel layer and vice-versa: this maintains the the efficient communication pattern. We illustrate column and row parallelism in Fig. 9. A similar approach can be taken to split attention blocks, splitting the heads across GPUs-wherein the K,Q,V calculations are column parallel and the final projection row parallel, with all computations in between independent of one another between GPUs. Accordingly, entire blocks in Transformers can be efficiently parallelized this way. Tensor parallel, however, requires both high-bandwith and low-latency interconnect to be effective: accordingly, on current GPU infrastructure, it is constrained within a single node, and cannot efficiently be used across nodes. Models will thus typically be trained with a tensor parallel degree up to 8; this may still be insufficient to create small enough shards of the model.\n' +
      '\n' +
      'Figure 8: **Data parallelism creates model replicas on each device and process different samples in parallel. Model replicas are placed on different devices and compute gradients on different data samples in parallel. The gradients are then reduced before the optimization step is carried out.**\n' +
      '\n' +
      '**Pipeline parallelism.** Model parallelism can also be performed depth-wise, by grouping layers into subsequent stages to be executed on different accelerators. However, naive pipeline parallelism would either be inefficient or result in an immediate roadblock: indeed, stages have to split batches to concurrently process multiple forwards and backwards. We adopt the PipeDream-Flush schedule of Narayanan et al. (2021), now commonly referred to as 1F1B. We found more involved schedules, such as Interleaved-1F1B (Narayanan et al., 2021), to be beneficial when the number of microbatches per model replica is small, typically below 64. After the batch size rampup is completed, we train with a larger number of microbatches; as such we do not observe any speedup from interleaving. In order to reduce the communication volumes, we consistently make use of so called scatter-gather optimizations, where the activations sent to the next stage are first sharded over the tensor parallel degree and then gathered again once received on the next stage (see Fig. 10 for an illustration). This optimizes for underlying interconnect topology (intranode communications over NVLink where the gatheris executed are significantly faster), even more so if internode links have been rail optimized. This optimization reduces communication volume for pipeline parallelism by a factor of the tensor parallel world size, which in our case is 8.\n' +
      '\n' +
      '**Optimizer sharding.** Model weights and gradients are materialized in a contiguous bfloat16 memory buffer. Similar to (Rae et al., 2021), the optimizer maintains a fp32 version of those weights/gradients with which to calculate updates. When using the Adam optimizer in this setting, as we also need to keep track of exponential averages, we require 20 bytes per model parameters:\n' +
      '\n' +
      '\\[\\underbrace{2_{\\text{model\\,param}}+2_{\\text{model\\,grad}}}_{\\text{bf float16}}+ \\underbrace{4_{\\text{opt\\,param}}+4_{\\text{opt\\,grad}}+4_{\\text{exp\\,avg}}+4_ {\\text{exp\\,avg}}}_{\\text{float32}}=20\\text{ bytes/param} \\tag{1}\\]\n' +
      '\n' +
      'A full state of Falcon-7/40/180B will occupy 140GB, 800GB, and 3,600GB of memory per replica, requiring at least 4, 20, or 90 A100 40GB per replica just in weights+gradients+states of memory.\n' +
      '\n' +
      'Figure 9: By **alternating column and row parallelism, no communications are required between two subsequent matrix multiplications, enabling tensor parallelism to efficiently split attention and MLP blocks across GPUs. For column parallel matrix multiplication, the input is replicated (all_reduce in the backward) across all GPUs, which can then independently perform their operation. The result is already split across GPUs to execute the next matrix multiplication in a row parallel fashion. Finally, the output of each GPU is summed through an all_reduce. Intermediary results between the two matrix multiplications are never communicated.**The optimizer state alone accounts for 80% of the memory footprint, and eventually bottlenecks our memory use, preventing us from, for instance, increasing the batch size to better saturate GPUs instead. Accordingly, we choose to shard the optimizer state across data parallel degrees-a practice commonly referred to as ZeRO-1 in the literature (Rajbhandari et al., 2020), and illustrated in Fig. 11. Rather than storing a full redundant copy of the optimizer on each data parallel degree, the optimizer is independently sharded DP times. With optimizer state sharding the number of bytes per parameter is further reduced with increased data parallelism, improving scalability:\n' +
      '\n' +
      '\\[\\text{Bytes per parameter}=4+\\frac{16}{\\overline{\\text{DP}}} \\tag{2}\\]\n' +
      '\n' +
      'For Falcon-7/40/180B, we now only require 30, 215, and 765GB of memory, or 1, 6, 20 A100 40GB to hold a model replica. Note that these do not account for activation memory. Now that the optimizer is split, we however have to face increased communication burden during the backward. First, a reduce_scatter is applied on the bfloat16 model gradients: for each optimizer shard, this derives the relevant gradient shard reduced across all DP degrees. The resulting synchronized gradient chunk is then copied to the fp32 optimizer gradient buffer for use by the optimizer. The optimizer step is performed, resulting in an updated copy of the relevant fp32 sharded optimizer weights. Finally, an all_gather is used to consolidate the bfloat16 model weights for all workers from the aforementioned fp32 sharded optimizer weights. Pseudo-code for this ZeRO-1 workflow is provided in Appendix F.3. Interestingly, in terms of total communication volume, this workflow is **equivalent** to the all_reduce usually used in the data parallel setting Rajbhandari et al. (2020).\n' +
      '\n' +
      'We found that across all scales (from 1 billion parameters to 180 billion parameters), optimizer sharding had no performance overhead compared to traditional data parallelism. In fact, since optimizer sharding frees-up memory, it enables us to use a larger microbatch size, hence resulting in better resource saturation and higher throughput systematically.\n' +
      '\n' +
      '**No sequence parallelism.** Li et al. (2021) proposed a novel strategy to reduce the activation memory during training: it\'s possible to shard the activations on the residual stream across the tensor parallel workers where these activations are otherwise replicated. This sharding is relatively cheap: instead of using an all_reduce after the MLP one can replace it with a reduce_scatter followed by an all_gather right before the next decoder block. However, after implementing the measures to save memory discussed in 5.3.3, we observe that sequence parallelism is not necessary. We also saw a slight decrease in throughput which made us decide against employing it during the final training.\n' +
      '\n' +
      'Figure 10: **Instead of each rank redundantly sending the full tensor, scatter/gather optimization has each rank sends a shard of the tensor, and then leverages the fast intranode interconnect to gather it back. This patterns is more efficient as internode communications are typically much slower than intranode ones; note that the scatter is effectively a no-op, as each rank already has the full tensor. Internode rail optimization typically enables GPUs of the same rank in different nodes to communicate peer-to-peer with minimal intermediaries, further accelerating this pattern.**\n' +
      '\n' +
      '#### 5.3.2 State-of-the-art throughput with dedicated Triton kernels\n' +
      '\n' +
      '**Flash Attention.** Memory efficient attention alternatives have long garnered the attention of the community-however, these came with significant changes to the attention scheme, often resulting in degraded downstream performance, especially when scaling-up. It\'s only (relatively) recently that exact algorithms have been developed to compute attention without requiring materialization of the full attention matrix in memory (Rabe and Staats, 2021). Dao et al. (2022) subsequently showed that unified kernels leveraging the same techniques can significantly speed-up model training. We use a custom implementation in Triton (Tillet et al., 2019), which incorporates some of the improvements concurrently developed for Dao (2023). We note that the use of FlashAttention is the main driver of improved throughput during training. Notably, the memory savings allow us to not rely on activation checkpointing during training, instead focusing our FLOPS on contributing to model training directly. Dedicated FlashAttention kernels are also faster in absolute terms than their naive counterparts. Interestingly, we note that FlashAttention is significantly more numerically accurate than traditional attention when both are performed in bfloat16 and compared to fp32 as the ground truth.\n' +
      '\n' +
      '**Other Triton kernels.** We also employ specialised kernels for the rotary positional embeddings as well as the layer norms in Triton. For rotary embeddings we see a particularly large improvement compared to their unfused PyTorch counterpart. We note that since the transformation is the same for all heads at the same position it\'s possible to reuse the expensive trigonometric functions, additionally simplifying its use as nothing needs to be cached in RAM, as is common in other implementations.\n' +
      '\n' +
      '#### 5.3.3 Efficient memory use via selective recomputation implemented as a monolayer\n' +
      '\n' +
      'As we target A100 40GB for cost-efficiency, memory footprint is a significant concern for us. Both ZeRO and FlashAttention already significantly improve memory availability for us, but we free up additionnal memory via selective recomputation. Li et al. (2021), on top of sequence parallelism, proposed to recompute some activations rather than storing their output in memory for backpropagation-as activations are typically cheap to evaluate. We take this idea a step further, and recompute not only all the activation functions but also the layer norms. We save to memory the statistics of the layer norm only, which makes recomputation trivial. Overall, this implementation of selective recomputation reduces the memory consumption of the decoder block by a factor 2x, while causing no degradation in throughput from the additional computations.\n' +
      '\n' +
      'Due to limitations in the autograd engine of PyTorch, it\'s rather difficult to only recompute e.g. the layer norms without also recomputing the subsequent linear layer. In order to achieve this, we create a single custom autograd function for the entire decoder block-we dub this idea the _monolayer_.\n' +
      '\n' +
      'Figure 11: **Optimizer sharding splits the large optimizer state across data parallel degrees, reducing memory footprint and improving scalability. The freed memory can be traded for an increased microbatch size, improving throughput. Figure inspired from Rajbhandari et al. (2020).**\n' +
      '\n' +
      '#### 5.3.4 Numerical precision: all you need is bfloat16?\n' +
      '\n' +
      'Brown et al. (2020); Zhang et al. (2022); Scao et al. (2022) all reported stability issues training models in the hundred billion parameters range when relying on fp16; we instead adopt bfloat16, which results in stable training more or less out-of-the-box. Rae et al. (2021) reported significant improvements from using stochastic rounding when quantizing the float32 parameters to bfloat16 after the optimization step. In our case however, where the entire optimizer state is stored in float32, we do not observe a similar improvement. Instead, we observe that stochastic rounding helps during the initial steps, before the optimizer state has equilibriated to the training dynamics, but that training without stochastic rounding approaches the same training trajectory shortly thereafter.\n' +
      '\n' +
      '#### 5.3.5 Quality-of-life features for improved flexibility and reliability\n' +
      '\n' +
      '**Topology-agnostic checkpoints.** Surprisingly, when we developed the Falcon series, most distributed training libraries strongly tied their checkpoint format and distribution topology: a change to the topology would require manually converting the checkpoint to a new format. Since we were planning to grow our clusters during training, we ensure that the model and optimizer checkpoints are readable/writable between any topology configurations-similar to t5x (Roberts et al., 2022).\n' +
      '\n' +
      '**Low-discrepancy data loading.** When aggregating different sources into a single mixed dataset it is common to randomly sample each source according to a target probability/weighing per dataset. However, this only guarantees sampling from the sources with the target probability in expectation. Instead, it is desirable that the effective weight during each subset of the training is constant. We use a predefined sampling pattern between different data-sources with a relative short length of \\(10,000\\) sequences which is guaranteed to contain the exact weights for each data source.\n' +
      '\n' +
      '**Topology discovery and optimization.** In order to optimize throughput, it\'s important to place ranks with high communication volume on instances close to one another. At variance with traditional HPC platforms, AWS does not expose topology information for the instances allocated to a training. Accordingly, to optimize placement, we first discover the topology by measuring the bandwidth between all pairs of nodes, and we subsequently optimize rank placement against the measured topology. Regarding the topology discovery phase, a naive solution would be for the first node to first check the bandwidth to all other nodes, then the second node and so on. That would require \\(\\mathcal{O}(n^{2})\\) sequential measurements. Instead, it is possible to achieve this in only \\(\\mathcal{O}(n)\\) steps, by doing many comparisons in parallel-see pseudocode provided in F.1. Once the topology has been (efficiently) discovered, we leverage Gromov-Wasserstein optimal transport (Memoli, 2011) for placement. We measure the source distance matrix based on the measured bandwidths, and define the target distance matrix from the distances in the pipeline/data parallel grid. We then use the Python Optimal Transport Toolbox (Flamary et al., 2021) to solve the resulting problem efficiently.\n' +
      '\n' +
      '### Run management: keeping large-scale infrastructure running\n' +
      '\n' +
      '**Hardware failures.** When scaling-up to hundreds of nodes, hardware failures become increasingly common-for Falcon-180B, every day on 4,096 A100 is equivalent to over 11 years of cumulative use! Furthermore, in our cloud setup, whenever the run is restarted, we sample a new anonymised selection of nodes-we are unable to keep a history of which nodes from previous allocations were suspect. Accordingly, it is critical to be able to rapidly identify and exclude faulty nodes. We found a large majority of hardware failures to be linked to faulty A100s, specifically to corrupted memory rows. These failures do not always manifest with an Xid code, and require manual tests to catch them-they typically result in computations returning NaNs. On start-up, we run a series of large matrix multiplications to catch these failures; during training, we also keep track of NaNs in communications to rapidly identify culprit nodes. Additionally, we run some simple communication tests on start-up, to ensure that the communication primitives work as expected.\n' +
      '\n' +
      '**Monitoring.** We find that many web-based monitoring tools sample metrics, even when no smoothing is applied: this can hide critical events like spikes. Accordingly, we deploy our own local viewers.\n' +
      '\n' +
      '**Spikes.** We encountered few spikes during training. Similar to Chowdhery et al. (2022), when a spike occurred, we resumed from the latest pre-spike checkpoint and skip 1 billion tokens. Nine spikes were encountered during the training of both the 40B model and the 180B model.\n' +
      '\n' +
      'Results\n' +
      '\n' +
      '**Background.** The natural language processing community has developed a plethora of benchmarks to assess the capabilities of models: from tasks inspired by reading comprehension tests (e.g., RACE Lai et al. (2017)), to world-knowledge evaluations (e.g., OpenBookQA Mihaylov et al. (2018)), or so-called commonsense tasks (e.g., HellaSwag (Zellers et al., 2019)). Historical benchmarks like GLUE and SuperGLUE also aggregate many linguistically-motivated tasks, measuring abilities such as word disambiguation or recognizing entailment (Wang et al., 2018, 2019). However, as large language models have gained in capabilities and broadened in their applications, the landscape of evaluations has shifted away from this last genre of tasks; instead, recent benchmarks such as MMLU (Hendrycks et al., 2020) or BigBench (Srivastava et al., 2023) attempt to capture generic knowledge and abilities, rather than linguistic behavior-perhaps in an attempt to stay closer to common use cases of large language models. Code evaluations have also grown increasingly common (Chen et al., 2021), along with mathematics tasks (Cobbe et al., 2021). Recently, technical reports on latest models have also included numerous academic and professionnal exams (OpenAI, 2023a; Anil et al., 2023).\n' +
      '\n' +
      'Along with this shift in subjects, practices around _how_ models are evaluated have also changed. For zero/few-shot evaluations, the canonical setup was popularized by Brown et al. (2020): as most classic NLP tasks have a limited number of choices, these can be evaluated by calculating the log-probabilities of the choices given the question, instead of generating freeform answers. This puts strict guardrails on the model, simplifying implementation and reproduction of evaluations as they do not depend on autoregressive inference with sampling in this framing. However, this inflexibility is not always desired: for instance, chain-of-thought prompting (Wei et al., 2022b) let the model write down intermediate steps of its reasoning before giving a final answer. This setup may also not best illustrate how models are used downstream; common freeform tasks, such as summarization, require autoregressive generation. However, for these tasks, a separate challenge of evaluating the model\'s answer rapidly arise. For instance, Stiennon et al. (2020) found that ROUGE scores for model-generated summaries are not necessarily aligned with human preferences.\n' +
      '\n' +
      'Typical large language models will be deployed as a chatbot/virtual assistant. This is an extremely challenging use case to quantify: it requires appropriate style and chatiness, wide world knowledge, and often reasoning/code abilities. Such deployments should also ideally be harmless and honest on top of being helpful (Bai et al., 2022a), further complexifying evaluation and tradeoffs. Notably, classical NLP recipes translate poorly into better chatbots: we illustrate this in Fig. 12, where we evaluate variants of Falcon-40B on the popular SuperGLUE benchmark (Wang et al., 2019), and on a set of 250 prompts with completions rated by human annotators. The popular FLAN recipe (Longpre et al., 2023) results in the model with the best zero-shot performance on SuperGLUE; however, such a model is also the one least preferred by human annotators. Instead, models trained on so-called self-instruct (Wang et al., 2022b; Taori et al., 2023) datasets improve SuperGLUE performance in a smaller way, but results in significant improvements to annotator preference-unfortunately, human ratings are more expensive to collect than evaluating on SuperGLUE!\n' +
      '\n' +
      'Figure 12: **Human ratings can be at odds with NLP task performance. Variants of Falcon–40B finetuned on different datasets, horizontal lines for baselines from the OpenAI API and from human annotators. Star ratings collected blind, from a pool of 15 annotators across 250 unique prompts.**\n' +
      '\n' +
      'As a middle ground between expensive human preference data and cheap NLP evaluations, it has been recently proposed to use a strong external model (e.g., GPT-4) as a judge (Chiang and Lee, 2023; Chiang et al., 2023; Zheng et al., 2023)-broadly inspired by RLAIF, which seeks to substitute human annotators in RLHF with models themselves (Bai et al., 2022; Dubois et al., 2023). However, it is unclear yet how reliable these practices are (Wang et al., 2023)-they are also predominantly focused on evaluating models finetuned for downstream usecases, not pretrained models.\n' +
      '\n' +
      'Finally, fair comparisons across models are difficult. First, task selections diverge widely: the PaLM papers (Chowdhery et al., 2022; Anil et al., 2023) typically reproduce the setup of Brown et al. (2020); recent technical reports (OpenAI, 2023a; Inflection, 2023) report arbitrary selection of tasks in varying settings; and state-of-the-art models like Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), or LLaMA (Touvron et al., 2023a, b) have all elected to report varying selections of NLP tasks. Notably, there is a lack of standardization across these setups; while standardized benchmarks exist, such as the Eleuther AI Evaluation Harness (Gao et al., 2021), or HELM (Liang et al., 2022), they have only seen limited adoption among open-access models-and most papers do not report details of their evaluation setup, including prompts used. Differing practices on data formatting or tokenization may result in widely different task scores (Fourrier et al., 2023); in a way, one-size-fits-all evaluation does not quite exist yet. We further discuss this issue in Section 6.1 with concrete examples.\n' +
      '\n' +
      '**Our evaluation setup.** Since this paper focuses on the pretrained models in the Falcon series, we choose to center our evaluations on the more classic logprobs-based setup of Brown et al. (2020). This simplifies comparisons, although fairness concerns remain across models. To address these, we split our evaluations across four comparisons: (1) in 1-shot against the PaLM models (Chowdhery et al., 2022; Anil et al., 2023), with the tasks of (Brown et al., 2020); (2) on a small set of few-shot tasks reported by the GPT-4 paper (OpenAI, 2023a); (3) against state-of-the-art models across common sense, question answering, and code tasks; (4) against models which also report results from the EAI Harness (Gao et al., 2021), for which we are able to compare with identical prompts and metrics. We note that this evaluation setup is only intended to provide a broad, first look at the performance of the model; adequate in-depth domain specific evaluations after dedicated specialization of the model would be required to inform about the use of the Falcon series in downstream use cases.\n' +
      '\n' +
      '### To prompt or not to prompt: comparing evaluations across codebases\n' +
      '\n' +
      'When evaluating models in few-shot, two main discrepancies across setups can arise: (1) variations in the style and writing of the prompt; (2) differences in the metrics used to measure accuracy.\n' +
      '\n' +
      '**Prompting.** Specifically regarding (1), not only can small wording tweaks drive changes in performance (use of plurals, yes/no instead of true/false, typos, etc.), but tasks can also be framed in different ways. For instance Rae et al. (2021) found that for multiple-choice question answering tasks, writing down the options in the prompt was beneficial for larger models, but detrimental for smaller models on RACE (Lai et al., 2017)-regardless of whether the model was asked to predict the answer itself or a letter key for it. In the case of RACE, this can account for an absolute change of 10-20%. As most commonly used tasks were created before zero/few-shot evaluations became popular, there is rarely such a thing as a canonical prompt, or even framing, for a task. Furthermore, the vast majority of papers do not report the prompts they used, which hinders reproducibility.\n' +
      '\n' +
      'Figure 13: **In all our evaluations, we rank possible answers by their logprobabilities according to the model given a prompt. In grey in the prompt, we highlight the alternative possibility of outlining each candidate explicitely in the prompt (CH)–in line with Rae et al. (2021), we find this beneficial for larger and more capable models, but detrimental for smaller ones.**\n' +
      '\n' +
      '**Metrics.** On most tasks, accuracy is typically reported; however, practices on the calculation of the logprobabilities used to evaluate candidate answers differ. For tasks with multiple choices, \\(P\\)(candidate\\(|\\)context) is evaluated with each choice used as a candidate successively, and the choice with the highest probability is taken as the model\'s answer. For tasks with long answer (i.e., which don\'t fit in a single token) the logprobabilities of each token in the candidate are summed to estimate the logprobabilities of the overall completion. When comparing choices/completions with widely differing lengths, this can be unfair to the longer ones: they end-up being more unlikely. To compensate for this, the logprobabilities are sometimes normalized by the length of the candidate answer-a first potential divergence. Brown et al. (2020) also found that some answers were heavily skewed by being more likely to follow the final sequence "Answer:" in the prompt. Accordingly, for ARC, OpenBookQA, and RACE they choose to normalize the probabilities by the so-called unconditional probability of each completion: \\(P\\)(candidate\\(|\\)context)\\(/P\\)(candidate\\(|\\)Answer."). Touvron et al. (2023) also adopts this practice for OpenBookQA and BoolQ, but it\'s often unclear whether other papers have adopted it as well, as details on the calculations of logprobabilities are rarely provided.\n' +
      '\n' +
      '**Experiments.** We explore the impact of outlining multiple-choice options in the prompt (see Fig. 13 for an illustration), and of normalizing by the unconditional probability, in zero-shot on ARC and OpenBookQA, for Falcon-7B/40B/180B in Table 17. We find widely different effects for the two practices across both model size and tasks. On ARC-Easy, unconditional normalization (UN) always degrade performance, while it improves it for all models on ARC-Challenge, and improves performance only for Falcon-180B on OpenBookQA. Outlining the choices in the prompt (CH) always degrade performance for Falcon-7B, but always improves it for Falcon-180B; on Falcon-40B, it slightly degrades performance on ARC-Easy, and improves it significantly on ARC-Challenge and OpenBookQA. These findings are in line with the observations of Rae et al. (2021) on RACE, and we find that the effect is indeed very significant (+10-20% absolute gains on Falcon-180B). Combining both practices never improves performance over using one individually.\n' +
      '\n' +
      'For fairness, we seek to match the evaluation setting of other papers and models when comparing to their results-so that using one of the trick above doesn\'t put us at an unfair advantage for example.\n' +
      '\n' +
      '* **Comparisons with PaLM.** Like PaLM (Chowdhery et al., 2022; Anil et al., 2023) we reproduce the evaluation setup of Brown et al. (2020): we do not outline candidates in the prompt for multiple-choice questions (e.g., ARC, RACE), and use unconditionnal normalization for OpenBookQA. When candidates are longer than one token, we normalize logprobabilities by length. We do not significantly tweak prompts, staying close to the ones detailed by Brown et al. (2020). These results are presented in detail in Section 6.2.\n' +
      '* **Comparisons with GPT-3.5/GPT-4.** For MMLU, we use the format proposed by the authors originally (Hendrycks et al., 2020); for ARC, we explicitly outline choices in the prompt, as reported in the GPT-4 paper (OpenAI, 2023). For HellaSwag and Winogrande, we do not change the prompt used by Gao et al. (2021). Results are reported in Section 6.3.\n' +
      '* **State-of-the-art comparisons.** Since these are the widest comparisons, they are likely the ones with the largest variation in practices. We use unconditional normalization on OpenBookQA, and outline answers explicitely on ARC, OpenBookQA, and RACE. We lightly tweak the wording of prompts, but stay close to the ones of Brown et al. (2020), as implemented in Gao et al. (2021). These results are reported in Section 6.4.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c|c c c c|c c c} \\hline \\hline  & \\multicolumn{2}{c}{**ARC-Easy**} & \\multicolumn{6}{c}{**ARC-Challenge**} & \\multicolumn{6}{c}{**OpenBookQA**} \\\\  & & UN & CH & BT & & UN & CH & BT & & UN & CH & BT \\\\ \\hline\n' +
      '**Falcon-7B** & **73,7** & 68,1 & 33,3 & 24,7 & 44,5 & **49,5** & 27,4 & 24,8 & **44,6** & 39,2 & 24,4 & 24,0 \\\\\n' +
      '**Falcon-40B** & **81,2** & 76,6 & 80,5 & 78,8 & 56,7 & 59,0 & **62,1** & 61,3 & 48,0 & 43,2 & **61,2** & 56,4 \\\\\n' +
      '**Falcon-180B** & 84,7 & 79,5 & **94,4** & 84,6 & 63,7 & 63,9 & **83,5** & 81,6 & 47,6 & 48,8 & **76,4** & 71,8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 17: **At different model sizes, different prompt formulations and metrics calculations deliver the best zero-shot performance. Notably, we finding that outlining multiple-choice questions options in the prompt explicitly (CH) nearly always improve performance for larger models, but degrades it for smaller, less capable models. Unconditional normalization (UN) has a smaller effect, with improvements on some tasks but not others. Using both tricks together (BT) never improves performance over using the best one individually. Degradation, improvement over baseline: Gao et al. (2021) default prompt and length-normalized logprob calculation for CH and UN, best of CH/UN for BT.*** **Comparisons with the EleutherAI Evaluation Harness.** We strictly use Gao et al. (2021), and do not make any change to the prompt formulation or logprobability calculations for these. We only compare against other papers that adopt the same practice. This is our fairest set-up, for which direct one-to-one comparisons are possible and for which we didn\'t had to guess the set-up used by other models. These results are reported in Section 6.5.\n' +
      '\n' +
      'Finally, for all set-ups, we provide the prompts used in Appendix G for reproducibility.\n' +
      '\n' +
      '### Comparisons with PaLM on a natural language tasks aggregate\n' +
      '\n' +
      '**Set-up.** We compare with PaLM (Chowdhery et al., 2022) and PaLM-2 (Anil et al., 2023) in the 1-shot NLP task benchmark reported in the PaLM-2 paper. This benchmark broadly reproduces the set of tasks used for GPT-3 (Brown et al., 2020). We note that we are a missing a few of these tasks (e.g., StoryCloze) as we did not implement and validate them in our evaluation framework.\n' +
      '\n' +
      '**Results.** We report detailed results in Table 18. We find that when averaging performance across tasks, Falcon-180B recovers 99.5% of the performance of PaLM-2 Large; significantly above the 94.8% and 94.4% of PaLM-2 Medium and PaLM. Notably, Falcon-180B even outperforms PaLM-2 on some classic benchmarks such as HellaSwag, Winogrande, and PIQA. However, Falcon-180B lags behind on two particular benchmarks: RACE and ANLI. We found RACE to be very sensitive to the formatting of the prompt and samples, which may explain some of the difference; furthermore, we observed that ANLI typically exhibits staggered progression throughout training, rather than a smooth increase (e.g., like HellaSwag)-it\'s possible Falcon-180B is just under the next phase transition that would allow it to catch-up to the range of performance of PaLM-2 Large.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Task** & **Subtask** & **PaLM** & \\multicolumn{2}{c}{**PaLM-2**} & \\multicolumn{2}{c}{**Falcon**} \\\\  & & **S** & **M** & **L** & **180B** \\\\ \\hline WebQuestions (EM) & 22,6 & 21,8 & 26,9 & 28,2 & **31,9** \\\\ \\hline HellaSwag & 83,6 & 82,0 & 84,0 & 86,8 & **87,5** \\\\ LAMBADA & 81,8 & 80,7 & 83,7 & **86,9** & 84,4 \\\\ \\hline WSC & 86,3 & 84,6 & **88,1** & 86,9 & 87,5 \\\\ Winogrande & & 83,7 & 77,9 & 79,2 & 83,0 & **85,1** \\\\ \\hline RACE & Hard & 52,1 & 53,3 & 57,2 & **62,3** & 56,7 \\\\ \\hline PIQA & & 83,9 & 82,2 & 83,2 & 85,0 & **86,1** \\\\ ARC & Challenge & 60,1 & 59,6 & 64,9 & **69,2** & 67,8 \\\\  & Easy & 85,0 & 85,6 & 88,0 & **89,7** & 88,8 \\\\  & Overall & 72,6 & 72,6 & 76,5 & **79,5** & 78,3 \\\\ OpenBookQA & & 53,6 & 57,4 & 56,2 & 58,5 & **64,2** \\\\ \\hline BoolQ & & 88,7 & 88,1 & 88,6 & **90,9** & 89,0 \\\\ CB & & 83,9 & 82,1 & 80,4 & 87,5 & **89,3** \\\\ COPA & & 91,0 & 89,0 & 90,0 & **96,0** & **96,0** \\\\ RTE & & 78,7 & 78,7 & **81,8** & 79,3 & 80,1 \\\\ WiC & & 63,2 & 50,6 & 52,0 & **66,8** & 66,1 \\\\ ReCORD & & 92,8 & 92,1 & 92,4 & **93,8** & 93,2 \\\\ \\hline ANLI & R1 & 52,6 & 53,1 & 58,1 & **73,1** & 60,5 \\\\  & R2 & 48,7 & 48,8 & 49,5 & **63,4** & 55,5 \\\\  & R3 & 52,3 & 53,2 & 54,5 & **67,1** & 56,8 \\\\ \\hline \\hline Task average & & 73,1 & 71,6 & 73,4 & **77,5** & 77,1 \\\\ Fraction of PaLM-2 L & & 94,4 & 92,4 & 94,8 & **99,5** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 18: **Falcon-180B matches the performance of PaLM-2 Large on most tasks, with the notable exception of ANLI and RACE for which it performs closer to PaLM-2 Medium. We found these tasks to be very sensitive to the prompt format used, in particular RACE, and suspect that further improvements could be unlocked with additional tweaking–however, for fairness, we stay close to the prompts proposed by Brown et al. (2020). Overall, we find that Falcon-180B recovers 99.5% of the performance of PaLM-2 Large, significantly above the 94.8% of PaLM-2 Medium.**Overall, these are strong scores, exhibiting the robustness of the Falcon recipe. Beside dataset composition and architectural tweaks, we note two differences between PaLM-2 Large and Falcon-180B: (1) PaLM-2 L was supposedly trained with a larger compute budget, with twice the parameters and a similar amount of tokens as Falcon-180B (see Appendix E); (2) PaLM-2 models used a mixture of objectives (Tay et al., 2022), which has been reported to enhance downstream task performance. Further a posteriori adaptation of Falcon-180B (e.g., with the PaLM-U recipe (Tay et al., 2022)) could help recover more of the performance of PaLM-2 L with Falcon-180B as a base model.\n' +
      '\n' +
      '### Comparisons with GPT-3.5 and GPT-4 on a limited set of tasks\n' +
      '\n' +
      '**Set-up.** We compare with GPT-3.5 and the GPT-4 as reported in GPT-4 paper (OpenAI, 2023), focusing on natural language tasks: MMLU, HellaSwag, Winogrande, and ARC. We use the same number of shots as proposed, except for ARC, for which we report 2-shot instead of 25-shot accuracy.\n' +
      '\n' +
      '**Results.** Results are in Table 19. We find that Falcon-180B systematically performs above GPT-3.5, but below GPT-4 on all tasks. Notably, Falcon-180B achieves strong performance on commonsense tasks: on HellaSwag, it is close to midway between GPT-3.5 and GPT-4, while on Winogrande, it nearly matches the performance of GPT-4. We find the performance of Falcon-180B on multiple choice question answering tasks to be closer to GPT-3.5, albeit always higher. We note that GPT-4 was allegedly trained with 4-5x more pretraining compute than Falcon-180B (see Appendix E), which likely contributes to most of the difference in performance between the two models.\n' +
      '\n' +
      '### State-of-the-art comparisons on common sense, question answering, and code tasks\n' +
      '\n' +
      '**Set-up.** In this section, we compare the Falcon series with other models on commonsense, question answering, and code tasks. We compare with models that have successively defined the state-of-the-art "before" PaLM-2 Large and GPT-4: GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), MT-NLG (Smith et al., 2022), PaLM (Chowdhery et al., 2022), LLaMA-2 (Touvron et al., 2023), and Inflection-1 (Inflection, 2023). For commonsense tasks, we include PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2019), BoolQ (Clark et al., 2019), and LAMBADA (Paperno et al., 2016)-only for BoolQ does our prompt differ slightly from the default one of the EleutherAI Evaluation Harness (Gao et al., 2021). For question answering datasets, we include ARC (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018), and MMLU (Hendrycks et al., 2020). To enable fair comparisons, we evaluate ARC without outlining choices (see Section 6.1). For OpenBookQA, we outline choices, as we find performance to be flat otherwise; finally, for MMLU, we use the canonical setup from Hendrycks et al. (2020). For all these tasks we take results for other models from the papers outlined above. For code, we reproduce the set-up of the BigCode Models Leaderboard for HumanEval in Python, and include further code-specialized models such as Codex (Chen et al., 2021), StarCoder (Li et al., 2023), and Code LLaMA (Roziere et al., 2023). As we focus on comparisons with the Falcon models immediately after pretraining, we do not consider instruct variants of the models above.\n' +
      '\n' +
      '**Commonsense.** We report results in Table 20. With the exception of BoolQ, Falcon-180B improves significantly over state-of-the-art models across all tasks. Broadly speaking, we find that Falcon-40B is slightly under LLaMA-2 34B, which we attribute to smaller pretraining compute: 2,800PF-days for Falcon-40B against 4,700PF-days for LLaMA-2 34B (nearly 70% more). Falcon-7B also slightly underperforms LLaMA-2 7B; this time the difference in compute is smaller (730PF-days against\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & **GPT-3.5** & **GPT-4** & **Falcon-180B** \\\\ \\hline\n' +
      '**HellaSwag** (10-shot) & 85.5 & **95.3** & 89.0 \\\\\n' +
      '**Winogrande** (5-shot) & 81.6 & **87.5** & 87.1 \\\\\n' +
      '**ARC Challenge** (25-shot) & 85.2 & **96.3** & 87.8\\({}^{*}\\) \\\\\n' +
      '**MMLU** (5-shot) & 70.0 & **86.5** & 70.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 19: **Falcon-180B delivers downstream performance between GPT-3.5 and GPT-4. Falcon-180B performs well on commonsense tasks (HellaSwag and Winogrande), where it is well ahead of GPT-3.5. For multiple choice question answering (ARC and MMLU), Falcon-180B performs above GPT-3.5 but not as significantly so.\\({}^{*}\\) we report 2-shot, not 25-shot performance on ARC Challenge.**970PF-days, 30% more), but we suspect that multiquery with a single head of dimension 64 is a very aggressive configuration for Falcon-7B. We find that LLaMA-2 and Inflection-1 achieve relatively similar performance, with Inflection-1 perhaps ahead thanks to exceptional performance on BoolQ.\n' +
      '\n' +
      '**Question answering.** In Table 21, we find again that Falcon-180B strongly outperforms other models from the state-of-art on question answering tasks. We do note that the Falcon series seems to underperform slightly (comparatively to other tasks) on MMLU: we believe this may be attributed to the large prevalence of web data in our pretraining dataset, compared to more technical sources which may be more immediately relevant to the style and content of questions found in MMLU.\n' +
      '\n' +
      '**Code.** We report results on HumanEval in Table 22. We find that Falcon-180B performs best amongst models focusing on natural language, with performance only matched by Inflection-1. In fact, despite being trained on only 3% code, Falcon-180B nearly matches the performance of both PaLM-Coder and PaLM-2 S\\({}^{*}\\), two models which have undergone dedicated code specialization following their pretraining. This is an encouraging result for the development of a Falcon-Coder specialization.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & **APQA** & **HellaSwag** & **Winogrande** & **BoolQ** & **LAMBADA** \\\\  & & & _(10-shot)_ & _(5-shot)_ & & \\\\ \\hline\n' +
      '**GPT-3** & & 81,0 & 78,9 & 70,2 & 60,5 & 76,2 \\\\\n' +
      '**Gopher** & & 81,8 & 79,2 & 70,1 & 79,4 & 74,5 \\\\\n' +
      '**Chinchilla** & & 81,8 & 80,8 & 74,9 & 83,7 & 77,4 \\\\\n' +
      '**MT-NLG** & & 82,0 & 80,2 & 73 & 78,2 & 76,6 \\\\\n' +
      '**PaLM** & & 82,3 & 83,4 & 81,1 & 88,0 & 77,9 \\\\\n' +
      '**LLaMA-2** & 7B & 78,8 & 77,2 & 78,6 & 69,2 & 77,4 \\\\  & 13B & 80,5 & 80,7 & 82,1 & 72,8 & 81,7 \\\\  & 34B & 81,9 & 83,3 & 76,7 & 83,7 & \\\\  & 70B & 82,8 & 85,3 & 87,3 & 80,2 & 85,0 \\\\\n' +
      '**Inflection-1** & & 84,2 & 84,3 & 85,8 & & 83,3 & **89,7** & 78,5 \\\\ \\hline\n' +
      '**Falcon** & 7B & 80,3 & 76,3 & 78,1 & 67,2 & 72,6 & 73,8 & 74,9 \\\\  & 40B & 83,0 & 82,7 & 85,3 & 76,0 & 81,8 & 81,9 & 77,3 \\\\  & 180B & **84,9** & **85,9** & **89,0** & **80,3** & **87,1** & 87,8 & **79,8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 20: **Outside of PaLM-2 Large and GPT-4, Falcon-180B significantly improves other state-of-the-art models such as LLaMA-2 or Inflection-1 on commonsense tasks. Falcon-40B performs slightly under LLaMA-2 34B, because of a significantly smaller compute budget (2,800PF-days against 4,700PF-days, 70% more for LLaMA-2). We note the exceptional performance of Inflection-1 on BoolQ; conversely, we note that, despite our best prompt engineering efforts, we were unable to reproduce the performance reported by the LLaMA-2 paper on BoolQ for the Falcon series: the results we report can likely be improved. Bold for best, underline for second-best.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & & **ARC-Challenge** & **ARC-Easy** & **OpenBookQA** & **MMLU** \\\\ \\hline\n' +
      '**GPT-3** & & 51,4 & 68,8 & 57,6 & \\\\\n' +
      '**PaLM** & & 53,0 & 76,6 & 53,4 & 69,3 \\\\\n' +
      '**LLaMA-2** & 7B & 45,9 & 75,2 & 58,6 & 45,3 \\\\  & 13B & 49,4 & 77,3 & 57,0 & 54,8 \\\\  & 34B & 54,5 & 79,4 & 58,2 & 62,6 \\\\  & 70B & 57,4 & 80,2 & 60,2 & 68,9 \\\\ \\hline\n' +
      '**Falcon** & 7B & 44,5 & 73,6 & 44,6\\({}^{*}\\) & 28,0 \\\\  & 40B & 56,7 & 81,2 & 61,2 & 57,0 \\\\  & 180B & **63,7** & **84,7** & **76,4** & **70,6** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 21: **Falcon-180B improves significantly over GPT-3, PaLM, and LLaMA-2 on question answering datasets, while Falcon-40B performs in-line with LLaMA-2 34B. \\({}^{*}\\): for Falcon-7B on OpenBookQA, we report accuracy without outlining candidates, as performance is otherwise close to random (see Table 17 for details). Bold for best, underline for second-best.**\n' +
      '\n' +
      '### Comparison with other models using the EleutherAI Evaluation Harness\n' +
      '\n' +
      '**Set-up.** For this final comparison, we only consider models which have had reports of evaluations performed with the EleutherAI Evaluation Harness (Gao et al., 2021). These results are the most directly comparable, as the preprocessing of samples, the prompt, and the calculations of the metrics are identical. We report results for GPT-3 evaluated through the API by the BigScience group, FairSeq (Artetxe et al., 2021), GPT-Neo-1.3B (Black et al., 2021), GPT-J (Wang and Komatsuzaki, 2021), GPT-NeoX-20B (Black et al., 2022), OPT (Zhang et al., 2022), Pythia (Biderman et al., 2023), CerebrasGPT (Dey et al., 2023), Aleph Alpha (Aleph Alpha, 2023), and BLOOM (Scao et al., 2022a). We average performance on HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), Winogrande (Sakaguchi et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and OpenBookQA (Milaylov et al., 2018)-this was the widest set of tasks we could assemble based on different reporting practices across papers. We note that these models mostly span a smaller compute range (up to a few thousands PF-days at most), and with performance starkly below other state-of-the-art models on with which we have compared before. We report results both for the Falcon series presented in this paper, and for the smaller-scale Falcon-RefinedWeb models trained in Penedo et al. (2023), which used only the RefinedWeb dataset for performance validation.\n' +
      '\n' +
      '**Results.** We present results in Fig. 14. We find that, across scales, the Falcon series significantly improves against other models in this set of comparisons. Notably, Falcon-40B outperforms GPT-3 175B, despite being trained with a smaller compute budget. In fact, even Falcon-7B approaches the performance of GPT-3 175B: we believe that with longer training and a less aggressive multiquery setup, it should be possible to match the performance of the original GPT-3 model with 7B parameters (or less). Finally, we also note that the smaller validation models trained on RefinedWeb alone also compare favorably, reproducing the performance of the GPT-3 models of the same size.\n' +
      '\n' +
      'Taking a step back, it\'s interesting to note some broader trends in this plot. Most older series of models achieve very similar performance, with the GPT-3 series as a roofline. The OPT models, despite underperforming at the smaller sizes, eventually match the performance of GPT-3 175B. Two outsiders stand out: GPT-Neo-1.3B, the first open-source large language model, likely because of issues in its pretraining, and the BLOOM series, likely because of its heavily multilingual pretraining data and conservative use of an additionnal layer norm after the embeddings (Scao et al., 2022b).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c} \\hline \\hline  & \\multicolumn{3}{c}{**Specialized for code?**} & **HumanEval** \\\\ \\hline\n' +
      '**PaLM** & & & 26,2 \\\\\n' +
      '**LLaMA-2** & 7B & & 12,2 \\\\  & 13B & & 20,1 \\\\  & 34B & & 22,6 \\\\  & 70B & & 30,5 \\\\\n' +
      '**Inflection-1** & & & **35,4** \\\\\n' +
      '**Falcon** & 180B & & **35,4** \\\\ \\hline\n' +
      '**Codex** & \\begin{tabular}{l} cushman-001 \\\\ davinci-002 \\\\ \\end{tabular} & \\begin{tabular}{l} ✓ \\\\ \\end{tabular} & \n' +
      '\\begin{tabular}{l} 33,5 \\\\ 45,9 \\\\ 30,4 \\\\ \\end{tabular} \\\\\n' +
      '**StarCoder** & & ✓ & 30,4 \\\\\n' +
      '**PaLM-Coder** & & ✓ & 35,9 \\\\\n' +
      '**PaLM-2 S\\({}^{*}\\)** & & ✓ & 37,6 \\\\\n' +
      '**GPT-3.5** & & ✓\\({}^{\\dagger}\\) & 48,1 \\\\\n' +
      '**GPT-4** & & ✓\\({}^{\\dagger}\\) & **67.0** \\\\\n' +
      '**Code LLaMA** & 7B & ✓ & 33,5 \\\\  & 13B & ✓ & 36,0 \\\\  & 34B & ✓ & 48,8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 22: **Falcon-180B outperforms all other predominantly English language models on HumanEval.** Morever, despite being trained on only 3% code, it nearly matches the performance of PaLM-Coder and PaLM-2 S\\({}^{*}\\), both having undergone code specialization. \\({}^{\\dagger}\\): the GPT-3.5 and GPT-4 models are somewhat unique in being pretrained on a large fraction of code on top of natural language (OpenAI, 2023b). **Bold** for best, underline for second-best (per specialization), pass@1.\n' +
      '\n' +
      '## 7 Limitations\n' +
      '\n' +
      'We highlight in this section limitations of our findings and of the Falcon series of models itself, both to nuance potential applications of our models, and to highlight further research directions. Broadly speaking, these limitations stem from two factors: (1) most of the research on the Falcon series was conducted prior to December 2022, when model training started-in the meanwhile significant developments have occurred in the world of large language models; (2) constraints in compute resources preventing a more exhaustive exploration of some directions.\n' +
      '\n' +
      '### Limitations of our findings and ablations\n' +
      '\n' +
      '**Scale.** Our ablations are conducted with 1B and 3B models trained to optimality; although we validated the final architecture and datasets with 1B and 7B models trained for 350B tokens, this remains order of magnitude below the compute budget of the actual models in the Falcon series. On the model side, as scale increases, other works have noted the emergence of outlier features, which may drive important dynamics which are not captured by our set-up (Dettmers et al., 2022). On the data side, increasingly large models are known to become increasingly sensitive to memorization (Carlini et al., 2022), potentially leading to catastrophic quality degradation (Hernandez et al., 2022).\n' +
      '\n' +
      '**Benchmarks.** In our ablations, we focused on measuring zero-shot performance on a limited set of tasks, using only logprobs-based evaluations. Not only can downstream task performance be at odds with actual human preference (see Fig. 12 for our own experience of that issue), but our set of tasks does not capture code or multilingual performance. Evaluating general purpose language models is difficult, and single aggregated metrics often poorly translate the nuances brought forth by modeling or data interventions. Broadly speaking, on our set of tasks, we never experienced a consistent trend with one intervention systematically improving a specific metrics but not the others (e.g., adding technical data did not consistently uplift performance on SciQ or PubMedQA); however, this is likely to be a limitation of our small-scale setup, which end-up predominantly measuring general gains. It is likely the performance on popular benchmarks such as MMLU (Hendrycks et al., 2020) or AGileval (Zhong et al., 2023) could be improved with dedicated in-domain data. We notably believe in the value of larger-scale ablations on optimal pretraining data composition, and on better understanding quantitative versus qualitative aspects of deduplication (i.e., deduplication on web data may be unreasonably effective because it happens to filter out some of the worst samples).\n' +
      '\n' +
      'Overall, we view Section 4 as a set of cursory experiments that helped us ground and broadly validate some decisions for the Falcon series. Future practitioners will likely want to revisit these experiments and results at increased scale, enabling broader benchmarking to better capture performance.\n' +
      '\n' +
      'Figure 14: **The Falcon series strongly improves at all scales against other models which have reported results with the EleutherAI Evaluation Harness. Models trained on RefinedWeb alone also reproduce the performance of the GPT-3 series, despite not using curated data. Aggregated zero-shot accuracy on HellaSwag, LAMBADA, Winogrande, PIQA, ARC, and OpenBookQA.**\n' +
      '\n' +
      '### Limitations of the Falcon models\n' +
      '\n' +
      '**Decoupling training and inference compute.** In the past year, there has been a dramatic explosion in the adoption of large language models, leading to significantly increased downstream use. Under that paradigm, inference costs can become predominant, shadowing pretraining costs; furthermore, smaller open-source models are easier for the community to build upon, enabling local deployments and even finetuning. When scaling-up pretraining, additional compute may be either spent towards a larger model, or towards longer training. While increasing parameter count caries over to inference, increasing costs over the entire lifecycle of the model, this is not the case for longer pretraining, offering an axis for decoupling training and inference compute. Recent open-source models, such as the LLaMA series (Touvron et al., 2023, 20), have adopted this practice by eventually training their 7-70B parameters models for a fixed 2,000B tokens. While our 7B follows a similar idea, our 40B and 180B models are trained closer to the pretraining optimality suggested by Hoffmann et al. (2022). This makes deployment of models in the Falcon series more challenging. At the time, this decision was partially motivated by constraints over data availability: we elected early on to strictly stick to a single epoch of training, and the total stock of RefinedWeb tokens was unknown to us when we started training (data processing jobs only concluded in early January). Recent works have however suggested that up to 4 epochs may lead to minimal degradation in performance, at least for models in the 10B parameters range (Xue et al., 2023; Muennighoff et al., 2023). Based on the final size of our available data (around 5,000B tokens of English and 1,000B tokens of code) we would recommend training future models on at least 4,000-5,000B tokens, and skewing towards 10,000-15,000B tokens for the larger ones. Note that this does pose some challenges during pretraining, as larger models are easier to distribute efficiently. We also see promise in enabling training and inference compute decoupling through architecture interventions such as layerwise mixture of experts (MoE), also known as routed language models (Clark et al., 2022; Fedus et al., 2022). In MoE models, parameters are only sparsely activated, allowing for another 8-16 reduction in inference costs.\n' +
      '\n' +
      '**Code performance and pretraining data.** As we did not allow upsampling for pretraining subsets, we predominantly trained Falcon on web data from RefinedWeb. We also were conservative with the fraction of code in the pretraining, whereas available data could have allowed us to reach 10-30% of code data in pretraining. For future models, we would recommend making code data significantly more prevalent, and potentially upsampling sources illustrative of common usecases or data domains of interest for downstream use cases. Code data is particularly promising, because it is widely available from a single source (i.e., GitHub) and can undergo large-scale processing similar to web data-deduplication is also extremely effective for code data (Kocetkov et al., 2022; Allal et al., 2023; Li et al., 2023). Furthermore, the GPT-3.5 and GPT-4 models have been seemingly trained on a mix of both natural language and code data (OpenAI, 2023, 20). Such hybrid language/code models are also likely to be easier to adapt for downstream use cases as chatbots which may make extensive use of code to answer user questions, or to interface with a variety of tools. We also see potential promise in the use of synthetic data (Gunasekar et al., 2023; Li et al., 2023); however, many of these methods are currently closer to distillation than true pretraining, and it is somewhat unclear how they may be used to bootstrap a new class of larger models beyond downstream adaptation.\n' +
      '\n' +
      '**Longer sequence lengths.** All Falcon models are pretrained with a 2,048 sequence length, which can be limiting for multi-turn chat or code use cases. Thanks to the use of rotary embeddings (Su et al., 2021), a posteriori adaptation to longer sequences is possible: kaiokenmdenv (2023); Chen et al. (2023) concurrently found that extending the context length with rotary positional embeddings was possible by interpolation and lightweight finetuning. In our own experiments on long context finetuning, we found long context data to be plentiful in RefinedWeb. Upward of and more than 13% of the tokens come from documents of over 8k tokens, with 1.5% of the tokens in documents of over 32k tokens. This approach was further refined for LLaMA-2 Long (Xiong et al., 2023), resulting in a significant performance boost on tasks with many shots (e.g., MMLU and ARC in Section 6.3).\n' +
      '\n' +
      '**Pretrained only.** Although we make lightly tuned versions of the Falcon models available to demonstrate possibilities as instruct or chat models, our work predominantly concerns the pretrained versions. Prior to deployment of the Falcon models, we strongly recommend for adequate guardrails to be deployed and for bias/harm evaluations specific to the target use case to be systematically undertaken. In particular, we note that reinforcement learning from human feedback may be relevant for not only making the models more helpful, but also more robust to adversarial queries (Ganguli et al., 2022). We also see promise in code adaptation of the Falcon models, similar to PaLM-Coder (Chowdhery et al., 2022) and PaLM-2 S\\({}^{*}\\)(Anil et al., 2023) or Code LLaMA (Roziere et al., 2023).\n' +
      '\n' +
      'Conclusion\n' +
      '\n' +
      'In this paper, we have introduced and extensively described the Falcon series of pretrained models, with Falcon-7/40/180B, trained on up to 3,500B tokens.\n' +
      '\n' +
      'First, we described some of the ablations and experiments we performed to prepare the training dataset and architecture of the Falcon models. We found adequately filtered and deduplicated web data to be a surprisingly strong baseline; concerns around memorization for larger models (Carlini et al., 2022; Hernandez et al., 2022) led us to elect not to upsample any sources, and to predominantly train on this web data. On the architecture-side, we found most interventions to have a limited effect, and adopted multigroup attention (an extension of multiquery (Shazeer, 2019)) as a way to improve inference scalability by significantly reducing the size of the K,V-cache. For future generations of the Falcon series, we see promise in significantly increasing the fraction of code in the pretraining data, and in training with longer sequence lengths in a staged process (e.g., half of the training up to 8k, and second half up to 16k, with downstream adaptation to 32-256k).\n' +
      '\n' +
      'Then, we described our implementation of our final strategy for the pretraining of the Falcon series. We report extensively on our data pipeline in Penedo et al. (2023). We described our approach to conversational masking, and our distributed training strategy for running on cost-efficient cloud infrastructure, relying notably on 3D parallelism combined with ZeRO. We also discussed some of our interventions for fast memory efficient training, such as dedicated FlashAttention (Dao et al., 2022) kernels in Triton (Tillet et al., 2019) and our monolayer strategy. We also discussed some of the details around setting hyper-parameters and managing runs over multiple thousand GPUs.\n' +
      '\n' +
      'Finally, we outlined some of the results obtained by the Falcon series on key benchmarks. We found Falcon-180B to near the performance of PaLM-2 Large (Anil et al., 2023), and to end-up in-between GPT-3.5 and GPT-4 (OpenAI, 2023a). Falcon-180B is the best open-source model currently available, and likely one of the best models overall. We note our evaluation predominantly focuses on classic natural language tasks, and that further work will be required for evaluating human preferences on downstream versions of Falcon having undergone dedicated finetuning or reinforcement learning.\n' +
      '\n' +
      'To foster open research on large language models, and accelerate technological development in this space, we make the following artefacts public available under permissive open-source licenses:\n' +
      '\n' +
      '* **Falcon-7/40/180B.** We make all models in the Falcon series available, with Falcon-7/40B under an Apache 2.0 license and Falcon-180B under a dedicated responsible AI use license. At time of release, Falcon-180B is the most powerful open large language model available.\n' +
      '* **A 600B tokens extract of RefinedWeb.** We make a 600B tokens extract of our web dataset available, for use by researchers to study large-scale filtered and deduplicated web data, and for other practioners to adopt as a standard for high-quality web data. We also open-source 1/7B models trained on 350B tokens from this extract.\n' +
      '* **Detailed research.** With this paper and the RefinedWeb paper (Penedo et al., 2023), we have detailed numerous of our decisions and experiments concerning the Falcon series.\n' +
      '\n' +
      'We believe large language models to be a foundational technology for the future of our civilization, and in turn we believe they should be shared responsibly. Widespread exchange of ideas is a staple of accelerated technological and economical progress in our history; in turn, this acceleration uplifts all. By open-sourcing artificial intelligence research and models, we can foster a broader and more diverse community, and benefit from vibrant collaborative efforts to improve the safety and reliability of large language models. We hope the Falcon series can be a small step towards this vision.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Abadi et al. (2015) Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, G., Irving, M., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org.\n' +
      '* Abadji et al. (2022) Abadji, J., Ortiz Suarez, P., Romary, L., and Sagot, B. (2022). Towards a Cleaner Document-Oriented Multilingual Crawled Corpus. _arXiv e-prints_, page arXiv:2201.06642.\n' +
      '* Adiwardana et al. (2020) Adiwardana, D., Luong, M.-T., So, D. R., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., et al. (2020). Towards a human-like open-domain chatbot. _arXiv preprint arXiv:2001.09977_.\n' +
      '* Aghajanyan et al. (2023) Aghajanyan, A., Yu, L., Conneau, A., Hsu, W.-N., Hambardzumyan, K., Zhang, S., Roller, S., Goyal, N., Levy, O., and Zettlemoyer, L. (2023). Scaling laws for generative mixed-modal language models. _arXiv preprint arXiv:2301.03728_.\n' +
      '* Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. (2023). Gqa: Training generalized multi-query transformer models from multi-head checkpoints. _arXiv preprint arXiv:2305.13245_.\n' +
      '* Aleph Alpha (2023) Aleph Alpha (2023). Luminous: performance benchmarks. _arXiv preprint arXiv:1810.12885_.\n' +
      '* Allal et al. (2023) Allal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., et al. (2023). Santacoder: don\'t reach for the stars! In _Deep Learning for Code (DLAC) Workshop_.\n' +
      '* Anil et al. (2023) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. (2023). Palm 2 technical report. _arXiv preprint arXiv:2305.10403_.\n' +
      '* Artetxe et al. (2021) Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., et al. (2021). Efficient large scale language modeling with mixtures of experts. _arXiv preprint arXiv:2112.10684_.\n' +
      '* Bahdanau et al. (2014) Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. _arXiv preprint arXiv:1409.0473_.\n' +
      '* Bai et al. (2022) Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022a). Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_.\n' +
      '* Bai et al. (2022b) Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kermion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. (2022b). Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_.\n' +
      '* Baumgartner et al. (2020) Baumgartner, J., Zannettou, S., Keegan, B., Squire, M., and Blackburn, J. (2020). The pushshift reddit dataset.\n' +
      '* Bavarian et al. (2022) Bavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey, C., Tworek, J., and Chen, M. (2022). Efficient training of language models to fill in the middle. _arXiv preprint arXiv:2207.14255_.\n' +
      '* Beltagy et al. (2019) Beltagy, I., Lo, K., and Cohan, A. (2019). Scibert: A pretrained language model for scientific text. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3615-3620.\n' +
      '* Biderman et al. (2023) Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O\'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. (2023). Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, pages 2397-2430. PMLR.\n' +
      '* Berman et al. (2020)Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. (2020). Piqa: Reasoning about physical commonsense in natural language. In _Thirty-Fourth AAAI Conference on Artificial Intelligence_.\n' +
      '* Black et al. (2022) Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., et al. (2022). Gpt-neox-20b: An open-source autoregressive language model. In _Proceedings of BigScience Episodet 5-Workshop on Challenges & Perspectives in Creating Large Language Models_, pages 95-136.\n' +
      '* Black et al. (2021) Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. (2021). GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata.\n' +
      '* Bradbury et al. (2021) Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., et al. (2021). Jax: Autograd and xla. _Astrophysics Source Code Library_, pages ascl-2111.\n' +
      '* Broder (1997) Broder, A. Z. (1997). On the resemblance and containment of documents. In _Proceedings. Compression and Complexity of Sequences 1997_, pages 21-29. IEEE.\n' +
      '* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901.\n' +
      '* Carlini et al. (2022) Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. (2022). Quantifying memorization across neural language models. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Chelba et al. (2013) Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. (2013). One billion word benchmark for measuring progress in statistical language modeling. _arXiv preprint arXiv:1312.3005_.\n' +
      '* Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021). Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_.\n' +
      '* Chen et al. (2023) Chen, S., Wong, S., Chen, L., and Tian, Y. (2023). Extending context window of large language models via positional interpolation. _arXiv preprint arXiv:2306.15595_.\n' +
      '* Chiang and Lee (2023) Chiang, C.-H. and Lee, H.-y. (2023). Can large language models be an alternative to human evaluations? _arXiv preprint arXiv:2305.01937_.\n' +
      '* Chiang et al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. (2023). Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\n' +
      '* Chowdhery et al. (2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_.\n' +
      '* Clark et al. (2022) Clark, A., De Las Casas, D., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T., Borgeaud, S., et al. (2022). Unified scaling laws for routed language models. In _International Conference on Machine Learning_, pages 4057-4086. PMLR.\n' +
      '* Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. (2019). Boolq: Exploring the surprising difficulty of natural yes/no questions. In _NAACL_.\n' +
      '* Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. (2018). Think you have solved question answering? try arc, the AI2 reasoning challenge. _CoRR_, abs/1803.05457.\n' +
      '* Clark et al. (2019)Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. (2021). Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_.\n' +
      '* Conneau et al. (2020) Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzman, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). Unsupervised cross-lingual representation learning at scale. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 8440-8451.\n' +
      '* Costa-jussa et al. (2022) Costa-jussa, M. R., Cross, J., Celebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J., Licht, D., Maillard, J., et al. (2022). No language left behind: Scaling human-centered machine translation. _arXiv preprint arXiv:2207.04672_.\n' +
      '* Dao (2023) Dao, T. (2023). Flashattention-2: Faster attention with better parallelism and work partitioning. _arXiv preprint arXiv:2307.08691_.\n' +
      '* Dao et al. (2022) Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. (2022). Flashattention: Fast and memory-efficient exact attention with io-awareness. _Advances in Neural Information Processing Systems_, 35:16344-16359.\n' +
      '* Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. (2022). Llm.int8(): 8-bit matrix multiplication for transformers at scale. _Advances in Neural Information Processing Systems_, 35:30318-30332.\n' +
      '* Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_.\n' +
      '* Dey et al. (2023) Dey, N., Gosal, G., Khachane, H., Marshall, W., Pathria, R., Tom, M., Hestness, J., et al. (2023). Cerebras-gpt: Open compute-optimal language models trained on the cerebral s wafer-scale cluster. _arXiv preprint arXiv:2304.03208_.\n' +
      '* Dinan et al. (2023) Dinan, E., Yaida, S., and Zhang, S. (2023). Effective theory of transformers at initialization. _arXiv preprint arXiv:2304.02034_.\n' +
      '* Dodge et al. (2021) Dodge, J., Sap, M., Marasovic, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., and Gardner, M. (2021). Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 1286-1305.\n' +
      '* Dubois et al. (2023) Dubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Alpacafarm: A simulation framework for methods that learn from human feedback. _arXiv preprint arXiv:2305.14387_.\n' +
      '* Eberhard et al. (2023) Eberhard, D. M., Simons, G. F., and Fennig, C. D. (2023). _Ethnologue: Languages of the World_. SIL International, Dallas, TX, USA, twenty-sixth edition.\n' +
      '* Fedus et al. (2022) Fedus, W., Zoph, B., and Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _The Journal of Machine Learning Research_, 23(1):5232-5270.\n' +
      '* Flamary et al. (2021) Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V., Sutherland, D. J., Tavenard, R., Tong, A., and Vayer, T. (2021). Pot: Python optimal transport. _Journal of Machine Learning Research_, 22(78):1-8.\n' +
      '* Fourrier et al. (2023) Fourrier, C., Habib, N., Launay, J., and Wolf, T. (2023). What\'s going on with the open llm leaderboard? "[https://huggingface.co/blog/evaluating-mmlu-leaderboard](https://huggingface.co/blog/evaluating-mmlu-leaderboard)".\n' +
      '* Fu et al. (2022) Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. (2022). Hungry hungry hippos: Towards language modeling with state space models. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Ganguli et al. (2022) Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. (2022). Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. _arXiv preprint arXiv:2209.07858_.\n' +
      '* Goyal et al. (2020)Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. (2020). The Pile: an 800GB dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_.\n' +
      '* Gao et al. (2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). A framework for few-shot language model evaluation.\n' +
      '* Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012)_, pages 394-398, Montreal, Canada. Association for Computational Linguistics.\n' +
      '* Gunasekar et al. (2023) Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., et al. (2023). Textbooks are all you need. _arXiv preprint arXiv:2306.11644_.\n' +
      '* Hamborg et al. (2017) Hamborg, F., Meuschke, N., Breitinger, C., and Gipp, B. (2017). news-please: A generic news crawler and extractor. In _Proceedings of the 15th International Symposium of Information Science_, pages 218-223.\n' +
      '* Haviv et al. (2022) Haviv, A., Ram, O., Press, O., Izsak, P., and Levy, O. (2022). Transformer language models without positional encodings still learn positional information. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 1382-1390.\n' +
      '* Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2020). Measuring massive multitask language understanding. In _International Conference on Learning Representations_.\n' +
      '* Hernandez et al. (2022) Hernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Henighan, T., Hume, T., et al. (2022). Scaling laws and interpretability of learning from repeated data. _arXiv preprint arXiv:2205.10487_.\n' +
      '* Hestness et al. (2017) Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. (2017). Deep learning scaling is predictable, empirically. _arXiv preprint arXiv:1712.00409_.\n' +
      '* Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_.\n' +
      '* Hooker (2021) Hooker, S. (2021). The hardware lottery. _Communications of the ACM_, 64(12):58-65.\n' +
      '* Howard and Ruder (2018) Howard, J. and Ruder, S. (2018). Universal language model fine-tuning for text classification. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 328-339.\n' +
      '* Inan et al. (2016) Inan, H., Khosravi, K., and Socher, R. (2016). Tying word vectors and word classifiers: A loss framework for language modeling. _arXiv preprint arXiv:1611.01462_.\n' +
      '* Inflection (2023) Inflection 1.\n' +
      '* Johannes Welbl et al. (2017) Johannes Welbl, Nelson F. Liu, M. G. (2017). Crowdsourcing multiple choice science questions.\n' +
      '* Jozefowicz et al. (2016) Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y. (2016). Exploring the limits of language modeling. _arXiv preprint arXiv:1602.02410_.\n' +
      '* kaiokenmdenv (2023) kaiokenmdenv (2023). Extending context is hard... but not impossible. Accessed: 2023-10-02.\n' +
      '* Kaplan et al. (2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_.\n' +
      '* Kaplan et al. (2020)Kenton, J. D. M.-W. C. and Toutanova, L. K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL-HLT_, pages 4171-4186.\n' +
      '* Koectkov et al. (2022) Koectkov, D., Li, R., Jia, L., Mou, C., Jernite, Y., Mitchell, M., Ferrandis, C. M., Hughes, S., Wolf, T., Bahdanau, D., et al. (2022). The stack: 3 tb of permissively licensed source code. _Transactions on Machine Learning Research_.\n' +
      '* Kreutzer et al. (2022) Kreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh, N., Tapo, A. A., Subramani, N., Sokolov, A., Sikasote, C., et al. (2022). Quality at a glance: An audit of web-crawled multilingual datasets. _Transactions of the Association for Computational Linguistics_, 10:50-72.\n' +
      '* Lai et al. (2017) Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. (2017). Race: Large-scale reading comprehension dataset from examinations. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 785-794.\n' +
      '* Laurencon et al. (2022) Laurencon, H., Saulnier, L., Wang, T., Akiki, C., del Moral, A. V., Scao, T. L., Werra, L. V., Mou, C., Ponferrada, E. G., Nguyen, H., Frohberg, J., Sako, M., Lhoest, Q., McMillan-Major, A., Dupont, G., Biderman, S., Rogers, A., allal, L. B., Toni, F. D., Pistilli, G., Nguyen, O., Nikpoor, S., Masoud, M., Colombo, P., de la Rosa, J., Villegas, P., Thrush, T., Longpre, S., Nagel, S., Weber, L., Munoz, M. R., Zhu, J., Strien, D. V., Alyafeai, Z., Almubarak, K., Chien, V. M., Gonzalez-Dios, I., Soroa, A., Lo, K., Dey, M., Suarez, P. O., Gokaslan, A., Bose, S., Adelani, D. I., Phan, L., Tran, H., Yu, I., Pai, S., Chim, J., Lepercq, V., Ilic, S., Mitchell, M., Luccioni, S., and Jernite, Y. (2022). The bigscience ROATS corpus: A 1.6TB composite multilingual dataset. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_.\n' +
      '* Le Scao et al. (2022) Le Scao, T., Wang, T., Hesslow, D., Bekman, S., Bari, M. S., Biderman, S., Elsahar, H., Muennighoff, N., Phang, J., Press, O., et al. (2022). What language model to train if you have one million gpu hours? In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 765-782.\n' +
      '* Lee et al. (2022) Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. (2022). Deduplicating training data makes language models better. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8424-8445.\n' +
      '* Levine et al. (2020) Levine, Y., Wies, N., Sharir, O., Bata, H., and Shashua, A. (2020). Limits to depth efficiencies of self-attention. _Advances in Neural Information Processing Systems_, 33:22640-22651.\n' +
      '* Lewkowycz et al. (2022) Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. (2022). Solving quantitative reasoning problems with language models.\n' +
      '* Li et al. (2023a) Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Koectkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. (2023a). Starcoder: may the source be with you! _arXiv preprint arXiv:2305.06161_.\n' +
      '* Li et al. (2021) Li, S., Xue, F., Li, Y., and You, Y. (2021). Sequence parallelism: Making 4d parallelism possible. _arXiv preprint arXiv:2105.13120_.\n' +
      '* Li et al. (2023b) Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. (2023b). Textbooks are all you need ii: phi-1.5 technical report. _arXiv preprint arXiv:2309.05463_.\n' +
      '* Liang et al. (2023) Liang, D., Gonen, H., Mao, Y., Hou, R., Goyal, N., Ghazvininejad, M., Zettlemoyer, L., and Khabsa, M. (2023). Xlm-v: Overcoming the vocabulary bottleneck in multilingual masked language models. _arXiv preprint arXiv:2301.10472_.\n' +
      '* Liang et al. (2022) Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al. (2022). Holistic evaluation of language models. _arXiv preprint arXiv:2211.09110_.\n' +
      '* Lieber et al. (2021) Lieber, O., Sharir, O., Lenz, B., and Shoham, Y. (2021). Jurassic-1: Technical details and evaluation. Technical report, AI21 Labs.\n' +
      '* Li et al. (2021)Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., Pasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V., O\'Horo, B., Wang, J., Zettlemoyer, L., Kozareva, Z., Diab, M., Stoyanov, V., and Li, X. (2021). Few-shot learning with multilingual language models. _ArXiv_, abs/2112.10668.\n' +
      '* Longpre et al. (2023) Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. (2023). The final collection: Designing data and methods for effective instruction tuning. _arXiv preprint arXiv:2301.13688_.\n' +
      '* Loshchilov and Hutter (2017) Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_.\n' +
      '* Loshchilov and Hutter (2018) Loshchilov, I. and Hutter, F. (2018). Decoupled weight decay regularization. In _International Conference on Learning Representations_.\n' +
      '* Luo et al. (2022) Luo, S., Li, S., Zheng, S., Liu, T.-Y., Wang, L., and He, D. (2022). Your transformer may not be as powerful as you expect. _arXiv preprint arXiv:2205.13401_.\n' +
      '* Luo et al. (2023) Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. (2023). Wizardcoder: Empowering code large language models with evol-instruct. _arXiv preprint arXiv:2306.08568_.\n' +
      '* Madaan et al. (2022) Madaan, A., Zhou, S., Alon, U., Yang, Y., and Neubig, G. (2022). Language models of code are few-shot commonsense learners. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 1384-1403.\n' +
      '* Manber and Myers (1993) Manber, U. and Myers, G. (1993). Suffix arrays: a new method for on-line string searches. _Journal on Computing_, 22(5):935-948.\n' +
      '* Memoli (2011) Memoli, F. (2011). Gromov-wasserstein distances and the metric approach to object matching. _Foundations of computational mathematics_, 11:417-487.\n' +
      '* Mihaylov et al. (2018) Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. (2018). Can a suit of armor conduct electricity? a new dataset for open book question answering. In _EMNLP_.\n' +
      '* Mikolov et al. (2013) Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. _arXiv preprint arXiv:1301.3781_.\n' +
      '* Mitchell et al. (2019) Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D., and Gebru, T. (2019). Model cards for model reporting. In _Proceedings of the conference on fairness, accountability, and transparency_, pages 220-229.\n' +
      '* MosaicML (2023) MosaicML (2023). Introducing mpt-30b: Raising the bar for open-source foundation models. Accessed: 2023-06-22.\n' +
      '* Muennighoff et al. (2023) Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., and Raffel, C. (2023). Scaling data-constrained language models. _arXiv preprint arXiv:2305.16264_.\n' +
      '* Narayanan et al. (2021a) Narayanan, D., Phanishayee, A., Shi, K., Chen, X., and Zaharia, M. (2021a). Memory-efficient pipeline-parallel dnn training. In _International Conference on Machine Learning_, pages 7937-7947. PMLR.\n' +
      '* Narayanan et al. (2021b) Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., et al. (2021b). Efficient large-scale language model training on gpu clusters using megatron-lm. In _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-15.\n' +
      '* OpenAI (2023a) OpenAI (2023a). Gpt-4 technical report. _arXiv_, pages 2303-08774.\n' +
      '* OpenAI (2023b) OpenAI (2023b). Model index for researchers. Accessed: 2023-09-26.\n' +
      '* 16, Mannheim. Leibniz-Institut fur Deutsche Sprache.\n' +
      '* Ortiz Suarez et al. (2019)Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744.\n' +
      '* Paperno et al. (2016) Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N. Q., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. (2016). The LAMBADA dataset: Word prediction requiring a broad discourse context. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1525-1534, Berlin, Germany. Association for Computational Linguistics.\n' +
      '* Paresh (2023) Paresh, D. (2023). Stack overflow will charge ai giants for training data.\n' +
      '* Paszke et al. (2017) Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic differentiation in pytorch. In _NIPS-W_.\n' +
      '* Penedo et al. (2023) Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. (2023). The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. _arXiv preprint arXiv:2306.01116_.\n' +
      '* Pennington et al. (2014) Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation. In _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, pages 1532-1543.\n' +
      '* Peters et al. (2018) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. (2018). Deep contextualized word representations. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 2227-2237, New Orleans, Louisiana. Association for Computational Linguistics.\n' +
      '* Pope et al. (2023) Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and Dean, J. (2023). Efficiently scaling transformer inference. _Proceedings of Machine Learning and Systems_, 5.\n' +
      '* Press et al. (2022) Press, O., Smith, N., and Lewis, M. (2022). Train short, test long: Attention with linear biases enables input length extrapolation. In _International Conference on Learning Representations_.\n' +
      '* Press and Wolf (2016) Press, O. and Wolf, L. (2016). Using the output embedding to improve language models. _arXiv preprint arXiv:1608.05859_.\n' +
      '* Rabe and Staats (2021) Rabe, M. N. and Staats, C. (2021). Self-attention does not need \\(o(n^{2})\\) memory. _arXiv preprint arXiv:2112.05682_.\n' +
      '* Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving language understanding by generative pre-training. _OpenAI Blog_.\n' +
      '* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9.\n' +
      '* Rae et al. (2021) Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. (2021). Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_.\n' +
      '* Rae et al. (2019) Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. (2019). Compressive transformers for long-range sequence modelling.\n' +
      '* Raffel et al. (2019) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer. _CoRR_, abs/1910.10683.\n' +
      '* Rajbhandari et al. (2020) Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. (2020). Zero: Memory optimizations toward training trillion parameter models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16. IEEE.\n' +
      '* Raffel et al. (2019)Roberts, A., Chung, H. W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D., Narang, S., Lester, B., Gaffney, C., Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Salcianu, A., van Zee, M., Austin, J., Goodman, S., Soares, L. B., Hu, H., Tsvyashchenko, S., Chowdhery, A., Bastings, J., Bulian, J., Garcia, X., Ni, J., Chen, A., Kenealy, K., Clark, J. H., Lee, S., Garrette, D., Lee-Thorp, J., Raffel, C., Shazeer, N., Ritter, M., Bosma, M., Passos, A., Maitin-Shepard, J., Fiedel, N., Omernick, M., Seta, B., Sepassi, R., Spiridonov, A., Newman, J., and Gesmundo, A. (2022). Scaling up models and data with t5x and seqio. _arXiv preprint arXiv:2203.17189_.\n' +
      '* Roziere et al. (2023) Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. (2023). Code lama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_.\n' +
      '* Sakaguchi et al. (2019) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. (2019). Winogrande: An adversarial winograd schema challenge at scale. _arXiv preprint arXiv:1907.10641_.\n' +
      '* Sanh et al. (2021) Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L. A., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., BARI, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N. V., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Biderman, S. R., Gao, L., Bers, T. G. O., Wolf, T., and Rush, A. M. (2021). Multitask prompted training enables zero-shot task generalization. _ArXiv_, abs/2110.08207.\n' +
      '* Scao et al. (2022) Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., et al. (2022a). Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_.\n' +
      '* Scao et al. (2022b) Scao, T. L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S., Bari, M. S., Bideman, S., Elsahar, H., Muennighoff, N., Phang, J., et al. (2022b). What language model to train if you have one million gpu hours? _arXiv preprint arXiv:2210.15424_.\n' +
      '* Shaham et al. (2022) Shaham, U., Elbayad, M., Goswami, V., Levy, O., and Bhosale, S. (2022). Causes and cures for interference in multilingual translation. _arXiv preprint arXiv:2212.07530_.\n' +
      '* Shannon (1951) Shannon, C. E. (1951). Prediction and entropy of printed english. _Bell system technical journal_, 30(1):50-64.\n' +
      '* Shaw et al. (2018) Shaw, P., Uszkoreit, J., and Vaswani, A. (2018). Self-attention with relative position representations. _arXiv preprint arXiv:1803.02155_.\n' +
      '* Shazeer (2019) Shazeer, N. (2019). Fast transformer decoding: One write-head is all you need. _arXiv preprint arXiv:1911.02150_.\n' +
      '* Shazeer (2020) Shazeer, N. (2020). Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_.\n' +
      '* Shazeer et al. (2018) Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., Sepassi, R., and Hechtman, B. (2018). Mesh-TensorFlow: Deep learning for supercomputers. In _Neural Information Processing Systems_.\n' +
      '* Shoeybi et al. (2019) Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019). Megatron-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_.\n' +
      '* Smith et al. (2022) Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., et al. (2022). Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. _arXiv preprint arXiv:2201.11990_.\n' +
      '* Soldaini et al. (2023) Soldaini, L., Lo, K., Kinney, R., Naik, A., Ravichander, A., Bhagia, A., Groeneveld, D., Schwenk, D., Magnusson, I., and Chandu, K. (2023). Dolma.\n' +
      '* Srivastava et al. (2023) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. (2023). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _Transactions on Machine Learning Research_.\n' +
      '* Srivastava et al. (2023)Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. _Journal of Machine Learning Research_, 15(56):1929-1958.\n' +
      '* Stiennon et al. (2020) Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. (2020). Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021.\n' +
      '* Su et al. (2021) Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. (2021). Roformer: Enhanced transformer with rotary position embedding. _arXiv preprint arXiv:2104.09864_.\n' +
      '* Sutton (2019) Sutton, R. (2019). The bitter lesson. _Incomplete Ideas (blog)_, 13(1).\n' +
      '* Taori et al. (2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).\n' +
      '* Tay et al. (2021) Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., Narang, S., Yogatama, D., Vaswani, A., and Metzler, D. (2021). Scale efficiently: Insights from pre-training and fine-tuning transformers. _ArXiv_, abs/2109.10686.\n' +
      '* Tay et al. (2022a) Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J., Wang, X., Chung, H. W., Bahri, D., Schuster, T., Zheng, S., et al. (2022a). Ul2: Unifying language learning paradigms. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Tay et al. (2022b) Tay, Y., Wei, J., Chung, H. W., Tran, V. Q., So, D. R., Shakeri, S., Garcia, X., Zheng, H. S., Rao, J., Chowdhery, A., et al. (2022b). Transcending scaling laws with 0.1% extra compute. _arXiv preprint arXiv:2210.11399_.\n' +
      '* Thoppilan et al. (2022) Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. (2022). Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_.\n' +
      '* Tian and Parikh (2022) Tian, R. and Parikh, A. P. (2022). Amos: An adam-style optimizer with adaptive weight decay towards model-oriented scale. _arXiv preprint arXiv:2210.11693_.\n' +
      '* Tiedemann (2016) Tiedemann, J. (2016). Finding alternative translations in a large corpus of movie subtitle. In _Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\'16)_, pages 3518-3522, Portoroz, Slovenia. European Language Resources Association (ELRA).\n' +
      '* Tillet et al. (2019) Tillet, P., Kung, H.-T., and Cox, D. (2019). Triton: an intermediate language and compiler for tiled neural network computations. In _Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages_, pages 10-19.\n' +
      '* Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023a). Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. (2023b). Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Trinh and Le (2018) Trinh, T. H. and Le, Q. V. (2018). A simple method for commonsense reasoning. _arXiv preprint arXiv:1806.02847_.\n' +
      '* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008.\n' +
      '* Villalobos et al. (2022) Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A. (2022). Will we run out of data? an analysis of the limits of scaling datasets in machine learning. _arXiv preprint arXiv:2211.04325_.\n' +
      '* Voss et al. (2018)Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. (2019). Superglue: A stickier benchmark for general-purpose language understanding systems. _Advances in neural information processing systems_, 32.\n' +
      '* Wang et al. (2018) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018). Glue: A multi-task benchmark and analysis platform for natural language understanding. In _International Conference on Learning Representations_.\n' +
      '* Wang and Komatsuzaki (2021) Wang, B. and Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax).\n' +
      '* Wang et al. (2023) Wang, P., Li, L., Chen, L., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., and Sui, Z. (2023). Large language models are not fair evaluators. _arXiv preprint arXiv:2305.17926_.\n' +
      '* Wang et al. (2022a) Wang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung, H. W., Beltagy, I., Launay, J., and Raffel, C. (2022a). What language model architecture and pretraining objective work best for zero-shot generalization? _arXiv preprint arXiv:2204.05832_.\n' +
      '* Wang et al. (2022b) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. (2022b). Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_.\n' +
      '* Wei et al. (2022a) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. (2022a). Emergent abilities of large language models. _Transactions on Machine Learning Research_.\n' +
      '* Wei et al. (2022b) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. (2022b). Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837.\n' +
      '* Welbl et al. (2021) Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S. (2021). Challenges in detoxifying language models. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 2447-2469.\n' +
      '* Wenzek et al. (2020) Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzman, F., Joulin, A., and Grave, E. (2020). Ccnet: Extracting high quality monolingual datasets from web crawl data. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pages 4003-4012.\n' +
      '* Wu et al. (2016) Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016). Google\'s neural machine translation system: Bridging the gap between human and machine translation. _arXiv preprint arXiv:1609.08144_.\n' +
      '* Xiong et al. (2023) Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. (2023). Effective long-context scaling of foundation models.\n' +
      '* Xue et al. (2023) Xue, F., Fu, Y., Zhou, W., Zheng, Z., and You, Y. (2023). To repeat or not to repeat: Insights from scaling llm under token-crisis. _arXiv preprint arXiv:2305.13230_.\n' +
      '* Xue et al. (2022) Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. (2022). Byt5: Towards a token-free future with pre-trained byte-to-byte models. _Transactions of the Association for Computational Linguistics_, 10:291-306.\n' +
      '* Xue et al. (2021) Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. (2021). mt5: A massively multilingual pre-trained text-to-text transformer. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 483-498.\n' +
      '* Yang et al. (2022) Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W., and Gao, J. (2022). Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. _arXiv preprint arXiv:2203.03466_.\n' +
      '* Yang et al. (2021)Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). HellaSwag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4791-4800, Florence, Italy. Association for Computational Linguistics.\n' +
      '* Zeng et al. (2021) Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z., Jiang, X., Yang, Z., Wang, K., Zhang, X., et al. (2021). Pangu-\\(\\alpha\\): Large-scale autoregressive pretrained chinese language models with auto-parallel computation. _arXiv preprint arXiv:2104.12369_.\n' +
      '* Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. (2022). Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_.\n' +
      '* Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_.\n' +
      '* Zhong et al. (2023) Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. (2023). Agieval: A human-centric benchmark for evaluating foundation models. _arXiv preprint arXiv:2304.06364_.\n' +
      '* Zhu et al. (2015) Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S. (2015). Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _Proceedings of the IEEE international conference on computer vision_, pages 19-27.\n' +
      '\n' +
      '## Appendix A Contributions\n' +
      '\n' +
      '* **Engineering & Tooling.*\n' +
      '* **Distributed training codebase.*\n' +
      '* Baptiste Pannier, Daniel Hesslow.\n' +
      '* **Web data.*\n' +
      '* Guilherme Penedo, Ruxandra Cojocaru _(multilingual data)_, Quentin Malatric _(data quality)_, Alessandro Cappelli, Hamza Alobeidli.\n' +
      '* **Curated data.*\n' +
      '* Alessandro Cappelli, Etienne Goffinet _(code data)_, Quentin Malatric, Ruxandra Cojocaru, Abdulaziz Alshamsi _(books data)_.\n' +
      '* Daniel Hesslow, Baptiste Pannier, Guilherme Penedo _(deployment)_.\n' +
      '* Daniele Mazzotta, Etienne Goffinet, Guilherme Penedo.\n' +
      '* **Hardware correctness.*\n' +
      '* Baptiste Pannier, Julien Launay, Daniel Hesslow.\n' +
      '* **Pretraining.** Baptiste Pannier, Julien Launay, Daniel Hesslow.\n' +
      '\n' +
      '* **Data ablations.*\n' +
      '* Julien Launay, Ruxandra Cojocaru _(curriculum learning)_, Alessandro Cappelli, Quentin Malatric _(fine-grained filters)_, Daniel Hesslow.\n' +
      '* **Architecture ablations.*\n' +
      '* Julien Launay, Daniel Hesslow, Etienne Goffinet, Quentin Malatric _(training objectives)_, Baptiste Pannier, Badreddine Noune _(optimizers)_.\n' +
      '* **Model finetuning.*\n' +
      '* Quentin Malatric, Alessandro Cappelli, Etienne Goffinet _(code specialization)_, Guilherme Penedo _(human evaluation)_, Baptiste Pannier _(long-context)_, Julien Launay.\n' +
      '* Julien Launay, Daniel Hesslow, Quentin Malatric.\n' +
      '* **Paper writing.*\n' +
      '* Julien Launay, Daniel Hesslow, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei.\n' +
      '* **Leadership.** Julien Launay, Ebtesam Almazrouei, Merouane Debbah.\n' +
      '\n' +
      '## Appendix B Acknowledgements\n' +
      '\n' +
      'We would like to thank the AWS team, in particular Olivier Cruchant, for their support throughout the project, enabling us to eventually scale to up to 4,096 A100s the training of Falcon-180B. We would also like to thank Axel Marmet, Tri Dao, Dan Fu, Colin Raffel, Katherine Lee, Thomas Wolf, Iz Beltagy, and Dirk Groeneveld for insightful discussions throughout the project.\n' +
      '\n' +
      '## Appendix C Model card\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{p{113.8pt}|p{284.5pt}} \\hline \\multicolumn{2}{c|}{**Model details**} \\\\ \\hline\n' +
      '**Organization** & The models were created by the Technology Innovation Institute. \\\\ \\hline\n' +
      '**Model date** & Training of the Falcon models started in December and completed in the first half of 2023. \\\\ \\hline\n' +
      '**Model type and information about training** & Falcon are autoregressive Transformer models trained with a causal language modeling objective. Architecture based on PaLM Chowdhery et al. (2022), with an extension of multiquery attention for tensor parallelism (multigroup) and minor tweaks (no SwiGLU, etc.). See Section 4 and Section 5 for details. \\\\ \\hline\n' +
      '**Licence** & Falcon-7B and Falcon-40B are made available under the Apache 2.0 license; Falcon-180B is made available under the Falcon-180B TII license, with restrictions related to responsible use. \\\\ \\hline\n' +
      '**Point of contact** & Falconllm@tii.ae \\\\ \\hline\n' +
      '**Intended use** \\\\ \\hline\n' +
      '**Primary intended uses** & Research on large language models; as a foundation for further specialization for specific use cases (e.g., chatbot, etc.) \\\\ \\hline\n' +
      '**Primary intended users** & NLP researchers and engineers. \\\\ \\hline\n' +
      '**Out-of-scope use cases** & Production use without adequate assessment of risks and mitigation; use cases which may be considered irresponsible or harmful. \\\\ \\hline\n' +
      '**Factors** & \\\\ \\hline\n' +
      '**Relevant factors** & The Falcon models are predominantly trained on English data from a large-scale web corpora representative of the web. Accordingly, they will carry the stereotypes and biases commonly encountered online, and are unlikely to generalize appropriately beyond English or European latin languages. \\\\ \\hline\n' +
      '**Evaluation factors** & We evaluated the toxicity of the underlying pretraining dataset and found it to be in line with common curated pretraining datasets such as The Pile, see Penedo et al. (2023). Note that this only accounts for toxicity under the definition of Perspective API: "content that is rude or disrespectful". Notably, this fails to include concerns about social biases or harmfulness. \\\\ \\hline\n' +
      '**Metrics** \\\\ \\hline\n' +
      '**Model performance measures** & We focus our evaluation on the zero-shot generalization capabilities of our models across a wide range of tasks, leveraging the Eleuther AI language model evaluation harness Gao et al. (2021). \\\\ \\hline\n' +
      '**Variation approaches** & Due to the costs associated with training Falcon we cannot train the models multiple times and measure variability across runs. \\\\ \\hline\n' +
      '**Evaluation data** \\\\ \\hline\n' +
      '**Datasets** & We evaluate zero-shot accuracy on 18 natural language tasks and one Python programming task, detailed in Section 6. \\\\ \\hline\n' +
      '**Motivation** & We selected and aggregated tasks to build comparisons with other models in the literature (see Section 6.1). \\\\ \\hline\n' +
      '**Preprocessing** & We mostly use the default setup of Gao et al. (2021), see Appendix G for the custom prompts we used for some evaluations \\\\ \\hline\n' +
      '**Training data** \\\\ \\hline \\multicolumn{2}{c}{**See the dedicated datasheet in Penedo et al. (2023).**} \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 23: **Model card for Falcon, following the framework introduced by Mitchell et al. (2019).**Datasheet\n' +
      '\n' +
      'See the dedicated RefinedWeb paper (Penedo et al., 2023).\n' +
      '\n' +
      '## Appendix E Comparisons with undocumented models\n' +
      '\n' +
      'A number of recent models have elected to release scant promotional technical reports instead of adequately documented research papers; they notably only provide sparse evaluations and modeling details are often absent. This makes comparisons challenging, even more so as many of the details end-up being leaked or known through back-channels-a format which lends itself poorly to citation and attribution. For Anil et al. (2023), parameter count and pretraining length were reported by CNBC3 for the largest of the three models. For OpenAI (2023a), a number of leaks have occurred, most of which have been summarized in a piece by SemiAnalysis4.\n' +
      '\n' +
      'Footnote 3: [https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html](https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html)\n' +
      '\n' +
      'Footnote 4: [https://www.semanalysis.com/p/gpt-4-architecture-infrastructure](https://www.semanalysis.com/p/gpt-4-architecture-infrastructure)\n' +
      '\n' +
      '## Appendix F Pseudocode samples\n' +
      '\n' +
      '### Measurement plan to measure all to all bandwidths/latencies efficiently\n' +
      '\n' +
      '```\n' +
      'defget_all_comps(n:int): #n:poweroftwo defop(l,d=4,r=l): l=1.reshape(-1,d) l[1::2]=np.roll(l[1::2],r,axis=1) returnl.T.reshape(-1) x=np.array(list(range(n))) comps=[] d=1 whiled<n: forinrange(d): comps.append(op(x,d=d,r=r).copy()) d=2 ret=np.stack(comps) returnret.reshape(ret.shape[0],-1,2)\n' +
      '```\n' +
      '\n' +
      '### Converting tree token depth into an attention mask:\n' +
      '\n' +
      '```\n' +
      'defattn_mask_pos_(x,attention_mask): foriinrange(len(x)): attn=False forjinrange(0,len(x)): attn|=(i+1)==j attn&=x[i]<x[j] attention_mask[j,i]=-attn&(i!=j)\n' +
      '```\n' +
      '\n' +
      '### Zero-1 pseudo-code\n' +
      '\n' +
      '```\n' +
      'defreduce_scatter_grads(buffer): chunk_size=buffer.model_grads.numel()//dp_world_size t=empty_tensor(chunk_size,dtype=bfloat16) reduce_scatter_tensor(t,buffer.model_grad,group=data_parallel_group ) buffer.optimizer_grads.copy(t)\n' +
      '```\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:54]\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Context** & Complete the \n' +
      '\\begin{tabular}{} \\end{tabular} & in the following extracts. \\\\\n' +
      '**Sample(s)** & Extract: _My wife refused to allow me to come to Hong Kong when the plague was at its height and –” Your wife, Johanne? You are married at last 2” Johanne grinned._ \\\\  & _”Well, when a man gets to my age, he starts to need a few home comforts. After my_ \\\\  & _dear mother passed away ten years ago now, I became_ \\\\  & Completion: \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 27: **LAMBADA.** We evaluate whether the correct answer is the most likely one according to the model overall, without constraint to a set of predetermined candidates.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Context** & \\begin{tabular}{} \\end{tabular} & \n' +
      '\\begin{tabular}{} \\end{tabular} \\\\\n' +
      '**Sample(s)** & \\begin{tabular}{} \\end{tabular} & \n' +
      '\\begin{tabular}{} \\end{tabular} \\\\\n' +
      '**Candidates** & \\begin{tabular}{} \\end{tabular} & \n' +
      '\\begin{tabular}{} \\end{tabular} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 26: **RTE.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Context** & \\begin{tabular}{} \\end{tabular} & \n' +
      '\\begin{tabular}{} \\end{tabular} \\\\\n' +
      '**Sample(s)** & \n' +
      '\\begin{tabular}{} \\end{tabular} \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 28: **OpenBookQA.** When options are not provided in the prompt, the candidates are directly the possible answers (e.g., puppies learning new tricks, etc.) instead of the letter keys.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Sample(s)** & Article: _Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects they thought they had. Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without “outside help”. “What kind of help is that?” I asked, expecting them to tell me that they would need a or family friend to help them out. “Surgery,” one replied. I was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job. One girl told me that she was considering surgery to increase her height. “They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!” At that point, I was shocked. I am short, I can’t deny that, but I don’t think I would put myself through months of agony just to be a few centimetres taller. I don’t even bother to wear shoes with thick soles, as I’m not trying to hide the fact that I am just not tall! It seems to me that there is a trend towards wanting “perfection”, and that is an ideal that just does not exist in reality. No one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that “perfection” is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career Question: We can know from the passage that the author works as a_. \\\\ A. _doctor_ \\\\ B. _model_ \\\\ C. _teacher_ \\\\ D. _reporter_ \\\\\n' +
      '**Answer: C. teacher** \\\\\n' +
      '**Question:**_Many graduates today turn to cosmetic surgery to_._ \\\\ A. _marry a better man/woman_ \\\\ B. _become a model_ \\\\ C. _get an advantage over others in job-hunting_ \\\\ D. _attract more admirers_ \\\\\n' +
      '**Answer: C. get an advantage over others in job-hunting** \\\\\n' +
      '**Question:**_According to the passage, the author believes that_._ \\\\ A. _everyone should purchase perfection, whatever the cost_ \\\\ B. _it’s right for graduates to ask for others to help them out in hunting for jobs_ \\\\ C. _it is one’s appearance instead of skills that really matters in one’s career_ \\\\ D. _media are to blame for misleading young people in their seeking for surgery_ \\\\\n' +
      '**Answer: D. media are to blame for misleading young people in their seeking for surgery** \\\\\n' +
      '**Question:**_Which’s the best title for the passage?_ \\\\ A. _Young Graduates Have Higher Expectations_ \\\\ B. _Young Graduates Look to Surgery for Better Jobs_ \\\\ C. _Young Graduates’ Opinion About Cosmetic Surgery_ \\\\ D. _Young Graduates Face a Different Situation in Job-hunting_ \\\\\n' +
      '**Answer:** \\\\\n' +
      '**Candidates** & **[A., B., C., D.]** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 29: **RACE.** Note that the way Brown et al. (2020) and Gao et al. (2021) implement RACE means that the number of shots is at the article level; 3 questions regarding each article are always provided, even in 0-shot.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Context** & **Compare the following sentences.** \\\\ \\hline \\hline \\multirow{2}{*}{**Sample(s)**} & Extract: _place_ has a similar meaning in the following two sentences. Yes or no? \\\\  & Sentence 1: _Do you want to come over to my place later?_ \\\\  & Sentence 2: _A political system with no place for the less prominent groups._ \\\\  & Answer: \\\\\n' +
      '**Candidates** & [yes, no] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 34: **Winograd. We keep the formatting of Gao et al. (2021) unchanged, but switch yes/no in the candidates for true/false.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Context** & **Compare the following questions about an extract by yes, or no.** \\\\ \\hline \\multirow{2}{*}{**Sample(s)**} & Extract: _Powdered sugar, also called confectioners’ sugar, icing sugar, and icing cake, is a finely ground sugar produced by milling granulated sugar into a powdered state. It usually contains a small amount of anti-caking agent to prevent clumping and improve flow. Although most often produced in a factory, powdered sugar can also be made by processing ordinary granulated sugar in a coffee grinder, or by crushing it by hand in a mortar and pestle._ \\\\  & Question: _is confectionary sugar the same as powdered suga, yes or no?_ \\\\  & Answer: \\\\\n' +
      '**Candidates** & [yes, no] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 30: **BoolQ.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Context** & **Compare whether the statement made about by the extract is True, False, or Unsurc.** \\\\ \\multirow{2}{*}{**Sample(s)**} & Extract: _It was a complex language. Not written down but handed down. One might say it was peeled down._ \\\\  & Statement: _the language was peeled down._ \\\\  & Question: True, False, or Unsurc? \\\\  & Answer: \\\\\n' +
      '**Candidates** & [True, False, Unsurc] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 31: **CB.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Context** & **Compare the following questions about an extract by yes, or no.** \\\\ \\hline \\multirow{2}{*}{**Sample(s)**} & Extract: _Powdered sugar, also called confectioners’ sugar, icing sugar, and icing cake, is a finely ground sugar produced by milling granulated sugar into a powdered state. It usually contains a small amount of anti-caking agent to prevent clumping and improve flow. Although most often produced in a factory, powdered sugar can also be made by processing ordinary granulated sugar in a coffee grinder, or by crushing it by hand in a mortar and pestle._ \\\\  & Question: _is confectionary sugar the same as powdered suga, yes or no?_ \\\\  & Answer: \\\\\n' +
      '**Candidates** & [yes, no] \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 32: **COPA. We keep the formatting of Gao et al. (2021) unchanged, but add an instruction.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline \\hline\n' +
      '**Candidates** & **[true, false]** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 33: **WiC.**\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
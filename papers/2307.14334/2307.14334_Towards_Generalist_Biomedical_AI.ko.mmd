[MISSING_PAGE_EMPTY:1]

작업. 생물의학 데이터 생성 및 혁신의 속도가 증가함에 따라 이러한 모델의 잠재적 영향도 마찬가지이며, 케어 전달을 위한 근본적인 생물의학 발견에 걸쳐 가능한 다운스트림 응용 프로그램의 폭이 넓어질 것이다.

이 작업에서 우리는 여러 생체 데이터 양식을 해석하고 동일한 모델 가중치 세트를 사용하여 많은 다운스트림 작업을 처리할 수 있는 통합 모델인 일반 의사 생체 의료 AI 시스템으로의 진행 상황을 자세히 설명한다. 이 목표의 핵심 과제 중 하나는 포괄적인 복합 의료 벤치마크의 부재였다. 이러한 충족되지 않은 요구를 해결하기 위해 질문 응답, 시각적 질문 응답, 의료 이미지 분류, 방사선 보고서 생성 및 요약, 유전체 변이 호출 등 14가지 다양한 생물의학 작업을 통해 언어, 의료 영상 및 유전체학 양식에 걸쳐 있는 오픈 소스 멀티모달 의료 벤치마크인 멀티메드벤치를 큐레이트한다.

멀티메드벤치를 활용하여 최근 언어[8, 9] 및 멀티모달 기반 모델[10, 11]의 발전에 따라 대규모 일반의 생의학 AI 시스템 구축인 Med-PaLM 멀티모달(Med-PaLM M)을 설계하고 개발한다. 특히 Med-PaLM M은 다양한 형태의 멀티모달 생체 의학 정보를 쉽게 통합하고 인터리빙할 수 있는 유연한 멀티모달 시퀀스 투 시퀀스 아키텍처이다. 또한, 모달리티-불가지론 언어 디코더의 표현성은 통합된 훈련 전략으로 간단한 생성 프레임워크에서 다양한 생물의학 작업을 처리할 수 있게 한다.

우리가 아는 한 Med-PaLM M은 단일 모델로 멀티모달 생의학 데이터를 해석하고 다양한 작업을 처리할 수 있는 일반주의 생의학 AI 시스템의 첫 시연이다. Med-PaLM M은 멀티메드벤치의 모든 태스크에서 최첨단(SOTA)과 경쟁하거나 이를 초과하는 성능에 도달하며, 종종 전문 도메인 및 태스크 특정 모델을 큰 마진만큼 능가한다. 특히 Med-PaLM M은 임상 효능에 대한 일반적인 성공 메트릭(micro-F1)에서 이전 최신 흉부 X선(CXR) 보고서 생성(MIMIC-CXR 데이터 세트)을 8% 이상 초과한다. MultiMedBench의 의료 시각 질의 응답 태스크 중 하나인 Slake-VQA[12])에서 Med-PaLM M은 BLEU-1 및 F1 메트릭에서 이전 SOTA 결과보다 10% 이상 성능이 향상되었다.

그림 1: **Med-PaLM M 개요**. 일반주의 바이오 의료 AI 시스템은 다양한 범위의 바이오 의료 데이터 양식과 작업을 처리할 수 있어야 한다. 이 중요한 목표를 향한 진전을 가능하게 하기 위해 질문 응답, 시각적 질문 응답, 이미지 분류, 방사선 보고서 생성 및 요약, 유전체 변이 호출 등 14가지 다양한 생의학 작업에 걸쳐 있는 벤치마크인 멀티메드벤치를 큐레이트한다. Med-PaLM 멀티모달(Med-PaLM M), 이러한 일반의 생의학 AI 시스템에 대한 개념 증명(음영 처리된 파란색 영역으로 표시됨)은 멀티메드벤치의 모든 작업에 대한 전문가 모델(빨간색 점선으로 표시됨)의 이전 SOTA 결과와 경쟁하거나 능가한다. 특히 Med-PaLM M은 작업별 사용자 지정 없이 단일 모델 가중치 집합을 사용하여 이를 달성합니다. **

우리는 일반주의적 멀티모달 생의학 모델에서 척도의 중요성을 이해하고 의료(시각) 질문 응답과 같은 더 높은 수준의 언어 능력이 필요한 작업에 대한 상당한 이점을 관찰하기 위해 절제 연구를 수행한다. 예비 실험은 또한 모델 규모에 걸쳐 새로운 의료 개념과 작업에 대한 제로샷 일반화의 증거와 제로샷 멀티모달 의료 추론과 같은 창발적 능력[13]을 제안한다. 우리는 AI 생성 흉부 X선 보고서에 대한 방사선사 평가를 추가로 수행하고 모델 척도에 걸쳐 고무적인 결과를 관찰한다.

전반적으로, 이러한 결과는 의학용 일반의 생의학 AI 시스템의 잠재력을 보여준다. 그러나 이러한 모델을 훈련하고 실제 응용 프로그램에서 성능을 검증하고 안전 영향을 이해하기 위한 대규모 생물의학 데이터 액세스 측면에서 중요한 작업이 남아 있다. 우리는 연구에서 이러한 주요 한계와 향후 연구의 방향을 요약한다. 요약하자면, 우리의 주요 기여는 다음과 같다.

* **MultiMedBench 큐레이션** 의료 영상, 임상 텍스트 및 유전체학을 포함한 여러 양식에 걸쳐 있는 새로운 멀티모달 생체 의학 벤치마크인 MultiMedBench를 소개합니다. 일반 의사 생체 의학 AI 시스템을 교육하고 평가하기 위한 14가지 다양한 작업이 있습니다.
* **Med-PaLM M, 일반 의료 의료 AI 시스템의 첫 번째 시연** 동일한 모델 가중치 집합으로 의료 이미지 분류, 의료 질문 응답, 시각적 질문 응답, 방사선 보고서 생성 및 요약, 게놈 변이 호출 등을 수행할 수 있는 단일 다중 작업, 다중 모드 생체 의료 AI 시스템 Med-PaLM M을 소개합니다. Med-PaLM M은 작업별 사용자 지정 없이 MultiMedBench의 여러 작업에 대해 최신(SOTA) 전문가 모델과 경쟁하거나 능가하는 성능에 도달합니다.
* **Med-PaLM M에서 새로운 창발적 기능의 증거** 작업 수행의 정량적 평가 외에도 제로 샷 의료 추론, 새로운 의료 개념과 작업에 대한 일반화 및 작업 전반에 걸친 긍정적인 전달의 증거를 관찰합니다. 이러한 실험은 다운스트림 데이터가 부족한 생의학 응용 프로그램에서 이러한 시스템의 유망한 잠재력을 시사한다.
* **Med-PaLM M 출력의 인간 평가** 자동화된 메트릭을 넘어 다양한 모델 규모에 걸쳐 Med-PaLM M에 의해 생성된 흉부 X선 보고서에 대한 방사선사 평가를 수행합니다. 246개의 후향적 흉부 X선에 대한 블라인드 나란히 순위에서 임상의는 최대 40.50%의 사례에서 방사선사가 생산한 보고서보다 Med-PaLM M 보고서에 대해 쌍별 선호도를 나타냈다. 또한 최상의 Med-PaLM M 모델은 보고서당 평균 0.25개의 임상적으로 유의미한 오류를 가지고 있다. 이러한 결과는 이전 작업 [14]의 인간 기준선과 동등하여 잠재적인 임상적 유용성을 시사한다.

## 2 관련 작업

### 기본 모델, 다중 모드 및 일반 사용자

**기반 모델** 패러다임의 출현 [5]는 언어 [8], 비전 [15] 및 기타 양식 [16]의 다양한 응용 프로그램에 널리 영향을 미쳤습니다. 사전 훈련된 모델의 가중치를 사용하는 전이 학습[17; 18]의 아이디어는 수십 년 동안 존재했지만[19; 20; 21; 22], 이러한 모델을 사전 훈련하는 데 사용되는 데이터 및 계산의 규모로 인해 이동이 발생했다[23]. 기초 모델의 개념은 모델이 광범위한 다운스트림 작업에 적응될 수 있음을 추가로 나타낸다[5].

기본 모델 패러다임 내에서 **다중 모드** [24]는 데이터 세트 [25], 모달리티 간 감독 [26], 작업 지정의 일반성 및 통일 [27; 28]에서도 다양 한 중요한 영향을 미쳤습니다. 예를 들어, 언어는 특히 다른 양식에서 기초 모델의 중요한 지원자였다[11;29]. CLIP[30]과 같은 시각적 기초 모델은 미리 결정된 클래스 레이블을 갖는 분류 데이터세트(즉, ImageNet[32])보다 대규모 인터넷 데이터로부터 수집하기 쉬운 언어-라벨링된 시각적 데이터세트[25; 31]에 대한 훈련에 의해 가능해진다. 공동 언어 및 비전 감독의 이점은 이미지의 생성 모델링[33]에서도 주목할 만하다. 여기서 텍스트 대 이미지 생성 모델링은 순수하게 무조건적인 생성 이미지 모델링[35]보다 고충실도 이미지 생성[34]을 생성하는 데 특히 더 성공적이었다. 또한, 언어의 유연성은 또한 모두 하나의 통합된 출력 공간을 통해 광범위한 작업 사양을 가능하게 한다[36]. - 객체 검출 및 객체 분류와 같은 상이한 출력 공간에 의해 전통적으로 다루어지는 작업을 모두 언어의 출력 공간을 통해 공동으로 표현하는 것이 가능하다[37]. Med-PaLM M은 또한 대규모 비전 언어 데이터 세트 [11]에 대해 사전 훈련된 모델 [10]을 통해 그리고 통합된 생성 언어 출력 공간을 통해 추가 생체 의학 도메인 피니튜닝을 통해 다중 모달리티의 일반성으로 이점을 얻는다.

기초 모델의 개념과 관련 된 개념은 **일반 모델** 의 개념입니다. - finetuning 없이 동일한 가중치 집합을 사용 하는 동일한 모델은 다양한 작업에서 탁월할 수 있습니다. 많은 태스크들을 다룰 수 있는 단일 멀티태스크[17] 모델은 예를 들어 강화 학습 커뮤니티[40]를 포함하여 오랫동안 관심을 가져왔다[38, 39]. GPT-3[6] 및 PaLM[8]과 같은 언어 전용 모델은 프롬프트 및 문맥 내 학습만을 사용하여 많은 태스크에서 동시에 탁월하다. 최근 작업은 또한 많은 작업을 수행할 수 있을 뿐만 아니라 많은 모달리티를 처리할 수 있는 일반주의 모델을 탐구했다[41]. 예를 들어, 가토[42]의 능력은 언어, 비전 및 에이전트 정책 학습에 걸쳐 있다. PaLM-E [10]은 추가로 언어 전용 태스크, 비전-언어 태스크 및 구체화된 비전-언어 태스크에서 우수한 단일 일반주의 모델을 얻는 것이 가능하다는 것을 보여준다. Med-PaLM M은 특히 PaLM-E 일반주의자 모델을 조정하고 정렬하여 구축된 생물의학 영역을 위해 설계된 일반주의자 모델이다.

### 바이오 메디신의 멀티모달 기반 모델

잠재성을 감안할 때 다양한 생물의학 응용을 위한 멀티모달 기반 모델에 상당한 관심이 있었다. Moor _et al._[43]은 구현이나 경험적 결과가 없음에도 불구하고 일반주의 의료 AI의 개념에 대해 논의한다. Todoris _et al._[44]는 낮은 데이터 네트워크 생물학 응용 프로그램에서 컨텍스트별 예측을 가능하게 하기 위해 약 3천만 개의 단일 세포 전사체의 코퍼스에서 사전 훈련된 트랜스포머 [45] 기반 모델인 Genformer를 소개한다. BiomedGPT [46]은 언어 모델(LM)과 마스킹된 이미지 채우기 목표의 조합을 사용하여 다양한 의료 이미지, 의학 문헌 및 임상 노트에서 사전 훈련된 다중 작업 바이오 메디컬 기초 모델이다. 그러나 이러한 모든 노력은 사전 훈련된 모델이므로 다운스트림 응용 프로그램을 활성화하기 위해 추가 작업별 데이터와 피니튜닝이 필요하다. 대조적으로, Med-PaLM M은 더 이상의 피니튜닝 또는 모델 파라미터 업데이트를 요구하지 않고 동시에 많은 생물의학 작업을 공동으로 해결하도록 직접 훈련된다. LLaVA-Med [47]은 아마도 우리의 노력과 가장 유사하다. 저자는 PubMed와 GPT-4 [48]을 사용하여 데이터 세트에 따라 다중 모드 명령을 큐레이션하고 LLaVA 모델을 조정한다. 그러나, 실험은 의료 영상에 조건화된 대화의 세 가지 의학적 시각적 질의 응답 데이터 세트와 정성적 예시에 국한된다. 대조적으로, 우리의 작업은 14가지 다양한 작업과 모델 출력에 대한 전문가 평가를 통해 의료 영상, 임상 텍스트 및 유전체학을 포함한 여러 양식에 걸쳐 더 포괄적이다.

### 멀티모달 의료 AI 벤치마크

우리가 아는 한, 일반의 생의학 AI 모델을 훈련하고 평가하기 위한 벤치마크를 큐레이트하려는 시도는 제한적이었다. 아마도 정신적으로 가장 가까운 작품은 BenchMD[49]. 벤치마크는 공개적으로 이용 가능한 19개의 데이터 세트와 1D 센서 데이터, 2D 이미지 및 3D 볼륨 스캔을 포함한 7개의 의료 모달리티에 걸쳐 있다. 그러나 이들의 작업은 주로 분류에 중점을 두지만 벤치마크에는 의료(시각) 질문 응답, 방사선 보고서 생성 및 요약과 같은 생성 작업도 포함된다. 나아가, 현재 이러한 모든 작업을 동시에 능숙하게 처리할 수 있는 일반주의 생체의학 AI 시스템의 구현은 전무한 실정이다.

## 3 MultiMedBench: Generalist Biomedical AI를 위한 벤치마크

다음으로 일반의 생의학 AI의 개발 및 평가를 가능하게 하기 위해 선별한 벤치마크인 멀티메드벤치를 설명한다. 멀티메드벤치는 12개의 비식별 오픈소스 데이터셋과 14개의 개별 태스크로 구성된 멀티태스크, 멀티모달 벤치마크이다. 다양한 임상 관련 작업을 수행할 수 있는 범용 생의학 AI의 능력을 측정합니다. 벤치마크는 의학 질문, 방사선 보고서, 병리학, 피부과, 흉부 X선, 유방 촬영 및 유전체학을 포함한 광범위한 데이터 소스를 다룬다. MultiMedBench의 작업은 다음 축에 따라 다양 합니다.

* **작업 유형:** 질문 응답, 보고서 생성 및 요약, 시각적 질문 응답, 의료 이미지 분류 및 게놈 변이체 호출입니다.
* **촬영장비:** 텍스트, 방사선(CT, MRI 및 X선), 병리학, 피부과, 유방 촬영 및 유전체학입니다.
* **출력 형식:** 분류를 포함한 모든 작업에 대한 개방형 생성입니다.

언어 전용 작업은 Singhal _et al._[9]에서 사용된 MultiMedQA 작업 중 3개와 영상의학 보고서 요약을 포함하여 의학적 질의 응답으로 구성된다. 그들은 의료 지식을 이해하고 회상하고 조작하는 모델의 능력을 평가하기 위해 선택되었다. 멀티모달 작업에는 의료 시각적 질문 응답(VQA), 의료 이미지 분류, 흉부 X선 보고서 생성 및 게놈 변이 호출이 있으며, 이는 이러한 모델의 시각적 이해와 멀티모달 추론 기능을 모두 평가하기에 적합하다. 표 1에는 MultiMedBench의 데이터 세트 및 작업에 대한 개요가 포함되어 있으며, 총 100만 개 이상의 샘플이 벤치마크에 포함되어 있다. 개별 데이터 세트 및 작업에 대한 자세한 설명은 섹션 A.1을 참조하십시오.

## 4 Med-PaLM M: Generalist Biomedical AI에 대한 개념 증명

이 섹션에서는 Med-PaLM M 모델의 개발을 뒷받침하는 방법을 자세히 설명한다. 먼저 Med-PaLM M이 상속하는 섹션 4.1의 사전 훈련된 모델의 예제를 검토한 다음 모델의 생명 의학 도메인 섹션 4.2로의 피니튜닝 및 전문화와 관련된 데이터 세트 및 훈련 세부 사항에 대해 논의한다.

### Model preliminaries

Med-PaLM M은 이러한 사전 훈련된 모델의 아키텍처뿐만 아니라 모델 매개변수에 인코딩된 일반 도메인 지식을 상속한다.

Chowdhery _et al._[8]에서 소개한 **PaLM(Pathways Language Model)** 은 TPU pod 전체에서 매우 효율적인 훈련을 가능하게 하는 대규모 ML 가속기 오케스트레이션 시스템인 Pathways [50]을 사용하여 훈련된 조밀하게 연결된 디코더 전용 Transformer [45] 기반 LLM(Large Language Model)입니다. PaLM 훈련 코퍼스는 웹 페이지, 위키피디아 기사, 소스 코드, 소셜 미디어 대화, 뉴스 기사 및 책의 혼합물을 나타내는 7800억 개의 토큰으로 구성된다. PaLM 모델은 8, 62 및 540억 매개변수의 크기로 훈련되었으며 세 가지 PaLM 모델 변형은 모두 훈련 데이터의 한 시대에 대해 훈련된다. 발표 당시 PaLM 540B는 다단계 추론 작업의 집합에서 미세 조정된 최첨단 모델을 능가하고 BIG 벤치에서 평균 인간 성능을 초과하는 획기적인 성능을 달성했다[51].

Dosovitskiy _et al._[52]에 의해 도입된 **ViT(Vision Transformer)** 는 Transformer [45] 아키텍처를 이미지 및 비디오와 같은 시각적 데이터로 확장합니다. 본 연구에서는 두 개의 ViT 사전 훈련 모델을 비전으로 간주한다.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Task Type** & **Modality** & **Dataset** & **Description** \\ \hline \multirow{3}{*}{Question Answering} & \multirow{3}{*}{Text} & MedQA & US medical licensing exam-style, multiple-choice \\  & & MedMCQA & Indian medical entrance exams, multiple-choice \\  & & PubMedQA & Biomedical literature questions, multiple-choice \\ \hline Report Summarization & Radiology & MIMIC-III & Summarizing findings in radiology reports \\ \hline \multirow{3}{*}{Question Answering} & \multirow{3}{*}{Radiology} & VQA-RAD & Close/open-ended VQA on radiology images \\  & & Slake-VQA & English-Chinese bilingual VQA on radiology images \\  & Pathology & Path-VQA & Close/open-ended VQA on pathology images \\ \hline Report Generation & Chest X-ray & MIMIC-CXR & Chest X-ray report generation \\ \hline \multirow{6}{*}{
\begin{tabular}{c} Medical \\ Image Classification \\ \end{tabular} } & Chest X-ray & MIMIC-CXR & Binary classification of chest X-ray abnormalities \\  & Dermatology & PAD-UFES-20 & 6-class skin lesion image classification \\  & & VinDr-Mammo & 5-class breast-level BI-RADS classification \\  & Mammography & CBIS-DDSM & 3-class lesion-level classification (mass) \\  & & CBIS-DDSM & 3-class lesion-level classification (calcification) \\  & Genomics & PrecisionFDA & Genomic variant calling as 3-class image classification \\  & Truth Challenge V2 & & \\ \hline \hline \end{tabular}
\end{table}
표 1: **MultiMedBench 개요.** Med-PaLM M의 개발 및 평가를 위해 소개하는 벤치마크인 MultiMedBench의 요약입니다. 멀티메드벤치는 5개의 작업 유형에 걸쳐 14개의 개별 작업과 7개의 생의학 데이터 양식에 걸쳐 12개의 데이터 세트로 구성된다. 총 100만 개 이상의 표본이 벤치마크에 포함되어 있다.

encoders, Chen _et al._[11]의 40억(4B) 파라미터 모델 및 Dehghani _et al._[15]의 220억(22B) 파라미터 모델. 이 두 모델은 약 40억 개의 이미지로 구성된 대규모 분류 데이터 세트 [53, 54]에서 지도 학습을 통해 사전 훈련되었다.

Driess _et al._[10]에서 소개한 **PaLM-E** 는 텍스트, 비전 및 센서 신호를 포함하는 멀티모달 입력의 시퀀스를 처리할 수 있는 멀티모달 언어 모델입니다. 기본 PaLM-E 모델은 사전 훈련된 PaLM과 ViT를 사용하며, 처음에 구현된 로봇 응용 프로그램을 위해 개발되었지만 OK-VQA[55] 및 VQA v2[56]와 같은 여러 비전 언어 벤치마크에서 강력한 성능을 보여주었다. 또한 PaLM-E는 단일 프롬프트에서 이미지, 텍스트 및 센서 신호를 인터리빙할 수 있는 유연성을 제공하여 모델이 완전히 다중 모드 컨텍스트로 예측을 수행할 수 있도록 한다. PaLM-E는 또한 제로-샷 멀티모달 체인-오브-사상(CoT) 추론, 적은-샷 인-컨텍스트 학습을 포함하는 광범위한 능력을 나타낸다. 따라서 Med-PaLM M의 기본 아키텍처로 PaLM-E 모델을 활용한다.

우리는 연구에서 ViT 4B(PaLM-E 12B)가 있는 PaLM 8B, ViT 22B(PaLM-E 84B)가 있는 PaLM 62B 및 ViT 22B(PaLM-E 562B)가 있는 PaLM 540B의 세 가지 다른 조합을 고려한다. 모든 모델은 Driess _et al._[10]에 설명된 바와 같이 여러 로봇 실시예에 걸쳐 작업 외에도 다양한 비전 언어 데이터 세트에 대해 사전 훈련되었다.

### Putting it all together: Med-PaLM M

Med-PaLM M은 MultiMedBench를 사용하여 PaLM-E 모델을 생체 의학 도메인에 피니튜닝하고 정렬하여 개발한다. 다음은 모델 개발의 기초가 되는 중요한 방법론적 세부 사항을 요약한다.

데이터셋과 전처리 과정은 멀티메드벤치의 모든 이미지를 \(224\times 224\times 3\)으로 리사이징하고 필요한 경우 패딩으로 원래 화면비를 보존한다. 그레이 스케일 이미지는 채널 차원을 따라 동일한 이미지를 쌓아 3채널 이미지로 변환했다. 클래스 밸런싱 및 이미지 데이터 증강과 같은 태스크별 프리포즈 방법은 섹션 A.1의 각 태스크에 대해 상세히 설명된다.

우리의 목표는 통일된 모델 아키텍처와 단일 모델 매개변수 세트를 사용하여 멀티모달 입력으로 여러 작업을 수행하도록 일반 의사 생의학 AI 모델을 훈련하는 것이다. 이를 위해, 명령어 튜닝을 통해 별개의 태스크들의 혼합으로 모델을 동시에 트레이닝하였다[57]. 구체적으로, 우리는 모델이 통일된 생성 프레임워크에서 다른 유형의 작업을 수행하도록 프롬프트하는 작업별 지침을 모델에 제공했다. 작업 프롬프트는 지시사항, 관련 상황 정보 및 질문으로 구성됩니다. 예를 들어, 그림 2와 같이 흉부 X-ray 보고서 생성 작업에서 모델의 예측을 조건화하기 위한 추가 컨텍스트 정보로 연구 이유와 이미지 방향 정보를 포함했다. 마찬가지로 피부과 분류 작업의 경우 피부 병변 이미지와 관련된 환자 임상 이력을 제공했다. 모든 가능한 클래스 레이블이 개별 답변 옵션으로 제공되고 모델이 목표 출력으로 가장 가능성 있는 답변을 생성하도록 프롬프트되는 모든 분류 작업을 객관식 질문으로 공식화했다. 시각적 질의 응답, 보고서 생성 및 요약과 같은 다른 생성 작업의 경우 모델은 대상 응답에 대해 미세 조정되었다.

모델이 지침을 더 잘 따를 수 있도록 하기 위해 대부분의 작업에 대해("표 A.1 참조) 텍스트 전용 _"원샷 예제"_를 작업 프롬프트에 추가하여 언어 모델의 예측을 조건화했다. 원샷 예제에서는 부분 입력-출력 쌍으로 모델을 프롬프트하는 데 도움이 됩니다. 중요한 것은, 멀티모달 태스크의 경우, 예시에서 실제 이미지를 더미 텍스트 자리 표시자로 대체하였다(텍스트 문자열 "\(<\)img\(>\)"): 이것은 (i) 단일 이미지 트레이닝에 대한 트레이닝 계산 효율을 보존하고, 또한 (ii) 주어진 텍스트 토큰과 다수의 이미지로부터의 이미지 토큰 사이의 교차 주의로부터의 잠재적인 간섭을 우회한다[28]. 우리의 결과는 이 계획이 섹션 6에 자세히 설명된 대로 모델이 원하는 응답 형식을 생성하도록 유도하는 데 효과적임을 보여준다.

모델 훈련은 표 A.1에 표시된 혼합 비율로 MultiMedBench 작업에서 PaLM-E의 사전 훈련된 12B, 84B 및 562B 매개변수 변형을 조정했다. 이러한 혼합 비율은 각 데이터 세트의 훈련 샘플 수에 대략 비례하고 각 작업에서 적어도 하나의 샘플이 하나의 배치에 존재하도록 경험적으로 결정되었다. 훈련 중에 업데이트된 전체 모델 매개변수 세트로 PaLM-E 모델의 종단간 조정을 수행했다. 멀티모달 태스크의 경우 이미지 토큰을 텍스트 토큰과 인터리빙하여 PaLM-E 모델에 대한 멀티모달 컨텍스트 입력을 형성했다. 멀티모달 컨텍스트 입력은 모든 피니튜닝 작업에 대해 최대 1개의 이미지를 포함한다. 그러나 Med-PaLM M은 추론 중에 여러 이미지로 입력을 처리할 수 있다는 점에 유의한다.

운동량 \(\beta_{1}=0.9\), 탈락률 0.1, 일정 학습률 스케줄의 Adafactor 최적화기 [58]를 사용하였다. 표 2에 자세히 설명된 다양한 모델 크기에 대한 피니튜닝 실험에서 다른 하이퍼파라미터 세트를 사용했다.

결과 모델인 Med-PaLM M(12B, 84B 및 562B)은 멀티모달 입력을 인코딩 및 해석하고 의료(시각) 질의 응답, 방사선학 보고서 생성 및 요약, 의료 이미지 분류 및 게놈 변이체 호출을 포함하는 작업을 수행할 수 있는 능력을 가진 생물의학 도메인에 적응된다.

## 5 Evaluation

이 절에서는 실험 평가의 목적, 범위 및 방법에 대해 설명한다. 결과는 섹션 6에 나와 있다. Med-PaLM M의 평가 실험은 다음과 같은 목적으로 설계되었다.

* **일반 사용자 기능 평가** 모델 규모에 걸쳐 MultiMedBench의 모든 작업에서 Med-PaLM M을 평가했습니다. 다양한 작업에 걸쳐 ViT 및 LLM 구성 요소를 스케일링하는 효과에 대한 초기 통찰력을 제공한다. 우리는 이전의 SOTA(전문 단일 작업 또는 단일 모달리티 방법 포함)와 생물학적 피니튜닝이 없는 최첨단 일반 모델(PaLM-E)과 성능을 비교했다.
* **새로운 창발적 기능 탐색** 다양한 작업에 걸쳐 단일 유연한 멀티모달 일반주의 AI 시스템을 훈련하는 것의 한 가지 가정된 이점은 언어에서 발생하는 새로운 기능의 출현입니다.

그림 2: **원샷 예시로 프롬프트 하는 지침 작업의 그림입니다.* * (상단)은 흉부 X선 보고서 생성 작업에 대 한 작업 프롬프트를 보여 줍니다. 과제별 지시, 텍스트 전용 "원샷 예시"(해당 이미지를 생략하되 목표 답을 보존) 및 실제 질문으로 구성된다. X-ray 이미지는 질문 외에도 뷰 방향 및 연구 이유를 포함한 텍스트 컨텍스트와 함께 삽입 및 인터리브된다. (아래)는 피부과 분류 작업에 대한 작업 프롬프트를 보여줍니다. 피부 병변 분류 작업을 개별 답변 옵션으로 제공되는 모든 클래스 레이블을 사용하여 객관식 질문 답변 작업으로 공식화한다. 흉부 X-선 보고서 생성 태스크와 유사하게, 피부 병변 이미지 토큰은 질문에 대한 추가 컨텍스트로서 환자 임상 이력과 인터리빙된다. 파란색 <img>는 이미지 토큰이 내장된 프롬프트 내의 위치를 나타낸다.

새로운 의학 개념 및 작업과 같은 조합 일반화를 가능하게 했다. 우리는 질적 실험과 질적 실험을 통해 이것을 탐구했다.
* **방사선 보고서 생성 품질 측정** 자동 자연어 생성(NLG) 메트릭은 AI 생성 방사선 보고서의 임상적 적용 가능성에 대한 충분한 평가를 제공하지 않습니다. 따라서 우리는 방사선사가 제공한 참조 보고서와의 비교를 포함하여 MIMIC-CXR 데이터 세트에 대해 AI 생성 보고서에 대한 전문가 방사선사 평가를 수행했다.

### MultiMedBench 평가

Med-PaLM M은 MultiMedBench의 언어 전용 및 다중 모드 생의학 작업의 혼합에서 동시에 미세 조정되었다. 이러한 작업들에 대한 모델 내 분배 성능을 별도의 전문가 모델에서 얻은 해당 SOTA 결과와 비교하여 평가했다. 특히, 평가 중 각 과제에 대한 훈련에서와 동일한 소수의 샷 설정을 사용했다. 작업별 메트릭은 각 작업의 테스트 분할에서 계산되었으며 이전 SOTA 전문가 AI 시스템과 비교되었다. 표 1에 설명된 소수의 작업에 대해 비교를 위해 충분히 유사한 이전 시도를 찾을 수 없었다.

### 언어 사용 제로 샷 일반화 평가

Med-PaLM M의 이전에 보이지 않은 의학적 개념으로 일반화하는 능력을 조사하기 위해 흉부 X선 이미지에서 결핵(TB)의 유무를 예측하는 모델의 능력을 평가한다. 이를 위해 몽고메리 카운티 흉부 X선 세트(MC)를 사용했다. 데이터 세트에는 138개의 정면 흉부 X선이 포함되어 있으며, 그 중 80개는 정상 사례이고 58개는 TB 징후가 있다[59]. 각 사례에는 폐에서 보이는 이상에 대한 주석도 포함되어 있다. Med-PaLM M은 MIMIC-CXR 데이터 세트에 대해 훈련되었지만 TB 질병 라벨을 명시적으로 예측하도록 훈련되지 않았다.

우리는 이 문제를 입력 이미지에서 TB의 존재에 대한 예/아니오 답변을 생성하기 위해 모델이 (텍스트 전용 단발성 예시로) 프롬프트되는 2가지 선택 질문 응답 태스크로 공식화하여 모델 규모 전반에 걸쳐 정확도를 평가했다.

또한 텍스트 전용 예시(해당 이미지 없음)로 프롬프트하고 모델이 클래스 예측 및 이미지 결과를 설명하는 첨부 보고서를 생성하도록 프롬프트하여 모델의 제로 샷 체인 오브 사상(CoT) 멀티모달 의료 추론 능력을 조사했다. 단일 텍스트 전용 입력-출력 쌍으로 모델을 프롬프트하는 동안 이미지를 생략하고(대신 더미 텍스트 자리 표시자를 사용) 텍스트 예가 훈련 세트에서 그려지지 않고 손으로 조작되었다는 점에 주목한다. 따라서, 이러한 접근법은 원샷이 아닌 제로샷으로 간주될 수 있다.

Med-PaLM M이 새로운 작업 시나리오로 일반화하는 능력을 평가하기 위해 2시점 흉부 X선 보고서 생성에 대한 모델 성능을 평가했는데, 이는 모델이 단일시점 흉부 X선에서만 보고서를 생성하도록 훈련된 경우 새로운 작업이다.

마지막으로, 다양한 생의학 과제를 해결하기 위해 단일 일반주의자 모델을 공동으로 훈련한 결과 긍정적인 과제 전달의 증거를 조사했다. 이를 위해 MIMIC-CXR 분류 작업을 작업 혼합물에서 제외하여 Med-PaLM M 84B 변형을 훈련하는 절제 연구를 수행했다. 흉부 X선 보고서 생성 작업에서 완전한 MultiMedBench 혼합물에 대해 훈련된 Med-PaLM M 84B 변형과 이 모델 변형을 비교하여 후자의 향상된 성능을 예상했다.

### 영상의학 보고서 생성의 임상 평가

Med-PaLM M에서 생성된 흉부 X선 보고서의 품질과 임상적 적용 가능성을 추가로 평가하고 모델 스케일링의 효과를 이해하기 위해 MIMIC-CXR 데이터 세트를 사용하여 인간 평가를 수행했다. 평가는 인도에 기반을 둔 4명의 자격을 갖춘 흉부 방사선사가 수행했다.

**데이터 세트**: 평가 세트는 MIMIC-CXR 테스트 분할에서 선택한 246개의 사례로 구성되었습니다. Med-PaLM M의 예상 입력 형식을 맞추기 위해 각 연구에서 단일 이미지를 선택했다. 우리는 동일한 환자에 대한 여러 X선 보기 또는 과거 검사를 언급한 지상 진실 보고서가 있는 연구를 제외했다.

**절차**: 두 가지 보완적인 인간 평가를 수행 했습니다. (1) 평가자가 여러 대체 보고서 결과를 비교 하 고 전체 품질을 기준으로 순위를 매기는 _side-by-side 평가_ 및 (2) 평가자가 개별 보고서 결과의 품질을 평가하는 _independent 평가_입니다. 최종 평가를 수행하기 전에 평가자에 대한 지침을 반복하고 평가 세트와 구별되는 25개 사례의 파일럿 세트를 사용하여 등급을 보정했다. 246명의 모든 사례에 대해 측면 평가를 수행했으며, 각 사례는 4명의 풀에서 무작위로 선택된 단일 방사선 전문의에 의해 평가되었다. 독립적인 평가를 위해 4명의 방사선 전문의 각각은 평가 세트의 모든 경우에 대해 3개의 Med-PaLM M 모델 변형(12B, 84B 및 562B)에 의해 생성된 결과에 독립적으로 주석을 달았다. 방사선사는 모든 평가 과제에 대한 보고서 결과의 출처를 알지 못했으며 보고서는 무작위 순서로 제시되었다.

**측면 평가** 각 측면 평가에 대한 입력은 MIMIC-CXR 연구의 "표시" 섹션과 함께 단일 흉부 X선이었습니다. 보고서의 "찾기" 섹션에 대한 4가지 대체 옵션이 그림 A.3에 표시된 대로 평가자에게 표시되었다. 4가지 대체 "찾기" 섹션은 데이터 세트 참조 보고서의 발견과 3개의 Med-PaLM M 모델 변형(12B, 84B, 562B)에 의해 생성된 발견에 해당한다. 등급은 최상의 임상 판단을 사용하여 전반적인 품질에 따라 4가지 대체 결과의 순위를 매기도록 요청받았다.

**독립 평가** 독립 평가를 위해 평가자는 MIMIC-CXR 연구의 표시 및 참조 보고서 결과와 함께 단일 흉부 X-ray를 제공했지만(명시적으로 표시됨), 이번에는 그림 A.4와 같이 Med-PaLM M에 의해 생성된 단일 결과 단락만 제공되었습니다. 평가자는 제공된 참조 입력과 흉부 X-ray 이미지에 대한 자체 판단에서 Med-PaLM M 생성 결과의 품질을 평가하도록 요청받았습니다. Yu _et al._[60]에서 제안한 평가 스키마는 우리의 평가 과제 설계에 영감을 주었다.

첫째, 평가자들은 제공된 이미지의 품질과 뷰가 평가 과제를 충분히 수행하기에 충분한지를 평가하였다. 다음으로, 그들은 그들이 동의하지 않는 모델 생성 결과의 모든 구절(오류)과 모든 누락된 부분(미션)에 주석을 달았다. 평가자들은 각 오류 통로를 유형(발견 없음, 잘못된 발견 위치, 잘못된 심각도, 존재하지 않는 보기 참조 또는 이전 연구)별로 분류하고 임상적 중요성을 평가하고 선택된 통로를 대체할 대체 텍스트를 제안했다. 마찬가지로, 각 누락에 대해 평가자는 누락이 임상적 의미가 있는지 포함하고 결정했어야 하는 계대를 지정했다.

## 6 Results

여기에서 섹션 5에 소개된 세 가지 다른 평가 설정에 걸쳐 결과를 제시한다.

### Med-PaLM M은 모든 MultiMedBench 작업에서 SOTA 근처 또는 초과를 수행 합니다.

**Med-PaLM M 성능 대 기준선** Med-PaLM M을 두 기준선과 비교했습니다.

* MultiMedBench 작업 각각에 대한 이전 SOTA 전문가 모델입니다.
* 생물의학 도메인 피니튜닝이 없는 기준 일반 모델(PaLM-E 84B). 계산 제약으로 인해 이 모델 크기 변형(PaLM-E 562B가 아님)을 사용했다.

결과는 표 2에 요약되어 있다. MultiMedBench 작업 전반에 걸쳐 Med-PaLM M의 최상의 결과(모델 크기 3개)는 12개 작업 중 5개(2개 작업의 경우 설정과 유사한 이전 SOTA를 찾을 수 없음)에서 이전 SOTA 결과를 초과했지만 나머지는 경쟁력이 있었다. 특히, 이러한 결과는 작업별 아키텍처 커스터마이징 또는 최적화 없이 동일한 모델 가중치 세트를 사용하는 일반주의자 모델로 달성되었다.

의료 질의 응답 작업에서 SOTA Med-PaLM 2 결과[61]와 비교하여 Med-PaLM 2의 더 높은 성능을 관찰했다. 그러나 Med-PaLM M이 구축된 기준 PaLM 모델과 비교했을 때, Med-PaLM M은 세 가지 질의 응답 데이터 세트 모두에서 동일한 소수의 샷 설정에서 이전 최고의 PaLM 결과[9]보다 큰 마진을 능가했다.

또한, 생체 의학 도메인 피니튜닝이 없는 일반 기준선으로서의 PaLM-E 84B와 비교할 때 Med-PaLM M은 종종 14개 작업 모두에서 상당한 마진만큼 성능 개선을 보여 도메인 적응의 중요성을 입증했다. 종합하면, 이러한 결과는 일반주의 생의학 AI 모델로서 Med-PaLM M의 강력한 능력을 보여준다. 우리는 섹션 A.3의 개별 작업 각각에 대한 결과를 자세히 설명한다.

**모델 규모에 대 한 Med-PaLM M 성능** 모델 규모에 대 한 Med-PaLM M 성능 (12B, 84B 및 562B)을 표 3에 요약 합니다. 주요 관찰은 다음과 같습니다.

* **언어 추론 작업은 규모의 이점을 제공합니다* * 의료 질문 응답, 의료 시각적 질문 응답 및 방사선 보고서 요약과 같은 언어 이해 및 추론이 필요한 작업의 경우 모델을 12B에서 562B로 확장함에 따라 상당한 개선이 표시됩니다.
* **비전 인코더 성능에 의해 병목 현상** 미묘한 시각적 이해가 필요하지만 최소한의 언어 추론이 필요한 유방 촬영 또는 피부과 이미지 분류와 같은 작업의 경우(출력은 분류 레이블 토큰만 사용) Med-PaLM M 12B에서 Med-PaLM 84B로 성능이 향상되었지만 562B 모델의 경우 안정적입니다. 아마도 비전 인코더가 해당 단계에서 더 이상 스케일링되지 않기 때문일 수 있습니다(Med-PaLM M 84B 및 562B 모델 모두 비전 인코더와 동일한 22B ViT를 사용 함). 따라서 스케일링 이점을 관찰하는 병목 현상으로 작용합니다. 입력 이미지 해상도와 같은 추가 혼란의 가능성에 주목한다.

흉부 X-ray 보고서 생성 작업에 대한 스케일링 결과는 흥미롭다(표 3). 표면적으로 볼 때, 이 작업은 복잡한 언어 이해 및 추론 능력을 필요로 하므로 도움이 될 것 같습니다.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Task Type & Modality & Dataset & Metric & SOTA & \begin{tabular}{c} PaLM-E \\ (84B) \\ \end{tabular} &
\begin{tabular}{c} Med-PaLM M \\ (Best) \\ \end{tabular} \\ \hline \multirow{3}{*}{Question Answering} & \multirow{3}{*}{Text} & MedQA & Accuracy & **86.50\%**[61] & 28.83\% & 69.68\% \\  & & MedMCQA & Accuracy & **72.30\%**[61] & 33.35\% & 62.59\% \\  & & PubMedQA & Accuracy & **81.80\%**[61] & 64.00\% & 80.00\% \\ \hline \multirow{3}{*}{Report Summarization} & \multirow{3}{*}{Radiology} & \multirow{3}{*}{MIMIC-III} & ROUGE-L & **38.70\%**[62] & 3.30\% & 32.03\% \\  & & & BLEU & **16.20\%**[62] & 0.34\% & 15.36\% \\  & & & F1-RadGraph & **40.80\%**[62] & 8.00\% & 34.71\% \\ \hline \multirow{3}{*}{Visual Question Answering} & \multirow{3}{*}{Radiology} & VQA-RAD & BLEU-1 & 71.03\% [63] & 59.19\% & **71.27\%** \\  & & & F1 & N/A & 38.67\% & **62.06\%** \\ \cline{1-1}  & & & BLEU-1 & 78.60\% [64] & 52.65\% & **92.77\%** \\ \cline{1-1}  & & & F1 & 78.10\% [64] & 24.53\% & **89.28\%** \\ \cline{1-1}  & & & BLEU-1 & 70.30\% [64] & 54.92\% & **72.27\%** \\ \cline{1-1}  & & & F1 & 58.40\% [64] & 29.68\% & **62.69\%** \\ \hline \multirow{3}{*}{Report Generation} & \multirow{3}{*}{Chest X-ray} & \multirow{3}{*}{MIMIC-CXR} & Micro-F1-14 & 44.20\% [65] & 15.40\% & **53.56\%** \\  & & & Macro-F1-14 & 30.70\% [65] & 10.11\% & **39.83\%** \\ \cline{1-1}  & & & Micro-F1-5 & 56.70\% [66] & 5.51\% & **57.88\%** \\ \cline{1-1}  & & & Macro-F1-5 & N/A & 4.85\% & **51.60\%** \\ \cline{1-1}  & & & F1-RadGraph & 24.40\% [14] & 11.66\% & **26.71\%** \\ \cline{1-1}  & & & BLEU-1 & **39.48\%**[65] & 19.86\% & 32.31\% \\ \cline{1-1}  & & & BLEU-4 & **13.30\%**[66] & 4.60\% & 11.50\% \\ \cline{1-1}  & & & ROUGE-L & **29.60\%**[67] & 16.53\% & 27.49\% \\ \cline{1-1}  & & & CIDEr-D & **49.50\%**[68] & 3.50\% & 26.17\% \\ \hline \multirow{3}{*}{Image Classification} & \multirow{3}{*}{Chest X-ray} & MIMIC-CXR & Macro-AUC & **81.27\%**[69] & 51.48\% & 79.09\% \\  & & (5 conditions) & Macro-F1 & N/A & 7.83\% & **41.57\%** \\ \cline{1-1}  & & & Macro-AUC & N/A & 63.37\% & **97.27\%** \\ \cline{1-1}  & & & Macro-F1 & N/A & 1.38\% & **84.32\%** \\ \cline{1-1}  & & & Micro-AUC & 64.50\% [49] & 51.49\% & **71.76\%** \\ \cline{1-1}  & & & Macro-F1 & N/A & 16.06\% & **35.70\%** \\ \cline{1-1}  & & & CBIS-DDSM & Macro-AUC & N/A & 47.75\% & **73.31\%** \\ \cline{1-1}  & & & (mass) & Macro-F1 & N/A & 7.77\% & **51.12\%** \\ \cline{1-1}  & & & CBIS-DDSM & Macro-AUC & N/A & 40.67\% & **82.22\%** \\ \cline{1-1}  & & & (calcification) & Macro-F1 & **70.71\%**[70] & 11.37\% & 67.86\% \\ \cline{1-1}  & & & PrecisionFDA & Indel-F1 & **99.40\%**[71] & 53.01\% & 97.04\% \\ \cline{1-1}  & & & (Variant Calling) & (Truth Challenge V2) & SNP-F1 & **99.70\%**[71] & 52.84\% & 99.35\% \\ \hline \hline \end{tabular}
\end{table}
표 2: **MultiMedBench에 대 한 성능 비교.** Med-PaLM M을 전문 SOTA 모델 및 생물의학 도메인 피니튜닝 없이 일반 모델 (PaLM-E 84B)과 비교 합니다. MultiMedBench의 모든 작업, 데이터 세트 및 메트릭 조합에 걸쳐 SOTA 근처에서 또는 초과 하는 Med-PaLM M 성능을 관찰 합니다. 이러한 결과는 작업별 맞춤화 없이 동일한 모델 가중치 세트를 사용하여 Med-PaLM M에 의해 달성된다.

언어 모델을 스케일링한 결과 Med-PaLM M 84B 모델이 대부분의 메트릭에서 대략 동등하거나 562B 모델을 약간 초과하는 것으로 나타났으며, 이는 단순히 더 큰 모델에 사용되는 훈련 단계가 적기 때문일 수 있다. 언어 모델의 크기 증가의 반환 감소에 대한 또 다른 가능성은 MIMIC-CXR 데이터세트에서 흉부 X선 보고서 생성을 위한 출력 공간이 템플릿 문장 세트와 제한된 수의 조건에 상당히 국한되어 있을 가능성이 있다. 이러한 통찰력은 이 데이터세트에서 흉부 X-선 보고서 생성 작업에 대한 완전한 생성 접근법과 대조적으로 검색 기반 접근법의 사용에 동기를 부여했다[72, 73]. 추가로, 더 큰 562B 모델은 84B 모델의 비교 간결성보다는 장황성을 향하는 경향을 가지며, 트레이닝에서 추가 선호 정렬 없이, 이것은 그것의 메트릭들에 영향을 미칠 수 있다.

### Med-PaLM M은 새로운 의료 작업 및 개념에 대한 제로 샷 일반화를 보여줍니다.

언어를 공통 접지로 하는 일반의 생의학 AI 시스템을 다양한 작업에 걸쳐 훈련하면 시스템이 다른 작업에 대해 배운 지식(즉, 조합)을 결합하여 새로운 작업을 해결할 수 있다.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Task Type & Modality & Dataset & Metric & Med-PaLM M & Med-PaLM M & Med-PaLM M \\  & & & & (12B) & (84B) & (562B) \\ \hline \multirow{3}{*}{Question Answering} & \multirow{3}{*}{Text} & MedQA & Accuracy & 29.22\% & 46.11\% & **69.68\%** \\  & & MedMCQA & Accuracy & 32.20\% & 47.60\% & **62.59\%** \\  & & PubMedQA & Accuracy & 48.60\% & 71.40\% & **80.00\%** \\ \hline \multirow{3}{*}{Report Summarization} & \multirow{3}{*}{Radiology} & ROUGE-L & 29.45\% & 31.47\% & **32.03\%** \\  & & MIMIC-III & BLEU & 12.14\% & **15.36\%** & 15.21\% \\  & & & F1-RadGraph & 31.43\% & 33.96\% & **34.71\%** \\ \hline \multirow{3}{*}{Visual Question Answering} & \multirow{3}{*}{Radiology} & VQA-RAD & BLEU-1 & 64.02\% & 69.38\% & **71.27\%** \\  & & & F1 & 50.66\% & 59.90\% & **62.06\%** \\  & & & BLEU-1 & 90.77\% & **92.70\%** & 91.64\% \\  & & & & F1 & 86.22\% & **89.28\%** & 87.50\% \\  & & Pathology & Path-VQA & BLEU-1 & 68.97\% & 70.16\% & **72.27\%** \\  & & & F1 & 57.24\% & 59.51\% & **62.69\%** \\ \hline \multirow{3}{*}{Report Generation} & \multirow{3}{*}{Chest X-ray} & \multirow{3}{*}{MIMIC-CXR} & Micro-F1-14 & 51.41\% & **53.56\%** & 51.60\% \\  & & & Macro-F1-14 & 37.31\% & **39.83\%** & 37.81\% \\  & & & Micro-F1-5 & 56.54\% & **57.88\%** & 56.28\% \\  & & & Macro-F1-5 & 50.57\% & **51.60\%** & 49.86\% \\  & & & F1-RadGraph & 25.20\% & **26.71\%** & 26.06\% \\  & & & BLEU-1 & 30.90\% & **32.31\%** & 31.73\% \\  & & & BLEU-4 & 10.43\% & 11.31\% & **11.50\%** \\  & & & ROUGE-L & 26.16\% & 27.29\% & **27.49\%** \\  & & & CIDer-D & 23.43\% & **26.17\%** & 25.27\% \\ \hline \multirow{3}{*}{Image Classification} & \multirow{3}{*}{Chest X-ray} & MIMIC-CXR & Macro-AUC & 76.67\% & 78.35\% & **79.09\%** \\  & & (5 conditions) & Macro-F1 & 38.33\% & 36.83\% & **41.57\%** \\ \cline{1-1}  & & & Macro-AUC & 95.57\% & **97.27\%** & 96.08\% \\ \cline{1-1}  & & & Macro-F1 & 78.42\% & **84.32\%** & 77.03\% \\ \cline{1-1}  & & & Macro-AUC & 66.29\% & **71.76\%** & 71.42\% \\ \cline{1-1}  & & & Macro-F1 & 29.81\% & **35.70\%** & 33.90\% \\ \cline{1-1}  & & & CBIS-DDSM & Macro-AUC & 70.11\% & 73.09\% & **73.31\%** \\ \cline{1-1}  & & (mass) & Macro-F1 & 47.23\% & 49.98\% & **51.12\%** \\ \cline{1-1}  & & CBIS-DDSM & Macro-AUC & 81.40\% & **82.22\%** & 80.90\% \\ \cline{1-1}  & & (calcification) & Macro-F1 & **67.86\%** & 63.81\% & 63.03\% \\ \cline{1-1}  & & & Indel-F1 & 96.42\% & **97.04\%** & 95.46\% \\ \cline{1-1}  & & & SNP-F1 & **99.35\%** & 99.32\% & 99.16\% \\ \hline \hline \end{tabular}
\end{table}
표 3: **모델 규모에 걸쳐 MultiMedBench에서 Med-PaLM M의 성능** 3가지 모델 규모 변형 12B, 84B, 562B에서 Med-PaLM M의 성능을 요약합니다. 모든 모델은 멀티메드벤치의 동일한 작업 세트에서 미세 조정되고 평가되었다. 우리는 스케일링이 언어 전용 태스크와 시각적 질문 응답과 같은 추론을 필요로 하는 멀티모달 태스크에서 핵심적인 역할을 한다는 것을 관찰한다. 그러나, 스케일링은 영상 분류 및 흉부 X-ray 보고서 생성 작업에 대한 이점을 감소시킨다.

일반화). 우리는 Med-PaLM M이 새로운 의료 개념과 보이지 않는 작업에 제로 샷 방식으로 일반화할 수 있음을 시사하는 예비 증거를 강조한다. 우리는 Med-PaLM M의 비상 능력 [13]으로서 제로 샷 멀티모달 추론을 추가로 관찰한다. 마지막으로, 모델의 다중 작업, 멀티모달 훈련의 결과로 긍정적인 작업 전달의 이점을 보여준다.

#### 6.2.1 새로운 의학 개념에 대한 일반화의 증거

몽고메리 카운티(MC) 데이터 세트의 흉부 X선 이미지에서 결핵(TB) 이상을 감지하는 능력을 평가하여 보이지 않는 의료 개념에 대한 Med-PaLM M의 제로 샷 일반화 능력을 조사했다. 표 4에 도시된 바와 같이, Med-PaLM M은 이 데이터셋에 최적화된 특화된 앙상블 모델에 의해 얻어진 SOTA 결과와 비교하여 경쟁적으로 수행되었다[74]. 우리는 MultiMedBench의 다른 의료 이미지 분류 작업에 대한 결과와 일치하는 세 가지 모델 변형에서 유사한 성능을 관찰했다. 분류 작업이 개방형 질문 응답 작업으로 설정되었기 때문에 가능한 각 클래스의 정규화된 예측 확률을 요구하는 AUC 메트릭을 보고하지 않았다.

#### 6.2.2 Evidence of emergent zero-shot multimodal medical reasoning

또한 MC TB 데이터 세트에서 Med-PaLM M의 CoT(zero-shot chain-of-thought) 능력을 정성적으로 탐구했다. 분류 설정과 달리 텍스트 전용 예제를 사용하여 모델에 예/아니오 분류 예측 외에 주어진 이미지에서 결과를 설명하는 보고서를 생성하도록 촉구했다. 그림 3에서 Med-PaLM M 84B 및 562B 변형에서 제로 샷 CoT 추론의 정성적 예를 제시한다. 특히, 두 Med-PaLM M 변이체는 정확한 위치에서 주요 TB 관련 병변을 식별할 수 있었다. 그러나 전문 방사선사 검토에 따르면 모델 생성 보고서에 아직 일부 누락 및 오류가 있어 개선의 여지가 있다. Med-PaLM M 12B가 일관성 있는 시각적 조건 응답을 생성하지 못했다는 점은 주목할 만하며, 이는 언어 모델의 스케일링이 제로 샷 CoT 멀티모달 추론 능력(즉, 이것은 창발적 능력[13])에서 핵심적인 역할을 한다는 것을 나타낸다.

#### 6.2.3 새로운 작업에 대한 일반화의 증거

Med-PaLM M은 단일 시점 흉부 X선 이미지 입력으로만 훈련되었지만, 우리는 다중 시점 시각적 입력을 사용하여 새로운 작업 설정으로 일반화하는 모델의 능력을 관찰했다. 특히, MIMIC-CXR의 연구 하위 집합에서 각 보고서는 정면 및 측면 뷰 X선 이미지를 모두 동반한다. Med-PaLM M은 표 5에서 설명한 단일 시점 보고서 생성 작업에 필적하는 제로 샷 성능을 달성할 수 있음을 알 수 있다. 이러한 능력은 의료 영상 연구가 최적의 성능을 위해 기존 사례 외에도 이전 연구의 해석에서 이점을 얻을 수 있다는 점을 감안할 때 유망하다.

#### 6.2.4 긍정적인 작업 전달의 증거

양식과 작업에 걸친 공동 훈련에서 발생하는 긍정적인 작업 전달을 입증하기 위해 작업 혼합물에서 MIMIC-CXR 분류 작업을 제외하여 Med-PaLM M 84B 변형을 훈련하고 이 모델 변형을 전체 MultiMedBench 혼합물에서 훈련된 Med-PaLM M 84B와 비교하는 절제 연구를 수행했다. 표 6에서 볼 수 있듯이 두 보고서에 대해 공동으로 훈련된 모델을 관찰했다.

\begin{table}
\begin{tabular}{c c c} \hline \hline Model & \# Training samples & Accuracy \\ \hline SOTA [74] & **138** & **92.60\%** \\ Med-PaLM M (12B) & 0 & 86.96\% \\ Med-PaLM M (84B) & 0 & 82.60\% \\ Med-PaLM M (562B) & 0 & 87.68\% \\ \hline \hline \end{tabular}
\end{table}
표 4: **결핵(TB) 탐지 작업에 대한 Med-PaLM M의 제로 샷 분류 성능.** Med-PaLM M은 모델 앙상블을 사용하여 Montgomery County TB 데이터 집합에서 피니튜닝된 SOTA 모델 [74]과 경쟁적으로 수행합니다. 특히 Med-PaLM M은 데이터 세트의 모든 샘플에 대한 교육이 필요한 전문가 모델과 달리 단일 텍스트 전용 예시(작업별 이미지 없음 및 따라서 제로 샷)로 구성된 간단한 작업 프롬프트로 이 결과를 달성한다.

생성 및 분류는 모든 보고서 생성 메트릭에 대해 전반적으로 더 높은 성능을 가지고 있다. 또한 흉부 X선 보고서 생성에만 훈련된 모델이 더 높은 매크로 F1 점수로 입증된 바와 같이 강력한 성능을 가진 제로 샷 방식으로 이상 검출에 일반화할 수 있음을 관찰했다. 이것은 모델이 더 복잡한 보고서 생성 작업에 대한 훈련으로부터 이상 유형을 구별하도록 학습하는 새로운 작업 설정으로의 일반화의 또 다른 예이다.

### Med-PaLM M은 모델 규모에서 방사선 보고서 생성에 대해 권장 사항을 수행 합니다.

Med-PaLM M의 임상적 적용 가능성을 더 이해하기 위해 모델 생성 흉부 X선 보고서(및 참조 인간 기준선)에 대한 방사선사 평가를 수행했다. 이 평가 프레임워크에서 아래에 자세히 설명된 대로 모델 규모에 걸쳐 Med-PaLM M 생성 보고서의 장려 품질을 관찰한다.

#### 6.3.1 Side-by-side evaluation

측면 평가에서 4명의 임상의 평가자는 MIMIC-CXR 데이터 세트의 방사선사가 제공한 참조 보고서를 다른 Med-PaLM M 모델 척도(12B, 84B 및 562B)에 의해 생성된 보고서와 비교하여 4개의 방사선학 보고서의 품질을 평가했다.

그림 3(a)는 각 평가자가 3개의 Med-PaLM M 변형 또는 참조 보고서 중 하나에 의해 생성된 보고서를 4개의 후보 보고서 중 최고로 순위를 매긴 빈도를 요약한다. 4명의 평가자 모두에서 평균, 방사선사가 제공한 참조 보고서는 사례의 37.14%에서 가장 높았고 Med-PaLM M(84B)이 그 뒤를 이었다.

도 3: **Med-PaLM M을 사용한 새로운 제로 샷 멀티모달 의학적 추론의 증거**. 대형 Med-PaLM M 모델은 흉부 X선 영상에서 결핵 관련 소견을 식별하고 설명하는 데 있어 제로 샷 CoT 추론 능력을 나타낸다. 모델은 주어진 X-선 이미지에서 결과를 설명하는 보고서를 생성하기 위해 작업별 명령어와 텍스트 전용 예시(해당 이미지 없음)로 프롬프트된다. Med-PaLM M 84B 및 562B의 모델 예측은 전문 방사선 전문의의 주석과 함께 표시된다. 두 모델 모두 우상엽의 주요 TB 관련 공동 병변을 정확하게 국소화했다. 그러나 두 모델 모두 좌상엽의 작은 공동 병변을 다루지 않았다(Med-PaLM M 562B는 우중엽의 불투명도를 암시하고 왼쪽 폐의 잘못된 진술을 분명히 하지 않았기 때문에 이 예에서 Med-PaLM M 64B보다 나은 것으로 간주되었다). 특히 Med-PaLM M 12B는 일관성 있는 보고서를 생성하지 못했으며, 이는 제로 샷 COT 추론을 위한 스케일링의 중요성을 나타냅니다.* *

그 외 모형 척도인 12B와 562B는 각각 19.49%와 17.59%로 가장 높게 나타났다.

각 Med-PaLM M 모델 척도에 의해 생성된 보고서를 방사선사가 제공한 참조 보고서와 직접 비교할 수 있도록 4방향 순위로부터 쌍별 선호도를 도출하고 그림 3(b)의 각 평가자와 모델 척도에 대한 분석을 제공했다. 4명의 평가자 모두에 대해 평균을 낸 Med-PaLM M 84B는 사례의 40.50%에서 참조 보고서보다 선호되었고, 다른 두 모델 척도인 12B 및 562B가 각각 사례의 34.05% 및 32.00%에서 참조 보고서보다 선호되었다.

#### 6.3.2 Independent evaluation

Med-PaLM M에 의해 생성된 결과 단락에서 확인된 누락 및 오류 비율을 보고한다. 도 5는 모델 스케일(12B, 84B, 562B)에 의한 분해를 제공한다. 우리는 누락과 오류에 대해 다른 경향을 관찰했다. 누락의 경우 Med-PaLM M 12B 및 84B 모델 모두에서 보고서당 평균 0.12(95% CI, 0.10 - 0.15) 누락의 가장 낮은 비율을 관찰했으며 562B 모델의 경우 0.13(95% CI, 0.11 - 0.16)이 그 뒤를 이었다.

대조적으로, Med-PaLM M 84B의 경우 0.25(95% CI, 0.22 - 0.28)의 가장 낮은 평균 오차율을 측정한 다음 Med-PaLM M 12B의 경우 0.28(95% CI, 0.24 - 0.31) 및 562B 모델의 경우 0.29(95% CI, 0.25 - 0.32)를 측정했다. 특히, 이 오류율은 MIMIC-CXR에 대한 인간 방사선 전문의 기준선에 대해 보고된 것과 유사하다.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dataset & Metric & Med-PaLM M (84B) &
\begin{tabular}{c} Med-PaLM M (84B) \\ No CXR classification \\ \end{tabular} \\ \hline \multirow{8}{*}{MIMIC-CXR} & Micro-F1-14 & **53.56\%** & 52.94\% \\  & Macro-F1-14 & **39.83\%** & 38.92\% \\  & Micro-F1-5 & **57.88\%** & 57.58\% \\  & Macro-F1-5 & **51.60\%** & 51.32\% \\ \cline{1-1}  & F1-RadGraph & **26.71\%** & 26.08\% \\ \cline{1-1}  & BLEU-1 & **32.31\%** & 31.72\% \\ \cline{1-1}  & BLEU-4 & **11.31\%** & 10.87\% \\ \cline{1-1}  & ROUGE-L & **27.29\%** & 26.67\% \\ \cline{1-1}  & CIDEr-D & **26.17\%** & 25.17\% \\ \hline MIMIC-CXR & Macro-AUC & **78.35\%** & 73.88\% \\ (5 conditions) & Macro-F1 & 36.83\% & **43.97\%** \\ \hline \hline \end{tabular}
\end{table}
표 6: **CXR 보고서 생성과 비정상 분류 간의 긍정적인 작업 전달** 흉부 X선 보고서 생성과 분류 작업 모두에 대해 공동으로 훈련 된 Med-PaLM M 모델을 사용 하 여 다중 작업 훈련의 결과로 긍정적인 전달을 관찰 합니다. 흉부 X선 보고서 분류 없이 훈련된 Med-PaLM M 모델에 비해 보고서 생성 메트릭에서 더 높은 성능을 나타낸다. 또한 흉부 X선 보고서 생성 작업에 대한 훈련만으로도 Med-PaLM M이 제로 샷 방식으로 이상 검출에 일반화할 수 있음을 관찰했다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\ \hline Micro-F1-14 & 44.20\% & 49.80\% & **50.54\%** & 48.85\% \\ Macro-F1-14 & 30.70\% & 37.69\% & **37.78\%** & 37.29\% \\ Micro-F1-5 & 56.70\% & 54.49\% & **56.37\%** & 54.36\% \\ Macro-F1-5 & N/A & 48.33\% & **51.23\%** & 48.49\% \\ F1-RadGraph & 24.40\% & 26.73\% & **28.30\%** & 27.28\% \\ BLEU-1 & **39.48\%** & 33.31\% & 34.58\% & 33.83\% \\ BLEU-4 & **13.30\%** & 11.51\% & 12.44\% & 12.47\% \\ ROUGE-L & **29.60\%** & 27.84\% & 28.71\% & 28.49\% \\ CIDEr-D & **49.50\%** & 27.58\% & 29.80\% & 29.80\% \\ \hline \hline \end{tabular}
\end{table}
표 5: **2 시점 흉부 X-ray 보고서 생성으로의 제로 샷 일반화** Med-PaLM M 성능은 이전에 두 개의 시각적 입력으로 훈련되지 않았음에도 불구하고 새로운 2 시점 보고서 생성 작업 설정에서 경쟁력을 유지합니다. Med-PaLM M은 두 개의 뷰 리포트 생성 태스크에 대한 임상 효능 메트릭에 대한 SOTA 결과를 달성한다.

dataset in a prior study[14].

우리의 분석이 임상 관련성의 오류로 제한되어 임상 해석에 대한 특정 초점을 보장한다는 점을 언급하는 것이 중요하다. 여기에는 임상 소견의 존재, 위치 또는 중증도와 관련된 오류가 포함된다. 비임상 오류의 예는 훈련 인공물에서 비롯된 견해 또는 존재하지 않는 이전 연구를 참조하는 대목이다.

모델 척도에 걸친 이러한 경향은 방사선사 평가자에 의해 유의미한 것으로 표시된 누락 및 오류의 하위 집합에 대해 동일했다. 비임상 오류를 포함한 오류 및 누락 비율에 대한 개요는 표 A.8을 참조한다.

그림 6에서 우리는 표적 참조 보고서와 함께 세 가지 모델 크기에 걸쳐 Med-PaLM M에 의해 생성된 흉부 X선 보고서의 정성적 예를 보여준다. 이 예를 위해 방사선 전문의 패널은 Med-PaLM M 12B 보고서가 임상적으로 유의미한 오류 2개와 누락 1개, Med-PaLM M 84B 보고서가 오류 0개와 누락 0개, Med-PaLM M 562B 보고서가 임상적으로 중요하지 않은 오류 1개와 누락 없음으로 판단했다.

그림 4: **사이드 바이 사이드 인간 평가** 4명의 임상의 평가자는 MIMIC-CXR의 방사선사 제공 참조 보고서를 다른 Med-PaLM M 모델 규모 변형(12B, 84B, 562B)에 의해 생성된 보고서와 비교하여 사이드 바이 사이드 평가에서 4개의 방사선학 보고서의 품질을 순위화했습니다.

도 5: **독립적인 인간 평가.** Med-PaLM M에 의해 생성된 방사선학 보고서에서 임상의 평가자에 의해 식별된 누락 및 임상 오류의 비율. 임상 오류는 임상 소견의 존재, 위치 또는 중증도와 관련된 오류이다.

## 7 Discussion

우리가 아는 한 Med-PaLM M은 광범위한 의료 양식을 해석하고 다양한 작업에 대해 유능하게 수행(이전 SOTA 근처 또는 초과 포함)하고 보이지 않는 생물의학 개념과 작업을 일반화할 수 있는 일반주의 생물의학 AI 시스템의 첫 번째 시연이다. 이것은 잠재적으로 과학적 발견에서 돌봄 전달에 이르는 응용 프로그램에서 새로운 가능성을 열어준다. 우리는 이 개발의 의미와 아래의 도전과 한계에 대해 자세히 설명한다.

**벤치마크 부족 일반 의사 생물의학 AI 개발을 위한 주요 병목 현상** 지금까지의 AI 진행은 대체로 고품질 벤치마크 개발에 의해 촉진되었습니다. 여러 단일 작업 생체 의료 AI 데이터 세트가 존재하지만, 이를 통합하고 일반 의사 생체 의료 AI 시스템 개발을 위한 벤치마크를 만들려는 시도는 제한적이었다. 멀티메드벤치의 큐레이션은 충족되지 않은 요구를 해결하기 위한 단계입니다. 그러나 벤치마크는 개별 데이터 세트의 제한된 크기(\tilde{1}\)백만 샘플의 누적 크기)와 제한된 양식 및 작업 다양성(예: 전사체학 및 단백질체학과 같은 생명과학이 부족함)을 포함한 몇 가지 중요한 한계를 가지고 있다. 훨씬 더 다양한 생물의학 데이터 유형에 걸쳐 사용하기 위한 모델 개발에 대한 또 다른 주요 장벽은 대규모 멀티모달 데이터 세트의 부족이며, 이는 디코더와 모달리티별 인코더의 공동 학습 및 정렬을 허용한다.

**의료 피니튜닝 및 전문화의 중요성** PaLM-E는 다양한 비전 언어 및 구현 로봇 작업에 대한 SOTA 성능에서 알 수 있듯이 매우 유능한 일반주의 AI 모델입니다. 그러나 MultiMedBench에 대한 기존 성능은 좋지 않았으며 Med-PaLM M은 모델 규모에 걸쳐 큰 마진으로 이를 능가한다. 이 결과는 도메인별 생의학 데이터를 사용한 피니튜닝이 생의학 작업에서 좋은 성능을 달성하는 데 중요하다는 것을 시사하며, 이는 아마도 도메인이 비의료 작업 및 양식에 비해 전반적으로 제시하는 분포 이동 때문일 수 있다.

**멀티모달 AI 모델 크기 조정은 어렵습니다* * 언어 도메인에서 모델을 크기 조정하면 성능과 창발적 기능이 크게 향상됩니다. 그러나 우리의 예비 실험은 의료 데이터 부족으로 인해 생물의학 작업 영역의 멀티모달 일반주의자 모델에 대해 이것이 더 어려울 가능성이 있음을 시사한다. 일반주의 모델이 이해하고 해결해야 할 다양한 모달리티와 작업을 고려할 때 이러한 다양한 모달리티에 대한 인코더를 공동으로 확장하는 것이 중요하다.

그림 6: **참조 및 Med-PaLM M에서 생성된 흉부 X선 보고서의 정성적 예** 대상 참조 보고서와 함께 모델 척도에 걸쳐 Med-PaLM M에서 생성된 흉부 X선 보고서의 정성적 예를 제시합니다. 이 예에서 방사선 전문의 패널은 Med-PaLM M 12B 보고서가 임상적으로 유의미한 오류 2개와 누락 1개를 갖고, Med-PaLM M 84B 보고서가 오류 0개와 누락 0개를 갖고, Med-PaLM M 562B 보고서가 임상적으로 중요하지 않은 오류 1개와 누락이 없다고 판결했다.

언어 모델. 그렇지 않으면, 모달리티들의 조합으로부터의 데이터의 해석을 필요로 하는 태스크들의 경우, 성능은 결국 가장 약한 인코더에 의해 병목될 것이다. 유방 촬영 및 피부과와 같은 의료 영상 분류 작업에서 이러한 증거가 발견되는데, 언어 모델 구성 요소를 스케일링하는 것은 잠재적인 주요 병목 현상이 비전 인코더이기 때문에 작업 성능에 거의 영향을 미치지 않는다. 멀티메드벤치의 적은 양의 의료 데이터는 자연 이미지에 대해 사전 훈련된 ViT를 의료 도메인에 효과적으로 적응시키기에 충분하지 않아 모델 스케일링의 이점을 제한할 수 있다. 따라서, 우리의 연구는 모델 스케일링이 생체 의료 작업 성능에 미치는 영향에 대한 몇 가지 초기 통찰력만 제공한다. 충분한 양의 생체 의학 데이터로 언어 모델의 스케일링 효과를 양식별 인코더의 스케일링 효과와 구별하여 모델 스케일링의 효과를 완전히 이해하기 위해서는 향후 연구가 필요하다.

일반의 생의학 AI Med-PaLM M에 대한 기술적 고려 사항은 최첨단 비전과 ViT 및 PaLM과 같은 언어 구성 요소를 기반으로 한다. 그러나 이를 통합하려면 시각적 인코더 출력에 할당된 토큰 길이, 모델의 전체 컨텍스트 길이, 샘플링 전략, 훈련 데이터 혼합물 등에 대한 신중한 고려가 필요하다. 또한, 더미 이미지 토큰과 함께 원샷 트레이닝을 사용하는 것과 같은 간단하지만 중요한 기술은 최종 모델의 품질과 계산 효율에 중요한 차이를 만든다. AI 시스템의 일반성이 증가함에 따라 신중한 고려가 필요한 세부 사항도 증가하는 경향이 있다. 우리는 또한 현재 설정으로서의 Med-PaLM M 아키텍처가 소수의 샷 인 컨텍스트 학습에 최적이 아니라는 점에 주목한다.

영상의학 보고서 생성을 위한 AI의 발전 Med-PaLM M 생성 영상의학 보고서의 방사선 전문의에 의한 평가는 도전적인 멀티모달 작업에 대한 모델의 성능을 장려하는 것을 시사한다. 사례의 최대 40.50%에서 Med-PaLM M 생성 보고서가 인간 생성 참조 보고서보다 선호되었다. 또한 모델 반응 내에서 임상적으로 유의미한 오류의 평균 수는 동일한 데이터 세트에 대한 이전 연구 [14]에서 인간 생성 보고서에 대해 보고된 것과 비슷하다. 이러한 유망한 결과는 자동 방사선 보고서 생성 작업에서 신속한 개발을 뒷받침하고 향후 임상적 유용성의 가능성을 시사한다.

일반론 에이전트들은 멀티모달 생의학 AI에 대한 유일한 접근법이 아니다. 일반론 생의학 AI 시스템들은 흥미로운 가능성들을 제공하는 반면[43], 데이터 가용성, 사전 훈련된 모델들, 컴퓨팅 및 애플리케이션 시나리오들에 따라 더 적용가능할 수 있는 멀티모달 생의학 AI 시스템들을 개발하기 위한 다른 접근법들이 있다. 여기에는 어댑터 층이 있는 냉동 인코더를 활용하여 멀티모달 바이오메디컬 AI 시스템을 접착하거나 도구 사용을 통해 전문 바이오메디컬 인코더 또는 작업별 에이전트와 인터페이스할 수 있는 LLM을 개발하는 것[75]이 포함된다.

일반주의 생의학 AI의 실제 응용에 대한 고려 사항 일반적으로 가능한 생의학 AI 시스템의 개발은 흥미진진하지만, 그러한 시스템이 실제 또는 새로운 응용의 문을 여는 데 유용하려면 전문화된 단일 작업 모델을 일치시키거나 초과하거나 그렇지 않으면 임상적으로 적용 가능한 성능 수준에 도달해야 한다. 이 작업의 범위를 벗어나지만, 여기서의 진행은 그러한 시스템의 개발 및 검증에서 안전과 형평성에 대한 신중한 고려를 필요로 한다.

## 8 Perspective of Generalist Biomedical AI

단일 모델 가중치 세트로 다양한 범위의 생의학 작업에 SOTA 근처 또는 이상에 도달하는 것은 일반주의 생의학 AI 시스템 개발에 주목할 만한 이정표이다. 인간 임상의는 "일반 실습"을 위해 훈련할 수 있지만 유용한 하위 전문 분야 전문 지식은 종종 다른 전문가[78]에서 발견되며, 비전문 임상의는 치료 과정에서 전문가의 의견을 참조할 수 있다. 또한 여러 의사 전문 분야가 치료 분만에서 함께 일하는 것은 일반적이다. 우리는 일반 의사 및 전문 AI 시스템이 전문 임상의 및 연구원과 긴밀한 피드백 루프에서 상호 작용하고 협력하여 생물의학의 큰 문제를 해결하는 생물의학 AI의 유사한 미래를 예상한다.

서로 다른 작업과 컨텍스트에 걸쳐 강력한 성능에 도달하는 단일 일반주의 생의학 AI에 대한 우리의 발견은 응용 프로그램에 영향을 미치는 새로운 영역을 암시한다. 여기에는 새로운 영역에서 거의 제로 샷 통찰력, 생물의학의 별개의 영역에서 통찰력을 통합하는 발견 도구, 다양한 분야의 전문 지식에 대한 접근을 제공하는 공통 보조 지점으로서의 잠재력이 포함된다.

## 9 Conclusion

의학은 다학제적인 노력이다. 대규모로 멀티모달 의료 데이터를 효과적으로 동화 및 인코딩하고 새로운 임상 상황에 빠르게 적응하는 일반주의 생의학 AI 시스템은 차세대 학습 의료 시스템의 기초가 될 것이며 의료를 보다 접근 가능하고 효율적이며 공평하고 인도적으로 만들 가능성이 높다. 추가 개발과 엄격한 검증이 필요하지만 Med-PaLM M은 이러한 일반의 생의학 AI 개발을 위한 중요한 단계를 나타낸다고 믿는다.

#### Acknowledgments

이 프로젝트는 구글 리서치와 구글 딥마인드의 많은 팀들 사이의 광범위한 협력이었다. 우리는 앤드류 셀러그렌, 위안 류, 마이클 하웰, 줄리 왕, 쇼 칸난, 크리스틴 킹슬리, 로이 리, 나마 함멜, 제이 하트포드, 프레티 싱, 카비타 쿨카르니, 가브리엘 고이델, 아닐 팔레푸, 시 와이맨, 에이미 왕, 사미 라흐가, 로렌 위너, 매기 시얼스, 아니샤 움라니, 존 길리아드, 샤르비아 셰티, 에반 라포포트에게 우리의 연구 동안 귀중한 통찰력과 피드백에 감사드린다. 우리는 또한 이 프로젝트 과정에서 카렌 드살보, 주빈 가흐라마니, 제임스 마니카, 제프 딘에게 그들의 지원에 감사한다.

#### Data Availability

본 연구에서 훈련 및 평가에 사용되는 벤치마크인 MultiMedBench는 모두 오픈 소스인 비식별 데이터 세트로 구성된다. 우리는 표 1에 데이터 세트의 개요를 제시한다.

#### Code Availability

본 연구에서 사용한 대용량 언어 모델(LLM)을 오픈 소스화할 수 없을 것이다. 기본 방법론에 대한 포괄적인 세부 정보를 제공하고 이전에 자세한 모델 [8, 10]을 기반으로 하여 다른 종류의 LLM에서도 유사한 접근법을 시도할 수 있다.

## References

* [1] Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M. & Thrun, S. Dermatologist-level classification of skin cancer with deep neural networks. nature542, pp. 115-118. Cited by: SS1.
*[2]V. 굴산 팽민 Cormam, M. C. Stumpe, D. Wu, A. Narayanaswamy, S. 베누고팔란 위드너 Madams, J. Cuadros, and et al. (2016) Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus 사진. Jama316, pp. 2402-2410. Cited by: SS1.
*[3]N. 토마세프 클리오토 Zielinski, H. Askham, A. Saraiva, A. Mottram, C. Meyer, S. Ravuri, and et al. (2019) A clinically applicable approach to continuous prediction of future acute kidney injury. Nature572, pp. 116-119. Cited by: SS1.
*[4]S. M. McKinney, M. 시니익 고돌 안트로포바 백민 Chesus, G. S. Corrado, A. Darzi, et al.(2020) International evaluation of a AI system for breast cancer screening. Nature577, pp. 89-94. Cited by: SS1.
*[5]R. Bommasani, D. A. Hudson, E. Adeli, R. 알트만 아로라 von Arx, M. S. Bernstein, J. Bohg, A. Bosseltu, E. Brunskill, et al.(2021) On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. Cited by: SS1.
*[6]T. 브라운 만 라이더 Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, et al. (2020) Language models are few-shot learners. Neural Information Processing Systems33(1), pp. 1877-1901. Cited by: SS1.
*[7]S. Azizi, L. Culp, J. Freyberg, B. Mustafa, S. 바우어티 콘블리스 Chen, J. Tomasev, J. Mitrovic, P. Strachan, and et al.(2023) Robust and data-efficient generalization of self-supervised machine learning for diagnostic imaging. Nature Biomedical Engineering1-24. 인용: SS1.
*[8]A. 조우다리 데블린 Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2022) PALM: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Cited by: SS1.
*[9]K. 신할 유태진 마다비 Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, et al.(2022) Large Language Models Encode Clinical Knowledge. arXiv preprint arXiv:2212.13138. Cited by: SS1.
*[10]D. Driess, F. Xia, M. S. Sajadi, M. Lynch, C. Lynch, E. Chowdhary, A. Ichter, A. Wahid, J. Tompson, Q. 붕태 복영 황영 체보타, P. 덕워스, D. 레빈, V. 반우케 하우즈만 투생기 Greff, A. Zeng, and I. K. Mordatch (2023) PyTorch: a framework for efficient image classification. arXiv:2303.03378. Cited by: SS1.
*[11]X. 진석 왕성 Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. 굿맨, B 그리크너, B 무스타파, L Beyer, et al.(2022) Split: a jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794. Cited by: SS1.
*[12]B. 유림 장락 서림 마영 Yang, X. Wu (2021) Slake: a semantically-labeled knowledge-enhanced dataset for medical visual question answer in 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). 에 의해 인용된다: SS1.
*[13]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[14]M. Dehghani, J. Dolonga, B. Mustafa, J. Padelewski, J. Heek, J. Gilmer, A. Steiner, R. Caron, I. Gerhos, I. Alabdulmohsini, et al.(2023) Scaling vision transformers to 220 billion parameters. arXiv preprint arXiv:2302.05442. Cited by: SS1.
*[15]Z. 보로스 마리니에, D. 빈센트, E. 카리토프, O. 피에틴 로벡 샤리피 테불 Tagliasacchi, et al. (2023) AudioML: a language modeling approach to audio generation. 오디오, 음성 및 언어 처리를 위한 IEEE/ACM 트랜잭션 에 의해 인용된다: SS1.
*[16]R. Caruana(1997) Multitask learning. Machine learning28, pp. 41-75. Cited by: SS1.
*[17]S. Thrun (1998) Learning to learn 181-209. Cited by: SS1.
*[18]G. E. Hinton, S. Osindero, Y. Teh(2006) A fast learning algorithm for deep belief nets. Neural computation18, pp.1527-1554. Cited by:SS1.
*[19]Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle (2006) Greedy layer-wise training of deep networks. 신경 정보 처리 시스템19에서의 진보. 인용: SS1.
*[20]P. 빈센트 Bengio, and P. Manzagol (2008) Extracting and composing robust features with denoising autoencoder. In Proceedings of the 25th international conference on Machine Learning, pp. 1096-1103. Cited by: SS1.
*[21]Y. Bengio (2012) Deep learning of representations for nonsupervised and transfer learning. Proceedings of ICML workshop on nonsupervised and transfer learning, pp. 17-36. Cited by: SS1.
*[22]J. 카플란 맥캔들리 티히건 브라운 체스 아동석 Gray, A. Radford, J. Wu, and D. Amodei (2020) Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Cited by: SS1.
*[23]J. 응암코슬라 Kim, J. Nam, H. Lee, A. Ng, and A. Y. Multimodal deep learning in ICML. 에 의해 인용된다: SS1.
*[24]C. 슈만 보몬트 C. 벤쿠 고든 와이트먼 체티티 쿤스, A. 카타, C. 멀리스, M. Wortsman, et al.(2022) Laion-5b: a open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems35, pp. 25278-25292. Cited by: SS1.
*[25]A. 재글, F. 기메노 A. 브록, O. Vinyals, O. Zisserman, and J. Carreira (2021) Perceiver: general perception with iterative attention in international conference on machine learning. 에 의해 인용된다: SS1.
*[26]M. 침푸켈리 Cabi, S. S. S. Sismiys, O. Hill, and F. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems34, pp. 200-212. Cited by: SS1.
*[27]J. L. Alayrac, P. Donahue, A. Luc, I. Mitsch, K. 바영 아멘쉬 렌치 Reynolds, et al.(2022) Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems35, pp. 23716-23736. Cited by: SS1.
*[28]A. 아고스티넬리 T. I. 덴코 보로스 버제티, G. 카이온, 큐 황, A. 얀센, A. 로버츠, M. Tagliasacchi, et al.(2023) M. et al. (2023) MusicIm: generating music from text. arXiv preprint arXiv:2301.11325. Cited by: SS1.
*[29]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. G. G. G. S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al.(2021) Learning transferable visual models from natural language supervision in international conference on machine learning. 에 의해 인용된다: SS1.
*[30]J. J. Jeong, K. 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[31]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[32]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[33]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[34]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[35]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[36]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[37]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[38]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint arXiv:2303.17579. Cited by: SS1.
*[39]J. 정기 천안리 Hartung, F. Behzadi, J. Calle, D. Osayande, M. Pohlen, S. Adithan(2023) 양방향 이미지-텍스트 매칭은 검색 기반 흉부 X-선 보고서 생성을 향상시킨다. arXiv preprint*[31] Thomee, B., Shamma, D. A., Friedland, G., Elizable, B., Ni, K., Poland, D., Borth, D. & Li, L. 제이 YFCC100M: 멀티미디어 연구의 새 데이터입니다. _ Communications of the ACM_**59,** 64-73 (2016).
* [32] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. & Fei-Fei, L. _Imagenet: A large-scale hierarchical image database_ in _2009 IEEE conference on computer vision and pattern recognition_ (2009), 248-255.
* [33] Rombach, R., Blattmann, A., Lorenz, D., Esser, P. & Ommer, B. _High-Resolution Image Synthesis with Latent Diffusion Models_ 2021. arXiv: 2112.10752 (cs.v).
* [34] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., _et al._ Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_**35,** 36479-36494 (2022).
* [35] Dhariwal, P. & Nichol, A. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_**34,** 8780-8794 (2021).
* [36] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., _et al._ Language models are unsupervised multitask learners. _OpenAI blog_**1,** 9 (2019).
* [37] Chen, T., Saxena, S., Li, L., Fleet, D. J. & Hinton, G. Pix2seq: A language modeling framework for object detection. _arXiv preprint arXiv:2109.10852_ (2021).
* [38] Collobert, R. & Weston, J. _A unified architecture for natural language processing: Deep neural networks with multitask learning_ in _Proceedings of the 25th international conference on Machine learning_ (2008), 160-167.
* [39] Ruder, S. An overview of multi-task learning in deep neural networks. _arXiv preprint arXiv:1706.05098_ (2017).
* [40] Lee, K.-H., Nachum, O., Yang, M. S., Lee, L., Freeman, D., Guadarrama, S., Fischer, I., Xu, W., Jang, E., Michalewski, H., _et al._ Multi-game decision transformers. _Advances in Neural Information Processing Systems_**35,** 27921-27936 (2022).
* [41] Lu, J., Clark, C., Zellers, R., Mottaghi, R. & Kembhavi, A. Unified-io: A unified model for vision, language, and multi-modal tasks. _arXiv preprint arXiv:2206.08916_ (2022).
* [42] Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., _et al._ A generalist agent. _arXiv preprint arXiv:2205.06175_ (2022).
* [43] Moor, M., Banerjee, O., Abad, Z. S. H., Krumholz, H. M., Leskovec, J., Topol, E. J. & Rajpurkar, P. Foundation models for generalist medical artificial intelligence. _Nature_**616,** 259-265 (2023).
* [44] Theodoris, C. V., Xiao, L., Chopra, A., Chaffin, M. D., Al Sayed, Z. R., Hill, M. C., Mantineo, H., Brydon, E. M., Zeng, Z., Liu, X. S., _et al._ Transfer learning enables predictions in network biology. _Nature_, 1-9 (2023).
* [45] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. & Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_**30** (2017).
* [46] Zhang, K., Yu, J., Yan, Z., Liu, Y., Adhikarika, E., Fu, S., Chen, X., Chen, C., Zhou, Y., Li, X., _et al._ BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks. _arXiv preprint arXiv:2305.17100_ (2023).
* [47] Li, C., Wong, C., Zhang, S., Uusyama, N., Liu, H., Yang, J., Naumann, T., Poon, H. & Gao, J. LLAVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day. _arXiv preprint arXiv:2306.00890_ (2023).
* [48] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., _et al._ Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_ (2023).
* [49] Wantlin, K., Wu, C., Huang, S.-C., Banerjee, O., Dadabby, F., Mehta, V. V., Han, R. W., Cao, F., Narayan, R. R., Colak, E., _et al._ BenchMD: A Benchmark for Modality-Agnostic Learning on Medical Images and Sensors. _arXiv preprint arXiv:2304.08486_ (2023).
* [50] Barham, P., Chowdhery, A., Dean, J., Ghemawat, S., Hand, S., Hurt, D., Isard, M., Lim, H., Pang, R., Roy, S., _et al._ Pathways: Asynchronous distributed dataflow for ML. _Proceedings of Machine Learning and Systems_**4,** 430-449 (2022).
* [51] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., _et al._ Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_ (2022).
* [52] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., _et al._ An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_ (2020).
* [53] Sun, C., Shrivastava, A., Singh, S. & Gupta, A. _Revisiting unreasonable effectiveness of data in deep learning era_ in _Proceedings of the IEEE international conference on computer vision_ (2017), 843-852.
* [54] Zhai, X., Kolesnikov, A., Houlsby, N. & Beyer, L. _Scaling vision transformers_ in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2022), 12104-12113.
* [55] Marino, K., Rastegari, M., Farhadi, A. & Mottaghi, R. _Ok-qa: A visual question answering benchmark requiring external knowledge in Proceedings of the IEEE/cvf conference on computer vision and pattern recognition_ (2019), 3195-3204.
* [56] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D. & Parikh, D. _Making the v in vqa matter: Elevating the role of image understanding in visual question answering_ in _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2017), 6904-6913.
* [57] Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M. & Le, Q. V. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_ (2021).
* [58] Shazeer, N. & Stern, M. _Adgator: Adaptive learning rates with sublinear memory cost in International Conference on Machine Learning_ (2018), 4596-4604.
* [59] Jaeger, S., Candemir, S., Antani, S., Wang, Y.-X. J., Lu, P.-X. & Thoma, G. Two public chest X-ray datasets for computer-aided screening of pulmonary diseases. _Quantitative imaging in medicine and surgery_**4,** 475 (2014).
* [60] Yu, F., Endo, M., Krishnan, R., Pan, I., Tsai, A., Reis, E. P., Fonseca, E. K. U. N., Lee, H. M. H., Abad, Z. S. H., Ng, A. Y., _et al._ Evaluating progress in automatic chest x-ray radiology report generation. _medRxiv_, 2022-08 (2022).
* [61] Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal, D., _et al._ Towards expert-level medical question answering with large language models. _arXiv preprint arXiv:2305.09617_ (2023).
* [62] Van Veen, D., Van Uden, C., Attias, M., Pareek, A., Bluthegen, C., Polacin, M., Chiu, W., Delbrouck, J.-B., Chaves, J. M. Z., Langlotz, C. P., _et al._ RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models. _arXiv preprint arXiv:2305.01146_ (2023).
* [63] Bazi, Y., Rahhal, M. M. A., Bashmal, L. & Zuair, M. Vision-Language Model for Visual Question Answering in Medical Imagery. _Bioengineering_**10,** 380 (2023).
*[*[64] Van Sosneek, T., Derakhshani, M. M., Najdenkoska, I., Snoek, C. G. & Worring, M. 언어 모델의 접두사 조정을 통한 개방형 의료 시각적 질문 응답 _ arXiv preprint arXiv:2303.05977_ (2023).
* [65] Nicolson, A., Dowling, J. & Koopman, B. Improving chest X-Ray report generation by leveraging warm-starting. _arXiv preprint arXiv:2201.09405_ (2022).
* [66] Miura, Y., Zhang, Y., Tsai, E. B., Langloz, C. P. & Jurafsky, D. Improving factual completeness and consistency of image-to-text radiology report generation. _arXiv preprint arXiv:2010.10042_ (2020).
* [67] Bannur, S., Hyland, S., Liu, Q., Perez-Garcia, F., Ilse, M., Castro, D. C., Boecking, B., Sharma, H., Bouzid, K., Thieme, A., _et al. Learning to exploit temporal structure for biomedical vision-language processing_ in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), 15016-15027.
* [68] Tanida, T., Muller, P., Kaissi, G. & Rueckert, D. Interactive and Explainable Region-guided Radiology Report Generation in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_ (2023), 7433-7442.
* [69] Rammuni Silva, R. S. & Fernando, P. Effective utilization of multiple convolutional neural networks for chest X-ray classification. _SN Computer Science_**3**, 492 (2022).
* [70] Panambur, A. B., Madhu, P. & Maier, A. _Effect of Random Histogram Equalization on Breast Calcification Analysis Using Deep Learning in Bluidereputing fur die Medizin 2022: Proceedings, German Workshop on Medical Image Computing, Heidelberg, June 26-28, 2022_ (2022), 173-178.
* [71] Poplin, R., Chang, P.-C., Alexander, D., Schwartz, S., Coltlhurst, T., Ku, A., Newburger, D., Dijamco, J., Nguyen, N., Afshar, P. T., Gross, S. S., Dorfman, L., McLean, C. Y. & DePristo, M. A. A universal SNP and small-indel variant caller using deep neural networks. _Nature Biotechnology_**36,** 983-987 (Sept. 2018).
* [72] Ye, S., Jang, J., Kim, D., Jo, Y. & Seo, M. Retrieval of Soft Prompt Enhances Zero-Shot Task Generalization. _arXiv preprint arXiv:2210.03029_ (2022).
* [73] Endo, M., Krishnan, R., Krishna, V., Ng, A. Y. & Rajpurkar, P. _Retrieval-based chest x-ray report generation using a pre-trained contrastive language-image model_ in _Machine Learning for Health_ (2021), 209-219.
* [74] Oloko-Oba, M., Viriri, S., _et al._ Ensemble of EfficientNets for the Diagnosis of Tuberculosis. _Computational Intelligence and Neuroscience_**2021** (2021).
* [75] Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P. & Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. _arXiv preprint arXiv:2303.16199_ (2023).
* [76] Schick, T., Dwivedi-Yu, J., Dess, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N. & Scialom, T. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_ (2023).
* [77] Marshall, M. _The future of general practice in England_ 2022.
* [78] Blank, L., Baxter, S., Woods, H. B., Goyder, E., Lee, A., Payne, N. & Rimmer, M. Referral interventions from primary to specialist care: a systematic review of international evidence. _British Journal of General Practice_**64,** e765-e774 (2014).
* [79] Jin, D., Pan, E., Olufatelo, N., Weng, W.-H., Fang, H. & Szolovits, P. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. _Applied Sciences_**11,** 6421 (2021).
* [80] Pal, A., Umapathi, L. K. & Sankarasububu, M. _MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering_ in _Conference on Health, Inference, and Learning_ (2022), 248-260.
* [81] Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X. PubMedQA: A dataset for biomedical research question answering. _arXiv preprint arXiv:1909.06146_ (2019).
* [82] Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi, L. & Mark, R. G. MIMIC-III, a freely accessible critical care database. _Scientific data_**3,** 1-9 (2016).
* [83] Delbruck, J.-B., Saab, K., Varma, M., Eyuboglu, S., Chambon, P., Dunnmon, J., Zambrano, J., Chaudhari, A. & Langlotz, C. _ViLM: Arctic a framework for research at the intersection of vision and language in medical AI_ in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_ (2022), 23-34.
* [84] Delbruck, J.-B., Varma, M. & Langlotz, C. P. Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. _arXiv preprint arXiv:2211.08584_ (2022).
* [85] Pacheco, A. G., Lima, G. R., Salomao, A. S., Krohling, B., Biral, I. P., de Angelo, G. G., Alves Jr, F. C., Esgario, J. G., Simora, A. C., Castro, P. B., _et al._ PAD-UFES-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones. _Data in brief_**32,** 106221 (2020).
* [86] Cubuk, E. D., Zoph, B., Shlens, J. & Le, Q. V. _Randaquement: Practical automated data augmentation with a reduced search space in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_ (2020), 702-703.
* [87] Nguyen, H. T., Nguyen, H. Q., Pham, H. H., Lam, K., Le, L. T., Dao, M. & Vu, V. VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography. _Scientific Data_**10**, 277 (2023).
* [88] Lee, R. S., Gimenez, F., Hoogi, A., Miyake, K. K., Gorovoy, M. & Rubin, D. L. A curated mammography data set for use in computer-aided detection and diagnosis research. _Scientific data_**4,** 1-9 (2017).
* [89] Olson, N. D., Wagner, J., McDaniel, J., Stephens, S. H., Westreich, S. T. & et al. PrecisionFDA Truth Challenge V2: Calling variants from short and long reads in difficult-to-map regions. _Cell Genomics_**2,** 100129 (May 2022).
* [90] DePristo, M. A., Banks, E., Poplin, R., Garimella, K. V., Maguire, J. R., Hartl, C. & et al. A framework for variation discovery and genotyping using next-generation DNA sequencing data. _Nature Genetics_**43,** 491-498 (Apr. 2011).
* [91] AlDubayan, S. H., Conway, J. R., Camp, S. Y., Witkowski, L., Kofman, E., Reardon, B., Han, S., Moore, N., Elmarakeby, H., Salari, K., Choudhry, H., Al-Rubaish, A. M., Al-Silai, A. A., Al-Ali, A. K., Taylor-Weiner, A. K. Allen, E. M. V. Detection of Pathogenic Variants With Germile Genetic Testing Using Deep Learning vs Standard Methods in Patients With Prostate Cancer and Melanoma. _JAMA_**324,** 1957 (Nov. 2020).
* [92] Liao, W.-W., Asri, M., Ebler, J., Doerr, D., Hauknes, M., Hickey, G. & et al. A draft human pangenome reference. _Nature_**617,** 312-324 (May 2023).
* [93] Thorvaldsdottir, H., Robinson, J. T. & Mesirov, J. P. Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration. _Briefings in Bioinformatics_**14,** 178-192 (Apr. 2012).
* [94] Zook, J. M., Catoe, D., McDaniel, J., Yang, L., Spies, N. & et al. Extensive sequencing of seven human genomes to characterize benchmark reference materials. _Scientific Data_**3** (June 2016).
* [95] Lau, J. J., Gayen, S., Ben Abacha, A. & Demner-Fushman, D. A dataset of clinically generated visual questions and answers about radiology images. _Scientific data_**5,** 1-10 (2018).

* [96] He, X., Zhang, Y., Mou, L., Xing, E. & Xie, P. Pathvqa: 30000+ questions for medical visual question answering. _arXiv preprint arXiv:2003.10286_ (2020).
* [97] Johnson, A. E., Pollard, T. J., Berkowitz, S. J., Greenbaum, N. R., Lungren, M. P., Deng, C.-y., Mark, R. G. & Horng, S. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. _Scientific data_**6,** 317 (2019).
* [98] Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H., Haghgoo, B., Ball, R., Shpanskaya, K., _et al.__Cherpert: A large chest radiograph dataset with uncertainty labels and expert comparison_ in _Proceedings of the AAAI conference on artificial intelligence_**33** (2019), 590-597.
* [99] Chen, Z., Song, Y., Chang, T.-H. & Wan, X. Generating radiology reports via memory-driven transformer. _arXiv preprint arXiv:2010.16056_ (2020).
* [100] Lin, C.-Y. _Rouge: A package for automatic evaluation of summaries_ in _Text summarization branches out_ (2004), 74-81.
* [101] Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. _Bleu: a method for automatic evaluation of machine translation_ in _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_ (2002), 311-318.
* [102] Jain, S., Agrawal, A., Saporta, A., Truong, S. Q., Duong, D. N., Bui, T., Chambon, P., Zhang, Y., Lungren, M. P., Ng, A. Y., _et al.__Radapgraph: Extracting clinical entities and relations from radiology reports. _arXiv preprint arXiv:2106.14463_ (2021).
* [103] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. & Chen, W. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_ (2021).
* [104] Lehman, E. & Johnson, A. _Clinical-t5: Large language models built using mimic clinical text_ 2023.
* [105] Petrini, D. G., Shimizu, C., Roela, R. A., Valente, G. V., Folgueira, M. A. A. K. & Kim, H. Y. Breast cancer diagnosis in two-view mammography using end-to-end trained efficientnet-based convolutional network. _Ieee access_**10,** 77723-77731 (2022).
* [106] Dai, W., Liu, R., Wu, T., Wang, M., Yin, J. & Liu, J. Deeply Supervised Skin Lesions Diagnosis with Stage and Branch Attention. _arXiv preprint arXiv:2205.04326_ (2022).
* [107] De Lima, L. M. & Krohling, R. _A. Exploring Advances in Transformers and CNN for Skin Lesion Diagnosis on Small Datasets_ in _Intelligent Systems: 11th Brazilian Conference, BRACIS 2022, Campinas, Brazil, November 28-December 1, 2022, Proceedings, Part II_ (2022), 282-296.
* [108] Liu, Y., Wang, Z., Xu, D. & Zhou, L. _Q2attransformer: Improving medical vqa via an answer querying decoder_ in _International Conference on Information Processing in Medical Imaging_ (2023), 445-456.
* [109] Eslami, S., de Melo, G. & Meinel, C. Does clip benefit visual question answering in the medical domain as much as it does in the general domain? _arXiv preprint arXiv:2112.13906_ (2021).
* [110] Vedantam, R., Lawrence Zitnick, C. & Parikh, D. _Cider: Consensus-based image description evaluation_ in _Proceedings of the IEEE conference on computer vision and pattern recognition_ (2015), 4566-4575.
* [111] Smit, A., Jain, S., Rajpurkar, P., Pareek, A., Ng, A. Y. & Lungren, M. P. CheXbert: combining automatic labelers and expert annotations for accurate radiology report labeling using BERT. _arXiv preprint arXiv:2004.09167_ (2020).
* [112] Liu, G., Hsu, T.-M. H., McDermott, M., Boag, W., Weng, W.-H., Szolovits, P. & Ghassemi, M. _Clinically accurate chest x-ray report generation in Machine Learning for Healthcare Conference_ (2019), 249-269.
* [113] Xu, Y., Zhang, Q., Zhang, J. & Tao, D. Vitae: Vision transformer advanced by exploring intrinsic inductive bias. _Advances in neural information processing systems_**34,** 28522-28535 (2021).
* [114] Ramesh, V., Chi, N. A. & Rajpurkar, P. _Improving radiology report generation systems by removing hallucinated references to non-existent priors_ in _Machine Learning for Health_ (2022), 456-473.

## Appendix

다음 섹션에서는 제안된 일반 모델인 Med-PaLM M의 성능을 추가로 설명하기 위해 추가 실험과 자세한 분석을 보고한다.

자세한 내용은 다음과 같습니다.

* MultiMedBench의 데이터 집합 및 작업
* Med-PaLM M 훈련 절차
* 작업 유형별 Med-PaLM M 성능 해석:
* 언어 전용 의료 질의 응답에 대한 성능 분석
* 영상의학 보고서 요약에 대한 성능 분석
* 의료 영상 분류 작업의 성능 분석
* 의료 영상 질의 응답의 성능 분석
* 흉부 X-선 보고서 생성에 대한 성능 분석
* 모델 생성 흉부 X-선 리포트의 인체 평가
* MultiMedBench 작업의 예제

### MultiMedBench

이 섹션에서는 데이터 세트, 데이터 전처리 및 작업 설정에 대한 자세한 설명을 포함하여 _MultiMedBench_에 대한 포괄적인 개요를 제공합니다. 그림 A.1은 다양한 생물의학 작업에 대한 멀티메드벤치를 요약한 것이다.

#### a.1.1 언어 전용 데이터 세트

**MultiMedQA** Med-PaLM M의 교육 및 평가를 위해 MultiMedQA [9]: MedQA [79], MedMCQA [80], PubMedQA [81] 데이터 세트의 객관식 의료 질문 응답 데이터 세트 중 3개를 사용했습니다. 이러한 질문 응답 작업은 언어 전용이며 추가 양식의 해석을 필요로 하지 않는다. 훈련 세트는 MedQA의 10,178개 질문과 MedMCQA의 182,822개 질문으로 구성된다. 테스트 세트는 MedQA의 1,273개 질문, MedMCQA의 4,183개 질문, PubMedQA의 500개 질문으로 구성된다. PubMedQA는 훈련 데이터 혼합물에 포함되지 않았으며 평가에만 사용되었다.

**MIMIC-III**는 중환자실에 입원한 환자의 의료 기록을 포함하는 대규모 공개 이용 가능한 의료 데이터베이스이다[82]. 여기에는 두 가지 영상 양식(CT 및 MRI)과 7개의 해부학적 영역(머리, 복부, 흉부, 머리, 목, 부비동, 척추, 골반)에 걸쳐 79,790개의 방사선학 보고서가 포함되어 있다. 보고서 길이와 같은 기준에 따라 총 78,875개의 보고서가 선택되었다. 훈련 및 평가를 위해 6개의 가장 일반적인 양식/해부학 쌍으로 구성된 [62]의 방사선 보고서 요약 데이터 세트를 사용했다: CT 머리, CT 복부, CT 흉부, MRI 머리, CT 척추 및 CT 목. OOD 성능을 평가하기 위해 MRI 척추, CT 동, MRI 복부, MRI 골반 및 MRI 목의 5가지 덜 일반적인 양식/해부학 쌍을 사용했다. 그 결과 총 58,405건의 교육 보고서, 7,413건의 검증 보고서, 13,057건의 테스트 보고서가 생성되었다. 보고서 생성 작업에 대한 MIMIC-CXR 데이터세트로 데이터 오염을 피하기 위해 흉부 X선 보고서는 이 데이터세트에서 제외된다는 점에 유의한다. 각 보고서에 대해 [83, 84]에서와 동일한 전처리 기능을 사용하여 결과 및 인상 섹션을 추출했다. 구체적으로, 우리는 발견 섹션이 600 토큰보다 긴 보고서를 걸러냈다. 연구 결과 섹션이 입력으로 주어진 인상 구간을 예측하여 보고서 요약 작업을 수행했는데, 이는 멀티 모달 입력이 필요하지 않은 또 다른 언어 전용 작업이다.

#### a.1.2 멀티모달 데이터 세트

**PAD-UFES-20**은 다양한 해상도, 크기 및 조명 조건을 갖는 상이한 스마트폰 디바이스로부터 수집된 피부 병변의 2,298개의 임상 이미지로 구성된다[85]. 데이터는 무료 피부 병변 치료를 제공하는 비영리 프로그램인 에스피리토 산토 연방 대학(UFES-Brazil)의 피부과 및 수술 지원 프로그램을 통해 수집되었다. 데이터 세트에는 기저 세포 암종(BCC), 악성 흑색종(MEL), 편평 세포 암종(SCC), 광선 각화증(ACK), 멜라닌 세포 모반(NEV) 및 지루성 각화증(SEK) 등 6가지 유형의 피부 병변이 포함되어 있다. 각 이미지는 환자 인구 통계, 가족 암 이력 병변 위치, 병변 크기와 같은 최대 21개의 환자 임상 특징과 연관된다. 우리는 피부 병변 이미지와 관련 임상 텍스트 특징을 멀티모달 입력으로 사용하는 언어 디코더를 통해 생성 프레임워크에서 6클래스 분류 작업을 설정했다. 구체적으로, 각 병변에 대한 메타데이터에서 _age_, _gender_, _smoke_, _drink_, _skin cancer history_, _cancer history_, _region_, _fitpatrick_, _horizontal and vertical diameter_, _itch_, _grew_, _bled_, _elevation_ 등 14개의 임상 속성을 선정하였다. 세 가지 피부암(BCC, MEL 및 SCC)과 세 가지 피부 질환(ACK, NEV 및 SEK)에 대해 등급 비율은 약 16:1:4:14:5:4이다. 공개된 공식 열차/테스트 분할이 없기 때문에 데이터 세트를 원래 클래스 비율을 보존하기 위해 계층화된 샘플링을 사용하여 훈련 세트(80%)와 테스트 테스트(20%)로 무작위로 분할했다. RandAugment [86]을 사용하여 일련의 이미지 증강 작업을 _autoContrast_, _equalize_, _invert_, _rotate_, _posterior_, _solarze_, _color_ 및 _contrast_를 포함하는 훈련 세트에 적용했다.

**VinDr-Mammo** 는 베트남 하노이의 두 병원에서 수집된 5000개의 유방 X선 영상 연구와 광범위한 유방 수준 평가 및 병변 수준 주석이 있는 총 20,000개의 그레이 스케일 이미지로 구성된 풀 필드 디지털 유방 촬영 데이터 세트입니다. [87]. 각 연구에는 왼쪽과 오른쪽 유방이 내측 외측 사위(MLO) 및 두개-미추(CC) 뷰로 이미지화되는 4개의 이미지가 포함되어 있다. 각 이미지에는 유방 영상 보고 및 데이터 시스템(BI-RADS)에 따른 유방 수준 평가가 있습니다. BI-RADS 평가는 1(음성)에서 5(악성을 크게 암시함)까지 다양하다. BI-RADS 점수 외에도 유방 밀도 수준 및 지역 이상 발견 주석이 제공된다. [49]의 설정과 유사한 유방 수준 5등급 BI-RADS 분류 작업을 수행했지만 [49]의 설정과 유사한 유방 수준 5등급 BI-RADS 분류 작업을 수행했다.

그림 A.1: **MultiMedBench 개요.** MultiMedBench는 질문 응답, 시각적 질문 응답, 이미지 분류, 방사선 보고서 생성 및 요약, 유전체 변이체 호출 등 14가지 다른 생물의학 작업을 다루는 벤치마크입니다. 멀티메드벤치는 다양한 의료 영상, 영상의학 보고서, 의료 질의 응답 및 시각적 질의 응답 쌍에서 100만 개 이상의 데이터 샘플로 구성된다.

영상의 측면성과 뷰 위치는 추가적인 맥락적 특징으로 제공되었다. 우리는 BI-RADS 1-5에서 각각 60:21:4:3:1의 클래스 비율을 가진 16,000개의 샘플을 포함하고 테스트 분할은 동일한 클래스 비율을 가진 4,000개의 샘플을 포함하는 공식 열차/테스트 분할을 사용했다. 훈련 집합의 이미지에 다음 변환을 적용했습니다. _contrast_, _equalize_, _rotate_, _shearX_, _shearY_, _translateX_ 및 _translateY_. 학습 데이터의 클래스 불균형을 완화하기 위해 각 소수 클래스(BI-RADS 2-5)에 대해 3배만큼 업샘플링했다.

**Cbis-DDSM** 은 유방 촬영 검사를 위한 디지털 데이터베이스의 큐레이션된 유방 이미지 하위 집합입니다. [88]. 이 데이터 세트에는 2,620개의 스캔된 필름 유방 촬영 연구가 포함되어 있다. VinDr-Mammo와 달리 CBIS-DDSM은 유방 수준의 BI-RADS 평가가 없다. 주석은 BI-RADS, 미묘 수준 및 병리 유형을 포함한 병변 수준에서 제공된다. 병변에는 종괴와 석회화의 두 가지 유형이 있다. 둘 다 양성, 콜백이 없는 양성 및 악성의 세 가지 가능한 병리 라벨로 주석이 달렸다. 우리는 질량 및 석회화 이상에 대해 이 데이터 세트에 대해 3등급 이상(패치 수준) 병리 분류 작업을 별도로 수행했다. 비정상 영상 패치는 전체 유방 사진에서 관심 영역(ROI)의 경계 상자에 의해 자르고 뷰 위치(CC 또는 MLO) 정보와 함께 모델 입력으로 사용된다. 우리는 두 가지 이상 유형에 대해 공식 열차/테스트 분할을 사용했다. 대량 케이스의 경우, 트레이닝 및 테스트 세트는 각각 1,318개 및 378개의 이미지(클래스 비율: 6:1:6)를 포함한다. 석회화 사례의 경우 훈련 및 테스트 세트의 총 이미지 수는 각각 1,544개 및 326개(클래스 비율: 1:1:1)이다. 두 경우 모두 VinDr-Mammo에서와 동일한 이미지 증강을 훈련 세트에 적용했다.

**PrecisionFDA Truth Challenge V2** 는 도전적인 유전체학 영역에서 호출하는 최신 변종을 벤치마킹하기 위해 개발되었습니다. [89]. 게놈 변이체 호출은 질병 유발 돌연변이를 식별할 수 있는 서열화 데이터로부터 유전자 변이체를 식별하는 것을 목표로 하는 작업이다[90]. 변이체 호출의 경우, 서열화 데이터는 참조 게놈의 좌표에 매핑된다[92]. 매핑들은 DeepVariant[71]와 같은 계산 방법들이 변이체들을 호출하기 위해 사용하는 이미지-유사 포맷, 또는 전문가들이 관심 변이체들을 검사하고 품질 관리하기 위해 사용하는 인간 친화적인 이미지 포맷으로 표현될 수 있다[93]. 이 작업을 위해 HG002 샘플에 대해 국립표준기술연구소 [94]에서 광범위하게 특성화된 접지진실을 사용했다. 우리는 PrecisionFDA Truth Challenge V2에서 시퀀싱으로 예제를 생성했다. 훈련을 위해 전체 유전체(염색체 20, 21 및 22 제외)의 예제의 4%를 사용한다. 평가를 위해 염색체 20, 염기 3000001-9444417을 사용하였다. 이를 통해 훈련용 197,038개의 후보 변이체와 평가용 13,030개의 후보 변이체를 생성하였다. 각 예에 대해 모델은 주어진 대체 대립유전자의 사본 수(0, 1 또는 2)에 해당하는 세 가지 가능한 유전자형을 예측한다. 훈련 세트는 클래스 0, 1, 2에 대해 각각 45,011, 93,246 및 58,781개의 샘플로 구성된다. 평가 집합은 클래스 0, 1, 2에 대해 각각 3,016, 6,169 및 3,845를 포함한다.

우리는 DeepVariant v1.3.0의 [71] 예제 생성 방법을 사용하여 기계 분류에 적합한 이미지와 유사한 예제를 생성했다. 구체적으로 DeepVariant v1.3.0에 대한 입력 예들은 (높이, 너비, 채널)에 해당하는 (100, 221, 6)의 형상을 갖는다. 채널은 다음의 순서로 아래 그레이-스케일로 표시된다:

1. 판독 기준: 상이한 강도들은 A, C, G 및 T를 나타낸다.
2. 베이스 품질: 시퀀싱 기계에 의해 설정된다. 화이트는 더 높은 퀄리티입니다.
3. 매핑 품질: 얼라이너에 의해 설정됨. 화이트는 더 높은 퀄리티입니다.
4. 정렬의 가닥: 검은색은 정방향이고, 흰색은 역방향이다.
5. 판독 지원 변이체: 화이트는 판독이 주어진 대체 대립유전자를 지원한다는 것을 의미하고, 그레이는 그렇지 않다는 것을 의미한다.
6. 베이스는 ref와 다르다: 화이트는 베이스가 기준과는 다르다는 것을 의미하고, 다크 그레이는 베이스가 기준과 일치한다는 것을 의미한다.

입력 예제를 Med-PaLM M(224, 224, 3)의 입력 모양과 호환되도록 재구성하기 위해 채널 1, 2, 3을 채널 4, 5, 6으로 쌓아 원래 텐서 모양(100, 221, 6)이 RGB 모양 이미지(200, 221, 3)가 되도록 했다. 그런 다음 폭 및 높이 치수에 이미지를 패딩하여 최종 모양(224, 224, 3)을 제공한다.

**VQA-RAD** 는 임상 의사가 만들고 유효성을 검사한 315개의 방사선 이미지 및 3,515개의 질문 응답 쌍으로 구성된 VQA (방사선 시각적 질문 응답) 데이터 세트입니다. [95]. 영상의학영상은 3가지 영상방법(CT, MRI, X-ray)과 3가지 해부학적 영역(머리, 복부, 가슴)에서 선택된다.

질문 유형은 양식, 평면, 장기 시스템, 이상, 크기, 평면, 위치 추론, 색상, 계수, 속성 및 기타를 포함한 11개 범주로 나뉜다. 질문-답변(QA) 쌍의 58%는 폐쇄형(예/아니오 또는 제한된 선택)이고 나머지 42%는 개방형(단답형)이다. 훈련 집합은 1,797개의 QA 쌍(자유형 및 패러프레이징된 질문만 포함)을 포함하고 테스트 집합은 451개의 QA 쌍(필터링되지 않음)을 포함하는 공식 열차/테스트 분할을 채택했다.

**Path-VQA**: 32,799개의 질문-답변 쌍이 있는 총 4,998개의 병리 이미지를 포함하는 병리학 VQA 데이터 세트입니다. [96]. 병리 이미지는 의학 교과서와 온라인 디지털 도서관에서 추출된다. 각각의 이미지는 색상, 위치, 외관, 모양 등을 포함하는 병리학의 상이한 양상들에 관한 다수의 QA 쌍들과 연관된다. 개방형 문항은 전체 문항의 50.2%를 차지하며, 무엇을, 어디서, 언제, 누구를, 어떻게, 얼마나/얼마나 많은지 등 7개 범주로 분류되어 전체 문항의 50.2%를 차지한다. 나머지는 간단한 "예/아니오" 답변으로 마무리된 질문입니다. 훈련, 검증 및 테스트 세트가 각각 19,755, 6,279 및 6,761 QA 쌍을 포함하는 공식 데이터 분할을 채택했다.

**Slake-VQA** 는 방사선 이미지에서 의미적으로 주석이 달리고 지식이 강화된 이중 언어(영어 및 중국어) VQA 데이터 세트입니다. [12]. 12개의 질병, 39개의 장기 시스템 및 3개의 영상 양식(CT, MRI 및 흉부 X선)을 포함하는 14,028개의 질문-응답 쌍이 있는 642개의 주석이 달린 이미지를 포함한다. 질문은 평면, 품질, 위치, 장기, 이상, 크기, 색상, 모양, 지식 그래프 등을 포함하는 이미지 콘텐츠의 다양한 측면과 관련된 개방형(자유 형식) 또는 폐쇄형(균형 예/아니오)이다. 훈련, 검증 및 테스트 세트에는 각각 9,849개, 2,109개 및 2,070개의 샘플이 포함되어 있다.

**MIMIC-CXR**: 자유 텍스트 방사선 보고서 [97]가 있는 흉부 방사선 사진의 대규모 데이터 세트입니다. 총 377,110개의 이미지가 65,379명의 환자를 위해 수집된 227,835개의 이미지 연구에서 데이터 세트에 사용할 수 있다. 각각의 환자는 다수의 연구를 가질 수 있고 각각의 연구는 동일한 자유-텍스트 리포트와 연관된 하나 이상의 이미지를 포함할 수 있다. MIMIC-CXR의 이미지는 전방-후방(AP), 후방-전방 및 측방(LA)과 같은 여러 뷰 위치에서 수집된다. 방사선학 보고서 및 이미지에서 보호된 건강 정보(PHI)가 제거되고, 이로 인해 보고서의 일부 문장에서 정보가 누락된다. 이 데이터 세트는 개별 환자에 대한 순차적 영상 연구를 포함하기 때문에 많은 수의 보고서가 동일한 환자에 대한 이전 연구의 정보를 참조한다. 각 보고서에는 CheXpert 레이블러[98]를 사용하여 14개의 일반적인 방사선 관찰의 구조화된 레이블이 주석을 달았다. 이 데이터 세트를 사용하여 임상 관련 병리 관찰의 흉부 X선 보고서 생성 및 이진 분류의 두 가지 작업을 수행했다. 우리는 이전 작업 [99]에 따라 보고서의 중복된 화이트 스페이스를 제거하고 표시, 결과 및 인상 섹션을 추출하여 방사선학 보고서를 전처리했다. 우리는 공식 열차/검증/테스트 분할을 사용했다. 우리는 보고서와 보고서 없이 이미지를 폐기했고, 기차 및 테스트를 통해 결과 섹션을 추출할 수 없는 보고서를 폐기했다. 또한 결과 섹션의 길이가 800자를 초과하는 보고서를 필터링했다. 그러나, 정면 뷰에만 초점을 맞추는 대부분의 이전 작업과 달리 동일한 보고서와 관련된 다른 방향의 이미지를 독립적인 샘플(테스트 데이터의 오염을 방지하기 위해 환자 수준 열차/테스트 분할을 유지)로 처리했다. 목표는 서로 다른 뷰 위치의 이미지를 처리하기 위한 모델의 이미지 이해 능력을 향상시키는 것이다. 별도의 평가에서 보고서에는 전면 뷰와 측면 뷰(2뷰 보고서 생성)가 모두 수반되는 샘플의 하위 집합도 연구했다.

보고서 생성 작업을 위해 흉부 X선 이미지와 표시 섹션(연구의 이유)의 상황 정보를 결합하여 대상 보고서의 결과 섹션을 예측했다. 훈련, 검증 및 테스트 세트의 총 샘플 수는 각각 353,542개, 2,866개 및 4,834개이다.

이진 분류 작업의 경우 발견 없음, 무기폐, 심비대, 통합, 부종, 흉막 삼출, 폐 혼탁, 종격동 비대, 골절, 폐렴 및 지원 장치의 11가지 병리학적 조건에 대해 음성 및 불확실한 레이블을 음성 클래스로 그룹화했다. 무전증, 심비대, 경화, 부종 및 흉막 삼출은 임상적 관련성과 유병률을 고려할 때 5가지 주요 조건이다. "발견 없음" 라벨은 병리학 없이 사례를 캡처하므로 이 분류 작업은 모델이 정상 사례를 모든 유형의 이상을 가진 사례와 구별하는 데 단순히 도움이 된다. 클래스 불균형으로 인해 훈련 중에 통합, 확대된 심장종격동, 골절 및 폐렴의 조건에 대해 양성 클래스를 2배 상향 샘플링했다. 이러한 이진 분류 작업은 모델이 흉부 X선 이미지에서 다른 유형의 임상 관찰을 구별하는 데 도움이 되기 때문에 동시에 훈련될 때 보고서 생성 작업에 보조적이다.

### Med-PaLM M 교육 세부 정보

#### a.2.1 Training data mixture

그림 A.2와 표 A.1은 훈련 데이터 혼합물의 혼합물 비율과 소수의 샷 작업 설정을 보여준다. 데이터 배포의 대부분은 의료 비전 언어 작업으로 15% 미만이 언어 전용 작업으로 구성된다. 대부분의 비전 언어 작업은 텍스트 전용 1샷 설정(해당 이미지 없음)으로 훈련된 반면 CBIS-DDSM 분류 및 유전체 변이 호출 작업은 0샷 설정으로 훈련되었다.

#### a.2.2 Training hyperparameters

PaLM-E는 연속적인 관찰들(예를 들어, 이미지들, 시계열)이 언어 토큰들로서 사전 트레이닝된 LLM에 의해 동일한 방식으로 처리될 수 있도록 멀티모달 입력들을 잠재 벡터들로서 동일한 언어 임베딩 공간으로 투영하고, 이에 의해 멀티모달 프롬프트가 주어지면 자동으로 텍스트 완성들을 생성한다. 실험에서 ViT는 시각적 입력을 추가 텍스트/멀티모달 토큰과 함께 LLM에 의해 추가로 처리되는 고정된 256개의 토큰으로 매핑한다[10]. Med-PaLM M은 사전 훈련된 PaLM-E 검사점에서 미세 조정되었다. 표 A.2는 Med-PaLM M 12B, 84B 및 562B에 대한 훈련 하이퍼파라미터를 각각 보여준다.

### 상세 Med-PaLM M 성능

텍스트 전용 의료 질문에 대한 성능 표 A.3에서 MedQA, MedMCQA 및 PubMedQA에 대한 Med-PaLM M의 몇 발 성능을 보고한다. SOTA 결과는 앙상블 정제 프롬프트와 함께 Med-PaLM 2에서 선택되었고 PaLM 540B 몇 발 결과는 [9, 61]에 보고되었다. Med-PaLM M은 앙상블 개선으로 얻은 Med-PaLM 2 최상의 결과에 뒤처졌음에도 불구하고 세 데이터 세트 모두에서 기준 PaLM 모델(그로부터 상속됨)을 큰 마진만큼 능가했다. 언어 모델을 8B에서 540B로 확장하면 객관식 의료 질의 응답의 정확도가 크게 향상된다.

그림 A.2: \(|\) **Med-PaLM M 데이터 혼합물 개요.** 표 A.1에 자세히 설명된 대로 MultiMedBench 데이터 세트에 걸쳐 Med-PaLM M 학습 데이터 혼합물의 혼합물 비율에 대한 요약입니다.

의학 지식을 이해하고, 회상하고, 추론할 수 있는 강력한 역량이 중요한 과제이다. 이러한 결과는 Med-PaLM 2에 사용된 개선된 기본 언어 모델에 의해 부분적으로 설명될 수 있다.

**방사선 보고서 요약에 대 한 성능** 표 4의 Van Veen _et al._[62]에서와 같이 방사선 보고서 요약 작업에 대 한 ROUGE-L [100], BLEU [101] 및 F1-RadGraph [102] 점수와 같이 일반적으로 사용 되는 메트릭을 보고 합니다. Med-PaLM M (562B)은 의료 질문 응답 작업에 대 한 관찰과 일치 하 여 더 작은 모델 변형에 비해 가장 좋은 전체 성능을 산출 했습니다. Med-PaLM M은 738M-파라미터 임상-T5 모델[104]에서 매개변수 효율적인 피니튜닝 방법(낮은 순위 적응, LoRA[103])으로 얻은 SOTA 결과보다 더 나쁜 성능을 보였다. 그러나 [62]에서 언급한 바와 같이 임상 T5의 한 가지 주의 사항은 리먼 & 존슨[104]이 MIMIC-III의 테스트 세트에서 모델을 사전 훈련하여 잠재적인 데이터 누출을 유도했는지 여부가 불분명하다는 것이다. 특히 Med-PaLM M은 PaLM 모델과 유사하게 임상 텍스트에 대해 사전 훈련되지 않은 T5 모델을 기반으로 한 Van Veen _et al._[62]의 결과와 유리하게 비교되었다.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dataset & Med-PaLM 2 & PaLM & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\ \hline MedQA (USMLE) & **86.50\%** & 58.90\% & 29.22\% & 46.11\% & 69.68\% \\ \hline MedMCQA & **72.30\%** & 54.50\% & 32.20\% & 47.60\% & 62.59\% \\ \hline PubMedQA & **81.80\%** & 55.00\% & 48.60\% & 71.40\% & 80.00\% \\ \hline \hline \end{tabular}
\end{table}
표 3: **MultiMedQA에 대한 언어 전용 의료 질의 응답 정확도.** 앙상블 정제[61] 및 PaLM 소수 샷 결과[9]를 갖는 Med-PaLM 2 결과가 비교를 위해 제시된다. 최첨단 Med-PaLM 2에 미치지 못함에도 불구하고 소수의 샷 Med-PaLM M은 해당 PaLM 기준선을 큰 마진만큼 능가한다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Task & Modality & Dataset & Mixture ratio & Few-shot setup \\ \hline \multirow{2}{*}{Question Answering} & \multirow{2}{*}{Text} & MedQA & 3.13\% & 2-shot \\  & & MedMCQA & 6.25\% & 2-shot \\ \hline \multirow{2}{*}{Report Summarization} & \multirow{2}{*}{Radiology} & MIMIC-III & 3.13\% & 0-shot \\ \hline \multirow{2}{*}{Visual Question Answering} & \multirow{2}{*}{Radiology} & VQA-RAD & 0.15\% & text-only 1-shot \\  & & Slake-VQA & 2.64\% & text-only 1-shot \\  & & Pathology & Path-VQA & 1.90\% & text-only 1-shot \\ \hline \multirow{2}{*}{Report Generation} & \multirow{2}{*}{Chest X-ray} & MIMIC-CXR & 59.90\% & text-only 1-shot \\ \hline \multirow{4}{*}{Medical Image Classification} & \multirow{2}{*}{Dermatology} & PAD-UFES-20 & 6.25\% & text-only 1-shot \\  & & VinDr-Mammo & 1.56\% & text-only 1-shot \\ \cline{1-1}  & & CBIS-DDSM & 1.56\% & 0-shot \\ \cline{1-1}  & & MINIC-CXR & 11.98\% & text-only 1-shot \\ \cline{1-1}  & & PrecisionFDA & 1.56\% & 0-shot \\ \cline{1-1}  & & Truth Challenge V2 [89] & 1.56\% & 0-shot \\ \hline \hline \end{tabular}
\end{table}
표 1: **Med-PaLM M 데이터 혼합물.** Med-PaLM M 학습 데이터 혼합물의 작업 유형, 양식, 혼합물 비율 및 소수의 샷 설정에 대한 요약입니다.

**의료 이미지 분류 작업에 대 한 성능** 표 A.5는 피부과, 방사선과 및 유전체학을 포함한 여러 양식에 걸쳐 분류 작업 세트에 대 한 Med-PaLM M의 성능을 보여 줍니다. 이러한 작업은 모두 불균형한 클래스 분포를 갖기 때문에 변이체 발견의 맥락에서 단일 뉴클레오티드 다형성(SNP) 및 짧은 삽입 및 삭제(인델)에 대한 F1 점수가 대신 사용된 유전체 변이체 호출 작업을 제외하고 분류 메트릭으로 매크로-AUC(모든 클래스별 AUC 점수의 비가중 평균) 및 매크로-F1 점수(모든 클래스별 F1 점수의 비가중 평균)를 보고했다.

VinDr-Mammo에서 Med-PaLM M의 모든 크기 변형은 매크로-AUC [49]에서 더 작은 ViT(9.7M)를 사용하여 이전 SOTA를 초과했다. CBIS-DDSM에서 본 모델은 석회화 분류에 보고된 SOTA F1 70.71%보다 질량과 석회화 분류에서 각각 51.12%와 67.86%의 가장 좋은 매크로 F1을 달성했다[70]. CBIS-DDSM에 대한 대부분의 이전 연구는 [105]에서 논의된 바와 같이 3-클래스 설정과 대조적으로 2-클래스 패치 수준 분류(양성 대 악성) 문제에 초점을 맞추었다. Pad-UFES-20에서는 공식 열차/테스트 분할을 사용할 수 없기 때문에 우리의 결과는 이전 연구와 직접 비교할 수 없다. Med-PaLM M 84B는 CNN 및 ViT 변이체[106, 107]를 사용하여 얻은 이전 보고된 결과(94% - 98%)와 동등한 97.27%의 매크로-AUC를 달성했다. MIMIC-CXR에서 무기폐, 심비대, 통합, 부종 및 흉막 삼출의 5가지 주요 조건의 이진 분류에 걸쳐 F1 점수의 거시적 평균을 보고했다. Med-PaLM M(562B)은 다양한 CNN 아키텍처의 병렬화를 사용한 ParallelXNet[69]에서 얻은 SOTA 결과 81.27%보다 약간 낮은 79.09%의 macro-AUC를 달성했다. 변이체 호출 작업에서 DeepVariant 모델 [71]은 Indel-F1 및 SNP-F1 점수 모두에서 Med-PaLM M을 능가했다. SOTA DeepVariant 모델은 2,633배 더 많은 훈련 예제로 훈련되었다. 동일한 예들을 사용한 트레이닝은 SNP(Med-PaLM M 99.35% 대 DeepVariant 99.63%) 및 Indel(Med-PaLM M 97.04% 대 DeepVariant 98.55%)에 대한 DeepVariant에 대해 더 좁은 이점을 가져왔다. 특히, Med-PaLM M은 SNP 호출에 대해 널리 사용되는 GATK4 방법 [90]의 정확도(Med-PaLM M 99.35% 대 GATK4 99.29%)를 능가했지만 Indel 호출은 그렇지 않았다(Med-PaLM M 97.04% 대 GATK4 99.32%).

종합하면 Med-PaLM M은 고도로 전문화된 SOTA 모델에 비해 단일 모델을 사용하여 다양한 분류 작업에 대해 경쟁적인 결과를 달성했다. 데이터 증강 및 클래스 밸런싱을 넘어 세밀한 작업별 커스터마이징 및 하이퍼파라미터 튜닝을 수행하지 않았다는 점에 주목할 필요가 있다. 언어 모델을 확장하는 것은 비전 인코더가 모델 성능에 대한 병목 현상이 발생할 가능성이 있는 분류 작업에 크게 도움이 되지 않을 것으로 예상된다. 더 큰 비전 모델이 모든 실험에서 작은 모델보다 성능이 우수하다는 전반적인 증거는 없으며, 이는 더 많은 도메인별 사전 훈련이 비전 인코더 성능을 개선하는 데 더 중요할 수 있음을 시사한다. 또한 여기에서 조사한 비교적 소규모 데이터 세트는 모델 규모에 걸쳐 결과가 일반적으로 서로 가까웠기 때문에 모델 크기와 작업 성능 사이에 이러한 강력한 스케일링 관계를 설정하기에 충분하지 않을 가능성이 있다.

**의료 시각적 질문에 대한 응답 성능** 시각적 입력을 조건으로 하는 개방형 언어 디코딩 작업으로 3개의 VQA 데이터 세트에서 close-end 및 open-end QA 쌍을 모두 공식화 했기 때문에 BLEU-1 및 토큰 수준 F1 점수를 사용 하 여 Med-PaLM M의 성능을 평가 했습니다. 이는 미리 정의된 고정 번호 응답 후보 집합에서 VQA를 분류 작업으로 간주하기 때문에 문자열 수준 정확도 평가 메트릭을 사용한 많은 이전 작업과 대조된다[108, 109]. 이 정확도 메트릭은 특히 개방형 생성 설정에서 지상 진실 답변의 "거의 누락"을 포착하지 못하는 약점을 가지고 있다. 또한 전문가의 인간 검증만이 토큰 수준 또는 문자열 수준 일치 메트릭을 넘어 모델 답변의 품질에 대한 추가 통찰력을 제공할 수 있다는 점에 주목했다. 표 A.6에서 볼 수 있듯이 Med-PaLM M은 세 데이터 세트와 메트릭 모두에 걸쳐 유사한 생성 접근법을 사용하여 이전 SOTA를 능가했다[63, 64]. 특히 VQA-RAD에서 언어 모델의 스케일링에 따라 모델 성능이 향상되었다.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Dataset & Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\ \hline \multirow{3}{*}{MIMIC-III} & ROUGE-L & **38.70**\% & 29.45\% & 31.47\% & 32.03\% \\  & BLEU & **16.20**\% & 12.14\% & 15.36\% & 15.21\% \\ \cline{1-1}  & F1-RadGraph & **40.80**\% & 31.43\% & 33.96\% & 34.71\% \\ \hline \hline \end{tabular}
\end{table}
표 A.4: \(|\)**Med-PaLM M 성능 MIMIC-III 방사선 보고서 요약.** 및 Path-VQA. Slake-VQA에서 중간 크기 모델 변형으로 최상의 성능을 달성했다. 이러한 결과는 언어 추론이 시각적 이해를 조건으로 하는 시각적 언어 작업에 언어 모델을 확장하는 것이 유익하다는 것을 시사한다.

흉부 X-ray 보고서 생성에 대한 성능 자동 메트릭을 사용하여 생성된 흉부 X-ray 보고서의 품질을 측정하기 위해 생성된 보고서의 사실성과 진단 정확도를 포착하기 위해 설계된 임상 효능(CE) 메트릭 및 F1-RadGraph 외에 BLEU-1, BLEU-4, ROUGE-L, CIDEr-D[110]와 같은 일반적인 자연 언어 생성 메트릭을 계산했다. 특히, 전문가 주석으로 개선된 BERT 모델을 기반으로 하는 자동 방사선 보고서 라벨러인 CheXbert [111]을 사용하여 주어진 보고서에서 14개의 CheXpert 병리학적 관찰을 추출했다. 각 관찰에 대해 예측된 레이블을 지상 진실 레이블과 비교하여 CE 메트릭을 계산했다. F1-RadGraph는 생성된 보고와 참조 보고서 사이의 중첩된 임상 개체 및 관계를 측정함으로써 CheXpert 라벨러를 통해 더 많은 관찰 범주로 일반화한다[60]. 이전 연구[65, 66, 67, 68, 14, 69, 112]에 따라 CE 메트릭에 대해 각각 5개의 주요 관찰 및 14개의 모든 관찰에 대해 평균화된 매크로-F1 및 마이크로-F1 점수를 보고했다. 표 7에서 볼 수 있듯이 Med-PaLM M은 모든 CE 메트릭 및 F1-RadGraph에서 새로운 SOTA를 달성했으며 [65, 14]의 이전 최상의 SOTA 결과에 비해 모든 임상 관련 관찰에 걸쳐 평균 매크로-F1-14 및 마이크로-F1-14에서 약 9포인트의 상당한 증가를 보였다. 거시 평균 F1은 훈련 데이터에서 매우 낮은 표현을 갖는 일부 범주에서 더 나쁜 모델 성능 때문에 14개 관찰 범주에 걸쳐 미시적 평균 F1보다 낮은 점수를 초래했다. 특히, F1 점수의 개선은 Med-PaLM M의 5개 주요 범주보다 14개 범주 모두에서 더 두드러졌다. 이는 분류 작업과 공동으로 훈련하는 이점이 있기 때문일 수 있습니다.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Dataset & Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\ \hline \multirow{2}{*}{VQA-RAD} & BLEU-1 & 71.03\% & 64.02\% & 69.38\% & **71.27\%** \\  & F1 & N/A & 50.66\% & 59.90\% & **62.06\%** \\ \hline \multirow{2}{*}{Path-VQA} & BLEU-1 & 70.30\% & 68.97\% & 70.16\% & **72.27\%** \\  & F1 & 58.40\% & 57.24\% & 59.51\% & **62.69\%** \\ \hline \multirow{2}{*}{Slake-VQA} & BLEU-1 & 78.60\% & 90.77\% & **92.7\%** & 91.64\% \\  & F1 & 78.10\% & 86.22\% & **89.28\%** & 87.50\% \\ \hline \hline \end{tabular}
\end{table}
표 6: 의료 영상 분류에 대한 \(|\)**Med-PaLM M 성능. 우리는 모든 작업에 대해 매크로 평균 AUC 및 F1을 보고한다. MIMIC-CXR의 경우 메트릭은 5 가지 주요 병리학적 조건에 대해 평균을 냅니다. **

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Dataset & \# Classes & Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\ \hline MIMIC-CXR (5 conditions) & 2-class & Macro-AUC & **81.27\%** & 76.67\% & 78.35\% & 79.09\% \\  & & Macro-F1 & N/A & 38.33\% & 36.83\% & **41.57\%** \\ \hline PAD-UFES-20 & 6-class & Macro-AUC & N/A & 95.57\% & **97.27\%** & 96.08\% \\  & & Macro-F1 & N/A & 78.42\% & **84.32\%** & 77.03\% \\ \hline \multirow{2}{*}{Variant Calling} & \multirow{2}{*}{3-class} & Indel-F1 & **99.40\%** & 96.42\% & 97.04\% & 95.46\% \\  & & SNP-F1 & **99.70\%** & 99.35\% & 99.32\% & 99.16\% \\ \hline \multirow{2}{*}{VinDr-Mammo} & \multirow{2}{*}{5-class} & Macro-AUC & 64.50\% & 66.29\% & **71.76\%** & 71.42\% \\  & & Macro-F1 & N/A & 29.81\% & **35.7\%** & 33.90\% \\ \hline CBIS-DDSM (mass) & 3-class & Macro-AUC & N/A & 70.11\% & 73.09\% & **73.31\%** \\  & & Macro-F1 & N/A & 47.23\% & 49.98\% & **51.12\%** \\ \hline CBIS-DDSM (calcification) & 3-class & Macro-AUC & N/A & 81.40\% & **82.22\%** & 80.90\% \\  & & Macro-F1 & **70.71\%** & 67.86\% & 63.81\% & 63.03\% \\ \hline \hline \end{tabular}
\end{table}
표 5: 의료 영상 분류에 대한 \(|\)**Med-PaLM M 성능. 우리는 모든 작업에 대해 매크로 평균 AUC 및 F1을 보고한다. MIMIC-CXR의 경우 메트릭은 5개의 주요 병리학적 조건에 대해 평균됩니다.* *소수 조건**. 우리는 이러한 긍정적인 작업 전달을 전문화된 단일 작업 모델에 비해 일반주의적 다중 작업 모델의 주요 이점 중 하나로 간주한다. 텍스트 오버랩 기반 자연어 생성 메트릭에서 Med-PaLM M은 기존 SOTA 결과를 능가하지 못했다. 그러나 이러한 자동 메트릭의 함정은 특히 사실적 정확성을 포착하지 못하고 방사선사 판단과 잘 일치하지 않는다는 점에서 많은 연구에 의해 제기되었다[60, 66, 112, 68].

흥미롭게도 가장 큰 모델인 Med-PaLM M(562B)은 최상의 성능을 달성하지 못했으며 Med-PaLM M(84B)에 약간 미치지 못했다. 또한, 세 가지 모델 크기에 따른 성능 차이는 모든 유형의 메트릭에 걸쳐 상대적으로 작다. 언어 모델의 크기 증가의 감소는 흉부 X선 보고서 생성을 위한 출력 공간이 템플릿 문장 집합과 제한된 수의 조건에 상당히 국한되어 있기 때문일 수 있다. 작업 수행이 주로 비전 인코더에 의해, 특히 이 도메인에 얼마나 잘 적응되는지에 의해 제한될 수도 있다. Xu _et al._[113]에서 언급한 바와 같이 ViT는 의료 영상을 해석하는 데 종종 중요한 국소 시각적 특징을 모델링하기 위한 귀납적 편향이 부족하다. 이러한 한계를 극복하기 위해 크기 스케일링의 이점을 가능하게 하기 위해 대규모 의료 훈련 데이터가 필요할 수 있다. 또한 입력 이미지 크기 \(224\times 224\times 3\)를 사용하여 해상도의 손실을 발생시켰으며, 이는 언어 모델의 문맥 길이 제한에 맞게 내장된 이미지 토큰의 길이를 단축하기 위해 만든 절충안이다.

### 인간 평가에 대한 세부 정보

그림 A.3 및 A.4는 방사선사 평가자에게 제공되는 작업 입력 및 주석 프롬프트를 포함하여 나란히 및 독립적인 방사선사 평가에 사용되는 작업 인터페이스를 보여준다. 상세한 검사(예를 들어, 미묘한 구조의 식별)의 용이성을 위해, 내장된 의료 이미지 뷰어는 줌, 감마, 및 블렌드 컨트롤을 포함하는 흉부 X선 이미지를 조정하기 위한 평가자를 위한 도구를 제공했다.

MIMIC-CXR 데이터세트에서 자명하지 않은 수의 보고서에는 이전 연구에 대한 참조(예를 들어, "이전 방사선 사진 [...]과 비교됨") 또는 여러 뷰에 대한 참조(예를 들어, "흉부의 Ap 및 측면 뷰가 비교됨")가 포함되어 있다는 점에 주목할 필요가 있다. 대조적으로, 모델에 대한 입력은 단일 연구의 단일 이미지 및 표시이다. 결과적으로, 훈련 코퍼스의 이러한 아티팩트는 모델이 존재하지 않는 이전 이미징 연구 또는 존재하지 않는 X선 뷰에 대한 참조의 환각을 일으키기 쉽도록 렌더링한다. 우리의 인간 평가 작업 인터페이스는 잘못된 구절을 "존재하지 않는 보기를 참조" 또는 "존재하지 않는 연구를 참조"로 분류하는 옵션을 제공하여 이러한 인공물의 존재를 설명했다. 향후 작업은 모델 개발 중에 이 문제를 완화하기 위해 모든 이전 참조가 제거된 MIMIC-CXR의 정리 버전인 CXR-PRO 데이터 세트[114]를 활용할 수 있다.

분석을 위해 임상 오류(즉, "동의하지 않는 발견이 있음", "발견 위치가 잘못됨")와 비임상 오류(즉, "존재하지 않는 보기를 참조" 또는 "존재하지 않는 연구를 참조")를 구별했다. 표 A.8은 서로 다른 Med-PaLM M 모델에 의해 생성된 방사선학 보고서에서 임상의 평가자가 식별한 누락 및 오류 비율을 요약한 것이다. 여기서는 모든 임상 및 비임상 오류 유형을 포함하여 총 오류 비율을 보고한다. 평균적으로 가장 성능이 좋은 Med-PaLM M

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Metric & SOTA & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\ \hline Micro-F1-14 & 44.20\% & 51.41\% & **53.56\%** & 51.60\% \\ Macro-F1-14 & 30.70\% & 37.31\% & **39.83\%** & 37.81\% \\ Micro-F1-5 & 56.70\% & 56.54\% & **57.88\%** & 56.28\% \\ Macro-F1-5 & N/A & 50.57\% & **51.60\%** & 49.86\% \\ F1-RadGraph & 24.40\% & 25.20\% & **26.71\%** & 26.06\% \\ BLEU-1 & **39.48\%** & 30.90\% & 32.31\% & 31.73\% \\ BLEU-4 & **13.30\%** & 10.43\% & 11.31\% & 11.50\% \\ ROUGE-L & **29.60\%** & 26.16\% & 27.29\% & 27.49\% \\ CIDEr-D & **49.50\%** & 23.43\% & 26.17\% & 25.27\% \\ \hline \hline \end{tabular}
\end{table}
표 A.7: **흉부 X-ray 보고서 생성에 대한 Med-PaLM M 성능** 텍스트 중첩 기반 및 임상 사실성 기반 자동 메트릭을 사용하여 모델 생성 보고서의 품질을 평가합니다. Med-PaLM M은 임상 효능과 정확성을 포착하도록 설계된 모든 메트릭에 새로운 SOTA를 설정한다. 3개의 Med-PaLM M 변형에 걸쳐, 중형 모델은 최상의 성능을 달성한다.

그림 A.3 **| 나란한 인간 평가 작업 인터페이스입니다.* * 방사선사 평가자는 흉부 X선 및 표시가 주어지면 전체 품질을 기준으로 4개의 결과 단락을 순위 매겼습니다. 4개의 발견은 참조 발견과 일치했으며 3개의 Med-PaLM M 모델 변이체(12B, 84B, 562B)에 의해 생성된 발견이다.

그림 A.4 **| 독립적인 인간 평가 작업 인터페이스.** 방사선사 평가자는 흉부 X선, 표시 및 참조 결과가 주어지면 오류 및 누락에 대해 Med-PaLM M(빨간색)에서 생성한 결과 단락에 주석을 달았습니다.

모형은 보고서당 0.58개의 총 오류를 생성합니다.

인간 평가 접근법의 한 가지 중요한 한계는 평가자 간 변동성이다. 비교 가능한 평가 체계를 사용한 [60]과 유사하게 동일한 방사선학 보고서가 종종 다른 방사선학자 평가자에 의해 다양한 오류 및 생략 통과로 주석이 달리는 것을 관찰했다. 이것은 임상의의 주관적인 등급을 사용하는 연구에서 일반적인 현상이지만, 향후 작업은 평가자 지침을 더욱 개선하고 변동성을 줄이기 위해 평가자 보정을 개선하는 것을 목표로 할 수 있다.

### MultiMedBench Examples

표 9 내지 표 13에서 우리는 다양한 MultiMedBench 작업의 예를 제공한다.

\begin{table}
\begin{tabular}{c c c c} \hline Model Size & Med-PaLM M (562B) & Med-PaLM M (84B) & Med-PaLM M (12B) \\ \hline Significant Omissions & 0.10 (95\% CI, 0.08 - 0.12) & 0.09 (95\% CI, 0.07 - 0.10) & 0.08 (95\% CI, 0.06 - 0.10) \\ Total Omissions & 0.13 (95\% CI, 0.11 - 0.16) & 0.12 (95\% CI, 0.10 - 0.15) & 0.12 (95\% CI, 0.10 - 0.15) \\ \hline Significant Clinical Errors & 0.26 (95\% CI, 0.23 - 0.29) & 0.23 (95\% CI, 0.20 - 0.27) & 0.26 (95\% CI, 0.22 - 0.29) \\ Total Clinical Errors & 0.29 (95\% CI, 0.25 - 0.32) & 0.25 (95\% CI, 0.22 - 0.28) & 0.28 (95\% CI, 0.24 - 0.31) \\ Total Errors & 0.63 (95\% CI, 0.58 - 0.68) & 0.59 (95\% CI, 0.54 - 0.64) & 0.58 (95\% CI, 0.53 - 0.63) \\ \hline \end{tabular}
\end{table}
표 8: **독립적인 인간 평가 세부사항**. 다른 Med-PaLM M 모델에 의해 생성된 방사선학 보고서에서 임상의 평가자가 식별한 누락 및 오류 비율. 임상 오류는 임상 소견의 존재, 위치 또는 중증도와 관련된 오류이다. 총 오류에는 임상 오류와 비임상 오류(예: 보기 또는 이전 연구가 없는 것을 참조하는 단락)가 모두 포함됩니다. **

\begin{table}
\begin{tabular}{l l} \hline \hline Image & Task and input prompt & Target \\ \hline \multicolumn{3}{l}{**Classification (PAD-UFES-20)**} \\ \multicolumn{3}{l}{**Instructions:** You are a helpful dermatology assistant. The following are questions about skin lesions. Categorize the skin lesions into the most likely class given the patient history. \\ \multicolumn{3}{l}{Given \(<\)img\(>\). Patient History: Age: 51, Gender: female, Smoke: false, Drink: false, Family skin cancer history: true, Family any cancer history: false, Lesion region: back, Lesion itch: false, Lesion grew: false, Lesion bled: false, Lesion elevation: false, Fitzpatrick scale: 1.0, DDiameters (mm): [12.0, 8.0]. Q: Which of the following is the most likely diagnosis of the patient’s skin lesion? (A) Nevus (B) Basal Cell Carcinoma (C) Squamous Cell Carcinoma (D) Actinic Keratosis (E) Seborrheic Keratosis (F) Melanoma A: Basal Cell Carcinoma. \\ \multicolumn{3}{l}{Given \(<\)img\(>\). Patient History: Age: 39, Gender: unknown, Smoke: unknown, Drink: unk, Family skin cancer history: unknown, Family any cancer history: unknown, Lesion region: neck, Lesion i:ck: false, Lesion grew: true, Lesion bled: false, Lesion elevation: true, Fitzpatrick scale: unknown, Diameters (mm): [unknown, unknown]. Q: Which of the following is the most likely diagnosis of the patient’s skin lesion? (A) Nevus (B) Basal Cell Carcinoma (C) Squamous Cell Carcinoma (D) Actinic Keratosis (E) Seborrheic Keratosis (F) Melanoma A: \\ \hline \multicolumn{3}{l}{**Classification (MIMIC-CXR)**} \\ \multicolumn{3}{l}{**Instructions:** You are a helpful radiology assistant. The following are questions about findings in chest X-ray in different views. Identify if a specific type of abnormality is shown in the X-ray. \\ \multicolumn{3}{l}{Given the AP view X-ray image \(<\)img\(>\). Q: Is cardiomegaly indicated by the image? (A) No (B) Yes} \\ \multicolumn{3}{l}{A: Yes.} \\ \multicolumn{3}{l}{Given the AP view X-ray image \(<\)img\(>\). Q: Is cardiomegaly indicated by the image? (A) No (B) Yes} \\ \multicolumn{3}{l}{A:} \\ \multicolumn{3}{l}{**Classification (VinDr-Mammo)**} \\ \multicolumn{3}{l}{**Instructions:** You are a helpful medical assistant. The following are questions about mammography reading. Provide a breast-level assessment based on the BI-RADS categories. \\ \multicolumn{3}{l}{Given mammogram image \(<\)img\(>\). Image view: bilateral craniocaudal Q: What is the most likely breast BI-RADS score? (A) 1 (B) 2 (C) 3 (D) 4 (E) 5 A: 4. \\ \multicolumn{3}{l}{Given mammogram image \(<\)img\(>\). Image view: bilateral craniocaudal Q: What is the most likely breast BI-RADS score? (A) 1 (B) 2 (C) 3 (D) 4 (E) 5 A: \\ \multicolumn{3}{l}{**Classification (CBIS-DDSM Calcification)**} \\ \multicolumn{3}{l}{Given mammogram image \(<\)**img\(>\)**. Image view: CC Q: Which of the following is the most likely type of the patient’s breast calcification? (A) BENIGN (B) BENIGN\_WITHOUT\_CALLBACK (C) MALIGNANT \\ \multicolumn{3}{l}{A:} \\ \multicolumn{3}{l}{**Classification (CBIS-DDSM Mass)**} \\ \multicolumn{3}{l}{Given mammogram image \(<\)**img\(>\)**. Image view: CC Q: Which of the following is the most likely type of the patient’s breast mass? (A) BENIGN (B) BENIGN\_WITHOUT\_CALLBACK (C) MALIGNANT A: \\ \multicolumn{3}{l}{**Generomic variant calling**} \\ \multicolumn{3}{l}{**Instructions:** You are a helpful genetic assistant. The following are questions about variant calling. Identify the number of copies of the putative variant in pileup images. \\ \multicolumn{3}{l}{Given \(<\)**img\(>\)**. Q: How many copies of this putative variant are shown in the middle of the image? (A) 0 (B) 1 (C) 2 A: \\ \hline \hline \end{tabular}
\end{table}
표 9: **MultiMedBench의 분류 작업 예제**

**표 A.10**: **MultiMedBench에서 VQA 및 흉부 X선 보고서 생성 작업의 예** 입니다.

\begin{tabular}{p{142.3pt} p{142.3pt} p{142.3pt}} \hline Image & Task and input prompt & Target \\ \hline  & **VQA-RAD** & Axial. \\  & **Instructions:** You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. & Brain. \\  & **Instructions:** You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. & Given \(<\)img\(>\). Q: Is the lung healthy? & A: No. \\  & **Given \(<\)img\(>\). Q: Which part of the body does this image belong to? A:** & \\ \hline  & **Path-VQA** & **Instructions:** You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. & Accumulation of large numbers of macrophage. \\  & **Given \(<\)img\(>\). Q: What is present? (other)** & \\  & **Given \(<\)img\(>\). Q: What is there of large numbers of macrophages within the alveolar spaces with only slight fibrous thickening of the alveolar walls? (other)** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again.** demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.** & **Again.** & **Again.** **Again.** \\  & **Instructions:** You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed. & **Given \(<\)img\(>\). Q: Is the lung healthy? & **A: No. & **A: No. & **Given \(<\)img\(>\). Q: Which part of the body does this image belong to?

[MISSING_PAGE_FAIL:36]

**표 A.13 \(|\) MultiMedBench의 MedQA 예제입니다.**

**Input**

**지침:** 의료 지식에 대 한 객관식 질문은 다음과 같습니다. 사용 가능한 정보를 요약하는 것으로 시작하여 단계별로 해결합니다. 네 가지 옵션 중 하나의 옵션을 최종 답변으로 출력합니다.

질문: 57세의 한 남자가 그의 주치의에게 검진을 위해 선물한다. 그는 13년 동안 제2형 당뇨병을 앓았고, 메트포르민과 빌다글립틴을 복용해 왔다. 그는 29년 동안 매일 10-15개의 담배를 피웠다. 가족사는 무관하다. 활력 징후는 온도 36.6\({}^{\circ}\)C (97.8\({}^{\circ}\)F), 혈압 152/87 mm Hg 및 맥박 88/분이다. 검사 결과 체질량 지수가 32 kg/m\({}^{2}\)인 중등도 복부 비만이 나타났다. 그 검사의 나머지 부분은 주목할 만한 것이 아니다. 그의 공복 지질 프로필은 총 콜레스테롤(TC) 280 mg/dL 저밀도 지단백(LDL)-콜레스테롤 210 mg/dL 고밀도 지단백(HDL)-콜레스테롤 40 mg/dL 트리글리세라이드(TG) 230 mg/dL 이 환자에게 가장 좋은 초기 요법의 작용 기전은 다음과 같다.

(A) 콜레스테롤 흡수 억제 (B) 담즙산 격리 (C) 콜레스테롤 합성 억제 (D) PPAR-α의 활성화

정답: 콜레스테롤 합성 억제.

질문: 3살 소녀는 잘 생긴 아이 검진을 위해 엄마와 함께 선물을 한다. 최근의 실험실 데이터는 지속적인 정상 세포성 빈혈을 입증했다. 어머니는 과거 혈전의 과거력을 부인하지만 최근 과거에도 폐색전증으로 치료를 받아야 했고, 동생은 평생 빈혈을 치료해야 했다고 한다. 환자의 과거 병력은 빈번한 중이 감염 외에 기여하지 않는다. 도착 시 활력징후는 체온 36.7\({}^{\circ}\)C (98.0\({}^{\circ}\)F); 혈압 106/74 mm Hg; 심박수 111/분 및 규칙성; 호흡수 17/분이다. 신체검사에서 그녀의 맥박은 경계를 이루고 손톱은 창백하지만 호흡음은 선명하다. 산소 포화도는 초기에 실내 공기에서 91%였으며 심전도(ECG)는 동성 빈맥을 보여준다. 환자의 1차 주치의는 이 소견을 추가로 평가하기 위해 말초 혈액 도말을 명령하며 예비 결과 용혈성 빈혈이 나타난다. 겸상적 세포 질환을 가장 잘 설명하는 병태생리학적 기전은 무엇인가?

(A) 보체 활성화에 대한 적혈구 민감도의 증가, 환자들로 하여금 혈전성 사건을 일으키기 쉽도록 하는 (B) RBC에 형태학적 변화를 일으키는 열성 베타 글로빈 돌연변이 (C) 적혈구가 산화 스트레스에 점점 더 민감하게 반응하는 X-결합 열성 질환 (D) 이차적으로 EBV, 마이코플라스마, CLL, 또는 류마티스 질환에 의해 야기되는

답: RBC에 형태학적 변화를 일으키는 열성 베타 글로빈 돌연변이.

질문: 급성 저산소성 호흡 부전으로 사망한 58세 여성의 폐 부검 표본을 검사했다. 그녀는 3개월 전에 대퇴골 골절로 최근에 수술을 받았다. 초기 병원 과정은 복잡하지 않았고 그녀는 건강하게 재활 시설로 퇴원했다. 재활원에서 집으로 퇴원하자마자 그녀는 갑작스러운 호흡 곤란과 심정지가 발생했다. 소생 작업에 실패했습니다. 폐 조직의 조직학적 검사에서 폐동맥 내강 주변의 섬유성 결합 조직이 관찰된다. 다음 중 현재 발견에 대한 가장 가능성 있는 발병기전은 무엇인가?

(A) 혈전색전증 (B) 폐허혈 (C) 폐고혈압 (D) 폐수동정체

Answer:

_Target_

Thromboembolism.

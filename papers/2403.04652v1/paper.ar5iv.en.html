<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.04652] Yi: Open Foundation Models by 01.AI</title><meta property="og:description" content="We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities.
The Yi model family is based on 6B and 34B pretrained language models, then we extend…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yi: Open Foundation Models by 01.AI">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Yi: Open Foundation Models by 01.AI">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.04652">

<!--Generated on Fri Apr  5 17:01:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Yi: Open Foundation Models by 01.AI </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id1.1.id1" class="ltx_text ltx_font_bold">01.AI</span> 
<br class="ltx_break">  
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_bold">Code:</span>  <a target="_blank" href="https://github.com/01-ai/Yi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/01-ai/Yi</a>
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_bold">Model:</span>  <a target="_blank" href="https://huggingface.co/01-ai" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/01-ai</a> 
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities.
The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models.
Our base models achieve strong performance on a wide range of benchmarks like MMLU,
and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena.
Building upon our scalable super-computing infrastructure and the classical transformer architecture,
we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts.
For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline.
For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers.
For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model.
We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance.
We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance.
We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.
</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S1" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S2" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS1" title="In 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS2" title="In 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tokenization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS3" title="In 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Model Architecture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S3" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Finetuning</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS1" title="In 3 Finetuning ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS2" title="In 3 Finetuning ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training Method</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S4" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S5" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Safety</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S6" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Evaluations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S6.SS1" title="In 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Base Model Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS1.SSS1" title="In 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS1.SSS2" title="In 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Discussions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS1.SSS3" title="In 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>In-Context Learning Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S6.SS2" title="In 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Chat Model Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS2.SSS1" title="In 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Automatic Evaluations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS2.SSS2" title="In 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Human Evaluations</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S7" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Capability Extension</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS1" title="In 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Long Context Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS2" title="In 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Vision-Language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS3" title="In 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Depth Upscaling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S8" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Final Discussions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a href="#A1" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Author List and Contributions</span></a></li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent breakthroughs in large language models have revolutionized the whole field of artificial intelligence and potentially radiate across the entire human society.
Our vision for large language models is to make them the next generation computational platform and empower the whole community with significantly amplified intelligence.
As a step towards this mission, we present the Yi model series, 6B and 34B language models pretrained from scratch on 3.1T highly-engineered large amount of data, and finetuned on a small but meticulously polished alignment data.
Due to the data quality resulting from our substantial engineering efforts, which we will detail in the upcoming sections, Yi achieves near GPT-3.5 benchmark scores and human preferences.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In designing the Yi model series, we are mostly concerned on the following dimensions regarding
<span id="S1.p2.1.1" class="ltx_text ltx_font_italic">model scale, data scale, and data quality</span>:
(1). when choosing model scale, the desiderata is to have small enough model that is feasible for inference on consumer-grade hardware like the RTX 4090 where the bounding factor is its limited 24G memory, yet still large enough with complex reasoning and emergent abilities. This is why we found 34B gives a nice performance-cost balance;
(2). since 34B is smaller than the conventional 70B used by Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, we increase the pretrain data scale to 3.1T tokens to compensate for the decreased compute flops. This makes the model-data scale combination fall into the post Chinchilla optimal regime&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, i.e., we overtrain the model on more tokens (3T) than the compute optimal (around 1T). The benefit is from the inference side, as we achieve stronger performance with reduced serving cost: after int4&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> quantization, one can serve the 34B chat model on 24G GPU memory with almost no performance drop;
(3). our data engineering principle is to promote quality over quantity for both pretraining and finetuning.
The pretraining data quality is guaranteed by a sophisticated data cleaning pipeline with cascaded filtering methods and intentionally increased deduplication strength;
(4). for finetuning data we heavily emphasize quality by handcrafting less than 10K instructions over multiple iterations based on user feedback. This approach significantly deviates from the quantity-scaling styled instruction tuning works like FLAN&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and UltraChat&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, but aligns more with the handcrafting styled works like LIMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our pretraining data cleaning system features a sophisticated filtering pipeline based on language, heuristic textual features, perplexity, semantics, topic, and safety, as well as a cascaded deduplication process based on paragraph, MinHash, and exact matching.
This thorough pipeline leads to a much higher removal ratio than existing pipelines like CCNet&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, RefinedWeb&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and RedPajama&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, which we believe is key to the success of data engineering.
The underlying principle is although pretraining requires data scaling, one would like to make sure the data used are of high quality, rather than training the model on large raw data, i.e.,
we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering.
Regarding the model architecture, we use standard implementation of the Transformer architecture with Grouped-Query Attention (GQA)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, SwiGLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> activation, and RoPE with an adjusted base frequency (RoPE ABF)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.
This design choice is the standard approach rooted from the Transformer original paper&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, later modified by GPT-3 and Chinchilla&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, then followed by LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, Baichuan&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>, Qwen&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and many related works.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To approach GPT-3.5-matching human preferences, our finetuning dataset is curated from carefully selected multi-turn instruction-response pairs, annotated directly by our team of machine learning engineers then polished over multiple iterations of user feedback.
As mentioned above, the size of our finetuning dataset is less than 10K, but improved over and over again across the model development timeline.
Benefiting from the dataset’s manageable size, we employed an extensive grid search to identify the optimal data composition, promote diversity, and discover effective hyperparameters.
After 8-bit and 4-bit quantization, the final chat model can be deployed on consumer-grade GPUs nearly without performance degradation compared to the bf16 format.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We further extend the Yi model capability from three dimensions: context scaling, vision-language adaptation, and depth-upscaling.
To achive 200K context length, we continue pretrain the model on about 5B length-upsampled data, similar to the concurrent work in&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Fu et&nbsp;al. [<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
To adapt the model to vision-language tasks, we integrate a vision encoder and develop a multi-stage training method, following and improving the practice of&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Liu et&nbsp;al. [<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
We also study the effectiveness of depth-upscailng&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, i.e., making the model deeper by continual pretraining, and confirming its effectiveness to further improve model performance.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our infrastructure provides strong support for the full-stack development of the Yi model series, from pretraining to finetuning to serving.
To support pretraining, we develop cross-cloud elastic task scheduling, automatic failure recovery, and topology-aware resource allocation which collectively enable us to run tasks according to the real-time available GPU nodes cross clusters with limited switching overhead.
To support finetuning, we build a hierarchical scheduling framework supporting different distributed backends for different models (e.g., Megatron&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> for the policy model and DeepSpeed&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> for the reward model).
For efficient inference, we use 4-bit model and 8-bit KV cache quantization, combining with PagedAttention&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and Dynamic Batching.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Extensive experiments demonstrate that Yi-34B can match GPT-3.5 in both performance and efficiency.
On most standard benchmarks like MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> (for the base model) and LMSys ELO Rating&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> (for the chat model), Yi-34B generally achieves scores on par with GPT-3.5.
After model parameter and KV cache quantization, the inference cost is also controlled such that a wide range of the community can deploy the model on cost effective devices.
We further report a detailed performance comparison between Yi and major LLMs on commonsense reasoning, college exams, math, coding, reading comprehension, and human preference win-rate on multiple evaluation benchmarks.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Since its release, the Yi model series has benefited the community from the following perspectives:
(1). it provides GPT-3.5-matching quality yet cost-effective models to researchers, and enables developers to build AI-native applications like language model based agents;
(2). it empowers end users with locally runnable chatbots, which consequently helps protecting user data privacy;
(3). it sheds light on the direction on further data and model scaling to achieve even stronger frontier models.
for both research and commercial use.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Pretraining</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2403.04652/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="306" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Yi’s pretraining data cleaning pipeline.
</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our approach to pretraining is to train a standard dense transformer architecture on a heavily engineered large pretraining corpora, where our underlying assumption is that when trained on extensive data of high-enough quality, a standard architecture can exhibit advanced capability.
This is to say, we may not need much architectural modification, although we have indeed conducted extensive preliminary architectural experiments.
In the following subsections, we first detail our data engineering pipeline, then briefly discuss the model architecture.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Processing</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The Yi data mixture is shown in Fig.&nbsp;<a href="#S2.F2" title="Figure 2 ‣ Heuristic Rule Filters ‣ 2.1 Data Processing ‣ 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
To produce a high-quality bilingual pretraining data,
we meticulously designed a cascaded data-processing pipeline, as illustrated in Fig&nbsp;<a href="#S2.F1" title="Figure 1 ‣ 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
This pipeline features a series of data-cleaning strategies targeting quality and diversity.
We start with web documents from Common Crawl, use the CCNet pipeline <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> for language identification and perplexity scoring.
Then we use a combination of filtering and deduplication process, as detailed below.</p>
</div>
<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Heuristic Rule Filters</h5>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px1.p1.1" class="ltx_p">This part of filter aims for removing text of low quality.
We filter out text based on:
(1). URL, domain, word blocklists and garbled text filters;
(2). document length, the ratio of special symbols, and the ratio of short, consecutive, or incomplete lines;
(3). repeated words, n-grams, or paragraphs <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>;
The filtering thresholds are based on a statistical analysis of large document samples, as described in <cite class="ltx_cite ltx_citemacro_citet">Nguyen et&nbsp;al. [<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
Furthermore, we identify and anonymize Personal Identifiable Information (PII), such as email addresses and phone numbers.
</p>
</div>
<figure id="S2.F2" class="ltx_figure ltx_align_floatright"><img src="/html/2403.04652/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="175" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Yi’s pre-training data mixture. Overall our data consist of 3.1T high-quality tokens in Both English and Chinese, and come from various sources. Our major differences from existing known mixtures like LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> and Falcon&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> are that we are bilingual, and of higher quality due to our more rigorous cleaning pipeline.</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Learned Filters</h5>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p">We use learned filters to address nuanced cases that exceed the capabilities of standard heuristic rules. Notably, the Chinese content extracted from Common Crawl present unique challenges, particularly with a higher ratio of inappropriate content like pornography and gambling. Traditional heuristic-rule-based filters struggle to effectively identify and eliminate all harmful content. To enhance our filtering process, we have integrated a suite of learned scorers for filtering, namely the perplexity scorer, quality scorer, safety scorer, and document coherence scorer:
(1). the <span id="S2.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">Perplexity Scorer</span>, utilizing the KenLM library as per CCNet <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, evaluates a vast array of web documents, discarding those with perplexity scores largely above average;
(2). the <span id="S2.SS1.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">Quality Scorer</span> is a classifier trained to recognize and favor pages similar to Wikipedia in quality and assign scores accordingly. Documents that fail to meet the quality standard are subsequently removed;
(3). the <span id="S2.SS1.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">Document Coherence Scorer</span> identifies low-quality web documents that consist of disparate sentences or paragraphs, thus being incoherence. Such documents are either segmented for further analysis or removed entirely.
(4). the <span id="S2.SS1.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_italic">Safety Scorer</span> identifies and removes web documents containing toxic content, such as violence, pornography, and political propaganda.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Cluster-based Filters</h5>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px3.p1.1" class="ltx_p">We further use unsupervised semantic clustering to group web documents. This clustering process enables efficient identification and analysis of documents sharing similar semantic features. The clustered data are subsequently annotated with quality labels, providing essential references for the optimization of Yi’s data mixture strategy. Documents identified as low-quality through automatic and manual verification are excluded from the dataset.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deduplication</h5>

<div id="S2.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px4.p1.1" class="ltx_p">After filtering, we implement a comprehensive deduplication pipeline following the procedure in Penedo et al. (2023) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. This pipeline integrates document-level MinHash deduplication and sub-document exact-match deduplication, effectively identifying and removing duplicate content within and across documents. We further categorize web documents into specific themes using a topic model predicting labels like as news, ads, and knowledge-based content. In the final pretraining dataset, we down-sample less helpful content, mostly advertisements, to ensure information density. The final composition of Yi’s pretraining data is shown in Fig.&nbsp;<a href="#S2.F2" title="Figure 2 ‣ Heuristic Rule Filters ‣ 2.1 Data Processing ‣ 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tokenization</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We use byte-pair encoding (BPE)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> implemented in the SentencePiece framework&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, to tokenize the pretraining data.
The vocabulary size of Yi is set to 64,000 to balance computational efficiency and word comprehension.
Specifically, we split numbers into individual digits to facilitate a better understanding of numeric data.
We allow rare characters to fall back to the unicode-byte encoding to ensure fault tolerance.
We employ the identity tokenizer to avoid transferring all punctuations to the half-width format.
LLMs prioritizing English usually utilize dummy prefix (whitespace at the beginning of text) in their tokenizers to generalize the same words at different positions of sentences.
We do not use this approach because the assumption does not always hold even in the English context, especially for sentences that begin with quotation marks, also it does not show positive effect in Chinese context.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Model Architecture</h3>

<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S2.T1.2.3" class="ltx_tr">
<td id="S2.T1.2.3.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Models</td>
<td id="S2.T1.2.3.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Hidden Size</td>
<td id="S2.T1.2.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Q-heads</td>
<td id="S2.T1.2.3.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">KV-heads</td>
<td id="S2.T1.2.3.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Layers</td>
<td id="S2.T1.2.3.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Pretrain Seq. Len</td>
<td id="S2.T1.2.3.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Max LR</td>
</tr>
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">6B</td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4</td>
<td id="S2.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><math id="S2.T1.1.1.1.m1.1" class="ltx_Math" alttext="3\times 10^{-4}" display="inline"><semantics id="S2.T1.1.1.1.m1.1a"><mrow id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.T1.1.1.1.m1.1.1.1" xref="S2.T1.1.1.1.m1.1.1.1.cmml">×</mo><msup id="S2.T1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.m1.1.1.3.cmml"><mn id="S2.T1.1.1.1.m1.1.1.3.2" xref="S2.T1.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.T1.1.1.1.m1.1.1.3.3" xref="S2.T1.1.1.1.m1.1.1.3.3.cmml"><mo id="S2.T1.1.1.1.m1.1.1.3.3a" xref="S2.T1.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.T1.1.1.1.m1.1.1.3.3.2" xref="S2.T1.1.1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1"><times id="S2.T1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.m1.1.1.2">3</cn><apply id="S2.T1.1.1.1.m1.1.1.3.cmml" xref="S2.T1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.1.1.1.m1.1.1.3.1.cmml" xref="S2.T1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.T1.1.1.1.m1.1.1.3.2.cmml" xref="S2.T1.1.1.1.m1.1.1.3.2">10</cn><apply id="S2.T1.1.1.1.m1.1.1.3.3.cmml" xref="S2.T1.1.1.1.m1.1.1.3.3"><minus id="S2.T1.1.1.1.m1.1.1.3.3.1.cmml" xref="S2.T1.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.T1.1.1.1.m1.1.1.3.3.2.cmml" xref="S2.T1.1.1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">3\times 10^{-4}</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.2.2" class="ltx_tr">
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S2.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">7168</td>
<td id="S2.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">56</td>
<td id="S2.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">8</td>
<td id="S2.T1.2.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">60</td>
<td id="S2.T1.2.2.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><math id="S2.T1.2.2.1.m1.1" class="ltx_Math" alttext="1.5\times 10^{-4}" display="inline"><semantics id="S2.T1.2.2.1.m1.1a"><mrow id="S2.T1.2.2.1.m1.1.1" xref="S2.T1.2.2.1.m1.1.1.cmml"><mn id="S2.T1.2.2.1.m1.1.1.2" xref="S2.T1.2.2.1.m1.1.1.2.cmml">1.5</mn><mo lspace="0.222em" rspace="0.222em" id="S2.T1.2.2.1.m1.1.1.1" xref="S2.T1.2.2.1.m1.1.1.1.cmml">×</mo><msup id="S2.T1.2.2.1.m1.1.1.3" xref="S2.T1.2.2.1.m1.1.1.3.cmml"><mn id="S2.T1.2.2.1.m1.1.1.3.2" xref="S2.T1.2.2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.T1.2.2.1.m1.1.1.3.3" xref="S2.T1.2.2.1.m1.1.1.3.3.cmml"><mo id="S2.T1.2.2.1.m1.1.1.3.3a" xref="S2.T1.2.2.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.T1.2.2.1.m1.1.1.3.3.2" xref="S2.T1.2.2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.m1.1b"><apply id="S2.T1.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1"><times id="S2.T1.2.2.1.m1.1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1.1"></times><cn type="float" id="S2.T1.2.2.1.m1.1.1.2.cmml" xref="S2.T1.2.2.1.m1.1.1.2">1.5</cn><apply id="S2.T1.2.2.1.m1.1.1.3.cmml" xref="S2.T1.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.2.2.1.m1.1.1.3.1.cmml" xref="S2.T1.2.2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.T1.2.2.1.m1.1.1.3.2.cmml" xref="S2.T1.2.2.1.m1.1.1.3.2">10</cn><apply id="S2.T1.2.2.1.m1.1.1.3.3.cmml" xref="S2.T1.2.2.1.m1.1.1.3.3"><minus id="S2.T1.2.2.1.m1.1.1.3.3.1.cmml" xref="S2.T1.2.2.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.T1.2.2.1.m1.1.1.3.3.2.cmml" xref="S2.T1.2.2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.1.m1.1c">1.5\times 10^{-4}</annotation></semantics></math></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Model configs of Yi-6B and Yi-34B. LR stands for learning rate.</figcaption>
</figure>
<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Yi uses a modified version of the classical decoder-only Transformer architecture&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> where the code is based on LLaMA’s&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> implementation.
The main parameter setting is summarized in Table&nbsp;<a href="#S2.T1" title="Table 1 ‣ 2.3 Model Architecture ‣ 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The modifications from LLaMA to Yi are further summarized below:</p>
</div>
<section id="S2.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Attention Mechanism</h5>

<div id="S2.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px1.p1.1" class="ltx_p">LLaMA 2 uses Grouped-Query Attention(GQA)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> only on its largest 70B model, and its 7B and 13B uses full attention. We incorporate GQA in both Yi-6B and Yi-34B.
GQA splits query-heads into G groups, sharing a single key and value head within each group of query&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
This approach offers substantial reductions of training and inference costs, compared to the original Multi-Head Attention (MHA)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>.
We do not observe performance degradation after applying GQA to our 6B smaller model.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Activation Function</h5>

<div id="S2.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px2.p1.3" class="ltx_p">We use SwiGLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> as Yi’s post-attention layer, reducing its activation size from <math id="S2.SS3.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="4h" display="inline"><semantics id="S2.SS3.SSS0.Px2.p1.1.m1.1a"><mrow id="S2.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.1" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1"><times id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.2">4</cn><ci id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px2.p1.1.m1.1c">4h</annotation></semantics></math> to <math id="S2.SS3.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="8/3h" display="inline"><semantics id="S2.SS3.SSS0.Px2.p1.2.m2.1a"><mrow id="S2.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.cmml"><mrow id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml"><mn id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.2" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.2.cmml">8</mn><mo id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.1" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.1.cmml">/</mo><mn id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.3" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">3</mn></mrow><mo lspace="0em" rspace="0em" id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.1" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1"><times id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.1"></times><apply id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2"><divide id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.1"></divide><cn type="integer" id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.2">8</cn><cn type="integer" id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.3">3</cn></apply><ci id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px2.p1.2.m2.1c">8/3h</annotation></semantics></math> (<math id="S2.SS3.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S2.SS3.SSS0.Px2.p1.3.m3.1a"><mi id="S2.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS3.SSS0.Px2.p1.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px2.p1.3.m3.1b"><ci id="S2.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.3.m3.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px2.p1.3.m3.1c">h</annotation></semantics></math> denotes hidden size) to be consistent with the normal post-attention layer.
This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Positional Embedding and Long Context</h5>

<div id="S2.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS3.SSS0.Px3.p1.1" class="ltx_p">We use Rotary Position Embedding
(RoPE)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> following the standard implementation.
We adjust the base frequency (RoPE ABF), introduced in&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Xiong et&nbsp;al. [<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, to support long context windows up to 200K where the base model itself is trained on 4K context length.
To adapt the base model to longer context, we continue pretrain the model on 10B tokens from our pretraining data mixture with slightly upsampled long sequences, mostly from book.
We observe that only 1-2B tokens is enough for the model to converge to low loss on 4K-200K length, and a lightweight finetuning further induces near-perfect long-context retrieval performance.
Based on this observation, we tend to view that the capability of modeling longer dependency than the pretrained length (4K) is a intrinsic capability (rather than an being injected by post-train).
This is to say, the base model already has the capability to model longer than 4K dependency even the model is trained shorter, and the post-train / finetuning procedure simply release this capability.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Finetuning</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our finetuning method significantly emphasizes data quality over quantity.
Our approach does <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">not</span> follow existing data-intensive approaches like FLAN&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and UltraChat&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, which scales the SFT data to millions of entries but each of the entries may not been examined carefully because the scale is too large.
In contrast, our method aligns with the LIMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> and DEITA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> approach, which focus on data selection rather than scaling.
With the scale being less than 10K, we are able to examine and optimize <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">every single data point</span>.
Below we discuss our data construction and training details.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Preprocessing</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Quality is All You Need</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">Our finetuning dataset consists of less than 10K multi-turn instruction-response dialog pairs, with each and every one of the entry constructed and polished over multiple iterations and from user feedback.
We take this approach because in our preliminary experiments, we observe that compared to the open-source data of several hundred thousand entries, the results from a smaller, manually annotated dataset are superior.
These observations align with those reported in&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. [<a href="#bib.bib77" title="" class="ltx_ref">77</a>], Zhou et&nbsp;al. [<a href="#bib.bib94" title="" class="ltx_ref">94</a>], Gemini&nbsp;Team et&nbsp;al. [<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p2.1" class="ltx_p">We use the following techniques to improve prompt distribution selection, response formatting, and chain-of-thought formatting:
(1). for prompt distribution selection,
drawing inspiration from WizardLM<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, we develope compound instructions and progressively evolved them to increase their complexity.
This approach has significantly reduced the size of SFT data
in our experiments;
(2). for response formatting, we generally use a default style extended from LIMA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. Overall, the responses are structured in an introduction-body-conclusion format where the body is usually a list of bullet point;
(3). for CoT data formatting, we have use a “Step-Back” pattern, inspired by <cite class="ltx_cite ltx_citemacro_citet">Zheng et&nbsp;al. [<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>, by performing abstraction to formulate higher-level solutions before delving into reasoning about the original, more concrete questions.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p3.1" class="ltx_p">We spend extra efforts on reducing hallucination and repetition:
(1). to reduce hallucinations,
we examine and ensure that the knowledge in the responses is not contained within the model, and eliminate responses that might lead to memorization;
(2). to reduce repetition, we rewrite the repetitive turns of the responses that usually exist but may be overlooked in the finetuning data.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Diversity and Mixture</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">To ensure the coverage of different capabilities, we have included a wide spectrum of open-source prompt,
encompassing areas such as question answering, creative writing, dialogue, reasoning, mathematics, coding, safety, bilingual capabilities, and others.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p2.1" class="ltx_p">To obtain a fine-grained control of different directions of capabilities, inspired by InsTag<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, we develop a instruction tagging system. By designing a diversity-focused sampling algorithm, we carefully balanced the distribution of instructions across various tags. This approach ensures a diverse finetuning dataset, aiming to achieve enhanced cross-task robustness.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p3.1" class="ltx_p">To achieve the optimal data ratio for balancing different directions of the capability, we use an approximate grid search to determine our data mixture. Motivated by <cite class="ltx_cite ltx_citemacro_citet">Dong et&nbsp;al. [<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, this process involved experimenting with {1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64} proportions for each ability. The search process was guided by validation results
and our in-house human evaluation sets.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">ChatML Format</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">Beyond the focus on data quality and diversity, our observations revealed that the format of the data substantially influences the model’s ultimate performance. To this end, we implemented the ChatML-style format&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. This structured approach empowers the model to differentiate among various information types, such as system configurations, user inputs, and assistant responses.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Method</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.5" class="ltx_p">We use next-word prediction loss for finetuning, and only compute loss on the responses, but not system and user instructions.
We use AdamW optimizer
with <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">β</mi><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝛽</ci><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\beta_{1}</annotation></semantics></math> set to 0.9, <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\beta_{2}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">β</mi><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝛽</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\beta_{2}</annotation></semantics></math> set to 0.999, and <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\epsilon</annotation></semantics></math> set to <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="10^{-8}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msup id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mn id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mo id="S3.SS2.p1.4.m4.1.1.3a" xref="S3.SS2.p1.4.m4.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">8</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">10</cn><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><minus id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">10^{-8}</annotation></semantics></math>.
We use a sequence length of 4096, alongside a batch size of 64.
We set training step to 300 with a constant <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="1\times 10^{-5}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mn id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.5.m5.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.cmml">×</mo><msup id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mn id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">10</mn><mrow id="S3.SS2.p1.5.m5.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml"><mo id="S3.SS2.p1.5.m5.1.1.3.3a" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml">−</mo><mn id="S3.SS2.p1.5.m5.1.1.3.3.2" xref="S3.SS2.p1.5.m5.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><times id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1"></times><cn type="integer" id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">1</cn><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.2">10</cn><apply id="S3.SS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3"><minus id="S3.SS2.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3"></minus><cn type="integer" id="S3.SS2.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">1\times 10^{-5}</annotation></semantics></math> learning rate, a weight decay of 0.1, gradient clipping with a maximum threshold of 1.0, and NEFTune&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> with a noise scale of 45 for Yi-34B-Chat and 5 for Yi-6B-Chat.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Infrastructure</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We build the infrastructure supporting the full-stack data processing, pretraining, finetuning, and serving. Our infrastructure feasures:
(1). automated managing and monitoring the computing resource;
(2). improved the training speed from optimized parallel strategies, kernel efficiency, and long-context support;
(3). unified finetuning framework supporting heterogeneous distributed training backend, such as simultaneously using Megatron and DeepSpeed for multiple models in Direct Preference Optimization (DPO)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>;
(4). reducing the deployment cost by various LLM serving accelerations such as quantization, continuous batching, and paged attention.
Below we explain these techniques one by one.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Computing Resources Management</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">To efficient schedule large-scale language model development, particularly pretraining, which may take months on thousands of GPUs ,
we build a highly efficient multi-cloud task scheduling algorithm to manage pre-training, SFT, and RLHF tasks of different priorities. We also build a high-performance in-house training framework that allows us to automatically elastic scale the pre-train jobs to different node sizes based on the GPU availability. More importantly, all the training-related hyper-parameters will be scaled at the same time seamlessly.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">During the large language model training stage, a wide range of failures regularly occur, ranging from GPU crashes to communication fabric errors to loss spikes. We use the following strategies to address these reliability challenges:
(1) we apply automated inspection, prediction, and labeling of nodes for different kind of software/hardware error categories. Nodes marked as tainted will be temporarily removed from the resource pool until the errors got cleared.
(2) we implement a task queuing system with pre-checks and the capability for fast, automatic recovery in the event of failures during training tasks.
(3) we develop of a user-friendly multi-task submission and management console, enabling developers to seamlessly manage and track their training tasks and hyper-parameters.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Performance and Cost Efficiency</h5>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p"><span id="S4.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">Memory</span> and <span id="S4.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">communication</span> restrictions are the two major technical challenges of large scale model training requiring integrated solutions beyond adding more GPUs.
We use and improve upon the following techniques to tackle the memory and communication restrictions: (1) ZeRO-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> to remove the memory consumption by partitioning optimizer states cross data-parallel processes; (2) tensor parallel combined with pipeline parallel&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> within each compute node to avoid inter-node communication bottleneck, and the 3D parallel strategy is well designed and optimized to avoid using activation checkpointing and minimize the pipeline bubbles; (3) kernel fusion techniques like flash attention<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and JIT kernels to reduce redundant global memory access and consumption; (4) topology-aware resource allocation (ranking strategy) to minimize the communication across different layers of switches, which is the limitation of a typical fat-tree-topology.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Finetuning Framework</h5>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">Different from pretraining, finetuning LLMs may require the orchestration of multiple models, as is the practice of DPO&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and PPO&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.
In such training jobs, a typical process is to use reference/reward model to predict a batch of data (which also requires nontrivial time), then let the target model use this data to calculate loss and update parameters.
To this end, we build a multi-model scheduling framework to support multiple backends for different LLMs in a single job. For example, when finetuning a language model with DPO, the intermediate results from the reference model can be cached and reused, improving the training speed and resource cost to be close to the supervised finetuning counterparts.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:432.0pt;height:195.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.0pt,10.8pt) scale(0.9,0.9) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">MMLU</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">BBH</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">C-Eval</span></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">CMMLU</span></td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.7.1" class="ltx_text ltx_font_bold">Gaokao</span></td>
<td id="S4.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.8.1" class="ltx_text ltx_font_bold">CR</span></td>
<td id="S4.T2.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.9.1" class="ltx_text ltx_font_bold">RC</span></td>
<td id="S4.T2.1.1.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.10.1" class="ltx_text ltx_font_bold">Code</span></td>
<td id="S4.T2.1.1.1.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.11.1" class="ltx_text ltx_font_bold">Math</span></td>
</tr>
<tr id="S4.T2.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.1.1" class="ltx_text ltx_font_bold">GPT-4</span></td>
<td id="S4.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.3.1" class="ltx_text ltx_font_bold">83.0</span></td>
<td id="S4.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.4.1" class="ltx_text ltx_font_bold">86.7</span></td>
<td id="S4.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">69.9</td>
<td id="S4.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">71.0</td>
<td id="S4.T2.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.3</td>
<td id="S4.T2.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.8.1" class="ltx_text ltx_font_bold">89.3</span></td>
<td id="S4.T2.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.10.1" class="ltx_text ltx_font_bold">65.3</span></td>
<td id="S4.T2.1.1.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.11.1" class="ltx_text ltx_font_bold">66.1</span></td>
</tr>
<tr id="S4.T2.1.1.3" class="ltx_tr">
<td id="S4.T2.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.3.1.1" class="ltx_text ltx_font_bold">GPT-3.5</span></td>
<td id="S4.T2.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">69.1</td>
<td id="S4.T2.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">70.1</td>
<td id="S4.T2.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">52.5</td>
<td id="S4.T2.1.1.3.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">55.5</td>
<td id="S4.T2.1.1.3.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">51.1</td>
<td id="S4.T2.1.1.3.8" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">83.1</td>
<td id="S4.T2.1.1.3.9" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.3.10" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">54.8</td>
<td id="S4.T2.1.1.3.11" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">35.6</td>
</tr>
<tr id="S4.T2.1.1.4" class="ltx_tr">
<td id="S4.T2.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.4.1.1" class="ltx_text ltx_font_bold">Qwen</span></td>
<td id="S4.T2.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14B</td>
<td id="S4.T2.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">66.7</td>
<td id="S4.T2.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">53.4</td>
<td id="S4.T2.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.1</td>
<td id="S4.T2.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">71.0</td>
<td id="S4.T2.1.1.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.5</td>
<td id="S4.T2.1.1.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">74.2</td>
<td id="S4.T2.1.1.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.5</td>
<td id="S4.T2.1.1.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">40.6</td>
<td id="S4.T2.1.1.4.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">43.1</td>
</tr>
<tr id="S4.T2.1.1.5" class="ltx_tr">
<td id="S4.T2.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T2.1.1.5.1.1" class="ltx_text ltx_font_bold">Llama2</span></td>
<td id="S4.T2.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S4.T2.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.6</td>
<td id="S4.T2.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">44.1</td>
<td id="S4.T2.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">71.1</td>
<td id="S4.T2.1.1.5.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">68.9</td>
<td id="S4.T2.1.1.5.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">27.8</td>
<td id="S4.T2.1.1.5.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">24.2</td>
</tr>
<tr id="S4.T2.1.1.6" class="ltx_tr">
<td id="S4.T2.1.1.6.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">70B</td>
<td id="S4.T2.1.1.6.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">69.7</td>
<td id="S4.T2.1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">64.9</td>
<td id="S4.T2.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.1</td>
<td id="S4.T2.1.1.6.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">53.3</td>
<td id="S4.T2.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">23.3</td>
<td id="S4.T2.1.1.6.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">72.7</td>
<td id="S4.T2.1.1.6.8" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">72.3</td>
<td id="S4.T2.1.1.6.9" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">38.4</td>
<td id="S4.T2.1.1.6.10" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">35.2</td>
</tr>
<tr id="S4.T2.1.1.7" class="ltx_tr">
<td id="S4.T2.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.7.1.1" class="ltx_text ltx_font_bold">Baichuan-2</span></td>
<td id="S4.T2.1.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13B</td>
<td id="S4.T2.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">55.0</td>
<td id="S4.T2.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">49.0</td>
<td id="S4.T2.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">59.0</td>
<td id="S4.T2.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">61.97</td>
<td id="S4.T2.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">45.6</td>
<td id="S4.T2.1.1.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">66.3</td>
<td id="S4.T2.1.1.7.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.4</td>
<td id="S4.T2.1.1.7.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">23.4</td>
<td id="S4.T2.1.1.7.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.1</td>
</tr>
<tr id="S4.T2.1.1.8" class="ltx_tr">
<td id="S4.T2.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.8.1.1" class="ltx_text ltx_font_bold">InternLM</span></td>
<td id="S4.T2.1.1.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">20B</td>
<td id="S4.T2.1.1.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.1</td>
<td id="S4.T2.1.1.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">52.5</td>
<td id="S4.T2.1.1.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">58.8</td>
<td id="S4.T2.1.1.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">59.0</td>
<td id="S4.T2.1.1.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">45.5</td>
<td id="S4.T2.1.1.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">78.3</td>
<td id="S4.T2.1.1.8.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.8.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">34.8</td>
<td id="S4.T2.1.1.8.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">30.26</td>
</tr>
<tr id="S4.T2.1.1.9" class="ltx_tr">
<td id="S4.T2.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.9.1.1" class="ltx_text ltx_font_bold">Skywork</span></td>
<td id="S4.T2.1.1.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13B</td>
<td id="S4.T2.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.1</td>
<td id="S4.T2.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">41.7</td>
<td id="S4.T2.1.1.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">60.6</td>
<td id="S4.T2.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">61.8</td>
<td id="S4.T2.1.1.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">68.1</td>
<td id="S4.T2.1.1.9.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.4</td>
<td id="S4.T2.1.1.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">61.4</td>
<td id="S4.T2.1.1.9.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">64.9</td>
<td id="S4.T2.1.1.9.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">18.1</td>
</tr>
<tr id="S4.T2.1.1.10" class="ltx_tr">
<td id="S4.T2.1.1.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.10.1.1" class="ltx_text ltx_font_bold">Falcon</span></td>
<td id="S4.T2.1.1.10.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">180B</td>
<td id="S4.T2.1.1.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">70.4</td>
<td id="S4.T2.1.1.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">54.0</td>
<td id="S4.T2.1.1.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">57.8</td>
<td id="S4.T2.1.1.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">58.0</td>
<td id="S4.T2.1.1.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">59.0</td>
<td id="S4.T2.1.1.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">74.4</td>
<td id="S4.T2.1.1.10.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.10.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.10.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
</tr>
<tr id="S4.T2.1.1.11" class="ltx_tr">
<td id="S4.T2.1.1.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T2.1.1.11.1.1" class="ltx_text ltx_font_bold">Yi</span></td>
<td id="S4.T2.1.1.11.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">6B</td>
<td id="S4.T2.1.1.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">63.2</td>
<td id="S4.T2.1.1.11.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">42.8</td>
<td id="S4.T2.1.1.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.0</td>
<td id="S4.T2.1.1.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">75.5</td>
<td id="S4.T2.1.1.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.2</td>
<td id="S4.T2.1.1.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.2</td>
<td id="S4.T2.1.1.11.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">68.7</td>
<td id="S4.T2.1.1.11.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">21.1</td>
<td id="S4.T2.1.1.11.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">18.6</td>
</tr>
<tr id="S4.T2.1.1.12" class="ltx_tr">
<td id="S4.T2.1.1.12.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S4.T2.1.1.12.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">76.3</td>
<td id="S4.T2.1.1.12.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">54.3</td>
<td id="S4.T2.1.1.12.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.12.4.1" class="ltx_text ltx_font_bold">81.4</span></td>
<td id="S4.T2.1.1.12.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.12.5.1" class="ltx_text ltx_font_bold">83.7</span></td>
<td id="S4.T2.1.1.12.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.12.6.1" class="ltx_text ltx_font_bold">82.8</span></td>
<td id="S4.T2.1.1.12.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">80.7</td>
<td id="S4.T2.1.1.12.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.12.8.1" class="ltx_text ltx_font_bold">76.5</span></td>
<td id="S4.T2.1.1.12.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">32.1</td>
<td id="S4.T2.1.1.12.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">40.8</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Overall performance on grouped academic benchmarks compared to open-source base models. <span id="S4.T2.4.1" class="ltx_text ltx_font_bold">CR</span> stands for Commonsense Reasoning. <span id="S4.T2.5.2" class="ltx_text ltx_font_bold">RC</span> stands for Reading Comprehension.
</figcaption>
</figure>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fast and Efficient Inference</h5>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.1" class="ltx_p">We primarily use quantization, dynamic batching, and Paged Attention for improving decoding speed and memory usage.
We use quantization to decrease both the memory footprint and computation demand.
By 4-bit model quantization&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> and 8-bit KV cache quantization&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, we are able to achieve significant GPU memory saving with near-zero performance degradation (e.g., less than 1<math id="S4.SS0.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.1.m1.1a"><mo id="S4.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.1.m1.1c">\%</annotation></semantics></math> accuracy drop in MMLU/CMMLU benchmark).
We use dynamic batching&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite> to minimize the response time and improve batching efficiency.
We use PagedAttention<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> to improve memory utilization and improve decoding.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Long-context Window Support</h5>

<div id="S4.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px5.p1.1" class="ltx_p">We implement and improve computation-communication overlapping, sequence parallelism, and communication compression to support up to 200K context length continue pretraining and finetuning.
Our method to scale the context length to 200K is <span id="S4.SS0.SSS0.Px5.p1.1.1" class="ltx_text ltx_font_italic">solely</span> based on engineering, that is to say, we do not modify the model architecture like sparse, local, or sliding window attention – the model remains using the full attention even the input is 200K.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Safety</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To enhance the model’s trustworthiness and safety, we develop a full-stack Responsible AI Safety Engine (RAISE).
RAISE ensures safe pretraining, alignment, and deployment.
This section discusses our safety measures in the pretraining and alignment stages.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Safety in Pretraining</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">Aligning with standard pretraining data safety practices&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, we build a set of filters based on heuristic rules, keyword matching, and learned classifiers
to remove text containing personal identifiers and private data, and reduce sexual, violent, and extremist content.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Safety in Alignment</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">Informed by existing research in&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, we first build a comprehensive safety taxonomy.
This taxonomy covers a broad spectrum of potential concerns, including environmental disharmony, superstitious, religious sensitivities, discriminatory practices, substance abuse, violent behavior, illegal activities, hate speech, ethical violations, privacy breaches, self-harm, sexually explicit content, mental health issues, and cybersecurity threats. We curated datasets reflecting these categories for a robust alignment, and mix them with our dialog SFT data.
We also include a targeted set of prompts simulating attack scenarios in the alignment phase, which effectively improved the model’s resilience against malicious use.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our evaluation demonstrates that the Yi model family achieves inspiring performance on a wide range of tasks and delivers close to GPT-3.5 user preference rate. We first report the base model performance on standard benchmarks, then we discuss the chat model performance and its user preference rate.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Base Model Performance</h3>

<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Main Results</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">Here we present the results for our base models and several other well-known base models across standard academic benchmarks. While benchmarking open-source models, we observed a disparity between the results generated by our pipeline and those reported in public sources. Upon conducting a more in-depth investigation of this difference, mostly because different models use different prompts, post-processing strategies, and sampling techniques.
These differences may potentially induce significant variations in the outcomes. Our prompt and post-processing strategy remains consistent with the default settings of the original benchmarks<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>.
We use greedy decoding without any post-processing for the generated content. For scores that were not reported publicly (or scores reported with different settings), we try to get results with our pipeline. For scores that can be found publicly, we directly report the existing numbers. We use the following benchmarks, largely following the practice of LLaMA 2&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>:</p>
</div>
<div id="S6.SS1.SSS1.p2" class="ltx_para">
<dl id="S6.I1" class="ltx_description">
<dt id="S6.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Commonsense Reasoning:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix1.p1" class="ltx_para">
<p id="S6.I1.ix1.p1.1" class="ltx_p">We included PIQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, SIQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, HellaSwag<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>, WinoGrande <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, ARC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, OpenBookQA(OBQA)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, and CommonsenseQA(CSQA)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> to assess common sense reasoning. CSQA was exclusively tested using a 7-shot setup, while all other tests were conducted with a 0-shot configuration.</p>
</div>
</dd>
<dt id="S6.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Reading Comprehension:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix2.p1" class="ltx_para">
<p id="S6.I1.ix2.p1.1" class="ltx_p">For reading comprehension, we report the 0-shot average on SQuAD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, QuAC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, and BoolQ<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
</dd>
<dt id="S6.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Math:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix3.p1" class="ltx_para">
<p id="S6.I1.ix3.p1.1" class="ltx_p">We report the average of the GSM8K<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> (8 shot), and MATH<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> (4 shot) benchmarks with pass@1 accuracy without any specific prompting strategy (e.g. Chain-of-Thought prompting) and other ensemble technique (e.g., majority voting).</p>
</div>
</dd>
<dt id="S6.I1.ix4" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix4.1.1.1" class="ltx_text ltx_font_bold">Code:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix4.p1" class="ltx_para">
<p id="S6.I1.ix4.p1.1" class="ltx_p">We report the average pass@1 scores of our models on HumanEval<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> (Chen et al., 2021) and MBPP<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> (Austin et al., 2021).</p>
</div>
</dd>
<dt id="S6.I1.ix5" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix5.1.1.1" class="ltx_text ltx_font_bold">Popular Aggregated Benchmark:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix5.p1" class="ltx_para">
<p id="S6.I1.ix5.p1.1" class="ltx_p">We report the overall results for MMLU<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>(5-shot), CMMLU<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> (5-shot), Gaokao-Bench<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> (5-shot), and BigBench<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> Hard (BBH<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>) (3-shot).</p>
</div>
</dd>
</dl>
</div>
<figure id="S6.T3" class="ltx_table">
<table id="S6.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T3.1.1" class="ltx_tr">
<td id="S6.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S6.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.3.1" class="ltx_text ltx_font_bold">GSM8k</span></td>
<td id="S6.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.4.1" class="ltx_text ltx_font_bold">MATH</span></td>
<td id="S6.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.5.1" class="ltx_text ltx_font_bold">Human-Eval pass@1</span></td>
<td id="S6.T3.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.6.1" class="ltx_text ltx_font_bold">MBPP pass@1</span></td>
</tr>
<tr id="S6.T3.1.2" class="ltx_tr">
<td id="S6.T3.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.2.1.1" class="ltx_text ltx_font_bold">GPT-3.5</span></td>
<td id="S6.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">57.1</td>
<td id="S6.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.0</td>
<td id="S6.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">48.1</td>
<td id="S6.T3.1.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">61.4</td>
</tr>
<tr id="S6.T3.1.3" class="ltx_tr">
<td id="S6.T3.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.1.1" class="ltx_text ltx_font_bold">GPT-4</span></td>
<td id="S6.T3.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T3.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.3.1" class="ltx_text ltx_font_bold">92.0</span></td>
<td id="S6.T3.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.4.1" class="ltx_text ltx_font_bold">40.2</span></td>
<td id="S6.T3.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.5.1" class="ltx_text ltx_font_bold">67.0</span></td>
<td id="S6.T3.1.3.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.6.1" class="ltx_text ltx_font_bold">63.6</span></td>
</tr>
<tr id="S6.T3.1.4" class="ltx_tr">
<td id="S6.T3.1.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.4.1.1" class="ltx_text ltx_font_bold">Falcon</span></td>
<td id="S6.T3.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">180B</td>
<td id="S6.T3.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">54.4</td>
<td id="S6.T3.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T3.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">0.61</td>
<td id="S6.T3.1.4.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">47.0</td>
</tr>
<tr id="S6.T3.1.5" class="ltx_tr">
<td id="S6.T3.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S6.T3.1.5.1.1" class="ltx_text ltx_font_bold">Qwen</span></td>
<td id="S6.T3.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">51.7</td>
<td id="S6.T3.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.6</td>
<td id="S6.T3.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">29.9</td>
<td id="S6.T3.1.5.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">34.0</td>
</tr>
<tr id="S6.T3.1.6" class="ltx_tr">
<td id="S6.T3.1.6.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">14B</td>
<td id="S6.T3.1.6.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">61.3</td>
<td id="S6.T3.1.6.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">24.8</td>
<td id="S6.T3.1.6.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.3</td>
<td id="S6.T3.1.6.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.9</td>
</tr>
<tr id="S6.T3.1.7" class="ltx_tr">
<td id="S6.T3.1.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S6.T3.1.7.1.1" class="ltx_text ltx_font_bold">Baichuan 2</span></td>
<td id="S6.T3.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">24.5</td>
<td id="S6.T3.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">5.6</td>
<td id="S6.T3.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">18.3</td>
<td id="S6.T3.1.7.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">28.3</td>
</tr>
<tr id="S6.T3.1.8" class="ltx_tr">
<td id="S6.T3.1.8.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">13B</td>
<td id="S6.T3.1.8.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">22.1</td>
<td id="S6.T3.1.8.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">10.1</td>
<td id="S6.T3.1.8.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">20.7</td>
<td id="S6.T3.1.8.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">26.1</td>
</tr>
<tr id="S6.T3.1.9" class="ltx_tr">
<td id="S6.T3.1.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="3"><span id="S6.T3.1.9.1.1" class="ltx_text ltx_font_bold">L<span id="S6.T3.1.9.1.1.1" class="ltx_text" style="font-size:83%;">LaMA</span> 2</span></td>
<td id="S6.T3.1.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.7</td>
<td id="S6.T3.1.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">3.3</td>
<td id="S6.T3.1.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.8</td>
<td id="S6.T3.1.9.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.8</td>
</tr>
<tr id="S6.T3.1.10" class="ltx_tr">
<td id="S6.T3.1.10.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S6.T3.1.10.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">42.2</td>
<td id="S6.T3.1.10.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">6.2</td>
<td id="S6.T3.1.10.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">22.6</td>
<td id="S6.T3.1.10.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.0</td>
</tr>
<tr id="S6.T3.1.11" class="ltx_tr">
<td id="S6.T3.1.11.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">70B</td>
<td id="S6.T3.1.11.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">56.8</td>
<td id="S6.T3.1.11.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">13.5</td>
<td id="S6.T3.1.11.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.7</td>
<td id="S6.T3.1.11.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">45.0</td>
</tr>
<tr id="S6.T3.1.12" class="ltx_tr">
<td id="S6.T3.1.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.12.1.1" class="ltx_text ltx_font_bold">Mistral</span></td>
<td id="S6.T3.1.12.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">47.5</td>
<td id="S6.T3.1.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.3</td>
<td id="S6.T3.1.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">30.5</td>
<td id="S6.T3.1.12.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">47.5</td>
</tr>
<tr id="S6.T3.1.13" class="ltx_tr">
<td id="S6.T3.1.13.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.13.1.1" class="ltx_text ltx_font_bold">InternLM</span></td>
<td id="S6.T3.1.13.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">20B</td>
<td id="S6.T3.1.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.9</td>
<td id="S6.T3.1.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">10.9</td>
<td id="S6.T3.1.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">28.1</td>
<td id="S6.T3.1.13.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">41.4</td>
</tr>
<tr id="S6.T3.1.14" class="ltx_tr">
<td id="S6.T3.1.14.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.14.1.1" class="ltx_text ltx_font_bold">Skywork</span></td>
<td id="S6.T3.1.14.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.14.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">55.8</td>
<td id="S6.T3.1.14.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7.8</td>
<td id="S6.T3.1.14.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13.4</td>
<td id="S6.T3.1.14.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">22.8</td>
</tr>
<tr id="S6.T3.1.15" class="ltx_tr">
<td id="S6.T3.1.15.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S6.T3.1.15.1.1" class="ltx_text ltx_font_bold">Yi</span></td>
<td id="S6.T3.1.15.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">6B</td>
<td id="S6.T3.1.15.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">32.5</td>
<td id="S6.T3.1.15.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4.6</td>
<td id="S6.T3.1.15.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.9</td>
<td id="S6.T3.1.15.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">26.3</td>
</tr>
<tr id="S6.T3.1.16" class="ltx_tr">
<td id="S6.T3.1.16.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S6.T3.1.16.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">67.2</td>
<td id="S6.T3.1.16.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">14.4</td>
<td id="S6.T3.1.16.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">23.2</td>
<td id="S6.T3.1.16.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">41.0</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of models on GSM8k, MATH, Human-Eval, and MBPP.</figcaption>
</figure>
<div id="S6.SS1.SSS1.p3" class="ltx_para">
<p id="S6.SS1.SSS1.p3.1" class="ltx_p">By training on a significantly larger number of tokens (3.1T) compared to prior work (usually <math id="S6.SS1.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S6.SS1.SSS1.p3.1.m1.1a"><mo id="S6.SS1.SSS1.p3.1.m1.1.1" xref="S6.SS1.SSS1.p3.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.1.m1.1b"><leq id="S6.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.1.m1.1c">\leq</annotation></semantics></math> 2T), we have observed a substantial performance gain across benchmarks, as shown in Table <a href="#S4.T2" title="Table 2 ‣ Finetuning Framework ‣ 4 Infrastructure ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. However, it is important to note that there are still discernible disparities between our model and existing open-source and close-source models, particularly in tasks related to mathematics and coding. As performance in these domains can be significantly improved by continual pretraining and instruction fine-tuning, we have refrained from incorporating extensive mathematical and coding content in the pretraining corpus when making the initial design choices. We do plan to release models with enhanced math and coding capabilities in the future.</p>
</div>
<figure id="S6.F3" class="ltx_figure"><img src="/html/2403.04652/assets/x3.png" id="S6.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="373" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Evaluating language model’s in-context learning capability by inferring the linear coefficients of a weighted sum.
Considering the discussions of whether emergent ability is an artifact of measurement&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, we use
difference to the target (target number - model prediction) as a continuous measure,
and exact match (target number == model prediction) as a discontinuous measure.
A: when there is two linear coefficients, Yi-34B performs the best when measuring by the difference to the target number.
B: increasing the number of linear coefficients to 5, only models that are large enough (LLaMA2 70B and Mixtral 8x7B) can achieve meaningful exact match, showing that in-context learning complex functions is an emergent ability.
</figcaption>
</figure>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Discussions</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<dl id="S6.I2" class="ltx_description">
<dt id="S6.I2.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I2.ix1.1.1.1" class="ltx_text ltx_font_bold">Gain from Model Scale.</span></span></dt>
<dd class="ltx_item">
<div id="S6.I2.ix1.p1" class="ltx_para">
<p id="S6.I2.ix1.p1.1" class="ltx_p">We observe that Yi-34B has substantial performance improvement compared to Yi-6B, though they utilized the same pretrain corpora.
Larger model size leads to higher performance gain on Code and Math benchmarks, referring to Tab.&nbsp;<a href="#S6.T3" title="Table 3 ‣ 6.1.1 Main Results ‣ 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, compared to benchmarks focusing on Commonsense Reasoning, Reading Comprehension, or Knowledge.</p>
</div>
</dd>
<dt id="S6.I2.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I2.ix2.1.1.1" class="ltx_text ltx_font_bold">Data Quality.</span></span></dt>
<dd class="ltx_item">
<div id="S6.I2.ix2.p1" class="ltx_para">
<p id="S6.I2.ix2.p1.1" class="ltx_p">Smaller models of higher quality pretrain data, like Yi-34B or Qwen-14B, usually demonstrate better performance than models of larger size but (presumably) lower quality data, such as Falcon-180B (though the focus of Falcon-180B might be more on the scaling side, which is definitely of important value on its own).</p>
</div>
</dd>
<dt id="S6.I2.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I2.ix3.1.1.1" class="ltx_text ltx_font_bold">Gap between GPT-4 and Open-source LLMs.</span></span></dt>
<dd class="ltx_item">
<div id="S6.I2.ix3.p1" class="ltx_para">
<p id="S6.I2.ix3.p1.1" class="ltx_p">Based on Tab.&nbsp;<a href="#S4.T2" title="Table 2 ‣ Finetuning Framework ‣ 4 Infrastructure ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we note that open-source LLMs still lag behind the performance of GPT-4 and GPT-3.5 on various benchmarks.
Yet representative bilingual LLMs, e.g. Qwen-14B and Yi-34B, can match or even surpass the performance of GPT-4 on Chinese knowledge related benchmarks, including C-Eval&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, CMMLU&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, and Gaokao&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>.
However, there is still a huge gap between GPT-4 and open-source models on reasoning-related benchmarks like BBH&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, code (HumanEval), and math (MATH).</p>
</div>
</dd>
</dl>
</div>
</section>
<section id="S6.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>In-Context Learning Study</h4>

<div id="S6.SS1.SSS3.p1" class="ltx_para">
<p id="S6.SS1.SSS3.p1.9" class="ltx_p">We further investigate the in-context learning capability, i.e., the capability of inferring the underlying function given the few-show input-output demonstrations.
We consider the task of inferring the linear coefficient of a weighted sum.
Specifically, define <math id="S6.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="y=w_{1}x_{1}+w2x_{2}+...+w_{n}x_{n}" display="inline"><semantics id="S6.SS1.SSS3.p1.1.m1.1a"><mrow id="S6.SS1.SSS3.p1.1.m1.1.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.2.cmml">y</mi><mo id="S6.SS1.SSS3.p1.1.m1.1.1.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S6.SS1.SSS3.p1.1.m1.1.1.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.cmml"><mrow id="S6.SS1.SSS3.p1.1.m1.1.1.3.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.cmml"><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.2.cmml">w</mi><mn id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.1.cmml">​</mo><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.2.cmml">x</mi><mn id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.3.cmml">1</mn></msub></mrow><mo id="S6.SS1.SSS3.p1.1.m1.1.1.3.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S6.SS1.SSS3.p1.1.m1.1.1.3.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1.cmml">​</mo><mn id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1a" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1.cmml">​</mo><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.2.cmml">x</mi><mn id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.3.cmml">2</mn></msub></mrow><mo id="S6.SS1.SSS3.p1.1.m1.1.1.3.1a" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">+</mo><mi mathvariant="normal" id="S6.SS1.SSS3.p1.1.m1.1.1.3.4" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.4.cmml">…</mi><mo id="S6.SS1.SSS3.p1.1.m1.1.1.3.1b" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S6.SS1.SSS3.p1.1.m1.1.1.3.5" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.cmml"><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.2.cmml">w</mi><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.3.cmml">n</mi></msub><mo lspace="0em" rspace="0em" id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.1.cmml">​</mo><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.2.cmml">x</mi><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.3.cmml">n</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.1.m1.1b"><apply id="S6.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1"><eq id="S6.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.1"></eq><ci id="S6.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.2">𝑦</ci><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3"><plus id="S6.SS1.SSS3.p1.1.m1.1.1.3.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.1"></plus><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2"><times id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.1"></times><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.2">𝑤</ci><cn type="integer" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.3">1</cn></apply><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.2">𝑥</ci><cn type="integer" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.3">1</cn></apply></apply><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3"><times id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1"></times><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.2">𝑤</ci><cn type="integer" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.3">2</cn><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.2">𝑥</ci><cn type="integer" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.3">2</cn></apply></apply><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.4.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.4">…</ci><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5"><times id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.1"></times><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.2">𝑤</ci><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.3">𝑛</ci></apply><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.2">𝑥</ci><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.1.m1.1c">y=w_{1}x_{1}+w2x_{2}+...+w_{n}x_{n}</annotation></semantics></math>, our few-shot demonstration is <math id="S6.SS1.SSS3.p1.2.m2.5" class="ltx_Math" alttext="x_{1},x_{2},...,x_{n},y" display="inline"><semantics id="S6.SS1.SSS3.p1.2.m2.5a"><mrow id="S6.SS1.SSS3.p1.2.m2.5.5.3" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml"><msub id="S6.SS1.SSS3.p1.2.m2.3.3.1.1" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.cmml"><mi id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.2" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.2.cmml">x</mi><mn id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.3" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.3.cmml">1</mn></msub><mo id="S6.SS1.SSS3.p1.2.m2.5.5.3.4" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml">,</mo><msub id="S6.SS1.SSS3.p1.2.m2.4.4.2.2" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.cmml"><mi id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.2" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.2.cmml">x</mi><mn id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.3" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.3.cmml">2</mn></msub><mo id="S6.SS1.SSS3.p1.2.m2.5.5.3.5" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml">,</mo><mi mathvariant="normal" id="S6.SS1.SSS3.p1.2.m2.1.1" xref="S6.SS1.SSS3.p1.2.m2.1.1.cmml">…</mi><mo id="S6.SS1.SSS3.p1.2.m2.5.5.3.6" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml">,</mo><msub id="S6.SS1.SSS3.p1.2.m2.5.5.3.3" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.cmml"><mi id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.2" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.2.cmml">x</mi><mi id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.3" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.3.cmml">n</mi></msub><mo id="S6.SS1.SSS3.p1.2.m2.5.5.3.7" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml">,</mo><mi id="S6.SS1.SSS3.p1.2.m2.2.2" xref="S6.SS1.SSS3.p1.2.m2.2.2.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.2.m2.5b"><list id="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3"><apply id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1">subscript</csymbol><ci id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.2">𝑥</ci><cn type="integer" id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.3.cmml" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.3">1</cn></apply><apply id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2">subscript</csymbol><ci id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.2">𝑥</ci><cn type="integer" id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.3.cmml" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.3">2</cn></apply><ci id="S6.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.1.1">…</ci><apply id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3">subscript</csymbol><ci id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.2">𝑥</ci><ci id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.3.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.3">𝑛</ci></apply><ci id="S6.SS1.SSS3.p1.2.m2.2.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.2.2">𝑦</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.2.m2.5c">x_{1},x_{2},...,x_{n},y</annotation></semantics></math>, and we ask the model to (implicitly) infer <math id="S6.SS1.SSS3.p1.3.m3.4" class="ltx_Math" alttext="w_{1},w_{2},...,w_{n}" display="inline"><semantics id="S6.SS1.SSS3.p1.3.m3.4a"><mrow id="S6.SS1.SSS3.p1.3.m3.4.4.3" xref="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml"><msub id="S6.SS1.SSS3.p1.3.m3.2.2.1.1" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.cmml"><mi id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.2" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.2.cmml">w</mi><mn id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.3" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S6.SS1.SSS3.p1.3.m3.4.4.3.4" xref="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml">,</mo><msub id="S6.SS1.SSS3.p1.3.m3.3.3.2.2" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.cmml"><mi id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.2" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.2.cmml">w</mi><mn id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.3" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.3.cmml">2</mn></msub><mo id="S6.SS1.SSS3.p1.3.m3.4.4.3.5" xref="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S6.SS1.SSS3.p1.3.m3.1.1" xref="S6.SS1.SSS3.p1.3.m3.1.1.cmml">…</mi><mo id="S6.SS1.SSS3.p1.3.m3.4.4.3.6" xref="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml">,</mo><msub id="S6.SS1.SSS3.p1.3.m3.4.4.3.3" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.cmml"><mi id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.2" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.2.cmml">w</mi><mi id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.3" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.3.m3.4b"><list id="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3"><apply id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.2.cmml" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.2">𝑤</ci><cn type="integer" id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.3.cmml" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.3">1</cn></apply><apply id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.cmml" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.2.cmml" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.2">𝑤</ci><cn type="integer" id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.3.cmml" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.3">2</cn></apply><ci id="S6.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.1.1">…</ci><apply id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3">subscript</csymbol><ci id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.2.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.2">𝑤</ci><ci id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.3.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.3.m3.4c">w_{1},w_{2},...,w_{n}</annotation></semantics></math> by predicting the <math id="S6.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S6.SS1.SSS3.p1.4.m4.1a"><mi id="S6.SS1.SSS3.p1.4.m4.1.1" xref="S6.SS1.SSS3.p1.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.4.m4.1b"><ci id="S6.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S6.SS1.SSS3.p1.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.4.m4.1c">y</annotation></semantics></math> given a new set of input <math id="S6.SS1.SSS3.p1.5.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S6.SS1.SSS3.p1.5.m5.1a"><mi id="S6.SS1.SSS3.p1.5.m5.1.1" xref="S6.SS1.SSS3.p1.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.5.m5.1b"><ci id="S6.SS1.SSS3.p1.5.m5.1.1.cmml" xref="S6.SS1.SSS3.p1.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.5.m5.1c">x</annotation></semantics></math>.
We use (a). the absolute difference between model prediction <math id="S6.SS1.SSS3.p1.6.m6.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S6.SS1.SSS3.p1.6.m6.1a"><mi id="S6.SS1.SSS3.p1.6.m6.1.1" xref="S6.SS1.SSS3.p1.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.6.m6.1b"><ci id="S6.SS1.SSS3.p1.6.m6.1.1.cmml" xref="S6.SS1.SSS3.p1.6.m6.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.6.m6.1c">y</annotation></semantics></math> and the ground truth <math id="S6.SS1.SSS3.p1.7.m7.1" class="ltx_Math" alttext="y^{*}" display="inline"><semantics id="S6.SS1.SSS3.p1.7.m7.1a"><msup id="S6.SS1.SSS3.p1.7.m7.1.1" xref="S6.SS1.SSS3.p1.7.m7.1.1.cmml"><mi id="S6.SS1.SSS3.p1.7.m7.1.1.2" xref="S6.SS1.SSS3.p1.7.m7.1.1.2.cmml">y</mi><mo id="S6.SS1.SSS3.p1.7.m7.1.1.3" xref="S6.SS1.SSS3.p1.7.m7.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.7.m7.1b"><apply id="S6.SS1.SSS3.p1.7.m7.1.1.cmml" xref="S6.SS1.SSS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.7.m7.1.1.1.cmml" xref="S6.SS1.SSS3.p1.7.m7.1.1">superscript</csymbol><ci id="S6.SS1.SSS3.p1.7.m7.1.1.2.cmml" xref="S6.SS1.SSS3.p1.7.m7.1.1.2">𝑦</ci><times id="S6.SS1.SSS3.p1.7.m7.1.1.3.cmml" xref="S6.SS1.SSS3.p1.7.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.7.m7.1c">y^{*}</annotation></semantics></math>, i.e., <math id="S6.SS1.SSS3.p1.8.m8.1" class="ltx_Math" alttext="|y-y^{*}|" display="inline"><semantics id="S6.SS1.SSS3.p1.8.m8.1a"><mrow id="S6.SS1.SSS3.p1.8.m8.1.1.1" xref="S6.SS1.SSS3.p1.8.m8.1.1.2.cmml"><mo stretchy="false" id="S6.SS1.SSS3.p1.8.m8.1.1.1.2" xref="S6.SS1.SSS3.p1.8.m8.1.1.2.1.cmml">|</mo><mrow id="S6.SS1.SSS3.p1.8.m8.1.1.1.1" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.cmml"><mi id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.2" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.2.cmml">y</mi><mo id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.1" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.1.cmml">−</mo><msup id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.cmml"><mi id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.2" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.2.cmml">y</mi><mo id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.3" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo stretchy="false" id="S6.SS1.SSS3.p1.8.m8.1.1.1.3" xref="S6.SS1.SSS3.p1.8.m8.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.8.m8.1b"><apply id="S6.SS1.SSS3.p1.8.m8.1.1.2.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1"><abs id="S6.SS1.SSS3.p1.8.m8.1.1.2.1.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.2"></abs><apply id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1"><minus id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.1.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.1"></minus><ci id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.2.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.2">𝑦</ci><apply id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.1.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3">superscript</csymbol><ci id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.2.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.2">𝑦</ci><times id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.3.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.8.m8.1c">|y-y^{*}|</annotation></semantics></math> as a continuous measure, and use (b). the exact match <math id="S6.SS1.SSS3.p1.9.m9.1" class="ltx_math_unparsed" alttext="y==y^{*}" display="inline"><semantics id="S6.SS1.SSS3.p1.9.m9.1a"><mrow id="S6.SS1.SSS3.p1.9.m9.1b"><mi id="S6.SS1.SSS3.p1.9.m9.1.1">y</mi><mo rspace="0em" id="S6.SS1.SSS3.p1.9.m9.1.2">=</mo><mo lspace="0em" id="S6.SS1.SSS3.p1.9.m9.1.3">=</mo><msup id="S6.SS1.SSS3.p1.9.m9.1.4"><mi id="S6.SS1.SSS3.p1.9.m9.1.4.2">y</mi><mo id="S6.SS1.SSS3.p1.9.m9.1.4.3">∗</mo></msup></mrow><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.9.m9.1c">y==y^{*}</annotation></semantics></math> as a discontinuous measure.
We further note that most of the models perform reasonably well on addition and subtraction, so the ability to do arithmetic, as a confounding factor, can be ruled out.</p>
</div>
<div id="S6.SS1.SSS3.p2" class="ltx_para">
<p id="S6.SS1.SSS3.p2.1" class="ltx_p">The results are shown in Figure&nbsp;<a href="#S6.F3" title="Figure 3 ‣ 6.1.1 Main Results ‣ 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
When setting the linear coefficients of be [1, -1], we see that Yi 34B and LLaMA-2 70B performs the best in-terms of answer exact match.
If we increase the number of the linear coefficients to be [1, 1, 1, 1, 1], we observe the emergent behavior that only large models (LLaMA-2 70B and Mixtral) can achieve good scores on exact match, although the differences to target is more continuous.
These observations give side evidence for Yi-34B’s performance on in-context learning and indicates that further scaling may allow the model to infer more complicated functions by in-context learning.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Chat Model Performance</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In this section, we report the automatic and human preference evaluation of the Chat Model.
We use greedy decoding to generate responses. For the automatic evaluation benchmarks, we extract answers from the model’s generated outputs and calculate accuracy. During the evaluation process, we observed that different prompts have varying influence on results. Therefore, for the same set of questions, we use identical prompts to evaluate all models, aiming to ensure as fair and unbiased results as possible.</p>
</div>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Automatic Evaluations</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.1" class="ltx_p">For automatic evaluation, we use the same benchmarks as is for the base model, detailed in Sec. <a href="#S6.SS1.SSS1" title="6.1.1 Main Results ‣ 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1.1</span></a>.
We use both zero-shot and few-shot methods but generally, zero-shot is more suitable for chat models. Our evaluation involves generating responses while following instructions explicitly or implicitly (such as the format in the few-shot examples). We then isolate relevant answers from the generated text. Unlike the base model, for the zero-shot evaluations on the GSM8K and BBH datasets, we employ the Chain-of-Thought (CoT) approach to guide the model in deliberation before reaching an answer.</p>
</div>
<div id="S6.SS2.SSS1.p2" class="ltx_para">
<p id="S6.SS2.SSS1.p2.1" class="ltx_p">The results shown in Tab. <a href="#S6.T4" title="Table 4 ‣ 6.2.1 Automatic Evaluations ‣ 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrate the effectiveness of our chat models in understanding human instructions and generating appropriate instruction-following responses.
We particularly highlight the 4-bit quantization results, as 4-bit quantization substantially reduces the memory requirement while the model performance nearly does not drop.
This observation serve as the foundation of serving the model on consumer-grade devices.</p>
</div>
<div id="S6.SS2.SSS1.p3" class="ltx_para">
<p id="S6.SS2.SSS1.p3.1" class="ltx_p">In line with Goodhart’s principle, when a measurement metric becomes the target of our pursuit, it ceases to serve as a reliable standard of assessment. Consequently, the outcomes of our evaluations on benchmarks are exclusively employed for ensuring that our alignment training does not detrimentally impact the foundational knowledge and capabilities of the base model. We do not engage in targeted optimization of our chat model with the objective of enhancing benchmark performance.</p>
</div>
<div id="S6.SS2.SSS1.p4" class="ltx_para">
<p id="S6.SS2.SSS1.p4.1" class="ltx_p">To further evaluate the generalizability of our model’s capabilities, we conducted assessments of its mathematical computation proficiency by subjecting it to the 2023 Hungarian high school mathematics final exam questions, first proposed by the xAI Grok team then reproduced by&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Paster [<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. This evaluation was undertaken with the aim of determining whether our model exhibited signs of overfitting to training datasets that are mathematically oriented.
The results in Fig.&nbsp;<a href="#S6.F4" title="Figure 4 ‣ 6.2.1 Automatic Evaluations ‣ 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show that Yi-34B-Chat performs inspiringly on both the GSM8K and the Hungarian mathematics exam. However, note that Yi-6B-Chat does not exhibit strong mathematical capabilities (on both GSM8K and the Hungarian mathematics exam). We speculate that smaller models may require more data to activate their corresponding abilities during the SFT stage.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<div id="S6.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:198.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.2pt,27.4pt) scale(0.782765244495076,0.782765244495076) ;">
<table id="S6.T4.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S6.T4.1.1.1" class="ltx_tr">
<td id="S6.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S6.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">MMLU</span></td>
<td id="S6.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">CMMLU</span></td>
<td id="S6.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.5.1" class="ltx_text ltx_font_bold">C-Eval(val)</span></td>
<td id="S6.T4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.6.1" class="ltx_text ltx_font_bold">TruthfulQA</span></td>
<td id="S6.T4.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.7.1" class="ltx_text ltx_font_bold">BBH</span></td>
<td id="S6.T4.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.8.1" class="ltx_text ltx_font_bold">GSM8K</span></td>
</tr>
<tr id="S6.T4.1.1.2" class="ltx_tr">
<td id="S6.T4.1.1.2.1" class="ltx_td" style="padding:1pt 2.0pt;"></td>
<td id="S6.T4.1.1.2.2" class="ltx_td" style="padding:1pt 2.0pt;"></td>
<td id="S6.T4.1.1.2.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.3.1" class="ltx_text ltx_font_bold">0-shot / 5-shot</span></td>
<td id="S6.T4.1.1.2.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.4.1" class="ltx_text ltx_font_bold">0-shot / 5-shot</span></td>
<td id="S6.T4.1.1.2.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.5.1" class="ltx_text ltx_font_bold">0-shot / 5-shot</span></td>
<td id="S6.T4.1.1.2.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.6.1" class="ltx_text ltx_font_bold">0-shot</span></td>
<td id="S6.T4.1.1.2.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.7.1" class="ltx_text ltx_font_bold">0-shot / 3-shot</span></td>
<td id="S6.T4.1.1.2.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.8.1" class="ltx_text ltx_font_bold">0-shot / 4-shot</span></td>
</tr>
<tr id="S6.T4.1.1.3" class="ltx_tr">
<td id="S6.T4.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">LLaMA2-Chat</td>
<td id="S6.T4.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">13B</td>
<td id="S6.T4.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">50.9 / 47.3</td>
<td id="S6.T4.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">27.5 / 35.1</td>
<td id="S6.T4.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">27.9 / 35.9</td>
<td id="S6.T4.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">36.8</td>
<td id="S6.T4.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">32.9 / 58.2</td>
<td id="S6.T4.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">36.9 / 2.7</td>
</tr>
<tr id="S6.T4.1.1.4" class="ltx_tr">
<td id="S6.T4.1.1.4.1" class="ltx_td" style="padding:1pt 2.0pt;"></td>
<td id="S6.T4.1.1.4.2" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">70B</td>
<td id="S6.T4.1.1.4.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">59.4 / 59.9</td>
<td id="S6.T4.1.1.4.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">36.1 / 41.0</td>
<td id="S6.T4.1.1.4.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">35.0 / 41.3</td>
<td id="S6.T4.1.1.4.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">54.0</td>
<td id="S6.T4.1.1.4.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">42.4 / 58.5</td>
<td id="S6.T4.1.1.4.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">47.1 / 58.7</td>
</tr>
<tr id="S6.T4.1.1.5" class="ltx_tr">
<td id="S6.T4.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">Baichuan2-Chat</td>
<td id="S6.T4.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">13B</td>
<td id="S6.T4.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">55.1 / 50.1</td>
<td id="S6.T4.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">58.6 / 59.5</td>
<td id="S6.T4.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">56.0 / 54.8</td>
<td id="S6.T4.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">49.0</td>
<td id="S6.T4.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">38.8 / 47.2</td>
<td id="S6.T4.1.1.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">45.7 / 23.3</td>
</tr>
<tr id="S6.T4.1.1.6" class="ltx_tr">
<td id="S6.T4.1.1.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">Qwen-Chat</td>
<td id="S6.T4.1.1.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">14B</td>
<td id="S6.T4.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">64.0 / 65.0</td>
<td id="S6.T4.1.1.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">67.7 / 70.6</td>
<td id="S6.T4.1.1.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">66.1 / 70.1</td>
<td id="S6.T4.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">52.5</td>
<td id="S6.T4.1.1.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">49.7 / 55.0</td>
<td id="S6.T4.1.1.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">59.5 / 61.2</td>
</tr>
<tr id="S6.T4.1.1.7" class="ltx_tr">
<td id="S6.T4.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">InternLM-Chat</td>
<td id="S6.T4.1.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">20B</td>
<td id="S6.T4.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">55.6 / 57.4</td>
<td id="S6.T4.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">53.6 / 53.8</td>
<td id="S6.T4.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">51.2 / 53.6</td>
<td id="S6.T4.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">51.8</td>
<td id="S6.T4.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">42.4 / 36.7</td>
<td id="S6.T4.1.1.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">15.7 / 43.4</td>
</tr>
<tr id="S6.T4.1.1.8" class="ltx_tr">
<td id="S6.T4.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">AquilaChat2</td>
<td id="S6.T4.1.1.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">34B</td>
<td id="S6.T4.1.1.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">65.2 / 66.7</td>
<td id="S6.T4.1.1.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">67.5 / 70.0</td>
<td id="S6.T4.1.1.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.8.5.1" class="ltx_text ltx_font_bold">83.0</span> / <span id="S6.T4.1.1.8.5.2" class="ltx_text ltx_font_bold">89.4</span>
</td>
<td id="S6.T4.1.1.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.8.6.1" class="ltx_text ltx_font_bold">64.3</span></td>
<td id="S6.T4.1.1.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">20.1 / 34.3</td>
<td id="S6.T4.1.1.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">11.5 / 48.5</td>
</tr>
<tr id="S6.T4.1.1.9" class="ltx_tr">
<td id="S6.T4.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">Yi-Chat</td>
<td id="S6.T4.1.1.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">6B</td>
<td id="S6.T4.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">58.2 / 61.0</td>
<td id="S6.T4.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">69.4 / 74.7</td>
<td id="S6.T4.1.1.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">68.8 / 74.2</td>
<td id="S6.T4.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">50.6</td>
<td id="S6.T4.1.1.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">39.7 / 47.2</td>
<td id="S6.T4.1.1.9.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">38.4 / 44.9</td>
</tr>
<tr id="S6.T4.1.1.10" class="ltx_tr">
<td id="S6.T4.1.1.10.1" class="ltx_td ltx_align_left" style="padding:1pt 2.0pt;">Yi-Chat-8bits(GPTQ)</td>
<td id="S6.T4.1.1.10.2" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">6B</td>
<td id="S6.T4.1.1.10.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">58.3 / 61.0</td>
<td id="S6.T4.1.1.10.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">69.2 / 74.7</td>
<td id="S6.T4.1.1.10.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">69.2 / 73.9</td>
<td id="S6.T4.1.1.10.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">49.9</td>
<td id="S6.T4.1.1.10.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">40.4 / 47.3</td>
<td id="S6.T4.1.1.10.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">39.4 / 44.9</td>
</tr>
<tr id="S6.T4.1.1.11" class="ltx_tr">
<td id="S6.T4.1.1.11.1" class="ltx_td ltx_align_left" style="padding:1pt 2.0pt;">Yi-Chat-4bits(AWQ)</td>
<td id="S6.T4.1.1.11.2" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">6B</td>
<td id="S6.T4.1.1.11.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">56.8 / 59.9</td>
<td id="S6.T4.1.1.11.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">67.7 / 73.3</td>
<td id="S6.T4.1.1.11.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">67.5 / 72.3</td>
<td id="S6.T4.1.1.11.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">50.3</td>
<td id="S6.T4.1.1.11.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">37.7 / 43.6</td>
<td id="S6.T4.1.1.11.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">35.7 / 38.4</td>
</tr>
<tr id="S6.T4.1.1.12" class="ltx_tr">
<td id="S6.T4.1.1.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">Yi-Chat</td>
<td id="S6.T4.1.1.12.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">34B</td>
<td id="S6.T4.1.1.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.12.3.1" class="ltx_text ltx_font_bold">67.6</span> / 73.5</td>
<td id="S6.T4.1.1.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.12.4.1" class="ltx_text ltx_font_bold">79.1</span> / <span id="S6.T4.1.1.12.4.2" class="ltx_text ltx_font_bold">81.3</span>
</td>
<td id="S6.T4.1.1.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">77.0 / 78.5</td>
<td id="S6.T4.1.1.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">62.4</td>
<td id="S6.T4.1.1.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">51.4 / <span id="S6.T4.1.1.12.7.1" class="ltx_text ltx_font_bold">71.7</span>
</td>
<td id="S6.T4.1.1.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.12.8.1" class="ltx_text ltx_font_bold">71.7</span> / <span id="S6.T4.1.1.12.8.2" class="ltx_text ltx_font_bold">76.0</span>
</td>
</tr>
<tr id="S6.T4.1.1.13" class="ltx_tr">
<td id="S6.T4.1.1.13.1" class="ltx_td ltx_align_left" style="padding:1pt 2.0pt;">Yi-Chat-8bits(GPTQ)</td>
<td id="S6.T4.1.1.13.2" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">34B</td>
<td id="S6.T4.1.1.13.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">66.2 / <span id="S6.T4.1.1.13.3.1" class="ltx_text ltx_font_bold">73.7</span>
</td>
<td id="S6.T4.1.1.13.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">79.1 / 81.2</td>
<td id="S6.T4.1.1.13.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">76.8 / 79.0</td>
<td id="S6.T4.1.1.13.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">61.8</td>
<td id="S6.T4.1.1.13.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.13.7.1" class="ltx_text ltx_font_bold">52.1</span> / 71.0</td>
<td id="S6.T4.1.1.13.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">70.7 / 75.7</td>
</tr>
<tr id="S6.T4.1.1.14" class="ltx_tr">
<td id="S6.T4.1.1.14.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:1pt 2.0pt;">Yi-Chat-4bits(AWQ)</td>
<td id="S6.T4.1.1.14.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">34B</td>
<td id="S6.T4.1.1.14.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">65.8 / 72.4</td>
<td id="S6.T4.1.1.14.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">78.2 / 80.5</td>
<td id="S6.T4.1.1.14.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">75.7 / 77.3</td>
<td id="S6.T4.1.1.14.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">61.8</td>
<td id="S6.T4.1.1.14.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">48.3 / 69.4</td>
<td id="S6.T4.1.1.14.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">70.5 / 74.0</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Overall performance on automatic benchmarks compared to open-source chat models. We highlight the 4-bit quantization results, as 4-bit quantization substantially reduces the memory requirement while the model performance nearly does not drop.
This observation serve as the foundation of serving the model on consumer-grade devices, e.g., RTX4090.
</figcaption>
</figure>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2403.04652/assets/x4.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="248" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Yi’s result of Hungarian mathematics exam.
</figcaption>
</figure>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Human Evaluations</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p id="S6.SS2.SSS2.p1.1" class="ltx_p">In this section we conducted an assessment of the model’s conversational abilities, considering aspects to ensure its effectiveness and safety. We have compiled a collection of open-source evaluation datasets from the community, such as alpaca-eval<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, Belle-eval&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, and MT-bench<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>. Additionally, we have established our own helpful and harmless evaluation dataset by gathering and constructing data of varying difficulty levels, for the purpose of comprehensively assessing the conversational abilities of chat models.</p>
</div>
<div id="S6.SS2.SSS2.p2" class="ltx_para">
<p id="S6.SS2.SSS2.p2.1" class="ltx_p">However, whether it is a public evaluation set or a self-built evaluation set, the evaluation results are strongly influenced by the assessment criteria and the design of the prompt. Our internal evaluation results may be unfair to other models, making it difficult to accurately represent the true capability level of our model.
Therefore, here we only present external evaluation results to demonstrate the current conversational abilities of our chat model.
We consider:
(1). AlapcaEval<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://tatsu-lab.github.io/alpaca_eval/</span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, which is designed to assess the English conversation capabilities of models by comparing the responses of a specified model to reference replies from Davinci003&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> in order to calculate a win-rate;
(2). LMSys<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> Chatbot Arena, which showcases the responses of different models through a dialogue platform, then asks users to make selections based on their preferences, then computes the Elo score; (3). SuperClue<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.superclueai.com/</span></span></span>, on the other hand, is a leaderboard aimed at comprehensively evaluating the Chinese language capabilities of models.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<div id="S6.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:311.8pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.3pt,4.5pt) scale(0.9,0.9) ;">
<table id="S6.T5.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S6.T5.1.1.1" class="ltx_tr">
<td id="S6.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S6.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">AlpacaEval</span></td>
<td id="S6.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.4.1" class="ltx_text ltx_font_bold">LMSys Chatbot Arena</span></td>
<td id="S6.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.5.1" class="ltx_text ltx_font_bold">SuperClue</span></td>
</tr>
<tr id="S6.T5.1.1.2" class="ltx_tr">
<td id="S6.T5.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">GPT-4-Turbo</td>
<td id="S6.T5.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T5.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.2.3.1" class="ltx_text ltx_font_bold">97.7</span></td>
<td id="S6.T5.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.2.4.1" class="ltx_text ltx_font_bold">1243</span></td>
<td id="S6.T5.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.2.5.1" class="ltx_text ltx_font_bold">89.79</span></td>
</tr>
<tr id="S6.T5.1.1.3" class="ltx_tr">
<td id="S6.T5.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">GPT-3.5-Turbo</td>
<td id="S6.T5.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T5.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">89.37</td>
<td id="S6.T5.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">1117</td>
<td id="S6.T5.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">59.39</td>
</tr>
<tr id="S6.T5.1.1.4" class="ltx_tr">
<td id="S6.T5.1.1.4.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">LLaMA2-Chat</td>
<td id="S6.T5.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">70B</td>
<td id="S6.T5.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">92.66</td>
<td id="S6.T5.1.1.4.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">1077</td>
<td id="S6.T5.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
</tr>
<tr id="S6.T5.1.1.5" class="ltx_tr">
<td id="S6.T5.1.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">Yi-Chat</td>
<td id="S6.T5.1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S6.T5.1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">94.08</td>
<td id="S6.T5.1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">1110</td>
<td id="S6.T5.1.1.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">71.87</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Human evaluation comparison with other open-source chat models.</figcaption>
</figure>
<div id="S6.SS2.SSS2.p3" class="ltx_para">
<p id="S6.SS2.SSS2.p3.1" class="ltx_p">Tab. <a href="#S6.T5" title="Table 5 ‣ 6.2.2 Human Evaluations ‣ 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the performance results of Yi-34B-Chat in the three third-party evaluations we consider, with the cutoff date for the results being December 21, 2023. The data demonstrates that, although there is still a gap compared to GPT-4, our model exhibits proficient bilingual (Chinese and English) dialogue capabilities and aligns well with user preferences. Additional comparative results of various models are accessible for review on the official website.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2403.04652/assets/x5.png" id="S6.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="151" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>SFT data scaling curve. Compared with UltraChat and its cleaned version UltraChat 200K, our SFT data demonstrates clear scaling advantages. We attribute its steep slope to the data quality.
</figcaption>
</figure>
<div id="S6.SS2.SSS2.p4" class="ltx_para">
<p id="S6.SS2.SSS2.p4.1" class="ltx_p">We further demonstrate the data quality by comparing the speed of preference increase during data scaling.
As is shown in Fig.&nbsp;<a href="#S6.F5" title="Figure 5 ‣ 6.2.2 Human Evaluations ‣ 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, when compared with UltraChat&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and its cleaned version UltraChat 200K, we see a clear tendency of performance improvements when scaling up the Yi data.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Capability Extension</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this section, we discuss our post-training methods to extend the Yi base model to 200K long-context, equip it with visual understanding capability, and enhance the 6B model by depth upsacaling.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Long Context Modeling</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Our long-context solution consists of a continual pretraining and a finetuning phase, both are light-weight.
We hold the basic hypothesis that the potential of utilizing information anywhere within the 200K input context is already
exist in the base model (same as&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Fu et&nbsp;al. <a href="#bib.bib22" title="" class="ltx_ref">22</a></cite>), the continue pretraining phase “unlocks” such capability, evidenced by a strong performance on Needle-in-a-Haystack test, then the finetuning phase further adapt the style of response to follow human instruction and preference.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p"><span id="S7.SS1.p2.1.1" class="ltx_text ltx_font_bold">Continue Pretraining</span> We continue pretrain the full-attention model using sequence parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and distributed attention.
This is to say, we do not use any sparse or linear attention, but use a brute force implementation of the full attention.
We continue pretrain the Yi 6B/ 34B base model on the data mixture of
(1). original pretraining data, as is introduced in section&nbsp;<a href="#S2" title="2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>;
(2). length-upsampled long-context data, where the long documents are mostly from books;
(3). multi-document question-answering synthetic data, where we construct QA pairs where the answer contains a recitation of the related paragraph before the answer.
Our data approach mostly follows the data engineering practice in&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Fu et&nbsp;al. [<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Yu et&nbsp;al. [<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>.
We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps.
Aligning with the concurrent work from&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Fu et&nbsp;al. [<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure&nbsp;<a href="#S7.F6" title="Figure 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p"><span id="S7.SS1.p3.1.1" class="ltx_text ltx_font_bold">Supervised Finetuning</span> We mix our short-context SFT data with long-context document question-answering data.
We use model-assisted automated methods (i.e., synthetic data) to construct document QA.
Specifically, we randomly concatenate multiple documents into a sequence, sample one or more paragraphs from the long sequence, and ask a chat model to construct question and answer pairs based on the sampled paragraph.
One important detail is recitation and rephrasing: before giving the answer, we ask the model to recite or paraphrase the original paragraph.
This data format encourages the model’s retrieval behavior and consequently discourages the hallucination behavior: given a question, the model is more likely to use the information within the input to construct the answer, rather than use its internal knowledge, which may be related but inaccurate.
Our finetuned model is deployed at <a href="www.wanzhi01.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.wanzhi01.com</a>, and we encourage the readers to try it out.</p>
</div>
<figure id="S7.F6" class="ltx_figure"><img src="/html/2403.04652/assets/x6.png" id="S7.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="225" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Needle-in-a-Haystack performance of Yi-34B-200K.
X-axis means length of the document, and Y-axis means the depth of the needle sentence within the document.
We continue pretrain the model on 5B tokens long-context data mixture and demonstrates a near-all-green performance.
</figcaption>
</figure>
<figure id="S7.T6" class="ltx_table">
<table id="S7.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S7.T6.1.1" class="ltx_tr">
<td id="S7.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Model</td>
<td id="S7.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.2.1" class="ltx_text ltx_font_bold">Average</span></td>
<td id="S7.T6.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.3.1" class="ltx_text ltx_font_bold">Humanity</span></td>
<td id="S7.T6.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.4.1" class="ltx_text ltx_font_bold">STEM</span></td>
<td id="S7.T6.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.5.1" class="ltx_text ltx_font_bold">Social Science</span></td>
<td id="S7.T6.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.6.1" class="ltx_text ltx_font_bold">Other</span></td>
</tr>
<tr id="S7.T6.1.2" class="ltx_tr">
<td id="S7.T6.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Yi-6B 4K</td>
<td id="S7.T6.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">63.24</td>
<td id="S7.T6.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">59.10</td>
<td id="S7.T6.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">53.15</td>
<td id="S7.T6.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">73.83</td>
<td id="S7.T6.1.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">69.26</td>
</tr>
<tr id="S7.T6.1.3" class="ltx_tr">
<td id="S7.T6.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Yi-6B 200K</td>
<td id="S7.T6.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">61.73</td>
<td id="S7.T6.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">56.17</td>
<td id="S7.T6.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">52.36</td>
<td id="S7.T6.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">72.54</td>
<td id="S7.T6.1.3.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">68.94</td>
</tr>
<tr id="S7.T6.1.4" class="ltx_tr">
<td id="S7.T6.1.4.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Yi-34B 4K</td>
<td id="S7.T6.1.4.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">76.32</td>
<td id="S7.T6.1.4.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">73.17</td>
<td id="S7.T6.1.4.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">68.03</td>
<td id="S7.T6.1.4.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">85.11</td>
<td id="S7.T6.1.4.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">80.78</td>
</tr>
<tr id="S7.T6.1.5" class="ltx_tr">
<td id="S7.T6.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">Yi-34B 200K</td>
<td id="S7.T6.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">75.56</td>
<td id="S7.T6.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">72.20</td>
<td id="S7.T6.1.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">66.83</td>
<td id="S7.T6.1.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">84.76</td>
<td id="S7.T6.1.5.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">80.40</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance on MMLU after 200K adaptation. Extending the context length to 200K does not significantly change the short context capability.
</figcaption>
</figure>
<div id="S7.SS1.p4" class="ltx_para">
<p id="S7.SS1.p4.1" class="ltx_p">The performance of the 200K models is shown in figure.&nbsp;<a href="#S7.F6" title="Figure 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and table&nbsp;<a href="#S7.T6" title="Table 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
Specifically, Figure&nbsp;<a href="#S7.F6" title="Figure 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the famous Needle-in-a-Haystack test of Yi-34B-200K, though we tend to view that this level of retrieval is relatively easy for long-context LLMs.
Table&nbsp;<a href="#S7.T6" title="Table 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows that our context scaling does not significantly influence the short-context generic capability.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Vision-Language</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">In the burgeoning field of multimodal research, the integration of image understanding capabilities into large language models has become increasingly viable.
Drawing inspiration from the open-sourced LLaVA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, we present Yi Vision Language (Yi-VL) models, <span id="S7.SS2.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>, Yi-VL-6B and Yi-VL-34B, based on Yi-6B-Chat and Yi-34B-Chat language models.
The architecture of Yi-VL models, as illustrated in Figure <a href="#S7.F7" title="Figure 7 ‣ 7.2 Vision-Language ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, comprises three primary modules. The Vision Transformer (ViT), used for image encoding, is initialized with CLIP ViT-H/14 model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. A Projection Module, designed to align image features with text feature spcae, consists of a two-layer Multilayer Perceptron (MLP) with layer normalizations. Finally, the large language model, initialized with the Yi-Chat models, demonstrating exceptional proficiency in understanding and generating both English and Chinese. To enhance the performance of Yi-VL models in bilingual multimodal understanding and generation, we leverage a rich dataset of bilingual image-text pairs.</p>
</div>
<figure id="S7.F7" class="ltx_figure"><img src="/html/2403.04652/assets/x7.png" id="S7.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Architecture of Yi-VL models. Symbols are used to denote the training status of various modules at three training stages: a fire icon (<img src="/html/2403.04652/assets/images/fire.png" id="S7.F7.5.g1" class="ltx_graphics ltx_img_square" width="14" height="13" alt="Refer to caption">) indicates the parameters of the module are trainable, while a snowflake icon (<img src="/html/2403.04652/assets/images/snow.png" id="S7.F7.6.g2" class="ltx_graphics ltx_img_square" width="13" height="12" alt="Refer to caption">) signifies that parameters are frozen. The image resolution used in ViT at each stage, either <math id="S7.F7.7.m1.1" class="ltx_Math" alttext="224^{2}" display="inline"><semantics id="S7.F7.7.m1.1b"><msup id="S7.F7.7.m1.1.1" xref="S7.F7.7.m1.1.1.cmml"><mn id="S7.F7.7.m1.1.1.2" xref="S7.F7.7.m1.1.1.2.cmml">224</mn><mn id="S7.F7.7.m1.1.1.3" xref="S7.F7.7.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S7.F7.7.m1.1c"><apply id="S7.F7.7.m1.1.1.cmml" xref="S7.F7.7.m1.1.1"><csymbol cd="ambiguous" id="S7.F7.7.m1.1.1.1.cmml" xref="S7.F7.7.m1.1.1">superscript</csymbol><cn type="integer" id="S7.F7.7.m1.1.1.2.cmml" xref="S7.F7.7.m1.1.1.2">224</cn><cn type="integer" id="S7.F7.7.m1.1.1.3.cmml" xref="S7.F7.7.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.7.m1.1d">224^{2}</annotation></semantics></math> or <math id="S7.F7.8.m2.1" class="ltx_Math" alttext="448^{2}" display="inline"><semantics id="S7.F7.8.m2.1b"><msup id="S7.F7.8.m2.1.1" xref="S7.F7.8.m2.1.1.cmml"><mn id="S7.F7.8.m2.1.1.2" xref="S7.F7.8.m2.1.1.2.cmml">448</mn><mn id="S7.F7.8.m2.1.1.3" xref="S7.F7.8.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S7.F7.8.m2.1c"><apply id="S7.F7.8.m2.1.1.cmml" xref="S7.F7.8.m2.1.1"><csymbol cd="ambiguous" id="S7.F7.8.m2.1.1.1.cmml" xref="S7.F7.8.m2.1.1">superscript</csymbol><cn type="integer" id="S7.F7.8.m2.1.1.2.cmml" xref="S7.F7.8.m2.1.1.2">448</cn><cn type="integer" id="S7.F7.8.m2.1.1.3.cmml" xref="S7.F7.8.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.8.m2.1d">448^{2}</annotation></semantics></math>, is also marked.</figcaption>
</figure>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">Yi-VL models undergo a three-stage training process:</p>
<dl id="S7.I1" class="ltx_description">
<dt id="S7.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S7.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Stage 1:</span></span></dt>
<dd class="ltx_item">
<div id="S7.I1.ix1.p1" class="ltx_para">
<p id="S7.I1.ix1.p1.2" class="ltx_p">we train the parameters of the ViT and the projection module using an image resolution of <math id="S7.I1.ix1.p1.1.m1.1" class="ltx_Math" alttext="224^{2}" display="inline"><semantics id="S7.I1.ix1.p1.1.m1.1a"><msup id="S7.I1.ix1.p1.1.m1.1.1" xref="S7.I1.ix1.p1.1.m1.1.1.cmml"><mn id="S7.I1.ix1.p1.1.m1.1.1.2" xref="S7.I1.ix1.p1.1.m1.1.1.2.cmml">224</mn><mn id="S7.I1.ix1.p1.1.m1.1.1.3" xref="S7.I1.ix1.p1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S7.I1.ix1.p1.1.m1.1b"><apply id="S7.I1.ix1.p1.1.m1.1.1.cmml" xref="S7.I1.ix1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.I1.ix1.p1.1.m1.1.1.1.cmml" xref="S7.I1.ix1.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S7.I1.ix1.p1.1.m1.1.1.2.cmml" xref="S7.I1.ix1.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S7.I1.ix1.p1.1.m1.1.1.3.cmml" xref="S7.I1.ix1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix1.p1.1.m1.1c">224^{2}</annotation></semantics></math>. The training leverages a substantial dataset comprising <math id="S7.I1.ix1.p1.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S7.I1.ix1.p1.2.m2.1a"><mn id="S7.I1.ix1.p1.2.m2.1.1" xref="S7.I1.ix1.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S7.I1.ix1.p1.2.m2.1b"><cn type="integer" id="S7.I1.ix1.p1.2.m2.1.1.cmml" xref="S7.I1.ix1.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix1.p1.2.m2.1c">100</annotation></semantics></math> million image-text pairs from LAION-400M&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. The primary objective is to enhance the ViT’s knowledge acquisition within our specified architecture and to achieve better alignment between the ViT and the LLM.</p>
</div>
</dd>
<dt id="S7.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S7.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Stage 2:</span></span></dt>
<dd class="ltx_item">
<div id="S7.I1.ix2.p1" class="ltx_para">
<p id="S7.I1.ix2.p1.3" class="ltx_p">we scale up the image resolution of ViT to <math id="S7.I1.ix2.p1.1.m1.1" class="ltx_Math" alttext="448^{2}" display="inline"><semantics id="S7.I1.ix2.p1.1.m1.1a"><msup id="S7.I1.ix2.p1.1.m1.1.1" xref="S7.I1.ix2.p1.1.m1.1.1.cmml"><mn id="S7.I1.ix2.p1.1.m1.1.1.2" xref="S7.I1.ix2.p1.1.m1.1.1.2.cmml">448</mn><mn id="S7.I1.ix2.p1.1.m1.1.1.3" xref="S7.I1.ix2.p1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S7.I1.ix2.p1.1.m1.1b"><apply id="S7.I1.ix2.p1.1.m1.1.1.cmml" xref="S7.I1.ix2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.I1.ix2.p1.1.m1.1.1.1.cmml" xref="S7.I1.ix2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S7.I1.ix2.p1.1.m1.1.1.2.cmml" xref="S7.I1.ix2.p1.1.m1.1.1.2">448</cn><cn type="integer" id="S7.I1.ix2.p1.1.m1.1.1.3.cmml" xref="S7.I1.ix2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix2.p1.1.m1.1c">448^{2}</annotation></semantics></math>, aiming to further boost the model’s capability for discerning intricate visual details. The dataset used in this stage includes <math id="S7.I1.ix2.p1.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S7.I1.ix2.p1.2.m2.1a"><mn id="S7.I1.ix2.p1.2.m2.1.1" xref="S7.I1.ix2.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S7.I1.ix2.p1.2.m2.1b"><cn type="integer" id="S7.I1.ix2.p1.2.m2.1.1.cmml" xref="S7.I1.ix2.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix2.p1.2.m2.1c">20</annotation></semantics></math> million image-text pairs derived from LAION-400M. Additionally, we incorporate around <math id="S7.I1.ix2.p1.3.m3.1" class="ltx_Math" alttext="4.8" display="inline"><semantics id="S7.I1.ix2.p1.3.m3.1a"><mn id="S7.I1.ix2.p1.3.m3.1.1" xref="S7.I1.ix2.p1.3.m3.1.1.cmml">4.8</mn><annotation-xml encoding="MathML-Content" id="S7.I1.ix2.p1.3.m3.1b"><cn type="float" id="S7.I1.ix2.p1.3.m3.1.1.cmml" xref="S7.I1.ix2.p1.3.m3.1.1">4.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix2.p1.3.m3.1c">4.8</annotation></semantics></math> million image-text pairsn from diverse sources, <span id="S7.I1.ix2.p1.3.1" class="ltx_text ltx_font_italic">e.g.</span>, CLLaVA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, LLaVAR&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>, Flickr&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, VQAv2&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, RefCOCO&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, Visual7w&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> and so on.</p>
</div>
</dd>
<dt id="S7.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S7.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Stage 3:</span></span></dt>
<dd class="ltx_item">
<div id="S7.I1.ix3.p1" class="ltx_para">
<p id="S7.I1.ix3.p1.2" class="ltx_p">the parameters of the entire model are trained. The primary goal is to enhance the model’s proficiency in multimodal chat interactions, thereby endowing it with the ability to seamlessly integrate and interpret visual and linguistic inputs. To this end, the training dataset encompasses a diverse range of sources, totalling approximately <math id="S7.I1.ix3.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.I1.ix3.p1.1.m1.1a"><mn id="S7.I1.ix3.p1.1.m1.1.1" xref="S7.I1.ix3.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.I1.ix3.p1.1.m1.1b"><cn type="integer" id="S7.I1.ix3.p1.1.m1.1.1.cmml" xref="S7.I1.ix3.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix3.p1.1.m1.1c">1</annotation></semantics></math> million image-text pairs, including GQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, VizWiz VQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, TextCaps&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, OCR-VQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, Visual Genome&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, ShareGPT4V&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and so on. To ensure data balancing, we impose a cap on the maximum data contribution from any single source, restricting it to no more than <math id="S7.I1.ix3.p1.2.m2.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S7.I1.ix3.p1.2.m2.2a"><mrow id="S7.I1.ix3.p1.2.m2.2.3.2" xref="S7.I1.ix3.p1.2.m2.2.3.1.cmml"><mn id="S7.I1.ix3.p1.2.m2.1.1" xref="S7.I1.ix3.p1.2.m2.1.1.cmml">50</mn><mo id="S7.I1.ix3.p1.2.m2.2.3.2.1" xref="S7.I1.ix3.p1.2.m2.2.3.1.cmml">,</mo><mn id="S7.I1.ix3.p1.2.m2.2.2" xref="S7.I1.ix3.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.I1.ix3.p1.2.m2.2b"><list id="S7.I1.ix3.p1.2.m2.2.3.1.cmml" xref="S7.I1.ix3.p1.2.m2.2.3.2"><cn type="integer" id="S7.I1.ix3.p1.2.m2.1.1.cmml" xref="S7.I1.ix3.p1.2.m2.1.1">50</cn><cn type="integer" id="S7.I1.ix3.p1.2.m2.2.2.cmml" xref="S7.I1.ix3.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix3.p1.2.m2.2c">50,000</annotation></semantics></math> pairs.</p>
</div>
</dd>
</dl>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.13" class="ltx_p">In Stage 1 and 2, we set the global batch size, the learning rate, the gradient clip and the number of epoch to <math id="S7.SS2.p3.1.m1.1" class="ltx_Math" alttext="4096" display="inline"><semantics id="S7.SS2.p3.1.m1.1a"><mn id="S7.SS2.p3.1.m1.1.1" xref="S7.SS2.p3.1.m1.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.1.m1.1b"><cn type="integer" id="S7.SS2.p3.1.m1.1.1.cmml" xref="S7.SS2.p3.1.m1.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.1.m1.1c">4096</annotation></semantics></math>, <math id="S7.SS2.p3.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.SS2.p3.2.m2.1a"><mn id="S7.SS2.p3.2.m2.1.1" xref="S7.SS2.p3.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.2.m2.1b"><cn type="integer" id="S7.SS2.p3.2.m2.1.1.cmml" xref="S7.SS2.p3.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.2.m2.1c">1</annotation></semantics></math>e<math id="S7.SS2.p3.3.m3.1" class="ltx_Math" alttext="-4" display="inline"><semantics id="S7.SS2.p3.3.m3.1a"><mrow id="S7.SS2.p3.3.m3.1.1" xref="S7.SS2.p3.3.m3.1.1.cmml"><mo id="S7.SS2.p3.3.m3.1.1a" xref="S7.SS2.p3.3.m3.1.1.cmml">−</mo><mn id="S7.SS2.p3.3.m3.1.1.2" xref="S7.SS2.p3.3.m3.1.1.2.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.3.m3.1b"><apply id="S7.SS2.p3.3.m3.1.1.cmml" xref="S7.SS2.p3.3.m3.1.1"><minus id="S7.SS2.p3.3.m3.1.1.1.cmml" xref="S7.SS2.p3.3.m3.1.1"></minus><cn type="integer" id="S7.SS2.p3.3.m3.1.1.2.cmml" xref="S7.SS2.p3.3.m3.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.3.m3.1c">-4</annotation></semantics></math>, <math id="S7.SS2.p3.4.m4.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S7.SS2.p3.4.m4.1a"><mn id="S7.SS2.p3.4.m4.1.1" xref="S7.SS2.p3.4.m4.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.4.m4.1b"><cn type="float" id="S7.SS2.p3.4.m4.1.1.cmml" xref="S7.SS2.p3.4.m4.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.4.m4.1c">0.5</annotation></semantics></math> and <math id="S7.SS2.p3.5.m5.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.SS2.p3.5.m5.1a"><mn id="S7.SS2.p3.5.m5.1.1" xref="S7.SS2.p3.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.5.m5.1b"><cn type="integer" id="S7.SS2.p3.5.m5.1.1.cmml" xref="S7.SS2.p3.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.5.m5.1c">1</annotation></semantics></math>, respectively. In Stage 3, these parameters are adjusted to <math id="S7.SS2.p3.6.m6.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S7.SS2.p3.6.m6.1a"><mn id="S7.SS2.p3.6.m6.1.1" xref="S7.SS2.p3.6.m6.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.6.m6.1b"><cn type="integer" id="S7.SS2.p3.6.m6.1.1.cmml" xref="S7.SS2.p3.6.m6.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.6.m6.1c">256</annotation></semantics></math>, <math id="S7.SS2.p3.7.m7.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S7.SS2.p3.7.m7.1a"><mn id="S7.SS2.p3.7.m7.1.1" xref="S7.SS2.p3.7.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.7.m7.1b"><cn type="integer" id="S7.SS2.p3.7.m7.1.1.cmml" xref="S7.SS2.p3.7.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.7.m7.1c">2</annotation></semantics></math>e<math id="S7.SS2.p3.8.m8.1" class="ltx_Math" alttext="-5" display="inline"><semantics id="S7.SS2.p3.8.m8.1a"><mrow id="S7.SS2.p3.8.m8.1.1" xref="S7.SS2.p3.8.m8.1.1.cmml"><mo id="S7.SS2.p3.8.m8.1.1a" xref="S7.SS2.p3.8.m8.1.1.cmml">−</mo><mn id="S7.SS2.p3.8.m8.1.1.2" xref="S7.SS2.p3.8.m8.1.1.2.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.8.m8.1b"><apply id="S7.SS2.p3.8.m8.1.1.cmml" xref="S7.SS2.p3.8.m8.1.1"><minus id="S7.SS2.p3.8.m8.1.1.1.cmml" xref="S7.SS2.p3.8.m8.1.1"></minus><cn type="integer" id="S7.SS2.p3.8.m8.1.1.2.cmml" xref="S7.SS2.p3.8.m8.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.8.m8.1c">-5</annotation></semantics></math>, <math id="S7.SS2.p3.9.m9.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="S7.SS2.p3.9.m9.1a"><mn id="S7.SS2.p3.9.m9.1.1" xref="S7.SS2.p3.9.m9.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.9.m9.1b"><cn type="float" id="S7.SS2.p3.9.m9.1.1.cmml" xref="S7.SS2.p3.9.m9.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.9.m9.1c">1.0</annotation></semantics></math> and <math id="S7.SS2.p3.10.m10.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S7.SS2.p3.10.m10.1a"><mn id="S7.SS2.p3.10.m10.1.1" xref="S7.SS2.p3.10.m10.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.10.m10.1b"><cn type="integer" id="S7.SS2.p3.10.m10.1.1.cmml" xref="S7.SS2.p3.10.m10.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.10.m10.1c">2</annotation></semantics></math>. The training consumes <math id="S7.SS2.p3.11.m11.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S7.SS2.p3.11.m11.1a"><mn id="S7.SS2.p3.11.m11.1.1" xref="S7.SS2.p3.11.m11.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.11.m11.1b"><cn type="integer" id="S7.SS2.p3.11.m11.1.1.cmml" xref="S7.SS2.p3.11.m11.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.11.m11.1c">128</annotation></semantics></math> NVIDIA A100 GPUs. The total training time amounted to approximately <math id="S7.SS2.p3.12.m12.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S7.SS2.p3.12.m12.1a"><mn id="S7.SS2.p3.12.m12.1.1" xref="S7.SS2.p3.12.m12.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.12.m12.1b"><cn type="integer" id="S7.SS2.p3.12.m12.1.1.cmml" xref="S7.SS2.p3.12.m12.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.12.m12.1c">3</annotation></semantics></math> days for Yi-VL-6B and <math id="S7.SS2.p3.13.m13.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S7.SS2.p3.13.m13.1a"><mn id="S7.SS2.p3.13.m13.1.1" xref="S7.SS2.p3.13.m13.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.13.m13.1b"><cn type="integer" id="S7.SS2.p3.13.m13.1.1.cmml" xref="S7.SS2.p3.13.m13.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.13.m13.1c">10</annotation></semantics></math> days for Yi-VL-34B.</p>
</div>
<figure id="S7.T7" class="ltx_table">
<table id="S7.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S7.T7.1.1" class="ltx_tr">
<td id="S7.T7.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S7.T7.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.2.1" class="ltx_text ltx_font_bold">Overall</span></td>
<td id="S7.T7.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.3.1" class="ltx_text ltx_font_bold">Art</span></td>
<td id="S7.T7.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.4.1" class="ltx_text ltx_font_bold">Business</span></td>
<td id="S7.T7.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.5.1" class="ltx_text ltx_font_bold">Science</span></td>
<td id="S7.T7.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.6.1" class="ltx_text ltx_font_bold">Health</span></td>
<td id="S7.T7.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.7.1" class="ltx_text ltx_font_bold">Society</span></td>
<td id="S7.T7.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.8.1" class="ltx_text ltx_font_bold">Engineering</span></td>
</tr>
<tr id="S7.T7.1.2" class="ltx_tr">
<td id="S7.T7.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">GPT-4V</td>
<td id="S7.T7.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">55.7</td>
<td id="S7.T7.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">65.3</td>
<td id="S7.T7.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">64.3</td>
<td id="S7.T7.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">48.4</td>
<td id="S7.T7.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">63.5</td>
<td id="S7.T7.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">76.3</td>
<td id="S7.T7.1.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">41.7</td>
</tr>
<tr id="S7.T7.1.3" class="ltx_tr">
<td id="S7.T7.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Yi-VL-34B</td>
<td id="S7.T7.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">41.6</td>
<td id="S7.T7.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">56.1</td>
<td id="S7.T7.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.3</td>
<td id="S7.T7.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.9</td>
<td id="S7.T7.1.3.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">45.9</td>
<td id="S7.T7.1.3.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">66.5</td>
<td id="S7.T7.1.3.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.0</td>
</tr>
<tr id="S7.T7.1.4" class="ltx_tr">
<td id="S7.T7.1.4.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Qwen-VL-PLUS</td>
<td id="S7.T7.1.4.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">40.8</td>
<td id="S7.T7.1.4.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">59.9</td>
<td id="S7.T7.1.4.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.5</td>
<td id="S7.T7.1.4.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.8</td>
<td id="S7.T7.1.4.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">43.7</td>
<td id="S7.T7.1.4.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">65.5</td>
<td id="S7.T7.1.4.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.9</td>
</tr>
<tr id="S7.T7.1.5" class="ltx_tr">
<td id="S7.T7.1.5.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Marco-VL</td>
<td id="S7.T7.1.5.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">40.4</td>
<td id="S7.T7.1.5.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">56.5</td>
<td id="S7.T7.1.5.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.0</td>
<td id="S7.T7.1.5.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.0</td>
<td id="S7.T7.1.5.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.9</td>
<td id="S7.T7.1.5.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">66.5</td>
<td id="S7.T7.1.5.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.8</td>
</tr>
<tr id="S7.T7.1.6" class="ltx_tr">
<td id="S7.T7.1.6.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Yi-VL-6B</td>
<td id="S7.T7.1.6.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">37.8</td>
<td id="S7.T7.1.6.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">53.4</td>
<td id="S7.T7.1.6.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.3</td>
<td id="S7.T7.1.6.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.0</td>
<td id="S7.T7.1.6.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">39.3</td>
<td id="S7.T7.1.6.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">58.5</td>
<td id="S7.T7.1.6.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.1</td>
</tr>
<tr id="S7.T7.1.7" class="ltx_tr">
<td id="S7.T7.1.7.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">InfMIM-Zephyr-7B</td>
<td id="S7.T7.1.7.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">35.5</td>
<td id="S7.T7.1.7.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.0</td>
<td id="S7.T7.1.7.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.6</td>
<td id="S7.T7.1.7.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.2</td>
<td id="S7.T7.1.7.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">37.5</td>
<td id="S7.T7.1.7.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">54.6</td>
<td id="S7.T7.1.7.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.1</td>
</tr>
<tr id="S7.T7.1.8" class="ltx_tr">
<td id="S7.T7.1.8.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">SVIT</td>
<td id="S7.T7.1.8.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.1</td>
<td id="S7.T7.1.8.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.9</td>
<td id="S7.T7.1.8.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.0</td>
<td id="S7.T7.1.8.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">26.8</td>
<td id="S7.T7.1.8.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">35.5</td>
<td id="S7.T7.1.8.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.9</td>
<td id="S7.T7.1.8.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.7</td>
</tr>
<tr id="S7.T7.1.9" class="ltx_tr">
<td id="S7.T7.1.9.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Emu2-Chat</td>
<td id="S7.T7.1.9.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.1</td>
<td id="S7.T7.1.9.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.6</td>
<td id="S7.T7.1.9.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.7</td>
<td id="S7.T7.1.9.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.0</td>
<td id="S7.T7.1.9.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.4</td>
<td id="S7.T7.1.9.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.3</td>
<td id="S7.T7.1.9.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.3</td>
</tr>
<tr id="S7.T7.1.10" class="ltx_tr">
<td id="S7.T7.1.10.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">BLIP-2 FLAN-T5-XXL</td>
<td id="S7.T7.1.10.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.0</td>
<td id="S7.T7.1.10.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">49.2</td>
<td id="S7.T7.1.10.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.6</td>
<td id="S7.T7.1.10.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.3</td>
<td id="S7.T7.1.10.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.7</td>
<td id="S7.T7.1.10.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">51.5</td>
<td id="S7.T7.1.10.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.4</td>
</tr>
<tr id="S7.T7.1.11" class="ltx_tr">
<td id="S7.T7.1.11.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">InstructBLIP-T5-XXL</td>
<td id="S7.T7.1.11.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.8</td>
<td id="S7.T7.1.11.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.5</td>
<td id="S7.T7.1.11.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.6</td>
<td id="S7.T7.1.11.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.6</td>
<td id="S7.T7.1.11.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.6</td>
<td id="S7.T7.1.11.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">49.8</td>
<td id="S7.T7.1.11.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.4</td>
</tr>
<tr id="S7.T7.1.12" class="ltx_tr">
<td id="S7.T7.1.12.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">LLaVA-1.5-13B</td>
<td id="S7.T7.1.12.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.6</td>
<td id="S7.T7.1.12.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">49.8</td>
<td id="S7.T7.1.12.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.2</td>
<td id="S7.T7.1.12.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.9</td>
<td id="S7.T7.1.12.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.9</td>
<td id="S7.T7.1.12.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">54.7</td>
<td id="S7.T7.1.12.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.3</td>
</tr>
<tr id="S7.T7.1.13" class="ltx_tr">
<td id="S7.T7.1.13.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Qwen-VL-7B-Chat</td>
<td id="S7.T7.1.13.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.9</td>
<td id="S7.T7.1.13.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">47.7</td>
<td id="S7.T7.1.13.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.8</td>
<td id="S7.T7.1.13.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.6</td>
<td id="S7.T7.1.13.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.6</td>
<td id="S7.T7.1.13.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">45.3</td>
<td id="S7.T7.1.13.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.2</td>
</tr>
<tr id="S7.T7.1.14" class="ltx_tr">
<td id="S7.T7.1.14.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">SPHINX*</td>
<td id="S7.T7.1.14.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.9</td>
<td id="S7.T7.1.14.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.9</td>
<td id="S7.T7.1.14.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.2</td>
<td id="S7.T7.1.14.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.3</td>
<td id="S7.T7.1.14.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.1</td>
<td id="S7.T7.1.14.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">51.2</td>
<td id="S7.T7.1.14.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.8</td>
</tr>
<tr id="S7.T7.1.15" class="ltx_tr">
<td id="S7.T7.1.15.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">mPLUG-OWL2</td>
<td id="S7.T7.1.15.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.1</td>
<td id="S7.T7.1.15.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.5</td>
<td id="S7.T7.1.15.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.6</td>
<td id="S7.T7.1.15.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">24.9</td>
<td id="S7.T7.1.15.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.8</td>
<td id="S7.T7.1.15.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.7</td>
<td id="S7.T7.1.15.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.6</td>
</tr>
<tr id="S7.T7.1.16" class="ltx_tr">
<td id="S7.T7.1.16.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">BLIP-2 FLAN-T5-XL</td>
<td id="S7.T7.1.16.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.0</td>
<td id="S7.T7.1.16.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">43.0</td>
<td id="S7.T7.1.16.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.6</td>
<td id="S7.T7.1.16.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.1</td>
<td id="S7.T7.1.16.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.8</td>
<td id="S7.T7.1.16.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.0</td>
<td id="S7.T7.1.16.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.8</td>
</tr>
<tr id="S7.T7.1.17" class="ltx_tr">
<td id="S7.T7.1.17.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">InstructBLIP-T5-XL</td>
<td id="S7.T7.1.17.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.6</td>
<td id="S7.T7.1.17.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">43.3</td>
<td id="S7.T7.1.17.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.2</td>
<td id="S7.T7.1.17.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.2</td>
<td id="S7.T7.1.17.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.3</td>
<td id="S7.T7.1.17.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">45.8</td>
<td id="S7.T7.1.17.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.6</td>
</tr>
<tr id="S7.T7.1.18" class="ltx_tr">
<td id="S7.T7.1.18.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">CogVLM</td>
<td id="S7.T7.1.18.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">30.1</td>
<td id="S7.T7.1.18.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">38.0</td>
<td id="S7.T7.1.18.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">25.6</td>
<td id="S7.T7.1.18.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">25.1</td>
<td id="S7.T7.1.18.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">31.2</td>
<td id="S7.T7.1.18.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">41.5</td>
<td id="S7.T7.1.18.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">28.9</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>MMMU test set
performance by the time of Yi-VL’s release.</figcaption>
</figure>
<div id="S7.SS2.p4" class="ltx_para">
<p id="S7.SS2.p4.1" class="ltx_p">Table&nbsp;<a href="#S7.T7" title="Table 7 ‣ 7.2 Vision-Language ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the MMMU test set leaderboard by Yi-VL’s release. We note that this area is currently actively under research, aligning with the community’s advances, we will continuously improve the update Yi-VL’s performance.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Depth Upscaling</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Recent studies on scaling laws&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> have underscored the predictable improvement in model performance with increases in computational budget, model size, and data size. Yet, identifying the most effective distribution of resources between model and data sizes upon expanding the computational budget remains a formidable challenge in the field of scaling laws. Additionally, research conducted by&nbsp;<cite class="ltx_cite ltx_citemacro_citet">DeepSeek-AI et&nbsp;al. [<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> has highlighted that the allocation of an increased computational budget towards model scaling should be proportional to the quality of the data available. In light of these insights, we propose a novel approach aimed at dynamically adjusting the resource allocation between data and model sizes through a series of staged training processes. This strategy iteratively fine-tunes the balance between data characteristics and model size according to scaling laws, enhancing both model training efficiency and performance.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p"><span id="S7.SS3.p2.1.1" class="ltx_text ltx_font_bold">Method</span> Following the methodology outlined by&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Kim et&nbsp;al. [<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, our goal is to upscale our Yi-6B base model, which has 32 layers, to a 9B model named the Yi-9B base model, featuring 48 layers, by duplicating the original 16 middle layers 12-28. Depth up-scaling involves expanding the base model’s depth and subsequently continuing the pretraining phase for the enhanced model.</p>
</div>
<figure id="S7.F8" class="ltx_figure"><img src="/html/2403.04652/assets/x8.png" id="S7.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Input/output cosine similarity score of each token per layer for text "Write a quiz about bits". The cosine similarity scores of the 16 newly added layers(layers 28-44), as depicted in the lower figure, are observed to be nearly 1.</figcaption>
</figure>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p">Our investigations reveal that the decision on which layers to replicate could be informed by evaluating the cosine similarity scores between the inputs and outputs of each layer. Such an approach allows for targeted model scaling without necessitating additional pretraining, leading only to minimal performance impacts. This minimal impact on performance is attributed to the high cosine similarity, approaching one, between the inputs and outputs of the duplicated layers, as evidenced in Figure <a href="#S7.F8" title="Figure 8 ‣ 7.3 Depth Upscaling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. This observation suggests that the replication of these layers does not significantly alter the output logits produced by the original model. This method ensures the efficient scaling of the model by optimizing its architecture based on the internal processing dynamics of its layers.</p>
</div>
<figure id="S7.T8" class="ltx_table">
<div id="S7.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:367.8pt;height:58.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.0pt,7.2pt) scale(0.8,0.8) ;">
<table id="S7.T8.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T8.1.1.1" class="ltx_tr">
<td id="S7.T8.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S7.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">Arc-C</span></td>
<td id="S7.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">HellaSwag</span></td>
<td id="S7.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">MMLU</span></td>
<td id="S7.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.5.1" class="ltx_text ltx_font_bold">Winogrande</span></td>
<td id="S7.T8.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.6.1" class="ltx_text ltx_font_bold">GSM8K</span></td>
<td id="S7.T8.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.7.1" class="ltx_text ltx_font_bold">MATH</span></td>
<td id="S7.T8.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.8.1" class="ltx_text ltx_font_bold">HumanEval</span></td>
<td id="S7.T8.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.9.1" class="ltx_text ltx_font_bold">MBPP</span></td>
</tr>
<tr id="S7.T8.1.1.2" class="ltx_tr">
<td id="S7.T8.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.2.1.1" class="ltx_text ltx_font_bold">Yi-6B</span></td>
<td id="S7.T8.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">50.3</td>
<td id="S7.T8.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">74.4</td>
<td id="S7.T8.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">63.2</td>
<td id="S7.T8.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">71.3</td>
<td id="S7.T8.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">32.5</td>
<td id="S7.T8.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4.6</td>
<td id="S7.T8.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.9</td>
<td id="S7.T8.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">26.3</td>
</tr>
<tr id="S7.T8.1.1.3" class="ltx_tr">
<td id="S7.T8.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.3.1.1" class="ltx_text ltx_font_bold">Yi-9B Init</span></td>
<td id="S7.T8.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">52.1</td>
<td id="S7.T8.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">73.3</td>
<td id="S7.T8.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">63.0</td>
<td id="S7.T8.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">69.4</td>
<td id="S7.T8.1.1.3.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.3</td>
<td id="S7.T8.1.1.3.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">4.1</td>
<td id="S7.T8.1.1.3.8" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">12.8</td>
<td id="S7.T8.1.1.3.9" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.8</td>
</tr>
<tr id="S7.T8.1.1.4" class="ltx_tr">
<td id="S7.T8.1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.1.1" class="ltx_text ltx_font_bold">Yi-9B</span></td>
<td id="S7.T8.1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.2.1" class="ltx_text ltx_font_bold">55.6</span></td>
<td id="S7.T8.1.1.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.3.1" class="ltx_text ltx_font_bold">76.4</span></td>
<td id="S7.T8.1.1.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.4.1" class="ltx_text ltx_font_bold">68.4</span></td>
<td id="S7.T8.1.1.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.5.1" class="ltx_text ltx_font_bold">73.0</span></td>
<td id="S7.T8.1.1.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.6.1" class="ltx_text ltx_font_bold">52.3</span></td>
<td id="S7.T8.1.1.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.7.1" class="ltx_text ltx_font_bold">15.9</span></td>
<td id="S7.T8.1.1.4.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.8.1" class="ltx_text ltx_font_bold">39.0</span></td>
<td id="S7.T8.1.1.4.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.9.1" class="ltx_text ltx_font_bold">54.4</span></td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Performance between Yi-6B and Yi-9B: Arc Challenge (25-shot), HellaSwag (10-shot) MMLU (5-shot), Winogrande (5-shot), GSM8K (5-shot), MATH (4-shot), HumanEval pass@1, MBPP pass@1(3-shot). Yi-9B Init is just depthwise upscaling from Yi-6B by duplicating layers 12-28 without further training.</figcaption>
</figure>
<div id="S7.SS3.p4" class="ltx_para">
<p id="S7.SS3.p4.1" class="ltx_p"><span id="S7.SS3.p4.1.1" class="ltx_text ltx_font_bold">Continual Training</span> The dataset is composed of approximately 800 billion tokens across two stages, with around 70% having been recently collected and carefully selected. We have enhanced the code coverage in the final stage to improve code performance.</p>
</div>
<div id="S7.SS3.p5" class="ltx_para">
<p id="S7.SS3.p5.1" class="ltx_p">To optimize the training process, we maintain a constant learning rate of 3e-5, and adopt a strategic approach to gradually increase the batch size from 4M tokens whenever the model’s loss plateaued. This incremental adjustment of the batch size, alongside maintaining all other parameters in alignment with the established Yi-6B base model configuration, was instrumental in navigating the challenges of training at scale.</p>
</div>
<div id="S7.SS3.p6" class="ltx_para">
<p id="S7.SS3.p6.1" class="ltx_p">The effectiveness of these strategies is demonstrated in Table <a href="#S7.T8" title="Table 8 ‣ 7.3 Depth Upscaling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, which details the Yi-9B base model’s performance across a variety of benchmarks, including common sense, reasoning, knowledge, coding, and mathematics. It underscores the competitive advantages of Yi-9B base model in specific domains, illustrating the efficacy of our methodology in enhancing model performance by optimally adjusting the interplay between data characteristics and model size.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Final Discussions</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this report, we discuss the full-stack development of the Yi language model family.
Yi-34B achieves GPT-3.5 matching performance and is deployable (thank to the 4/8-bit quantization) on consumer-grade devices, making it an ideal model for local deployment.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">The key takeaways from the Yi pretraining procedure are about data quantity and quality:
(1). training the model on a larger amount of data than the Chinchilla optimal delivers clear and consistent performance gain, which we highly recommend for all pretraining teams.
Our model is trained on 3.1T tokens, yet we belive with larger amount of data, we can continue improve the model performance (i.e., the model have not saturated at 3.1T);
(2). when it comes to the pretraining data quality, we believe the most critical two factors are the source of the data (e.g., whether the text is produced for professional usage or for casual social media posting) and the details of the data cleaning (e.g., the strength of filtering and deduplication). Since data cleaning is a very complicated pipeline and it is extremely difficult to conduct extensive grid-search styled optimizations, our current solution may still have room for improvements.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">The key takeaways from the Yi finetuning procedure is to heavily iterate on a small amount of data (<math id="S8.p3.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S8.p3.1.m1.1a"><mo id="S8.p3.1.m1.1.1" xref="S8.p3.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S8.p3.1.m1.1b"><leq id="S8.p3.1.m1.1.1.cmml" xref="S8.p3.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S8.p3.1.m1.1c">\leq</annotation></semantics></math> 10K), case by case, over multiple iterations, directly by the machine learning engineer, and improved from real user feedback.
This approach clearly deviates from the instruction-scaling approach, initially introduced by the FLAN series&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> then followed by the UltraChat series&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">As is demonstrated by our current results, the reasoning capability, which we view as the core capability for real-world deployment of language models, is strongly correlated with model scale when the amount of pretraining data is fixed.
We believe that given our current
results, continuing to scale up model parameters using thoroughly optimized data
will lead to even stronger frontier models in our upcoming next versions.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Author List and Contributions</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Our team members contribute to the development of Yi from the following perspectives:</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="A1.p2" class="ltx_para">
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">Frontier Research</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">Machine Learning Infrastructure</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">Pretraining</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">Finetuning and AI Alignment</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p">Multimodal</p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p">Safety and Responsible AI</p>
</div>
</li>
<li id="A1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i7.p1" class="ltx_para">
<p id="A1.I1.i7.p1.1" class="ltx_p">Deployment</p>
</div>
</li>
</ul>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">We list our team members in alphabetical order. All authors contributed equally to this work.</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="A1.p4" class="ltx_para">
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p">Alex Young</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p">Bei Chen</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.1" class="ltx_p">Chao Li</p>
</div>
</li>
<li id="A1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i4.p1" class="ltx_para">
<p id="A1.I2.i4.p1.1" class="ltx_p">Chengen Huang</p>
</div>
</li>
<li id="A1.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i5.p1" class="ltx_para">
<p id="A1.I2.i5.p1.1" class="ltx_p">Ge Zhang</p>
</div>
</li>
<li id="A1.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i6.p1" class="ltx_para">
<p id="A1.I2.i6.p1.1" class="ltx_p">Guanwei Zhang</p>
</div>
</li>
<li id="A1.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i7.p1" class="ltx_para">
<p id="A1.I2.i7.p1.1" class="ltx_p">Heng Li</p>
</div>
</li>
<li id="A1.I2.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i8.p1" class="ltx_para">
<p id="A1.I2.i8.p1.1" class="ltx_p">Jiangcheng Zhu</p>
</div>
</li>
<li id="A1.I2.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i9.p1" class="ltx_para">
<p id="A1.I2.i9.p1.1" class="ltx_p">Jianqun Chen</p>
</div>
</li>
<li id="A1.I2.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i10.p1" class="ltx_para">
<p id="A1.I2.i10.p1.1" class="ltx_p">Jing Chang</p>
</div>
</li>
<li id="A1.I2.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i11.p1" class="ltx_para">
<p id="A1.I2.i11.p1.1" class="ltx_p">Kaidong Yu</p>
</div>
</li>
<li id="A1.I2.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i12.p1" class="ltx_para">
<p id="A1.I2.i12.p1.1" class="ltx_p">Peng Liu</p>
</div>
</li>
<li id="A1.I2.i13" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i13.p1" class="ltx_para">
<p id="A1.I2.i13.p1.1" class="ltx_p">Qiang Liu</p>
</div>
</li>
<li id="A1.I2.i14" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i14.p1" class="ltx_para">
<p id="A1.I2.i14.p1.1" class="ltx_p">Shawn Yue</p>
</div>
</li>
<li id="A1.I2.i15" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i15.p1" class="ltx_para">
<p id="A1.I2.i15.p1.1" class="ltx_p">Senbin Yang</p>
</div>
</li>
<li id="A1.I2.i16" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i16.p1" class="ltx_para">
<p id="A1.I2.i16.p1.1" class="ltx_p">Shiming Yang</p>
</div>
</li>
<li id="A1.I2.i17" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i17.p1" class="ltx_para">
<p id="A1.I2.i17.p1.1" class="ltx_p">Tao Yu</p>
</div>
</li>
<li id="A1.I2.i18" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i18.p1" class="ltx_para">
<p id="A1.I2.i18.p1.1" class="ltx_p">Wen Xie</p>
</div>
</li>
<li id="A1.I2.i19" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i19.p1" class="ltx_para">
<p id="A1.I2.i19.p1.1" class="ltx_p">Wenhao Huang</p>
</div>
</li>
<li id="A1.I2.i20" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i20.p1" class="ltx_para">
<p id="A1.I2.i20.p1.1" class="ltx_p">Xiaohui Hu</p>
</div>
</li>
<li id="A1.I2.i21" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i21.p1" class="ltx_para">
<p id="A1.I2.i21.p1.1" class="ltx_p">Xiaoyi Ren</p>
</div>
</li>
<li id="A1.I2.i22" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i22.p1" class="ltx_para">
<p id="A1.I2.i22.p1.1" class="ltx_p">Xinyao Niu</p>
</div>
</li>
<li id="A1.I2.i23" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i23.p1" class="ltx_para">
<p id="A1.I2.i23.p1.1" class="ltx_p">Pengcheng Nie</p>
</div>
</li>
<li id="A1.I2.i24" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i24.p1" class="ltx_para">
<p id="A1.I2.i24.p1.1" class="ltx_p">Yuchi Xu</p>
</div>
</li>
<li id="A1.I2.i25" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i25.p1" class="ltx_para">
<p id="A1.I2.i25.p1.1" class="ltx_p">Yudong Liu</p>
</div>
</li>
<li id="A1.I2.i26" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i26.p1" class="ltx_para">
<p id="A1.I2.i26.p1.1" class="ltx_p">Yue Wang</p>
</div>
</li>
<li id="A1.I2.i27" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i27.p1" class="ltx_para">
<p id="A1.I2.i27.p1.1" class="ltx_p">Yuxuan Cai</p>
</div>
</li>
<li id="A1.I2.i28" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i28.p1" class="ltx_para">
<p id="A1.I2.i28.p1.1" class="ltx_p">Zhenyu Gu</p>
</div>
</li>
<li id="A1.I2.i29" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i29.p1" class="ltx_para">
<p id="A1.I2.i29.p1.1" class="ltx_p">Zhiyuan Liu</p>
</div>
</li>
<li id="A1.I2.i30" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i30.p1" class="ltx_para">
<p id="A1.I2.i30.p1.1" class="ltx_p">Zonghong Dai</p>
</div>
</li>
</ul>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainslie et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Joshua Ainslie, James Lee-Thorp, Michiel de&nbsp;Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.

</span>
<span class="ltx_bibblock">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13245</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et&nbsp;al.

</span>
<span class="ltx_bibblock">Program Synthesis With lLarge Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07732</em>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An&nbsp;Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock">09 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/pdf/2309.16609.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/2309.16609.pdf</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">PIQA: Reasoning about Physical Commonsense in Natural Language.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1911.11641, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:208290939" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:208290939</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.

</span>
<span class="ltx_bibblock">Sharegpt4v: Improving large multi-modal models with better captions.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.12793</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Pondé de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew&nbsp;N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating Large Language Models Trained on Code.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2107.03374, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2107.03374" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2107.03374</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Eunsol Choi, He&nbsp;He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">QuAC : Question Answering in Context, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, Shayne Longpre, Barret Zoph, Yi&nbsp;Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et&nbsp;al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.11416</em>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training Verifiers to Solve Math Word Problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Computer [2023]</span>
<span class="ltx_bibblock">
Together Computer.

</span>
<span class="ltx_bibblock">Redpajama: an open dataset for training large language models, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/togethercomputer/RedPajama-Data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/togethercomputer/RedPajama-Data</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao [2023]</span>
<span class="ltx_bibblock">
Tri Dao.

</span>
<span class="ltx_bibblock">FlashAttention-2: Faster attention with better parallelism and work partitioning.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Tri Dao, Daniel&nbsp;Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">FlashAttention: Fast and memory-efficient exact attention with IO-awareness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de&nbsp;Jong et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Michiel de&nbsp;Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and William Cohen.

</span>
<span class="ltx_bibblock">FiDO: Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.08153</em>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.&nbsp;K. Li, Wenfeng Liang, Fangyun Lin, A.&nbsp;X. Liu, Bo&nbsp;Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y.&nbsp;Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.&nbsp;X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B.&nbsp;Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng
Zhou, Qihao Zhu, and Yuheng Zou.

</span>
<span class="ltx_bibblock">Deepseek llm: Scaling open-source language models with longtermism.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.

</span>
<span class="ltx_bibblock">Enhancing chat language models by scaling high-quality instructional conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14233</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">How abilities in large language models are affected by supervised fine-tuning data composition, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.

</span>
<span class="ltx_bibblock">Data engineering for scaling language models to 128k context.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.10171</em>, 2024.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini&nbsp;Team et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Gemini Gemini&nbsp;Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glaese et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et&nbsp;al.

</span>
<span class="ltx_bibblock">Improving alignment of dialogue agents via targeted human judgements.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.14375</em>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. [2017]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 6904–6913, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale&nbsp;J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey&nbsp;P Bigham.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 3608–3617, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2009.03300, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2009.03300" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2009.03300</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring Mathematical Problem Solving With the MATH Dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.03874</em>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Henighan et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom&nbsp;B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel&nbsp;M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish.

</span>
<span class="ltx_bibblock">Scaling laws for autoregressive generative modeling.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et&nbsp;al.

</span>
<span class="ltx_bibblock">C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.08322</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning [2019]</span>
<span class="ltx_bibblock">
Drew&nbsp;A Hudson and Christopher&nbsp;D Manning.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 6700–6709, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilharco et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Openclip, July 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5143773" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5143773</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian&nbsp;R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et&nbsp;al.

</span>
<span class="ltx_bibblock">Neftune: Noisy embeddings improve instruction finetuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.05914</em>, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce&nbsp;Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang.

</span>
<span class="ltx_bibblock">Beavertails: Towards improved safety alignment of llm via a human-preference dataset, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemzadeh et&nbsp;al. [2014]</span>
<span class="ltx_bibblock">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.

</span>
<span class="ltx_bibblock">Referitgame: Referring to objects in photographs of natural scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em>, pages 787–798, 2014.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim.

</span>
<span class="ltx_bibblock">Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et&nbsp;al. [2017]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David&nbsp;A Shamma, et&nbsp;al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123:32–73, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson [2018]</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.06226</em>, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient Memory Management for Large Language Model Serving with PagedAttention.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.06180</em>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin.

</span>
<span class="ltx_bibblock">CMMLU: Measuring Massive Multitask Language Understanding in Chinese.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.09212</em>, 2023a.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You.

</span>
<span class="ltx_bibblock">Sequence parallelism: Long sequence training from system perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.13120</em>, 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/alpaca_eval" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/alpaca_eval</a>, 2023b.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LinkSoul-AI [2023]</span>
<span class="ltx_bibblock">
LinkSoul-AI.

</span>
<span class="ltx_bibblock">Chinese llava.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/LinkSoul-AI/Chinese-LLaVA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/LinkSoul-AI/Chinese-LLaVA</a>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong&nbsp;Jae Lee.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03744</em>, 2023a.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong&nbsp;Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08485</em>, 2023b.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2023c]</span>
<span class="ltx_bibblock">
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.

</span>
<span class="ltx_bibblock">What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15685</em>, 2023c.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">#instag: Instruction tagging for analyzing supervised fine-tuning of large language models, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Anand Mishra, Shashank Shekhar, Ajeet&nbsp;Kumar Singh, and Anirban Chakraborty.

</span>
<span class="ltx_bibblock">Ocr-vqa: Visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">2019 international conference on document analysis and recognition (ICDAR)</em>, pages 947–952. IEEE, 2019.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Thuat Nguyen, Chien Van&nbsp;Nguyen, Viet&nbsp;Dac Lai, Hieu Man, Nghia&nbsp;Trung Ngo, Franck Dernoncourt, Ryan&nbsp;A Rossi, and Thien&nbsp;Huu Nguyen.

</span>
<span class="ltx_bibblock">CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.09400</em>, 2023.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2022]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">ChatML, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training Language Models to Follow Instructions with Human Feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:27730–27744, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paster [2023]</span>
<span class="ltx_bibblock">
Keiran Paster.

</span>
<span class="ltx_bibblock">Testing language models on a held-out high school national finals exam.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam</a>, 2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.

</span>
<span class="ltx_bibblock">The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only, 2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pope et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.

</span>
<span class="ltx_bibblock">Efficiently Scaling Transformer Inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, 5, 2023.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Jack&nbsp;W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et&nbsp;al.

</span>
<span class="ltx_bibblock">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.11446</em>, 2021.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher&nbsp;D Manning, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18290</em>, 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</em>, pages 1–16. IEEE, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et&nbsp;al. [2016]</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ Questions for Machine Comprehension of Text, 2016.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">WinoGrande: An Adversarial Winograd Schema Challenge at Scale, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.

</span>
<span class="ltx_bibblock">SocialIQA: Commonsense Reasoning about Social Interactions, 2019.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sardana and Frankle [2023]</span>
<span class="ltx_bibblock">
Nikhil Sardana and Jonathan Frankle.

</span>
<span class="ltx_bibblock">Beyond chinchilla-optimal: Accounting for inference in language model scaling laws.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.00448</em>, 2023.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.

</span>
<span class="ltx_bibblock">Are emergent abilities of large language models a mirage?

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.02114</em>, 2021.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer [2019]</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">Fast Transformer Decoding: One Write-Head is All You Need.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.02150</em>, 2019.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer [2020]</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">GLU Variants Improve Transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.05202</em>, 2020.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shibata et&nbsp;al. [1999]</span>
<span class="ltx_bibblock">
Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa.

</span>
<span class="ltx_bibblock">Byte Pair Encoding: A Text Compression Scheme That Accelerates Pattern Matching.

</span>
<span class="ltx_bibblock">Technical report, Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidorov et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.

</span>
<span class="ltx_bibblock">Textcaps: a dataset for image captioning with reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16</em>, pages 742–758. Springer, 2020.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal&nbsp;Md Shoeb, Abubakar Abid, Adam Fisch, Adam&nbsp;R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander&nbsp;W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman&nbsp;S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B.&nbsp;Ryan Roberts, Bao&nbsp;Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill&nbsp;Yuchen Lin, Blake Howald, Bryan
Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César&nbsp;Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher&nbsp;D. Manning, Christopher Potts, Cindy Ramirez, Clara&nbsp;E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel&nbsp;Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, and Daphne&nbsp;Ippolito et&nbsp;al. (351 additional authors&nbsp;not shown).

</span>
<span class="ltx_bibblock">Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 2023.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Ahmed Murtadha, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced Transformer with Rotary Position Embedding.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.09864</em>, 2021.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V Le, Ed&nbsp;H Chi, Denny Zhou, et&nbsp;al.

</span>
<span class="ltx_bibblock">Challenging Big-Bench Tasks and Whether Chain-of-Thought can Solve Them.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.09261</em>, 2022.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.

</span>
<span class="ltx_bibblock">CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge, 2019.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023b.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention Is All You Need.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 06 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/pdf/1706.03762.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1706.03762.pdf</a>.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. [2019a]</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00359</em>, 11 2019a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/pdf/1911.00359.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1911.00359.pdf</a>.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. [2019b]</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00359</em>, 2019b.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Xiaoxia Wu, Cheng Li, Reza&nbsp;Yazdani Aminabadi, Zhewei Yao, and Yuxiong He.

</span>
<span class="ltx_bibblock">Understanding int4 quantization for transformer models: Latency speedup, composability, and failure cases.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12017</em>, 2023.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik&nbsp;Abinav Sankararaman, Barlas Oguz, et&nbsp;al.

</span>
<span class="ltx_bibblock">Effective long-context scaling of foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16039</em>, 2023.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu&nbsp;Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.12244</em>, 2023.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce&nbsp;Bian, Chao Yin, Chenxu Lv, Da&nbsp;Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock">09 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/pdf/2309.10305.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/2309.10305.pdf</a>.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young et&nbsp;al. [2014]</span>
<span class="ltx_bibblock">
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.

</span>
<span class="ltx_bibblock">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 2:67–78, 2014.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Gyeong-In Yu, Joo&nbsp;Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.

</span>
<span class="ltx_bibblock">Orca: A Distributed Serving System for Transformer-Based Generative Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)</em>, pages 521–538, 2022.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Yijiong Yu, Zhe Zhou, Zhixiao Qi, and Yongfeng Huang.

</span>
<span class="ltx_bibblock">Paraphrasing the original text makes high accuracy long-context qa.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11193</em>, 2023.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yunjie et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Ji&nbsp;Yunjie, Deng Yong, Gong Yan, Peng Yiping, Niu Qiang, Zhang Lei, Ma&nbsp;Baochang, and Li&nbsp;Xiangang.

</span>
<span class="ltx_bibblock">Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.14742</em>, 2023.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">HellaSwag: Can a Machine Really Finish Your Sentence?, 2019.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Xiaotian Zhang, Chunyang Li, Yi&nbsp;Zong, Zhengyu Ying, Liang He, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">Evaluating the Performance of Large Language Models on GAOKAO Benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.12474</em>, 2023a.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.

</span>
<span class="ltx_bibblock">Llavar: Enhanced visual instruction tuning for text-rich image understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.17107</em>, 2023b.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Huaixiu&nbsp;Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed&nbsp;H. Chi, Quoc&nbsp;V Le, and Denny Zhou.

</span>
<span class="ltx_bibblock">Take a step back: Evoking reasoning via abstraction in large language models, 2023a.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric.&nbsp;P Xing, Hao Zhang, Joseph&nbsp;E. Gonzalez, and Ion Stoica.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena, 2023b.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment, 2023.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. [2016]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li&nbsp;Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 4995–5004, 2016.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.04651" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.04652" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2403.04652">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.04652" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.04653" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:01:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
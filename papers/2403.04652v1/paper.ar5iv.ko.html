<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.04652] Yi: Open Foundation Models by 01.AI</title><meta property="og:description" content="We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities.
The Yi model family is based on 6B and 34B pretrained language models, then we extend…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yi: Open Foundation Models by 01.AI">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Yi: Open Foundation Models by 01.AI">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.04652">

<!--Generated on Fri Apr  5 17:01:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Yi: Open Foundation Models by 01.AI </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id1.1.id1" class="ltx_text ltx_font_bold">01.AI</span> 
<br class="ltx_break">  
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_bold">Code:</span>  <a target="_blank" href="https://github.com/01-ai/Yi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/01-ai/Yi</a>
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_bold">Model:</span>  <a target="_blank" href="https://huggingface.co/01-ai" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/01-ai</a> 
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">강력한 다차원 기능을 보여주는 일련의 언어 및 멀티모달 모델인 Yi 모델 패밀리를 소개합니다. Yi 모델 패밀리는 6B 및 34B 사전 학습된 언어 모델을 기반으로 하며 채팅 모델, 200K 긴 컨텍스트 모델, 깊이 확장 모델 및 비전 언어 모델로 확장한다. 우리의 기본 모델은 MMLU와 같은 광범위한 벤치마크에서 강력한 성능을 달성하고, 우리의 미세 조정 채팅 모델은 알파카발 및 챗봇 아레나와 같은 주요 평가 플랫폼에서 강력한 인간 선호율을 제공한다. 확장 가능한 슈퍼 컴퓨팅 인프라와 고전적인 변압기 아키텍처를 기반으로, Yi 모델의 성능은 주로 데이터 엔지니어링 노력의 결과로 인한 데이터 품질에 기인한다. 사전 학습을 위해 캐스케이드 데이터 중복 제거 및 품질 필터링 파이프라인을 사용하여 3.1조 개의 영어 및 중국어 말뭉치를 구성합니다. 미세 조정을 위해 모든 인스턴스가 기계 학습 엔지니어에 의해 직접 검증되도록 여러 반복에 걸쳐 소규모(10K 미만) 명령어 데이터 세트를 연마합니다. 비전 언어의 경우 채팅 언어 모델과 비전 트랜스포머 인코더를 결합하고 언어 모델의 의미 공간에 시각적 표현을 정렬하도록 모델을 훈련한다. 가벼운 연속 사전 훈련을 통해 컨텍스트 길이를 200K로 확장하고 강력한 건초 스택 내 바늘 검색 성능을 보여준다. 우리는 지속적인 사전 훈련을 통해 사전 훈련된 체크포인트의 깊이를 확장하는 것이 성능을 더욱 향상시킨다는 것을 보여준다. 현재 결과를 감안할 때 철저히 최적화된 데이터를 사용하여 모델 매개변수를 계속 확장하면 훨씬 더 강력한 프론티어 모델로 이어질 것이라고 믿습니다.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S1" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S2" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS1" title="In 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS2" title="In 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tokenization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS3" title="In 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Model Architecture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S3" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Finetuning</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS1" title="In 3 Finetuning ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS2" title="In 3 Finetuning ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Training Method</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S4" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S5" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Safety</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S6" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Evaluations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S6.SS1" title="In 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Base Model Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS1.SSS1" title="In 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS1.SSS2" title="In 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Discussions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS1.SSS3" title="In 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>In-Context Learning Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S6.SS2" title="In 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Chat Model Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS2.SSS1" title="In 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Automatic Evaluations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S6.SS2.SSS2" title="In 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Human Evaluations</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S7" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Capability Extension</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS1" title="In 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Long Context Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS2" title="In 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Vision-Language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS3" title="In 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Depth Upscaling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S8" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Final Discussions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a href="#A1" title="In Yi: Open Foundation Models by 01.AI" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Author List and Contributions</span></a></li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">대형 언어 모델의 최근 돌파구는 인공 지능의 전체 분야에 혁명을 일으켰고 잠재적으로 인간 사회 전체에 걸쳐 방사되었다. 대규모 언어 모델에 대한 우리의 비전은 그것들을 차세대 컴퓨팅 플랫폼으로 만들고 크게 증폭된 지능으로 전체 커뮤니티에 힘을 실어주는 것입니다. 이 미션을 위한 단계로, 우리는 3.1T 고도로 설계된 많은 양의 데이터에 대해 처음부터 사전 훈련된 Yi 모델 시리즈, 6B 및 34B 언어 모델을 제시하고 작지만 세심하게 연마된 정렬 데이터에 대해 미세 조정한다. 다음 섹션에서 자세히 설명할 상당한 엔지니어링 노력의 결과 데이터 품질로 인해 Yi는 GPT-3.5 벤치마크 점수와 인간 선호도에 근접한다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">Yi 모델 시리즈를 설계할 때 대부분 <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">모델 규모, 데이터 규모 및 데이터 품질</span>: (1)에 관한 다음 차원에 관심이 있습니다. 모델 척도를 선택할 때 데시데라타는 제한적 24G 메모리인 RTX 4090과 같은 소비자 등급 하드웨어에서 추론할 수 있는 충분히 작은 모델을 가지고 있지만 복잡한 추론과 창발 능력으로 여전히 충분히 크다. 이것이 우리가 34B가 좋은 성과-비용 균형을 제공한다는 것을 발견한 이유이다. 34B는 Chinchilla<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib30" title="">30</a>]</cite>와 LLaMA<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib77" title="">77</a>]</cite>에서 사용하는 기존의 70B보다 작기 때문에 감소된 컴퓨팅 플롭을 보상하기 위해 프리트레인 데이터 규모를 3.1T 토큰으로 증가시킨다. 이것은 모델-데이터 스케일 조합이 포스트 친칠라 최적 체제<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib64" title="">64</a>]</cite>에 속하게 한다. 즉, 우리는 계산 최적(1T 주변)보다 더 많은 토큰(3T)에서 모델을 능가한다. 이점은 추론 측면에서 얻을 수 있는데, 서비스 비용을 줄임으로써 더 강력한 성능을 얻을 수 있기 때문이다: int4 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib81" title="">81</a>]</cite> 양자화 후, 성능 저하가 거의 없는 24G GPU 메모리에서 34B 채팅 모델을 서비스할 수 있다; (3). 우리의 데이터 엔지니어링 원칙은 사전 훈련과 미세 조정 모두를 위해 양보다 질을 촉진하는 것이다. 사전 훈련 데이터 품질은 캐스케이드 필터링 방법 및 의도적으로 증가된 중복 제거 강도를 갖는 정교한 데이터 클리닝 파이프라인에 의해 보장된다; (4). 데이터 미세조정의 경우 사용자 피드백을 기반으로 여러 번의 반복에 걸쳐 10K 미만의 지침을 수작업으로 만들어 품질을 크게 강조한다. 이 방법은 FLAN <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite> 및 UltraChat <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>]</cite>와 같은 수량 스케일링 스타일 명령어 튜닝 작업에서 크게 벗어났지만 LIMA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib94" title="">94</a>]</cite>와 같은 핸드 크래프팅 스타일 작업과 더 많이 정렬된다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">사전 학습 데이터 클리닝 시스템은 언어, 휴리스틱 텍스트 특징, 복잡성, 의미, 주제, 안전성에 기반한 정교한 필터링 파이프라인과 단락, MinHash, 정확한 매칭에 기반한 계단식 중복 제거 프로세스를 특징으로 한다. 이 철저한 파이프라인은 CCNet <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib80" title="">80</a>]</cite>, RefinedWeb <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib56" title="">56</a>]</cite> 및 RedPajama <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib13" title="">13</a>]</cite>와 같은 기존 파이프라인보다 훨씬 높은 제거율로 이어지며, 이는 데이터 엔지니어링 성공의 핵심이라고 생각한다. 기본 원칙은 사전 훈련이 데이터 스케일링을 요구하지만, 큰 원시 데이터에 대해 모델을 훈련시키는 것보다 사용되는 데이터가 고품질인지 확인하고 싶은 것이다. 즉, 우리는 광범위한 필터링 없이 10T 토큰보다 정교한 엔지니어링보다 3T 토큰을 선호한다. 모델 아키텍처와 관련하여, GQA(Grouped-Query Attention) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>, SwiGLU <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib68" title="">68</a>]</cite> activation, RoPE ABF(adjusted base frequency) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib82" title="">82</a>]</cite>를 갖는 Transformer 아키텍처의 표준 구현을 사용한다. 이 디자인 선택은 트랜스포머 원지 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib78" title="">78</a>]</cite>, 나중에 GPT-3와 Chinchilla<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib30" title="">30</a>]</cite>에 의해 수정된 다음, LLaMA<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib77" title="">77</a>]</cite>, Baichuan<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib84" title="">84</a>]</cite>, Qwen<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib3" title="">3</a>]</cite> 및 많은 관련 작업에서 뿌리를 둔 표준 접근법이다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">GPT-3.5 매칭 인간 선호도에 접근하기 위해, 미세 조정 데이터 세트는 엄선된 다중 회전 명령-응답 쌍으로부터 선별되고, 기계 학습 엔지니어 팀에 의해 직접 주석이 달린 다음 사용자 피드백의 여러 반복을 통해 연마된다. 위에서 언급한 바와 같이, 미세 조정 데이터 세트의 크기는 10K 미만이지만 모델 개발 타임라인에 걸쳐 계속해서 개선되었다. 데이터 세트의 관리 가능한 크기로 인해 최적의 데이터 구성을 식별하고 다양성을 촉진하며 효과적인 하이퍼파라미터를 찾기 위해 광범위한 그리드 검색을 사용했다. 8-비트 및 4-비트 양자화 후에, 최종 채팅 모델은 bf16 포맷에 비해 성능 저하 없이 거의 소비자 등급 GPU에 배치될 수 있다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">이 모델의 성능을 컨텍스트 스케일링, 비전 언어 적응 및 깊이 업스케일링의 세 가지 차원에서 추가로 확장한다. 200K 컨텍스트 길이를 달성하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Fu et al. [<a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite>의 동시 작업과 유사하게 약 5B 길이 업샘플링된 데이터에 대해 모델을 계속 사전 훈련한다. 이 모델을 비전 언어 작업에 적용하기 위해 비전 인코더를 통합하고 <cite class="ltx_cite ltx_citemacro_citet">Liu et al. [<a class="ltx_ref" href="#bib.bib47" title="">47</a>]</cite>의 실습을 따르고 개선하는 다단계 훈련 방법을 개발한다. 또한 깊이-업스케일링 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib38" title="">38</a>]</cite>의 효과, 즉 지속적인 사전 훈련으로 모델을 더 깊게 만들고 모델의 성능을 더욱 향상시키기 위한 효과를 확인한다.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p" id="S1.p6.1">우리의 인프라는 사전 훈련에서 미세 조정에서 서빙에 이르기까지 Yi 모델 시리즈의 전체 스택 개발에 대한 강력한 지원을 제공한다. 사전 학습을 지원하기 위해 클라우드 간 탄력적 태스크 스케줄링, 자동 장애 복구 및 토폴로지 인식 자원 할당을 개발하며, 이는 제한된 스위칭 오버헤드로 실시간 사용 가능한 GPU 노드 간 클러스터에 따라 태스크를 실행할 수 있도록 한다. 핀튜닝을 지원하기 위해 정책 모델의 경우 Megatron <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib70" title="">70</a>]</cite>를, 보상 모델의 경우 DeepSpeed <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib60" title="">60</a>]</cite>를 지원하는 계층적 스케줄링 프레임워크를 구축한다. 효율적인 추론을 위해 PagedAttention <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite>와 Dynamic Batching을 결합한 4비트 모델과 8비트 KV 캐시 양자화를 사용한다.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p" id="S1.p7.1">광범위한 실험은 Yi-34B가 성능과 효율성 모두에서 GPT-3.5와 일치할 수 있음을 보여준다. MLU<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite> (기본 모델의 경우), LMSys ELO Rating<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib93" title="">93</a>]</cite> (채팅 모델의 경우)와 같은 대부분의 표준 벤치마크에서 Yi-34B는 일반적으로 GPT-3.5와 동등한 점수를 달성한다. 모델 파라미터 및 KV 캐시 양자화를 수행한 후, 추론 비용은 또한 광범위한 커뮤니티가 비용 효율적인 장치에 모델을 배치할 수 있도록 제어된다. 또한 다중평가 벤치마크에서 상식 추론, 대학 시험, 수학, 코딩, 독해력 및 인간 선호율에 대한 Yi와 주요 LLM의 세부 성능 비교를 보고한다.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p class="ltx_p" id="S1.p8.1">출시 이후, Yi 모델 시리즈는 다음과 같은 관점에서 커뮤니티에 혜택을 주었다: (1). GPT-3.5 매칭 품질이지만 비용 효율적인 모델을 연구자에게 제공하고, 개발자는 언어 모델 기반 에이전트와 같은 AI 네이티브 애플리케이션을 구축할 수 있다. (2). 이는 로컬 실행 가능한 챗봇으로 최종 사용자에게 권한을 부여하므로 결과적으로 사용자 데이터 프라이버시를 보호하는 데 도움이 된다. (3). 이는 더 강력한 프론티어 모델을 달성하기 위해 추가 데이터 및 모델 스케일링에 대한 방향을 조명한다. 연구 및 상업적 용도로 모두 사용할 수 있습니다.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Pretraining</h2>

<figure id="S2.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="306" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1:</span>Yi’s pretraining data cleaning pipeline.</figcaption>
</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.1">사전 훈련에 대한 우리의 접근법은 고도로 조작된 대규모 사전 훈련 코퍼라에서 표준 밀집 변압기 아키텍처를 훈련시키는 것인데, 여기서 우리의 기본 가정은 높은 품질의 광범위한 데이터에 대해 훈련될 때 표준 아키텍처가 고급 능력을 나타낼 수 있다는 것이다. 즉, 우리는 실제로 광범위한 예비 건축 실험을 수행했지만 많은 건축 수정이 필요하지 않을 수 있습니다. 다음 하위 섹션에서는 먼저 데이터 엔지니어링 파이프라인에 대해 자세히 설명한 다음 모델 아키텍처에 대해 간략하게 설명합니다.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Processing</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p1.1">Yi 데이터 혼합물은 그림 1에 나와 있다. <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ Heuristic Rule Filters ‣ 2.1 Data Processing ‣ 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">2</span></a>. 고품질 이중 언어 사전 훈련 데이터를 생성하기 위해 그림 <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">1</span></a>와 같이 캐스케이드 데이터 처리 파이프라인을 세심하게 설계했다. 이 파이프라인은 품질과 다양성을 목표로 하는 일련의 데이터 정리 전략을 특징으로 합니다. 우리는 Common Crawl의 웹 문서부터 시작하여 언어 식별 및 복잡도 점수화를 위해 CCNet 파이프라인 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib79" title="">79</a>]</cite>를 사용한다. 그런 다음 아래에 자세히 설명된 대로 필터링 및 중복 제거 프로세스의 조합을 사용합니다.</p>
</div>
<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Heuristic Rule Filters</h5>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.1">필터의 이 부분은 낮은 품질의 텍스트를 제거하는 것을 목표로 합니다. 우리는 다음 (1)을 기준으로 텍스트를 필터링합니다. URL, 도메인, 단어 블록리스트 및 가블링된 텍스트 필터; (2). 문서 길이, 특수 기호의 비율 및 짧은 줄, 연속 줄 또는 불완전 줄의 비율; (3). 반복된 단어들, n-그램들, 또는 단락들 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib58" title="">58</a>]</cite>; 필터링 임계치들은 <cite class="ltx_cite ltx_citemacro_citet">Nguyen et al. [<a class="ltx_ref" href="#bib.bib52" title="">52</a>]</cite>에 설명된 바와 같이, 대형 문서 샘플들의 통계적 분석에 기초한다. 또한, 이메일 주소 및 전화번호와 같은 개인 식별 정보(PII)를 식별하고 익명화한다.</p>
</div>
<figure id="S2.F2" class="ltx_figure ltx_align_floatright"><img src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="175" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">그림 2:</span>Yi’s pre-training data mixture. 전반적으로 우리의 데이터는 영어와 중국어 모두 3.1T 고품질 토큰으로 구성되며 다양한 출처에서 나온다. LLaMA<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib76" title="">76</a>]</cite> 및 Falcon<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib56" title="">56</a>]</cite>와 같은 기존 혼합물과 가장 큰 차이점은 우리는 이중 언어이며 더 엄격한 세척 파이프라인으로 인해 더 높은 품질을 가지고 있다는 것이다.</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Learned Filters</h5>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.1">우리는 학습된 필터를 사용하여 표준 휴리스틱 규칙의 기능을 초과하는 미묘한 경우를 해결한다. 특히 커먼 크롤에서 추출한 중국 콘텐츠는 특히 음란물과 도박과 같은 부적절한 콘텐츠의 비율이 더 높은 독특한 과제를 제시한다. 전통적인 휴리스틱 규칙 기반 필터는 모든 유해한 콘텐츠를 효과적으로 식별하고 제거하기 위해 고군분투한다. 필터링 프로세스를 개선하기 위해, 우리는 필터링을 위해 학습된 채점자, 즉 복잡도 채점자, 품질 채점자, 안전 채점자 및 문서 일관성 채점자(1)를 통합했다. the <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.1.1">Perplexity Scorer</span>, utilizing the KenLM library as per CCNet <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib80" title="">80</a>]</cite>, evaluate a vast array of web documents, discarding with perplexity scores largely over average; (2). the <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.1.2">Quality Scorer</span>은 Wikipedia와 유사한 페이지를 품질에서 인식하고 선호하도록 훈련되고 그에 따라 점수를 할당하도록 훈련된 분류기이다. 품질 표준을 충족하지 못한 문서는 이후에 제거됩니다. (3). <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.1.3">Document Coherence Scorer</span>은 이질적인 문장 또는 문단으로 구성된 저품질 웹 문서를 식별하므로 일관성이 없습니다. 이러한 문서는 추가 분석을 위해 분할되거나 완전히 제거된다. (4). <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS0.Px2.p1.1.4">Safety Scorer</span>은 폭력, 포르노, 정치적 선전물과 같은 독성 콘텐츠를 포함하는 웹 문서를 식별하고 제거합니다.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Cluster-based Filters</h5>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px3.p1.1">또한 비감독 시맨틱 클러스터링을 사용하여 웹 문서를 그룹화한다. 이러한 클러스터링 과정을 통해 유사한 의미 특징을 공유하는 문서들을 효율적으로 식별하고 분석할 수 있다. 클러스터링된 데이터는 후속적으로 품질 라벨로 주석을 달아서 Yi의 데이터 혼합 전략의 최적화를 위한 필수 참조를 제공한다. 자동 및 수동 검증을 통해 저품질로 식별된 문서는 데이터셋에서 제외된다.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deduplication</h5>

<div id="S2.SS1.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.SSS0.Px4.p1.1">필터링 후 Penedo et al. (2023) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib56" title="">56</a>]</cite>의 절차에 따라 포괄적인 중복제거 파이프라인을 구현한다. 이 파이프라인은 문서 수준 MinHash 중복 제거 및 하위 문서 정확 일치 중복 제거를 통합하여 문서 내 및 문서 전체에서 중복된 내용을 효과적으로 식별하고 제거합니다. 또한 뉴스, 광고 및 지식 기반 콘텐츠와 같은 레이블을 예측하는 토픽 모델을 사용하여 웹 문서를 특정 테마로 분류한다. 최종 사전 훈련 데이터 세트에서는 정보 밀도를 보장하기 위해 도움이 덜 되는 콘텐츠, 주로 광고를 다운 샘플링한다. Yi의 사전 훈련 데이터의 최종 구성은 그림과 같다. <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ Heuristic Rule Filters ‣ 2.1 Data Processing ‣ 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tokenization</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p1.1">PentencePiece 프레임워크에서 구현된 바이트 쌍 인코딩(BPE) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib69" title="">69</a>]</cite> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib40" title="">40</a>]</cite>를 사용하여 사전 학습 데이터를 토큰화한다. Yi의 어휘 크기는 계산 효율성과 단어 이해의 균형을 맞추기 위해 64,000으로 설정된다. 특히 숫자 데이터를 더 잘 이해하기 위해 숫자를 개별 숫자로 나눕니다. 우리는 희귀 캐릭터가 결함 허용을 보장하기 위해 유니코드 바이트 인코딩으로 후퇴하도록 허용합니다. 모든 구두점을 반치폭 형식으로 전송하지 않도록 ID 토큰화기를 사용합니다. 영어를 우선시하는 LLM은 일반적으로 문장의 서로 다른 위치에서 동일한 단어를 일반화하기 위해 토큰라이저에서 더미 프리픽스(텍스트의 시작 부분에 공백)를 활용한다. 이 접근법은 특히 따옴표로 시작하는 문장의 경우 가정이 영어 문맥에서도 항상 성립하지 않으며 중국 문맥에서도 긍정적인 효과를 나타내지 않기 때문에 사용하지 않는다.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Model Architecture</h3>

<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S2.T1.2.3" class="ltx_tr">
<td id="S2.T1.2.3.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Models</td>
<td id="S2.T1.2.3.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Hidden Size</td>
<td id="S2.T1.2.3.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Q-heads</td>
<td id="S2.T1.2.3.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">KV-heads</td>
<td id="S2.T1.2.3.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Layers</td>
<td id="S2.T1.2.3.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Pretrain Seq. Len</td>
<td id="S2.T1.2.3.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Max LR</td>
</tr>
<tr id="S2.T1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">6B</td>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4</td>
<td id="S2.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><math id="S2.T1.1.1.1.m1.1" class="ltx_Math" alttext="3\times 10^{-4}" display="inline"><semantics id="S2.T1.1.1.1.m1.1a"><mrow id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.1.1.1.m1.1.1.2" xref="S2.T1.1.1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.T1.1.1.1.m1.1.1.1" xref="S2.T1.1.1.1.m1.1.1.1.cmml">×</mo><msup id="S2.T1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.m1.1.1.3.cmml"><mn id="S2.T1.1.1.1.m1.1.1.3.2" xref="S2.T1.1.1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.T1.1.1.1.m1.1.1.3.3" xref="S2.T1.1.1.1.m1.1.1.3.3.cmml"><mo id="S2.T1.1.1.1.m1.1.1.3.3a" xref="S2.T1.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.T1.1.1.1.m1.1.1.3.3.2" xref="S2.T1.1.1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1"><times id="S2.T1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1.1"></times><cn type="integer" id="S2.T1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.m1.1.1.2">3</cn><apply id="S2.T1.1.1.1.m1.1.1.3.cmml" xref="S2.T1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.1.1.1.m1.1.1.3.1.cmml" xref="S2.T1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.T1.1.1.1.m1.1.1.3.2.cmml" xref="S2.T1.1.1.1.m1.1.1.3.2">10</cn><apply id="S2.T1.1.1.1.m1.1.1.3.3.cmml" xref="S2.T1.1.1.1.m1.1.1.3.3"><minus id="S2.T1.1.1.1.m1.1.1.3.3.1.cmml" xref="S2.T1.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.T1.1.1.1.m1.1.1.3.3.2.cmml" xref="S2.T1.1.1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">3\times 10^{-4}</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.2.2" class="ltx_tr">
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S2.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">7168</td>
<td id="S2.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">56</td>
<td id="S2.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">8</td>
<td id="S2.T1.2.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">60</td>
<td id="S2.T1.2.2.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><math id="S2.T1.2.2.1.m1.1" class="ltx_Math" alttext="1.5\times 10^{-4}" display="inline"><semantics id="S2.T1.2.2.1.m1.1a"><mrow id="S2.T1.2.2.1.m1.1.1" xref="S2.T1.2.2.1.m1.1.1.cmml"><mn id="S2.T1.2.2.1.m1.1.1.2" xref="S2.T1.2.2.1.m1.1.1.2.cmml">1.5</mn><mo lspace="0.222em" rspace="0.222em" id="S2.T1.2.2.1.m1.1.1.1" xref="S2.T1.2.2.1.m1.1.1.1.cmml">×</mo><msup id="S2.T1.2.2.1.m1.1.1.3" xref="S2.T1.2.2.1.m1.1.1.3.cmml"><mn id="S2.T1.2.2.1.m1.1.1.3.2" xref="S2.T1.2.2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.T1.2.2.1.m1.1.1.3.3" xref="S2.T1.2.2.1.m1.1.1.3.3.cmml"><mo id="S2.T1.2.2.1.m1.1.1.3.3a" xref="S2.T1.2.2.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.T1.2.2.1.m1.1.1.3.3.2" xref="S2.T1.2.2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.m1.1b"><apply id="S2.T1.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1"><times id="S2.T1.2.2.1.m1.1.1.1.cmml" xref="S2.T1.2.2.1.m1.1.1.1"></times><cn type="float" id="S2.T1.2.2.1.m1.1.1.2.cmml" xref="S2.T1.2.2.1.m1.1.1.2">1.5</cn><apply id="S2.T1.2.2.1.m1.1.1.3.cmml" xref="S2.T1.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.2.2.1.m1.1.1.3.1.cmml" xref="S2.T1.2.2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.T1.2.2.1.m1.1.1.3.2.cmml" xref="S2.T1.2.2.1.m1.1.1.3.2">10</cn><apply id="S2.T1.2.2.1.m1.1.1.3.3.cmml" xref="S2.T1.2.2.1.m1.1.1.3.3"><minus id="S2.T1.2.2.1.m1.1.1.3.3.1.cmml" xref="S2.T1.2.2.1.m1.1.1.3.3"></minus><cn type="integer" id="S2.T1.2.2.1.m1.1.1.3.3.2.cmml" xref="S2.T1.2.2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.1.m1.1c">1.5\times 10^{-4}</annotation></semantics></math></td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>Yi-6B 및 Yi-34B의 모델 configs. LR은 학습률을 의미합니다.</figcaption>
</figure>
<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p1.1">Yi는 LLaMA의 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib77" title="">77</a>]</cite> 구현을 기반으로 하는 고전적인 디코더 전용 트랜스포머 아키텍처 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib78" title="">78</a>]</cite>의 수정된 버전을 사용한다. 주요 파라미터 설정은 Table <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2.3 Model Architecture ‣ 2 Pretraining ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">1</span></a>에 요약되어 있다. LLaMA에서 Yi로의 수정은 아래에 추가로 요약된다:</p>
</div>
<section id="S2.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Attention Mechanism</h5>

<div id="S2.SS3.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS3.SSS0.Px1.p1.1">LLaMA 2는 가장 큰 70B 모델에서만 Grouped-Query Attention(GQA) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>를 사용하고, 7B와 13B는 full attention를 사용한다. 우리는 Yi-6B와 Yi-34B 모두에 GQA를 통합한다. GQA는 쿼리 헤드를 G 그룹으로 분할하여 쿼리 [<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib1" title="">1</a>]</cite>]의 각 그룹 내에서 단일 키 및 값 헤드를 공유합니다. 이 접근법은 원래의 Multi-Head Attention (MHA) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib67" title="">67</a>, <a class="ltx_ref" href="#bib.bib16" title="">16</a>, <a class="ltx_ref" href="#bib.bib57" title="">57</a>]</cite>와 비교하여 훈련 및 추론 비용의 상당한 감소를 제공한다. 우리는 6B 소형 모델에 GQA를 적용한 후 성능 저하를 관찰하지 않는다.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Activation Function</h5>

<div id="S2.SS3.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS3.SSS0.Px2.p1.3">Yi의 주의 후 계층으로 SwiGLU<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib68" title="">68</a>]</cite>를 사용하여 활성화 크기를 <math alttext="4h" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS3.SSS0.Px2.p1.1.m1.1a"><mrow id="S2.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">4</mn><mo id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.1" lspace="0em" rspace="0em" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1"><times id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.1"></times><cn id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" type="integer" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.2">4</cn><ci id="S2.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS0.Px2.p1.1.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px2.p1.1.m1.1c">4h</annotation></semantics></math>에서 <math alttext="8/3h" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS3.SSS0.Px2.p1.2.m2.1a"><mrow id="S2.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.cmml"><mrow id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml"><mn id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.2" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.2.cmml">8</mn><mo id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.1" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.1.cmml">/</mo><mn id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.3" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">3</mn></mrow><mo id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.1" lspace="0em" rspace="0em" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1"><times id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.1"></times><apply id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2"><divide id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.1"></divide><cn id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" type="integer" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.2">8</cn><cn id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.2.3">3</cn></apply><ci id="S2.SS3.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS3.SSS0.Px2.p1.2.m2.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px2.p1.2.m2.1c">8/3h</annotation></semantics></math>(<math alttext="h" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px2.p1.3.m3.1"><semantics id="S2.SS3.SSS0.Px2.p1.3.m3.1a"><mi id="S2.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS3.SSS0.Px2.p1.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px2.p1.3.m3.1b"><ci id="S2.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS3.SSS0.Px2.p1.3.m3.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px2.p1.3.m3.1c">h</annotation></semantics></math>는 숨겨진 크기를 나타냄)로 줄여 정상적인 주의 후 계층과 일치하도록 한다. 이 조정은 또한 GQA로 인한 매개변수 감소를 보상하여 전체 매개변수 수를 기존 7B 및 34B 모델과 비교할 수 있게 한다.</p>
</div>
</section>
<section id="S2.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Positional Embedding and Long Context</h5>

<div id="S2.SS3.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS3.SSS0.Px3.p1.1">표준 구현에 따라 RoPE(Rotary Position Embedding) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib73" title="">73</a>]</cite>를 사용한다. <cite class="ltx_cite ltx_citemacro_citet">Xiong et al. [<a class="ltx_ref" href="#bib.bib82" title="">82</a>]</cite>에 소개된 기본 주파수(RoPE ABF)를 조정하여 기본 모델 자체가 4K 컨텍스트 길이에 대해 훈련되는 200K까지의 긴 컨텍스트 창을 지원한다. 기본 모델을 더 긴 컨텍스트에 적용하기 위해 주로 책에서 약간 업샘플링된 긴 시퀀스가 있는 사전 훈련 데이터 혼합물에서 10B 토큰에 대해 모델을 계속 사전 훈련한다. 1-2B 토큰만이 4K-200K 길이의 낮은 손실로 수렴하기에 충분하며, 경량 미세 조정은 더 나아가 거의 완전한 장문맥 검색 성능을 유도한다는 것을 관찰한다. 이러한 관찰에 기초하여, 우리는 사전 훈련된 길이(4K)보다 더 긴 의존성을 모델링하는 능력이 (사후 훈련에 의해 주입되는 것이 아니라) 본질적인 능력이라고 보는 경향이 있다. 즉, 기본 모델은 모델이 더 짧게 훈련되더라도 4K 종속성보다 더 길게 모델링할 수 있는 능력을 이미 가지고 있으며, 포스트-트레인/미세조정 절차는 단순히 이 능력을 해제한다.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Finetuning</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">우리의 미세 조정 방법은 양보다 데이터 품질을 크게 강조한다. 우리의 접근법은 <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">not</span>은 FLAN <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite> 및 UltraChat <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>]</cite>와 같은 기존 데이터 집약적 접근법을 따르며, 이는 SFT 데이터를 수백만 개의 엔트리로 확장하지만 각 엔트리는 스케일이 너무 크기 때문에 주의 깊게 조사되지 않을 수 있다. 이와는 대조적으로, 우리의 방법은 스케일링보다는 데이터 선택에 초점을 맞춘 LIMA<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib94" title="">94</a>]</cite> 및 DEITA<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib48" title="">48</a>]</cite> 접근법과 일치한다. 척도가 10K 미만인 경우 <span class="ltx_text ltx_font_italic" id="S3.p1.1.2">every single data point</span>을 검사 하 고 최적화할 수 있습니다. 아래에서는 데이터 구성 및 교육 세부 사항에 대해 설명합니다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Preprocessing</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Quality is All You Need</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">우리의 미세 조정 데이터 세트는 10K 미만의 다중 회전 명령-응답 대화 쌍으로 구성되며, 각 항목은 여러 반복 및 사용자 피드백에서 구성되고 연마된다. 예비 실험에서 수십만 항목의 오픈 소스 데이터와 비교하여 더 작고 수동으로 주석이 달린 데이터 세트의 결과가 우수하다는 것을 관찰하기 때문에 이 접근법을 취한다. 이러한 관찰은 <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. [<a class="ltx_ref" href="#bib.bib77" title="">77</a>], Zhou et al. [<a class="ltx_ref" href="#bib.bib94" title="">94</a>], Gemini Team et al. [<a class="ltx_ref" href="#bib.bib23" title="">23</a>]</cite>에서 보고된 것과 일치한다.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p2.1">다음 기술을 사용하여 신속한 배포 선택, 응답 형식화 및 연쇄 사고 형식화를 개선합니다. (1). 빠른 배포 선택을 위해 WizardLM<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib83" title="">83</a>]</cite>에서 영감을 받아 복합 명령어를 개발하고 복잡도를 높이기 위해 점진적으로 진화시켰다. 이 방법은 실험에서 SFT 데이터의 크기를 크게 줄였다. 응답 형식의 경우 일반적으로 LIMA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib94" title="">94</a>]</cite>에서 확장 된 기본 스타일을 사용 합니다. 전반적으로 응답은 본문이 일반적으로 글머리 기호 목록인 도입-본체-결론 형식으로 구성된다. CoT 데이터 포맷팅의 경우, 우리는 추상화를 수행하여 더 높은 수준의 솔루션을 공식화함으로써 <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. [<a class="ltx_ref" href="#bib.bib92" title="">92</a>]</cite>에서 영감을 받은 "Step-Back" 패턴을 사용하고, 더 구체적인 원문에 대한 추론을 탐구한다.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p3.1">우리는 환각과 반복을 줄이기 위해 추가적인 노력을 기울인다: (1). 환각을 줄이기 위해, 우리는 반응에 있는 지식이 모델 안에 포함되지 않도록 조사하고 보장하며, 암기로 이어질 수 있는 반응을 제거한다; (2). 우리는 반복을 줄이기 위해 일반적으로 존재하지만 미세 조정 데이터에서 간과될 수 있는 응답의 반복적인 회전을 다시 작성한다.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Diversity and Mixture</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">다양한 능력의 적용 범위를 보장하기 위해 우리는 질의 응답, 창의적 쓰기, 대화, 추론, 수학, 코딩, 안전, 이중 언어 능력 등과 같은 영역을 포괄하는 광범위한 오픈 소스 프롬프트를 포함했다.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p2.1">InsTag<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib49" title="">49</a>]</cite>에서 영감을 받아 다양한 기능의 방향에 대한 세밀한 제어를 얻기 위해 명령어 태깅 시스템을 개발한다. 다양성에 초점을 맞춘 샘플링 알고리즘을 설계함으로써 다양한 태그에 걸친 명령어 분포의 균형을 주의 깊게 맞추었다. 이 접근법은 향상된 교차 작업 견고성을 달성하기 위해 다양한 미세 조정 데이터 세트를 보장한다.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p3.1">성능의 서로 다른 방향의 균형을 맞추기 위한 최적의 데이터 비율을 달성하기 위해 근사 격자 탐색을 사용하여 데이터 혼합을 결정한다. <cite class="ltx_cite ltx_citemacro_citet">Dong et al. [<a class="ltx_ref" href="#bib.bib20" title="">20</a>]</cite>에 의해 동기화된 이 과정은 각 능력에 대해 {1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64} 비율로 실험하는 것을 포함했다. 검색 프로세스는 검증 결과와 사내 인간 평가 세트에 의해 안내되었다.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">ChatML Format</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">데이터 품질과 다양성에 대한 초점을 넘어, 우리의 관찰은 데이터의 형식이 모델의 궁극적인 성능에 실질적으로 영향을 미친다는 것을 보여주었다. 이를 위해 ChatML-style format <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib53" title="">53</a>]</cite>를 구현하였다. 이러한 구조화된 접근법은 모델이 시스템 구성들, 사용자 입력들, 및 어시스턴트 응답들과 같은 다양한 정보 유형들 사이에서 구별하는 것을 가능하게 한다.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Method</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.5">우리는 미세조정을 위해 다음 단어 예측 손실을 사용하고 응답에 대한 손실만 계산하지만 시스템 및 사용자 명령은 계산하지 않는다. <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">β</mi><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝛽</ci><cn id="S3.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\beta_{1}</annotation></semantics></math>가 0.9로 설정되고, <math alttext="\beta_{2}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">β</mi><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝛽</ci><cn id="S3.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\beta_{2}</annotation></semantics></math>가 0.999로 설정되며, <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\epsilon</annotation></semantics></math>가 <math alttext="10^{-8}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><msup id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mn id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mo id="S3.SS2.p1.4.m4.1.1.3a" xref="S3.SS2.p1.4.m4.1.1.3.cmml">−</mo><mn id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">8</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">superscript</csymbol><cn id="S3.SS2.p1.4.m4.1.1.2.cmml" type="integer" xref="S3.SS2.p1.4.m4.1.1.2">10</cn><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><minus id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3"></minus><cn id="S3.SS2.p1.4.m4.1.1.3.2.cmml" type="integer" xref="S3.SS2.p1.4.m4.1.1.3.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">10^{-8}</annotation></semantics></math>로 설정된 AdamW Optimizer를 사용한다. 배치 크기 64와 함께 시퀀스 길이 4096을 사용한다. 훈련 단계는 일정한 <math alttext="1\times 10^{-5}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mn id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">1</mn><mo id="S3.SS2.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.5.m5.1.1.1.cmml">×</mo><msup id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mn id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">10</mn><mrow id="S3.SS2.p1.5.m5.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml"><mo id="S3.SS2.p1.5.m5.1.1.3.3a" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml">−</mo><mn id="S3.SS2.p1.5.m5.1.1.3.3.2" xref="S3.SS2.p1.5.m5.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><times id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1"></times><cn id="S3.SS2.p1.5.m5.1.1.2.cmml" type="integer" xref="S3.SS2.p1.5.m5.1.1.2">1</cn><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3">superscript</csymbol><cn id="S3.SS2.p1.5.m5.1.1.3.2.cmml" type="integer" xref="S3.SS2.p1.5.m5.1.1.3.2">10</cn><apply id="S3.SS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3"><minus id="S3.SS2.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3"></minus><cn id="S3.SS2.p1.5.m5.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.p1.5.m5.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">1\times 10^{-5}</annotation></semantics></math> 학습률, 무게 감소 0.1, 최대 임계값 1.0의 그래디언트 클리핑, Yi-34B-Chat의 경우 45, Yi-6B-Chat의 경우 5의 노이즈 스케일로 300으로 설정했다.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Infrastructure</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">우리는 전체 스택 데이터 처리, 사전 훈련, 미세 조정 및 서비스를 지원하는 인프라를 구축합니다. 우리의 인프라의 성과: (1). 상기 컴퓨팅 리소스를 자동 관리 및 모니터링하는 단계; (2). 최적화된 병렬 전략, 커널 효율성 및 긴 컨텍스트 지원으로부터 훈련 속도를 향상시켰다; (3). DPO(Direct Preference Optimization) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite>; (4)에서 Megatron과 DeepSpeed for multiple model을 동시에 사용하는 등 이질적인 분산 훈련 백엔드를 지원하는 통일된 미세 조정 프레임워크. 양자화, 연속 배칭 및 페이징된 주의와 같은 가속을 제공하는 다양한 LLM에 의해 배치 비용을 감소시킨다. 아래에서는 이러한 기법들을 하나씩 설명한다.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Computing Resources Management</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">대규모 언어 모델 개발, 특히 수천 개의 GPU에서 수개월이 걸릴 수 있는 사전 트레이닝을 효율적으로 스케줄링하기 위해, 서로 다른 우선순위의 사전 트레이닝, SFT 및 RLHF 태스크를 관리하기 위해 고효율의 멀티 클라우드 태스크 스케줄링 알고리즘을 구축한다. 또한 GPU 가용성을 기반으로 사전 훈련 작업을 다양한 노드 크기로 자동으로 탄력적으로 확장할 수 있는 고성능 사내 훈련 프레임워크를 구축한다. 더 중요한 것은 모든 훈련 관련 하이퍼 매개 변수가 동시에 원활하게 조정된다는 것입니다.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p2.1">대규모 언어 모델 훈련 단계에서 GPU 충돌에서 통신 패브릭 오류에서 손실 스파이크에 이르기까지 광범위한 오류가 정기적으로 발생한다. 다음 전략을 사용하여 이러한 신뢰성 문제를 해결합니다. (1) 다양한 종류의 소프트웨어/하드웨어 오류 범주에 대해 노드의 자동화된 검사, 예측 및 레이블링을 적용합니다. 오염된 것으로 표시된 노드는 오류가 제거될 때까지 리소스 풀에서 일시적으로 제거됩니다. (2) 훈련 작업 중 장애 발생 시 빠르고 자동 복구를 위한 사전 점검과 능력을 갖춘 작업 큐잉 시스템을 구현한다. (3) 사용자 친화적인 다중 작업 제출 및 관리 콘솔을 개발하여 개발자가 훈련 작업 및 하이퍼 매개 변수를 원활하게 관리하고 추적할 수 있도록 한다.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Performance and Cost Efficiency</h5>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.1.1">Memory</span> 및 <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.1.2">communication</span> 제한은 더 많은 GPU를 추가하는 것을 넘어 통합 솔루션을 요구하는 대규모 모델 훈련의 두 가지 주요 기술적 과제이다. 메모리 및 통신 제한 사항을 해결하기 위해 다음과 같은 기술을 사용하고 개선한다: (1) ZeRO-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib60" title="">60</a>]</cite> 최적화 상태 분할을 통해 메모리 소비를 제거하는 데이터 병렬 프로세스 교차; (2) 노드 간 통신 병목 현상을 피하기 위해 각 컴퓨팅 노드 내에서 파이프라인 병렬과 결합된 텐서 병렬 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib70" title="">70</a>]</cite> 그리고 3D 병렬 전략은 활성화 체크포인팅을 사용하지 않고 파이프라인 버블을 최소화하도록 잘 설계되고 최적화된다; (3) 플래시 어텐션 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib15" title="">15</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib14" title="">14</a>]</cite> 및 JIT 커널과 같은 커널 융합 기술은 리던던트 글로벌 메모리 액세스 및 소비를 줄이기 위해 사용된다; (4) 일반적인 팻 트리 토폴로지(fat-tree-topology)의 한계인 스위치의 다른 계층에 걸친 통신을 최소화하기 위한 토폴로지 인식 자원 할당(랭킹 전략)</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Finetuning Framework</h5>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">사전 훈련과 달리 미세 조정 LLM은 DPO<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib59" title="">59</a>]</cite> 및 PPO<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib54" title="">54</a>]</cite>의 관행과 같이 여러 모델의 오케스트레이션을 필요로 할 수 있다. 이러한 훈련 작업에서 일반적인 프로세스는 참조/보상 모델을 사용하여 데이터의 배치를 예측한 다음(또한 사소한 시간이 필요함), 타겟 모델이 손실 및 업데이트 파라미터를 계산하기 위해 이 데이터를 사용하도록 한다. 이를 위해 단일 작업에서 서로 다른 LLM에 대한 여러 백 엔드를 지원하기 위한 다중 모델 스케줄링 프레임워크를 구축한다. 예를 들어, DPO로 언어 모델을 미세조정할 때, 참조 모델로부터의 중간 결과들이 캐싱되고 재사용될 수 있어, 훈련 속도 및 자원 비용이 감독된 미세조정 대응물들에 근접하도록 개선된다.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:432.0pt;height:195.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.0pt,10.8pt) scale(0.9,0.9) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">MMLU</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">BBH</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">C-Eval</span></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">CMMLU</span></td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.7.1" class="ltx_text ltx_font_bold">Gaokao</span></td>
<td id="S4.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.8.1" class="ltx_text ltx_font_bold">CR</span></td>
<td id="S4.T2.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.9.1" class="ltx_text ltx_font_bold">RC</span></td>
<td id="S4.T2.1.1.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.10.1" class="ltx_text ltx_font_bold">Code</span></td>
<td id="S4.T2.1.1.1.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.1.11.1" class="ltx_text ltx_font_bold">Math</span></td>
</tr>
<tr id="S4.T2.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.1.1" class="ltx_text ltx_font_bold">GPT-4</span></td>
<td id="S4.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.3.1" class="ltx_text ltx_font_bold">83.0</span></td>
<td id="S4.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.4.1" class="ltx_text ltx_font_bold">86.7</span></td>
<td id="S4.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">69.9</td>
<td id="S4.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">71.0</td>
<td id="S4.T2.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.3</td>
<td id="S4.T2.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.8.1" class="ltx_text ltx_font_bold">89.3</span></td>
<td id="S4.T2.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.10.1" class="ltx_text ltx_font_bold">65.3</span></td>
<td id="S4.T2.1.1.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.2.11.1" class="ltx_text ltx_font_bold">66.1</span></td>
</tr>
<tr id="S4.T2.1.1.3" class="ltx_tr">
<td id="S4.T2.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.3.1.1" class="ltx_text ltx_font_bold">GPT-3.5</span></td>
<td id="S4.T2.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">69.1</td>
<td id="S4.T2.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">70.1</td>
<td id="S4.T2.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">52.5</td>
<td id="S4.T2.1.1.3.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">55.5</td>
<td id="S4.T2.1.1.3.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">51.1</td>
<td id="S4.T2.1.1.3.8" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">83.1</td>
<td id="S4.T2.1.1.3.9" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.3.10" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">54.8</td>
<td id="S4.T2.1.1.3.11" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">35.6</td>
</tr>
<tr id="S4.T2.1.1.4" class="ltx_tr">
<td id="S4.T2.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.4.1.1" class="ltx_text ltx_font_bold">Qwen</span></td>
<td id="S4.T2.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14B</td>
<td id="S4.T2.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">66.7</td>
<td id="S4.T2.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">53.4</td>
<td id="S4.T2.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.1</td>
<td id="S4.T2.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">71.0</td>
<td id="S4.T2.1.1.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.5</td>
<td id="S4.T2.1.1.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">74.2</td>
<td id="S4.T2.1.1.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.5</td>
<td id="S4.T2.1.1.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">40.6</td>
<td id="S4.T2.1.1.4.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">43.1</td>
</tr>
<tr id="S4.T2.1.1.5" class="ltx_tr">
<td id="S4.T2.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T2.1.1.5.1.1" class="ltx_text ltx_font_bold">Llama2</span></td>
<td id="S4.T2.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S4.T2.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.6</td>
<td id="S4.T2.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">44.1</td>
<td id="S4.T2.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">71.1</td>
<td id="S4.T2.1.1.5.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">68.9</td>
<td id="S4.T2.1.1.5.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">27.8</td>
<td id="S4.T2.1.1.5.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">24.2</td>
</tr>
<tr id="S4.T2.1.1.6" class="ltx_tr">
<td id="S4.T2.1.1.6.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">70B</td>
<td id="S4.T2.1.1.6.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">69.7</td>
<td id="S4.T2.1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">64.9</td>
<td id="S4.T2.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.1</td>
<td id="S4.T2.1.1.6.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">53.3</td>
<td id="S4.T2.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">23.3</td>
<td id="S4.T2.1.1.6.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">72.7</td>
<td id="S4.T2.1.1.6.8" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">72.3</td>
<td id="S4.T2.1.1.6.9" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">38.4</td>
<td id="S4.T2.1.1.6.10" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">35.2</td>
</tr>
<tr id="S4.T2.1.1.7" class="ltx_tr">
<td id="S4.T2.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.7.1.1" class="ltx_text ltx_font_bold">Baichuan-2</span></td>
<td id="S4.T2.1.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13B</td>
<td id="S4.T2.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">55.0</td>
<td id="S4.T2.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">49.0</td>
<td id="S4.T2.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">59.0</td>
<td id="S4.T2.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">61.97</td>
<td id="S4.T2.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">45.6</td>
<td id="S4.T2.1.1.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">66.3</td>
<td id="S4.T2.1.1.7.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.4</td>
<td id="S4.T2.1.1.7.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">23.4</td>
<td id="S4.T2.1.1.7.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.1</td>
</tr>
<tr id="S4.T2.1.1.8" class="ltx_tr">
<td id="S4.T2.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.8.1.1" class="ltx_text ltx_font_bold">InternLM</span></td>
<td id="S4.T2.1.1.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">20B</td>
<td id="S4.T2.1.1.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.1</td>
<td id="S4.T2.1.1.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">52.5</td>
<td id="S4.T2.1.1.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">58.8</td>
<td id="S4.T2.1.1.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">59.0</td>
<td id="S4.T2.1.1.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">45.5</td>
<td id="S4.T2.1.1.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">78.3</td>
<td id="S4.T2.1.1.8.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.8.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">34.8</td>
<td id="S4.T2.1.1.8.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">30.26</td>
</tr>
<tr id="S4.T2.1.1.9" class="ltx_tr">
<td id="S4.T2.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.9.1.1" class="ltx_text ltx_font_bold">Skywork</span></td>
<td id="S4.T2.1.1.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13B</td>
<td id="S4.T2.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.1</td>
<td id="S4.T2.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">41.7</td>
<td id="S4.T2.1.1.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">60.6</td>
<td id="S4.T2.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">61.8</td>
<td id="S4.T2.1.1.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">68.1</td>
<td id="S4.T2.1.1.9.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.4</td>
<td id="S4.T2.1.1.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">61.4</td>
<td id="S4.T2.1.1.9.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">64.9</td>
<td id="S4.T2.1.1.9.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">18.1</td>
</tr>
<tr id="S4.T2.1.1.10" class="ltx_tr">
<td id="S4.T2.1.1.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.10.1.1" class="ltx_text ltx_font_bold">Falcon</span></td>
<td id="S4.T2.1.1.10.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">180B</td>
<td id="S4.T2.1.1.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">70.4</td>
<td id="S4.T2.1.1.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">54.0</td>
<td id="S4.T2.1.1.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">57.8</td>
<td id="S4.T2.1.1.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">58.0</td>
<td id="S4.T2.1.1.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">59.0</td>
<td id="S4.T2.1.1.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">74.4</td>
<td id="S4.T2.1.1.10.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.10.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S4.T2.1.1.10.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
</tr>
<tr id="S4.T2.1.1.11" class="ltx_tr">
<td id="S4.T2.1.1.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S4.T2.1.1.11.1.1" class="ltx_text ltx_font_bold">Yi</span></td>
<td id="S4.T2.1.1.11.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">6B</td>
<td id="S4.T2.1.1.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">63.2</td>
<td id="S4.T2.1.1.11.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">42.8</td>
<td id="S4.T2.1.1.11.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.0</td>
<td id="S4.T2.1.1.11.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">75.5</td>
<td id="S4.T2.1.1.11.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.2</td>
<td id="S4.T2.1.1.11.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">72.2</td>
<td id="S4.T2.1.1.11.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">68.7</td>
<td id="S4.T2.1.1.11.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">21.1</td>
<td id="S4.T2.1.1.11.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">18.6</td>
</tr>
<tr id="S4.T2.1.1.12" class="ltx_tr">
<td id="S4.T2.1.1.12.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S4.T2.1.1.12.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">76.3</td>
<td id="S4.T2.1.1.12.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">54.3</td>
<td id="S4.T2.1.1.12.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.12.4.1" class="ltx_text ltx_font_bold">81.4</span></td>
<td id="S4.T2.1.1.12.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.12.5.1" class="ltx_text ltx_font_bold">83.7</span></td>
<td id="S4.T2.1.1.12.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.12.6.1" class="ltx_text ltx_font_bold">82.8</span></td>
<td id="S4.T2.1.1.12.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">80.7</td>
<td id="S4.T2.1.1.12.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S4.T2.1.1.12.8.1" class="ltx_text ltx_font_bold">76.5</span></td>
<td id="S4.T2.1.1.12.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">32.1</td>
<td id="S4.T2.1.1.12.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">40.8</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span> 오픈 소스 기반 모델과 비교하여 그룹화된 학술 벤치마크에 대한 전체 성능. <span class="ltx_text ltx_font_bold" id="S4.T2.4.1">CR</span>은 Commonsense Reasoning의 약자입니다. <span class="ltx_text ltx_font_bold" id="S4.T2.5.2">RC</span>은 Reading Comprehension의 약자이다.</figcaption>
</figcaption>
</figure>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fast and Efficient Inference</h5>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.1">우리는 주로 디코딩 속도와 메모리 사용을 개선하기 위해 양자화, 동적 배칭 및 페이지드 어텐션을 사용한다. 우리는 메모리 공간과 계산 수요를 줄이기 위해 양자화를 사용한다. 4비트 모델 양자화 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib81" title="">81</a>]</cite> 및 8비트 KV 캐시 양자화 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib18" title="">18</a>]</cite>에 의해, 우리는 거의 제로 성능 저하(예를 들어, MMLU/CMMLU 벤치마크에서 1<math alttext="\%" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px4.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px4.p1.1.m1.1a"><mo id="S4.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.1.m1.1c">\%</annotation></semantics></math> 정확도 저하)와 함께 상당한 GPU 메모리 절약을 달성할 수 있다. 동적 배칭 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib86" title="">86</a>]</cite>를 사용하여 응답 시간을 최소화하고 배칭 효율을 향상시켰다. 메모리 활용도를 높이고 디코딩을 개선하기 위해 PagedAttention<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib41" title="">41</a>]</cite>를 사용한다.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Long-context Window Support</h5>

<div id="S4.SS0.SSS0.Px5.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS0.SSS0.Px5.p1.1">본 논문에서는 최대 200K 컨텍스트 길이의 지속적인 사전 훈련과 미세 조정을 지원하기 위해 계산-통신 중복, 시퀀스 병렬, 통신 압축을 구현하고 개선한다. 컨텍스트 길이를 200K로 확장하는 방법은 <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px5.p1.1.1">solely</span> 엔지니어링 기반, 즉 희소, 로컬 또는 슬라이딩 윈도우 주의와 같은 모델 아키텍처를 수정하지 않습니다. 모델은 입력이 200K인 경우에도 전체 주의를 사용하여 유지됩니다.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Safety</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">모델의 신뢰성과 안전성을 높이기 위해 풀스택 책임형 AI 안전 엔진(RAISE)을 개발합니다. RAISE는 안전한 사전 훈련, 정렬 및 배치를 보장합니다. 이 섹션에서는 사전 훈련 및 정렬 단계에서 안전 조치에 대해 설명합니다.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Safety in Pretraining</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">표준 사전 훈련 데이터 안전 관행 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib77" title="">77</a>, <a class="ltx_ref" href="#bib.bib58" title="">58</a>, <a class="ltx_ref" href="#bib.bib5" title="">5</a>]</cite>와 정렬하여 휴리스틱 규칙, 키워드 매칭 및 학습된 분류기를 기반으로 필터 세트를 구축하여 개인 식별자와 개인 데이터가 포함된 텍스트를 제거하고 성적, 폭력적, 극단주의 콘텐츠를 줄인다.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Safety in Alignment</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1"><cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib24" title="">24</a>, <a class="ltx_ref" href="#bib.bib35" title="">35</a>]</cite>의 기존 연구를 통해 먼저 포괄적인 안전 분류 체계를 구축한다. 이 분류법은 환경적 불협화음, 미신, 종교적 민감성, 차별적 관행, 약물 남용, 폭력적 행동, 불법 행위, 혐오 발언, 윤리 위반, 사생활 침해, 자해, 성적 명시적 내용, 정신 건강 문제 및 사이버 보안 위협을 포함한 광범위한 잠재적 문제를 다룬다. 강력한 정렬을 위해 이러한 범주를 반영하는 데이터 세트를 선별하고 대화 상자 SFT 데이터와 혼합한다. 또한 정렬 단계에서 공격 시나리오를 시뮬레이션하는 대상 프롬프트 세트를 포함하여 악의적인 사용에 대한 모델의 복원력을 효과적으로 개선했습니다.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluations</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">평가 결과, Yi 모델 패밀리는 광범위한 태스크에서 영감을 주는 성능을 달성하고 GPT-3.5 사용자 선호율에 근접함을 보여준다. 먼저 표준 벤치마크에 대한 기본 모델 성능을 보고하고 채팅 모델 성능과 사용자 선호율에 대해 논의한다.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Base Model Performance</h3>

<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Main Results</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">여기서는 표준 학술 벤치마크에 걸쳐 기본 모델과 잘 알려진 여러 기본 모델에 대한 결과를 제시한다. 오픈 소스 모델을 벤치마킹하는 동안 파이프라인에 의해 생성된 결과와 공개 소스에 보고된 결과 사이의 차이를 관찰했다. 이 차이에 대한 보다 심층적인 조사를 수행할 때, 주로 다른 모델이 다른 프롬프트, 후처리 전략 및 샘플링 기술을 사용하기 때문이다. 이러한 차이는 잠재적으로 결과에 상당한 변화를 유발할 수 있다. 프롬프트 및 후처리 전략은 원래 벤치마크<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>, <a class="ltx_ref" href="#bib.bib63" title="">63</a>, <a class="ltx_ref" href="#bib.bib89" title="">89</a>, <a class="ltx_ref" href="#bib.bib62" title="">62</a>, <a class="ltx_ref" href="#bib.bib11" title="">11</a>, <a class="ltx_ref" href="#bib.bib50" title="">50</a>, <a class="ltx_ref" href="#bib.bib75" title="">75</a>, <a class="ltx_ref" href="#bib.bib61" title="">61</a>, <a class="ltx_ref" href="#bib.bib8" title="">8</a>, <a class="ltx_ref" href="#bib.bib10" title="">10</a>, <a class="ltx_ref" href="#bib.bib12" title="">12</a>, <a class="ltx_ref" href="#bib.bib28" title="">28</a>, <a class="ltx_ref" href="#bib.bib7" title="">7</a>, <a class="ltx_ref" href="#bib.bib2" title="">2</a>, <a class="ltx_ref" href="#bib.bib27" title="">27</a>, <a class="ltx_ref" href="#bib.bib42" title="">42</a>, <a class="ltx_ref" href="#bib.bib90" title="">90</a>, <a class="ltx_ref" href="#bib.bib72" title="">72</a>, <a class="ltx_ref" href="#bib.bib74" title="">74</a>]</cite>의 기본 설정과 일치합니다. 생성된 콘텐츠에 대해 후처리 없이 탐욕적 디코딩을 사용한다. 공개적으로 보고되지 않은 점수(또는 다른 설정으로 보고된 점수)의 경우 파이프라인으로 결과를 얻으려고 한다. 공개적으로 찾을 수 있는 점수에 대해서는 기존 수치를 직접 보고한다. 우리는 주로 LLaMA 2<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib77" title="">77</a>]</cite>의 관행을 따르는 다음 벤치마크를 사용한다:</p>
</div>
<div id="S6.SS1.SSS1.p2" class="ltx_para">
<dl id="S6.I1" class="ltx_description">
<dt id="S6.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Commonsense Reasoning:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix1.p1" class="ltx_para">
<p class="ltx_p" id="S6.I1.ix1.p1.1">상식추론을 평가하기 위해 PIQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib4" title="">4</a>]</cite>, SIQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib63" title="">63</a>]</cite>, HellaSwag<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib89" title="">89</a>]</cite>, WinoGrande<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib62" title="">62</a>]</cite>, ARC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib11" title="">11</a>]</cite>, OpenBookQA(OBQA)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib50" title="">50</a>]</cite>, CommonsenseQA(CSQA)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib75" title="">75</a>]</cite>를 포함하였다. CSQA는 7샷 설정을 사용하여 독점적으로 테스트되었으며 다른 모든 테스트는 0샷 구성으로 수행되었다.</p>
</div>
</dd>
<dt id="S6.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Reading Comprehension:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix2.p1" class="ltx_para">
<p class="ltx_p" id="S6.I1.ix2.p1.1">읽기 이해를 위해 SQuAD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib61" title="">61</a>]</cite>, QuAC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib8" title="">8</a>]</cite>, BoolQ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib10" title="">10</a>]</cite>에서 0-shot 평균을 보고한다.</p>
</div>
</dd>
<dt id="S6.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Math:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix3.p1" class="ltx_para">
<p class="ltx_p" id="S6.I1.ix3.p1.1">GSM8K<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib12" title="">12</a>]</cite> (8 shot), MATH<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib28" title="">28</a>]</cite> (4 shot) 벤치마크의 평균에 대해 구체적인 프롬프팅 전략(예: Chain-of-Think 프롬프팅) 및 기타 앙상블 기법(예: majority voting) 없이 pass@1 정확도를 가지고 보고한다.</p>
</div>
</dd>
<dt id="S6.I1.ix4" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix4.1.1.1" class="ltx_text ltx_font_bold">Code:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix4.p1" class="ltx_para">
<p class="ltx_p" id="S6.I1.ix4.p1.1">HumanEval<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib7" title="">7</a>]</cite> (Chen et al., 2021)과 MBPP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib2" title="">2</a>]</cite> (Austin et al., 2021)에 대한 모델의 평균 pass@1 점수를 보고한다.</p>
</div>
</dd>
<dt id="S6.I1.ix5" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix5.1.1.1" class="ltx_text ltx_font_bold">Popular Aggregated Benchmark:</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix5.p1" class="ltx_para">
<p class="ltx_p" id="S6.I1.ix5.p1.1">MLU<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib27" title="">27</a>]</cite>(5-shot), CMMLU<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib42" title="">42</a>]</cite>(5-shot), Gaokao-Bench<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib90" title="">90</a>]</cite>(5-shot), BigBench<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib72" title="">72</a>]</cite>Hard(BBH<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib74" title="">74</a>]</cite>)(3-shot)에 대한 전반적인 결과를 보고한다.</p>
</div>
</dd>
</dl>
</div>
<figure id="S6.T3" class="ltx_table">
<table id="S6.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S6.T3.1.1" class="ltx_tr">
<td id="S6.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S6.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.3.1" class="ltx_text ltx_font_bold">GSM8k</span></td>
<td id="S6.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.4.1" class="ltx_text ltx_font_bold">MATH</span></td>
<td id="S6.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.5.1" class="ltx_text ltx_font_bold">Human-Eval pass@1</span></td>
<td id="S6.T3.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.1.6.1" class="ltx_text ltx_font_bold">MBPP pass@1</span></td>
</tr>
<tr id="S6.T3.1.2" class="ltx_tr">
<td id="S6.T3.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.2.1.1" class="ltx_text ltx_font_bold">GPT-3.5</span></td>
<td id="S6.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">57.1</td>
<td id="S6.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.0</td>
<td id="S6.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">48.1</td>
<td id="S6.T3.1.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">61.4</td>
</tr>
<tr id="S6.T3.1.3" class="ltx_tr">
<td id="S6.T3.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.1.1" class="ltx_text ltx_font_bold">GPT-4</span></td>
<td id="S6.T3.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T3.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.3.1" class="ltx_text ltx_font_bold">92.0</span></td>
<td id="S6.T3.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.4.1" class="ltx_text ltx_font_bold">40.2</span></td>
<td id="S6.T3.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.5.1" class="ltx_text ltx_font_bold">67.0</span></td>
<td id="S6.T3.1.3.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.3.6.1" class="ltx_text ltx_font_bold">63.6</span></td>
</tr>
<tr id="S6.T3.1.4" class="ltx_tr">
<td id="S6.T3.1.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.4.1.1" class="ltx_text ltx_font_bold">Falcon</span></td>
<td id="S6.T3.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">180B</td>
<td id="S6.T3.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">54.4</td>
<td id="S6.T3.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T3.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">0.61</td>
<td id="S6.T3.1.4.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">47.0</td>
</tr>
<tr id="S6.T3.1.5" class="ltx_tr">
<td id="S6.T3.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S6.T3.1.5.1.1" class="ltx_text ltx_font_bold">Qwen</span></td>
<td id="S6.T3.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">51.7</td>
<td id="S6.T3.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.6</td>
<td id="S6.T3.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">29.9</td>
<td id="S6.T3.1.5.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">34.0</td>
</tr>
<tr id="S6.T3.1.6" class="ltx_tr">
<td id="S6.T3.1.6.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">14B</td>
<td id="S6.T3.1.6.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">61.3</td>
<td id="S6.T3.1.6.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">24.8</td>
<td id="S6.T3.1.6.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.3</td>
<td id="S6.T3.1.6.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.9</td>
</tr>
<tr id="S6.T3.1.7" class="ltx_tr">
<td id="S6.T3.1.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S6.T3.1.7.1.1" class="ltx_text ltx_font_bold">Baichuan 2</span></td>
<td id="S6.T3.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">24.5</td>
<td id="S6.T3.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">5.6</td>
<td id="S6.T3.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">18.3</td>
<td id="S6.T3.1.7.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">28.3</td>
</tr>
<tr id="S6.T3.1.8" class="ltx_tr">
<td id="S6.T3.1.8.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">13B</td>
<td id="S6.T3.1.8.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">22.1</td>
<td id="S6.T3.1.8.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">10.1</td>
<td id="S6.T3.1.8.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">20.7</td>
<td id="S6.T3.1.8.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">26.1</td>
</tr>
<tr id="S6.T3.1.9" class="ltx_tr">
<td id="S6.T3.1.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="3"><span id="S6.T3.1.9.1.1" class="ltx_text ltx_font_bold">L<span id="S6.T3.1.9.1.1.1" class="ltx_text" style="font-size:83%;">LaMA</span> 2</span></td>
<td id="S6.T3.1.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16.7</td>
<td id="S6.T3.1.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">3.3</td>
<td id="S6.T3.1.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">12.8</td>
<td id="S6.T3.1.9.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">14.8</td>
</tr>
<tr id="S6.T3.1.10" class="ltx_tr">
<td id="S6.T3.1.10.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S6.T3.1.10.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">42.2</td>
<td id="S6.T3.1.10.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">6.2</td>
<td id="S6.T3.1.10.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">22.6</td>
<td id="S6.T3.1.10.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.0</td>
</tr>
<tr id="S6.T3.1.11" class="ltx_tr">
<td id="S6.T3.1.11.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">70B</td>
<td id="S6.T3.1.11.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">56.8</td>
<td id="S6.T3.1.11.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">13.5</td>
<td id="S6.T3.1.11.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.7</td>
<td id="S6.T3.1.11.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">45.0</td>
</tr>
<tr id="S6.T3.1.12" class="ltx_tr">
<td id="S6.T3.1.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.12.1.1" class="ltx_text ltx_font_bold">Mistral</span></td>
<td id="S6.T3.1.12.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">47.5</td>
<td id="S6.T3.1.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">11.3</td>
<td id="S6.T3.1.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">30.5</td>
<td id="S6.T3.1.12.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">47.5</td>
</tr>
<tr id="S6.T3.1.13" class="ltx_tr">
<td id="S6.T3.1.13.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.13.1.1" class="ltx_text ltx_font_bold">InternLM</span></td>
<td id="S6.T3.1.13.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">20B</td>
<td id="S6.T3.1.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">62.9</td>
<td id="S6.T3.1.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">10.9</td>
<td id="S6.T3.1.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">28.1</td>
<td id="S6.T3.1.13.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">41.4</td>
</tr>
<tr id="S6.T3.1.14" class="ltx_tr">
<td id="S6.T3.1.14.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T3.1.14.1.1" class="ltx_text ltx_font_bold">Skywork</span></td>
<td id="S6.T3.1.14.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7B</td>
<td id="S6.T3.1.14.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">55.8</td>
<td id="S6.T3.1.14.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">7.8</td>
<td id="S6.T3.1.14.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">13.4</td>
<td id="S6.T3.1.14.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">22.8</td>
</tr>
<tr id="S6.T3.1.15" class="ltx_tr">
<td id="S6.T3.1.15.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2"><span id="S6.T3.1.15.1.1" class="ltx_text ltx_font_bold">Yi</span></td>
<td id="S6.T3.1.15.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">6B</td>
<td id="S6.T3.1.15.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">32.5</td>
<td id="S6.T3.1.15.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4.6</td>
<td id="S6.T3.1.15.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.9</td>
<td id="S6.T3.1.15.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">26.3</td>
</tr>
<tr id="S6.T3.1.16" class="ltx_tr">
<td id="S6.T3.1.16.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S6.T3.1.16.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">67.2</td>
<td id="S6.T3.1.16.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">14.4</td>
<td id="S6.T3.1.16.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">23.2</td>
<td id="S6.T3.1.16.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">41.0</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span>Comparison of models on GSM8k, MATH, Human-Eval, and MBPP.</figcaption>
</figure>
<div id="S6.SS1.SSS1.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS1.p3.1">이전 작업(일반적으로 <math alttext="\leq" class="ltx_Math" display="inline" id="S6.SS1.SSS1.p3.1.m1.1"><semantics id="S6.SS1.SSS1.p3.1.m1.1a"><mo id="S6.SS1.SSS1.p3.1.m1.1.1" xref="S6.SS1.SSS1.p3.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p3.1.m1.1b"><leq id="S6.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p3.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p3.1.m1.1c">\leq</annotation></semantics></math> 2T)에 비해 훨씬 더 많은 수의 토큰(3.1T)에 대해 훈련함으로써 표 <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ Finetuning Framework ‣ 4 Infrastructure ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">2</span></a>와 같이 벤치마크 전반에 걸쳐 상당한 성능 이득을 관찰했다. 그러나 특히 수학 및 코딩과 관련된 작업에서 우리 모델과 기존 오픈 소스 및 근접 소스 모델 사이에는 여전히 식별 가능한 차이가 있다는 점에 유의하는 것이 중요하다. 이러한 영역의 성능은 지속적인 사전 훈련 및 명령어 미세 조정에 의해 크게 향상될 수 있으므로 초기 설계 선택을 할 때 사전 훈련 코퍼스에 광범위한 수학적 및 코딩 콘텐츠를 통합하는 것을 자제했다. 앞으로 수학 및 코딩 기능이 강화된 모델을 출시할 계획입니다.</p>
</div>
<figure id="S6.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/x3.png" id="S6.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="373" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3:</span>Evaluating language model's in-context learning capability by inference of a weighted sum.</figcaption>
Considering the discussions of whether emergent ability is an artifact of measurement&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, we use
difference to the target (target number - model prediction) as a continuous measure,
and exact match (target number == model prediction) as a discontinuous measure.
A: when there is two linear coefficients, Yi-34B performs the best when measuring by the difference to the target number.
B: increasing the number of linear coefficients to 5, only models that are large enough (LLaMA2 70B and Mixtral 8x7B) can achieve meaningful exact match, showing that in-context learning complex functions is an emergent ability.
</figcaption>
</figure>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Discussions</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<dl id="S6.I2" class="ltx_description">
<dt id="S6.I2.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I2.ix1.1.1.1" class="ltx_text ltx_font_bold">Gain from Model Scale.</span></span></dt>
<dd class="ltx_item">
<div id="S6.I2.ix1.p1" class="ltx_para">
<p class="ltx_p" id="S6.I2.ix1.p1.1">우리는 Yi-34B가 동일한 프리트레이닝 코퍼스를 사용함에도 불구하고 Yi-6B에 비해 상당한 성능 향상이 있음을 관찰한다. 모델 크기가 클수록 탭(Tab)을 참조하여 코드 및 수학 벤치마크에서 성능이 향상됩니다. <a class="ltx_ref" href="#S6.T3" title="Table 3 ‣ 6.1.1 Main Results ‣ 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">3</span></a>는 상식 추론, 읽기 이해 또는 지식에 초점을 맞춘 벤치마크와 비교된다.</p>
</div>
</dd>
<dt id="S6.I2.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I2.ix2.1.1.1" class="ltx_text ltx_font_bold">Data Quality.</span></span></dt>
<dd class="ltx_item">
<div id="S6.I2.ix2.p1" class="ltx_para">
<p class="ltx_p" id="S6.I2.ix2.p1.1">Lee-34B 또는 Qwen-14B와 같은 고품질 프리트레인 데이터의 더 작은 모델은 일반적으로 더 큰 크기의 모델보다 더 나은 성능을 보여주지만 (아마도) Falcon-180B와 같은 더 낮은 품질의 데이터(Falcon-180B의 초점이 스케일링 쪽에 더 있을 수 있지만, 이는 분명히 그 자체로 중요한 가치)이다.</p>
</div>
</dd>
<dt id="S6.I2.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I2.ix3.1.1.1" class="ltx_text ltx_font_bold">Gap between GPT-4 and Open-source LLMs.</span></span></dt>
<dd class="ltx_item">
<div id="S6.I2.ix3.p1" class="ltx_para">
<p class="ltx_p" id="S6.I2.ix3.p1.1">탭을 기반으로 합니다. <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ Finetuning Framework ‣ 4 Infrastructure ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">2</span></a> 오픈 소스 LLM은 다양한 벤치마크에서 GPT-4 및 GPT-3.5의 성능에 여전히 뒤처져 있다는 점에 주목한다. 그러나 대표적인 이중언어 LLMs, 예를 들어 Qwen-14B 및 Yi-34B는 C-Eval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib31" title="">31</a>]</cite>, CMMLU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib42" title="">42</a>]</cite>, Gaokao <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib90" title="">90</a>]</cite>를 포함한 중국 지식 관련 벤치마크에서 GPT-4의 성능을 일치시키거나 능가할 수 있다. 그러나 BBH<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib72" title="">72</a>]</cite>, 코드(HumanEval), 수학(MATH)과 같은 추론 관련 벤치마크에서 GPT-4와 오픈 소스 모델 사이에는 여전히 큰 격차가 있다.</p>
</div>
</dd>
</dl>
</div>
</section>
<section id="S6.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>In-Context Learning Study</h4>

<div id="S6.SS1.SSS3.p1" class="ltx_para">
<p id="S6.SS1.SSS3.p1.9" class="ltx_p">We further investigate the in-context learning capability, i.e., the capability of inferring the underlying function given the few-show input-output demonstrations. We consider the task of inferring the linear coefficient of a weighted sum. Specifically, define <math id="S6.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="y=w_{1}x_{1}+w2x_{2}+...+w_{n}x_{n}" display="inline"><semantics id="S6.SS1.SSS3.p1.1.m1.1a"><mrow id="S6.SS1.SSS3.p1.1.m1.1.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.2.cmml">y</mi><mo id="S6.SS1.SSS3.p1.1.m1.1.1.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S6.SS1.SSS3.p1.1.m1.1.1.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.cmml"><mrow id="S6.SS1.SSS3.p1.1.m1.1.1.3.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.cmml"><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.2.cmml">w</mi><mn id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.1.cmml">​</mo><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.2.cmml">x</mi><mn id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.3.cmml">1</mn></msub></mrow><mo id="S6.SS1.SSS3.p1.1.m1.1.1.3.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S6.SS1.SSS3.p1.1.m1.1.1.3.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1.cmml">​</mo><mn id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1a" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1.cmml">​</mo><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.2.cmml">x</mi><mn id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.3.cmml">2</mn></msub></mrow><mo id="S6.SS1.SSS3.p1.1.m1.1.1.3.1a" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">+</mo><mi mathvariant="normal" id="S6.SS1.SSS3.p1.1.m1.1.1.3.4" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.4.cmml">…</mi><mo id="S6.SS1.SSS3.p1.1.m1.1.1.3.1b" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S6.SS1.SSS3.p1.1.m1.1.1.3.5" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.cmml"><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.2.cmml">w</mi><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.3.cmml">n</mi></msub><mo lspace="0em" rspace="0em" id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.1" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.1.cmml">​</mo><msub id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.cmml"><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.2" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.2.cmml">x</mi><mi id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.3" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.3.cmml">n</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.1.m1.1b"><apply id="S6.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1"><eq id="S6.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.1"></eq><ci id="S6.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.2">𝑦</ci><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3"><plus id="S6.SS1.SSS3.p1.1.m1.1.1.3.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.1"></plus><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2"><times id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.1"></times><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.2">𝑤</ci><cn type="integer" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.2.3">1</cn></apply><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.2">𝑥</ci><cn type="integer" id="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.2.3.3">1</cn></apply></apply><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3"><times id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.1"></times><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.2">𝑤</ci><cn type="integer" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.3">2</cn><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.2">𝑥</ci><cn type="integer" id="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.3.4.3">2</cn></apply></apply><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.4.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.4">…</ci><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5"><times id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.1"></times><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.2">𝑤</ci><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.2.3">𝑛</ci></apply><apply id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.1.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3">subscript</csymbol><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.2.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.2">𝑥</ci><ci id="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.3.cmml" xref="S6.SS1.SSS3.p1.1.m1.1.1.3.5.3.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.1.m1.1c">y=w_{1}x_{1}+w2x_{2}+...+w_{n}x_{n}</annotation></semantics></math>, our few-shot demonstration is <math id="S6.SS1.SSS3.p1.2.m2.5" class="ltx_Math" alttext="x_{1},x_{2},...,x_{n},y" display="inline"><semantics id="S6.SS1.SSS3.p1.2.m2.5a"><mrow id="S6.SS1.SSS3.p1.2.m2.5.5.3" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml"><msub id="S6.SS1.SSS3.p1.2.m2.3.3.1.1" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.cmml"><mi id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.2" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.2.cmml">x</mi><mn id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.3" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.3.cmml">1</mn></msub><mo id="S6.SS1.SSS3.p1.2.m2.5.5.3.4" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml">,</mo><msub id="S6.SS1.SSS3.p1.2.m2.4.4.2.2" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.cmml"><mi id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.2" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.2.cmml">x</mi><mn id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.3" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.3.cmml">2</mn></msub><mo id="S6.SS1.SSS3.p1.2.m2.5.5.3.5" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml">,</mo><mi mathvariant="normal" id="S6.SS1.SSS3.p1.2.m2.1.1" xref="S6.SS1.SSS3.p1.2.m2.1.1.cmml">…</mi><mo id="S6.SS1.SSS3.p1.2.m2.5.5.3.6" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml">,</mo><msub id="S6.SS1.SSS3.p1.2.m2.5.5.3.3" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.cmml"><mi id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.2" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.2.cmml">x</mi><mi id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.3" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.3.cmml">n</mi></msub><mo id="S6.SS1.SSS3.p1.2.m2.5.5.3.7" xref="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml">,</mo><mi id="S6.SS1.SSS3.p1.2.m2.2.2" xref="S6.SS1.SSS3.p1.2.m2.2.2.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.2.m2.5b"><list id="S6.SS1.SSS3.p1.2.m2.5.5.4.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3"><apply id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1">subscript</csymbol><ci id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.2">𝑥</ci><cn type="integer" id="S6.SS1.SSS3.p1.2.m2.3.3.1.1.3.cmml" xref="S6.SS1.SSS3.p1.2.m2.3.3.1.1.3">1</cn></apply><apply id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2">subscript</csymbol><ci id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.2">𝑥</ci><cn type="integer" id="S6.SS1.SSS3.p1.2.m2.4.4.2.2.3.cmml" xref="S6.SS1.SSS3.p1.2.m2.4.4.2.2.3">2</cn></apply><ci id="S6.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.1.1">…</ci><apply id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.1.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3">subscript</csymbol><ci id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.2">𝑥</ci><ci id="S6.SS1.SSS3.p1.2.m2.5.5.3.3.3.cmml" xref="S6.SS1.SSS3.p1.2.m2.5.5.3.3.3">𝑛</ci></apply><ci id="S6.SS1.SSS3.p1.2.m2.2.2.cmml" xref="S6.SS1.SSS3.p1.2.m2.2.2">𝑦</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.2.m2.5c">x_{1},x_{2},...,x_{n},y</annotation></semantics></math>, and we ask the model to (implicitly) infer <math id="S6.SS1.SSS3.p1.3.m3.4" class="ltx_Math" alttext="w_{1},w_{2},...,w_{n}" display="inline"><semantics id="S6.SS1.SSS3.p1.3.m3.4a"><mrow id="S6.SS1.SSS3.p1.3.m3.4.4.3" xref="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml"><msub id="S6.SS1.SSS3.p1.3.m3.2.2.1.1" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.cmml"><mi id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.2" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.2.cmml">w</mi><mn id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.3" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S6.SS1.SSS3.p1.3.m3.4.4.3.4" xref="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml">,</mo><msub id="S6.SS1.SSS3.p1.3.m3.3.3.2.2" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.cmml"><mi id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.2" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.2.cmml">w</mi><mn id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.3" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.3.cmml">2</mn></msub><mo id="S6.SS1.SSS3.p1.3.m3.4.4.3.5" xref="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S6.SS1.SSS3.p1.3.m3.1.1" xref="S6.SS1.SSS3.p1.3.m3.1.1.cmml">…</mi><mo id="S6.SS1.SSS3.p1.3.m3.4.4.3.6" xref="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml">,</mo><msub id="S6.SS1.SSS3.p1.3.m3.4.4.3.3" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.cmml"><mi id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.2" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.2.cmml">w</mi><mi id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.3" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.3.m3.4b"><list id="S6.SS1.SSS3.p1.3.m3.4.4.4.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3"><apply id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.2.cmml" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.2">𝑤</ci><cn type="integer" id="S6.SS1.SSS3.p1.3.m3.2.2.1.1.3.cmml" xref="S6.SS1.SSS3.p1.3.m3.2.2.1.1.3">1</cn></apply><apply id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.cmml" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.2.cmml" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.2">𝑤</ci><cn type="integer" id="S6.SS1.SSS3.p1.3.m3.3.3.2.2.3.cmml" xref="S6.SS1.SSS3.p1.3.m3.3.3.2.2.3">2</cn></apply><ci id="S6.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.1.1">…</ci><apply id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.1.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3">subscript</csymbol><ci id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.2.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.2">𝑤</ci><ci id="S6.SS1.SSS3.p1.3.m3.4.4.3.3.3.cmml" xref="S6.SS1.SSS3.p1.3.m3.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.3.m3.4c">w_{1},w_{2},...,w_{n}</annotation></semantics></math> by predicting the <math id="S6.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S6.SS1.SSS3.p1.4.m4.1a"><mi id="S6.SS1.SSS3.p1.4.m4.1.1" xref="S6.SS1.SSS3.p1.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.4.m4.1b"><ci id="S6.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S6.SS1.SSS3.p1.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.4.m4.1c">y</annotation></semantics></math> given a new set of input <math id="S6.SS1.SSS3.p1.5.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S6.SS1.SSS3.p1.5.m5.1a"><mi id="S6.SS1.SSS3.p1.5.m5.1.1" xref="S6.SS1.SSS3.p1.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.5.m5.1b"><ci id="S6.SS1.SSS3.p1.5.m5.1.1.cmml" xref="S6.SS1.SSS3.p1.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.5.m5.1c">x</annotation></semantics></math>. We use (a). the absolute difference between model prediction <math id="S6.SS1.SSS3.p1.6.m6.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S6.SS1.SSS3.p1.6.m6.1a"><mi id="S6.SS1.SSS3.p1.6.m6.1.1" xref="S6.SS1.SSS3.p1.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.6.m6.1b"><ci id="S6.SS1.SSS3.p1.6.m6.1.1.cmml" xref="S6.SS1.SSS3.p1.6.m6.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.6.m6.1c">y</annotation></semantics></math> and the ground truth <math id="S6.SS1.SSS3.p1.7.m7.1" class="ltx_Math" alttext="y^{*}" display="inline"><semantics id="S6.SS1.SSS3.p1.7.m7.1a"><msup id="S6.SS1.SSS3.p1.7.m7.1.1" xref="S6.SS1.SSS3.p1.7.m7.1.1.cmml"><mi id="S6.SS1.SSS3.p1.7.m7.1.1.2" xref="S6.SS1.SSS3.p1.7.m7.1.1.2.cmml">y</mi><mo id="S6.SS1.SSS3.p1.7.m7.1.1.3" xref="S6.SS1.SSS3.p1.7.m7.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.7.m7.1b"><apply id="S6.SS1.SSS3.p1.7.m7.1.1.cmml" xref="S6.SS1.SSS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.7.m7.1.1.1.cmml" xref="S6.SS1.SSS3.p1.7.m7.1.1">superscript</csymbol><ci id="S6.SS1.SSS3.p1.7.m7.1.1.2.cmml" xref="S6.SS1.SSS3.p1.7.m7.1.1.2">𝑦</ci><times id="S6.SS1.SSS3.p1.7.m7.1.1.3.cmml" xref="S6.SS1.SSS3.p1.7.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.7.m7.1c">y^{*}</annotation></semantics></math>, i.e., <math id="S6.SS1.SSS3.p1.8.m8.1" class="ltx_Math" alttext="|y-y^{*}|" display="inline"><semantics id="S6.SS1.SSS3.p1.8.m8.1a"><mrow id="S6.SS1.SSS3.p1.8.m8.1.1.1" xref="S6.SS1.SSS3.p1.8.m8.1.1.2.cmml"><mo stretchy="false" id="S6.SS1.SSS3.p1.8.m8.1.1.1.2" xref="S6.SS1.SSS3.p1.8.m8.1.1.2.1.cmml">|</mo><mrow id="S6.SS1.SSS3.p1.8.m8.1.1.1.1" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.cmml"><mi id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.2" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.2.cmml">y</mi><mo id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.1" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.1.cmml">−</mo><msup id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.cmml"><mi id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.2" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.2.cmml">y</mi><mo id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.3" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo stretchy="false" id="S6.SS1.SSS3.p1.8.m8.1.1.1.3" xref="S6.SS1.SSS3.p1.8.m8.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS3.p1.8.m8.1b"><apply id="S6.SS1.SSS3.p1.8.m8.1.1.2.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1"><abs id="S6.SS1.SSS3.p1.8.m8.1.1.2.1.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.2"></abs><apply id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1"><minus id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.1.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.1"></minus><ci id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.2.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.2">𝑦</ci><apply id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3"><csymbol cd="ambiguous" id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.1.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3">superscript</csymbol><ci id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.2.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.2">𝑦</ci><times id="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.3.cmml" xref="S6.SS1.SSS3.p1.8.m8.1.1.1.1.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.8.m8.1c">|y-y^{*}|</annotation></semantics></math> as a continuous measure, and use (b). the exact match <math id="S6.SS1.SSS3.p1.9.m9.1" class="ltx_math_unparsed" alttext="y==y^{*}" display="inline"><semantics id="S6.SS1.SSS3.p1.9.m9.1a"><mrow id="S6.SS1.SSS3.p1.9.m9.1b"><mi id="S6.SS1.SSS3.p1.9.m9.1.1">y</mi><mo rspace="0em" id="S6.SS1.SSS3.p1.9.m9.1.2">=</mo><mo lspace="0em" id="S6.SS1.SSS3.p1.9.m9.1.3">=</mo><msup id="S6.SS1.SSS3.p1.9.m9.1.4"><mi id="S6.SS1.SSS3.p1.9.m9.1.4.2">y</mi><mo id="S6.SS1.SSS3.p1.9.m9.1.4.3">∗</mo></msup></mrow><annotation encoding="application/x-tex" id="S6.SS1.SSS3.p1.9.m9.1c">y==y^{*}</annotation></semantics></math> as a discontinuous measure. We further note that most of the models perform reasonably well on addition and subtraction, so the ability to do arithmetic, as a confounding factor, can be ruled out.</p>
</div>
<div id="S6.SS1.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS3.p2.1">결과는 그림 <a class="ltx_ref" href="#S6.F3" title="Figure 3 ‣ 6.1.1 Main Results ‣ 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">3</span></a>에 나와 있다. 선형 계수를 [1, -1]로 설정할 때 Yi 34B와 LLaMA-2 70B가 정확한 일치의 가장 좋은 항을 수행한다는 것을 알 수 있다. 선형 계수의 수를 [1, 1, 1, 1, 1]로 증가시키면 목표와의 차이는 더 연속적이지만 큰 모델(LLaMA-2 70B 및 Mixtral)만이 정확한 일치에서 좋은 점수를 얻을 수 있는 창발 행동을 관찰한다. 이러한 관찰은 In-context 학습에 대한 Yi-34B의 성능에 대한 부수적인 증거를 제공하며 추가 스케일링을 통해 모델이 In-context 학습에 의해 더 복잡한 함수를 추론할 수 있음을 나타낸다.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Chat Model Performance</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS2.p1.1">이 섹션에서는 채팅 모델의 자동 및 인간 선호도 평가를 보고한다. 우리는 응답을 생성하기 위해 탐욕스러운 디코딩을 사용한다. 자동 평가 벤치마크의 경우 모델의 생성된 출력에서 답변을 추출하고 정확도를 계산한다. 평가 과정에서 서로 다른 프롬프트가 결과에 다양한 영향을 미친다는 것을 관찰했다. 따라서 동일한 질문 세트에 대해 동일한 프롬프트를 사용하여 모든 모델을 평가하여 가능한 한 공정하고 편향되지 않은 결과를 보장한다.</p>
</div>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Automatic Evaluations</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">자동 평가를 위해 Sec에 자세히 설명된 기본 모델에 대해 동일한 벤치마크를 사용한다. <a class="ltx_ref" href="#S6.SS1.SSS1" title="6.1.1 Main Results ‣ 6.1 Base Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">6.1.1</span></a>. 우리는 제로 샷과 소수 샷 방법을 모두 사용하지만 일반적으로 제로 샷이 채팅 모델에 더 적합하다. 우리의 평가는 명시적으로 또는 암시적으로 (소수 샷 예제의 포맷과 같은) 명령을 따르면서 응답을 생성하는 것을 포함한다. 그런 다음 생성된 텍스트에서 관련 답변을 분리합니다. 기본 모델과 달리 GSM8K 및 BBH 데이터 세트에 대한 제로 샷 평가를 위해 CoT(Chain-of-Think) 접근법을 사용하여 해답에 도달하기 전에 모델을 숙고하도록 안내한다.</p>
</div>
<div id="S6.SS2.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS2.SSS1.p2.1">결과는 탭에 표시됩니다. <a class="ltx_ref" href="#S6.T4" title="Table 4 ‣ 6.2.1 Automatic Evaluations ‣ 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">4</span></a>는 인간의 명령을 이해하고 적절한 명령 후속 응답을 생성하는 데 있어 채팅 모델의 효과를 보여준다. 특히 4-비트 양자화는 메모리 요구량을 크게 감소시키지만 모델 성능은 거의 떨어지지 않기 때문에 4-비트 양자화 결과를 강조한다. 이 관찰은 소비자 등급 장치에서 모델을 제공하는 기초 역할을 한다.</p>
</div>
<div id="S6.SS2.SSS1.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS2.SSS1.p3.1">굿하트의 원칙에 따라 측정 척도가 우리의 추구의 대상이 되면 신뢰할 수 있는 평가 기준이 되는 역할을 중단한다. 결과적으로 벤치마크에 대한 평가 결과는 정렬 훈련이 기본 모델의 기본 지식과 능력에 해로운 영향을 미치지 않도록 하기 위해 독점적으로 사용된다. 우리는 벤치마크 성능 향상을 목표로 채팅 모델의 목표 최적화에 참여하지 않습니다.</p>
</div>
<div id="S6.SS2.SSS1.p4" class="ltx_para">
<p class="ltx_p" id="S6.SS2.SSS1.p4.1">우리 모델의 능력의 일반화 가능성을 더 평가하기 위해, 우리는 xAI Grok 팀이 처음 제안한 다음 <cite class="ltx_cite ltx_citemacro_citet">Paster [<a class="ltx_ref" href="#bib.bib55" title="">55</a>]</cite>에 의해 재현된 2023 헝가리 고등학교 수학 기말고사 문제를 대상으로 수학적 계산 능력에 대한 평가를 수행했다. 이 평가는 우리 모델이 수학적으로 지향된 훈련 데이터 세트에 과적합 징후를 나타내는지 여부를 결정하기 위한 목적으로 수행되었다. 결과는 다음과 같다. <a class="ltx_ref" href="#S6.F4" title="Figure 4 ‣ 6.2.1 Automatic Evaluations ‣ 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">4</span></a>는 Yi-34B-Chat이 GSM8K와 헝가리 수학 시험 모두에서 영감을 주는 공연을 한다는 것을 보여준다. 그러나, Yi-6B-Chat은 (GSM8K와 헝가리 수학 시험 모두에서) 강력한 수학적 능력을 나타내지 않는다는 점에 주목하라. 우리는 더 작은 모델이 SFT 단계에서 해당 능력을 활성화하기 위해 더 많은 데이터가 필요할 수 있다고 추측한다.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<div id="S6.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:198.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.2pt,27.4pt) scale(0.782765244495076,0.782765244495076) ;">
<table id="S6.T4.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S6.T4.1.1.1" class="ltx_tr">
<td id="S6.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S6.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">MMLU</span></td>
<td id="S6.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">CMMLU</span></td>
<td id="S6.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.5.1" class="ltx_text ltx_font_bold">C-Eval(val)</span></td>
<td id="S6.T4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.6.1" class="ltx_text ltx_font_bold">TruthfulQA</span></td>
<td id="S6.T4.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.7.1" class="ltx_text ltx_font_bold">BBH</span></td>
<td id="S6.T4.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.1.8.1" class="ltx_text ltx_font_bold">GSM8K</span></td>
</tr>
<tr id="S6.T4.1.1.2" class="ltx_tr">
<td id="S6.T4.1.1.2.1" class="ltx_td" style="padding:1pt 2.0pt;"></td>
<td id="S6.T4.1.1.2.2" class="ltx_td" style="padding:1pt 2.0pt;"></td>
<td id="S6.T4.1.1.2.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.3.1" class="ltx_text ltx_font_bold">0-shot / 5-shot</span></td>
<td id="S6.T4.1.1.2.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.4.1" class="ltx_text ltx_font_bold">0-shot / 5-shot</span></td>
<td id="S6.T4.1.1.2.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.5.1" class="ltx_text ltx_font_bold">0-shot / 5-shot</span></td>
<td id="S6.T4.1.1.2.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.6.1" class="ltx_text ltx_font_bold">0-shot</span></td>
<td id="S6.T4.1.1.2.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.7.1" class="ltx_text ltx_font_bold">0-shot / 3-shot</span></td>
<td id="S6.T4.1.1.2.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.2.8.1" class="ltx_text ltx_font_bold">0-shot / 4-shot</span></td>
</tr>
<tr id="S6.T4.1.1.3" class="ltx_tr">
<td id="S6.T4.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">LLaMA2-Chat</td>
<td id="S6.T4.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">13B</td>
<td id="S6.T4.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">50.9 / 47.3</td>
<td id="S6.T4.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">27.5 / 35.1</td>
<td id="S6.T4.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">27.9 / 35.9</td>
<td id="S6.T4.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">36.8</td>
<td id="S6.T4.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">32.9 / 58.2</td>
<td id="S6.T4.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">36.9 / 2.7</td>
</tr>
<tr id="S6.T4.1.1.4" class="ltx_tr">
<td id="S6.T4.1.1.4.1" class="ltx_td" style="padding:1pt 2.0pt;"></td>
<td id="S6.T4.1.1.4.2" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">70B</td>
<td id="S6.T4.1.1.4.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">59.4 / 59.9</td>
<td id="S6.T4.1.1.4.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">36.1 / 41.0</td>
<td id="S6.T4.1.1.4.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">35.0 / 41.3</td>
<td id="S6.T4.1.1.4.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">54.0</td>
<td id="S6.T4.1.1.4.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">42.4 / 58.5</td>
<td id="S6.T4.1.1.4.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">47.1 / 58.7</td>
</tr>
<tr id="S6.T4.1.1.5" class="ltx_tr">
<td id="S6.T4.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">Baichuan2-Chat</td>
<td id="S6.T4.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">13B</td>
<td id="S6.T4.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">55.1 / 50.1</td>
<td id="S6.T4.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">58.6 / 59.5</td>
<td id="S6.T4.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">56.0 / 54.8</td>
<td id="S6.T4.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">49.0</td>
<td id="S6.T4.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">38.8 / 47.2</td>
<td id="S6.T4.1.1.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">45.7 / 23.3</td>
</tr>
<tr id="S6.T4.1.1.6" class="ltx_tr">
<td id="S6.T4.1.1.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">Qwen-Chat</td>
<td id="S6.T4.1.1.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">14B</td>
<td id="S6.T4.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">64.0 / 65.0</td>
<td id="S6.T4.1.1.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">67.7 / 70.6</td>
<td id="S6.T4.1.1.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">66.1 / 70.1</td>
<td id="S6.T4.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">52.5</td>
<td id="S6.T4.1.1.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">49.7 / 55.0</td>
<td id="S6.T4.1.1.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">59.5 / 61.2</td>
</tr>
<tr id="S6.T4.1.1.7" class="ltx_tr">
<td id="S6.T4.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">InternLM-Chat</td>
<td id="S6.T4.1.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">20B</td>
<td id="S6.T4.1.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">55.6 / 57.4</td>
<td id="S6.T4.1.1.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">53.6 / 53.8</td>
<td id="S6.T4.1.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">51.2 / 53.6</td>
<td id="S6.T4.1.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">51.8</td>
<td id="S6.T4.1.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">42.4 / 36.7</td>
<td id="S6.T4.1.1.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">15.7 / 43.4</td>
</tr>
<tr id="S6.T4.1.1.8" class="ltx_tr">
<td id="S6.T4.1.1.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">AquilaChat2</td>
<td id="S6.T4.1.1.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">34B</td>
<td id="S6.T4.1.1.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">65.2 / 66.7</td>
<td id="S6.T4.1.1.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">67.5 / 70.0</td>
<td id="S6.T4.1.1.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.8.5.1" class="ltx_text ltx_font_bold">83.0</span> / <span id="S6.T4.1.1.8.5.2" class="ltx_text ltx_font_bold">89.4</span>
</td>
<td id="S6.T4.1.1.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;"><span id="S6.T4.1.1.8.6.1" class="ltx_text ltx_font_bold">64.3</span></td>
<td id="S6.T4.1.1.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">20.1 / 34.3</td>
<td id="S6.T4.1.1.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">11.5 / 48.5</td>
</tr>
<tr id="S6.T4.1.1.9" class="ltx_tr">
<td id="S6.T4.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">Yi-Chat</td>
<td id="S6.T4.1.1.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">6B</td>
<td id="S6.T4.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">58.2 / 61.0</td>
<td id="S6.T4.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">69.4 / 74.7</td>
<td id="S6.T4.1.1.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">68.8 / 74.2</td>
<td id="S6.T4.1.1.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">50.6</td>
<td id="S6.T4.1.1.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">39.7 / 47.2</td>
<td id="S6.T4.1.1.9.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">38.4 / 44.9</td>
</tr>
<tr id="S6.T4.1.1.10" class="ltx_tr">
<td id="S6.T4.1.1.10.1" class="ltx_td ltx_align_left" style="padding:1pt 2.0pt;">Yi-Chat-8bits(GPTQ)</td>
<td id="S6.T4.1.1.10.2" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">6B</td>
<td id="S6.T4.1.1.10.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">58.3 / 61.0</td>
<td id="S6.T4.1.1.10.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">69.2 / 74.7</td>
<td id="S6.T4.1.1.10.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">69.2 / 73.9</td>
<td id="S6.T4.1.1.10.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">49.9</td>
<td id="S6.T4.1.1.10.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">40.4 / 47.3</td>
<td id="S6.T4.1.1.10.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">39.4 / 44.9</td>
</tr>
<tr id="S6.T4.1.1.11" class="ltx_tr">
<td id="S6.T4.1.1.11.1" class="ltx_td ltx_align_left" style="padding:1pt 2.0pt;">Yi-Chat-4bits(AWQ)</td>
<td id="S6.T4.1.1.11.2" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">6B</td>
<td id="S6.T4.1.1.11.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">56.8 / 59.9</td>
<td id="S6.T4.1.1.11.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">67.7 / 73.3</td>
<td id="S6.T4.1.1.11.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">67.5 / 72.3</td>
<td id="S6.T4.1.1.11.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">50.3</td>
<td id="S6.T4.1.1.11.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">37.7 / 43.6</td>
<td id="S6.T4.1.1.11.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">35.7 / 38.4</td>
</tr>
<tr id="S6.T4.1.1.12" class="ltx_tr">
<td id="S6.T4.1.1.12.1" class="ltx_td ltx_align_left ltx_border_t" style="padding:1pt 2.0pt;">Yi-Chat</td>
<td id="S6.T4.1.1.12.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">34B</td>
<td id="S6.T4.1.1.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.12.3.1" class="ltx_text ltx_font_bold">67.6</span> / 73.5</td>
<td id="S6.T4.1.1.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.12.4.1" class="ltx_text ltx_font_bold">79.1</span> / <span id="S6.T4.1.1.12.4.2" class="ltx_text ltx_font_bold">81.3</span>
</td>
<td id="S6.T4.1.1.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">77.0 / 78.5</td>
<td id="S6.T4.1.1.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">62.4</td>
<td id="S6.T4.1.1.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">51.4 / <span id="S6.T4.1.1.12.7.1" class="ltx_text ltx_font_bold">71.7</span>
</td>
<td id="S6.T4.1.1.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.12.8.1" class="ltx_text ltx_font_bold">71.7</span> / <span id="S6.T4.1.1.12.8.2" class="ltx_text ltx_font_bold">76.0</span>
</td>
</tr>
<tr id="S6.T4.1.1.13" class="ltx_tr">
<td id="S6.T4.1.1.13.1" class="ltx_td ltx_align_left" style="padding:1pt 2.0pt;">Yi-Chat-8bits(GPTQ)</td>
<td id="S6.T4.1.1.13.2" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">34B</td>
<td id="S6.T4.1.1.13.3" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">66.2 / <span id="S6.T4.1.1.13.3.1" class="ltx_text ltx_font_bold">73.7</span>
</td>
<td id="S6.T4.1.1.13.4" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">79.1 / 81.2</td>
<td id="S6.T4.1.1.13.5" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">76.8 / 79.0</td>
<td id="S6.T4.1.1.13.6" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">61.8</td>
<td id="S6.T4.1.1.13.7" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">
<span id="S6.T4.1.1.13.7.1" class="ltx_text ltx_font_bold">52.1</span> / 71.0</td>
<td id="S6.T4.1.1.13.8" class="ltx_td ltx_align_center" style="padding:1pt 2.0pt;">70.7 / 75.7</td>
</tr>
<tr id="S6.T4.1.1.14" class="ltx_tr">
<td id="S6.T4.1.1.14.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding:1pt 2.0pt;">Yi-Chat-4bits(AWQ)</td>
<td id="S6.T4.1.1.14.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">34B</td>
<td id="S6.T4.1.1.14.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">65.8 / 72.4</td>
<td id="S6.T4.1.1.14.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">78.2 / 80.5</td>
<td id="S6.T4.1.1.14.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">75.7 / 77.3</td>
<td id="S6.T4.1.1.14.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">61.8</td>
<td id="S6.T4.1.1.14.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">48.3 / 69.4</td>
<td id="S6.T4.1.1.14.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1pt 2.0pt;">70.5 / 74.0</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4:</span>오픈 소스 채팅 모델에 비해 자동 벤치마크에 대한 전반적인 성능. 4-비트 양자화는 메모리 요구량을 감소시키지만 모델 성능은 거의 떨어지지 않기 때문에 4-비트 양자화 결과를 강조한다.</figcaption>
This observation serve as the foundation of serving the model on consumer-grade devices, e.g., RTX4090.
</figcaption>
</figure>
<figure id="S6.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/x4.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="248" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 4:</span>Yi’s result of Hungarian mathematics exam.</figcaption>
</figcaption>
</figure>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Human Evaluations</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">이 절에서는 모델의 효과와 안전성을 보장하기 위해 측면을 고려하여 모델의 대화 능력에 대한 평가를 수행했다. 우리는 커뮤니티의 오픈 소스 평가 데이터 세트 모음인 alpaca-eval<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>, Belle-eval<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib88" title="">88</a>]</cite>, MT-bench<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib.bib93" title="">93</a>]</cite>를 컴파일했다. 또한 채팅 모델의 대화 능력을 종합적으로 평가하기 위해 다양한 난이도의 데이터를 수집하고 구성하여 유용하고 무해한 평가 데이터 세트를 구축했다.</p>
</div>
<div id="S6.SS2.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS2.SSS2.p2.1">그러나 공공 평가 집합이든 자체 구축 평가 집합이든 평가 결과는 평가 기준과 프롬프트의 설계에 의해 크게 영향을 받는다. 우리의 내부 평가 결과는 다른 모델에 불공평할 수 있어 우리 모델의 진정한 능력 수준을 정확하게 나타내기가 어렵다. 따라서 여기서는 채팅 모델의 현재 대화 능력을 입증하기 위해 외부 평가 결과만 제시한다. 우리는 다음을 고려한다: (1). AlapcaEval<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://tatsu-lab.github.io/alpaca_eval/</span></span></span>><cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib44" title="">44</a>]</cite>는 Win-rate를 계산하기 위해 Davinci003<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib21" title="">21</a>]</cite>의 참조 응답에 대한 지정된 모델의 응답을 비교하여 모델의 영어 회화 능력을 평가하도록 설계되어 있다; (2). LMSys<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib93" title="">93</a>]</cite> Chatbot Arena는 대화 플랫폼을 통해 서로 다른 모델의 응답을 보여주고, 사용자의 선호도에 따라 선택을 요청한 후 Elo 점수를 계산한다. (3). 반면에 SuperClue<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.superclueai.com/</span></span></span>은 모델의 중국어 능력을 종합적으로 평가하는 것을 목표로 하는 리더보드이다.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<div id="S6.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:311.8pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.3pt,4.5pt) scale(0.9,0.9) ;">
<table id="S6.T5.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S6.T5.1.1.1" class="ltx_tr">
<td id="S6.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S6.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Size</span></td>
<td id="S6.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">AlpacaEval</span></td>
<td id="S6.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.4.1" class="ltx_text ltx_font_bold">LMSys Chatbot Arena</span></td>
<td id="S6.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.1.5.1" class="ltx_text ltx_font_bold">SuperClue</span></td>
</tr>
<tr id="S6.T5.1.1.2" class="ltx_tr">
<td id="S6.T5.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">GPT-4-Turbo</td>
<td id="S6.T5.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T5.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.2.3.1" class="ltx_text ltx_font_bold">97.7</span></td>
<td id="S6.T5.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.2.4.1" class="ltx_text ltx_font_bold">1243</span></td>
<td id="S6.T5.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S6.T5.1.1.2.5.1" class="ltx_text ltx_font_bold">89.79</span></td>
</tr>
<tr id="S6.T5.1.1.3" class="ltx_tr">
<td id="S6.T5.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">GPT-3.5-Turbo</td>
<td id="S6.T5.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
<td id="S6.T5.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">89.37</td>
<td id="S6.T5.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">1117</td>
<td id="S6.T5.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">59.39</td>
</tr>
<tr id="S6.T5.1.1.4" class="ltx_tr">
<td id="S6.T5.1.1.4.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">LLaMA2-Chat</td>
<td id="S6.T5.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">70B</td>
<td id="S6.T5.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">92.66</td>
<td id="S6.T5.1.1.4.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">1077</td>
<td id="S6.T5.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">-</td>
</tr>
<tr id="S6.T5.1.1.5" class="ltx_tr">
<td id="S6.T5.1.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">Yi-Chat</td>
<td id="S6.T5.1.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">34B</td>
<td id="S6.T5.1.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">94.08</td>
<td id="S6.T5.1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">1110</td>
<td id="S6.T5.1.1.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">71.87</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span> 다른 오픈 소스 채팅 모델과의 인간 평가 비교.</figcaption>
</figure>
<div id="S6.SS2.SSS2.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS2.SSS2.p3.1">탭. <a class="ltx_ref" href="#S6.T5" title="Table 5 ‣ 6.2.2 Human Evaluations ‣ 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">5</span></a>는 우리가 고려하는 세 가지 타사 평가에서 Yi-34B-Chat의 성능 결과를 제시하며, 그 결과의 컷오프 날짜는 2023년 12월 21일이다. 데이터는 GPT-4에 비해 여전히 격차가 있지만, 우리 모델이 능숙한 이중 언어(중국어 및 영어) 대화 능력을 나타내고 사용자 선호도와 잘 일치한다는 것을 보여준다. 다양한 모델의 추가 비교 결과는 공식 웹사이트에서 검토하기 위해 액세스할 수 있다.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/x5.png" id="S6.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="151" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5:</span>SFT data scaling curve. 울트라챗 및 그 정제된 버전 울트라챗 200K와 비교하여, 우리의 SFT 데이터는 명확한 스케일링 이점을 보여준다. 우리는 가파른 경사를 데이터 품질 탓으로 돌립니다.</figcaption>
</figcaption>
</figure>
<div id="S6.SS2.SSS2.p4" class="ltx_para">
<p class="ltx_p" id="S6.SS2.SSS2.p4.1">또한 데이터 크기 조정 시 선호도 증가 속도를 비교하여 데이터 품질을 입증한다. 에 도시한 바와 같다. <a class="ltx_ref" href="#S6.F5" title="Figure 5 ‣ 6.2.2 Human Evaluations ‣ 6.2 Chat Model Performance ‣ 6 Evaluations ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">5</span></a>, UltraChat <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>]</cite> 및 그 세정된 버전 UltraChat 200K와 비교할 때, Yi 데이터를 스케일링할 때 성능 개선의 분명한 경향을 볼 수 있다.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Capability Extension</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p" id="S7.p1.1">이 절에서는 Yi 기반 모델을 200K의 긴 컨텍스트로 확장하고 시각적 이해 능력을 갖추고 깊이 향상을 통해 6B 모델을 향상시키는 사후 훈련 방법에 대해 논의한다.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Long Context Modeling</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS1.p1.1">우리의 긴 컨텍스트 솔루션은 지속적인 사전 훈련과 미세 조정 단계로 구성되며 둘 다 가볍다. 우리는 200K 입력 컨텍스트 내에서 정보를 활용할 수 있는 잠재력이 이미 기본 모델(<cite class="ltx_cite ltx_citemacro_citet">Fu et al. <a class="ltx_ref" href="#bib.bib22" title="">22</a></cite>와 동일)에 존재한다는 기본 가설을 세우고, 니들-인-어-헤이스택 테스트에서 강력한 성능으로 입증된 지속적인 사전 훈련 단계 "잠금 해제" 및 미세 조정 단계는 인간의 명령 및 선호도에 따라 응답 스타일을 추가로 적응시킨다.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.1">Continue Pretraining</span> We continue pretrain the full-attention model using sequence parallelism <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib43" title="">43</a>]</cite> and distributed attention. 즉, 우리는 희박하거나 선형적인 주의력을 사용하지 않고 완전한 주의력의 무차별적 실행을 사용한다. 우리는 (1)의 데이터 혼합물에 대해 Yi 6B/34B 기본 모델을 계속 사전 훈련한다. original pretraining data, as introduced in section <a class="ltx_ref" href="#S2" title="2 Pretraining ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">2</span></a>; (2). 길이-업샘플링된 긴 컨텍스트 데이터 - 긴 문서는 대부분 책에서 가져온 것입니다. (3). 다문서 질의응답 합성 데이터로서, 질의응답이 답변 전에 관련 단락의 암송을 포함하는 QA 쌍을 구성한다. 우리의 데이터 접근 방식은 대부분 <cite class="ltx_cite ltx_citemacro_citet">Fu et al. [<a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite>와 <cite class="ltx_cite ltx_citemacro_citet">Yu et al. [<a class="ltx_ref" href="#bib.bib87" title="">87</a>]</cite>의 데이터 엔지니어링 관행을 따른다. 우리는 4M 배치 크기를 가진 5B 토큰에 대해 모델을 계속 사전 훈련하며, 이는 100개의 최적화 단계로 변환된다. <cite class="ltx_cite ltx_citemacro_citet">Fu et al. [<a class="ltx_ref" href="#bib.bib22" title="">22</a>]</cite>의 동시 작업과 정렬하면 그림 <a class="ltx_ref" href="#S7.F6" title="Figure 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">6</span></a>에서 볼 수 있듯이 이러한 경량 연속 사전 훈련이 이미 Needle-in-a-Haystack 테스트에서 강력한 성능을 가능하게 할 수 있음을 관찰한다.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S7.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p3.1.1">Supervised Finetuning</span> We mix our short-context SFT data with long-context document question-answering data. 우리는 문서 QA를 구성하기 위해 모델 지원 자동화 방법(즉, 합성 데이터)을 사용한다. 구체적으로, 여러 문서를 무작위로 시퀀스로 연결하고 긴 시퀀스에서 하나 이상의 문단을 샘플링하고 채팅 모델을 요청하여 샘플링된 문단을 기반으로 질문 및 답변 쌍을 구성한다. 한 가지 중요한 세부 사항은 암송과 다시 말하기입니다: 답을 주기 전에, 우리는 모델이 원래 문단을 암송하거나 바꿔 말할 것을 요청합니다. 이 데이터 형식은 모델의 검색 행동을 장려하고 결과적으로 환각 행동을 억제한다: 질문이 주어지면 모델은 관련되지만 부정확할 수 있는 내부 지식을 사용하기보다는 입력 내의 정보를 사용하여 답변을 구성할 가능성이 더 높다. 미세 조정된 모델은 <a class="ltx_ref ltx_url ltx_font_typewriter" href="www.wanzhi01.com" title="">www.wanzhi01.com</a>에 배포되며 독자들이 이 모델을 사용해 볼 것을 권장합니다.</p>
</div>
<figure id="S7.F6" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/x6.png" id="S7.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="225" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6:</span>Needle-in-a-Haystack performance of Yi-34B-200K.</figcaption>
X-axis means length of the document, and Y-axis means the depth of the needle sentence within the document.
We continue pretrain the model on 5B tokens long-context data mixture and demonstrates a near-all-green performance.
</figcaption>
</figure>
<figure id="S7.T6" class="ltx_table">
<table id="S7.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S7.T6.1.1" class="ltx_tr">
<td id="S7.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Model</td>
<td id="S7.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.2.1" class="ltx_text ltx_font_bold">Average</span></td>
<td id="S7.T6.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.3.1" class="ltx_text ltx_font_bold">Humanity</span></td>
<td id="S7.T6.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.4.1" class="ltx_text ltx_font_bold">STEM</span></td>
<td id="S7.T6.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.5.1" class="ltx_text ltx_font_bold">Social Science</span></td>
<td id="S7.T6.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T6.1.1.6.1" class="ltx_text ltx_font_bold">Other</span></td>
</tr>
<tr id="S7.T6.1.2" class="ltx_tr">
<td id="S7.T6.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Yi-6B 4K</td>
<td id="S7.T6.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">63.24</td>
<td id="S7.T6.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">59.10</td>
<td id="S7.T6.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">53.15</td>
<td id="S7.T6.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">73.83</td>
<td id="S7.T6.1.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">69.26</td>
</tr>
<tr id="S7.T6.1.3" class="ltx_tr">
<td id="S7.T6.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Yi-6B 200K</td>
<td id="S7.T6.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">61.73</td>
<td id="S7.T6.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">56.17</td>
<td id="S7.T6.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">52.36</td>
<td id="S7.T6.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">72.54</td>
<td id="S7.T6.1.3.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">68.94</td>
</tr>
<tr id="S7.T6.1.4" class="ltx_tr">
<td id="S7.T6.1.4.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Yi-34B 4K</td>
<td id="S7.T6.1.4.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">76.32</td>
<td id="S7.T6.1.4.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">73.17</td>
<td id="S7.T6.1.4.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">68.03</td>
<td id="S7.T6.1.4.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">85.11</td>
<td id="S7.T6.1.4.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">80.78</td>
</tr>
<tr id="S7.T6.1.5" class="ltx_tr">
<td id="S7.T6.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">Yi-34B 200K</td>
<td id="S7.T6.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">75.56</td>
<td id="S7.T6.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">72.20</td>
<td id="S7.T6.1.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">66.83</td>
<td id="S7.T6.1.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">84.76</td>
<td id="S7.T6.1.5.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">80.40</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6:</span>Performance on MMLU after 200K adaptation. 컨텍스트 길이를 200K로 연장하는 것은 짧은 컨텍스트 능력을 크게 변화시키지 않는다.</figcaption>
</figcaption>
</figure>
<div id="S7.SS1.p4" class="ltx_para">
<p class="ltx_p" id="S7.SS1.p4.1">200K 모델의 성능은 그림에 나와 있습니다. <a class="ltx_ref" href="#S7.F6" title="Figure 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">6</span></a> 및 table <a class="ltx_ref" href="#S7.T6" title="Table 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">6</span></a>. 구체적으로, 그림 <a class="ltx_ref" href="#S7.F6" title="Figure 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">6</span></a>는 Yi-34B-200K의 유명한 Needle-in-a-Haystack 테스트를 보여주지만, 우리는 이 수준의 검색이 긴 컨텍스트 LLMs에 대해 비교적 쉽다고 보는 경향이 있다. Table <a class="ltx_ref" href="#S7.T6" title="Table 6 ‣ 7.1 Long Context Modeling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">6</span></a>는 문맥 스케일링이 short-context generic capability에 큰 영향을 미치지 않음을 보여준다.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Vision-Language</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS2.p1.1">급성장하는 멀티모달 연구 분야에서 이미지 이해 능력을 대규모 언어 모델에 통합하는 것은 점점 더 실행 가능해졌다. Open-sourced LLaVA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib47" title="">47</a>, <a class="ltx_ref" href="#bib.bib46" title="">46</a>]</cite>에서 영감을 얻어 Yi-VL(Yi Vision Language) 모델, <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1">i.e.</span>, Yi-VL-6B 및 Yi-VL-34B 언어 모델을 제시한다. 그림 <a class="ltx_ref" href="#S7.F7" title="Figure 7 ‣ 7.2 Vision-Language ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">7</span></a>에 예시된 바와 같이 Yi-VL 모델의 아키텍처는 세 개의 기본 모듈로 구성된다. 영상 부호화에 사용되는 Vision Transformer(ViT)는 CLIP ViT-H/14 모델<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib33" title="">33</a>]</cite>로 초기화된다. 이미지 피쳐를 텍스트 피쳐 spcae와 정렬하도록 설계된 투영 모듈은 레이어 정규화가 있는 2층 다층 퍼셉트론(MLP)으로 구성된다. 마지막으로, Yi-Chat 모델로 초기화된 대형 언어 모델은 영어와 중국어를 모두 이해하고 생성하는 탁월한 능력을 보여준다. 이중 언어 다중 모드 이해 및 생성에서 Yi-VL 모델의 성능을 향상시키기 위해 이중 언어 이미지-텍스트 쌍의 풍부한 데이터 세트를 활용한다.</p>
</div>
<figure id="S7.F7" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/x7.png" id="S7.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7:</span>Architecture of Yi-VL models. 기호는 세 가지 훈련 단계에서 다양한 모듈의 훈련 상태를 나타내는 데 사용됩니다. 화재 아이콘(<img alt="캡션 참조" class="ltx_graphics ltx_img_square" height="13" id="S7.F7.5.g1" src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/images/fire.png" width="14"/>)은 모듈의 매개 변수가 훈련 가능한 반면 눈송이 아이콘(<img alt="캡션 참조" class="ltx_graphics ltx_img_square" height="12" id="S7.F7.6.g2" src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/images/snow.png" width="13"/>)은 매개 변수가 동결되었음을 나타냅니다. 각 스테이지에서 ViT에 사용되는 이미지 해상도, <math alttext="224^{2}" class="ltx_Math" display="inline" id="S7.F7.7.m1.1"><semantics id="S7.F7.7.m1.1b"><msup id="S7.F7.7.m1.1.1" xref="S7.F7.7.m1.1.1.cmml"><mn id="S7.F7.7.m1.1.1.2" xref="S7.F7.7.m1.1.1.2.cmml">224</mn><mn id="S7.F7.7.m1.1.1.3" xref="S7.F7.7.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S7.F7.7.m1.1c"><apply id="S7.F7.7.m1.1.1.cmml" xref="S7.F7.7.m1.1.1"><csymbol cd="ambiguous" id="S7.F7.7.m1.1.1.1.cmml" xref="S7.F7.7.m1.1.1">superscript</csymbol><cn id="S7.F7.7.m1.1.1.2.cmml" type="integer" xref="S7.F7.7.m1.1.1.2">224</cn><cn id="S7.F7.7.m1.1.1.3.cmml" type="integer" xref="S7.F7.7.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.7.m1.1d">224^{2}</annotation></semantics></math> 또는 <math alttext="448^{2}" class="ltx_Math" display="inline" id="S7.F7.8.m2.1"><semantics id="S7.F7.8.m2.1b"><msup id="S7.F7.8.m2.1.1" xref="S7.F7.8.m2.1.1.cmml"><mn id="S7.F7.8.m2.1.1.2" xref="S7.F7.8.m2.1.1.2.cmml">448</mn><mn id="S7.F7.8.m2.1.1.3" xref="S7.F7.8.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S7.F7.8.m2.1c"><apply id="S7.F7.8.m2.1.1.cmml" xref="S7.F7.8.m2.1.1"><csymbol cd="ambiguous" id="S7.F7.8.m2.1.1.1.cmml" xref="S7.F7.8.m2.1.1">superscript</csymbol><cn id="S7.F7.8.m2.1.1.2.cmml" type="integer" xref="S7.F7.8.m2.1.1.2">448</cn><cn id="S7.F7.8.m2.1.1.3.cmml" type="integer" xref="S7.F7.8.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F7.8.m2.1d">448^{2}</annotation></semantics></math>도 마킹된다.</figcaption>
</figure>
<div id="S7.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS2.p2.1">Yi-VL 모델은 3단계 훈련 과정을 거친다:</p>
<dl id="S7.I1" class="ltx_description">
<dt id="S7.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S7.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Stage 1:</span></span></dt>
<dd class="ltx_item">
<div id="S7.I1.ix1.p1" class="ltx_para">
<p class="ltx_p" id="S7.I1.ix1.p1.2"><math alttext="224^{2}" class="ltx_Math" display="inline" id="S7.I1.ix1.p1.1.m1.1"><semantics id="S7.I1.ix1.p1.1.m1.1a"><msup id="S7.I1.ix1.p1.1.m1.1.1" xref="S7.I1.ix1.p1.1.m1.1.1.cmml"><mn id="S7.I1.ix1.p1.1.m1.1.1.2" xref="S7.I1.ix1.p1.1.m1.1.1.2.cmml">224</mn><mn id="S7.I1.ix1.p1.1.m1.1.1.3" xref="S7.I1.ix1.p1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S7.I1.ix1.p1.1.m1.1b"><apply id="S7.I1.ix1.p1.1.m1.1.1.cmml" xref="S7.I1.ix1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.I1.ix1.p1.1.m1.1.1.1.cmml" xref="S7.I1.ix1.p1.1.m1.1.1">superscript</csymbol><cn id="S7.I1.ix1.p1.1.m1.1.1.2.cmml" type="integer" xref="S7.I1.ix1.p1.1.m1.1.1.2">224</cn><cn id="S7.I1.ix1.p1.1.m1.1.1.3.cmml" type="integer" xref="S7.I1.ix1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix1.p1.1.m1.1c">224^{2}</annotation></semantics></math>의 이미지 해상도를 이용하여 ViT와 프로젝션 모듈의 파라미터를 학습한다. 학습은 LAION-400M <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib66" title="">66</a>]</cite>로부터 <math alttext="100" class="ltx_Math" display="inline" id="S7.I1.ix1.p1.2.m2.1"><semantics id="S7.I1.ix1.p1.2.m2.1a"><mn id="S7.I1.ix1.p1.2.m2.1.1" xref="S7.I1.ix1.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S7.I1.ix1.p1.2.m2.1b"><cn id="S7.I1.ix1.p1.2.m2.1.1.cmml" type="integer" xref="S7.I1.ix1.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix1.p1.2.m2.1c">100</annotation></semantics></math> 백만 개의 이미지-텍스트 쌍을 포함하는 실질적인 데이터세트를 활용한다. 주요 목표는 지정된 아키텍처 내에서 ViT의 지식 획득을 강화하고 ViT와 LLM 간의 더 나은 정렬을 달성하는 것이다.</p>
</div>
</dd>
<dt id="S7.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S7.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Stage 2:</span></span></dt>
<dd class="ltx_item">
<div id="S7.I1.ix2.p1" class="ltx_para">
<p class="ltx_p" id="S7.I1.ix2.p1.3">우리는 ViT의 이미지 해상도를 <math alttext="448^{2}" class="ltx_Math" display="inline" id="S7.I1.ix2.p1.1.m1.1"><semantics id="S7.I1.ix2.p1.1.m1.1a"><msup id="S7.I1.ix2.p1.1.m1.1.1" xref="S7.I1.ix2.p1.1.m1.1.1.cmml"><mn id="S7.I1.ix2.p1.1.m1.1.1.2" xref="S7.I1.ix2.p1.1.m1.1.1.2.cmml">448</mn><mn id="S7.I1.ix2.p1.1.m1.1.1.3" xref="S7.I1.ix2.p1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S7.I1.ix2.p1.1.m1.1b"><apply id="S7.I1.ix2.p1.1.m1.1.1.cmml" xref="S7.I1.ix2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.I1.ix2.p1.1.m1.1.1.1.cmml" xref="S7.I1.ix2.p1.1.m1.1.1">superscript</csymbol><cn id="S7.I1.ix2.p1.1.m1.1.1.2.cmml" type="integer" xref="S7.I1.ix2.p1.1.m1.1.1.2">448</cn><cn id="S7.I1.ix2.p1.1.m1.1.1.3.cmml" type="integer" xref="S7.I1.ix2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix2.p1.1.m1.1c">448^{2}</annotation></semantics></math>로 확장하여 복잡한 시각적 세부 정보를 식별할 수 있는 모델의 기능을 더욱 향상시키는 것을 목표로 한다. 이 단계에서 사용된 데이터셋은 LAION-400M에서 파생된 <math alttext="20" class="ltx_Math" display="inline" id="S7.I1.ix2.p1.2.m2.1"><semantics id="S7.I1.ix2.p1.2.m2.1a"><mn id="S7.I1.ix2.p1.2.m2.1.1" xref="S7.I1.ix2.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S7.I1.ix2.p1.2.m2.1b"><cn id="S7.I1.ix2.p1.2.m2.1.1.cmml" type="integer" xref="S7.I1.ix2.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix2.p1.2.m2.1c">20</annotation></semantics></math> 백만 개의 이미지-텍스트 쌍을 포함한다. 또한 다양한 소스의 <math alttext="4.8" class="ltx_Math" display="inline" id="S7.I1.ix2.p1.3.m3.1"><semantics id="S7.I1.ix2.p1.3.m3.1a"><mn id="S7.I1.ix2.p1.3.m3.1.1" xref="S7.I1.ix2.p1.3.m3.1.1.cmml">4.8</mn><annotation-xml encoding="MathML-Content" id="S7.I1.ix2.p1.3.m3.1b"><cn id="S7.I1.ix2.p1.3.m3.1.1.cmml" type="float" xref="S7.I1.ix2.p1.3.m3.1.1">4.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix2.p1.3.m3.1c">4.8</annotation></semantics></math> million image-text pairsn, <span class="ltx_text ltx_font_italic" id="S7.I1.ix2.p1.3.1">e.g.</span>, CLLaVA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib45" title="">45</a>]</cite>, LLaVAR <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib91" title="">91</a>]</cite>, Flickr <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib85" title="">85</a>]</cite>, VQAv2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib25" title="">25</a>]</cite>, RefCOCO <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib37" title="">37</a>]</cite>, Visual7w <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib95" title="">95</a>]</cite> 등을 통합한다.</p>
</div>
</dd>
<dt id="S7.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S7.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Stage 3:</span></span></dt>
<dd class="ltx_item">
<div id="S7.I1.ix3.p1" class="ltx_para">
<p class="ltx_p" id="S7.I1.ix3.p1.2">전체 모델의 파라미터가 트레이닝된다. 주요 목표는 멀티모달 채팅 상호작용에서 모델의 숙련도를 향상시켜 시각적 및 언어적 입력을 원활하게 통합하고 해석할 수 있는 능력을 부여하는 것이다. 이를 위해, 훈련 데이터 세트는 GQA<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib32" title="">32</a>]</cite>, VizWiz VQA<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib26" title="">26</a>]</cite>, TextCaps<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib71" title="">71</a>]</cite>, OCR-VQA<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib51" title="">51</a>]</cite>, Visual Genome<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib39" title="">39</a>]</cite>, ShareGPT4V<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib6" title="">6</a>]</cite> 등을 포함하여 대략 <math> 백만 개의 이미지-텍스트 쌍을 포괄한다. 데이터 균형을 보장하기 위해 단일 소스의 최대 데이터 기여도에 상한을 부과하여 <math alttext="50,000" class="ltx_Math" display="inline" id="S7.I1.ix3.p1.2.m2.2"><semantics id="S7.I1.ix3.p1.2.m2.2a"><mrow id="S7.I1.ix3.p1.2.m2.2.3.2" xref="S7.I1.ix3.p1.2.m2.2.3.1.cmml"><mn id="S7.I1.ix3.p1.2.m2.1.1" xref="S7.I1.ix3.p1.2.m2.1.1.cmml">50</mn><mo id="S7.I1.ix3.p1.2.m2.2.3.2.1" xref="S7.I1.ix3.p1.2.m2.2.3.1.cmml">,</mo><mn id="S7.I1.ix3.p1.2.m2.2.2" xref="S7.I1.ix3.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.I1.ix3.p1.2.m2.2b"><list id="S7.I1.ix3.p1.2.m2.2.3.1.cmml" xref="S7.I1.ix3.p1.2.m2.2.3.2"><cn id="S7.I1.ix3.p1.2.m2.1.1.cmml" type="integer" xref="S7.I1.ix3.p1.2.m2.1.1">50</cn><cn id="S7.I1.ix3.p1.2.m2.2.2.cmml" type="integer" xref="S7.I1.ix3.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.ix3.p1.2.m2.2c">50,000</annotation></semantics></math> 쌍 이하로 제한한다.</p>
</div>
</dd>
</dl>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.13" class="ltx_p">In Stage 1 and 2, we set the global batch size, the learning rate, the gradient clip and the number of epoch to <math id="S7.SS2.p3.1.m1.1" class="ltx_Math" alttext="4096" display="inline"><semantics id="S7.SS2.p3.1.m1.1a"><mn id="S7.SS2.p3.1.m1.1.1" xref="S7.SS2.p3.1.m1.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.1.m1.1b"><cn type="integer" id="S7.SS2.p3.1.m1.1.1.cmml" xref="S7.SS2.p3.1.m1.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.1.m1.1c">4096</annotation></semantics></math>, <math id="S7.SS2.p3.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.SS2.p3.2.m2.1a"><mn id="S7.SS2.p3.2.m2.1.1" xref="S7.SS2.p3.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.2.m2.1b"><cn type="integer" id="S7.SS2.p3.2.m2.1.1.cmml" xref="S7.SS2.p3.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.2.m2.1c">1</annotation></semantics></math>e<math id="S7.SS2.p3.3.m3.1" class="ltx_Math" alttext="-4" display="inline"><semantics id="S7.SS2.p3.3.m3.1a"><mrow id="S7.SS2.p3.3.m3.1.1" xref="S7.SS2.p3.3.m3.1.1.cmml"><mo id="S7.SS2.p3.3.m3.1.1a" xref="S7.SS2.p3.3.m3.1.1.cmml">−</mo><mn id="S7.SS2.p3.3.m3.1.1.2" xref="S7.SS2.p3.3.m3.1.1.2.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.3.m3.1b"><apply id="S7.SS2.p3.3.m3.1.1.cmml" xref="S7.SS2.p3.3.m3.1.1"><minus id="S7.SS2.p3.3.m3.1.1.1.cmml" xref="S7.SS2.p3.3.m3.1.1"></minus><cn type="integer" id="S7.SS2.p3.3.m3.1.1.2.cmml" xref="S7.SS2.p3.3.m3.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.3.m3.1c">-4</annotation></semantics></math>, <math id="S7.SS2.p3.4.m4.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S7.SS2.p3.4.m4.1a"><mn id="S7.SS2.p3.4.m4.1.1" xref="S7.SS2.p3.4.m4.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.4.m4.1b"><cn type="float" id="S7.SS2.p3.4.m4.1.1.cmml" xref="S7.SS2.p3.4.m4.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.4.m4.1c">0.5</annotation></semantics></math> and <math id="S7.SS2.p3.5.m5.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.SS2.p3.5.m5.1a"><mn id="S7.SS2.p3.5.m5.1.1" xref="S7.SS2.p3.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.5.m5.1b"><cn type="integer" id="S7.SS2.p3.5.m5.1.1.cmml" xref="S7.SS2.p3.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.5.m5.1c">1</annotation></semantics></math>, respectively. In Stage 3, these parameters are adjusted to <math id="S7.SS2.p3.6.m6.1" class="ltx_Math" alttext="256" display="inline"><semantics id="S7.SS2.p3.6.m6.1a"><mn id="S7.SS2.p3.6.m6.1.1" xref="S7.SS2.p3.6.m6.1.1.cmml">256</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.6.m6.1b"><cn type="integer" id="S7.SS2.p3.6.m6.1.1.cmml" xref="S7.SS2.p3.6.m6.1.1">256</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.6.m6.1c">256</annotation></semantics></math>, <math id="S7.SS2.p3.7.m7.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S7.SS2.p3.7.m7.1a"><mn id="S7.SS2.p3.7.m7.1.1" xref="S7.SS2.p3.7.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.7.m7.1b"><cn type="integer" id="S7.SS2.p3.7.m7.1.1.cmml" xref="S7.SS2.p3.7.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.7.m7.1c">2</annotation></semantics></math>e<math id="S7.SS2.p3.8.m8.1" class="ltx_Math" alttext="-5" display="inline"><semantics id="S7.SS2.p3.8.m8.1a"><mrow id="S7.SS2.p3.8.m8.1.1" xref="S7.SS2.p3.8.m8.1.1.cmml"><mo id="S7.SS2.p3.8.m8.1.1a" xref="S7.SS2.p3.8.m8.1.1.cmml">−</mo><mn id="S7.SS2.p3.8.m8.1.1.2" xref="S7.SS2.p3.8.m8.1.1.2.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.8.m8.1b"><apply id="S7.SS2.p3.8.m8.1.1.cmml" xref="S7.SS2.p3.8.m8.1.1"><minus id="S7.SS2.p3.8.m8.1.1.1.cmml" xref="S7.SS2.p3.8.m8.1.1"></minus><cn type="integer" id="S7.SS2.p3.8.m8.1.1.2.cmml" xref="S7.SS2.p3.8.m8.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.8.m8.1c">-5</annotation></semantics></math>, <math id="S7.SS2.p3.9.m9.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="S7.SS2.p3.9.m9.1a"><mn id="S7.SS2.p3.9.m9.1.1" xref="S7.SS2.p3.9.m9.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.9.m9.1b"><cn type="float" id="S7.SS2.p3.9.m9.1.1.cmml" xref="S7.SS2.p3.9.m9.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.9.m9.1c">1.0</annotation></semantics></math> and <math id="S7.SS2.p3.10.m10.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S7.SS2.p3.10.m10.1a"><mn id="S7.SS2.p3.10.m10.1.1" xref="S7.SS2.p3.10.m10.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.10.m10.1b"><cn type="integer" id="S7.SS2.p3.10.m10.1.1.cmml" xref="S7.SS2.p3.10.m10.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.10.m10.1c">2</annotation></semantics></math>. The training consumes <math id="S7.SS2.p3.11.m11.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S7.SS2.p3.11.m11.1a"><mn id="S7.SS2.p3.11.m11.1.1" xref="S7.SS2.p3.11.m11.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.11.m11.1b"><cn type="integer" id="S7.SS2.p3.11.m11.1.1.cmml" xref="S7.SS2.p3.11.m11.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.11.m11.1c">128</annotation></semantics></math> NVIDIA A100 GPUs. The total training time amounted to approximately <math id="S7.SS2.p3.12.m12.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S7.SS2.p3.12.m12.1a"><mn id="S7.SS2.p3.12.m12.1.1" xref="S7.SS2.p3.12.m12.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.12.m12.1b"><cn type="integer" id="S7.SS2.p3.12.m12.1.1.cmml" xref="S7.SS2.p3.12.m12.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.12.m12.1c">3</annotation></semantics></math> days for Yi-VL-6B and <math id="S7.SS2.p3.13.m13.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S7.SS2.p3.13.m13.1a"><mn id="S7.SS2.p3.13.m13.1.1" xref="S7.SS2.p3.13.m13.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S7.SS2.p3.13.m13.1b"><cn type="integer" id="S7.SS2.p3.13.m13.1.1.cmml" xref="S7.SS2.p3.13.m13.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p3.13.m13.1c">10</annotation></semantics></math> days for Yi-VL-34B.</p>
</div>
<figure id="S7.T7" class="ltx_table">
<table id="S7.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="S7.T7.1.1" class="ltx_tr">
<td id="S7.T7.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S7.T7.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.2.1" class="ltx_text ltx_font_bold">Overall</span></td>
<td id="S7.T7.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.3.1" class="ltx_text ltx_font_bold">Art</span></td>
<td id="S7.T7.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.4.1" class="ltx_text ltx_font_bold">Business</span></td>
<td id="S7.T7.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.5.1" class="ltx_text ltx_font_bold">Science</span></td>
<td id="S7.T7.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.6.1" class="ltx_text ltx_font_bold">Health</span></td>
<td id="S7.T7.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.7.1" class="ltx_text ltx_font_bold">Society</span></td>
<td id="S7.T7.1.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T7.1.1.8.1" class="ltx_text ltx_font_bold">Engineering</span></td>
</tr>
<tr id="S7.T7.1.2" class="ltx_tr">
<td id="S7.T7.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">GPT-4V</td>
<td id="S7.T7.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">55.7</td>
<td id="S7.T7.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">65.3</td>
<td id="S7.T7.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">64.3</td>
<td id="S7.T7.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">48.4</td>
<td id="S7.T7.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">63.5</td>
<td id="S7.T7.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">76.3</td>
<td id="S7.T7.1.2.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">41.7</td>
</tr>
<tr id="S7.T7.1.3" class="ltx_tr">
<td id="S7.T7.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Yi-VL-34B</td>
<td id="S7.T7.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">41.6</td>
<td id="S7.T7.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">56.1</td>
<td id="S7.T7.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.3</td>
<td id="S7.T7.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.9</td>
<td id="S7.T7.1.3.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">45.9</td>
<td id="S7.T7.1.3.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">66.5</td>
<td id="S7.T7.1.3.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">36.0</td>
</tr>
<tr id="S7.T7.1.4" class="ltx_tr">
<td id="S7.T7.1.4.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Qwen-VL-PLUS</td>
<td id="S7.T7.1.4.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">40.8</td>
<td id="S7.T7.1.4.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">59.9</td>
<td id="S7.T7.1.4.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.5</td>
<td id="S7.T7.1.4.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.8</td>
<td id="S7.T7.1.4.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">43.7</td>
<td id="S7.T7.1.4.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">65.5</td>
<td id="S7.T7.1.4.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.9</td>
</tr>
<tr id="S7.T7.1.5" class="ltx_tr">
<td id="S7.T7.1.5.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Marco-VL</td>
<td id="S7.T7.1.5.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">40.4</td>
<td id="S7.T7.1.5.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">56.5</td>
<td id="S7.T7.1.5.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.0</td>
<td id="S7.T7.1.5.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.0</td>
<td id="S7.T7.1.5.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.9</td>
<td id="S7.T7.1.5.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">66.5</td>
<td id="S7.T7.1.5.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.8</td>
</tr>
<tr id="S7.T7.1.6" class="ltx_tr">
<td id="S7.T7.1.6.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Yi-VL-6B</td>
<td id="S7.T7.1.6.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">37.8</td>
<td id="S7.T7.1.6.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">53.4</td>
<td id="S7.T7.1.6.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.3</td>
<td id="S7.T7.1.6.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.0</td>
<td id="S7.T7.1.6.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">39.3</td>
<td id="S7.T7.1.6.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">58.5</td>
<td id="S7.T7.1.6.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.1</td>
</tr>
<tr id="S7.T7.1.7" class="ltx_tr">
<td id="S7.T7.1.7.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">InfMIM-Zephyr-7B</td>
<td id="S7.T7.1.7.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">35.5</td>
<td id="S7.T7.1.7.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.0</td>
<td id="S7.T7.1.7.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.6</td>
<td id="S7.T7.1.7.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.2</td>
<td id="S7.T7.1.7.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">37.5</td>
<td id="S7.T7.1.7.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">54.6</td>
<td id="S7.T7.1.7.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.1</td>
</tr>
<tr id="S7.T7.1.8" class="ltx_tr">
<td id="S7.T7.1.8.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">SVIT</td>
<td id="S7.T7.1.8.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.1</td>
<td id="S7.T7.1.8.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.9</td>
<td id="S7.T7.1.8.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.0</td>
<td id="S7.T7.1.8.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">26.8</td>
<td id="S7.T7.1.8.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">35.5</td>
<td id="S7.T7.1.8.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.9</td>
<td id="S7.T7.1.8.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.7</td>
</tr>
<tr id="S7.T7.1.9" class="ltx_tr">
<td id="S7.T7.1.9.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Emu2-Chat</td>
<td id="S7.T7.1.9.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.1</td>
<td id="S7.T7.1.9.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.6</td>
<td id="S7.T7.1.9.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.7</td>
<td id="S7.T7.1.9.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.0</td>
<td id="S7.T7.1.9.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.4</td>
<td id="S7.T7.1.9.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.3</td>
<td id="S7.T7.1.9.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.3</td>
</tr>
<tr id="S7.T7.1.10" class="ltx_tr">
<td id="S7.T7.1.10.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">BLIP-2 FLAN-T5-XXL</td>
<td id="S7.T7.1.10.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.0</td>
<td id="S7.T7.1.10.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">49.2</td>
<td id="S7.T7.1.10.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.6</td>
<td id="S7.T7.1.10.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.3</td>
<td id="S7.T7.1.10.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.7</td>
<td id="S7.T7.1.10.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">51.5</td>
<td id="S7.T7.1.10.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.4</td>
</tr>
<tr id="S7.T7.1.11" class="ltx_tr">
<td id="S7.T7.1.11.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">InstructBLIP-T5-XXL</td>
<td id="S7.T7.1.11.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.8</td>
<td id="S7.T7.1.11.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.5</td>
<td id="S7.T7.1.11.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.6</td>
<td id="S7.T7.1.11.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.6</td>
<td id="S7.T7.1.11.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.6</td>
<td id="S7.T7.1.11.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">49.8</td>
<td id="S7.T7.1.11.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.4</td>
</tr>
<tr id="S7.T7.1.12" class="ltx_tr">
<td id="S7.T7.1.12.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">LLaVA-1.5-13B</td>
<td id="S7.T7.1.12.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.6</td>
<td id="S7.T7.1.12.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">49.8</td>
<td id="S7.T7.1.12.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.2</td>
<td id="S7.T7.1.12.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.9</td>
<td id="S7.T7.1.12.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.9</td>
<td id="S7.T7.1.12.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">54.7</td>
<td id="S7.T7.1.12.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.3</td>
</tr>
<tr id="S7.T7.1.13" class="ltx_tr">
<td id="S7.T7.1.13.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">Qwen-VL-7B-Chat</td>
<td id="S7.T7.1.13.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.9</td>
<td id="S7.T7.1.13.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">47.7</td>
<td id="S7.T7.1.13.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.8</td>
<td id="S7.T7.1.13.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.6</td>
<td id="S7.T7.1.13.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">33.6</td>
<td id="S7.T7.1.13.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">45.3</td>
<td id="S7.T7.1.13.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.2</td>
</tr>
<tr id="S7.T7.1.14" class="ltx_tr">
<td id="S7.T7.1.14.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">SPHINX*</td>
<td id="S7.T7.1.14.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.9</td>
<td id="S7.T7.1.14.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">50.9</td>
<td id="S7.T7.1.14.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.2</td>
<td id="S7.T7.1.14.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.3</td>
<td id="S7.T7.1.14.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">34.1</td>
<td id="S7.T7.1.14.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">51.2</td>
<td id="S7.T7.1.14.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.8</td>
</tr>
<tr id="S7.T7.1.15" class="ltx_tr">
<td id="S7.T7.1.15.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">mPLUG-OWL2</td>
<td id="S7.T7.1.15.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.1</td>
<td id="S7.T7.1.15.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.5</td>
<td id="S7.T7.1.15.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.6</td>
<td id="S7.T7.1.15.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">24.9</td>
<td id="S7.T7.1.15.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32.8</td>
<td id="S7.T7.1.15.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">46.7</td>
<td id="S7.T7.1.15.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.6</td>
</tr>
<tr id="S7.T7.1.16" class="ltx_tr">
<td id="S7.T7.1.16.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">BLIP-2 FLAN-T5-XL</td>
<td id="S7.T7.1.16.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.0</td>
<td id="S7.T7.1.16.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">43.0</td>
<td id="S7.T7.1.16.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.6</td>
<td id="S7.T7.1.16.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.1</td>
<td id="S7.T7.1.16.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.8</td>
<td id="S7.T7.1.16.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">48.0</td>
<td id="S7.T7.1.16.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">27.8</td>
</tr>
<tr id="S7.T7.1.17" class="ltx_tr">
<td id="S7.T7.1.17.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">InstructBLIP-T5-XL</td>
<td id="S7.T7.1.17.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">30.6</td>
<td id="S7.T7.1.17.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">43.3</td>
<td id="S7.T7.1.17.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.2</td>
<td id="S7.T7.1.17.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.2</td>
<td id="S7.T7.1.17.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">29.3</td>
<td id="S7.T7.1.17.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">45.8</td>
<td id="S7.T7.1.17.8" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">28.6</td>
</tr>
<tr id="S7.T7.1.18" class="ltx_tr">
<td id="S7.T7.1.18.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">CogVLM</td>
<td id="S7.T7.1.18.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">30.1</td>
<td id="S7.T7.1.18.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">38.0</td>
<td id="S7.T7.1.18.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">25.6</td>
<td id="S7.T7.1.18.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">25.1</td>
<td id="S7.T7.1.18.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">31.2</td>
<td id="S7.T7.1.18.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">41.5</td>
<td id="S7.T7.1.18.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">28.9</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 7:</span>MMMU 테스트 세트</figcaption>
performance by the time of Yi-VL’s release.</figcaption>
</figure>
<div id="S7.SS2.p4" class="ltx_para">
<p class="ltx_p" id="S7.SS2.p4.1">Table <a class="ltx_ref" href="#S7.T7" title="Table 7 ‣ 7.2 Vision-Language ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">7</span></a>는 Yi-VL의 릴리스에 의한 MMMU 테스트 세트 리더보드를 보여준다. 우리는 이 지역이 현재 활발히 연구 중이며 지역 사회의 발전에 맞춰 Yi-VL의 업데이트를 지속적으로 개선할 것이라는 점에 주목한다.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Depth Upscaling</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S7.SS3.p1.1">스케일링 법칙 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib29" title="">29</a>, <a class="ltx_ref" href="#bib.bib30" title="">30</a>, <a class="ltx_ref" href="#bib.bib36" title="">36</a>]</cite>에 대한 최근 연구는 계산 예산, 모델 크기 및 데이터 크기가 증가함에 따라 모델 성능의 예측 가능한 개선을 강조했다. 그러나 계산 예산을 확장할 때 모델과 데이터 크기 사이의 가장 효과적인 자원 분포를 식별하는 것은 스케일링 법률 분야에서 여전히 만만치 않은 과제로 남아 있다. 또한, <cite class="ltx_cite ltx_citemacro_citet">DeepSeek-AI et al. [<a class="ltx_ref" href="#bib.bib17" title="">17</a>]</cite>에 의해 수행된 연구는 모델 스케일링을 위한 증가된 계산 예산의 할당이 사용 가능한 데이터의 품질에 비례해야 함을 강조했다. 이러한 통찰력에 비추어, 우리는 일련의 단계적 훈련 프로세스를 통해 데이터와 모델 크기 사이의 자원 할당을 동적으로 조정하는 것을 목표로 하는 새로운 접근법을 제안한다. 이 전략은 스케일링 법칙에 따라 데이터 특성과 모델 크기 사이의 균형을 반복적으로 미세 조정함으로써 모델 훈련 효율성과 성능을 모두 향상시킨다.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S7.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.p2.1.1">Method</span> <cite class="ltx_cite ltx_citemacro_citet">Kim et al. [<a class="ltx_ref" href="#bib.bib38" title="">38</a>]</cite>에 의해 요약된 방법론에 따라, 우리의 목표는 32개의 레이어를 갖는 우리의 Yi-6B 기본 모델을 원래의 16개의 중간 레이어 12-28을 복제함으로써 48개의 레이어를 특징으로 하는 Yi-9B 기본 모델이라는 9B 모델로 업스케일링하는 것이다. 깊이 업스케일링은 기본 모델의 깊이를 확장하고 후속적으로 향상된 모델에 대한 프리트레이닝 단계를 계속하는 것을 포함한다.</p>
</div>
<figure id="S7.F8" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2403.04652/assets/x8.png" id="S7.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 8: </span> 텍스트 "Write a quiz about bits"에 대한 레이어별 각 토큰의 입출력 코사인 유사도 점수. 아래 그림에 표시된 16개의 새로 추가된 레이어(레이어 28-44)의 코사인 유사성 점수는 거의 1로 관찰된다.</figcaption>
</figure>
<div id="S7.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S7.SS3.p3.1">우리의 조사는 복제할 계층에 대한 결정이 각 계층의 입력과 출력 사이의 코사인 유사성 점수를 평가하여 알려질 수 있음을 보여준다. 이러한 접근법은 추가 사전 훈련을 필요로 하지 않고 표적화된 모델 스케일링을 허용하여 최소한의 성능 영향만 초래한다. 성능에 대한 이러한 최소한의 영향은 그림 <a class="ltx_ref" href="#S7.F8" title="Figure 8 ‣ 7.3 Depth Upscaling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">8</span></a>에서 알 수 있듯이 복제된 레이어의 입력과 출력 사이에 높은 코사인 유사성이 접근하기 때문이다. 이 관찰은 이러한 계층의 복제가 원래 모델에 의해 생성된 출력 로짓을 크게 변경하지 않는다는 것을 시사한다. 이 방법은 레이어의 내부 처리 역학에 기초하여 아키텍처를 최적화함으로써 모델의 효율적인 스케일링을 보장한다.</p>
</div>
<figure id="S7.T8" class="ltx_table">
<div id="S7.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:367.8pt;height:58.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.0pt,7.2pt) scale(0.8,0.8) ;">
<table id="S7.T8.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S7.T8.1.1.1" class="ltx_tr">
<td id="S7.T8.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S7.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">Arc-C</span></td>
<td id="S7.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">HellaSwag</span></td>
<td id="S7.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">MMLU</span></td>
<td id="S7.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.5.1" class="ltx_text ltx_font_bold">Winogrande</span></td>
<td id="S7.T8.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.6.1" class="ltx_text ltx_font_bold">GSM8K</span></td>
<td id="S7.T8.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.7.1" class="ltx_text ltx_font_bold">MATH</span></td>
<td id="S7.T8.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.8.1" class="ltx_text ltx_font_bold">HumanEval</span></td>
<td id="S7.T8.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.1.9.1" class="ltx_text ltx_font_bold">MBPP</span></td>
</tr>
<tr id="S7.T8.1.1.2" class="ltx_tr">
<td id="S7.T8.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.2.1.1" class="ltx_text ltx_font_bold">Yi-6B</span></td>
<td id="S7.T8.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">50.3</td>
<td id="S7.T8.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">74.4</td>
<td id="S7.T8.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">63.2</td>
<td id="S7.T8.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">71.3</td>
<td id="S7.T8.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">32.5</td>
<td id="S7.T8.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4.6</td>
<td id="S7.T8.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">15.9</td>
<td id="S7.T8.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">26.3</td>
</tr>
<tr id="S7.T8.1.1.3" class="ltx_tr">
<td id="S7.T8.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.3.1.1" class="ltx_text ltx_font_bold">Yi-9B Init</span></td>
<td id="S7.T8.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">52.1</td>
<td id="S7.T8.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">73.3</td>
<td id="S7.T8.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">63.0</td>
<td id="S7.T8.1.1.3.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">69.4</td>
<td id="S7.T8.1.1.3.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">31.3</td>
<td id="S7.T8.1.1.3.7" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">4.1</td>
<td id="S7.T8.1.1.3.8" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">12.8</td>
<td id="S7.T8.1.1.3.9" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">25.8</td>
</tr>
<tr id="S7.T8.1.1.4" class="ltx_tr">
<td id="S7.T8.1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.1.1" class="ltx_text ltx_font_bold">Yi-9B</span></td>
<td id="S7.T8.1.1.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.2.1" class="ltx_text ltx_font_bold">55.6</span></td>
<td id="S7.T8.1.1.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.3.1" class="ltx_text ltx_font_bold">76.4</span></td>
<td id="S7.T8.1.1.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.4.1" class="ltx_text ltx_font_bold">68.4</span></td>
<td id="S7.T8.1.1.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.5.1" class="ltx_text ltx_font_bold">73.0</span></td>
<td id="S7.T8.1.1.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.6.1" class="ltx_text ltx_font_bold">52.3</span></td>
<td id="S7.T8.1.1.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.7.1" class="ltx_text ltx_font_bold">15.9</span></td>
<td id="S7.T8.1.1.4.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.8.1" class="ltx_text ltx_font_bold">39.0</span></td>
<td id="S7.T8.1.1.4.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S7.T8.1.1.4.9.1" class="ltx_text ltx_font_bold">54.4</span></td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8:</span>Performance between Yi-6B: Arc Challenge (25-shot), HellaSwag (10-shot) MMLU (5-shot), Winogrande (5-shot), GSM8K (5-shot), MATH (4-shot), HumanEval pass@1, MBPP pass@1(3-shot). Yi-9B Init는 추가 훈련 없이 층 12-28을 복제함으로써 Yi-6B로부터 깊이 방향으로 업스케일링하고 있다.</figcaption>
</figure>
<div id="S7.SS3.p4" class="ltx_para">
<p class="ltx_p" id="S7.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.p4.1.1">Continual Training</span> 데이터세트는 두 단계에 걸쳐 약 8,000억 개의 토큰으로 구성되며, 약 70%가 최근에 수집되고 신중하게 선택되었다. 최종 단계에서 코드 커버리지를 향상시켜 코드 성능을 향상시켰습니다.</p>
</div>
<div id="S7.SS3.p5" class="ltx_para">
<p class="ltx_p" id="S7.SS3.p5.1">학습 프로세스를 최적화하기 위해 3e-5의 일정한 학습 속도를 유지하고 모델의 손실이 안정될 때마다 4M 토큰에서 배치 크기를 점진적으로 증가시키는 전략적 접근법을 채택한다. 배치 크기의 이러한 점진적인 조정은 확립된 Yi-6B 기본 모델 구성과 정렬하여 다른 모든 매개변수를 유지하는 것과 함께 규모에서 훈련의 문제를 해결하는 데 중요한 역할을 했다.</p>
</div>
<div id="S7.SS3.p6" class="ltx_para">
<p class="ltx_p" id="S7.SS3.p6.1">이러한 전략의 효과는 상식, 추론, 지식, 코딩 및 수학을 포함한 다양한 벤치마크에 걸쳐 Yi-9B 기본 모델의 성능을 자세히 설명하는 표 <a class="ltx_ref" href="#S7.T8" title="Table 8 ‣ 7.3 Depth Upscaling ‣ 7 Capability Extension ‣ Yi: Open Foundation Models by 01.AI"><span class="ltx_text ltx_ref_tag">8</span></a>에서 입증된다. 이는 특정 도메인에서 Yi-9B 기반 모델의 경쟁 우위를 강조하며 데이터 특성과 모델 크기 간의 상호 작용을 최적으로 조정하여 모델 성능을 향상시키는 방법론의 효율성을 보여준다.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Final Discussions</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p" id="S8.p1.1">이 보고서에서는 이 언어 모델 패밀리의 전체 스택 개발에 대해 논의한다. Yi-34B는 GPT-3.5 매칭 성능을 달성하고 소비자 등급 장치에 배포(4/8 비트 양자화 덕분에) 가능하므로 로컬 배포에 이상적인 모델이다.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p class="ltx_p" id="S8.p2.1">Lee 사전 훈련 절차의 주요 테이크아웃은 데이터 양과 질에 관한 것이다: (1). 친칠라 최적보다 더 많은 양의 데이터에서 모델을 훈련하면 명확하고 일관된 성능 이득을 얻을 수 있으므로 모든 사전 훈련 팀에 적극 권장합니다. 우리의 모델은 3.1T 토큰에 대해 훈련되었지만, 더 많은 양의 데이터로 인해 모델 성능을 계속 향상시킬 수 있다(즉, 모델이 3.1T에서 포화되지 않음); (2). 사전 훈련 데이터 품질과 관련하여 가장 중요한 두 가지 요소는 데이터의 출처(예: 텍스트가 전문적인 사용을 위해 생성되었는지 또는 비공식 소셜 미디어 게시인지)와 데이터 청소 세부 정보(예: 필터링 및 중복 제거의 강도)라고 믿는다. 데이터 청소는 매우 복잡한 파이프라인이며 광범위한 그리드 검색 스타일 최적화를 수행하는 것이 매우 어렵기 때문에 현재 솔루션은 여전히 개선의 여지가 있을 수 있다.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p class="ltx_p" id="S8.p3.1">Lee finetuning 과정을 통해 얻은 주요 결과는 적은 양의 데이터(<math alttext="\leq" class="ltx_Math" display="inline" id="S8.p3.1.m1.1"><semantics id="S8.p3.1.m1.1a"><mo id="S8.p3.1.m1.1.1" xref="S8.p3.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S8.p3.1.m1.1b"><leq id="S8.p3.1.m1.1.1.cmml" xref="S8.p3.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S8.p3.1.m1.1c">\leq</annotation></semantics></math>10K)에서, 경우에 따라, 여러 번의 반복을 통해, 기계 학습 엔지니어에 의해 직접, 실제 사용자 피드백으로부터 개선되는 것이다. 이 접근법은 처음에 FLAN 시리즈 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib9" title="">9</a>]</cite>에 의해 도입된 다음 UltraChat 시리즈 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="#bib.bib19" title="">19</a>]</cite>에 의해 도입된 명령어 크기 조정 접근법에서 분명히 벗어난다.</p>
</div>
<div id="S8.p4" class="ltx_para">
<p class="ltx_p" id="S8.p4.1">본 논문의 결과를 통해 알 수 있듯이, 언어 모델의 실제 배치를 위한 핵심 능력으로 간주되는 추론 능력은 사전 훈련 데이터의 양이 고정될 때 모델 규모와 강한 상관 관계가 있다. 현재 결과를 감안할 때 철저히 최적화된 데이터를 사용하여 모델 매개변수를 계속 확장하면 다음 버전에서 더 강력한 프론티어 모델로 이어질 것이라고 믿습니다.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Author List and Contributions</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p" id="A1.p1.1">우리 팀원들은 다음과 같은 관점에서 Yi의 발전에 기여한다.</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="A1.p2" class="ltx_para">
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i1.p1.1">프론티어 연구소</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i2.p1.1">머신 러닝 인프라</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i3.p1.1">사전 훈련</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i4.p1.1">파인튜닝과 AI 정렬</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i5.p1.1">멀티모달</p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i6.p1.1">안전하고 책임감 있는 AI</p>
</div>
</li>
<li id="A1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i7.p1" class="ltx_para">
<p class="ltx_p" id="A1.I1.i7.p1.1">배포</p>
</div>
</li>
</ul>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<div id="A1.p3" class="ltx_para">
<p class="ltx_p" id="A1.p3.1">우리는 팀원을 알파벳 순서로 나열합니다. 모든 저자들이 이 작품에 동등하게 기여했다.</p>
</div>
<div class="ltx_pagination ltx_role_start_2_columns"></div>
<div id="A1.p4" class="ltx_para">
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i1.p1.1">알렉스 영</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i2.p1.1">배천</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i3.p1.1">차오리</p>
</div>
</li>
<li id="A1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i4.p1.1">칭겐황</p>
</div>
</li>
<li id="A1.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i5.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i5.p1.1">게장</p>
</div>
</li>
<li id="A1.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i6.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i6.p1.1">장관웨이</p>
</div>
</li>
<li id="A1.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i7.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i7.p1.1">헝리</p>
</div>
</li>
<li id="A1.I2.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i8.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i8.p1.1">주강청</p>
</div>
</li>
<li id="A1.I2.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i9.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i9.p1.1">천젠춘</p>
</div>
</li>
<li id="A1.I2.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i10.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i10.p1.1">정창</p>
</div>
</li>
<li id="A1.I2.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i11.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i11.p1.1">유가동</p>
</div>
</li>
<li id="A1.I2.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i12.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i12.p1.1">펑류</p>
</div>
</li>
<li id="A1.I2.i13" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i13.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i13.p1.1">류창</p>
</div>
</li>
<li id="A1.I2.i14" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i14.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i14.p1.1">숀웨</p>
</div>
</li>
<li id="A1.I2.i15" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i15.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i15.p1.1">양선빈</p>
</div>
</li>
<li id="A1.I2.i16" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i16.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i16.p1.1">심양</p>
</div>
</li>
<li id="A1.I2.i17" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i17.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i17.p1.1">타오유</p>
</div>
</li>
<li id="A1.I2.i18" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i18.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i18.p1.1">"문시"</p>
</div>
</li>
<li id="A1.I2.i19" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i19.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i19.p1.1">원호황</p>
</div>
</li>
<li id="A1.I2.i20" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i20.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i20.p1.1">허소희</p>
</div>
</li>
<li id="A1.I2.i21" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i21.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i21.p1.1">샤오이런</p>
</div>
</li>
<li id="A1.I2.i22" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i22.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i22.p1.1">신야오니우</p>
</div>
</li>
<li id="A1.I2.i23" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i23.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i23.p1.1">펑청니</p>
</div>
</li>
<li id="A1.I2.i24" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i24.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i24.p1.1">서유치</p>
</div>
</li>
<li id="A1.I2.i25" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i25.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i25.p1.1">류동유</p>
</div>
</li>
<li id="A1.I2.i26" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i26.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i26.p1.1">유왕</p>
</div>
</li>
<li id="A1.I2.i27" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i27.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i27.p1.1">위쑤안 차이</p>
</div>
</li>
<li id="A1.I2.i28" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i28.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i28.p1.1">구전유</p>
</div>
</li>
<li id="A1.I2.i29" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i29.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i29.p1.1">류즈위안</p>
</div>
</li>
<li id="A1.I2.i30" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i30.p1" class="ltx_para">
<p class="ltx_p" id="A1.I2.i30.p1.1">종홍대</p>
</div>
</li>
</ul>
<div class="ltx_pagination ltx_role_end_2_columns"></div>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ainslie et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Joshua Ainslie, James Lee-Thorp, Michiel de&nbsp;Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.

</span>
<span class="ltx_bibblock">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.13245</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et&nbsp;al.

</span>
<span class="ltx_bibblock">Program Synthesis With lLarge Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07732</em>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An&nbsp;Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.

</span>
<span class="ltx_bibblock">Qwen Technical Report.

</span>
<span class="ltx_bibblock">09 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/pdf/2309.16609.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/2309.16609.pdf</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">PIQA: Reasoning about Physical Commonsense in Natural Language.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1911.11641, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:208290939" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:208290939</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.

</span>
<span class="ltx_bibblock">Sharegpt4v: Improving large multi-modal models with better captions.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.12793</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique&nbsp;Pondé de&nbsp;Oliveira&nbsp;Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe&nbsp;Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William&nbsp;Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew&nbsp;N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating Large Language Models Trained on Code.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2107.03374, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2107.03374" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2107.03374</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Eunsol Choi, He&nbsp;He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">QuAC : Question Answering in Context, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, Shayne Longpre, Barret Zoph, Yi&nbsp;Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et&nbsp;al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.11416</em>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training Verifiers to Solve Math Word Problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Computer [2023]</span>
<span class="ltx_bibblock">
Together Computer.

</span>
<span class="ltx_bibblock">Redpajama: an open dataset for training large language models, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/togethercomputer/RedPajama-Data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/togethercomputer/RedPajama-Data</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao [2023]</span>
<span class="ltx_bibblock">
Tri Dao.

</span>
<span class="ltx_bibblock">FlashAttention-2: Faster attention with better parallelism and work partitioning.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Tri Dao, Daniel&nbsp;Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">FlashAttention: Fast and memory-efficient exact attention with IO-awareness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de&nbsp;Jong et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Michiel de&nbsp;Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and William Cohen.

</span>
<span class="ltx_bibblock">FiDO: Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.08153</em>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.&nbsp;K. Li, Wenfeng Liang, Fangyun Lin, A.&nbsp;X. Liu, Bo&nbsp;Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y.&nbsp;Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.&nbsp;X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B.&nbsp;Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng
Zhou, Qihao Zhu, and Yuheng Zou.

</span>
<span class="ltx_bibblock">Deepseek llm: Scaling open-source language models with longtermism.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.

</span>
<span class="ltx_bibblock">Enhancing chat language models by scaling high-quality instructional conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14233</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">How abilities in large language models are affected by supervised fine-tuning data composition, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng.

</span>
<span class="ltx_bibblock">Data engineering for scaling language models to 128k context.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.10171</em>, 2024.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini&nbsp;Team et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Gemini Gemini&nbsp;Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glaese et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et&nbsp;al.

</span>
<span class="ltx_bibblock">Improving alignment of dialogue agents via targeted human judgements.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.14375</em>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et&nbsp;al. [2017]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding in visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 6904–6913, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale&nbsp;J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey&nbsp;P Bigham.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 3608–3617, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2009.03300, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2009.03300" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2009.03300</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring Mathematical Problem Solving With the MATH Dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.03874</em>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Henighan et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom&nbsp;B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel&nbsp;M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish.

</span>
<span class="ltx_bibblock">Scaling laws for autoregressive generative modeling.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et&nbsp;al.

</span>
<span class="ltx_bibblock">C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.08322</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning [2019]</span>
<span class="ltx_bibblock">
Drew&nbsp;A Hudson and Christopher&nbsp;D Manning.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 6700–6709, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ilharco et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Openclip, July 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.5281/zenodo.5143773" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.5143773</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian&nbsp;R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et&nbsp;al.

</span>
<span class="ltx_bibblock">Neftune: Noisy embeddings improve instruction finetuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.05914</em>, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce&nbsp;Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang.

</span>
<span class="ltx_bibblock">Beavertails: Towards improved safety alignment of llm via a human-preference dataset, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemzadeh et&nbsp;al. [2014]</span>
<span class="ltx_bibblock">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.

</span>
<span class="ltx_bibblock">Referitgame: Referring to objects in photographs of natural scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em>, pages 787–798, 2014.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim.

</span>
<span class="ltx_bibblock">Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et&nbsp;al. [2017]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David&nbsp;A Shamma, et&nbsp;al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123:32–73, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson [2018]</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.06226</em>, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient Memory Management for Large Language Model Serving with PagedAttention.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.06180</em>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin.

</span>
<span class="ltx_bibblock">CMMLU: Measuring Massive Multitask Language Understanding in Chinese.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.09212</em>, 2023a.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You.

</span>
<span class="ltx_bibblock">Sequence parallelism: Long sequence training from system perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.13120</em>, 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/alpaca_eval" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/alpaca_eval</a>, 2023b.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LinkSoul-AI [2023]</span>
<span class="ltx_bibblock">
LinkSoul-AI.

</span>
<span class="ltx_bibblock">Chinese llava.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/LinkSoul-AI/Chinese-LLaVA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/LinkSoul-AI/Chinese-LLaVA</a>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong&nbsp;Jae Lee.

</span>
<span class="ltx_bibblock">Improved baselines with visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03744</em>, 2023a.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong&nbsp;Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08485</em>, 2023b.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2023c]</span>
<span class="ltx_bibblock">
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.

</span>
<span class="ltx_bibblock">What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15685</em>, 2023c.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">#instag: Instruction tagging for analyzing supervised fine-tuning of large language models, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. [2018]</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Anand Mishra, Shashank Shekhar, Ajeet&nbsp;Kumar Singh, and Anirban Chakraborty.

</span>
<span class="ltx_bibblock">Ocr-vqa: Visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">2019 international conference on document analysis and recognition (ICDAR)</em>, pages 947–952. IEEE, 2019.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Thuat Nguyen, Chien Van&nbsp;Nguyen, Viet&nbsp;Dac Lai, Hieu Man, Nghia&nbsp;Trung Ngo, Franck Dernoncourt, Ryan&nbsp;A Rossi, and Thien&nbsp;Huu Nguyen.

</span>
<span class="ltx_bibblock">CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.09400</em>, 2023.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2022]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">ChatML, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md</a>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training Language Models to Follow Instructions with Human Feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:27730–27744, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paster [2023]</span>
<span class="ltx_bibblock">
Keiran Paster.

</span>
<span class="ltx_bibblock">Testing language models on a held-out high school national finals exam.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam</a>, 2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.

</span>
<span class="ltx_bibblock">The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only, 2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pope et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.

</span>
<span class="ltx_bibblock">Efficiently Scaling Transformer Inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, 5, 2023.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Jack&nbsp;W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et&nbsp;al.

</span>
<span class="ltx_bibblock">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.11446</em>, 2021.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher&nbsp;D Manning, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18290</em>, 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</em>, pages 1–16. IEEE, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et&nbsp;al. [2016]</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ Questions for Machine Comprehension of Text, 2016.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">WinoGrande: An Adversarial Winograd Schema Challenge at Scale, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.

</span>
<span class="ltx_bibblock">SocialIQA: Commonsense Reasoning about Social Interactions, 2019.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sardana and Frankle [2023]</span>
<span class="ltx_bibblock">
Nikhil Sardana and Jonathan Frankle.

</span>
<span class="ltx_bibblock">Beyond chinchilla-optimal: Accounting for inference in language model scaling laws.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.00448</em>, 2023.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et&nbsp;al. [2024]</span>
<span class="ltx_bibblock">
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.

</span>
<span class="ltx_bibblock">Are emergent abilities of large language models a mirage?

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.02114</em>, 2021.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer [2019]</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">Fast Transformer Decoding: One Write-Head is All You Need.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.02150</em>, 2019.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer [2020]</span>
<span class="ltx_bibblock">
Noam Shazeer.

</span>
<span class="ltx_bibblock">GLU Variants Improve Transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.05202</em>, 2020.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shibata et&nbsp;al. [1999]</span>
<span class="ltx_bibblock">
Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa.

</span>
<span class="ltx_bibblock">Byte Pair Encoding: A Text Compression Scheme That Accelerates Pattern Matching.

</span>
<span class="ltx_bibblock">Technical report, Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidorov et&nbsp;al. [2020]</span>
<span class="ltx_bibblock">
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.

</span>
<span class="ltx_bibblock">Textcaps: a dataset for image captioning with reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16</em>, pages 742–758. Springer, 2020.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal&nbsp;Md Shoeb, Abubakar Abid, Adam Fisch, Adam&nbsp;R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander&nbsp;W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman&nbsp;S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B.&nbsp;Ryan Roberts, Bao&nbsp;Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill&nbsp;Yuchen Lin, Blake Howald, Bryan
Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César&nbsp;Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher&nbsp;D. Manning, Christopher Potts, Cindy Ramirez, Clara&nbsp;E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel&nbsp;Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, and Daphne&nbsp;Ippolito et&nbsp;al. (351 additional authors&nbsp;not shown).

</span>
<span class="ltx_bibblock">Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 2023.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. [2021]</span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Ahmed Murtadha, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced Transformer with Rotary Position Embedding.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.09864</em>, 2021.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc&nbsp;V Le, Ed&nbsp;H Chi, Denny Zhou, et&nbsp;al.

</span>
<span class="ltx_bibblock">Challenging Big-Bench Tasks and Whether Chain-of-Thought can Solve Them.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.09261</em>, 2022.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.

</span>
<span class="ltx_bibblock">CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge, 2019.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023b.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention Is All You Need.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 06 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/pdf/1706.03762.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1706.03762.pdf</a>.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. [2019a]</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00359</em>, 11 2019a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/pdf/1911.00359.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1911.00359.pdf</a>.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. [2019b]</span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00359</em>, 2019b.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Xiaoxia Wu, Cheng Li, Reza&nbsp;Yazdani Aminabadi, Zhewei Yao, and Yuxiong He.

</span>
<span class="ltx_bibblock">Understanding int4 quantization for transformer models: Latency speedup, composability, and failure cases.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12017</em>, 2023.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik&nbsp;Abinav Sankararaman, Barlas Oguz, et&nbsp;al.

</span>
<span class="ltx_bibblock">Effective long-context scaling of foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16039</em>, 2023.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu&nbsp;Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.12244</em>, 2023.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce&nbsp;Bian, Chao Yin, Chenxu Lv, Da&nbsp;Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock">09 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/pdf/2309.10305.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/2309.10305.pdf</a>.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young et&nbsp;al. [2014]</span>
<span class="ltx_bibblock">
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.

</span>
<span class="ltx_bibblock">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 2:67–78, 2014.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. [2022]</span>
<span class="ltx_bibblock">
Gyeong-In Yu, Joo&nbsp;Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.

</span>
<span class="ltx_bibblock">Orca: A Distributed Serving System for Transformer-Based Generative Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)</em>, pages 521–538, 2022.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Yijiong Yu, Zhe Zhou, Zhixiao Qi, and Yongfeng Huang.

</span>
<span class="ltx_bibblock">Paraphrasing the original text makes high accuracy long-context qa.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11193</em>, 2023.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yunjie et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Ji&nbsp;Yunjie, Deng Yong, Gong Yan, Peng Yiping, Niu Qiang, Zhang Lei, Ma&nbsp;Baochang, and Li&nbsp;Xiangang.

</span>
<span class="ltx_bibblock">Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.14742</em>, 2023.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. [2019]</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">HellaSwag: Can a Machine Really Finish Your Sentence?, 2019.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Xiaotian Zhang, Chunyang Li, Yi&nbsp;Zong, Zhengyu Ying, Liang He, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">Evaluating the Performance of Large Language Models on GAOKAO Benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.12474</em>, 2023a.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.

</span>
<span class="ltx_bibblock">Llavar: Enhanced visual instruction tuning for text-rich image understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.17107</em>, 2023b.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. [2023a]</span>
<span class="ltx_bibblock">
Huaixiu&nbsp;Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed&nbsp;H. Chi, Quoc&nbsp;V Le, and Denny Zhou.

</span>
<span class="ltx_bibblock">Take a step back: Evoking reasoning via abstraction in large language models, 2023a.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. [2023b]</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi&nbsp;Lin, Zhuohan Li, Dacheng Li, Eric.&nbsp;P Xing, Hao Zhang, Joseph&nbsp;E. Gonzalez, and Ion Stoica.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena, 2023b.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. [2023]</span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment, 2023.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. [2016]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li&nbsp;Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 4995–5004, 2016.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2403.04651" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2403.04652" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2403.04652">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.04652" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2403.04653" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:01:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
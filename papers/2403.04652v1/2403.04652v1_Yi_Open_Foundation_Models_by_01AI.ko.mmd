**Yi: 기초 모델을 01.AI로 엽니다**

**01.AI**

**Code:** [https://github.com/01-ai/Yi](https://github.com/01-ai/Yi)

**Model:** [https://huggingface.co/01-ai](https://huggingface.co/01-ai)

**Abstract**

강력한 다차원 기능을 보여주는 일련의 언어 및 멀티모달 모델인 Yi 모델 패밀리를 소개합니다. Yi 모델 패밀리는 6B 및 34B 사전 학습된 언어 모델을 기반으로 하며 채팅 모델, 200K 긴 컨텍스트 모델, 깊이 확장 모델 및 비전 언어 모델로 확장한다. 우리의 기본 모델은 MMLU와 같은 광범위한 벤치마크에서 강력한 성능을 달성하고, 우리의 미세 조정 채팅 모델은 알파카발 및 챗봇 아레나와 같은 주요 평가 플랫폼에서 강력한 인간 선호율을 제공한다. 확장 가능한 슈퍼 컴퓨팅 인프라와 고전적인 변압기 아키텍처를 기반으로, Yi 모델의 성능은 주로 데이터 엔지니어링 노력의 결과로 인한 데이터 품질에 기인한다. 사전 학습을 위해 캐스케이드 데이터 중복 제거 및 품질 필터링 파이프라인을 사용하여 3.1조 개의 영어 및 중국어 말뭉치를 구성합니다. 미세 조정을 위해 모든 인스턴스가 기계 학습 엔지니어에 의해 직접 검증되도록 여러 반복에 걸쳐 소규모(10K 미만) 명령어 데이터 세트를 연마합니다. 비전 언어의 경우 채팅 언어 모델과 비전 트랜스포머 인코더를 결합하고 언어 모델의 의미 공간에 시각적 표현을 정렬하도록 모델을 훈련한다. 가벼운 연속 사전 훈련을 통해 컨텍스트 길이를 200K로 확장하고 강력한 건초 스택 내 바늘 검색 성능을 보여준다. 우리는 지속적인 사전 훈련을 통해 사전 훈련된 체크포인트의 깊이를 확장하는 것이 성능을 더욱 향상시킨다는 것을 보여준다. 현재 결과를 감안할 때 철저히 최적화된 데이터를 사용하여 모델 매개변수를 계속 확장하면 훨씬 더 강력한 프론티어 모델로 이어질 것이라고 믿습니다.

###### Contents

* 1 소개
* 2 프리트레이닝
	* 2.1 데이터 처리
	* 2.2 Tokenization
	* 2.3 모델 아키텍처
* 3 Finetuning
	* 3.1 데이터 전처리
	* 3.2 훈련방법
* 4 Infrastructure
* 5 안전성
* 6 평가
	* 6.1 기본 모델 성능
		* 6.1.1 주요 결과
		* 6.1.2 토론
		* 6.1.3 In-Context Learning Study
	* 6.2 채팅 모델 성능
		* 6.2.1 자동 평가
		* 6.2.2 인간 평가
*7 능력 확장
	* 7.1 Long Context Modeling
	* 7.2 Vision-Language
	* 7.3 Depth Upscaling
* 8 최종 토론
* 저작자 목록 및 기여도

Introduction

대형 언어 모델의 최근 돌파구는 인공 지능의 전체 분야에 혁명을 일으켰고 잠재적으로 인간 사회 전체에 걸쳐 방사되었다. 대규모 언어 모델에 대한 우리의 비전은 그것들을 차세대 컴퓨팅 플랫폼으로 만들고 크게 증폭된 지능으로 전체 커뮤니티에 힘을 실어주는 것입니다. 이 미션을 위한 단계로, 우리는 3.1T 고도로 설계된 많은 양의 데이터에 대해 처음부터 사전 훈련된 Yi 모델 시리즈, 6B 및 34B 언어 모델을 제시하고 작지만 세심하게 연마된 정렬 데이터에 대해 미세 조정한다. 다음 섹션에서 자세히 설명할 상당한 엔지니어링 노력의 결과 데이터 품질로 인해 Yi는 GPT-3.5 벤치마크 점수와 인간 선호도에 근접한다.

Yi 모델 시리즈를 설계할 때 대부분 _모델 규모, 데이터 규모 및 데이터 품질_ 에 관한 다음 차원에 관심이 있습니다. 모델 척도를 선택할 때 데시데라타는 제한적 24G 메모리인 RTX 4090과 같은 소비자 등급 하드웨어에서 추론할 수 있는 충분히 작은 모델을 가지고 있지만 복잡한 추론과 창발 능력으로 여전히 충분히 크다. 이것이 우리가 34B가 좋은 성과-비용 균형을 제공한다는 것을 발견한 이유이다. 34B는 Chinchilla [30] 및 LLaMA [77]에서 사용하는 기존 70B보다 작기 때문에 감소된 컴퓨팅 플롭을 보상하기 위해 사전 훈련 데이터 규모를 3.1T 토큰으로 증가시킨다. 이는 모델-데이터 스케일 조합이 포스트 친칠라 최적 레짐(post Chinchilla optimal regime, [64])에 속하게 한다. 즉, 우리는 계산 최적(1T 전후)보다 더 많은 토큰(3T)에서 모델을 능가한다. 이점은 추론 측면에서 얻을 수 있는데, 우리가 감소된 서빙 비용으로 더 강한 성능을 달성하기 때문이다: int4 [81] 양자화 후에, 성능 저하가 거의 없이 24G GPU 메모리 상에서 34B 채팅 모델을 서빙할 수 있다; (3). 우리의 데이터 엔지니어링 원칙은 사전 훈련과 미세 조정 모두를 위해 양보다 질을 촉진하는 것이다. 사전 훈련 데이터 품질은 캐스케이드 필터링 방법 및 의도적으로 증가된 중복 제거 강도를 갖는 정교한 데이터 클리닝 파이프라인에 의해 보장된다; (4). 데이터 미세조정의 경우 사용자 피드백을 기반으로 여러 번의 반복에 걸쳐 10K 미만의 지침을 수작업으로 만들어 품질을 크게 강조한다. 이 접근법은 FLAN[9] 및 UltraChat[19]와 같은 수량 스케일링 스타일링 명령어 튜닝 작업에서 크게 벗어났지만 LIMA[94]와 같은 핸드크래프팅 스타일링 작업과 더 많이 정렬한다.

사전 학습 데이터 클리닝 시스템은 언어, 휴리스틱 텍스트 특징, 복잡성, 의미, 주제, 안전성에 기반한 정교한 필터링 파이프라인과 단락, MinHash, 정확한 매칭에 기반한 계단식 중복 제거 프로세스를 특징으로 한다. 이러한 철저한 파이프라인은 CCNet[80], RefinedWeb[56] 및 RedPajama[13]과 같은 기존 파이프라인보다 훨씬 높은 제거율로 이어지며, 이는 데이터 엔지니어링 성공의 핵심이라고 생각합니다. 기본 원칙은 사전 훈련이 데이터 스케일링을 요구하지만, 큰 원시 데이터에 대해 모델을 훈련시키는 것보다 사용되는 데이터가 고품질인지 확인하고 싶은 것이다. 즉, 우리는 광범위한 필터링 없이 10T 토큰보다 정교한 엔지니어링보다 3T 토큰을 선호한다. 모델 아키텍처와 관련하여, 우리는 Grouped-Query Attention (GQA) [1], SwiGLU [68] 활성화 및 조정된 기본 주파수 (RoPE ABF) [82)를 갖는 RoPE를 갖는 트랜스포머 아키텍처의 표준 구현을 사용한다. 이 설계 선택은 나중에 GPT-3 및 친칠라[30]에 의해 수정된 트랜스포머 원본 논문[78]에서 뿌리를 내린 표준 접근법이며, 그 다음 LLaMA[77], 바이촨[84], Qwen[3] 및 많은 관련 작업이 뒤따른다.

GPT-3.5 매칭 인간 선호도에 접근하기 위해, 미세 조정 데이터 세트는 엄선된 다중 회전 명령-응답 쌍으로부터 선별되고, 기계 학습 엔지니어 팀에 의해 직접 주석이 달린 다음 사용자 피드백의 여러 반복을 통해 연마된다. 위에서 언급한 바와 같이, 미세 조정 데이터 세트의 크기는 10K 미만이지만 모델 개발 타임라인에 걸쳐 계속해서 개선되었다. 데이터 세트의 관리 가능한 크기로 인해 최적의 데이터 구성을 식별하고 다양성을 촉진하며 효과적인 하이퍼파라미터를 찾기 위해 광범위한 그리드 검색을 사용했다. 8-비트 및 4-비트 양자화 후에, 최종 채팅 모델은 bf16 포맷에 비해 성능 저하 없이 거의 소비자 등급 GPU에 배치될 수 있다.

이 모델의 성능을 컨텍스트 스케일링, 비전 언어 적응 및 깊이 업스케일링의 세 가지 차원에서 추가로 확장한다. 200K 컨텍스트 길이를 달성하기 위해 Fu et al. [22]의 동시 작업과 유사하게 약 5B 길이 업샘플링된 데이터에 대해 모델을 계속 사전 훈련한다. 이 모델을 비전 언어 작업에 적용하기 위해, 우리는 비전 인코더를 통합하고 Liu et al. [47]의 연습을 따르고 개선하는 다단계 훈련 방법을 개발한다. 또한 깊이-업스케일링(depth-upscaling, [38]의 효과, 즉 지속적인 사전 훈련에 의해 모델을 더 깊게 만들고 모델 성능을 더욱 향상시키기 위한 효과를 확인한다.

사전 훈련에서 세부 조정에서 서빙까지. 사전 학습을 지원하기 위해 클라우드 간 탄력적 태스크 스케줄링, 자동 장애 복구 및 토폴로지 인식 자원 할당을 개발하며, 이는 제한된 스위칭 오버헤드로 실시간 사용 가능한 GPU 노드 간 클러스터에 따라 태스크를 실행할 수 있도록 한다. 피니튜닝을 지원하기 위해, 상이한 모델(예를 들어, 정책 모델의 경우 메가트론[70] 및 보상 모델의 경우 딥스피드[60])에 대해 상이한 분산 백엔드를 지원하는 계층적 스케줄링 프레임워크를 구축한다. 효율적인 추론을 위해 4비트 모델과 8비트 KV 캐쉬 양자화를 사용하였으며, PagedAttention [41]과 Dynamic Batching을 결합하였다.

광범위한 실험은 Yi-34B가 성능과 효율성 모두에서 GPT-3.5와 일치할 수 있음을 보여준다. MLU[27](기본 모델의 경우) 및 LMSys ELO 등급[93](채팅 모델의 경우)과 같은 대부분의 표준 벤치마크에서 Yi-34B는 일반적으로 GPT-3.5와 동등한 점수를 달성한다. 모델 파라미터 및 KV 캐시 양자화를 수행한 후, 추론 비용은 또한 광범위한 커뮤니티가 비용 효율적인 장치에 모델을 배치할 수 있도록 제어된다. 또한 다중평가 벤치마크에서 상식 추론, 대학 시험, 수학, 코딩, 독해력 및 인간 선호율에 대한 Yi와 주요 LLM의 세부 성능 비교를 보고한다.

출시 이후, Yi 모델 시리즈는 다음과 같은 관점에서 커뮤니티에 혜택을 주었다: (1). GPT-3.5 매칭 품질이지만 비용 효율적인 모델을 연구자에게 제공하고, 개발자는 언어 모델 기반 에이전트와 같은 AI 네이티브 애플리케이션을 구축할 수 있다. (2). 이는 로컬 실행 가능한 챗봇으로 최종 사용자에게 권한을 부여하므로 결과적으로 사용자 데이터 프라이버시를 보호하는 데 도움이 된다. (3). 이는 더 강력한 프론티어 모델을 달성하기 위해 추가 데이터 및 모델 스케일링에 대한 방향을 조명한다. 연구 및 상업적 용도로 모두 사용할 수 있습니다.

## 2 Pretraining

사전 훈련에 대한 우리의 접근법은 고도로 조작된 대규모 사전 훈련 코퍼라에서 표준 밀집 변압기 아키텍처를 훈련시키는 것인데, 여기서 우리의 기본 가정은 높은 품질의 광범위한 데이터에 대해 훈련될 때 표준 아키텍처가 고급 능력을 나타낼 수 있다는 것이다. 즉, 우리는 실제로 광범위한 예비 건축 실험을 수행했지만 많은 건축 수정이 필요하지 않을 수 있습니다. 다음 하위 섹션에서는 먼저 데이터 엔지니어링 파이프라인에 대해 자세히 설명한 다음 모델 아키텍처에 대해 간략하게 설명합니다.

### Data Processing

Yi 데이터 혼합물은 그림 2에 나와 있다. 고품질 이중 언어 사전 훈련 데이터를 생성하기 위해 그림 1과 같이 캐스케이드 데이터 처리 파이프라인을 세심하게 설계했다. 이 파이프라인은 품질과 다양성을 목표로 하는 일련의 데이터 정리 전략을 특징으로 한다. 우리는 Common Crawl의 웹 문서부터 시작하여 언어 식별 및 복잡도 점수화를 위해 CCNet 파이프라인 [79]를 사용한다. 그런 다음 아래에 자세히 설명된 대로 필터링 및 중복 제거 프로세스의 조합을 사용합니다.

그림 1: Yi의 사전 훈련 데이터 클리닝 파이프라인입니다.

휴리스틱 규칙 필터링 필터의 이 부분은 낮은 품질의 텍스트를 제거하는 것을 목표로 한다. 우리는 다음 (1)을 기준으로 텍스트를 필터링합니다. URL, 도메인, 단어 블록리스트 및 가블링된 텍스트 필터; (2). 문서 길이, 특수 기호의 비율 및 짧은 줄, 연속 줄 또는 불완전 줄의 비율; (3). 반복된 단어들, n-그램들, 또는 단락들 [58]; 필터링 임계치들은 Nguyen 등 [52]에 설명된 바와 같이, 대형 문서 샘플들의 통계적 분석에 기초한다. 또한, 이메일 주소 및 전화번호와 같은 개인 식별 정보(PII)를 식별하고 익명화한다.

학습된 필터를 사용하여 표준 휴리스틱 규칙의 성능을 초과하는 미묘한 경우를 해결합니다. 특히 커먼 크롤에서 추출한 중국 콘텐츠는 특히 음란물과 도박과 같은 부적절한 콘텐츠의 비율이 더 높은 독특한 과제를 제시한다. 전통적인 휴리스틱 규칙 기반 필터는 모든 유해한 콘텐츠를 효과적으로 식별하고 제거하기 위해 고군분투한다. 필터링 프로세스를 개선하기 위해, 우리는 필터링을 위해 학습된 채점자, 즉 복잡도 채점자, 품질 채점자, 안전 채점자 및 문서 일관성 채점자(1)를 통합했다. CCNet [80]에 따라 KenLM 라이브러리를 사용하여 _Perplexity Scorer_ 는 방대한 웹 문서 배열을 평가하여 Perplexity 점수가 평균 이상인 문서를 폐기합니다. (2). _품질 점수기_는 위키피디아와 유사한 페이지를 품질로 인식하고 선호하고 그에 따라 점수를 할당하도록 훈련된 분류기입니다. 품질 표준을 충족하지 못한 문서는 이후에 제거됩니다. (3). "문서 일관성 점수"는 서로 다른 문장 또는 문단으로 구성된 품질이 낮은 웹 문서를 식별하므로 일관성이 없습니다. 이러한 문서는 추가 분석을 위해 분할되거나 완전히 제거된다. (4). "안전 점수"는 폭력, 음란물, 정치적 선전물과 같은 유해한 내용이 포함된 웹 문서를 식별하고 제거합니다.

클러스터 기반 필터링 우리는 또한 웹 문서를 그룹화하기 위해 감독되지 않은 의미론적 클러스터링을 사용한다. 이러한 클러스터링 과정을 통해 유사한 의미 특징을 공유하는 문서들을 효율적으로 식별하고 분석할 수 있다. 클러스터링된 데이터는 후속적으로 품질 라벨로 주석을 달아서 Yi의 데이터 혼합 전략의 최적화를 위한 필수 참조를 제공한다. 자동 및 수동 검증을 통해 저품질로 식별된 문서는 데이터셋에서 제외된다.

필터링 후, 우리는 Penedo et al.(2023) [56]의 절차에 따라 포괄적인 중복 제거 파이프라인을 구현한다. 이 파이프라인은 문서 수준 MinHash 중복 제거 및 하위 문서 정확 일치 중복 제거를 통합하여 문서 내 및 문서 전체에서 중복된 내용을 효과적으로 식별하고 제거합니다. 또한 뉴스, 광고 및 지식 기반 콘텐츠와 같은 레이블을 예측하는 토픽 모델을 사용하여 웹 문서를 특정 테마로 분류한다. 최종 사전 훈련 데이터 세트에서는 정보 밀도를 보장하기 위해 도움이 덜 되는 콘텐츠, 주로 광고를 다운 샘플링한다. Yi의 사전 훈련 데이터의 최종 구성은 그림 2에 나와 있다.

### Tokenization

사전 훈련 데이터를 토큰화하기 위해 SentencePiece 프레임워크 [40]에서 구현된 바이트 쌍 인코딩(BPE) [69]을 사용한다. Yi의 어휘 크기는 계산 효율성과 단어 이해의 균형을 맞추기 위해 64,000으로 설정된다. 특히 숫자 데이터를 더 잘 이해하기 위해 숫자를 개별 숫자로 나눕니다. 우리는 희귀 캐릭터가 결함 허용을 보장하기 위해 유니코드 바이트 인코딩으로 후퇴하도록 허용합니다. 모든 구두점을 반치폭 형식으로 전송하지 않도록 ID 토큰화기를 사용합니다. 영어를 우선시하는 LLM은 일반적으로 문장의 서로 다른 위치에서 동일한 단어를 일반화하기 위해 토큰라이저에서 더미 프리픽스(텍스트의 시작 부분에 공백)를 활용한다. 이 접근법은 특히 따옴표로 시작하는 문장의 경우 가정이 영어 문맥에서도 항상 성립하지 않으며 중국 문맥에서도 긍정적인 효과를 나타내지 않기 때문에 사용하지 않는다.

그림 2: Yi의 사전 훈련 데이터 혼합물입니다. 전반적으로 우리의 데이터는 영어와 중국어 모두 3.1T 고품질 토큰으로 구성되며 다양한 출처에서 나온다. LLaMA [76] 및 Falcon [56]과 같은 기존 알려진 혼합물과의 주요 차이점은 우리는 이중 언어를 구사하고 더 엄격한 세척 파이프라인으로 인해 고품질이라는 것입니다.

### Model Architecture

Yi는 코드가 LLaMA의 [77] 구현을 기반으로 하는 고전적인 디코더 전용 트랜스포머 아키텍처 [78]의 수정된 버전을 사용한다. 주요 파라미터 설정은 표 1에 요약되어 있다. LLaMA로부터 Yi로의 수정은 아래에 더 요약되어 있다:

Attention MechanismLLaMA 2는 최대 70B 모델에서만 Grouped-Query Attention(GQA) [1]을 사용하고, 7B와 13B는 full attention를 사용한다. 우리는 Yi-6B와 Yi-34B 모두에 GQA를 통합한다. GQA는 질의-헤드를 G 그룹으로 분할하고, 질의의 각 그룹 내에서 단일 키 및 값 헤드를 공유한다[1]. 이 접근법은 원래의 Multi-Head Attention (MHA) [16; 57; 67]에 비해 트레이닝 및 추론 비용의 실질적인 감소를 제공한다. 우리는 6B 소형 모델에 GQA를 적용한 후 성능 저하를 관찰하지 않는다.

활성화 함수는 SwiGLU [68]을 Yi의 주의 후 레이어로 사용하여 활성화 크기를 \(4h\)에서 \(8/3h\)(\(h\)는 숨겨진 크기를 나타냄)로 줄여 정상적인 주의 후 레이어와 일치하도록 한다. 이 조정은 또한 GQA로 인한 매개변수 감소를 보상하여 전체 매개변수 수를 기존 7B 및 34B 모델과 호환되도록 한다.

Positional Embedding과 Long Context는 표준 구현에 따라 RoPE(Rotary Position Embedding) [73]을 사용한다. Xiong et al. [82]에 소개된 기저 주파수(RoPE ABF)는 기저 모델 자체가 4K 컨텍스트 길이에 대해 훈련되는 200K까지의 긴 컨텍스트 윈도우를 지원하기 위해 조정된다. 기본 모델을 더 긴 컨텍스트에 적용하기 위해 주로 책에서 약간 업샘플링된 긴 시퀀스가 있는 사전 훈련 데이터 혼합물에서 10B 토큰에 대해 모델을 계속 사전 훈련한다. 1-2B 토큰만이 4K-200K 길이의 낮은 손실로 수렴하기에 충분하며, 경량 미세 조정은 더 나아가 거의 완전한 장문맥 검색 성능을 유도한다는 것을 관찰한다. 이러한 관찰에 기초하여, 우리는 사전 훈련된 길이(4K)보다 더 긴 의존성을 모델링하는 능력이 (사후 훈련에 의해 주입되는 것이 아니라) 본질적인 능력이라고 보는 경향이 있다. 즉, 기본 모델은 모델이 더 짧게 훈련되더라도 4K 종속성보다 더 길게 모델링할 수 있는 능력을 이미 가지고 있으며, 포스트-트레인/미세조정 절차는 단순히 이 능력을 해제한다.

## 3 Finetuning

우리의 미세 조정 방법은 양보다 데이터 품질을 크게 강조한다. 우리의 접근법은 FLAN [9] 및 UltraChat [19]와 같은 기존 데이터 집약적 접근법을 따르지 않으며, 이는 SFT 데이터를 수백만 개의 항목으로 확장하지만 규모가 너무 크기 때문에 각 항목을 주의 깊게 조사하지 않을 수 있다. 대조적으로, 우리의 방법은 스케일링보다는 데이터 선택에 초점을 맞춘 LIMA [94] 및 DEITA [48] 접근법과 일치한다. 규모가 10K 미만인 경우 _모든 단일 데이터 지점_ 을 검사하고 최적화할 수 있습니다. 아래에서는 데이터 구성 및 교육 세부 사항에 대해 설명합니다.

### Data Preprocessing

품질은 All You Need 당사의 미세 조정 데이터 세트는 10K 미만의 다중 회전 명령-응답 대화 쌍으로 구성되며, 각 항목은 여러 반복 및 사용자 피드백에서 구성되고 연마됩니다. 예비 실험에서 수십만 항목의 오픈 소스 데이터와 비교하여 더 작고 수동으로 주석이 달린 데이터 세트의 결과가 우수하다는 것을 관찰하기 때문에 이 접근법을 취한다. 이러한 관찰은 Gemini Team et al. [23], Touvron et al. [77], Zhou et al. [94]에서 보고된 것과 일치한다.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Models & Hidden Size & Q-heads & KV-heads & Layers & Pretrain Seq. Len & Max LR \\ \hline
6B & 4096 & 32 & 4 & 32 & 4096 & \(3\times 10^{-4}\) \\
34B & 7168 & 56 & 8 & 60 & 4096 & \(1.5\times 10^{-4}\) \\ \hline \hline \end{tabular}
\end{table}
표 1: Yi-6B 및 Yi-34B의 모델 구성. LR은 학습률을 의미합니다.

다음 기술을 사용하여 신속한 배포 선택, 응답 형식화 및 연쇄 사고 형식화를 개선합니다. (1). 위저드LM[83]에서 영감을 얻어 신속한 배포 선택을 위해 복합 명령어를 개발하고 복잡성을 높이기 위해 점진적으로 진화시켰다. 이 방법은 실험에서 SFT 데이터의 크기를 크게 줄였다. 응답 형식의 경우 일반적으로 LIMA[94]에서 확장한 기본 스타일을 사용합니다. 전반적으로 응답은 본문이 일반적으로 글머리 기호 목록인 도입-본체-결론 형식으로 구성된다. CoT 데이터 형식화를 위해 Zheng et al. [92]에서 영감을 받은 "Step-Back" 패턴을 사용하여 추상화를 수행하여 상위 수준의 솔루션을 공식화한 후 원본에 대한 더 구체적인 질문을 추론합니다.

우리는 환각과 반복을 줄이기 위해 추가적인 노력을 기울인다: (1). 환각을 줄이기 위해, 우리는 반응에 있는 지식이 모델 안에 포함되지 않도록 조사하고 보장하며, 암기로 이어질 수 있는 반응을 제거한다; (2). 우리는 반복을 줄이기 위해 일반적으로 존재하지만 미세 조정 데이터에서 간과될 수 있는 응답의 반복적인 회전을 다시 작성한다.

다양성과 혼합은 다양한 능력의 범위를 보장하기 위해 질문 응답, 창의적 쓰기, 대화, 추론, 수학, 코딩, 안전, 이중 언어 능력 등과 같은 영역을 포괄하는 광범위한 오픈 소스 프롬프트를 포함했다.

InsTag[49]에서 영감을 받아 다양한 기능의 방향을 세밀하게 제어할 수 있도록 명령어 태깅 시스템을 개발한다. 다양성에 초점을 맞춘 샘플링 알고리즘을 설계함으로써 다양한 태그에 걸친 명령어 분포의 균형을 주의 깊게 맞추었다. 이 접근법은 향상된 교차 작업 견고성을 달성하기 위해 다양한 미세 조정 데이터 세트를 보장한다.

성능의 서로 다른 방향의 균형을 맞추기 위한 최적의 데이터 비율을 달성하기 위해 근사 격자 탐색을 사용하여 데이터 혼합을 결정한다. Dong et al. [20]에 의해 동기부여된 이 과정은 각 능력에 대해 {1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64} 비율로 실험하는 것을 포함했다. 검색 프로세스는 검증 결과와 사내 인간 평가 세트에 의해 안내되었다.

ChatML FormatBe focused of data quality and diversity, 우리의 관찰은 데이터의 포맷이 모델의 궁극적인 성능에 실질적으로 영향을 미친다는 것을 보여주었다. 이를 위해 ChatML-style format[53]을 구현하였다. 이러한 구조화된 접근법은 모델이 시스템 구성들, 사용자 입력들, 및 어시스턴트 응답들과 같은 다양한 정보 유형들 사이에서 구별하는 것을 가능하게 한다.

### Training Method

우리는 미세조정을 위해 다음 단어 예측 손실을 사용하고 응답에 대한 손실만 계산하지만 시스템 및 사용자 명령은 계산하지 않는다. AdamW 최적화기는 \(\beta_{1}\)를 0.9로, \(\beta_{2}\)를 0.999로, \(\epsilon\)를 10^{-8}\로 설정하였다. 배치 크기 64와 함께 시퀀스 길이 4096을 사용하였다. 학습 단계는 일정한 \(1\times 10^{-5}\) 학습률, 무게 감소 0.1, 기울기 클리핑 최대 임계값 1.0, 노이즈 척도인 NEFTune [34]는 Yi-34B-Chat의 경우 45, Yi-6B-Chat의 경우 5로 300으로 설정하였다.

## 4 Infrastructure

우리는 전체 스택 데이터 처리, 사전 훈련, 미세 조정 및 서비스를 지원하는 인프라를 구축합니다. 인프라 기능: (1). 상기 컴퓨팅 리소스를 자동 관리 및 모니터링하는 단계; (2). 최적화된 병렬 전략, 커널 효율성 및 긴 컨텍스트 지원으로부터 훈련 속도를 향상시켰다; (3). 직접 선호 최적화(DPO)에서 다중 모델에 대해 메가트론 및 딥스피드를 동시에 사용하는 것과 같은 이질적인 분산 훈련 백엔드를 지원하는 통합된 미세 조정 프레임워크[59]; (4). 양자화, 연속 배칭 및 페이징된 주의와 같은 가속을 제공하는 다양한 LLM에 의해 배치 비용을 감소시킨다. 아래에서는 이러한 기법들을 하나씩 설명한다.

컴퓨팅 자원 관리 수천 개의 GPU에서 수개월이 걸릴 수 있는 대규모 언어 모델 개발, 특히 사전 훈련을 효율적으로 스케줄링하기 위해 우리는 우선순위가 다른 사전 훈련, SFT 및 RLHF 작업을 관리하기 위해 고효율의 멀티 클라우드 작업 스케줄링 알고리즘을 구축한다. 또한 GPU 가용성을 기반으로 사전 훈련 작업을 다양한 노드 크기로 자동으로 탄력적으로 확장할 수 있는 고성능 사내 훈련 프레임워크를 구축한다. 더 중요한 것은 모든 훈련 관련 하이퍼 매개 변수가 동시에 원활하게 조정된다는 것입니다.

대규모 언어 모델 훈련 단계에서 GPU 충돌에서 통신 패브릭 오류에서 손실 스파이크에 이르기까지 광범위한 오류가 정기적으로 발생한다. 다음 전략을 사용하여 이러한 신뢰성 문제를 해결합니다. (1) 다양한 종류의 소프트웨어/하드웨어 오류 범주에 대해 노드의 자동화된 검사, 예측 및 레이블링을 적용합니다. 오염된 것으로 표시된 노드는 오류가 제거될 때까지 리소스 풀에서 일시적으로 제거됩니다. (2) 훈련 작업 중 장애 발생 시 빠르고 자동 복구를 위한 사전 점검과 능력을 갖춘 작업 큐잉 시스템을 구현한다. (3) 사용자 친화적인 다중 작업 제출 및 관리 콘솔을 개발하여 개발자가 훈련 작업 및 하이퍼 매개 변수를 원활하게 관리하고 추적할 수 있도록 한다.

성능 및 비용 효율성_메모리_ 및 _통신_ 제한은 GPU를 추가하는 것을 넘어 통합 솔루션을 필요로 하는 대규모 모델 교육의 두 가지 주요 기술적 과제입니다. 메모리 및 통신 제한을 해결하기 위해 다음과 같은 기술을 사용하고 개선한다: (1) ZeRO-1 [60]은 데이터-병렬 프로세스 교차 최적화 상태를 분할함으로써 메모리 소비를 제거하는 것; (2) 노드 간 통신 병목 현상을 피하기 위해 각 컴퓨팅 노드 내에서 파이프라인 병렬 [70]과 결합된 텐서 병렬, 3D 병렬 전략은 활성화 체크포인팅을 사용하지 않고 파이프라인 버블을 최소화하도록 잘 설계되고 최적화된다; (3) 중복 글로벌 메모리 액세스 및 소비를 줄이기 위해 플래시 어텐션[15][14] 및 JIT 커널과 같은 커널 융합 기술; (4) 일반적인 팻 트리-토폴로지(fat-tree-topology)의 한계인 스위치의 서로 다른 계층에 걸친 통신을 최소화하기 위한 토폴로지 인식 자원 할당(랭킹 전략)

프리트레이닝과 달리, 파인튜닝 LLM은 DPO[59] 및 PPO[54]의 관행과 같이 여러 모델의 오케스트레이션을 요구할 수 있다. 이러한 훈련 작업에서 일반적인 프로세스는 참조/보상 모델을 사용하여 데이터의 배치를 예측한 다음(또한 사소한 시간이 필요함), 타겟 모델이 손실 및 업데이트 파라미터를 계산하기 위해 이 데이터를 사용하도록 한다. 이를 위해 단일 작업에서 서로 다른 LLM에 대한 여러 백 엔드를 지원하기 위한 다중 모델 스케줄링 프레임워크를 구축한다. 예를 들어, DPO로 언어 모델을 미세조정할 때, 참조 모델로부터의 중간 결과들이 캐싱되고 재사용될 수 있어, 훈련 속도 및 자원 비용이 감독된 미세조정 대응물들에 근접하도록 개선된다.

빠르고 효율적인 추론 우리는 주로 디코딩 속도와 메모리 사용량을 개선하기 위해 양자화, 동적 배칭 및 페이지드 어텐션을 사용한다. 우리는 메모리 공간과 계산 수요를 줄이기 위해 양자화를 사용한다. 4-비트 모델 양자화 [81]와 8-비트 KV 캐시 양자화 [18]에 의해, MMLU/CMMLU 벤치마크에서 0에 가까운 성능 저하(예: \(1\%\) 미만의 정확도 저하)로 상당한 GPU 메모리 절약을 달성할 수 있다. 동적 배칭[86]을 사용하여 응답 시간을 최소화하고 배칭 효율을 향상시킨다. 우리는 PagedAttention[41]을 사용하여 메모리 활용도를 높이고 디코딩을 개선한다.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & **Size** & **MMLU** & **BBH** & **C-Eval** & **CMMLU** & **Gaokao** & **CR** & **RC** & **Code** & **Math** \\ \hline
**GPT-4** & - & **83.0** & **86.7** & 69.9 & 71.0 & 72.3 & **89.3** & - & **65.3** & **66.1** \\
**GPT-3.5** & - & 69.1 & 70.1 & 52.5 & 55.5 & 51.1 & 83.1 & - & 54.8 & 35.6 \\ \hline
**Qwen** & 14B & 66.7 & 53.4 & 72.1 & 71.0 & 62.5 & 74.2 & 72.5 & 40.6 & 43.1 \\ \hline
**Llama2** & 34B & 62.6 & 44.1 & - & - & - & 71.1 & 68.9 & 27.8 & 24.2 \\  & 70B & 69.7 & 64.9 & 50.1 & 53.3 & 23.3 & 72.7 & 72.3 & 38.4 & 35.2 \\ \hline
**Baichuan-2** & 13B & 55.0 & 49.0 & 59.0 & 61.97 & 45.6 & 66.3 & 62.4 & 23.4 & 16.1 \\
**InternLM** & 20B & 62.1 & 52.5 & 58.8 & 59.0 & 45.5 & 78.3 & - & 34.8 & 30.26 \\ \hline
**Skywork** & 13B & 62.1 & 41.7 & 60.6 & 61.8 & 68.1 & 72.4 & 61.4 & 64.9 & 18.1 \\ \hline
**Falcon** & 180B & 70.4 & 54.0 & 57.8 & 58.0 & 59.0 & 74.4 & - & - & - \\ \hline \multirow{2}{*}{**Yi**} & 6B & 63.2 & 42.8 & 72.0 & 75.5 & 72.2 & 72.2 & 68.7 & 21.1 & 18.6 \\  & 34B & 76.3 & 54.3 & **81.4** & **83.7** & **82.8** & 80.7 & **76.5** & 32.1 & 40.8 \\ \hline \hline \end{tabular}
\end{table}
표 2: 오픈 소스 기반 모델과 비교하여 그룹화된 학술 벤치마크에 대한 전반적인 성능. **CR** 은 상식 추론의 약자입니다. **RC** 는 읽기 이해의 약자입니다.

긴 컨텍스트 윈도우 지원 최대 200K 컨텍스트 길이 계속 사전 훈련 및 미세 조정을 지원하기 위해 계산-통신 중첩, 시퀀스 병렬성 및 통신 압축을 구현하고 개선한다. 컨텍스트 길이를 200K로 확장하는 방법은 엔지니어링을 기반으로 _솔리_합니다. 즉, 희소, 로컬 또는 슬라이딩 윈도우 주의와 같은 모델 아키텍처를 수정하지 않습니다. 모델은 입력이 200K인 경우에도 전체 주의를 사용하여 유지됩니다.

## 5 Safety

모델의 신뢰성과 안전성을 높이기 위해 풀스택 책임형 AI 안전 엔진(RAISE)을 개발합니다. RAISE는 안전한 사전 훈련, 정렬 및 배치를 보장합니다. 이 섹션에서는 사전 훈련 및 정렬 단계에서 안전 조치에 대해 설명합니다.

사전 훈련에서 안전 표준 사전 훈련 데이터 안전 관행과 정렬[5; 58; 77]은 휴리스틱 규칙, 키워드 매칭 및 학습된 분류기를 기반으로 필터 세트를 구축하여 개인 식별자와 개인 데이터를 포함하는 텍스트를 제거하고 성적, 폭력성 및 극단주의 콘텐츠를 줄인다.

[24; 35]의 기존 연구에 의해 설명된 Alignment의 안전 먼저 포괄적인 안전 분류법을 구축한다. 이 분류법은 환경적 불협화음, 미신, 종교적 민감성, 차별적 관행, 약물 남용, 폭력적 행동, 불법 행위, 혐오 발언, 윤리 위반, 사생활 침해, 자해, 성적 명시적 내용, 정신 건강 문제 및 사이버 보안 위협을 포함한 광범위한 잠재적 문제를 다룬다. 강력한 정렬을 위해 이러한 범주를 반영하는 데이터 세트를 선별하고 대화 상자 SFT 데이터와 혼합한다. 또한 정렬 단계에서 공격 시나리오를 시뮬레이션하는 대상 프롬프트 세트를 포함하여 악의적인 사용에 대한 모델의 복원력을 효과적으로 개선했습니다.

## 6 Evaluations

평가 결과, Yi 모델 패밀리는 광범위한 태스크에서 영감을 주는 성능을 달성하고 GPT-3.5 사용자 선호율에 근접함을 보여준다. 먼저 표준 벤치마크에 대한 기본 모델 성능을 보고하고 채팅 모델 성능과 사용자 선호율에 대해 논의한다.

### 기본 모델 성능

#### 6.1.1 Main Results

여기서는 표준 학술 벤치마크에 걸쳐 기본 모델과 잘 알려진 여러 기본 모델에 대한 결과를 제시한다. 오픈 소스 모델을 벤치마킹하는 동안 파이프라인에 의해 생성된 결과와 공개 소스에 보고된 결과 사이의 차이를 관찰했다. 이 차이에 대한 보다 심층적인 조사를 수행할 때, 주로 다른 모델이 다른 프롬프트, 후처리 전략 및 샘플링 기술을 사용하기 때문이다. 이러한 차이는 잠재적으로 결과에 상당한 변화를 유발할 수 있다. 우리의 신속한 후처리 전략은 원래 벤치마크의 기본 설정과 일치한다[2; 4; 7; 8; 10; 11; 12; 27; 28; 42; 50; 61; 62; 63; 72; 74; 75; 89; 90]. 생성된 콘텐츠에 대해 후처리 없이 탐욕적 디코딩을 사용한다. 공개적으로 보고되지 않은 점수(또는 다른 설정으로 보고된 점수)의 경우 파이프라인으로 결과를 얻으려고 한다. 공개적으로 찾을 수 있는 점수에 대해서는 기존 수치를 직접 보고한다. 우리는 주로 LLaMA 2 [77]의 관행을 따르는 다음의 벤치마크를 사용한다:

**상식 추론:**: 상식 추론을 평가하기 위해 PIQA[4], SIQA[63], HellaSwag[89], WinoGrande [62], ARC[11], OpenBookQA(OBQA)[50] 및 CommonsenseQA(CSQA)[75]를 포함했습니다. CSQA는 7샷 설정을 사용하여 독점적으로 테스트되었으며 다른 모든 테스트는 0샷 구성으로 수행되었다.
**읽기 이해:**: 읽기 이해의 경우 SQuAD[61], QuAC[8] 및 BoolQ[10]에서 0샷 평균을 보고합니다.
**수학:**: 특정 프롬프팅 전략(예: 연쇄 사상 프롬프팅) 및 기타 앙상블 기법(예: 다수 투표) 없이 패스@1 정확도로 GSM8K[12](8 샷) 및 MATH[28](4 샷) 벤치마크의 평균을 보고합니다.

**코드:**: HumanEval[7](Chen 등, 2021) 및 MBPP[2](Austin 등, 2021)에 대한 모델의 평균 패스@1 점수를 보고합니다.
**인기 집계 벤치마크:**: MMLU[27](5-shot), CMMLU[42](5-shot), Gaokao-Bench[90](5-shot) 및 BigBench[72] Hard (BBH[74]) (3-shot)에 대한 전체 결과를 보고합니다.

이전 작업(일반적으로 \(\leq 2\)T)에 비해 훨씬 더 많은 수의 토큰(3.1T)에 대한 훈련을 통해 표 2와 같이 벤치마크 전반에 걸쳐 상당한 성능 향상을 관찰했다. 그러나 특히 수학 및 코딩과 관련된 작업에서 우리의 모델과 기존 오픈 소스 및 근접 소스 모델 간에 여전히 식별 가능한 차이가 있다는 점에 유의하는 것이 중요하다. 이러한 영역의 성능은 지속적인 사전 훈련 및 명령어 미세 조정에 의해 크게 향상될 수 있으므로 초기 설계 선택을 할 때 사전 훈련 코퍼스에 광범위한 수학적 및 코딩 콘텐츠를 통합하는 것을 자제했다. 앞으로 수학 및 코딩 기능이 강화된 모델을 출시할 계획입니다.

#### 6.1.2 Discussions

**모델 규모에서 획득**: Yi-34B는 동일한 사전 훈련 말뭉치를 사용했지만 Yi-6B에 비해 상당한 성능 향상이 있음을 관찰합니다. Tab을 참조하여 코드 및 수학 벤치마크에서 모델 크기가 클수록 성능이 향상됩니다. 3은 상식 추론, 읽기 이해 또는 지식에 초점을 맞춘 벤치마크와 비교된다.
**데이터 품질**: Yi-34B 또는 Qwen-14B와 같은 더 높은 품질의 사전 훈련 데이터의 작은 모델은 일반적으로 더 큰 크기의 모델보다 더 나은 성능을 보여주지만 (아마도) Falcon-180B와 같은 더 낮은 품질의 데이터(Falcon-180B의 초점이 스케일링 쪽에 있을 수 있지만, 이는 분명히 그 자체로 중요한 가치)입니다.
**GPT-4와 오픈 소스 LLM 간의 간격**: 탭을 기반으로 합니다. 2, 우리는 오픈 소스 LLM이 다양한 벤치마크에서 GPT-4 및 GPT-3.5의 성능에 여전히 뒤처져 있다는 점에 주목한다. 그러나 대표적인 이중 언어 LLM(예: Qwen-14B 및 Yi-34B)은 C-Eval[31], CMMLU[42] 및 가오카오[90]를 포함한 중국 지식 관련 벤치마크에서 GPT-4의 성능을 일치시키거나 능가할 수 있다. 그러나 BBH[72], 코드(HumanEval), 수학(MATH)과 같은 추론 관련 벤치마크에서 GPT-4와 오픈 소스 모델 사이에는 여전히 큰 격차가 있다.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Model** & **Size** & ** GSM8k** & **MATH** & **Human-Eval pass@1** & **MBPP pass@1** \\ \hline
**GPT-3.5** & - & 57.1 & 14.0 & 48.1 & 61.4 \\
**GPT-4** & - & **92.0** & **40.2** & **67.0** & **63.6** \\ \hline
**Falcon** & 180B & 54.4 & - & 0.61 & 47.0 \\ \hline
**Qwen** & 7B & 51.7 & 11.6 & 29.9 & 34.0 \\  & 14B & 61.3 & 24.8 & 32.3 & 48.9 \\ \hline
**Baichuan 2** & 7B & 24.5 & 5.6 & 18.3 & 28.3 \\  & 13B & 22.1 & 10.1 & 20.7 & 26.1 \\ \hline
**LLaMA 2** & 7B & 16.7 & 3.3 & 12.8 & 14.8 \\  & 34B & 42.2 & 6.2 & 22.6 & 33.0 \\  & 70B & 56.8 & 13.5 & 31.7 & 45.0 \\ \hline
**Mistral** & 7B & 47.5 & 11.3 & 30.5 & 47.5 \\ \hline
**InternLM** & 20B & 62.9 & 10.9 & 28.1 & 41.4 \\ \hline
**Skywork** & 7B & 55.8 & 7.8 & 13.4 & 22.8 \\ \hline
**Yi** & 6B & 32.5 & 4.6 & 15.9 & 26.3 \\  & 34B & 67.2 & 14.4 & 23.2 & 41.0 \\ \hline \hline \end{tabular}
\end{table}
표 3: GSM8k, MATH, Human-Eval, MBPP에 대한 모델 비교.

#### 6.1.3 In-Context Learning Study

우리는 더 나아가 상황 내 학습 능력, 즉 소수의 표시 입력-출력 데모를 감안할 때 기본 함수를 추론하는 능력을 조사한다. 우리는 가중합의 선형계수를 추론하는 작업을 고려한다. 구체적으로 \(y=w_{1}x_{1}+w2x_{2}+...+ w_{n}x_{n}\, 우리의 소샷 시연은 \(x_{1},x_{2},...,x_{n},y\)이고, 우리는 새로운 입력 집합 \(x\)이 주어졌을 때 \(y\)을 예측하여 (암묵적으로) \(w_{1},w_{2},...,w_{n}\)을 추론하도록 모델을 요청한다. (a)를 사용한다. 모델 예측 \(y\)과 지상 진리 \(y^{*}\)의 절대 차이, 즉 \(|y-y^{*}|\)를 연속 척도로 사용하고 (b)를 사용한다. 불연속 측정으로 정확한 일치 \(y==y^{*}\)입니다. 우리는 또한 대부분의 모델이 덧셈과 뺄셈에 대해 합리적으로 잘 수행하므로 교란 요인으로서 산술을 수행하는 능력을 배제할 수 있다는 점에 주목한다.

결과는 그림 3에 나와 있습니다. 선형 계수를 [1, -1]로 설정할 때 Yi 34B와 LLaMA-2 70B가 정답 정확 일치의 가장 좋은 항을 수행한다는 것을 알 수 있습니다. 선형 계수의 수를 [1, 1, 1, 1, 1]로 증가시키면 목표와의 차이는 더 연속적이지만 큰 모델(LLaMA-2 70B 및 Mixtral)만이 정확한 일치에서 좋은 점수를 얻을 수 있는 창발 행동을 관찰한다. 이러한 관찰은 In-context 학습에 대한 Yi-34B의 성능에 대한 부수적인 증거를 제공하고 추가 스케일링이 모델이 In-context 학습에 의해 더 복잡한 함수를 추론할 수 있게 할 수 있음을 나타낸다.

그림 3: 가중 합계의 선형 계수를 추론하여 언어 모델의 상황 내 학습 능력을 평가한다. 창출 능력이 측정의 인공물인지에 대한 논의를 고려하여 [65] 연속 측정으로 목표와의 차이(목표 수 - 모델 예측)를 사용하고 불연속 측정으로 정확한 일치(목표 수 == 모델 예측)를 사용한다. A: 두 개의 선형 계수가 있을 때, Yi-34B는 목표 수와의 차이로 측정할 때 가장 잘 수행한다. B: 선형 계수의 수를 5로 증가시키면, 충분히 큰 모델(LLaMA2 70B 및 Mixtral 8x7B)만이 의미 있는 정확한 일치를 달성할 수 있으며, 이는 상황 내 학습 복합 기능이 창발적 능력임을 보여준다.

### 채팅 모델 성능

이 섹션에서는 채팅 모델의 자동 및 인간 선호도 평가를 보고한다. 우리는 응답을 생성하기 위해 탐욕스러운 디코딩을 사용한다. 자동 평가 벤치마크의 경우 모델의 생성된 출력에서 답변을 추출하고 정확도를 계산한다. 평가 과정에서 서로 다른 프롬프트가 결과에 다양한 영향을 미친다는 것을 관찰했다. 따라서 동일한 질문 세트에 대해 동일한 프롬프트를 사용하여 모든 모델을 평가하여 가능한 한 공정하고 편향되지 않은 결과를 보장한다.

#### 6.2.1 Automatic Evaluations

자동 평가를 위해 Sec. 6.1.1에 자세히 설명된 기본 모델에 대해 동일한 벤치마크를 사용한다. 제로-샷 및 소수-샷 방법을 모두 사용하지만 일반적으로 제로-샷이 채팅 모델에 더 적합하다. 우리의 평가는 명시적으로 또는 암시적으로 (소수 샷 예제의 포맷과 같은) 명령을 따르면서 응답을 생성하는 것을 포함한다. 그런 다음 생성된 텍스트에서 관련 답변을 분리합니다. 기본 모델과 달리 GSM8K 및 BBH 데이터 세트에 대한 제로 샷 평가를 위해 CoT(Chain-of-Think) 접근법을 사용하여 해답에 도달하기 전에 모델을 숙고하도록 안내한다.

결과는 탭에 표시됩니다. 도 4는 인간의 명령을 이해하고 적절한 명령 후속 응답을 생성하는 데 있어 채팅 모델의 효과를 보여준다. 특히 4-비트 양자화는 메모리 요구량을 크게 감소시키지만 모델 성능은 거의 떨어지지 않기 때문에 4-비트 양자화 결과를 강조한다. 이 관찰은 소비자 등급 장치에서 모델을 제공하는 기초 역할을 한다.

굿하트의 원칙에 따라 측정 척도가 우리의 추구의 대상이 되면 신뢰할 수 있는 평가 기준이 되는 역할을 중단한다. 결과적으로 벤치마크에 대한 평가 결과는 정렬 훈련이 기본 모델의 기본 지식과 능력에 해로운 영향을 미치지 않도록 하기 위해 독점적으로 사용된다. 우리는 벤치마크 성능 향상을 목표로 채팅 모델의 목표 최적화에 참여하지 않습니다.

모델 역량의 일반화 가능성을 추가로 평가하기 위해 xAI Groke 팀이 먼저 제안한 다음 Paster[55]에 의해 재현된 2023 헝가리 고등학교 수학 기말고사 문제에 적용하여 수학적 계산 능력에 대한 평가를 수행했다. 이 평가는 우리 모델이 수학적으로 지향된 훈련 데이터 세트에 과적합 징후를 나타내는지 여부를 결정하기 위한 목적으로 수행되었다. 결과는 다음과 같다. 도 4는 Yi-34B-Chat이 GSM8K와 헝가리 수학 시험 모두에서 영감을 주는 공연을 한다는 것을 보여준다. 그러나, Yi-6B-Chat은 (GSM8K와 헝가리 수학 시험 모두에서) 강력한 수학적 능력을 나타내지 않는다는 점에 주목하라. 우리는 더 작은 모델이 SFT 단계에서 해당 능력을 활성화하기 위해 더 많은 데이터가 필요할 수 있다고 추측한다.

#### 6.2.2 인간 평가

이 섹션에서는 모델의 효과와 안전을 보장하기 위해 측면을 고려하여 모델의 대화 능력에 대한 평가를 수행했다. 우리는 알파카-에벌[21], 벨-에벌[88], MT-벤치[93]와 같은 커뮤니티의 오픈 소스 평가 데이터 세트를 수집했다. 또한 채팅 모델의 대화 능력을 종합적으로 평가하기 위해 다양한 난이도의 데이터를 수집하고 구성하여 유용하고 무해한 평가 데이터 세트를 구축했다.

그러나 공공 평가 집합이든 자체 구축 평가 집합이든 평가 결과는 평가 기준과 프롬프트의 설계에 의해 크게 영향을 받는다. 우리의 내부 평가 결과는 다른 모델에 불공평할 수 있어 우리 모델의 진정한 능력 수준을 정확하게 나타내기가 어렵다. 따라서 여기서는 채팅 모델의 현재 대화 능력을 입증하기 위해 외부 평가 결과만 제시한다. 우리는 다음을 고려한다: (1). AlapcaEval1[44]는 승률을 계산하기 위해 다빈치003[21]로부터의 참조 응답들에 대해 특정된 모델의 응답들을 비교함으로써 모델들의 영어 회화 능력들을 평가하도록 설계되어 있다; (2). LMSys2[93] 대화 플랫폼을 통해 상이한 모델들의 응답들을 보여주는 챗봇 아레나는, 그 다음, 사용자들에게 그들의 선호도에 기초하여 선택을 하도록 요청하고, 그 다음, Elo 스코어를 계산한다;(3). 반면 슈퍼클루3는 모델의 중국어 능력을 종합적으로 평가하기 위한 리더보드다.

각주 3: [https://www.superclueai.com/](https://www.superclueai.com/)

탭. 5는 우리가 고려하는 세 가지 제3자 평가에서 Yi-34B-Chat의 성능 결과를 제시하며, 그 결과의 컷오프 날짜는 2023년 12월 21일이다. 데이터는 GPT-4에 비해 여전히 격차가 있지만, 우리의 모델이 능숙한 이중 언어(중국어 및 영어) 대화 능력을 나타내고 사용자 선호도와 잘 일치한다는 것을 보여준다. 다양한 모델의 추가 비교 결과는 공식 웹사이트에서 검토하기 위해 액세스할 수 있다.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Model** & **Size** & **MMLU** & **CMMLU** & **C-Eval(val)** & **TruthfulQA** & **BBH** & **GSM8K** \\  & \multicolumn{3}{c}{**0-shot / 5-shot 0-shot / 5-shot 0-shot / 5-shot**} & **0-shot & **0-shot / 3-shot** & **0-shot / 3-shot** & **0-shot / 4-shot** \\ \hline
1LaMA2-Chat & 13B & 50.9 / 47.3 & 27.5 / 35.1 & 27.9 / 35.9 & 36.8 & 32.9 / 58.2 & 36.9 / 2.7 \\  & 70B & 59.4 / 59.9 & 36.1 / 41.0 & 35.0 / 41.3 & 54.0 & 42.4 / 58.5 & 47.1 / 58.7 \\ \hline Baichuan2-Chat & 13B & 55.1 / 50.1 & 58.6 / 59.5 & 56.0 / 54.8 & 49.0 & 38.8 / 47.2 & 45.7 / 23.3 \\ \hline Qwen-Chat & 14B & 64.0 / 65.0 & 67.7 / 70.6 & 66.1 / 70.1 & 52.5 & 49.7 / 55.0 & 59.5 / 61.2 \\ \hline InterLM-Chat & 20B & 55.6 / 57.4 & 53.6 / 53.8 & 51.2 / 53.6 & 51.8 & 42.4 / 36.7 & 15.7 / 43.4 \\ \hline AquilaChat2 & 34B & 65.2 / 66.7 & 67.5 / 70.0 & **83.0 / 89.4** & **64.3** & 20.1 / 34.3 & 11.5 / 48.5 \\ \hline Yi-Chat & 6B & 58.2 / 61.0 & 69.4 / 74.7 & 68.8 / 74.2 & 50.6 & 39.7 / 47.2 & 38.4 / 44.9 \\ Yi-Chat-8bits(GPTQ) & 6B & 58.3 / 61.0 & 69.2 / 74.7 & 69.2 / 73.9 & 49.9 & 40.4 / 47.3 & 39.4 / 44.9 \\ Yi-Chat-4bits(AWQ) & 6B & 56.8 / 59.9 & 67.7 / 73.3 & 67.5 / 72.3 & 50.3 & 37.7 / 43.6 & 35.7 / 38.4 \\ \hline Yi-Chat & 34B & **67.6** / 73.5 & **79.1 / 81.3** & 77.0 / 78.5 & 62.4 & 51.4 / **71.7** & **71.7** / **76.0** \\ Yi-Chat-8bits(GPTQ) & 34B & 66.2 / **73.7** & 79.1 / 81.2 & 76.8 / 79.0 & 61.8 & **52.1** / 71.0 & 70.7 / 75.7 \\ Yi-Chat-4bits(AWQ) & 34B & 65.8 / 72.4 & 78.2 / 80.5 & 75.7 / 77.3 & 61.8 & 48.3 / 69.4 & 70.5 / 74.0 \\ \hline \hline \end{tabular}
\end{table}
표 4: 오픈 소스 채팅 모델과 비교하여 자동 벤치마크에 대한 전반적인 성능. 4-비트 양자화는 메모리 요구량을 감소시키지만 모델 성능은 거의 떨어지지 않기 때문에 4-비트 양자화 결과를 강조한다. 이 관찰은 RTX4090과 같은 소비자 등급 장치에서 모델을 제공하는 기초 역할을 한다.

그림 4: Yi의 헝가리 수학 시험 결과.

또한 데이터 크기 조정 시 선호도 증가 속도를 비교하여 데이터 품질을 입증한다. 를 더 포함할 수 있다. 도 5를 참조하면, UltraChat [19] 및 그 정제된 버전 UltraChat 200K와 비교할 때, Yi 데이터를 스케일링할 때 성능 개선의 분명한 경향을 볼 수 있다.

## 7 Capability Extension

이 절에서는 Yi 기반 모델을 200K의 긴 컨텍스트로 확장하고 시각적 이해 능력을 갖추고 깊이 업스케일링을 통해 6B 모델을 향상시키는 사후 훈련 방법에 대해 논의한다.

### Long Context Modeling

우리의 긴 컨텍스트 솔루션은 연속 사전 훈련과 미세 조정 단계로 구성되며 둘 다 가볍다. 우리는 200K 입력 컨텍스트 내에서 정보를 활용하는 잠재력이 이미 기본 모델에 존재한다는 기본 가설을 세우고(Fu 등 22와 동일), 이러한 능력을 "잠금 해제"하는 사전 훈련 단계를 계속하고, 니들-인-어-헤이스택 테스트에 대한 강력한 성능으로 입증된 다음, 미세 조정 단계는 인간의 지시 및 선호를 따르기 위해 응답 스타일을 추가로 적응시킨다.

계속 사전 훈련 우리는 시퀀스 병렬[43]과 분산 주의력을 사용하여 전체 주의 모델을 계속 사전 훈련한다. 즉, 우리는 희박하거나 선형적인 주의력을 사용하지 않고 완전한 주의력의 무차별적 실행을 사용한다. 우리는 (1)의 데이터 혼합물에 대해 Yi 6B/34B 기본 모델을 계속 사전 훈련한다. 원본 사전 훈련 데이터는 섹션 2에 소개된 바와 같다; (2). 길이-업샘플링된 긴 컨텍스트 데이터 - 긴 문서는 대부분 책에서 가져온 것입니다. (3). 다문서 질의응답 합성 데이터로서, 질의응답이 답변 전에 관련 단락의 암송을 포함하는 QA 쌍을 구성한다. 우리의 데이터 접근 방식은 대부분 Fu et al. [22]와 Yu et al. [87]의 데이터 엔지니어링 실습을 따른다. 우리는 4M 배치 크기를 가진 5B 토큰에 대해 모델을 계속 사전 훈련하며, 이는 100개의 최적화 단계로 변환된다. Fu et al. [22]의 동시 작업과 정렬하면 그림 6에서 볼 수 있듯이 이러한 경량 연속 사전 훈련이 이미 니들-인-어-헤이스택 테스트에서 강력한 성능을 가능하게 할 수 있음을 관찰한다.

감독된 FinetuningWe는 짧은 컨텍스트 SFT 데이터와 긴 컨텍스트 문서 질의 응답 데이터를 혼합한다. 우리는 문서 QA를 구성하기 위해 모델 지원 자동화 방법(즉, 합성 데이터)을 사용한다. 구체적으로, 여러 문서를 무작위로 시퀀스로 연결하고 긴 시퀀스에서 하나 이상의 문단을 샘플링하고 채팅 모델에 질문 및 답변 쌍을 구성합니다.

그림 5: SFT 데이터 스케일링 곡선. 울트라챗 및 그 정제된 버전 울트라챗 200K와 비교하여, 우리의 SFT 데이터는 명확한 스케일링 이점을 보여준다. 우리는 가파른 경사를 데이터 품질 탓으로 돌립니다.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **Size** & **AlpacaEval** & **LMSys Chatbot Arena** & **SuperClue** \\ \hline GPT-4-Turbo & - & **97.7** & **1243** & **89.79** \\ GPT-3.5-Turbo & - & 89.37 & 1117 & 59.39 \\ LLaMA2-Chat & 70B & 92.66 & 1077 & - \\ Yi-Chat & 34B & 94.08 & 1110 & 71.87 \\ \hline \hline \end{tabular}
\end{table}
표 5: 다른 오픈 소스 채팅 모델과의 인간 평가 비교.

[MISSING_PAGE_FAIL:15]

Yi-VL 모델은 3단계 훈련 과정을 거친다:

**1단계:**: 이미지 해상도 \(224^{2}\)를 사용하여 ViT 및 프로젝션 모듈의 매개 변수를 학습합니다. 학습은 LAION-400M [66]로부터 백만 개의 이미지-텍스트 쌍을 포함하는 실질적인 데이터세트를 활용한다. 주요 목적은 지정된 아키텍처 내에서 ViT의 지식 획득을 강화하고 ViT와 LLM 간의 더 나은 정렬을 달성하는 것이다.
**2단계:**: ViT의 이미지 해상도를 \(448^{2}\)로 확장하여 복잡한 시각적 세부 정보를 식별하는 모델의 기능을 더욱 향상시킵니다. 이 단계에서 사용된 데이터셋은 LAION-400M에서 파생된 200만 개의 이미지-텍스트 쌍을 포함한다. 또한 다양한 소스, _e.g._, CLLaVA [45], LLaVAR [91], Flickr [85], VQAv2 [25], RefCOCO [37], Visual7w [95] 등의 약 4.8\의 이미지 텍스트 쌍을 통합합니다.
**3 단계:**: 전체 모델의 매개 변수가 학습됩니다. 주요 목표는 멀티모달 채팅 상호작용에서 모델의 숙련도를 향상시켜 시각적 및 언어적 입력을 원활하게 통합하고 해석할 수 있는 능력을 부여하는 것이다. 이를 위해 학습 데이터 세트는 GQA [32], VizWiz VQA [26], TextCaps [71], OCR-VQA [51], Visual Genome [39], ShareGPT4V [6] 등을 포함하여 약 100만 개의 이미지-텍스트 쌍을 포함하는 다양한 범위의 소스를 포함한다. 데이터 균형을 보장하기 위해 단일 소스의 최대 데이터 기여도에 상한을 부과하여 \(50,000\) 쌍 이하로 제한한다.

1단계와 2단계에서는 전체 배치 크기, 학습률, 그래디언트 클립 및 epoch 수를 각각 \(4096\), \(1\mathrm{e}{-4}\), \(0.5\) 및 \(1\)로 설정하였다. 3단계에서 이러한 매개 변수는 \(256\), \(2\mathrm{e}{-5}\), \(1.0\) 및 \(2\)로 조정됩니다. 훈련은 \(128\) NVIDIA A100 GPU를 사용합니다. 총 훈련 시간은 Yi-VL-6B의 경우 \(3\)일, Yi-VL-34B의 경우 \(10\)일에 달했다.

표 7은 Yi-VL의 릴리스에 의한 MMMU 테스트 세트 리더보드를 보여준다. 우리는 이 지역이 현재 활발히 연구 중이며 커뮤니티의 발전과 일치하여 Yi-VL의 업데이트를 지속적으로 개선할 것이라는 점에 주목한다.

### Depth Upscaling

스케일링 법칙에 대한 최근 연구[29; 30; 36]는 계산 예산, 모델 크기 및 데이터 크기가 증가함에 따라 모델 성능의 예측 가능한 개선을 강조했다. 그러나 계산 예산을 확장할 때 모델과 데이터 크기 사이의 가장 효과적인 자원 분포를 식별하는 것은 스케일링 법률 분야에서 여전히 만만치 않은 과제로 남아 있다. 또한 DeepSeek-AI et al. [17]에 의해 수행된 연구는 모델 스케일링을 위한 증가된 계산 예산의 할당이 이용 가능한 데이터의 품질에 비례해야 함을 강조했다. 이러한 통찰력에 비추어, 우리는 일련의 단계적 훈련 프로세스를 통해 데이터와 모델 크기 사이의 자원 할당을 동적으로 조정하는 것을 목표로 하는 새로운 접근법을 제안한다. 이 전략은 스케일링 법칙에 따라 데이터 특성과 모델 크기 사이의 균형을 반복적으로 미세 조정함으로써 모델 훈련 효율성과 성능을 모두 향상시킨다.

그림 7: Yi-VL 모델의 아키텍처입니다. 기호는 세 가지 훈련 단계에서 다양한 모듈의 훈련 상태를 나타내기 위해 사용된다: 화재 아이콘()은 모듈의 파라미터가 훈련될 수 있음을 나타내고, 눈송이 아이콘()은 파라미터가 동결되었음을 나타낸다. 각 단계에서 ViT에 사용된 이미지 해상도 \(224^{2}\) 또는 \(448^{2}\)도 표시된다.

본 연구에서는 Kim et al. [38]의 방법론에 따라 32개의 층을 갖는 Yi-6B 기반 모델을 48개의 층을 갖는 Yi-9B 기반 모델이라고 명명된 9B 모델로 확장하는 것을 목표로 한다. 깊이 확장에는 기본 모델의 깊이를 확장한 후 향상된 모델에 대한 사전 훈련 단계를 계속하는 것이 포함된다.

우리의 조사는 복제할 계층에 대한 결정이 각 계층의 입력과 출력 사이의 코사인 유사성 점수를 평가하여 알려질 수 있음을 보여준다. 이러한 접근법은 추가 사전 훈련을 필요로 하지 않고 표적화된 모델 스케일링을 허용하여 최소한의 성능 영향만 초래한다. 성능에 대한 이러한 최소한의 영향은 그림 8에서 알 수 있듯이 복제된 레이어의 입력과 출력 사이에 있는 높은 코사인 유사성에 기인한다. 이 관찰은 이러한 레이어의 복제가 출력을 크게 변경하지 않는다는 것을 시사한다.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Model** & **Overall** & **Art** & **Business** & **Science** & **Health** & **Society** & **Engineering** \\ \hline GPT-4V & 55.7 & 65.3 & 64.3 & 48.4 & 63.5 & 76.3 & 41.7 \\ Yi-VL-34B & 41.6 & 56.1 & 33.3 & 32.9 & 45.9 & 66.5 & 36.0 \\ Qwen-VL-PLUS & 40.8 & 59.9 & 34.5 & 32.8 & 43.7 & 65.5 & 32.9 \\ Marco-VL & 40.4 & 56.5 & 31.0 & 31.0 & 46.9 & 66.5 & 33.8 \\ Yi-VL-6B & 37.8 & 53.4 & 30.3 & 30.0 & 39.3 & 58.5 & 34.1 \\ InfMIM-Zephyr-7B & 35.5 & 50.0 & 29.6 & 28.2 & 37.5 & 54.6 & 31.1 \\ SVIT & 34.1 & 48.9 & 28.0 & 26.8 & 35.5 & 50.9 & 30.7 \\ Emu2-Chat & 34.1 & 50.6 & 27.7 & 28.0 & 32.4 & 50.3 & 31.3 \\ BLIP-2 FLAN-T5-XXL & 34.0 & 49.2 & 28.6 & 27.3 & 33.7 & 51.5 & 30.4 \\ InstructBLIP-T5-XXL & 33.8 & 48.5 & 30.6 & 27.6 & 33.6 & 49.8 & 29.4 \\ LLAVA-1.5-13B & 33.6 & 49.8 & 28.2 & 25.9 & 34.9 & 54.7 & 28.3 \\ Qwen-VL-7B-Chat & 32.9 & 47.7 & 29.8 & 25.6 & 33.6 & 45.3 & 30.2 \\ SPHINX* & 32.9 & 50.9 & 27.2 & 25.3 & 34.1 & 51.2 & 27.8 \\ mPLUG-OWL2 & 32.1 & 48.5 & 25.6 & 24.9 & 32.8 & 46.7 & 29.6 \\ BLIP-2 FLAN-T5-XL & 31.0 & 43.0 & 25.6 & 25.1 & 31.8 & 48.0 & 27.8 \\ InstructBLIP-T5-XL & 30.6 & 43.3 & 25.2 & 25.2 & 29.3 & 45.8 & 28.6 \\ CogVLM & 30.1 & 38.0 & 25.6 & 25.1 & 31.2 & 41.5 & 28.9 \\ \hline \hline \end{tabular}
\end{table}
표 7: Yi-VL의 출시 시점별 MMMU 테스트 세트 성능.

그림 8: 텍스트 "비트에 대한 퀴즈 작성"에 대한 레이어별 각 토큰의 입출력 코사인 유사도 점수. 아래 그림에 표시된 16개의 새로 추가된 레이어(레이어 28-44)의 코사인 유사성 점수는 거의 1로 관찰된다.

원래 모델에서 생성된 로그입니다. 이 방법은 레이어의 내부 처리 역학에 기초하여 아키텍처를 최적화함으로써 모델의 효율적인 스케일링을 보장한다.

지속적 훈련 데이터 세트는 두 단계에 걸쳐 약 8,000억 개의 토큰으로 구성되며 약 70%가 최근에 수집되고 신중하게 선택되었다. 최종 단계에서 코드 커버리지를 향상시켜 코드 성능을 향상시켰습니다.

학습 프로세스를 최적화하기 위해 3e-5의 일정한 학습률을 유지하고 모델의 손실이 정체될 때마다 4M 토큰에서 배치 크기를 점진적으로 증가시키는 전략적 접근법을 채택한다. 배치 크기의 이러한 점진적인 조정은 확립된 Yi-6B 기본 모델 구성과 정렬하여 다른 모든 매개변수를 유지하는 것과 함께 규모에서 훈련의 문제를 해결하는 데 중요한 역할을 했다.

이러한 전략의 효과는 상식, 추론, 지식, 코딩 및 수학을 포함한 다양한 벤치마크에 걸쳐 Yi-9B 기본 모델의 성능을 자세히 설명하는 표 8에서 입증된다. 이는 특정 도메인에서 Yi-9B 기반 모델의 경쟁 우위를 강조하며 데이터 특성과 모델 크기 간의 상호 작용을 최적으로 조정하여 모델 성능을 향상시키는 방법론의 효율성을 보여준다.

## 8 최종 토론

이 보고서에서는 이 언어 모델 패밀리의 전체 스택 개발에 대해 논의한다. Yi-34B는 GPT-3.5 매칭 성능을 달성하고 소비자 등급 장치에 배포(4/8 비트 양자화 덕분에) 가능하므로 로컬 배포에 이상적인 모델이다.

Lee 사전 훈련 절차의 주요 테이크아웃은 데이터 양과 질에 관한 것이다: (1). 친칠라 최적보다 더 많은 양의 데이터에서 모델을 훈련하면 명확하고 일관된 성능 이득을 얻을 수 있으므로 모든 사전 훈련 팀에 적극 권장합니다. 우리의 모델은 3.1T 토큰에 대해 훈련되었지만, 더 많은 양의 데이터로 인해 모델 성능을 계속 향상시킬 수 있다(즉, 모델이 3.1T에서 포화되지 않음); (2). 사전 훈련 데이터 품질과 관련하여 가장 중요한 두 가지 요소는 데이터의 출처(예: 텍스트가 전문적인 사용을 위해 생성되었는지 또는 비공식 소셜 미디어 게시인지)와 데이터 청소 세부 정보(예: 필터링 및 중복 제거의 강도)라고 믿는다. 데이터 청소는 매우 복잡한 파이프라인이며 광범위한 그리드 검색 스타일 최적화를 수행하는 것이 매우 어렵기 때문에 현재 솔루션은 여전히 개선의 여지가 있을 수 있다.

Lee finetuning 절차에서 가장 중요한 것은 적은 양의 데이터(\(\leq\) 10K)에 대해, 경우에 따라, 여러 번의 반복을 통해, 기계 학습 엔지니어가 직접, 그리고 실제 사용자 피드백으로부터 개선하는 것이다. 이 접근법은 처음에 FLAN 시리즈[9]에 의해 도입된 다음 울트라챗 시리즈[19]에 의해 도입된 명령어-스케일링 접근법으로부터 명백히 벗어난다.

본 논문의 결과를 통해 알 수 있듯이, 언어 모델의 실제 배치를 위한 핵심 능력으로 간주되는 추론 능력은 사전 훈련 데이터의 양이 고정될 때 모델 규모와 강한 상관 관계가 있다. 현재 결과를 감안할 때 철저히 최적화된 데이터를 사용하여 모델 매개변수를 계속 확장하면 다음 버전에서 더 강력한 프론티어 모델로 이어질 것이라고 믿습니다.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Model** & **Arc-C** & **HellaSwag** & **MMLU** & **Winogrande** & ** GSM8K** & **MATH** & **HumanEval** & **MBPP** \\ \hline
**Vi-6B** & 50.3 & 74.4 & 63.2 & 71.3 & 32.3 & 32.5 & 4.6 & 15.9 & 26.3 \\
**Vi-9B Init** & 52.1 & 73.3 & 63.0 & 69.4 & 31.3 & 4.1 & 12.8 & 25.8 \\
**Vi-9B** & **55.6** & **76.4** & **68.4** & **73.0** & **52.3** & **15.9** & **39.0** & **54.4** \\ \hline \hline \end{tabular}
\end{table}
표 8: Yi-6B와 Yi-9B 사이의 성능: Arc Challenge (25-shot), HellaSwag (10-shot) MMLU (5-shot), Winogrande (5-shot), GSM8K (5-shot), MATH (4-shot), HumanEval pass@1, MBPP pass@1 (3-shot). Yi-9B Init는 추가 훈련 없이 층 12-28을 복제함으로써 Yi-6B로부터 깊이 방향으로 업스케일링하고 있다.

작성자 목록 및 기여도

우리 팀원들은 다음과 같은 관점에서 Yi의 발전에 기여한다.

* Frontier Research
* Machine Learning Infrastructure
* Pretraining
* 파인튜닝 및 AI 정렬

* Multimodal
* 안전 및 책임 있는 AI
* Deployment

우리는 팀원을 알파벳 순서로 나열합니다. 모든 저자들이 이 작품에 동등하게 기여했다.

* Alex Young
* Bei Chen
* Chao Li
* Chengen Huang
* Ge Zhang
* 관웨이 장
*Heng Li
* 장청주
* Jianqun Chen
* 징창
* 카이동유
* Peng Liu
* 치앙류
* Shawn Yue
* Senbin Yang
* Multimodal
* 안전 및 책임 있는 AI
* Deployment
* Shiming Yang
* Tao Yu
* Wen Xie
* 원하오황
* 샤오후이
* 샤오이 렌
* 신야오 니우
* 펭청 니
* 유치쉐
* 유동류
* Yue Wang
* Yuxuan Cai
* Zhenyu Gu
* Zhiyuan Liu
* Zonghong Dai

## References

* [1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. _arXiv preprint arXiv:2305.13245_, 2023.
* [2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program Synthesis With ILarge Language Models. _arXiv preprint arXiv:2108.07732_, 2021.
* [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen Technical Report. 09 2023. URL [https://arxiv.org/pdf/2309.16609.pdf](https://arxiv.org/pdf/2309.16609.pdf).
* [4] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about Physical Commonsense in Natural Language. _ArXiv_, abs/1911.11641, 2019. URL [https://api.semanticscholar.org/CorpusID:208290939](https://api.semanticscholar.org/CorpusID:208290939).
* [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [6] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.
* [7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating Large Language Models Trained on Code. _CoRR_, abs/2107.03374, 2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).
* [8] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. QuAC : Question Answering in Context, 2018.
* [9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.
* [10] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions, 2019.
* [11] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge, 2018.
* [12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training Verifiers to Solve Math Word Problems. _arXiv preprint arXiv:2110.14168_, 2021.

* [13] Together Computer. Redpajama: an open dataset for training large language models, 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).
* [14] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
* [15] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In _Advances in Neural Information Processing Systems_, 2022.
* [16] Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, and William Cohen. FiDO: Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference. _arXiv preprint arXiv:2212.08153_, 2022.
* [17] DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism. 2024.
* [18] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm: int8 (): 8-bit matrix multiplication for transformers at scale. _arXiv preprint arXiv:2208.07339_, 2022.
* [19] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. _arXiv preprint arXiv:2305.14233_, 2023.
* [20] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition, 2023.
* [21] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.
* [22] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. _arXiv preprint arXiv:2402.10171_, 2024.
* [23] Gemini Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: A family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [24] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. _arXiv preprint arXiv:2209.14375_, 2022.
* [25] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6904-6913, 2017.
* [26] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3608-3617, 2018.

* [27] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. _CoRR_, abs/2009.03300, 2020. URL [https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300).
* [28] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. _arXiv preprint arXiv:2103.03874_, 2021.
* [29] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewowo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. 2020.
* [30] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* [31] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. _arXiv preprint arXiv:2305.08322_, 2023.
* [32] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6700-6709, 2019.
* [33] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL [https://doi.org/10.5281/zenodo.5143773](https://doi.org/10.5281/zenodo.5143773).
* [34] Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Sompalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. Neptune: Noisy embeddings improve instruction finetuning. _arXiv preprint arXiv:2310.05914_, 2023.
* [35] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llvm via a human-preference dataset, 2023.
* [36] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. 2020.
* [37] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In _Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)_, pages 787-798, 2014.
* [38] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim. Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling. 2023.
* [39] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _International journal of computer vision_, 123:32-73, 2017.
* [40] Taku Kudo and John Richardson. SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing. _arXiv preprint arXiv:1808.06226_, 2018.
* [41] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. _arXiv preprint arXiv:2309.06180_, 2023.

* [42] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring Massive Multitask Language Understanding in Chinese. _arXiv preprint arXiv:2306.09212_, 2023.
* [43] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long sequence training from system perspective. _arXiv preprint arXiv:2105.13120_, 2021.
* [44] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaceval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval), 2023.
* [45] LinkSoul-AI. Chinese llava. [https://github.com/LinkSoul-AI/Chinese-LLAVA](https://github.com/LinkSoul-AI/Chinese-LLAVA), 2023.
* [46] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
* [47] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _arXiv preprint arXiv:2304.08485_, 2023.
* [48] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. _arXiv preprint arXiv:2312.15685_, 2023.
* [49] Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. #instag: Instruction tagging for analyzing supervised fine-tuning of large language models, 2023.
* [50] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering, 2018.
* [51] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In _2019 international conference on document analysis and recognition (ICDAR)_, pages 947-952. IEEE, 2019.
* [52] Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen. CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages. _arXiv preprint arXiv:2309.09400_, 2023.
* [53] OpenAI. ChatML, 2022. URL [https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md](https://github.com/openai/openai-python/blob/e389823ba013a24b4c32ce38fa0bd87e6bccae94/chatml.md).
* [54] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training Language Models to Follow Instructions with Human Feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [55] Keiran Paster. Testing language models on a held-out high school national finals exam. [https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam](https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam), 2023.
* [56] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only, 2023.
* [57] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently Scaling Transformer Inference. _Proceedings of Machine Learning and Systems_, 5, 2023.
* [58] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. _arXiv preprint arXiv:2112.11446_, 2021.

* [59] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023.
* [60] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16. IEEE, 2020.
* [61] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text, 2016.
* [62] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An Adversarial Winograd Schema Challenge at Scale, 2019.
* [63] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. SocialIQA: Commonsense Reasoning about Social Interactions, 2019.
* [64] Nikhil Sardana and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. _arXiv preprint arXiv:2401.00448_, 2023.
* [65] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a mirage? _Advances in Neural Information Processing Systems_, 36, 2024.
* [66] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. _arXiv preprint arXiv:2111.02114_, 2021.
* [67] Noam Shazeer. Fast Transformer Decoding: One Write-Head is All You Need. _arXiv preprint arXiv:1911.02150_, 2019.
* [68] Noam Shazeer. GLU Variants Improve Transformer. _arXiv preprint arXiv:2002.05202_, 2020.
* [69] Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. Byte Pair Encoding: A Text Compression Scheme That Accelerates Pattern Matching. Technical report, Technical Report DOI-TR-161, Department of Informatics, Kyushu University, 1999.
* [70] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. _arXiv preprint arXiv:1909.08053_, 2019.
* [71] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 742-758. Springer, 2020.
* [72] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmuller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Erfat, Aykat Erdem, Ayla Karakas, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartomiej Bojanowski, Bathan Ozyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri Ramirez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Dami Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi,Daniel Levy, Daniel Mosegui Gonzalez, Danielle Perszyk, Danny Hernandez, Danqi Chen, and Daphne Ippolito et al. (351 additional authors not shown). Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models, 2023.
* Su et al.(2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Rotary Position Embedding을 가진 Enhanced Transformer. _ arXiv preprint arXiv:2104.09864_, 2021.
* Suzgun 등(2022) Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging BigBench Tasks and Whether Chain-of-Thought can Solve them. _ arXiv preprint arXiv:2210.09261_, 2022.
* Talmor 등(2019) Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 상식QA: 상식 지식을 목표로 하는 질문 응답 챌린지, 2019.
* Touvron 등(2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.
* 투브론 등(2021) 휴고 투브론, 루이 마틴, 케빈 스톤, 피터 알버트, 암자드 알마하리, 야스민 바베이, 니콜라 바슐리코프, 수미 바트라, 슈루티 보살레, 단 비켈, 루카스 블레처, 크리스티안 칸톤 페러, 모야 첸, 기옌 쿠쿠룰, 다비드 에시오부, 주드 페르난데스, 제레미 푸, 원린 푸, 브라이언 풀러, 신시아 가오, 제레미 푸, 베다누즈 고셰이, 사그하르 호세이니, 루이 호우, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 마디안 하바사, 이사벨 클루만, 아르템 고레네프, 신시아 가오, 사그하르 호세이니, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 마디안 카르네프, 이사벨 클루만, 마리-안네 라쇼, 티보트 라브릴, 제냐 리, 다이아나 리스코비치, 잉하이 루, 라마 2: 오픈 파운데이션 및 미세 조정된 채팅 모델, 2023.
* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. 고메즈, 루카스 카이저 일리아 폴로수킨 주목해 주십시오. _ 신경 정보 처리 시스템의 발전_, 06 2017. URL [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf).
* Wenzek 등(2019) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: 웹 크롤 데이터에서 고품질 단일 언어 데이터 집합 추출 _ arXiv preprint arXiv:1911.00359_, 11 2019. URL [https://arxiv.org/pdf/1911.00359.pdf](https://arxiv.org/pdf/1911.00359.pdf).
* Wenzek 등(2019) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: 웹 크롤 데이터에서 고품질 단일 언어 데이터 집합 추출 _ arXiv preprint arXiv:1911.00359_, 2019.
* Wu et al. (2023) Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. 변압기 모델에 대한 int4 양자화에 대한 이해: 지연 속도 향상, 합성 가능성 및 고장 사례. _ arXiv preprint arXiv:2301.12017_, 2023.
* Xiong 등(2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. _ arXiv preprint arXiv:2309.16039_, 2023.
* Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 마법사: 복잡한 지침을 따르도록 대규모 언어 모델에 권한을 부여합니다. _ arXiv preprint arXiv:2304.12244_, 2023.
*양 등(2021) 아이위안 양, 빈샤오, 빙닝왕, 보롱장, 세비안, 차오인, 첸쉐훤, 다판, 다이안 왕, 동옌, 판양, 페이뎅, 펑왕, 광웨이아이, 궐성동, 하이저우저우, 항허, 하오쑹, 후이류, 지밍지, 지안시에, 준타오다이, 쿤팡, 레이수, 량송, 리윤루, 루야오마, 망왕, 미켈류, 밍안린, 누올란니에, 페이동궈, 뤄양순, 타오장, 톈펑리, 톈유리, 위청, 웨이펑천, 샹룽정, 샤오촨왕, 샤오시천, 신멘, 신유, 슈하이판, 옌준심, 이딩왕, 이유리, 유신장, 유첸가오, 유펑장, 제난저우, 지잉우. 바이촨 2: 대규모 언어 모델을 엽니다. 09 2023. URL [https://arxiv.org/pdf/2309.10305.pdf](https://arxiv.org/pdf/2309.10305.pdf).
* Young et al. [2014] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014.
* Yu et al. [2022] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A Distributed Serving System for Transformer-Based Generative Models. In _16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_, pages 521-538, 2022.
* Yu et al. [2023] Yijiong Yu, Zhe Zhou, Zhixiao Qi, and Yongfeng Huang. Paraphrasing the original text makes high accuracy long-context qa. _arXiv preprint arXiv:2312.11193_, 2023.
* Yunjie et al. [2023] Ji Yunjie, Deng Yong, Gong Yan, Peng Yiping, Niu Qiang, Zhang Lei, Ma Baochang, and Li Xiangang. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. _arXiv preprint arXiv:2303.14742_, 2023.
* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Machine Really Finish Your Sentence?, 2019.
* Zhang et al. [2023] Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the Performance of Large Language Models on GAOKAO Benchmark. _arXiv preprint arXiv:2305.12474_, 2023.
* Zhang et al. [2023] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. _arXiv preprint arXiv:2306.17107_, 2023.
* Zheng et al. [2023] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, and Denny Zhou. Take a step back: Evoking reasoning via abstraction in large language models, 2023.
* Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* Zhou et al. [2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment, 2023.
* Zhu et al. [2016] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4995-5004, 2016.

<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Resthrasing Web:\n' +
      '\n' +
      '계산 및 데이터 효율적인 언어 모델링을 위한 레시피\n' +
      '\n' +
      ' "프라투시 마이니"\n' +
      '\n' +
      'Equal Contribution\n' +
      '\n' +
      '카네기멜론대학교\n' +
      '\n' +
      'pratyushmaini@cmu.edu\n' +
      '\n' +
      '&Skyler Seto; He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly\n' +
      '\n' +
      'Apple\n' +
      '\n' +
      '{sseto,hbai22,grangier,yizhe_zhang,njailtly}@apple.com\n' +
      '\n' +
      '애플 인턴 시절에 한 일\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '대형 언어 모델은 종종 구조화되지 않고 시끄럽고 표현이 좋지 않은 웹의 거대한 스크래프에서 훈련된다. 현재의 스케일링 법칙들은 그러한 데이터로부터 학습하는 것이 트레이닝되는 모델의 크기에 따라 증가하는 컴퓨팅 및 데이터 둘 다를 필요로 한다는 것을 보여준다. 이는 사전 훈련과 관련된 계산 비용과 기간이 크고 웹에서 고품질 데이터의 희소성이 임박하기 때문에 실현 불가능하다. 이 작업에서는 "위키피디아와 같은" 또는 "질문 답변 형식"과 같은 특정 스타일로 웹에서 문서를 패러프레이징하여 실제 및 합성 리프레이징에서 LLM을 공동으로 사전 학습하도록 프롬프트된 기성 명령어 조정 모델을 사용하는 **웹** 재학습 **A** 연결 **P** 재학습(**WRAP**)을 제안합니다. 먼저, 자연 소음인 C4 데이터 세트에서 **WRAP** 를 사용하면 \\(\\sim 3\\times\\)만큼 사전 훈련 속도가 빨라진다는 것을 보여 줍니다. 동일한 사전 훈련 컴퓨팅 예산에서 파일의 서로 다른 하위 집합에 걸쳐 평균 10% 이상의 복잡도를 개선하고 13개의 태스크에 걸쳐 제로 샷 질문 답변 정확도를 2% 이상 개선한다. 둘째, 학습 데이터의 구성이 OOD 설정에서 LLM의 성능에 어떻게 영향을 미칠 수 있는지에 대한 통찰력을 제공하여 재표현 스타일이 모델의 성능에 미치는 영향을 조사한다. 본 연구의 결과는 재구문 합성자료가 (i) 하류의 평가 스타일을 밀접하게 반영하는 스타일 다양성을 포함하고, (ii) 웹 스크래핑 자료보다 \'품질\'이 높기 때문에 실제 자료보다 활용도가 높기 때문이다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '대규모 언어 모델(LLM) 사전 훈련은 크게 민주화되고 개방되어 다양한 학술 실험실 및 산업계가 맞춤형 LLM을 사전 훈련할 수 있다. 그러나 이러한 모델 간의 주요 차별점은 훈련에 사용되는 데이터의 구성과 크기이다. 데이터 큐레이션 전략은 구조화되지 않거나 제대로 표현되지 않은 웹의 스크래프를 걸러내기 위해 필요하다(Eisenstein, 2013). 이러한 전략 중 일부가 공개되었지만(Brown 등, 2020; Wenzek 등, 2020; Penedo 등, 2023), 대부분의 최첨단 데이터 큐레이션 기술은 연구 커뮤니티에 알려지지 않았으며 일화적인 증거만 남아 있다. 데이터 큐레이션에 대한 연구는 여러 번의 재교육을 필요로 하므로 실용적인 개선으로 이어지는 기술을 문서화하는 데 많은 노력이 필요하다. 한편, 언어 모델에 대한 스케일링 법칙(친칠라 스케일링 법칙(Hoffmann et al., 2022))은 모델 크기가 증가함에 따라 트레이닝 컴퓨팅과 데이터 크기도 선형적으로 증가시켜야 함을 보여준다. 이는 (a) 고품질 데이터가 제한되고(Villalobos et al., 2022), 심지어 적은 수의 에폭(4개 이상)에 대해 반복하면 수익 감소 또는 오버피팅이 초래되기 때문에 실행 불가능하다(Muennighoff et al., 2023; Touvron et al., 2023; Xue et al., 2023); 및 (b) 이러한 긴 지속기간에 대한 사전 훈련은 엄청나게 비싸다.\n' +
      '\n' +
      '한편, 합성 데이터의 사용은 명령어 미세 조정, RLHF(Ouyang et al., 2022) 및 명령어 역번역(Li et al., 2023b)을 통해 사전 훈련된 LLM을 정렬하는 패러다임에서 두드러지게 되었다. 최근, 사전 훈련의 맥락에서, 합성 데이터는 Tiny Stories(Eldan & Li, 2023) 및 교과서 품질 합성 데이터(Gunasekar et al., 2023; Li et al., 2023c)와 같은 데이터 세트를 생성하는데 사용되었다. 이들은 특정 작업에서 더 큰 언어 모델만큼 성능이 뛰어난 (Phi 모델 패밀리와 같은) 더 작은 언어 모델을 훈련하는 데 사용되었다. 그러나, 그들의 데이터 생성 프로세스는 대체로 불투명하고 엄청나게 비싸며, 수십억 개의 토큰을 생성하기 위한 GPT-3.5 모델을 촉구해야 한다. 또한, 이러한 데이터 생성은 우리가 잘 수행하기를 원하는 태스크와 관련된 데이터를 구체적으로 생성함으로써 큰 "지식 편향"을 생성할 수 있다. 합성 데이터는 가능성을 보여주었지만, 이것이 합성 데이터의 더 높은 품질 특성 때문인지 또는 전략적 주제 선택 때문인지는 불분명하다(Maini, 2023).\n' +
      '\n' +
      '이 작업에서는 데이터 큐레이션에 대한 모호성에서 비롯된 세 가지 중요한 문제를 해결하려고 시도하는 **웹** 재학습 **A**ugmented Pre-training (**WRAP**) --(**i**) 어떤 데이터를 미리 훈련해야 합니까? (ii) 제한된 데이터로 어떻게 사전 훈련을 할 수 있는가? (iii) 어떻게 계산적으로 효율적으로 사전 훈련을 할 수 있는가? 특히, 오프라인 중간 크기의 LLM을 사용하여 웹에서 문서를 다시 표현하면 모델이 웹의 원시 텍스트에서 학습하는 것보다 훨씬 더 효율적으로 학습할 수 있으며 추가 웹 데이터와 상쇄할 수 없는 배포 데이터 세트에 대한 성능 향상을 설명한다. 제안된 방법은 사전 학습된 기성 LLM을 사용하여 웹 코퍼스에서 문서를 다른 스타일로 재구문화하는 것을 포함한다. 우리의 접근법의 개요는 그림 0(a)에 나와 있다.\n' +
      '\n' +
      '본 연구에서는 Gunaasekar et al. (2023)의 합성 데이터 큐레이션 과정에서 직면하는 두 가지 중요한 문제(세대 비용 및 데이터 편향)를 웹에서 기사를 변경하여 해결한다. (i) **WRAP** 은 오픈 소스를 사용 하 고 훨씬 작은 LLM (1.8B/7B v/s GPT3.5)을 사용 하 여 지식 은행으로 LLM에 의존 하지 않기 때문에 구조화되지 않은 문서 및 구문 분석이 불량 한 문서를 다른 스타일로 다시 말할 수 있습니다. (ii) 리프레이징의 특성을 유지하는 정보 덕분에, 우리는 사실적 오류 및/또는 데이터 편향에 취약할 수 있는 정보에 대해 LLM에 의존하지 않고 웹의 자연적 다양성을 활용할 수 있다. 우리의 작업은 "스타일"만으로도 다운스트림 성능에서 큰 이득을 얻을 수 있음을 보여준다.\n' +
      '\n' +
      'C4에서 **WRAP** 를 사용 하 여 13 개의 서로 다른 제로 샷 태스크 및 파일의 21 개의 서로 다른 언어 모델링 도메인에 대 한 모델 성능을 평가 하 고 합성 데이터를 사용 하 여 LLM 사전 학습을 통해 5 배 더 적은 데이터 또는 3 배 더 적은 계산으로 동등한 모델을 학습 할 수 있음을 확인 합니다. 사실, 우리의 합성 데이터 훈련 모델은 또한 여러 제로 샷 Q/A 태스크에 걸쳐 3조 토큰(10x 데이터 및 컴퓨팅)에 대해 훈련된 최근의 TinyLLama 모델을 능가한다. 또한 파일에서 \\(\\sim\\)50%의 복잡도 감소를 관찰하였으며, 전체 C4 말뭉치의 15%에 대해서만 실제 및 합성 재구문의 조합으로 학습된 350M 파라미터 모델이 전체 C4에서 1.3B 파라미터를 사전 학습하는 것보다 우수하다는 것을 알 수 있었다. 마지막으로 데이터 유출 가능성, 합성 데이터 스타일의 속성, 합성 데이터를 결합하는 방법에 대한 분석을 수행하여 **WRAP** 기반 LLM 사전 학습을 개선하였다.\n' +
      '\n' +
      '그림 1: (a) **WRAP** 레시피: 웹에서 문서를 다시 말하고 실제 데이터와 합성 데이터의 혼합물에서 LLM을 사전 훈련하도록 기성 명령어 조정 모델을 프롬프트합니다. (b) C4 및 합성 변형의 조합에 대해 트레이닝된 GPT 1.3B 모델의 제로 샷 성능. 각 단계는 1M 샘플의 배치에 해당한다. (c) 다양한 모델 크기와 사전 훈련 데이터 양에 대한 파일의 21개 하위 도메인에 대한 가중 평균 복잡도.\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '언어 모델들을 위한 신경 스케일링 법칙들 신경 스케일링 법칙들은 고정된 양의 컴퓨팅을 위한 최적의 모델 파라미터들의 수 및 트레이닝 데이터의 양을 관련시킨다. Hoffmann et al.(2022)은 언어 모델에 대한 친칠라 스케일링 법칙을 제시하여 모델의 크기와 필요한 훈련 데이터의 양 사이에 선형 관계가 있음을 증명하였다. 그들의 연구 결과는 고퍼(Rae et al., 2021)와 같은 이전 모델이 심각하게 훈련되었음을 나타낸다. 최근, Llama(Touvron et al., 2023)와 같은 모델들은 훨씬 더 많은 데이터로 트레이닝된다. 이러한 스케일링 법칙은 단일-에포크 훈련의 패러다임을 위해 도출되었다. 최근 Muennighoff et al. (2023)은 반복 데이터의 한계 효용성이 4회 이상 훈련될 때 급격히 감소한다는 것을 보여주었고, 반복 데이터에서 스케일링 법칙을 공식화했다. 동시에 Xue et al. (2023)은 사전 훈련 데이터의 작은 부분이라도 반복하면 과적합으로 이어질 수 있고 모델 성능을 감소시킬 수 있음을 보여주었다.\n' +
      '\n' +
      '사전 훈련 LLM을 위한 고품질 데이터를 선택하는 데이터 세트 선택은 여전히 활성화되고 영향을 많이 받지만 연구되지 않은 연구 영역으로 남아 있다. 예를 들어, GPT-2 모델은 적어도 3개의 업보를 받은 소셜 미디어 플랫폼인 Reddit의 모든 아웃바운드 링크에서 사전 훈련되었다(Brown et al., 2020). 이것은 문서가 _흥미롭다_, _교육적_ 또는 _그저 재미있다_일 수 있다는 휴리스틱 지표로 사용되었다. 후속 작업은 위키피디아와 유사한 문서를 우선시하는 것과 같은 다른 휴리스틱을 사용했다(Gururangan et al., 2022). Rae 등(2021)은 특정 불용어 부재, 문서의 길이, 알파벳 문자의 백분율, 평균 단어 길이, 기호 대 단어 비율, 글머리말로 시작하는 선의 백분율 또는 줄임표로 끝나는 것과 같은 문서를 제거하기 위해 다중 휴리스틱 필터를 사용했다. 그들의 작업은 텍스트 데이터를 필터링하는 복잡성을 강조한다. 교육을 위해 더 나은 데이터 세트를 구축하기 위한 대안적인 패러다임은 고품질 데이터 세트를 증류하는 것이다. Xie et al. (2023)은 다양한 도메인의 데이터를 재가중하여 사전 훈련 언어 모델에 적합한 최적의 데이터 혼합물을 선택하는 방법, DoReMi를 제안했다. 동시에 Abbas et al.(2023)은 de-duplicating pre-training data가 pre-training 효율을 향상시킬 수 있음을 보였다. 최근 LLMs의 보다 빠른 미세 조정을 위해 저품질 데이터의 자동 필터링을 위한 몇 가지 방법들이 제안되었다 (Chen et al., 2023; Solaiman and Dennison, 2021; Zhou et al., 2023). 동시에, CLIP(Radford et al., 2021)와 같은 이미지-언어 모델들의 영역에서, Datacomp 벤치마크(Gadre et al., 2023) 및 최근 엔트리들(Maini et al., 2023; Yu et al., 2023)은 LIAION(Schuhmann et al., 2022)과 같은 사전 트레이닝 데이터 세트들로부터 또는 공통 크롤의 스크래프들로부터 저-품질 서브세트들을 필터링하는 접근법들을 개발하였다.\n' +
      '\n' +
      '데이터 증강 및 합성 데이터 엘단과 리(2023)는 유아들이 이해할 수 있는 이야기 형태의 합성으로 생성된 데이터 세트가 일관된 문장을 생성할 수 있는 작은 언어 모델을 훈련시킬 수 있음을 보여주었다. Gunasekar et al. (2023)은 교과서 품질(합성) 데이터만으로도 모델이 추론 및 코딩 작업에 대한 최신 성능을 달성하는 데 도움이 된다는 것을 보여주었다. Liu et al. (2023); Wei et al. (2023)을 미세 조정하면서 코딩 및 수학적 추론 능력을 향상시키기 위한 동시 작업에서 유사한 접근법이 사용된다. Shumailov et al. (2023)은 합성 데이터에 대한 트레이닝이 모델 성능에 실제로 해로울 수 있음을 보여주며, 특히 LLM을 사전 트레이닝한 다음 이전 데이터에 의해 생성된 데이터에 대해 다음 LLM을 트레이닝할 때 더욱 그렇다. 반면에, 몇몇 다른 작품들은 그러한 전략이 실제로 유용할 수 있다는 것을 보여주었다. Li et al.(2023)과 Koksal et al.(2023)은 모델이 명령어 데이터를 생성한 후 생성된 데이터를 스스로 미세 조정하여 성능을 향상시키는 방법에 대해 논의한다. Jung et al.(2023)은 이러한 합성 데이터의 반복 주기가 GPT-3보다 훨씬 우수한 매우 작은 패러프레이즈 및 요약 모델을 훈련하는 데 어떻게 도움이 될 수 있는지에 대해 논의한다.\n' +
      '\n' +
      '비전과 멀티모달 문헌은 합성 데이터를 훈련에 사용하는 것을 검토하는 작업도 급증했다. Bansal and Grover (2023)의 작품; Trabucco et al. (2023); Azizi et al. (2023)은 합성 데이터를 실제 데이터와 결합하여 사용하는 것이 in-distribution과 out-of-distribution 둘 다의 최신 모델 성능을 달성한다는 것을 보여주었다. Cubuk et al.(2020)은 더 나은 도메인 일반화를 위해 이미지 증강을 생성하기 위해 생성 모델을 사용했다. 또한 일반화 개선을 위한 증가의 다중성 및 그 가치에 대한 다수의 연구가 있다(Choi et al., 2019; Fort et al., 2021; Hoffer et al., 2020). 그러나, Alemohammad et al. (2023)은 자신의 생성된 데이터의 5주기 이상 동안 훈련된 생성된 모델이 심각한 모드 붕괴를 겪을 수 있음을 보여주었다.\n' +
      '\n' +
      '## 3 WRAP: Web Rephrase Augmented Pretraining\n' +
      '\n' +
      '일반 언어 모델을 사용하여 합성 데이터를 생성하는 것은 계산상 비용이 많이 들고 조작상 어려울 수 있다. LLM(Gunasekar et al., 2023)을 사용하여 합성 교과서 품질 데이터를 생성하는 이전의 접근법은 (1) 훈련할 가치가 있는 기사를 생성하기에 충분한 세계 지식을 포함하는 언어 모델을 요구하여 생성 비용을 증가시켰으며, (2) 고품질 생성을 가능하게 하는 프롬프트의 신중한 선택 및 합성 코퍼스의 지식 격차를 채우는 다양한 기사를 요구했다. 이 도전은 Li 등의 후속 작업(2023c)에서 강조되었으며 웹의 자연적 다양성에 대해 훈련된 것과 달리 언어 모델(Maini, 2023)에서 부주의하게 편향될 가능성이 있다. (i) 생성 비용 및 (ii) 데이터 다양성의 문제에 대한 해결책으로 웹에서 문서의 자연적 다양성을 활용하는 **WRAP** 를 제안하여 GPT-3.5보다 훨씬 작은 LLM을 사용하여 웹에서 잡음이 많고 구조화되지 않은 문서의 고품질 패러프레이즈를 생성할 수 있습니다.\n' +
      '\n' +
      '### 웹 다시 표시\n' +
      '\n' +
      '과거 연구에서 위키피디아의 텍스트와 같은 높은 가중치의 고품질 데이터가 언어 모델링을 개선하는 데 유용할 수 있음이 관찰되었다. 이러한 용어는 일반적으로 매우 느슨하게 정의되었으며 동일한 것에 대한 일화적인 증거만 있다(Brown et al., 2020; Wenzek et al., 2020). 동시에 웹 데이터는 질의 응답이나 대화 형식의 텍스트가 부족하여 언어 모델의 두드러진 사용 사례이다. 이 두 가지 통찰력을 바탕으로 작업에 대한 리프레이징 스타일을 설계합니다.\n' +
      '\n' +
      '위의 일화적인 증거 대신에, 우리는 웹에서 문서들을 4가지 다른 스타일로 바꾸려고 시도한다 - (i) 쉬운(심지어 유아도 이해할 텍스트), (ii) 중간(위키피디아에서 발견되는 것과 같은 고품질 영어), (iii) 딱딱한(간결하고 난해한 언어), (iv) 질의응답 형식. 이러한 양식적 변형에서 리프레이징을 작동화하기 위해 명령 조정 모델을 적절하게 프롬프트한다. 이 네 가지 스타일의 변경된 예와 작업에 사용된 프롬프트 템플릿은 부록 G에 나와 있다.\n' +
      '\n' +
      'Synthetic DataNow 생성, 우리는 C4(Raffel et al., 2020)와 같은 웹 크롤링된 데이터 세트에서 텍스트를 재구문화하기 위해 명령어 조정 언어 모델을 활용하는 방법을 자세히 설명한다(모든 실험에 사용). 특히, 우리는 동결된 미스트랄-7B 명령-조정 모델(Jiang et al., 2023)을 사용한다(다른 모델들은 섹션 6의 Ablations 참조). "중간" 스타일로 합성 데이터를 생성하기 위해, 미스트랄 모델은 다음 명령어를 사용하여 프롬프트된다: "_다음 단락에 대해 위키피디아 상의 문장에서와 같은 고품질 영어의 패러프레이즈를 제공"_. 프롬프트는 "중간" 크기의 LLM과 GPT-4의 출력을 비교하여 반복적인 인간 피드백을 사용하여 생성되었다. 우리는 모델 출력을 사용하여 원본 노이즈 웹 데이터에 해당하는 "고품질" 합성 데이터의 병렬 코퍼스를 생성한다. 각 예제에는 최대 300개의 토큰이 있으며, 이는 LLM에 300개 이상의 토큰을 다시 말하도록 요청하는 것이 종종 정보 손실로 이어진다는 경험적 관찰을 기반으로 결정되었다. 데이터 품질에 대한 논의는 C절에서 찾을 수 있다.\n' +
      '\n' +
      '실제 데이터와 합성 데이터를 결합하는 우리의 웹 데이터 재구성 방법은 인터넷에서 발견되는 정보 다양성을 자연스럽게 통합한다. 그러나 실제 데이터에 노이즈를 포함하지 않는다. 합성 데이터가 LLMs 사전 학습에 더 빠르게 도움이 될 수 있지만, 우리는 또한 LLMs이 사용자에 직면하는 상황에서 실패하지 않도록 오타 및 언어 오류로 채워질 수 있는 노이즈 있는 웹 텍스트를 이해할 수 있기를 원한다. 이러한 스타일 다양성을 언어 모델링에 통합하기 위해 실제 데이터와 합성 데이터를 1:1 비율로 샘플링한다.\n' +
      '\n' +
      '### Implementation Details\n' +
      '\n' +
      '아키텍처는 디코더 전용 변압기 모델(Vaswani et al., 2017)을 소형, 중형 및 XL의 세 가지 다른 스케일로 훈련한다. 소규모(128M 파라미터) 모델은 12개의 레이어, 12개의 어텐션 헤드, 768의 은닉 차원 크기로 구성된다. 중규모(350M 파라미터) 모델은 24개의 레이어, 16개의 어텐션 헤드, 1024의 은닉 차원 크기로 구성된다. XL-스케일(1.3B 파라미터) 모델은 24개의 레이어, 16개의 어텐션 헤드, 2048의 은닉 차원 크기로 구성된다. 두 모델 모두 드롭아웃을 사용하지 않으며 최대 시퀀스 길이가 1024이다. 모델은 NVIDIA의 Megatron-LM 저장소를 사용하여 학습된다.\n' +
      '\n' +
      '사전 훈련 우리는 달리 명시되지 않는 한 100만 토큰의 배치 크기로 총 300k 단계에 대해 모든 XL 모델을 훈련한다. 128M, 350M 파라미터 모델에서는 \\(3e^{-4}\\), 1.3B 파라미터 모델에서는 \\(2e^{-4}\\)의 최대 학습률을 사용한다. 최소 학습률은 \\(1e^{-5}\\)이다. 가중치 감쇠는 0.01이고 기울기 클리핑 규준은 1.0이다. 전체 단계의 1%를 워밍업과 함께 코사인 학습률 스케줄러를 사용하며, 아담 최적화기는 \\(\\beta_{1}=0.9\\)와 \\(\\beta_{2}=0.999\\)를 사용한다.\n' +
      '\n' +
      '## 4 Perplexity Evaluation\n' +
      '\n' +
      '다중 분포 외 데이터 세트의 검증 세트에 대해 사전 훈련된 모델의 복잡성을 평가한다. 모든 모델은 C4 데이터세트(Raffel et al., 2020) 또는 동일한 특정 양식적 재구문에 대해 훈련된다. 모든 평가는 Pile의 21개 하위 영역에 대해 수행된다(Gao et al., 2020). 이러한 부분 집합은 파일 데이터 세트의 각 도메인에서 처음 10,000개의 문서에서 만들어집니다. 그런 다음 이러한 부분 집합에 대한 모델의 복잡성을 평가한다. 추가 평가 세부 사항은 부록 D에 나와 있다. C4 대신 파일에서 복잡도를 평가하는 것이 중요하다. C4 검증 세트에서 텍스트(합성 및 실제 웹)의 여러 배포에 대한 훈련은 1 복잡도 미만의 작은 비용으로 발생한다. 평가 선택 및 이러한 복잡성 증가를 관찰하는 이유를 이해하기 위해 C4 코퍼스에 대한 훈련은 목표를 최소화하는 것에 해당한다는 점에 주목한다.\n' +
      '\n' +
      '\\[\\theta_{\\mathcal{C}4}=\\min_{\\theta}\\mathds{E}_{x\\sim D_{\\mathcal{C}4}}\\left[ \\mathcal{L}(\\theta;x)\\right], \\tag{1}\\]\n' +
      '\n' +
      '그것은 C4 웹 텍스트를 정확하게 모델링하려고 시도한다. 대조적으로, 다수의 스타일에 걸친 훈련은 상이한 분포에 걸친 위험을 최소화하는 것에 대응하고,\n' +
      '\n' +
      '\\[\\theta_{\\mathbf{WRAP}}=\\min_{\\theta}\\mathds{E}_{x\\sim D_{\\mathcal{C}4}\\cup D_{ \\mathcal{D}m}}\\left[\\mathcal{L}(\\theta;x)\\right]. \\tag{2}\\]\n' +
      '\n' +
      '방정식 2에 대한 해법은 C4에 대한 위험을 최소화하지 못하므로 C4에 대한 \\(\\theta_{\\mathcal{C}4}\\)와 \\(\\theta_{\\mathbf{WRAP}}\\)를 비교하는 것은 부당하다. C4에 대해 훈련된 모델과 그 합성 구문을 의미 있게 비교하기 위해, 우리는 Pile의 21개 도메인에 대한 일반화 능력을 평가한다(Gao et al., 2020). 각 도메인에 대한 결과는 그림 2에 나와 있다.\n' +
      '\n' +
      '그림 2: **WRAP (C4 + QA-85B) v/s C4**: 300B 토큰에 대해 훈련된 1.3B LLM에 대한 파일의 복잡성을 비교하면 WRAP가 2x 실제 데이터에 대해 훈련된 모델보다 성능이 우수함을 알 수 있습니다.\n' +
      '\n' +
      '데이터 복잡도 그림 0(c)에서 더 적은 토큰(150B)에 대해 훈련된 모델과 더 작은 350M 모델이 합성 재구문을 사용하여 더 빠른 학습을 나타내는 300B 토큰에 대해 전체 C4에 대한 훈련보다 우수하다는 것을 보여준다. ArXiv 및 HackerNews와 같은 일부 도메인에서 합성 데이터로 훈련하면 실제 데이터만으로 훈련된 모델의 복잡도를 거의 3배 줄일 수 있음을 관찰한다. 이는 더 많은 실제 데이터에 대한 학습만으로는 합성 데이터에 대한 사전 학습의 성능 우위를 상쇄할 수 없는 경우가 많을 수 있음을 시사한다. 전반적으로, 파일의 여러 부분 집합의 평균에 대해, 우리 모델은 실제 데이터만으로 훈련된 모델에 비해 복잡성을 50% 개선한다.\n' +
      '\n' +
      '학습 속도 **WRAP** 훈련의 첫 번째 체크포인트(10B 토큰)에서도 15개의 체크포인트에 대해 C4에서 사전 훈련하여 얻은 것보다 파일에서 LLM의 평균 복잡도가 낮다는 것을 관찰합니다. 이것은 15배 사전 훈련 속도 향상을 시사한다. 우리는 더 의미 있는 비교를 하기 위해 학습 속도에 대한 논의를 \'제로 샷\' 과제로 미루었다.\n' +
      '\n' +
      '## 5 Zero-shot 작업\n' +
      '\n' +
      '우리는 이제 LLM Evaluation Harness1Gao et al. (2023)을 사용하여 다양한 제로 샷 질의 응답(QA) 벤치마크에 대해 사전 훈련된 언어 모델을 평가한다.\n' +
      '\n' +
      '각주 1: 배치 크기가 32인 모든 실험에서 일관성을 위해 git 커밋 - 89618bf8을 사용합니다.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      '우리는 총 13개의 서로 다른 제로샷 벤치마크에 대해 모델을 평가하여 상식 추론, 언어 및 지식 이해 및 수학적 추론과 같은 다양한 자연 언어 작업에 대한 능력을 평가한다.\n' +
      '\n' +
      '일반 이해 일반 이해 범주는 광범위한 인지 기술과 언어 이해를 테스트하는 데이터 세트로 구성된다. **ARC Easy(ARC-E)**Clark 등(2018)은 기본적인 추론 기술이 필요한 질문을 특징으로 하는 ARC-C의 덜 도전적인 대응물입니다. **BoolQ**Clark 등(2019)에는 읽기 이해 및 일반적인 언어 이해에 중점을 두는 부울 질문이 포함됩니다. **위노그란데(위노.)**ai(2019)는 언어, 특히 대명사 명확화에서 상식 추론을 사용하여 모델에 도전합니다. **PIQA**Bisk 등(2020)은 실제 상식의 필수적인 부분인 물리적 프로세스에 대한 이해를 평가합니다. **HellaSwag** Zellers 등(2019)은 언어 이해와 상식을 모두 요구하면서 일관성 있게 시나리오를 완료하는 능력을 테스트합니다. **TruthfulQA**Lin 등(2021)은 진실되고 정확한 답변을 생성하는 데 중점을 두고 있으므로 모델의 사실적 정확성을 테스트합니다. **OpenBookQA(OBQA)**Mihaylov 등(2018)은 광범위한 사실과 개념에 대한 이해를 평가합니다. 마지막으로, **LogiQA-2**Liu 등(2023)은 논리적 원칙을 이해하고 적용하는 모델의 용량을 평가합니다.\n' +
      '\n' +
      '전문 지식 전문 지식 범주에는 특정 도메인에 대한 전문 지식을 요구하는 데이터 세트가 포함됩니다. **ARC 챌린지(ARC-C)**Clark 등(2018)은 3학년에서 9학년까지의 도전적인 과학 시험 문제를 포함하여 고급 지식을 요구합니다. **SciQ**Johannes Welbl(2017)은 과학 영역에서 모델의 이해와 추론을 테스트하기 위해 과학 시험 문제를 제공합니다. **PubMedQA**Jin 등(2019)은 의료 및 건강 관련 정보의 이해도를 평가하는 생체 의학 문헌에 중점을 둡니다. **MathQA**Amini 등(2019)은 수학적 문제 해결을 테스트하여 수치 이해와 추론을 모두 필요로 합니다. 마지막으로 **MMLU**Hendrycks 등(2021)은 전문가에서 학술에 이르기까지 여러 도메인에 걸쳐 있으며 전문 주제에 대한 모델을 테스트합니다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '실제 데이터와 합성 데이터의 혼합으로 학습된 모델과 실제 데이터의 다양한 분할로 학습된 모델의 성능을 비교한다. 모든 실험에서 합성 데이터의 분할을 재구성하고 생성하기 위해 C4(Raffel et al., 2020) 데이터 세트를 사용한다. 우리는 사전 훈련에 사용할 수 있는 웹 데이터의 토큰 수를 나타내기 위해 \'Real Tok\'라는 약어를 사용한다. 합성 + 실제\' 실험에서 우리는 동일한 수의 합성 구문을 증가시킨다. 우리는 잠재적으로 동일한 문서를 여러 번 다시 말할 수 있기 때문에 비교 기준으로 \'진짜 토큰\'을 선택하며, 이는 총 말뭉치 크기가 의미가 없음을 암시하고 말뭉치 \'지식\'이 실제 관심 통화임을 의미한다.\n' +
      '\n' +
      'Baselines MethodsWe pre-train LLMs of (i) Half of C4, and the (ii) Full C4 corresponding to approximately 85 Billion and 170 Billion real tokens respectively (Raffel et al., 2020). 우리는 또한 정제된 웹 데이터 세트의 (iii) 1,600억 및 (iv) 3,200억 토큰에 대해 자체 모델을 사전 훈련한다(Penedo et al., 2023). 또한, Pile에서 훈련된 (iv) Pythia-1.4B 모델과 비교하였다 (Gao et al., 2020). 이 데이터 세트는 더 이상 공개적으로 사용할 수 없으므로 사전 훈련된 모델을 활용한다. 마지막으로, 우리는 또한 SlimPajama(Shen 등, 2023) 및 StarCoder(Li 등, 2023a)로부터 1조 토큰의 데이터에 대해 3개의 에포크에 대해 트레이닝된 최근의 (v) TinyLlama 모델(Zhang 등, 2024)과 비교한다.\n' +
      '\n' +
      '일반 개선표 1의 모든 작업에 걸쳐 C4 데이터 세트(합성+C4)와 결합된 합성 데이터로 훈련된 모델이 평균 47.4%의 점수를 받은 85B 토큰 분할이 있는 실제 C4 데이터 세트에서만 훈련된 모델에 비해 전체적으로 49.4%의 더 높은 평균 성능을 나타냄을 관찰한다. 이는 합성 데이터의 포함이 NLP 모델의 일반적인 이해 능력을 향상시킬 수 있음을 보여준다. 또한, 10배 계산 및 데이터에 대해 훈련된 TinyLlama 모델조차도 실제 데이터에 대해 훈련된 다른 모델들과 비교해서 수행한다. 이는 필터링을 하거나 실제 데이터를 더 추가하는 데 따른 이득이 매우 낮음을 시사한다. 이와 반대로 **WRAP** 은 적은 양의 합성 데이터에도 사전 교육이 큰 성능 향상에 기여할 수 있음을 보여 줍니다.\n' +
      '\n' +
      '전문화된 지식 과제 표 2의 결과에서 나온 핵심 메시지는 합성 데이터가 \'새로운 지식\'을 전달할 수 없다는 것이다. 그것은 우리의 작업의 전제이기도 했던 사전 훈련을 더 빠르게 도울 수 있을 뿐이다. 특히, 우리는 몇 가지 주요 발견에 주목한다.\n' +
      '\n' +
      '1. 더 큰 데이터 세트에 대한 사전 훈련은 아마도 LLM을 더 많은 "지식"에 노출시킴으로써 성능을 향상시키는 데 도움이 된다. 예를 들어, 피티아(300B) 모델은 평균 44.6%의 점수를 달성하여 더 작은 C4(85B) 데이터 세트의 점수 43.5%를 능가한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline Dataset (Real Tok.) & ARC-E & BoolQ & Wino. & PIQA & HellaSwag & TruthfulQA & OBQA & LogQA & Avg \\\\ \\hline Half C4 (85B) & 61.2 & 59.1 & 57.3 & 74.9 & 46.5 & 34.1 & 22.4 & 23.5 & 47.4 \\\\ Full C4 (170B) & 61.6 & 54.2 & 59.0 & 74.9 & 46.8 & 33.5 & 25.0 & 23.4 & 47.3 \\\\ RW (160B) & 61.6 & 60.7 & 57.5 & 74.3 & 45.2 & 36.8 & 21.8 & 23.2 & 47.6 \\\\ RW (320B) & 60.7 & 61.1 & 57.1 & 74.4 & 45.6 & 36.0 & 22.6 & 22.5 & 47.5 \\\\ Pythia-Pile (300B) & 60.5 & 63.3 & 57.5 & 70.8 & 40.4 & 38.9 & 22.2 & 22.2 & 47.0 \\\\ TinyLlama (17) & 60.3 & 57.8 & 59.1 & 73.3 & 45.0 & 37.6 & 21.8 & 24.5 & 47.4 \\\\ \\hline Synthetic (85B) & 63.9 & 60.0 & 58.8 & 76.1 & 45.2 & 44.0 & 23.0 & 24.1 & 49.4 \\\\ Synthetic+C4 (85B) & 64.1 & 62.2 & 58.9 & 75.4 & 46.2 & 40.6 & 24.1 & 23.9 & 49.4 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 일반적인 추론, 언어 이해 및 상식에 초점을 맞춘 데이터 세트에 대한 \'일반적인 이해 작업\'에 대한 \\(\\sim\\) 1.3B 매개변수 LLM의 평가. **WRAP** 의 결과는 평균 3회 실행입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline Dataset (Real Tok.) & ARC-C & SciQ & PubMedQA & MathQA & MMLU & Avg \\\\ \\hline Half C4 (85B) & 26.3 & 84.5 & 57.2 & 23.4 & 24.2 & 43.1 \\\\ Full C4 (170B) & 26.8 & 85.0 & 57.4 & 24.3 & 23.9 & 43.5 \\\\ RW (160B) & 27.2 & 87.2 & 56.2 & 24.1 & 25.9 & 44.1 \\\\ RW (320B) & 27.8 & 88.0 & 57.4 & 23.0 & 25.4 & 44.3 \\\\ Pythia-Pile (300B) & 26.1 & 86.6 & 60.6 & 25.2 & 24.3 & 44.6 \\\\ TinyLlama (1T) & 27.8 & 88.9 & 61.4 & 24.1 & 25.8 & 45.6 \\\\ \\hline Synthetic (85B) & 29.7 & 87.0 & 60.2 & 23.4 & 24.6 & 45.0 \\\\ Synthetic+C4 (85B) & 29.9 & 87.6 & 61.5 & 23.9 & 24.8 & 45.5 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 과학, 의학, 수학, 논리 등 특정 영역 지식이 필요한 \'전문화된 지식 과제\'에 대한 \\(\\sim\\) 1.3B 매개변수 LLM의 평가이다. **WRAP** 에 대 한 결과는 3 실행에 걸쳐 평균화 됩니다.\n' +
      '\n' +
      '2. 더 큰 데이터 세트의 장점에도 불구하고, 개선은 포화된다. 예를 들어, RefinedWeb(320B) 모델이 RefinedWeb(160B) 모델보다 0.2%만 우수하다. 유사하게, TinyLlama 모델(1T 토큰)은 원시 웹 데이터의 85B 토큰만 있는 **WRAP** 모델과 비교적으로 수행합니다.\n' +
      '\n' +
      '특정 개선 사항 TruthfulQA 데이터 세트에서 최대 개선을 볼 수 있으며, 합성(85B) 모델은 44.0%로 이 데이터 세트에서 다른 모델의 성능보다 훨씬 높다. 이것은 잠재적으로 명령 조정 LLM이 텍스트를 다시 바꾸는 동안 잠재적인 오개념을 이미 수정하기 때문이다. 흥미롭게도 합성 모델(합성+C4)에 실제 데이터를 추가하면 TruthfulQA의 성능이 4% 감소하여 40.5%로 감소하며, 이는 실제 데이터와 결합할 때 합성 데이터에서 얻은 이점이 잠재적으로 희석되었음을 나타낸다. C4 훈련된 모델이 잘 작동하는 헬라스웨그 및 BoolQ와 같은 다른 데이터 세트는 C4와 합성 구문의 조합을 통합하는 이점을 계속 보여준다.\n' +
      '\n' +
      '## 6 분석 및 삭제\n' +
      '\n' +
      '또한 다음과 같은 연구 질문(RQ)을 통해 성능을 최적으로 향상시키는 방법을 보다 세밀하게 조사할 수 있습니다.\n' +
      '\n' +
      '### 데이터 조합 분석\n' +
      '\n' +
      'RQ1: 실제 C4 데이터를 갖는 것이 얼마나 중요한가? 표 1-2의 우리의 발견은 QA 프롬프트를 사용하는 합성 데이터가 QA에 대한 강력한 성능에 충분하다는 것을 나타낸다.\n' +
      '\n' +
      '그림 3: **실제 데이터의 중요성:** C4에 대한 사전 교육을 합성 데이터와 비교할 때 파일의 복잡성을 비교합니다. 합성 데이터만 사용합니다. 모델들은 C4의 350억 토큰들을 포함하는 실제 데이터 서브세트 상의 총 150B 토큰들에 대해 트레이닝된 1.3B 파라미터들이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline \\hline Dataset (Real Tok.) & ARC-E & BoolQ & Wino. & PIQA & HellaSwag & TruthfulQA & OBQA & LogQA & Avg \\\\ \\hline Med+C4-35B & 59.8 & 57.0 & 55.7 & 74.6 & 44.5 & 36.5 & 23.8 & 21.5 & 46.7 \\\\ QA+C4-35B & 62.2 & 63.3 & 55.7 & 74.8 & 44.6 & 41.4 & 22.4 & 23.2 & 48.4 \\\\ Med-35B & 56.6 & 59.5 & 53.4 & 74.0 & 41.9 & 36.3 & 22.2 & 22.7 & 45.8 \\\\ QA-35B & 61.7 & 62.0 & 53.9 & 75.2 & 43.4 & 43.0 & 22.8 & 23.4 & 48.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: **실제 데이터의 중요성:** 일반 이해 작업의 150B 토큰에 대해 훈련된 \\(\\sim\\) 1.3B 매개 변수 LLM의 평가입니다. 결과는 실제 데이터를 추가하는 것이 \'미디엄\' 또는 \'위키피디아 스타일\' 패러프레이즈에서 사전 훈련할 때 모델 성능을 향상시키는 데 도움이 된다는 것을 보여준다.\n' +
      '\n' +
      '작업들. 그러나 파일 복잡도에 대해 평가했을 때 그림 3의 많은 하위 영역에 걸쳐 복잡도의 현저한 저하를 관찰한다. 이는 합성 데이터가 특수 문자가 거의 포함되지 않고 고도로 구조화되어 있기 때문에 그럴 가능성이 높다. 대조적으로 OWT와 해커뉴스와 같은 파일의 여러 하위 도메인은 그러한 특별한 토큰을 가지고 있다. 필리퍼스와 구텐베르크와 같은 도메인에서 우리는 사전 훈련 데이터에서 실제 C4 텍스트를 떨어뜨리고 합성 문서에 대한 훈련만으로도 성능이 크게 떨어지는 것을 관찰한다(당황 증가). 이는 다시 한 번 합성 데이터가 실제 데이터 스크래프에 만연한 특정 \'태그\'와 \'스타일\'을 포함하지 않는다는 사실에 기인하며, 합성 데이터 단독에 대한 사전 훈련보다 **WRAP**이 더 나은 전략임을 강조했습니다. 제로 샷 태스크에 대한 성능 측면에서, 우리는 표 3,4에서 실제 데이터의 존재가 제로 샷 성능을 개선하는 데 도움이 된다는 것을 다시 한 번 주목한다. 제로 샷 태스크는 잘 작성된 Q/A 쌍을 포함하기 때문에 이러한 효과는 실제 데이터에 대한 복잡도에 대한 효과만큼 분명하지 않다.\n' +
      '\n' +
      'RQ2: 여러 합성 데이터 세트의 조합이 성능을 향상시키나요?우리는 훈련을 위해 여러 합성 스타일을 C4와 결합하는 것의 영향을 측정합니다. 우리는 두 가지 변형을 고려한다: 1:1 비율로 결합한다는 것은 두 합성 스타일(중간 및 QA)과 일치하도록 C4의 사본이 두 개 있다는 것을 의미하며, C4 데이터 세트의 한 인스턴스만 결합하는 1:2 비율이다. 제로샷 QA 작업의 경우 표 5-6에서 찾은 결과는 QA와 C4 데이터만 결합하는 것보다 성능이 낮다는 것을 나타낸다. 파일에 대한 평가는 그림 4와 같다. 우리는 \'Q/A\'와 \'위키피디아\' 패러프레이즈가 특정 도메인에 대한 성능을 향상시키는 데 도움이 된다는 것을 알 수 있다. 예를 들어, 질의응답이 많은 \'Stackexchange\'는 합성 데이터가 Q/A 스타일로 존재함으로써 이점이 있다. 전반적으로, 우리는 여러 스타일을 결합하여 파일에서 평균 당혹도에 약간의 개선이 있음을 주목한다.\n' +
      '\n' +
      '### Method Ablations\n' +
      '\n' +
      'RQ3: 고품질 리프레이저를 갖는 것이 얼마나 중요한가? 이에 답하기 위해, 우리는 4개의 별개의 리프레이징 모델(T5-베이스(Raffel et al., 2020), Owen-L8B-chat(Bai et al., 2023a), Mistral-7B-chat(Jiang et al., 2023), 및 Vicuna-13B-chat-v1.3(Chiang et al., 2023))의 데이터를 사용하고 30B 토큰에 대해 345M 모델을 트레이닝한다. 동일한 프롬프트를 사용하여 모든 모델에서 데이터를 생성합니다. T5-base 모델의 경우, Vicuna-13b-chat 모델의 재구어 쌍에서 1 에폭에 대한 모델을 미세 조정한다. 우리는 Qwen-1.8B 및 미스트랄-7B와 같은 더 작은 재구문 모델에 의해 생성된 데이터에 대한 사전 훈련이 비쿠나 13B보다 더 낮은 당혹도를 달성한다는 것을 발견한다(그림 5). 동시에 미세 조정된 T5 기반 모델은 나머지 모델보다 훨씬 더 나쁜 성능을 보인다. 그럼에도 불구하고 모든 재구문 모델은 실제 C4 데이터에만 대한 복잡성을 줄입니다. **WRAP** 적용 가능성을 더욱 확장하기 위해 고품질 합성 데이터를 생성할 수 있는 패러프레이즈 모델을 얼마나 작게 훈련할 수 있는지에 대한 한계를 테스트하는 것은 아직 미해결 문제로 남아 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Dataset (Real Tok.) & ARC-C & SciQ & PubMedQA & MathQA & MMLU & Avg \\\\ \\hline Med+C4-35B & 27.2 & 82.2 & 46.2 & 23.1 & 25.2 & 40.8 \\\\ QA+C4-35B & 29.0 & 85.1 & 62.2 & 22.5 & 26.1 & 45.0 \\\\ Med-35B & 27.0 & 80.0 & 59.4 & 22.5 & 24.7 & 42.7 \\\\ QA-35B & 27.1 & 85.5 & 59.2 & 22.2 & 25.0 & 43.8 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **실제 데이터의 중요성:** 전문화된 지식 작업에 대한 \\(\\sim 1.3\\)B 매개 변수 LLM의 평가입니다. 그 결과, \'Q/A-style\' 패러프레이즈에서 실제 데이터를 추가하는 것이 모델 성능을 향상시키는 데 도움이 된다는 것을 알 수 있었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline \\hline Dataset (Real Tok.) & ARC-C & SciQ & PubMedQA & MathQA & MMLU & Avg \\\\ \\hline Med+C4-35B & 27.2 & 82.2 & 46.2 & 23.1 & 25.2 & 40.8 \\\\ QA+C4-35B & 29.0 & 85.1 & 62.2 & 22.5 & 26.1 & 45.0 \\\\ Combined-1:1-35B & 28.2 & 85.9 & 61.2 & 23.2 & 23.9 & 44.5 \\\\ Combined-1:2-35B & 29.0 & 85.7 & 57.4 & 23.5 & 23.1 & 43.\n' +
      '\n' +
      'RQ4: 합성 데이터가 증강보다 개선되는가?합성 데이터에 대한 사전 훈련에 의해 관찰된 이득은 증강을 갖는 사전 훈련과 동일한가? 이를 테스트하기 위해, 우리는 NL-Augmenter 라이브러리를 사용하여 동의어 대체 및 무작위 삭제라는 두 가지 인기 있는 텍스트 증강 기준선을 고려한다(Dhole et al., 2021). 이 일련의 실험을 수행하기 위해 15B 토큰에 대해 350M 매개변수 모델을 사전 훈련한다. 총 풀 크기는 약 1.5B 토큰에 불과하며, 이는 모델이 보강되지 않는 한 사전 훈련 단계 동안 약 10회 데이터를 반복해야 함을 의미한다. 그림 6의 복잡도 분석에서 볼 수 있듯이 증강 데이터에 대해 훈련된 모델은 실제 데이터와 합성 데이터의 조합에 대해 훈련된 모델보다 훨씬 더 나쁜 성능을 보인다. 이는 합성 데이터가 학습 과정을 향상시키며 단순히 증강의 다른 형태가 아님을 시사한다.\n' +
      '\n' +
      'RQ5: 합성 데이터의 스타일이 특수 도메인에 대한 성능에 어떻게 영향을 미치는가? 우리는 합성 데이터의 다양한 스타일에 대해 훈련된 다양한 모델의 성능을 비교한다. 특히, 합성 데이터의 4가지 스타일(쉬운, 중간, 하드, Q/A)을 생성하고, 파일 하위 집합에 대한 각 스타일 조합에 대한 학습 성능을 평가한다. 이러한 합성 데이터 스타일을 생성하기 위한 프롬프트는 부록 G에 요약되어 있다. Vicuna-v1.3 모델의 세대에 해당하는 결과 및 3B 토큰에 대해 트레이닝된 128M 모델에 대한 결과는 그림 7에 요약되어 있다. 평가 시 도메인의 스타일과 일치하는 실제 C4 및 합성 데이터의 조합을 사용한 트레이닝이 성능을 향상시킨다는 것을 알 수 있다. 그러나 단일 합성 데이터 스타일이 모든 도메인에서 가장 좋은 성능을 발휘하지 못하므로 실제 C4 데이터와 각 합성 스타일 변형의 조합을 사용한 훈련에서 유사한 성능을 보인다. LLM을 사전 훈련하기 위한 최상의 합성 스타일을 아는 것은 비현실적이지만 모든 도메인에 걸쳐 최상의 합성 스타일을 선택하는 오라클은 16%의 복잡성을 개선할 것이며, 이는 기본 지식이 동일하게 유지되는 경우에도 LLM 일반화를 위한 다양한 데이터 스타일로 훈련하는 것의 중요성을 나타낸다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline Dataset (Real Tok.) & ARC-E & BoolQ & Wino. & PIQA & HellaSwag & TruthfulQA & OBQA & LogQA & Avg \\\\ \\hline Med+C4-35B & 59.8 & 57.0 & 55.7 & 74.6 & 44.5 & 36.5 & 23.8 & 21.5 & 46.7 \\\\ QA+C4-35B & 62.2 & 63.3 & 55.7 & 74.8 & 44.6 & 41.4 & 22.4 & 23.2 & 48.4 \\\\ Combined-1:1-35B & 60.6 & 60.2 & 57.7 & 73.8 & 43.7 & 40.2 & 22.0 & 22.1 & 47.5 \\\\ Combined-1:2-35B & 61.4 & 62.0 & 57.0 & 74.8 & 44.6 & 39.5 & 23.0 & 21.3 & 48.0 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: **여러 스타일 결합:** 일반 이해 작업의 150B 토큰에 대해 훈련된 \\(\\sim 1.3\\)B 매개 변수 LLM의 평가입니다. 분석 결과, 재출제 스타일을 조합하는 것이 질의응답 스타일보다 제로샷 작업에서 성능 향상을 가져오지 못하는 것으로 나타났다.\n' +
      '\n' +
      '그림 4: **여러 스타일 결합:** 여러 스타일의 합성 데이터를 결합하는 것을 비교하는 파일의 모든 도메인에 걸친 복잡성입니다. 모델은 총 150B 토큰에 대해 훈련된 1.3B 파라미터이다. 우리는 여러 스타일을 결합하여 작은 복잡성 개선을 봅니다.\n' +
      '\n' +
      'RQ6: 재구문 모델에서 훈련된 모델로의 데이터 유출이 있는가? 우리는 합성 데이터가 원래의 C4 데이터와 양식적으로 다르고 서로 다른 PILE 도메인의 스타일과 일치하면서 유사한 의미 의미를 유지하는지를 조사한다. 먼저 합성 데이터와 실제 데이터의 예제 쌍을 비교하여 성능 이득이 재구문 모델로부터의 지식 유출에 기인하지 않음을 확인한다. 우리는 각 데이터 세트에서 처음 1000개 샘플의 하위 집합을 취한다.\n' +
      '\n' +
      '우리는 그림 8의 (a)와 (b)에서 중간 및 qa 프롬프트에 대해 SimCSE 목적(Gao et al., 2021)으로 훈련된 사전 훈련된 BERT 모델에서 문장 임베딩의 코사인 유사성을 보여준다. 유사성을 계산할 때 이상치를 제거합니다. 분포가 있는 그림은 가우시안 커널 밀도 추정기(KDE)를 사용하여 통계에 대한 분포를 구성합니다.\n' +
      '\n' +
      '그림 5: **고품질 패러프레이저의 중요성:** 다른 LLM에서 생성된 데이터에 대한 **WRAP** 에 대한 모든 파일 도메인에서 복잡성입니다. 결과는 Qwen-1.8B와 같은 작은 모델도 고품질 패러프레이즈를 생성할 수 있음을 보여준다. 그러나 우리의 미세 조정된 T5 기반 모델과 같은 낮은 품질의 재구성은 훨씬 더 나쁜 언어 모델링으로 이어진다.\n' +
      '\n' +
      '그림 6: **재구문은 증강과 동일합니까?* * 다른 증강 전략에 대해 파일의 복잡성을 비교합니다. 350M 파라미터 모델들은 총 15B 토큰들에 대해 트레이닝된다. **WRAP** (중간 + C4)는 기존 보강보다 훨씬 성능이 우수 합니다.\n' +
      '\n' +
      '1000 값입니다. 실제 합성 쌍의 코사인 유사도는 C4의 두 개의 무작위 실제 샘플, 샘플의 전반부와 전체 샘플 사이의 코사인 계산 연속 기준선, 동일한 샘플의 전반부와 후반부 사이의 코사인 유사성을 포함하는 여러 기준선보다 높다. 유사도가 높다는 것은 재구문이 정보를 추가하지 않고 실제 상대방과 유사한 의미를 유지한다는 것을 나타낸다.\n' +
      '\n' +
      '## 7 제한 사항 및 기회\n' +
      '\n' +
      '### Cost Analysis\n' +
      '\n' +
      '_합성 데이터를 생성해야 합니까, 아니면 실제 데이터에 대해 더 오래 훈련해야 합니까_\n' +
      '\n' +
      '**WRAP** 응용 프로그램은 두 패러다임 모두에 있습니다. (i) 핀란드 언어에 대 한 언어 모델 (Luukkonen 등, 2023)과 같은 자원이 부족한 데이터 설정 및 (ii) 일반적인 크롤링에 대 한 훈련과 같은 데이터가 풍부한 설정입니다. 전자의 경우 더 많은 데이터를 순진하게 수집할 수 있는 대안이 없기 때문에 합성 데이터는 도메인 내 데이터만으로 학습을 능가해야 하는 자연스러운 솔루션이다. 그러나, 영어, 또는 보다 광범위하게 일반적인 웹 데이터에 대한 언어 모델을 훈련시키는 것에 상당한 관심이 있다. 이러한 패러다임에서도 합성 데이터를 사용하는 것이 실행 가능한 옵션인가?\n' +
      '\n' +
      '그림 8: 합성 데이터가 실제 C4 데이터와 비교하여 의미적 의미를 유지하고 주로 (a) C4의 매체 구문과 (b) C4의 QA 구문에 대한 스타일을 변경한다는 것을 보여주는 C4 말뭉치의 합성 데이터와 실제 데이터 비교.\n' +
      '\n' +
      '그림 7: **합성 구문의 스타일 영향:** 다른 스타일의 합성 데이터를 비교하는 파일의 모든 도메인에 대한 복잡성입니다. 3B 토큰에 대해 128M 매개 변수 모델을 학습합니다.\n' +
      '\n' +
      '먼저, 합성 데이터에 대한 사전 훈련의 실현 가능성에 대해 알아봅니다. 표 1의 결과를 인정해야 합니다. 3조 토큰에 대해 훈련된 TinyLama 모델도 실제 데이터와 합성 데이터에 대해 공동으로 훈련된 모델보다 성능이 낮습니다. 실제로 실제 데이터에서도 300B 토큰에 대해 훈련된 모델과 상당히 유사한 성능을 보입니다. 이것은 더 긴 트레이닝에 의한 개선을 위한 천장이 그렇게 높지 않을 수 있음을 시사한다(크기 350M/1.3B 파라미터들의 모델의 경우; 더 큰 모델들은 더 긴 트레이닝으로부터 이익을 얻을 수 있다).\n' +
      '\n' +
      '이 비용 절충을 분석하기 위해 합성 데이터 생성 비용과 추가 데이터에 대한 언어 모델을 훈련하는 비용을 비교한다. 합성 데이터 생성 실험을 위해 빠른 생성을 위해 vLLM (Kwon et al., 2023) 라이브러리를 사용한다. 특히, 우리는 미스트랄-7B를 사용할 때 단일 A100에서 시간당 3M 토큰을 생성할 수 있다. (우리 작업에서와 같이) 85B 토큰을 생성하는 것은 약 25K GPU 시간에 대해 계정을 생성합니다.\n' +
      '\n' +
      '이에 비해 64개의 A100s에서 초당 0.5M 토큰의 처리량을 달성한다. 300B 토큰에 대한 훈련을 가정하면 256 GPU일을 의미하며, 단일 모델을 훈련하기 위해 약 6k GPU 시간을 차지한다. 반대로, 13B 모델을 훈련시키는 것은 약 30K GPU 시간이 걸릴 것이다. 13B 모델을 훈련하는 규모에서 훈련 비용을 3-10배 줄이는 것은 훈련의 비용 오버헤드를 단일 실행에서 합성 데이터와 통합할 수 있다.\n' +
      '\n' +
      '고품질 데이터 생성 비용은 여전히 상대적으로 높지만 개선의 두 가지 중요한 출처는 이러한 비용 분석에 영향을 미친다. 먼저, Qwen-1.8B 모델 Bai 등(2023b)을 재구문에 사용하면 3배 높은 토큰 처리량을 얻을 수 있다. 그림 5의 예비 결과에서 볼 수 있듯이 Qwen 모델에 의해 생성된 재구문에 대해 사전 훈련된 모델은 미스트랄 모델에 의한 것과 유사한 성능을 보인다. 이에 따라, 발전 비용이 3배 절감된다. 추측적 디코딩(Liu et al., 2023c) 및 최적화된 추론(Xia et al., 2024)에 대한 보다 최근의 작업은 생성 비용에서 또 다른 3-5배 개선을 레버리지할 수 있음을 시사한다. 따라서 실제로 1.3B 매개변수 모델 훈련 규모에서도 실제 데이터만 사용하여 사전 훈련 비용을 개선할 수 있다.\n' +
      '\n' +
      '위의 논의에서 설명할 수 없었던 합성 데이터 생성의 두 가지 추가적인 중요한 이점:\n' +
      '\n' +
      '1. 합성 데이터 생성 비용은 일회성 투자이며 데이터가 생성되면 다양한 규모의 많은 모델을 훈련할 수 있다.\n' +
      '2. 데이터 생성은 100% 병렬화할 수 있는 반면, 훈련은 노드 간 연결이 빠른 대규모 클러스터의 가용성을 요구합니다. 이것은 훨씬 더 비싸다. 반면에 생성은 대규모 컴퓨팅 클러스터에서 빈 GPU를 채우고 단일 GPU 머신에서 실행할 수 있는 측면 프로세스로 생각할 수 있습니다.\n' +
      '\n' +
      '### 합성 생성기의 다양성\n' +
      '\n' +
      '또 다른 한계는 생성된 데이터의 다양성을 강화하는 것이다. 이러한 다양성은 생성된 데이터에 포함된 "스타일"과 "지식" 모두에서 비롯된다. 최근의 작품들(Li et al., 2023b;c)은 새로운 텍스트들을 생성하기 위해 모델을 시드하기 위해 토픽들의 선택, 또는 시나리오들을 사용하였다. 그럼에도 불구하고 Padmakumar et al.(2023)의 최근 연구에 따르면 AI 지원 글쓰기에 언어 모델을 사용하는 것은 특히 명령어 조정 모델을 사용할 때 콘텐츠 다양성을 줄이는 경향이 있다. 우리는 새로운 콘텐츠 생성의 다양성과 관련된 문제를 완화하기 위해 특별히 재구성의 패러다임을 사용했지만, 패러프레이즈 모델에서 콘텐츠 다양성의 존재(또는 부족) 및 영향을 평가하는 것은 향후 작업으로 남아 있다.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      '강력한 언어 모델은 실제 데이터와 합성 데이터의 조합에 대해 사전 훈련되고 있다. 합성 데이터를 사용하면 공정성, 편향성 및 스타일(명령어 후속과 같은)과 같은 바람직한 속성에서 데이터를 직접 베이킹할 수 있으므로 훈련 알고리즘을 구체적으로 조정할 필요가 없다. 이것은 언어 모델을 인간의 가치에 맞추는 대안적인 접근법을 제공한다. 최근 합성 데이터, 특히 명령어 조정 언어 모델에 대한 관심의 증가는 주목할 만하며 동시 연구자도 사전 훈련을 위해 이를 활용한다. 이 패러다임으로 전환함에 따라 모델에 공급되는 데이터의 속성을 이해하는 것이 가장 중요합니다. This paper aims to be comprehensive guide on employing\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:14]\n' +
      '\n' +
      '* Bisk et al.(2020) Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: 자연어의 물리적 상식에 대한 추론. 2020년 "인공지능에 관한 30번째 AAAI 회의"에서.\n' +
      '* Brown 등 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 언어 모델은 샷이 적은 학습자입니다. 인현라로셸 란자토 Hadsell, M.F. Balcan 및 H. Lin(eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcd967418bf8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcd967418bf8ac142f64a-Paper.pdf)\n' +
      '* Chen et al.(2023) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: training a better alpaca with less data. _ arXiv preprint arXiv:2307.08701_, 2023.\n' +
      '* 치앙 등(2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhang, Yonghao Zhang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: 2023년 3월, 90%* 채팅gpt 품질의 gpt-4를 나타내는 오픈 소스 챗봇. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)입니다.\n' +
      '* Choi et al. (2019) Dami Choi, Alexandre Passos, Christopher J Shallue, and George E Dahl. 데이터 에코를 사용하여 더 빠른 신경망 학습입니다. _ arXiv preprint arXiv:1907.05550_, 2019.\n' +
      '* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: 자연스러운 예/아니오 질문의 놀라운 난이도를 탐구합니다. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 2924-2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL [https://aclanthology.org/N19-1300](https://aclanthology.org/N19-1300).\n' +
      '* Clark 등(2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문을 풀었다고 생각해? try arc, ai2 reasoning challenge. _ arXiv:1803.05457v1_, 2018.\n' +
      '* Computer (2023) Together Computer. 레드파자마: 대규모 언어 모델을 훈련하기 위한 오픈 데이터 세트입니다. 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)\n' +
      '* Cubuk et al.(2020) Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: 검색 공간이 감소된 실용적인 자동화된 데이터 증강. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pp. 702-703, 2020.\n' +
      '* Dhole 등 (2020) Kaustubh D. Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahmood, Abinaya Mahendiran, Simon Mille, Ashish Srivastava, Samson Tan, Jascha Sohl-Dickstein, Jinho D. Choi, Eduard Hovy, Ondrej Dusek, Sebastian Ruder, Sajant Anand, Nagender Aneja, Rabin Banjade, Lisa Barthe, Hanna Behnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio Sobrevilla Cabezudo, Samuel Chayawjiaya, Pierre Colombo, Filip Cornell, Gautier Dagan, Mayukh Das, Tanay Dixit, Thomas Dopiere, Paul-Alexis Dray, Suchitta Dubey, Tatiana Kiehnor, Marco Di Giovanni, Rishabh Gupta, L Joniak, Denis Kleyko, Venelin Kovatchev, Kalpesh Krishna, Ashutosh Kumar, Stefan Langer, Seungjae Ryan Lee, Corey James Levinson, Hualou Liang, Kaizhao Liang, Zhexiong Liu, Andrey Lukyanenko, Vukosi Marivate, Gerard de Melo, Simon Meoni, Maxime Meyer, Afnan Mir, Nafise Sadat Moosavi, Niklas Muennighoff, Timothy Sum Hon Mun, Kenton Murray, Marcin Namyski, Maria Obedkova, Priit Oli, Nivranshu Pasricha, Jan Pfister, Richard Plant, Vinay Prabhu, Vasile Pais, Libo Qin, Shahab Raji, Pawan Kumar Rajpoot, Vikas Raunak, Roy Rinberg, Nicolas Roberts, Juan Diego Rodriguez, Claude Roux, Vasconcellos P. H. S., Ananya B. Sai, Robin\n' +
      '\n' +
      '슈미트, 토마스 사이알롬, 세피쇼 세파라, 사칩 N. Shamsi, Xudong Shen, Haoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel, Damien Sileo, Jamie Simon, Chandan Singh, Roman Sitelew, Priyank Soni, Taylor Sorensen, William Soto, Aman Srivastava, KV Aditya Srivatsa, Tony Sun, Mukund Varma T, A Tabassum, Fiona Anting Tan, Ryan Teehan, Mo Tiwari, Marie Tolkiehn, Athena Wang, Zijian Wang, Gloria Wang, Zijie J. Wang, Fuxuan Wei, Bryan Wille, Genta Indra Winata, Xinyi Wu, Witold Wydmanski, Tianbao Xie, Usama Yaseen, M. 이, 징장, 웨장 NI-augmenter: A framework for task-sensitive natural language augmentation, 2021.\n' +
      '* Eisenstein (2013) Jacob Eisenstein. 인터넷에서 나쁜 언어에 대해 어떻게 해야 할까요? In _Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 359-369, Atlanta, Georgia, June 2013. Association for Computational Linguistics. URL [https://aclanthology.org/N13-1037](https://aclanthology.org/N13-1037).\n' +
      '* 엘단 및 리(2023) 로넨 엘단 및 원지 리. Tinynstories: 언어 모델이 얼마나 작고 여전히 일관된 영어를 말할 수 있습니까? _ arXiv preprint arXiv:2305.07759_, 2023.\n' +
      '* Fort 등(2021) Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, and Samuel L Smith. 훈련 중에 이미지당 여러 개의 증강 샘플을 그리면 테스트 오류가 효율적으로 줄어듭니다. _ arXiv preprint arXiv:2105.13343_, 2021.\n' +
      '* Futrell 등(2015) Richard Futrell, Kyle Mahowald, and Edward Gibson. 37개 언어에서 종속성 길이 최소화에 대한 대규모 증거 _ Proceedings of the National Academy of Sciences_, 112(33):10336-10341, 2015.\n' +
      '* Gadre et al. (2023) Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets _ arXiv preprint arXiv:2304.14108_, 2023.\n' +
      '* Gao 등(2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of various text for language modeling _ arXiv preprint arXiv:2101.00027_, 2020.\n' +
      '* Gao 등(2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\'h, Haonan Li, Kyle McDonnell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 소샷 언어 모델 평가를 위한 프레임워크, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).\n' +
      '*Gao et al.(2021) Tianyu Gao, Xingcheng Yao, and Danqi Chen. 심세: 문장 임베딩의 단순한 대조적 학습. In _2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021_, pp. 6894-6910. Association for Computational Linguistics (ACL), 2021.\n' +
      '* Gibson et al. (2000) Edward Gibson et al. The dependency locality theory: A distance-based theory of linguistic complexity _ 이미지, 언어, 뇌_, 2000:95-126, 2000.\n' +
      '* Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 교재만 있으면 됩니다. _ arXiv preprint arXiv:2306.11644_, 2023.\n' +
      '* Gururangan et al. (2022) Suchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. 누구의 언어가 고품질로 간주되나요? 텍스트 데이터 선택에서 언어 이데올로기를 측정합니다. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pp. 2562-2580, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.165. URL [https://aclanthology.org/2022.emnlp-main.165](https://aclanthology.org/2022.emnlp-main.165)\n' +
      '* Hendrycks 등(2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 멀티태스킹 언어 이해를 측정하는 중입니다. _ International Conference on Learning Representations (ICLR)_, 2021.\n' +
      '\n' +
      '* Hoffer 등(2020) Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. 일괄 처리: 인스턴스 반복을 통해 일반화를 향상시킵니다. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 8129-8138, 2020.\n' +
      '* Hoffmann 등 (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* Jiang 등(2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _ arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* Jin et al.(2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: 생의학 연구 질의 응답을 위한 데이터 세트. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 2567-2577, 2019.\n' +
      '* Welbl et al.(2017) Matt Gardner Johannes Welbl, Nelson F. Liu. 선다형 과학 문제를 크라우드소싱합니다. 2017년\n' +
      '* Jung et al.(2023) Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. 불가능한 증류: 저품질 모델에서 고품질 데이터 세트 및 요약 및 패러프레이징을 위한 모델입니다. _ arXiv preprint arXiv:2305.16635_, 2023.\n' +
      '* Koksal et al. (2023) Abdullatif Koksal, Timo Schick, Anna Korhonen, and Hinrich Schutze. Longform: 말뭉치 추출을 통해 긴 텍스트 생성을 위한 명령어 튜닝 최적화_ arXiv preprint arXiv:2304.08460_, 2023.\n' +
      '* Kwon et al. (2023) 우석 권, 주한 리, 시위안 장, 영성, 리안민 정, 코디 하오 유, 조셉 E. 곤잘레스, 하오 장, 및 이온 스토이카. 페이지 주의와 함께 제공되는 대용량 언어 모델을 위한 효율적인 메모리 관리. 2023년 ACM SIGOPS 29th Symposium에서.\n' +
      '* Li et al.(2021) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Koetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonzhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadori, Lopesh Kumar Umapatih, Ziruo Wang, Rudra Murthy, Jason Stillerman, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Manhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Bhattacharyya, Manhao Yu, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane 스토커: 출처가 당신과 함께 있기를! 2023a.\n' +
      '* Li et al.(2023b) Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 명령어 역번역과의 자체 정렬입니다. _ arXiv preprint arXiv:2308.06259_, 2023b.\n' +
      '* Li et al.(2023c) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. ii: phi-1.5 기술 보고서만 있으면 됩니다. _ arXiv preprint arXiv:2309.05463_, 2023c.\n' +
      '* Lin et al.(2021) Stephanie Lin, Jacob Hilton, and Owain Evans. 진실성: 모델들이 어떻게 인간의 거짓을 모방하는지 측정하는 것, 2021년.\n' +
      '\n' +
      '* Liu et al.(2023) Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi Zhang. Tinygsm: 작은 언어 모델을 사용하여 gsm8k에서 80%를 달성합니다. _ arXiv preprint arXiv:2312.09241_, 2023a.\n' +
      '* Liu et al.(2023b) Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang. Logiqa 2.0 -- 자연어 이해에서 논리적 추론을 위한 개선된 데이터 세트입니다. _ IEEE/ACM Transactions on Audio, Speech, and Language Processing_, pp. 1-16, 2023b. doi: 10.1109/TASLP.2023.3293046.\n' +
      '* Liu et al.(2023c) Xiaoxuan Liu, Lanziang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. 온라인 추측 디코딩입니다. _ arXiv preprint arXiv:2310.07177_, 2023c.\n' +
      '* Luukkonen 등(2023) Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Liappala, Niklas Muennighoff, Aleksandra Piktus, et al. Fingpt: Large generative models for a small language. _ arXiv preprint arXiv:2311.05640_, 2023.\n' +
      '* Maini (2023) Pratyush Maini. Phi-1.5 모델: 사과와 오렌지를 비교하는 경우? 2023. URL [https://pratyushmaini.github.io/phi-1.5/](https://pratyushmaini.github.io/phi-1.5/)\n' +
      '* Maini 등 (2023) Pratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico Kolter, and Aditi Raghunathan. T-화성: 텍스트 특징 학습을 우회하여 시각적 표현을 개선합니다. _ arXiv preprint arXiv:2307.03132_, 2023.\n' +
      '* Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 갑옷이 전기를 통할 수 있나요? 오픈 북 질문 응답을 위한 새 데이터 세트 2018년 \'EMNLP\'에서요\n' +
      '* Muennighoff 등(2022) Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 데이터 제한 언어 모델의 크기 조정 _ arXiv preprint arXiv:2305.16264_, 2023.\n' +
      '* Ouyang et al.(2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _ Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.\n' +
      '* Oya (2021) Masanori Oya. 다국어 병렬 말뭉치에서 문장의 평균 의존 거리는 세 가지 유형이다. In _Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation_, pp. 652-661, 2021.\n' +
      '* Padmakumar et al. (2023) Vishakh Padmakumar, Behnam Hailyatnia, Di Jin, Patrick Lange, Seokhwan Kim, Nanyun Peng, Yang Liu, and Dilek Hakkani-Tur. 변압기 모델에 대한 오픈 도메인 대화 컨텍스트의 표현을 조사한다. In _Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue_, pp. 538-547, Prague, Czechia, September 2023. Association for Computational Linguistics. URL [https://aclanthology.org/2023.sigdial-1.50](https://aclanthology.org/2023.sigdial-1.50).\n' +
      '* Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 매 llm에 대한 정제된 웹 데이터 세트: 웹 데이터 및 웹 데이터만으로 선별된 말뭉치를 능가합니다. _ arXiv preprint arXiv:2306.01116_, 2023.\n' +
      '* Radford et al.(2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pp. 8748-8763. PMLR, 2021.\n' +
      '* Rae 등(2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _ arXiv preprint arXiv:2112.11446_, 2021.\n' +
      '* Rae et al.(2021)* Raffel et al.(2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 통합 텍스트 대 텍스트 변환기를 사용하여 전이 학습의 한계를 탐색합니다. _ The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.\n' +
      '* Schuhmann 등(2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models _ Advances in Neural Information Processing Systems_, 35:25278-25294, 2022.\n' +
      '* Shen et al. (2023) Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. 슬림파자마-dc: llm 훈련을 위한 데이터 조합을 이해합니다. _ arXiv preprint arXiv:2309.10818_, 2023.\n' +
      '* Shumailov 등(2023) Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 모델 치매: 생성된 데이터는 모델을 잊게 합니다. _ arXiv preprint arXiv:2305.17493_, 2023.\n' +
      '* Solaiman & Dennison (2021) Irene Solaiman and Christy Dennison. 언어 모델을 값을 대상으로 한 데이터 세트를 사용하여 사회(손바닥)에 적용하는 프로세스 _ Advanced in Neural Information Processing Systems_, 34:5861-5873, 2021.\n' +
      '* Touvron 등(2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* Trabucco 등(2023) Brandon Trabucco, Kyle Doherty, Max Gurnias, and Ruslan Salakhutdinov. 확산 모델을 사용하여 효과적인 데이터 증강. _ arXiv preprint arXiv:2302.07944_, 2023.\n' +
      '* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 주목해 주십시오. _ 신경 정보 처리 시스템의 발전_, 30, 2017.\n' +
      '* Villalobos 등(2022) Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 데이터가 부족할까요? 기계 학습에서 데이터 세트의 크기 조정 한계에 대한 분석 _ arXiv preprint arXiv:2211.04325_, 2022.\n' +
      '* Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 매직코더: 소스 코드만 있으면 됩니다. _ arXiv preprint arXiv:2312.02120_, 2023.\n' +
      '* Wenzek 등 (2020) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: 웹 크롤 데이터에서 고품질 단일 언어 데이터 세트를 추출합니다. In _Proceedings of the 12th Language Resources and Evaluation Conference_, pp. 4003-4012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL [https://aclanthology.org/2020.1rec-1.494](https://aclanthology.org/2020.1rec-1.494).\n' +
      '*Xia et al.(2024) Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 대규모 언어 모델 추론의 잠금 해제 효율성: 추측적 디코딩에 대한 포괄적인 조사, 2024.\n' +
      '*Xie et al. (2023) Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. 도레미: 데이터 혼합을 최적화하면 언어 모델 사전 훈련이 빨라집니다. _ arXiv preprint arXiv:2305.10429_, 2023.\n' +
      '*Xue et al. (2023) Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. 반복하거나 반복하지 않으려면: 토큰 위기에서 llm 크기 조정에 대한 통찰력 _ arXiv preprint arXiv:2305.13230_, 2023.\n' +
      '* Yu et al. (2023) Haichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, and Heng Wang. 악마는 세부 사항에 있습니다: 데이터 필터링의 토끼 구멍으로 깊이 잠입합니다. _ arXiv preprint arXiv:2309.15954_, 2023.\n' +
      '\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yoatan Bisk, Ali Farhadi, and Yejin Choi. 헬라스와그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? <컴퓨팅 언어학 협회 제57차 연례 회의>, 2019.\n' +
      '* Zhang 등(2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: 오픈소스 소형 언어 모델, 2024.\n' +
      '* Zhou 등(2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment _ arXiv preprint arXiv:2305.11206_, 2023.\n' +
      '\n' +
      'Dataset Details\n' +
      '\n' +
      '### Training Dataset\n' +
      '\n' +
      '실험에서 주요 사전 훈련 코퍼스는 1,700억 개 이상의 토큰으로 구성된 선별된 영어 텍스트 데이터 세트인 거대 클린 크롤드 코퍼스(C4)이다. 이 코퍼스는 LLMs Brown 등(2020); Raffel 등(2020); Touvron 등(2023)의 사전 훈련에서 일반적인 관행인 CommonCrawl에서 파생되었다. 이 데이터 소스는 또한 The Pile Gao et al.(2020) 및 RedPajama Computer(2023)를 포함하여 공개적으로 사용 가능한 LLM 사전 훈련 말뭉치에 두드러지게 나타난다. CommonCrawl 데이터에는 다양한 버전이 있으며 사전 훈련을 위한 C4의 선택은 크기와 품질에 의해 결정된다.\n' +
      '\n' +
      '또한 정제된 웹 코퍼스 Penedo et al.(2023)에 대한 사전 훈련과 비교한다. 데이터 세트도 CommonCrawl에서 파생되지만 더 엄격한 필터링 프로세스가 있습니다. 개선된 웹의 선택은 합성 재구문을 웹 데이터의 고품질 하위 집합과 비교하기 위한 것이며, 이는 큐레이트된 데이터 집합 Penedo et al.(2023)과 유사한 성능을 달성하는 것으로 나타났다. 실험은 처음 3050개의 파일을 사용하여 C4에 대한 훈련과 일치하는 300B 토큰에 대해 훈련하고, 정제된 웹 데이터 세트의 여러 에포크를 설명하기 위해 처음 1650개의 파일을 사용하여 실험을 수행했다.\n' +
      '\n' +
      '### 파일 Perplexity 평가\n' +
      '\n' +
      '평가 단계에서는 파일 코퍼스에서 20개의 하위 집합을 사용했다. 우리는 유로파일의 하위 집합이 비영어 언어를 포함하고 있기 때문에 제외했다. 사용된 하위 집합은 CC, StackExchange, Wikipedia, GitHub, PubMed Abstracts, Openwebtext2, Freelaw, Math, NIH, USPTO, Hackernews, Enron, Books3, PubMed Central, Gutenberg, Arxiv, Bookcorpus2, Opensubtitles, Youtubesubtitles, Ubuntu, 및 Philippers이다. 각 부분 집합에서 처음 10000개의 샘플을 취하여 최대 길이 1024의 문서로 분할한다. 모든 복잡도 도표에서 보고된 평균은 표 7의 비율에 따라 모든 도메인의 복잡도에 대한 가중 평균이다.\n' +
      '\n' +
      '#### a.2.1 파일 가중 평균 비율\n' +
      '\n' +
      '표 7의 파일 검증 세트에서 처음 10,000개의 문서에 따라 샘플에 대한 비율을 보고한다. 가오 등(2020)에 보고된 비율과 비교하여 비율에 약간의 차이가 있지만 대부분의 비율은 유사하다.\n' +
      '\n' +
      '### Zero-shot 평가 데이터 세트\n' +
      '\n' +
      '우리는 다양한 자연 언어 작업에 걸친 능력을 평가하기 위해 총 13개의 서로 다른 제로 샷 벤치마크에 대해 모델을 평가한다. 이러한 벤치마크는 전문 지식과 일반 이해의 두 가지 하위 집합으로 분류된다.\n' +
      '\n' +
      '전문 지식 이 하위 집합은 도메인별 지식 및 전문 지식에 초점을 맞춘 데이터 집합으로 구성됩니다.\n' +
      '\n' +
      '* **ARC 챌린지(ARC-C)**: 이 데이터 세트는 3~9학년 과학 시험 문제를 포함하는 AI2 추론 챌린지(ARC) Clark 등(2018)의 일부입니다. ARC 챌린지 세트에는 고차 추론이 필요한 더 어려운 문제가 포함됩니다.\n' +
      '* **SciQ:** 과학 영역 Johannes Welbl(2017) 내에서 이해 및 추론에서 NLP 모델의 능력을 평가하기 위해 특별히 설계된 과학 시험 문제의 데이터 세트입니다.\n' +
      '* **PubMedQA:** 이 데이터 세트는 생물의학 문헌에 초점을 맞추고 Jin 등(2019)의 의료 및 의료 관련 정보에 대한 이해도를 평가하도록 설계되었습니다.\n' +
      '* **MathQA:** 이 데이터 세트는 수학적 문제 해결의 모델에 도전하며, 숫자 이해 및 추론 기술 Amini 등(2019)이 모두 필요합니다.\n' +
      '\n' +
      '* **MMLU**: 다중 도메인 질문 응답, MMLU는 전문 도메인에서 학계에 이르기까지 광범위한 전문 주제에 대한 모델의 전문성을 평가합니다(헨드릭 등, 2021).\n' +
      '\n' +
      '일반 이해 이 하위 집합에는 일반적인 인지 기술, 언어 이해 및 상식 추론을 테스트하는 데이터 세트가 포함되어 있다.\n' +
      '\n' +
      '* **ARC 이지(ARC-E)**: AI2 추론 챌린지의 이지 세트(Clark 등, 2018)는 ARC-C와 동일한 소스의 질문을 특징으로 하지만 덜 도전적인 것으로 간주되며 고급 추론 기술로 필요하지 않습니다.\n' +
      '* **BoolQ**: 자연어 텍스트의 읽기 이해 및 일반 이해에 중점을 둔 부울(예/아니오) 질문으로 구성된 데이터 세트입니다(Clark 등, 2019).\n' +
      '* **위노그란데(위노)**: 이 데이터 세트는 대명사 모호성 해소 작업(ai2, 2019)에 초점을 맞춘 언어 컨텍스트의 상식 추론 모델에 도전합니다.\n' +
      '* **PIQA**: 물리적 상호 작용 질문 응답은 실제 상식의 한 측면인 일상적인 물리적 프로세스에 대한 이해를 테스트합니다(Bisk et al., 2020).\n' +
      '* **HellaSwag**: 이 데이터 세트는 언어 이해 및 상식 추론 모두를 필요로 하는 컨텍스트 및 논리 일관성 있는 방식으로 시나리오를 완료하는 모델의 능력을 평가합니다(Zellers et al., 2019).\n' +
      '* **TruthfulQA**: 진실되고 정확한 답변 생성을 중심으로 이 데이터 집합은 사실적으로 올바른 정보를 식별 하 고 재현 하는 능력에 대 한 모델에 도전 합니다 (Lin 등, 2021).\n' +
      '* **OpenBookQA(OBQA)**: OpenBookQA는 광범위한 사실 및 개념을 이해해야 하므로 모델의 광범위한 지식 및 추론 기술을 평가합니다(Mihaylov et al., 2018).\n' +
      '* **LogiQA-2**: 이 데이터 세트에는 논리적 추론, 논리적 구성 및 원칙을 이해하고 적용하는 모델의 기능을 테스트하는 작업이 포함됩니다(Liu 등, 2023b).\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c} \\hline \\hline Dataset & Validation Ratio (\\%) & Published Ratio (\\%) \\\\ \\hline ArXiv & 10.4 & 9.0 \\\\ BookCorpus2 & 0.8 & 0.8 \\\\ Books3 & 11.8 & 12.1 \\\\ Pile-CC & 14.0 & 18.11 \\\\ Enron & 0.1 & 0.1 \\\\ EuroParl & 1.1 & 0.7 \\\\ FreeLaw & 5.3 & 6.1 \\\\ Github & 10.9 & 7.6 \\\\ Gutenberg & 1.5 & 2.2 \\\\ Hackernews & 0.6 & 0.6 \\\\ Dm Mathematics & 2.0 & 1.2 \\\\ NIH & 0.2 & 0.3 \\\\ OpenSubtitles & 1.3 & 1.6 \\\\ OpenWebText2 & 8.2 & 10.0 \\\\ PhilPapers & 0.7 & 0.4 \\\\ PubMed Abstracts & 0.7 & 3.1 \\\\ PubMed Central & 14.9 & 14.4 \\\\ StackExchange & 5.8 & 5.1 \\\\ Ubuntu & 1.3 & 0.9 \\\\ USPTO & 2.7 & 3.7 \\\\ Wikipedia & 3.4 & 1.5 \\\\ YoutubeSubtitles & 0.6 & 0.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: 공개된 비율과 비교한 평가를 위한 파일 비율 이러한 하위 집합의 각 데이터 세트는 과학, 의학 및 수학의 영역별 지식에서 상식 추론 및 일반 언어 이해와 같은 광범위한 기술에 이르기까지 자연어 처리 모델의 특정 측면에 도전하고 평가하기 위해 신중하게 선택된다.\n' +
      '\n' +
      '## 합성 데이터에 대한 부록 B 필터링 세부 정보\n' +
      '\n' +
      '언어 모델을 사용하여 합성 패러프레이즈를 생성할 때 생성된 출력에서 외부 도입의 문제에 종종 직면한다. 이러한 패러프레이징은 "여기 패러프레이징이 있습니다...", "다음,..."과 같은 문구로 시작되거나 "고급 영어"와 같은 키워드가 포함될 수 있습니다. 이를 완화하기 위해 합성 출력을 필터링하고 정제하는 방법을 개발했습니다.\n' +
      '\n' +
      '### Methodology\n' +
      '\n' +
      '기본 함수인 remove_unwanted_part는 입력 데이터를 개별 문장으로 분할하는 것으로 시작한다. 첫 문장이 "\\n\\n"(새로운 단락을 나타냄) 또는 ":"과 같은 구분자를 포함하는 경우, 함수는 전술한 원하지 않는 요소에 대해 구분자 앞의 세그먼트를 확인한다. 이러한 요소들이 검출되면, 선행 세그먼트가 제거된다. 그런 다음 수정된 전체 내용을 재구성하고 반환합니다. 수정이 적용되지 않지만 플래그된 키워드가 여전히 있는 경우 패러프레이즈를 완전히 제거합니다. 이를 달성하기 위하여:\n' +
      '\n' +
      '1. NLTK의 문장 분할기 함수를 이용하여 입력 데이터를 개별 문장으로 분할한다.\n' +
      '2. 구분 기호의 존재에 대한 첫 번째 문장을 검사한다.\n' +
      '3. 구분 기호가 탐지되면 이전 세그먼트에 원하지 않는 요소가 있는지 확인합니다.\n' +
      '4. 원하지 않는 엘리먼트들이 발견되면, 선행 세그먼트("\\n\\n" 또는 ":"의 발생 전에)를 폐기한다.\n' +
      '5. 필터링된 문단을 수정하고 반환한다.\n' +
      '\n' +
      '수동 검사 결과, 수정 후 오류율(원하지 않는 요소를 가진 문장의 발생)은 0.1% 미만임을 알 수 있었다.\n' +
      '\n' +
      '## 부록 C 합성 말뭉치의 속성\n' +
      '\n' +
      '재구문 모델에서 생성된 합성 데이터의 특성을 이해하기 위해 합성 데이터, C4 데이터 및 파일 데이터 간의 의미 유사성, 구문 복잡성 및 다양성을 비교한다. 우리의 주요 초점은 합성 데이터에 대한 다음 질문에 답하는 것입니다. (i) 합성 데이터에 대해 훈련된 모델이 재구어 모델로부터의 정보 유출로 인해 더 나은 성능을 발휘합니까? (ii) 재구문 모델이 여러 스타일을 정확하게 포착하는가? (iii) 합성 데이터의 어떤 속성이 이를 고품질로 만드는가? 우리의 조사는 특정 도메인에 대한 더 나은 일반화에 도움이 되는 데이터를 해결하고 데이터 변동성과 품질의 중요성을 정량화하는 데 도움이 된다.\n' +
      '\n' +
      '### Experimental Setup\n' +
      '\n' +
      '우리는 각각의 데이터 세트에서 처음 1000개의 문서의 부분 집합을 취한다. 실제 C4 데이터와의 합성 비교를 위해 샘플 쌍을 취하는 반면 파일 하위 집합의 경우 테스트 하위 집합에서 처음 1000개의 샘플을 취한다. 데이터 세트 품질 통계를 계산할 때 메트릭 값에서 두 개 이상의 표준 편차를 제거합니다. 파일 하위 집합의 샘플 수가 1000개 미만일 때 샘플을 분할했다. 분포가 있는 그림은 가우시안 커널 밀도 추정기(KDE)를 사용하여 1000개의 값으로부터 통계에 대한 분포를 구성한다.\n' +
      '\n' +
      '### Semantic Properties\n' +
      '\n' +
      '섹션 6에서는 성능 이득이 그림 8(a) 및 (b)의 중간 및 qa 프롬프트에 대해 SimCSE 목적(Gao 등, 2021)으로 훈련된 사전 훈련된 BERT 모델을 사용하여 재구어 모델로부터의 지식 누출에 기인하지 않는다는 것을 확인하기 위해 합성 및 실제 데이터의 예 쌍을 비교했다. 그림 9(c)의 MRPC 말뭉치를 이용하여 합성 말뭉치와 실제 말뭉치의 유사성을 추가적으로 비교한다. 우리는 R1과 R2 문장의 분할 비교를 유지하면서 RealP(real paraphrase)로 이 추가 비교를 나타낸다. 합성 구문은 MRPC 말뭉치에 따라 실제 구문에 비해 평균 코사인 유사도가 유사하고 스프레드가 낮다.\n' +
      '\n' +
      '의미 정보는 C4와 합성 데이터 간에 유사하므로 데이터의 양식적 차이를 추가로 조사한다. 그림 10(a)는 다른 재구어 스타일과 파일에 대한 플레쉬-킨케이드 읽기 수준을 보여준다. 우리의 연구 결과는 C4가 읽기 수준의 낮은 끝(7-8)에 있음을 나타낸다. 대조적으로, 매체는 판독 수준을 10으로 증가시키고, qa 합성 변이체는 판독 수준을 6으로 추가로 감소시킨다. 중간 합성 데이터는 위키피디아의 판독 수준과 일치하고, 다른 높은 판독 수준 데이터 세트는 이들 도메인에서 더 나은 성능을 산출한다. QA 합성 데이터에서 우리는 감소된 읽기 수준을 관찰한다. 문장이 일반적으로 문장과 답변으로 분할되어 원문과 중간 스타일의 구문에 비해 짧은 문장으로 이어지는 것을 관찰했기 때문이다.\n' +
      '\n' +
      '그림 10: C4 및 파일의 서로 다른 하위 집합과 비교하여 합성 데이터의 가독성 및 다양성(ttr) 비교.\n' +
      '\n' +
      '그림 9: 코사인 유사성 매체 합성 MRPC 재구문은 많은 메트릭에 대해 더 낮은 메트릭 값으로 이어진다. 유형 토큰 비율의 경우 다양성은 파일의 중간 하위 집합과 대부분의 하위 집합 간에 매우 유사하다는 점에 유의한다. QA 데이터 세트는 QA 형식 데이터 세트와 더 유사하고 질문 및 답변 형식의 반복이 심하기 때문에 특히 TTR 매칭 ubuntu, github 및 수학이 낮다.\n' +
      '\n' +
      '### Syntactic Properties\n' +
      '\n' +
      '마지막으로, 구문 난이도 Futrell 등(2015), Gibson 등(2000), Oya(2021)의 좋은 척도로 나타난 그림 11의 평균 트리 깊이(의존 트리 깊이의 문장에 대한 평균으로 측정됨)와 평균 의존 거리(문장 내의 모든 단어 쌍의 평균 의존 거리로 측정됨)를 비교한다. 우리는 일반적으로 매체 스타일이 깊이, mdd 및 구문 복잡성을 증가시키는 읽기 수준 및 TTR 다양성과 유사한 경향을 발견한다. 우리는 QA 스타일이 이러한 복잡성을 감소시킨다는 것을 다시 발견한다.\n' +
      '\n' +
      '## 부록 D 평가 메트릭\n' +
      '\n' +
      '평가에 사용되는 메트릭은 _매크로 토큰 수준 perplexity_ 입니다. 인코딩된 텍스트의 배치가 주어지면 토큰 수준에서의 복잡도는 다음과 같이 계산되었다:\n' +
      '\n' +
      '전체 데이터 세트에 대한 누적 손실이 \\(L\\)로 표시되고, 총 토큰 수가 \\(T\\)로 표시되는 경우, 매크로 토큰 수준 복잡도가 \\(\\mathcal{P}\\)로 표시되는 경우 다음과 같이 계산됩니다.\n' +
      '\n' +
      '\\[\\mathcal{P}=\\exp\\left(\\min\\left(20,\\frac{L}{T}\\right)\\right) \\tag{3}\\]\n' +
      '\n' +
      'Where:\n' +
      '\n' +
      '* \\(\\exp\\)는 지수 함수입니다.\n' +
      '* \\(L\\)는 데이터 세트의 모든 이동된 로짓 및 레이블에 대한 누적 손실입니다.\n' +
      '* \\(T\\)는 데이터 집합의 총 토큰 수입니다.\n' +
      '\n' +
      '20의 값은 손실 값이 높은 경우에 메트릭을 안정화시키는 상한으로 작용한다.\n' +
      '\n' +
      '## 부록 E 작은 모델 및 토큰 크기에 대한 추가 결과\n' +
      '\n' +
      '### 350M 모델에 대한 결과 75B 토큰에 대해 학습된 모델\n' +
      '\n' +
      '우리는 더 작은 규모로 모델을 훈련하고 개선을 입증합니다. 특히 총 75B 토큰에 대해 350M GPT-2-medium 아키텍처를 훈련한다. 21개 도메인에 걸쳐 평균화된 파일 복잡도가에만 훈련된 모델의 것보다 훨씬 낮다는 것을 보여준다.\n' +
      '\n' +
      '도 11: 합성 데이터가 더 높은 평균 트리 깊이 및 더 높은 평균 의존 거리(MDD)에 의해 표시되는 더 높은 구문 복잡성을 갖는다는 것을 보여주는 C4 코퍼스로부터의 합성 데이터와 실제 데이터 간의 비교.\n' +
      '\n' +
      '그림 12의 C4, 심지어 그림 1c의 C4에서만 훈련된 1.3B 모델보다 낮다. 우리는 또한 QA 재구문을 추가할 때 표 8-9의 일반 이해 언어 작업에 대해 1.5% 증가하고 전문 지식 작업에 대해 약 3% 증가함을 보여준다. 우리는 또한 이 더 작은 규모의 중간 구문을 실험했다. 우리의 연구 결과는 중간 재구문이 제공하는 고품질은 C4보다 성능이 향상되지만 QA 재구문 성능이 나타내는 스타일과 일치하면 성능이 더욱 향상된다는 것을 나타낸다.\n' +
      '\n' +
      '### 1.3B 모델에 대한 결과 150B 토큰에 대해 훈련된 모델\n' +
      '\n' +
      '150B 토큰에서 1.3B GPT-2-XL 모델을 추가로 훈련하여 걸음 수를 절반으로 줄입니다. 우리는 20개 도메인에 걸쳐 평균화된 파일 복잡도가 그림 13의 C4에서만 훈련된 모델보다 훨씬 낮고 그림 1c의 C4에서만 2배 동안 훈련된 1.3B 모델보다 훨씬 낮다는 것을 보여준다. 우리는 또한 QA 재구문을 추가할 때 표 10-11의 전문 지식 작업에 대해 2% 증가하고 일반 이해 작업에 대해 약 2.5% 증가함을 보여준다. 우리는 또한 이 더 작은 규모의 중간 구문을 실험했으며 다른 소규모 실험과 일치하는 유사한 결과를 보고한다.\n' +
      '\n' +
      '## 부록 F LLM 리더보드 Few-shot 결과\n' +
      '\n' +
      '섹션 4의 주요 실험에서 우리는 모델이 사전 훈련 동안 질문 응답 형식 및 스타일을 학습하기 때문에 합성 재구문으로 훈련된 LLM이 제로 샷 질문 응답 작업에 더 나은 백본임을 보여준다. 이 섹션에서는 합성 재구사에 대한 사전 훈련의 개선이 거의 샷에서도 여전히 존재한다는 것을 보여준다.\n' +
      '\n' +
      '그림 12: 합성 데이터의 여러 스타일을 결합하는 것을 비교하는 파일의 모든 도메인에 걸친 복잡성. 모델은 총 75B 토큰에 대해 훈련된 350M 파라미터이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline Dataset (Real Tok.) & ARC-C & ScIQ & PubMedQA & MathQA & MMLU & Avg \\\\ \\hline C4-15B & 21.2 & 77.1 & 50.6 & 22.2 & 23.1 & 38.8 \\\\ C4-60B & 23.4 & 76.2 & 46.4 & 22.0 & 23.0 & 38.2 \\\\ QA+C4-15B & 24.4 & 79.8 & 56.0 & 21.7 & 22.9 & 41.0 \\\\ Med+C4-15B & 22.7 & 74.5 & 53.6 & 22.0 & 23.1 & 39.2 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 전문화된 지식 태스크들 상의 75B 토큰들에 대해 트레이닝된 350M 파라미터 LLM들의 평가. 이 표는 과학, 의학, 수학, 논리 등 구체적인 영역 지식이 필요한 과제에 대한 성과를 제시하고 있다.\n' +
      '\n' +
      '모델이 테스트 샘플에 액세스할 수 있는 설정입니다. 소샷 성능을 연구하기 위해 OpenLLMLeaderboard2에 존재하는 6개의 태스크에 대해 평가한다:\n' +
      '\n' +
      '각주 2: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n' +
      '\n' +
      '1. ARC-챌린지(25 샷)\n' +
      '2. HellaSwag(10 shot)\n' +
      '3. MMLU(5 shot)\n' +
      '4. 진실-QA(5 shot)\n' +
      '5. 위노그란데(5샷)\n' +
      '6. GSM8k(5 shot)\n' +
      '\n' +
      '우리는 각각 대략 85B 및 100B 고유 C4 토큰에 해당하는 300B 및 350B 토큰에 대해 훈련된 두 모델을 평가한다. 우리의 연구 결과는 ARC-챌린지 벤치마크와 제로 샷 설정에서 일관된 진실-QA 및 다른 데이터 세트 전반에 걸쳐 유사한 성능에서 상당한 개선을 보여준다. 우리의 모델은 또한 정제된 웹 데이터 세트에서 훈련된 공개적으로 출시된 팔콘-1.3B 모델과 파일에서 훈련된 피티아-1.4B 모델보다 더 나은 성능을 보인다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c} \\hline \\hline Dataset (Real Tok.) & ARC-E & BoolQ & Wino. & PIQA & HellaSwag & TruthfulQA & OBQA & LogQA & Avg \\\\ \\hline C4-18B & 50.5 & 52.8 & 53.0 & 69.8 & 35.6 & 37.8 & 18.6 & 23.0 & 42.6 \\\\ C4-75B & 51.4 & 53.4 & 51.6 & 70.3 & 36.1 & 39.0 & 17.4 & 22.6 & 42.7 \\\\ QA+C4-18B & 53.4 & 60.7 & 52.2 & 70.0 & 36.3 & 40.0 & 17.6 & 22.3 & 44.1 \\\\ Med+C4-18B & 50.6 & 57.3 & 53.6 & 70.8 & 36.1 & 36.9 & 18.6 & 22.0 & 43.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 일반 이해 작업 상의 75B 토큰에 대해 트레이닝된 350M 파라미터 LLM의 평가. 이 표는 일반적인 추론, 언어 이해 및 훈련을 비교하는 상식에 초점을 맞춘 다양한 데이터 세트에 대한 성능을 보여준다.\n' +
      '\n' +
      '그림 13: 합성 데이터의 여러 스타일을 결합하는 것을 비교하는 파일의 모든 도메인에 걸친 복잡성. 모델은 총 75B 토큰에 대해 훈련된 350M 파라미터이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c} \\hline Dataset (Real Tok.) & ARC-C & SciQ & PubMedQA & MathQA & MMLU & Avg \\\\ \\hline C4-35B & 27.0 & 83.4 & 55.0 & 22.5 & 24.3 & 42.4 \\\\ C4-150B & 25.9 & 83.8 & 55.4 & 23.5 & 25.4 & 42.8 \\\\ Med+C4-35B & 27.2 & 82.2 & 46.2 & 23.1 & 25.2 & 40.8 \\\\ QA+C4-35B & 29.0 & 85.1 & 62.2 & 22.5 & 26.1 & 45.0 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 전문화된 지식 태스크 상의 150B 토큰에 대해 트레이닝된 \\(\\sim\\) 1.3B 파라미터 LLM의 평가. 이 표는 과학, 의학, 수학, 논리 등 구체적인 영역 지식이 필요한 과제에 대한 성과를 제시하고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c} \\hline Dataset (Real Tok.) & ARC-E & BoolQ & Wino. & PIQA & HellaSwag & TruthfulQA & OBQA & LogiQA & Avg \\\\ \\hline C4-35B & 58.6 & 55.2 & 56.1 & 73.9 & 44.5 & 36.0 & 22.2 & 22.8 & 46.2 \\\\ C4-150B & 59.1 & 54.4 & 56.4 & 74.5 & 44.9 & 34.3 & 22.2 & 22.1 & 46.0 \\\\ Med+C4-35B & 59.8 & 57.0 & 55.7 & 74.6 & 44.5 & 36.5 & 23.8 & 21.5 & 46.7 \\\\ QA+C4-35B & 62.2 & 63.3 & 55.7 & 74.8 & 44.6 & 41.4 & 22.4 & 23.2 & 48.4 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: 일반 이해 작업 상의 150B 토큰에 대해 트레이닝된 \\(\\sim\\) 1.3B 파라미터 LLM의 평가. 이 표는 일반적인 추론, 언어 이해 및 훈련을 비교하는 상식에 초점을 맞춘 다양한 데이터 세트에 대한 성능을 보여준다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline Dataset & ARC & Hellaswag & MMLU & TruthfulQA & WinoGrande & GSM8K & Avg \\\\ \\hline C4 & 31.7 & 62.1 & 26.7 & 33.4 & 57.9 & 0.9 & 35.5 \\\\ Falcon-RW & 35.1 & 63.6 & 25.3 & 36.0 & 62.0 & 0.5 & 37.1 \\\\ Pythia-1.4b-Pile & 32.7 & 55.0 & 25.6 & 38.7 & 57.3 & 0.8 & 35.0 \\\\ \\hline QA+C4-85B (300K) & 36.4 & 60.9 & 25.5 & 40.6 & 59.4 & 0.4 & 37.2 \\\\ QA+C4-100B (350K) & 35.5 & 60.5 & 26.8 & 40.6 & 61.3 & 0.3 & 37.5 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 12: 1.3B 300K LLM 리더보드 Eval. 평가는 단일 종자에 대해 수행된다(1234).\n' +
      '\n' +
      '다시 표시 프롬프트 템플릿\n' +
      '\n' +
      '특정 스타일로 C4 데이터 세트의 합성 버전을 생성하기 위해 미스트랄-7B 모델에 제공된 프롬프트를 자세히 설명합니다. _ 참고: 다른 냉동 LLM에 사용된 프롬프트에는 약간의 변형이 있으며 T5 모델에는 프롬프트가 사용되지 않았다._\n' +
      '\n' +
      '#### Easy Style\n' +
      '\n' +
      '유아들이 이해할 수 있는 콘텐츠를 생성하도록 설계된 스타일입니다.\n' +
      '\n' +
      '```\n' +
      '호기심 많은 사용자와 인공 지능 비서 간의 채팅. 조수는 질문에 대해 도움이 되고 세밀하며 예의 바른 답변을 제공합니다. USER: 다음 단락에 대해 유아들이 이해할 수 있는 매우 작은 어휘와 매우 간단한 문장을 사용하여 같은 말을 바꿔주세요.\n' +
      '```\n' +
      '\n' +
      '#### Hard Style\n' +
      '\n' +
      '난해한 언어를 사용하여 주로 학자들이 이해할 수 있는 콘텐츠를 생성하도록 설계된 스타일입니다.\n' +
      '\n' +
      '```\n' +
      '호기심 많은 사용자와 인공 지능 비서 간의 채팅. 조수는 질문에 대해 도움이 되고 세밀하며 예의 바른 답변을 제공합니다. USER: 다음 단락에 대해 박식한 학자만이 이해할 수 있는 매우 긴장되고 난해한 언어를 사용하여 같은 말을 바꿔주세요. 단순한 단어와 구절을 희귀하고 복잡한 단어로 대체합니다.\n' +
      '```\n' +
      '\n' +
      '#### Medium Style\n' +
      '\n' +
      '표준 백과사전 항목에 필적하는 콘텐츠를 생성하도록 설계된 스타일입니다.\n' +
      '\n' +
      '```\n' +
      '호기심 많은 사용자와 인공 지능 비서 간의 채팅. 조수는 질문에 대해 도움이 되고 세밀하며 예의 바른 답변을 제공합니다. USER: 다음 단락에서는 위키피디아의 문장에서와 같은 고급 영어의 다양한 패러프레이즈를 제공합니다.\n' +
      '```\n' +
      '\n' +
      '#### Q/A Style\n' +
      '\n' +
      '서사를 대화 형식으로 변환하기 위한 스타일입니다.\n' +
      '\n' +
      '``` A chat between a curious user and a artificial intelligence assistant. 조수는 질문에 대해 도움이 되고 세밀하며 예의 바른 답변을 제공합니다. USER: 다음 문단을 "Question:"의 여러 태그와 함께 대화 형식으로 변환한 후 "Answer:":Rephrase 예제\n' +
      '\n' +
      '미스트랄-7B 모델에 의해 생성된 MRPC 말뭉치의 샘플이다.\n' +
      '\n' +
      'Original\n' +
      '\n' +
      '그 주식은 금요일 뉴욕 증권 거래소에서 21.51달러로 마감되기까지 약 11%인 2.11달러가 올랐다.\n' +
      '\n' +
      '올해 1분기 매출은 전년 동기 대비 15% 감소했다.\n' +
      '\n' +
      'Medium Style\n' +
      '\n' +
      '이 주식은 금요일 뉴욕 증권 거래소에서 21.51달러에 마감하며 약 11% 상승했으며 2.11달러 상승했다.\n' +
      '\n' +
      '당년도 초기 3개월 동안 전년도의 해당 분기에 비해 15%의 수익 감소가 있었다.\n' +
      '\n' +
      'Q/A Style\n' +
      '\n' +
      '질문: 금요일 주식 종가가 얼마였습니까? 정답: 21.51달러 질문: 금요일에 주가가 얼마나 올랐습니까? 정답: 2.11달러 또는 약 11%입니다.\n' +
      '\n' +
      '질문: 1분기의 수익 감소가 작년 동기 대비 어느 정도였습니까? 정답: 수익은 15퍼센트 하락했다.\n' +
      '\n' +
      '#### Mistral-7B 모델에 의해 생성된 C4 코퍼스의 샘플입니다.\n' +
      '\n' +
      'Original\n' +
      '\n' +
      '직장에서의 스트레스 1차 조사 설문지에 답하는 것은 자발적이며 모든 답변은 익명으로 저장됩니다. 일부 근무 경력, 파트 또는 풀 타임이 있는 경우에만 이 설문지를 작성해 주십시오. 그렇지 않으면 일부 질문에 답할 수 없습니다! 여기 모든 언어 버전에 대한 링크가 있습니다.\n' +
      '\n' +
      '냉동 버거에 문제가 있다는 것은 아닙니다. 핵심은 고기 조미료예요 아주 강하고 맵고 맛있죠 칠면조 버거에는 다른 게 필요해요 갈은 칠면조는 맛이 없을 수 있거든요 햄버거에는 갈은 칠면조, 양파 가루, 고춧가루, 소금, 후추, 계림 후추가 필요합니다. 그리고 나서 마요네즈는 마늘과 양파를 가져갑니다. 그리고 빵, 분명히 스위스 치즈, 양상추, 양파가 필요합니다. 나는 토마토를 좋아하지만 가끔 다른 맛을 방해한다는 것을 발견하기 때문에 이 버거를 생략했다. 다양한 토핑을 원하시면 추가하세요! 먼저 마요네즈를 만들 거예요. 마요네즈에 마늘을 직접 갈아서 소금 한 꼬집을 넣고 레몬즙을 짜세요. 저어주세요 됐어! 맘에 들어요 그리고 나서, 우리는 햄버거를 작업할 것이다. 큰 냄비를 올리브 오일과 함께 중불로 예열하고, 육계를 높게 예열한 다음, 갈은 칠면조에 모든 향신료를 넣으세요.\n' +
      '\n' +
      '벨벳을 으스러뜨리고, 활기차고, 머리부터 발끝까지 좋아하든, 이 시대를 초월한 직물의 순전한 고급스러움과 우아함을 부인할 수 없습니다. 매우 스타일리시할 뿐만 아니라 일상복에 매우 착용할 수 있습니다. 네, 정말이에요! 올해는 재미있는 보석 톤의 벨벳 조각들을 포용하는 것이 전부다. 벨벳이 남색과 검은색의 어두운 음울한 색조와만 연관되었던 시대는 오래전에 사라졌다. 아래에서 우리는 지금 높은 거리에서 가장 탐나는 벨벳 조각들을 모았습니다. 우리는 이미 의상 아이디어를 생각해내고 있어요! 완전히 집착하는 거야 아니면 지루한 거야?\n' +
      '\n' +
      '당신 근처에 있는 1,258 크라이슬러 200 중 하나에 최대 8,086달러를 저축하세요. 에드먼즈 전문가와 소비자 자동차 리뷰, 딜러 리뷰, 자동차 비교 및 가격 책정 도구로 완벽한 자동차를 찾으세요. 연구 2015 크라이슬러 200세단 4D 200C I4 가격, 사용된 값 및 200세단 4D 200C I4 가격, 사양 등이 있습니다. 몇 년 전, 우리는 2011년 크라이슬러 200의 정지 문제에 대해 썼고, 믿거나 말거나 여전히 문제와 관련하여 가끔 전화를 받습니다. 그러나 2015년 말 크라이슬러 200 변속기 문제로 인해 훨씬 더 큰 문제가 전화선을 독점하여 운전자에게 엔진 조명 점검, 가혹한 변속 및 가끔 전력 손실을 초래했습니다. 2015년 크라이슬러 200은 스타일과 마력에 대한 프리미엄을 얻을 수 있지만, 뒷좌석 방과 취급은 다른 곳에서 더 나은 할인이다. 2015년 크라이슬러 200이 왜 The에 의해 8.4 등급인지 알아보세요. 2015년 크라이슬러 200 CARiD.com의 완벽한 림을 어디에서 찾을 수 있는지 모르겠습니다. 크롬, 블랙, 실버 등을 포함한 수많은 디자인과 마감 옵션으로 제공되는 2015년 크라이슬러 200 휠을 대량으로 저장하고 있습니다.\n' +
      '\n' +
      'Medium Style\n' +
      '\n' +
      '업무 관련 스트레스에 대한 조사 초기 단계에서 참여는 선택 사항이며, 모든 응답은 기밀로 유지됩니다. 아르바이트든 풀타임이든 근무 경험이 있는 사람만이 특정 질문에 정확하게 응답할 수 있다는 점에 유의할 필요가 있다. 설문지를 작성하려면 다음 링크를 사용하십시오. 여러 언어로 제공됩니다.\n' +
      '\n' +
      '냉동 버거는 본질적으로 결함이 있는 것은 아니지만 고기 양념은 요리의 풍미를 높이는 중요한 요소입니다. 갈은 칠면조는 향신료를 제대로 섞지 않으면 상당히 밋밋할 수 있으므로 혼합물에 양파 가루, 고춧가루, 소금, 후추, 시엔 후추를 첨가하는 것이 필수적이다. 또한, 버거의 토핑에 필요한 마요네즈에 마늘과 양파를 사용해야 합니다. 마요네즈를 만들기 위해, 마늘을 혼합물에 직접 갈고, 소금 한 꼬집을 넣고, 레몬즙을 짜세요. 잘 섞어주세요. 마요네즈는 버거 토핑에 맛있는 추가입니다. 자, 이제 햄버거를 만들자. 큰 냄비를 중간 센 불에서 올리브 오일로 가열하고, 육계를 높은 온도로 예열하세요. 갈은 칠면조에 양념을 넣고 잘 섞으세요. 버거는 프라이팬에서 각각 5분 정도 익힌 후 육계로 옮겨 2-3분 정도 더 익혀야 한다. 햄버거는 스위스 치즈, 양상추, 양파와 함께 빵에 조립할 준비가 되어 있습니다. 토마토는 토핑으로 추가할 수 있지만 다른 맛을 압도할 수 있습니다. 버거에 포함하고 싶은지 결정하는 것은 개인에게 달려 있습니다. 결론적으로 냉동 버거는 맛있을 수 있지만 요리에 깊이와 풍미를 더하기 위해서는 고기 양념이 필수적이다. 마요네즈와 토핑도 개인의 취향에 맞게 맞춤 제작할 수 있습니다.\n' +
      '\n' +
      '벨벳의 고급스럽고 우아한 성질은 으스러지거나 생동감 넘치거나 머리부터 발끝까지 거부할 수 없다. 이 시대를 초월한 직물은 패셔너블할 뿐만 아니라 데일리 웨어에 착용할 수 있습니다. 최근 몇 년 동안, 벨벳과 남색 및 검은색의 어두운 무디 색조의 전통적인 연관성에서 벗어나 재미 있는 보석 톤의 벨벳 조각을 수용하는 방향으로 전환되었다. 아래는 오늘날 높은 거리에서 구할 수 있는 가장 바람직한 벨벳 조각 중 일부이며, 우리는 이미 그 주변의 의상을 구상하고 있습니다. 벨벳에 완전히 매료되었나요, 아니면 단순히 지루함을 넘어섰나요?\n' +
      '\n' +
      '가까운 1,258개의 크라이슬러 200 중 하나에 최대 8,086달러의 저축을 발견하세요. 에드먼즈에서 전문가 및 소비자 자동차 리뷰, 딜러 리뷰, 자동차 비교 및 가격 책정 도구를 가져옵니다. 2015년 크라이슬러 200 세단 4D 200C I4 가격, 중고 값 등에 대해 4,850,420개의 목록이 있습니다. 2011년 크라이슬러 200의 지연 문제에 대해 썼지만 여전히 문제와 관련하여 가끔 전화를 받습니다. 그러나 최근 더 중요한 문제가 전화선을 넘어섰습니다 - 2015년 크라이슬러 200 변속기 문제로 운전자가 엔진 조명을 확인하고, 심한 변속을 하고, 가끔 전력이 손실됩니다. 스타일과 마력에 대한 프리미엄 가격에도 불구하고 뒷좌석과 취급은 다른 곳에서 더 나은 할인입니다. 2015년 크라이슬러 200은 The Car Connection에 의해 8.4 등급이다. 2015년 크라이슬러 200 CARiD.com의 완벽한 림을 찾고 있다면 크롬, 블랙, 실버 및 더 많은 Q/A 스타일을 포함한 다양한 디자인과 마감 옵션으로 2015년 크라이슬러 200 바퀴를 광범위하게 제공합니다.\n' +
      '\n' +
      '질문: 직장 조사에서 스트레스에 참여하는 것이 의무적인가? 정답: 아니, 그것은 자발적이다. 질문: 모든 답은 익명으로 저장되나요? 정답: 네, 그럴 겁니다. 질문: 질문지를 받을 자격이 있는 사람은 누구인가? 답변: 아르바이트든 풀타임이든, 업무 경험이 있는 사람만 가능합니다. 질문: 만약 누군가가 질문지를 받을 자격이 없다면 어떻게 되나요? 대답: 그들은 몇 가지 질문에 대답할 수 없을 것이다. 질문: 질문지의 모든 언어 버전에 대한 링크를 제공할 수 있습니까? 정답: 네, 여기 링크가 있습니다.\n' +
      '\n' +
      '질문: 무엇이 이 냉동 버거를 그렇게 맛있게 만드는가? 정답: 고기 양념은 강하고 맵고, 햄버거에 좋은 맛을 줍니다. 질문은 갈은 칠면조가 맛이 없을 수 있다고 생각하나요? 정답: 네, 갈은 칠면조는 맛이 없을 수 있지만 향신료와 양념을 첨가하는 것이 도움이 될 수 있습니다. 질문: 이 칠면조 버거를 만드는 데 무엇이 필요합니까? 정답: 버거에 갈은 칠면조, 양파 가루, 고춧가루, 소금, 후추, 사이엠페퍼. 그리고 나서 마요네즈는 마늘과 양파를 가져갑니다. 빵, 스위스 치즈, 양상추, 양파도 필요합니다. 질문: 토핑은 어떤가요? 정답: 토핑의 배열을 원한다면 토마토를 넣으세요. 질문: 마요네즈는 어떻게 만드나요? 정답: 마늘을 마요네즈에 직접 넣고 소금 한 꼬집을 넣은 다음 레몬즙을 짜세요. 저어주세요 도널 질문: 당신은 햄버거를 어떻게 만드나요? 정답: 큰 냄비를 올리브 오일로 중간 센 불로 예열하고, 육계를 높은 불로 예열한 다음, 갈은 칠면조에 모든 향신료를 넣으세요.\n' +
      '\n' +
      '질문: 벨벳에 대해 어떻게 생각하세요? 정답: 여러분이 벨벳을 으스러뜨리고, 활기차고, 머리부터 발끝까지 좋아하는지, 이 시대를 초월한 직물의 순전한 고급스러움과 우아함을 부인할 수 없습니다. 질문: 벨벳이 정말 멋있나요? 정답: 네, 정말이에요! 올해는 재미있는 보석 톤의 벨벳 조각들을 포용하는 것이 전부다. 질문: 벨벳은 남색과 흑색의 어두운 음울한 색조와만 관련이 있었는가? 정답: 벨벳이 남색과 검은색의 어두운 음울한 색조와만 연관되었던 시대는 오래전에 사라졌다. 질문: 지금 높은 거리에 탐나는 벨벳 조각이 있습니까? 정답: 우리는 지금 높은 거리에서 가장 탐나는 벨벳 조각들을 모았습니다. 질문: 당신은 그것에 완전히 집착하는가 아니면 지겹지 않은가? 정답: 우리는 이미 의상 아이디어를 생각해내고 있습니다!\n' +
      '\n' +
      '질문: 내가 어떻게 크라이슬러 200에 돈을 저축할 수 있을까? 대답: 당신 근처에 있는 1,258개의 크라이슬러 200 중 하나에 8,086달러까지 저축하라. 에드먼즈 전문가와 소비자 자동차 리뷰, 딜러 리뷰, 자동차 비교 및 가격 책정 도구로 완벽한 자동차를 찾으세요. 질문: 2015년 크라이슬러 200의 문제점은 무엇입니까? 답변: 우리는 4,850,420을 가지고 있습니다. 연구 2015년 크라이슬러 200 세단 4D 200C I4 가격, 사용된 값 및 200 세단 4D 200C I4 가격, 사양 등입니다. 몇 년 전, 우리는 2011년 크라이슬러 200의 지연 문제에 대해 썼고, 믿거나 말거나 여전히 문제와 관련하여 가끔 전화를 받습니다. 그러나 2015년 말 크라이슬러 200 변속기 문제로 인해 운전자가 엔진 조명 점검, 가혹한 변속, 간헐적인 전력 손실로 인해 훨씬 더 큰 문제가 전화선을 독점했다. 질문: 2015년 크라이슬러 200을 사는 것의 이점은 무엇입니까? 대답: 2015년 크라이슬러 200은 스타일에 대한 프리미엄을 얻을 수 있으며 마력은 있지만 뒷좌석 방과 취급은 다른 곳에서 더 나은 할인입니다. 질문: 2015년 크라이슬러 200은 어떻게 평가됩니까? 정답: The의 등급은 8.4입니다. 질문: 내 2015 크라이슬러 200을 위한 완벽한 림을 어디서 찾을 수 있을까? 정답: CARiD.com은 크롬, 블랙, 실버 등을 포함한 수많은 디자인과 마감 옵션으로 제공되는 2015 크라이슬러 200 휠을 대량으로 저장한다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
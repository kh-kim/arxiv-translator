<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling</title>
<!--Generated on Mon Jan 29 18:12:18 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.16380v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2401.16380v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2401.16380v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2401.16380v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S1" title="1 Introduction ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S2" title="2 Related Work ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S2.SS0.SSS0.Px1" title="Neural Scaling Laws for Language Models ‣ 2 Related Work ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Neural Scaling Laws for Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S2.SS0.SSS0.Px2" title="Dataset Selection ‣ 2 Related Work ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Dataset Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S2.SS0.SSS0.Px3" title="Data Augmentation and synthetic data ‣ 2 Related Work ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Data Augmentation and synthetic data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S3" title="3 WRAP: Web Rephrase Augmented Pretraining ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_bold">WRAP</span>: Web Rephrase Augmented Pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S3.SS1" title="3.1 Rephrasing the Web ‣ 3 WRAP: Web Rephrase Augmented Pretraining ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Rephrasing the Web</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S3.SS1.SSS0.Px1" title="Rephrasing Styles ‣ 3.1 Rephrasing the Web ‣ 3 WRAP: Web Rephrase Augmented Pretraining ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Rephrasing Styles</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S3.SS1.SSS0.Px2" title="Generating Synthetic Data ‣ 3.1 Rephrasing the Web ‣ 3 WRAP: Web Rephrase Augmented Pretraining ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Generating Synthetic Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S3.SS1.SSS0.Px3" title="Combining Real and Synthetic Data ‣ 3.1 Rephrasing the Web ‣ 3 WRAP: Web Rephrase Augmented Pretraining ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Combining Real and Synthetic Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S3.SS2" title="3.2 Implementation Details ‣ 3 WRAP: Web Rephrase Augmented Pretraining ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S3.SS2.SSS0.Px1" title="Architecture ‣ 3.2 Implementation Details ‣ 3 WRAP: Web Rephrase Augmented Pretraining ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S3.SS2.SSS0.Px2" title="Pre-training ‣ 3.2 Implementation Details ‣ 3 WRAP: Web Rephrase Augmented Pretraining ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Pre-training</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S4" title="4 Perplexity Evaluation ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Perplexity Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S4.SS0.SSS0.Px1" title="Data Complexity ‣ 4 Perplexity Evaluation ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Data Complexity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S4.SS0.SSS0.Px2" title="Learning Speed ‣ 4 Perplexity Evaluation ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Learning Speed</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5" title="5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Zero-shot Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.SS1" title="5.1 Datasets ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.SS1.SSS0.Px1" title="General Understanding ‣ 5.1 Datasets ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">General Understanding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.SS1.SSS0.Px2" title="Specialized Knowledge ‣ 5.1 Datasets ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Specialized Knowledge</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.SS2" title="5.2 Results ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.SS2.SSS0.Px1" title="Baselines Methods ‣ 5.2 Results ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Baselines Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.SS2.SSS0.Px2" title="General Improvements ‣ 5.2 Results ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">General Improvements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.SS2.SSS0.Px3" title="Specialized Knowledge Tasks ‣ 5.2 Results ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Specialized Knowledge Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.SS2.SSS0.Px4" title="Specific Improvements ‣ 5.2 Results ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Specific Improvements</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6" title="6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Analysis and Ablations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.SS1" title="6.1 Data Combination Analysis ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Data Combination Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.SS1.SSS0.Px1" title="RQ1: How important is it to have real C4 data? ‣ 6.1 Data Combination Analysis ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">RQ1: How important is it to have real C4 data?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.SS1.SSS0.Px2" title="RQ2: Does a combination of multiple synthetic datasets improve performance? ‣ 6.1 Data Combination Analysis ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">RQ2: Does a combination of multiple synthetic datasets improve performance?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.SS2" title="6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Method Ablations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.SS2.SSS0.Px1" title="RQ3: How important is to have a high-quality re-phraser? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">RQ3: How important is to have a high-quality re-phraser?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.SS2.SSS0.Px2" title="RQ4: Does synthetic data improve over augmentations? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">RQ4: Does synthetic data improve over augmentations?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.SS2.SSS0.Px3" title="RQ5: How does the style of synthetic data impact performance on specialized domains? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">RQ5: How does the style of synthetic data impact performance on specialized domains?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.SS2.SSS0.Px4" title="RQ6: Is there data leakage from the rephrase model to the trained model? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">RQ6: Is there data leakage from the rephrase model to the trained model?</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S7" title="7 Limitations and Opportunities ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations and Opportunities</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S7.SS1" title="7.1 Cost Analysis ‣ 7 Limitations and Opportunities ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Cost Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S7.SS2" title="7.2 Diversity of Synthetic Generations ‣ 7 Limitations and Opportunities ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Diversity of Synthetic Generations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S8" title="8 Conclusion ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A1" title="Appendix A Dataset Details ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Dataset Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A1.SS1" title="A.1 Training Dataset ‣ Appendix A Dataset Details ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Training Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A1.SS2" title="A.2 Pile Perplexity Evaluation ‣ Appendix A Dataset Details ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Pile Perplexity Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A1.SS2.SSS1" title="A.2.1 Pile Weighted Average Ratios ‣ A.2 Pile Perplexity Evaluation ‣ Appendix A Dataset Details ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.1 </span>Pile Weighted Average Ratios</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A1.SS3" title="A.3 Zero-shot Evaluation Dataset ‣ Appendix A Dataset Details ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Zero-shot Evaluation Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A1.SS3.SSS0.Px1" title="Specialized Knowledge ‣ A.3 Zero-shot Evaluation Dataset ‣ Appendix A Dataset Details ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">Specialized Knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A1.SS3.SSS0.Px2" title="General Understanding ‣ A.3 Zero-shot Evaluation Dataset ‣ Appendix A Dataset Details ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title">General Understanding</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A2" title="Appendix B Filtering Details for Synthetic Data ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Filtering Details for Synthetic Data</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A2.SS1" title="B.1 Methodology ‣ Appendix B Filtering Details for Synthetic Data ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Methodology</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A3" title="Appendix C Properties of Synthetic Corpus ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Properties of Synthetic Corpus</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A3.SS1" title="C.1 Experimental Setup ‣ Appendix C Properties of Synthetic Corpus ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A3.SS2" title="C.2 Semantic Properties ‣ Appendix C Properties of Synthetic Corpus ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Semantic Properties</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A3.SS3" title="C.3 Syntactic Properties ‣ Appendix C Properties of Synthetic Corpus ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Syntactic Properties</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A4" title="Appendix D Evaluation Metrics ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A5" title="Appendix E Additional Results for Smaller Model and Token Sizes ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Additional Results for Smaller Model and Token Sizes</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A5.SS1" title="E.1 Results for 350M Models Trained for 75B Tokens ‣ Appendix E Additional Results for Smaller Model and Token Sizes ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>Results for 350M Models Trained for 75B Tokens</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A5.SS2" title="E.2 Results for 1.3B Models Trained for 150B Tokens ‣ Appendix E Additional Results for Smaller Model and Token Sizes ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.2 </span>Results for 1.3B Models Trained for 150B Tokens</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A6" title="Appendix F LLM Leaderboard Few-shot Results ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>LLM Leaderboard Few-shot Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A7" title="Appendix G Rephrase Prompt Templates ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Rephrase Prompt Templates</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A8" title="Appendix H Rephrase Examples ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>Rephrase Examples</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
<li>failed: pdfcol</li>
<li>failed: shellesc</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2401.16380v1 [cs.CL] 29 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line"><span class="ltx_ERROR undefined" id="id1">\pdfcolInitStack</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">tcb@breakable















































































































































































































































































































</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<h1 class="ltx_title ltx_title_document">Rephrasing the Web:
<br class="ltx_break">A Recipe for Compute and Data-Efficient Language Modeling</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pratyush Maini   
<br class="ltx_break">Carnegie Mellon Univeristy
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id2.1.id1">pratyushmaini@cmu.edu</span>
&amp;Skyler Seto<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>, He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly
<br class="ltx_break">Apple 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id3.2.id2">{sseto,hbai22,grangier,yizhe_zhang,njaitly}@apple.com
<br class="ltx_break"></span>
</span><span class="ltx_author_notes">Equal ContributionWork done during internship at Apple</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.1">Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased.
Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained.
This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web.
In this work, we propose <span class="ltx_text ltx_font_bold" id="id1.1.1">W</span>eb <span class="ltx_text ltx_font_bold" id="id1.1.2">R</span>ephrase <span class="ltx_text ltx_font_bold" id="id1.1.3">A</span>ugmented <span class="ltx_text ltx_font_bold" id="id1.1.4">P</span>re-training&nbsp;(<span class="ltx_text ltx_font_bold" id="id1.1.5">WRAP</span>) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as “like Wikipedia” or in “question-answer format” to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using <span class="ltx_text ltx_font_bold" id="id1.1.6">WRAP</span>&nbsp;on the C4 dataset, which is naturally noisy,
speeds up pre-training by <math alttext="\sim 3\times" class="ltx_math_unparsed" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1b"><mo id="id1.1.m1.1.1">∼</mo><mn id="id1.1.m1.1.2">3</mn><mo id="id1.1.m1.1.3" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="id1.1.m1.1c">\sim 3\times</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">∼ 3 ×</annotation></semantics></math>. At the same pre-training compute budget, it
improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher ‘quality’ than web-scraped data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language model (LLM) pre-training has been largely democratized and open-sourced, allowing various academic labs, and industries to pre-train custom LLMs.
Yet, a key differentiator between these models is the composition and size of the data used to train them.
Data curation strategies are required to filter out scrapes of the web that are unstructured and/or poorly phrased&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Eisenstein, <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib19" title="">2013</a>)</cite>.
While some of these strategies have been made public&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib10" title="">2020</a>; Wenzek et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib67" title="">2020</a>; Penedo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib54" title="">2023</a>)</cite>, most state-of-the-art data curation techniques are unknown to the research community, and only anecdotal evidence remains. Research on data curation requires multiple rounds of re-training, making it an expensive endeavour to document techniques that lead to practical improvements.
On the other hand, scaling laws for language models (such as Chinchilla scaling laws&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hoffmann et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib32" title="">2022</a>)</cite>) show that with increasing model sizes, we should also increase both the training compute and data size linearly. This is infeasible because (a) high-quality data is limited <cite class="ltx_cite ltx_citemacro_citep">(Villalobos et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib65" title="">2022</a>)</cite>, and repeating for even a small number of epochs (4 or more) results in diminishing returns or overfitting <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib50" title="">2023</a>; Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib62" title="">2023</a>; Xue et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib70" title="">2023</a>)</cite>; and (b) pre-training for such long durations is prohibitively expensive.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Meanwhile, the use of synthetic data has gained prominence in the paradigm of aligning pre-trained LLMs via instruction fine-tuning, RLHF&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib51" title="">2022</a>)</cite>, and instruction backtranslation <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib40" title="">2023b</a>)</cite>.
Recently, in the context of pre-training, synthetic data was used to generate datasets such as Tiny Stories&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Eldan &amp; Li, <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib20" title="">2023</a>)</cite>
and Textbook quality synthetic data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib28" title="">2023</a>; Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib41" title="">2023c</a>)</cite>. These were used to train smaller language models (like the Phi model family) that were as performant as larger language models on certain tasks.
However, their data generation process stays largely opaque, and prohibitively expensive, requiring prompting a GPT-3.5 model for generating billions of tokens. Additionally, such data generation can create a large “knowledge bias” by specifically generating data pertaining to tasks that we want to perform well on. While synthetic data has shown promise, it is unclear if this is because of the higher quality nature of synthetic data, or because of strategic topic selection&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Maini, <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib47" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S1.F0.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="742" id="S1.F0.sf1.g1" src="x1.png" width="761">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S1.F0.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="787" id="S1.F0.sf2.g1" src="x2.png" width="761">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_flex_size_3 ltx_align_center" id="S1.F0.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="789" id="S1.F0.sf3.g1" src="x3.png" width="760">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(a) <span class="ltx_text ltx_font_bold" id="S1.F1.2.1">WRAP</span>&nbsp;Recipe: We prompt an off-the-shelf instruction-tuned model to rephrase articles on the web, and pre-train an LLM on a mixture of real and synthetic data. &nbsp;(b) Zero-shot performance of GPT 1.3B models trained on combinations of C4 and synthetic variations. Each step corresponds to a batch of 1M samples. (c) Weighted average perplexity over 21 sub-domains of the Pile for varying model sizes and amount of pre-training data.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we propose <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">W</span>eb <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">R</span>ephrase <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">A</span>ugmented <span class="ltx_text" id="S1.p3.1.4">P</span>re-training&nbsp;(<span class="ltx_text ltx_font_bold" id="S1.p3.1.5">WRAP</span>)—that attempts to bridge
three important challenges stemming from the ambiguity around data curation—
(i) what data should you pre-train on?
(ii) how can you pre-train with limited data?
(iii) how can you pre-train computationally efficiently?
In particular, we show that re-phrasing documents on the web using an off-the-shelf medium size LLM allows models to learn much more efficiently than learning from raw text on the web, and
accounts for
performance gains on out of distribution datasets that <em class="ltx_emph ltx_font_italic" id="S1.p3.1.6">can not</em> be offset with additional web data. Our proposed method involves using a pre-trained off-the-shelf LLM to re-phrase documents from a web corpus into different styles. An overview of our approach is shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S1.F0.sf1" title="0(a) ‣ Figure 1 ‣ 1 Introduction ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">0(a)</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In our work, we tackle two important challenges faced during synthetic data curation in the works of&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Gunasekar et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib28" title="">2023</a>)</cite>—generation cost, and data bias—by rephrasing articles on the web. (i) <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">WRAP</span>&nbsp;allows for using an open source, and much smaller LLM (1.8B/7B v/s GPT3.5) to rephrase unstructured and poorly phrased documents in different styles, since it does not rely on the LLM as a knowledge bank. (ii) Thanks to the information maintaining nature of rephrasing, we are able to leverage the natural diversity of the web, rather than relying on an LLM for information which may be prone to factual errors, and/or data biases. Our work shows that the “style” alone can result in large gains in downstream performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.2">Using <span class="ltx_text ltx_font_bold" id="S1.p5.2.1">WRAP</span>&nbsp;on the C4, we evaluate model performance on 13 different zero-shot tasks, and 21 different language modeling domains of the Pile, and find that pre-training LLMs with synthetic data allows us to train equivalent models with 5x lesser data, or 3x lesser compute. In fact, our synthetic data trained models, also outperform the recent TinyLLama models that were trained for 3 trillion tokens (10x data and compute) across several zero-shot Q/A tasks.
We further observe
a reduction in perplexity by <math alttext="\sim 50\%" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mrow id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml"><mi id="S1.p5.1.m1.1.1.2" xref="S1.p5.1.m1.1.1.2.cmml"></mi><mo id="S1.p5.1.m1.1.1.1" xref="S1.p5.1.m1.1.1.1.cmml">∼</mo><mrow id="S1.p5.1.m1.1.1.3" xref="S1.p5.1.m1.1.1.3.cmml"><mn id="S1.p5.1.m1.1.1.3.2" xref="S1.p5.1.m1.1.1.3.2.cmml">50</mn><mo id="S1.p5.1.m1.1.1.3.1" xref="S1.p5.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><apply id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"><csymbol cd="latexml" id="S1.p5.1.m1.1.1.1.cmml" xref="S1.p5.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S1.p5.1.m1.1.1.2.cmml" xref="S1.p5.1.m1.1.1.2">absent</csymbol><apply id="S1.p5.1.m1.1.1.3.cmml" xref="S1.p5.1.m1.1.1.3"><csymbol cd="latexml" id="S1.p5.1.m1.1.1.3.1.cmml" xref="S1.p5.1.m1.1.1.3.1">percent</csymbol><cn id="S1.p5.1.m1.1.1.3.2.cmml" type="integer" xref="S1.p5.1.m1.1.1.3.2">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">\sim 50\%</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">∼ 50 %</annotation></semantics></math> on the Pile,
and note that our 350M parameter model trained on combinations of real and synthetic rephrases on just <math alttext="15\%" class="ltx_Math" display="inline" id="S1.p5.2.m2.1"><semantics id="S1.p5.2.m2.1a"><mrow id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml"><mn id="S1.p5.2.m2.1.1.2" xref="S1.p5.2.m2.1.1.2.cmml">15</mn><mo id="S1.p5.2.m2.1.1.1" xref="S1.p5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><apply id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1"><csymbol cd="latexml" id="S1.p5.2.m2.1.1.1.cmml" xref="S1.p5.2.m2.1.1.1">percent</csymbol><cn id="S1.p5.2.m2.1.1.2.cmml" type="integer" xref="S1.p5.2.m2.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">15\%</annotation><annotation encoding="application/x-llamapun" id="S1.p5.2.m2.1d">15 %</annotation></semantics></math> of the entire C4 corpus, outperforms pre-training a 1.3B parameter on the entire C4.
Finally, we conduct an analysis on the
potential of data leakage,
properties of synthetic data styles, and how to combine synthetic data for improving <span class="ltx_text ltx_font_bold" id="S1.p5.2.2">WRAP</span>&nbsp;based LLM pre-training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Neural Scaling Laws for Language Models</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Neural scaling laws relate the optimal number of model parameters and amount of training data for a fixed amount of compute. <cite class="ltx_cite ltx_citemacro_citet">Hoffmann et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib32" title="">2022</a>)</cite> presented the Chinchilla scaling laws for language models demonstrating that there was a linear relationship between the size of the model and the amount of training data needed. Their findings indicated that prior models such as Gopher&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib56" title="">2021</a>)</cite> are severely undertrained. Recently, models such as Llama&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib62" title="">2023</a>)</cite> are trained with much more data. These scaling laws were drawn for the paradigm of single-epoch training. Recently, <cite class="ltx_cite ltx_citemacro_citet">Muennighoff et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib50" title="">2023</a>)</cite> showed that
the marginal utility of repeated data rapidly diminishes when training for more than 4 epochs, and formulated scaling laws under repeated data.
Concurrently, <cite class="ltx_cite ltx_citemacro_citet">Xue et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib70" title="">2023</a>)</cite> showed that repeating even small fractions of the pre-training data can lead to overfitting and reduce model performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Dataset Selection</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Selecting high quality data for pre-training LLMs remains an active, high-impact, yet understudied area of research. For instance,
GPT-2 model was pre-trained on all outbound links from Reddit, a social media platform, which received at least 3 karma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib10" title="">2020</a>)</cite>. This was used as a heuristic indicator that the document may be <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.1">interesting, educational, or just funny.</em>
Follow-up works have used other heuristics such as prioritizing documents that resemble wikipedia&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gururangan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib29" title="">2022</a>)</cite>.&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Rae et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib56" title="">2021</a>)</cite>
used multiple heuristic filters to remove documents, such as the absence of certain stopwords, length of the document, percentage of alphabetic characters,
mean word length,
symbol-to-word ratio, percentage of lines starting with a bullet point, or ending with an ellipsis etc. Their work highlights the intricacies of filtering out text data.
An alternative paradigm for building better datasets for training is to distill high-quality datasets. <cite class="ltx_cite ltx_citemacro_citet">Xie et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib69" title="">2023</a>)</cite> proposed a method, DoReMi, to select the best data mixture for pre-training language models by reweighting data from various domains.
Concurrently, <cite class="ltx_cite ltx_citemacro_citet">Abbas et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib2" title="">2023</a>)</cite> showed that de-duplicating pre-training data can improve pre-training efficiency.
Recently several methods were proposed for automatic filtering of low-quality data for faster fine-tuning of LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib11" title="">2023</a>; Solaiman &amp; Dennison, <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib61" title="">2021</a>; Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib74" title="">2023</a>)</cite>. Simultaneously, in the realm of image-language models such as CLIP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Radford et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib55" title="">2021</a>)</cite>, the Datacomp benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gadre et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib23" title="">2023</a>)</cite> and recent entries&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Maini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib48" title="">2023</a>; Yu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib71" title="">2023</a>)</cite> have developed approaches at filtering out low-quality subsets from pre-training datasets like LAION&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Schuhmann et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib58" title="">2022</a>)</cite>, or from scrapes of the common crawl.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Data Augmentation and synthetic data</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Eldan &amp; Li (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib20" title="">2023</a>)</cite> showed that a synthetically generated dataset in the form of stories that toddlers can understand
allows training a small language model that can generate coherent sentences.
<cite class="ltx_cite ltx_citemacro_citet">Gunasekar et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib28" title="">2023</a>)</cite> showed that textbook quality (synthetic) data alone helps models achieve state-of-the-art performance on reasoning and coding tasks. Similar approaches are used in concurrent work for enhancing coding and mathematical reasoning abilities while finetuning <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib43" title="">2023a</a>); Wei et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib66" title="">2023</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Shumailov et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib60" title="">2023</a>)</cite> show that training on synthetic data can actually be harmful for model performance, especially when
we do multiple rounds of pre-training an LLM and then training the next LLM on data generated by the previous one.
On the other hand, some other works have shown that such a strategy can actually be useful.
<cite class="ltx_cite ltx_citemacro_citet">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib40" title="">2023b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Köksal et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib37" title="">2023</a>)</cite> discuss how a model can generate instruction data and then fine-tune on its own generated data
to improve performance.
<cite class="ltx_cite ltx_citemacro_citet">Jung et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib36" title="">2023</a>)</cite> discuss how such repeated cycles of synthetic data can help train a very small paraphrase and summarization model that even outperforms GPT-3.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">The vision and multimodal literatures have also seen a surge of works examining the use of synthetic data for training.
The works of <cite class="ltx_cite ltx_citemacro_citet">Bansal &amp; Grover (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib8" title="">2023</a>); Trabucco et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib63" title="">2023</a>); Azizi et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib5" title="">2023</a>)</cite> have shown that using synthetic data in
combination with real data achieves state of art model performance both in-distribution and out-of-distribution. <cite class="ltx_cite ltx_citemacro_citet">Cubuk et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib17" title="">2020</a>)</cite> used generative models to generate image augmentations for better domain generalization. There are also multiple studies on increasing multiplicity of augmentations and their value for improving generalization <cite class="ltx_cite ltx_citemacro_citep">(Choi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib13" title="">2019</a>; Fort et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib21" title="">2021</a>; Hoffer et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib31" title="">2020</a>)</cite>.
However, <cite class="ltx_cite ltx_citemacro_citet">Alemohammad et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib3" title="">2023</a>)</cite> showed that generated models trained for more than five cycles of their own generated data can undergo severe mode collapse.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_bold" id="S3.1.1">WRAP</span>: Web Rephrase Augmented Pretraining</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Generating synthetic data using an off-the-shelf language model can be both computationally expensive and operationally challenging. Prior approaches to generating synthetic textbook quality data using LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gunasekar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib28" title="">2023</a>)</cite> required (1) a language model that contains sufficient world knowledge to generate articles worth training on, thereby increasing generation cost; (2) a careful selection of prompts that enable generating high quality, and diverse articles that fill any knowledge gaps in the synthetic corpus. This challenge was highlighted in follow-up work of <cite class="ltx_cite ltx_citemacro_citet">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib41" title="">2023c</a>)</cite>, and has the potential of inadvertently creeping in biases in the language models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Maini, <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib47" title="">2023</a>)</cite>, as opposed to those trained on the natural diversity of the web.
As a remedy to the challenge of (i) generation cost, and (ii) data diversity, we propose <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">WRAP</span>&nbsp; that leverages the natural diversity of articles on the web, allowing us to utilize significantly smaller LLMs (than GPT-3.5) to generate high-quality paraphrases of noisy and unstructured articles on the web.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Rephrasing the Web</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">It has been observed in past work that up-weighting high-quality data, such as texts from Wikipedia, can be useful to improve language modeling. These terms have generally been very loosely defined and there is only anecdotal evidence of the same&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib10" title="">2020</a>; Wenzek et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib67" title="">2020</a>)</cite>. At the same time, web data is deficient of text in question-answering or conversational format, which is a prominent use case of language models. Based on these two insights, we design the rephrasing styles for our work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Rephrasing Styles</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">In lieu of the anecdotal evidence above, we attempt rephrasing documents on the web in four different styles—(i) Easy (text that even a toddler will understand); (ii) Medium (in high quality English such as that found on Wikipedia); (iii) Hard (in terse and abstruse language); (iv) Q/A (in conversation question-answering format). In order to operationalize rephrasing in these stylistic variations, we appropriately prompt an instruction-tuned model. The rephrased examples of these four styles and the prompts templates used in our work are provided in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A7" title="Appendix G Rephrase Prompt Templates ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">G</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Generating Synthetic Data</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">Now, we detail how we utilize an instruction-tuned language model to rephrase texts from web-crawled datasets such as C4&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib57" title="">2020</a>)</cite> (which we use for all our experiments). In particular, we use a frozen Mistral-7B instruction-tuned model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib33" title="">2023</a>)</cite> (see Ablations in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6" title="6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">6</span></a> for other models).
To generate synthetic data in “medium” style, the Mistral model is prompted using the following instruction: <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS0.Px2.p1.1.1">“For the following paragraph give me a paraphrase of the same in high-quality English language as in sentences on Wikipedia”</em>. The prompt was created using iterative human feedback by comparing outputs of ‘medium’ sized LLMs with those of GPT-4.
We use the model output to create a parallel corpus of “high-quality” synthetic data corresponding to the original noisy web data. Each example has a maximum of 300 tokens, which was decided based on our empirical observation that asking an LLM to rephrase more than 300 tokens, often led to loss of information. Discussions on data quality can be found in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A3" title="Appendix C Properties of Synthetic Corpus ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">C</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Combining Real and Synthetic Data</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">Our method of re-phrasing web data naturally incorporates the information diversity found on the internet. However, it does not incorporate the noise
in real data. While synthetic data may help LLMs pre-train faster, we also want them to be able to understand noisy web text that may be filled with typos and linguistic errors so that the LLMs do not fail in user facing situations. In order to incorporate this style diversity in language modeling, we sample real and synthetic data in a 1:1 ratio.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Implementation Details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Architecture</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">We train decoder-only transformer models &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib64" title="">2017</a>)</cite> at three different scales, small, medium and XL.
The small-scale (128M parameter) model consists of 12 layers, 12 attention heads, and a hidden dimension size of 768.
The medium-scale (350M parameter) model consists of 24 layers, 16 attention heads, and a hidden dimension size of 1024.
The XL-scale (1.3B parameter) model consists of 24 layers, 16 attention heads, and a hidden dimension size of 2048.
We do not use dropout in either model and a maximum sequence length of 1024. The models are trained using NVIDIA’s <a class="ltx_ref ltx_href" href="https://github.com/NVIDIA/Megatron-LM" title="">Megatron-LM</a> repository.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Pre-training</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.7">We train all our XL models for a total of 300k steps with a batch size of one million tokens, unless otherwise specified.
We use a maximum learning rate of <math alttext="3e^{-4}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">3</mn><mo id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml">⁢</mo><msup id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">e</mi><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml"><mo id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3a" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1"><times id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1"></times><cn id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2">3</cn><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2">𝑒</ci><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3"><minus id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3"></minus><cn id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">3e^{-4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.1.m1.1d">3 italic_e start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> for the 128M, and 350M parameter models, and <math alttext="2e^{-4}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">2</mn><mo id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml">⁢</mo><msup id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.2.cmml">e</mi><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.cmml"><mo id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3a" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1"><times id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1"></times><cn id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2">2</cn><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.2">𝑒</ci><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3"><minus id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3"></minus><cn id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">2e^{-4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.2.m2.1d">2 italic_e start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> for the 1.3B parameter model. The minimum learning rate is <math alttext="1e^{-5}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml">1</mn><mo id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml">⁢</mo><msup id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2.cmml">e</mi><mrow id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.cmml"><mo id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3a" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.cmml">−</mo><mn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"><times id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1"></times><cn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2">1</cn><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.2">𝑒</ci><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3"><minus id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3"></minus><cn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">1e^{-5}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.3.m3.1d">1 italic_e start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>. We use a weight decay of <math alttext="0.01" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><mn id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><cn id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" type="float" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">0.01</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.4.m4.1d">0.01</annotation></semantics></math>, along with a gradient clipping norm of <math alttext="1.0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><mn id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><cn id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" type="float" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">1.0</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.5.m5.1d">1.0</annotation></semantics></math>.
We use cosine learning rate scheduler with a warmup for 1% of the total steps;
and the Adam optimizer with <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.6.m6.1"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml"><msub id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.2.cmml">β</mi><mn id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1"><eq id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1"></eq><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.2">𝛽</ci><cn id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.3">1</cn></apply><cn id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml" type="float" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.6.m6.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math> and <math alttext="\beta_{2}=0.999" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.7.m7.1"><semantics id="S3.SS2.SSS0.Px2.p1.7.m7.1a"><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml"><msub id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.2.cmml">β</mi><mn id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.7.m7.1b"><apply id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1"><eq id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1"></eq><apply id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.2">𝛽</ci><cn id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.3">2</cn></apply><cn id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml" type="float" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.7.m7.1c">\beta_{2}=0.999</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.7.m7.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.999</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S3.F2.g1" src="x4.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S3.F2.2.1">WRAP&nbsp;(C4 + QA-85B) v/s C4</span>: Comparison of perplexity on the Pile for a 1.3B LLM trained for 300B tokens shows that WRAP outperforms models trained on 2x real data.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Perplexity Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate the perplexity of the pre-trained model on the validation set of multiple out-of-distribution datasets.
All models are either trained on the C4 dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib57" title="">2020</a>)</cite>, or a particular stylistic rephrase of the same. All the evaluations are done on 21 sub-domains of the Pile&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib24" title="">2020</a>)</cite>.
These subsets
are created from the first 10,000 documents from each domain of the Pile dataset. We then evaluate the perplexity of the model on these subsets. Additional evaluation details are provided in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A4" title="Appendix D Evaluation Metrics ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">D</span></a>.
It is important to note that we evaluate perplexities on the Pile instead of C4.
Training on multiple distributions of text (synthetic and real web) does come at a small cost of less than 1 perplexity on the C4 validation set. To understand our choice of evaluation, and why we observe this perplexity increase, we note that training over the C4 corpus corresponds to minimizing the objective</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\theta_{c4}=\min_{\theta}\mathbb{E}_{x\sim D_{c4}}\left[\mathcal{L}(\theta;x)%
\right]," class="ltx_Math" display="block" id="S4.E1.m1.3"><semantics id="S4.E1.m1.3a"><mrow id="S4.E1.m1.3.3.1" xref="S4.E1.m1.3.3.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1" xref="S4.E1.m1.3.3.1.1.cmml"><msub id="S4.E1.m1.3.3.1.1.3" xref="S4.E1.m1.3.3.1.1.3.cmml"><mi id="S4.E1.m1.3.3.1.1.3.2" xref="S4.E1.m1.3.3.1.1.3.2.cmml">θ</mi><mrow id="S4.E1.m1.3.3.1.1.3.3" xref="S4.E1.m1.3.3.1.1.3.3.cmml"><mi id="S4.E1.m1.3.3.1.1.3.3.2" xref="S4.E1.m1.3.3.1.1.3.3.2.cmml">c</mi><mo id="S4.E1.m1.3.3.1.1.3.3.1" xref="S4.E1.m1.3.3.1.1.3.3.1.cmml">⁢</mo><mn id="S4.E1.m1.3.3.1.1.3.3.3" xref="S4.E1.m1.3.3.1.1.3.3.3.cmml">4</mn></mrow></msub><mo id="S4.E1.m1.3.3.1.1.2" xref="S4.E1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.3.3.1.1.1" xref="S4.E1.m1.3.3.1.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.3.cmml"><munder id="S4.E1.m1.3.3.1.1.1.3.1" xref="S4.E1.m1.3.3.1.1.1.3.1.cmml"><mi id="S4.E1.m1.3.3.1.1.1.3.1.2" xref="S4.E1.m1.3.3.1.1.1.3.1.2.cmml">min</mi><mi id="S4.E1.m1.3.3.1.1.1.3.1.3" xref="S4.E1.m1.3.3.1.1.1.3.1.3.cmml">θ</mi></munder><mo id="S4.E1.m1.3.3.1.1.1.3a" lspace="0.167em" xref="S4.E1.m1.3.3.1.1.1.3.cmml">⁡</mo><msub id="S4.E1.m1.3.3.1.1.1.3.2" xref="S4.E1.m1.3.3.1.1.1.3.2.cmml"><mi id="S4.E1.m1.3.3.1.1.1.3.2.2" xref="S4.E1.m1.3.3.1.1.1.3.2.2.cmml">𝔼</mi><mrow id="S4.E1.m1.3.3.1.1.1.3.2.3" xref="S4.E1.m1.3.3.1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.3.3.1.1.1.3.2.3.2" xref="S4.E1.m1.3.3.1.1.1.3.2.3.2.cmml">x</mi><mo id="S4.E1.m1.3.3.1.1.1.3.2.3.1" xref="S4.E1.m1.3.3.1.1.1.3.2.3.1.cmml">∼</mo><msub id="S4.E1.m1.3.3.1.1.1.3.2.3.3" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.cmml"><mi id="S4.E1.m1.3.3.1.1.1.3.2.3.3.2" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.2.cmml">D</mi><mrow id="S4.E1.m1.3.3.1.1.1.3.2.3.3.3" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.cmml"><mi id="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.2" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.2.cmml">c</mi><mo id="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.1" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.1.cmml">⁢</mo><mn id="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.3" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.3.cmml">4</mn></mrow></msub></mrow></msub></mrow><mo id="S4.E1.m1.3.3.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.2.cmml"><mo id="S4.E1.m1.3.3.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.2.1.cmml">[</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.3.3.1.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.cmml">ℒ</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml"><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml">(</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">θ</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml">;</mo><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">x</mi><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3" stretchy="false" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.3.3.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S4.E1.m1.3.3.1.2" xref="S4.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.3b"><apply id="S4.E1.m1.3.3.1.1.cmml" xref="S4.E1.m1.3.3.1"><eq id="S4.E1.m1.3.3.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.2"></eq><apply id="S4.E1.m1.3.3.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.3.1.cmml" xref="S4.E1.m1.3.3.1.1.3">subscript</csymbol><ci id="S4.E1.m1.3.3.1.1.3.2.cmml" xref="S4.E1.m1.3.3.1.1.3.2">𝜃</ci><apply id="S4.E1.m1.3.3.1.1.3.3.cmml" xref="S4.E1.m1.3.3.1.1.3.3"><times id="S4.E1.m1.3.3.1.1.3.3.1.cmml" xref="S4.E1.m1.3.3.1.1.3.3.1"></times><ci id="S4.E1.m1.3.3.1.1.3.3.2.cmml" xref="S4.E1.m1.3.3.1.1.3.3.2">𝑐</ci><cn id="S4.E1.m1.3.3.1.1.3.3.3.cmml" type="integer" xref="S4.E1.m1.3.3.1.1.3.3.3">4</cn></apply></apply><apply id="S4.E1.m1.3.3.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1"><times id="S4.E1.m1.3.3.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.2"></times><apply id="S4.E1.m1.3.3.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.3"><apply id="S4.E1.m1.3.3.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.3.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.3.1">subscript</csymbol><min id="S4.E1.m1.3.3.1.1.1.3.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.3.1.2"></min><ci id="S4.E1.m1.3.3.1.1.1.3.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.3.1.3">𝜃</ci></apply><apply id="S4.E1.m1.3.3.1.1.1.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.3.2.1.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.3.3.1.1.1.3.2.2.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.2">𝔼</ci><apply id="S4.E1.m1.3.3.1.1.1.3.2.3.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.3"><csymbol cd="latexml" id="S4.E1.m1.3.3.1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.3.1">similar-to</csymbol><ci id="S4.E1.m1.3.3.1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.3.2">𝑥</ci><apply id="S4.E1.m1.3.3.1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.3.2.3.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3">subscript</csymbol><ci id="S4.E1.m1.3.3.1.1.1.3.2.3.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.2">𝐷</ci><apply id="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.3"><times id="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.1"></times><ci id="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.2">𝑐</ci><cn id="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.3.cmml" type="integer" xref="S4.E1.m1.3.3.1.1.1.3.2.3.3.3.3">4</cn></apply></apply></apply></apply></apply><apply id="S4.E1.m1.3.3.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S4.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1"><times id="S4.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1"></times><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2">ℒ</ci><list id="S4.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2"><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">𝜃</ci><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">𝑥</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.3c">\theta_{c4}=\min_{\theta}\mathbb{E}_{x\sim D_{c4}}\left[\mathcal{L}(\theta;x)%
\right],</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.3d">italic_θ start_POSTSUBSCRIPT italic_c 4 end_POSTSUBSCRIPT = roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_x ∼ italic_D start_POSTSUBSCRIPT italic_c 4 end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ caligraphic_L ( italic_θ ; italic_x ) ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.p1.2">that attempts to exactly model the C4 web text. In contrast, training over multiple styles corresponds to minimizing the risk over a different distribution,</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\theta_{\textbf{WRAP}}=\min_{\theta}\mathbb{E}_{x\sim D_{c4}\cup D_{syn}}\left%
[\mathcal{L}(\theta;x)\right]." class="ltx_Math" display="block" id="S4.E2.m1.3"><semantics id="S4.E2.m1.3a"><mrow id="S4.E2.m1.3.3.1" xref="S4.E2.m1.3.3.1.1.cmml"><mrow id="S4.E2.m1.3.3.1.1" xref="S4.E2.m1.3.3.1.1.cmml"><msub id="S4.E2.m1.3.3.1.1.3" xref="S4.E2.m1.3.3.1.1.3.cmml"><mi id="S4.E2.m1.3.3.1.1.3.2" xref="S4.E2.m1.3.3.1.1.3.2.cmml">θ</mi><mtext id="S4.E2.m1.3.3.1.1.3.3" xref="S4.E2.m1.3.3.1.1.3.3a.cmml">𝐖𝐑𝐀𝐏</mtext></msub><mo id="S4.E2.m1.3.3.1.1.2" xref="S4.E2.m1.3.3.1.1.2.cmml">=</mo><mrow id="S4.E2.m1.3.3.1.1.1" xref="S4.E2.m1.3.3.1.1.1.cmml"><mrow id="S4.E2.m1.3.3.1.1.1.3" xref="S4.E2.m1.3.3.1.1.1.3.cmml"><munder id="S4.E2.m1.3.3.1.1.1.3.1" xref="S4.E2.m1.3.3.1.1.1.3.1.cmml"><mi id="S4.E2.m1.3.3.1.1.1.3.1.2" xref="S4.E2.m1.3.3.1.1.1.3.1.2.cmml">min</mi><mi id="S4.E2.m1.3.3.1.1.1.3.1.3" xref="S4.E2.m1.3.3.1.1.1.3.1.3.cmml">θ</mi></munder><mo id="S4.E2.m1.3.3.1.1.1.3a" lspace="0.167em" xref="S4.E2.m1.3.3.1.1.1.3.cmml">⁡</mo><msub id="S4.E2.m1.3.3.1.1.1.3.2" xref="S4.E2.m1.3.3.1.1.1.3.2.cmml"><mi id="S4.E2.m1.3.3.1.1.1.3.2.2" xref="S4.E2.m1.3.3.1.1.1.3.2.2.cmml">𝔼</mi><mrow id="S4.E2.m1.3.3.1.1.1.3.2.3" xref="S4.E2.m1.3.3.1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.3.3.1.1.1.3.2.3.2" xref="S4.E2.m1.3.3.1.1.1.3.2.3.2.cmml">x</mi><mo id="S4.E2.m1.3.3.1.1.1.3.2.3.1" xref="S4.E2.m1.3.3.1.1.1.3.2.3.1.cmml">∼</mo><mrow id="S4.E2.m1.3.3.1.1.1.3.2.3.3" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.cmml"><msub id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.cmml"><mi id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.2" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.2.cmml">D</mi><mrow id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.cmml"><mi id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.2" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.2.cmml">c</mi><mo id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.1" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.1.cmml">⁢</mo><mn id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.3" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.3.cmml">4</mn></mrow></msub><mo id="S4.E2.m1.3.3.1.1.1.3.2.3.3.1" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.1.cmml">∪</mo><msub id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.cmml"><mi id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.2" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.2.cmml">D</mi><mrow id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.cmml"><mi id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.2" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.2.cmml">s</mi><mo id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.1" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.1.cmml">⁢</mo><mi id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.3" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.3.cmml">y</mi><mo id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.1a" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.1.cmml">⁢</mo><mi id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.4" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.4.cmml">n</mi></mrow></msub></mrow></mrow></msub></mrow><mo id="S4.E2.m1.3.3.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S4.E2.m1.3.3.1.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.2.cmml"><mo id="S4.E2.m1.3.3.1.1.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.1.2.1.cmml">[</mo><mrow id="S4.E2.m1.3.3.1.1.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.3.3.1.1.1.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.2.cmml">ℒ</mi><mo id="S4.E2.m1.3.3.1.1.1.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S4.E2.m1.3.3.1.1.1.1.1.1.3.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml"><mo id="S4.E2.m1.3.3.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S4.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml">(</mo><mi id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">θ</mi><mo id="S4.E2.m1.3.3.1.1.1.1.1.1.3.2.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml">;</mo><mi id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml">x</mi><mo id="S4.E2.m1.3.3.1.1.1.1.1.1.3.2.3" stretchy="false" xref="S4.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.3.3.1.1.1.1.1.3" xref="S4.E2.m1.3.3.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S4.E2.m1.3.3.1.2" lspace="0em" xref="S4.E2.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.3b"><apply id="S4.E2.m1.3.3.1.1.cmml" xref="S4.E2.m1.3.3.1"><eq id="S4.E2.m1.3.3.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.2"></eq><apply id="S4.E2.m1.3.3.1.1.3.cmml" xref="S4.E2.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.3.1.cmml" xref="S4.E2.m1.3.3.1.1.3">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.3.2.cmml" xref="S4.E2.m1.3.3.1.1.3.2">𝜃</ci><ci id="S4.E2.m1.3.3.1.1.3.3a.cmml" xref="S4.E2.m1.3.3.1.1.3.3"><mtext id="S4.E2.m1.3.3.1.1.3.3.cmml" mathsize="70%" xref="S4.E2.m1.3.3.1.1.3.3">𝐖𝐑𝐀𝐏</mtext></ci></apply><apply id="S4.E2.m1.3.3.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1"><times id="S4.E2.m1.3.3.1.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.2"></times><apply id="S4.E2.m1.3.3.1.1.1.3.cmml" xref="S4.E2.m1.3.3.1.1.1.3"><apply id="S4.E2.m1.3.3.1.1.1.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.3.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.3.1">subscript</csymbol><min id="S4.E2.m1.3.3.1.1.1.3.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.3.1.2"></min><ci id="S4.E2.m1.3.3.1.1.1.3.1.3.cmml" xref="S4.E2.m1.3.3.1.1.1.3.1.3">𝜃</ci></apply><apply id="S4.E2.m1.3.3.1.1.1.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.3.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.3.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.2">𝔼</ci><apply id="S4.E2.m1.3.3.1.1.1.3.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3"><csymbol cd="latexml" id="S4.E2.m1.3.3.1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.1">similar-to</csymbol><ci id="S4.E2.m1.3.3.1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.2">𝑥</ci><apply id="S4.E2.m1.3.3.1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3"><union id="S4.E2.m1.3.3.1.1.1.3.2.3.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.1"></union><apply id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.2">𝐷</ci><apply id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3"><times id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.1"></times><ci id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.2">𝑐</ci><cn id="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.3.cmml" type="integer" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.2.3.3">4</cn></apply></apply><apply id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.2">𝐷</ci><apply id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3"><times id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.1"></times><ci id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.2">𝑠</ci><ci id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.3">𝑦</ci><ci id="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.4.cmml" xref="S4.E2.m1.3.3.1.1.1.3.2.3.3.3.3.4">𝑛</ci></apply></apply></apply></apply></apply></apply><apply id="S4.E2.m1.3.3.1.1.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S4.E2.m1.3.3.1.1.1.1.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1"><times id="S4.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1"></times><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.2">ℒ</ci><list id="S4.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.3.2"><ci id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1">𝜃</ci><ci id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2">𝑥</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.3c">\theta_{\textbf{WRAP}}=\min_{\theta}\mathbb{E}_{x\sim D_{c4}\cup D_{syn}}\left%
[\mathcal{L}(\theta;x)\right].</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.3d">italic_θ start_POSTSUBSCRIPT WRAP end_POSTSUBSCRIPT = roman_min start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_x ∼ italic_D start_POSTSUBSCRIPT italic_c 4 end_POSTSUBSCRIPT ∪ italic_D start_POSTSUBSCRIPT italic_s italic_y italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ caligraphic_L ( italic_θ ; italic_x ) ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.2">Solving for equation&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S4.E2" title="2 ‣ 4 Perplexity Evaluation ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">2</span></a> does not minimize the risk over C4-only, and hence it is unfair to compare
<math alttext="\theta_{c4}" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><msub id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mi id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">θ</mi><mrow id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml"><mi id="S4.p3.1.m1.1.1.3.2" xref="S4.p3.1.m1.1.1.3.2.cmml">c</mi><mo id="S4.p3.1.m1.1.1.3.1" xref="S4.p3.1.m1.1.1.3.1.cmml">⁢</mo><mn id="S4.p3.1.m1.1.1.3.3" xref="S4.p3.1.m1.1.1.3.3.cmml">4</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1">subscript</csymbol><ci id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">𝜃</ci><apply id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3"><times id="S4.p3.1.m1.1.1.3.1.cmml" xref="S4.p3.1.m1.1.1.3.1"></times><ci id="S4.p3.1.m1.1.1.3.2.cmml" xref="S4.p3.1.m1.1.1.3.2">𝑐</ci><cn id="S4.p3.1.m1.1.1.3.3.cmml" type="integer" xref="S4.p3.1.m1.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\theta_{c4}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">italic_θ start_POSTSUBSCRIPT italic_c 4 end_POSTSUBSCRIPT</annotation></semantics></math> and
<math alttext="\theta_{\textbf{WRAP}}" class="ltx_Math" display="inline" id="S4.p3.2.m2.1"><semantics id="S4.p3.2.m2.1a"><msub id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mi id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">θ</mi><mtext id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3a.cmml">𝐖𝐑𝐀𝐏</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1">subscript</csymbol><ci id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">𝜃</ci><ci id="S4.p3.2.m2.1.1.3a.cmml" xref="S4.p3.2.m2.1.1.3"><mtext id="S4.p3.2.m2.1.1.3.cmml" mathsize="70%" xref="S4.p3.2.m2.1.1.3">𝐖𝐑𝐀𝐏</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">\theta_{\textbf{WRAP}}</annotation><annotation encoding="application/x-llamapun" id="S4.p3.2.m2.1d">italic_θ start_POSTSUBSCRIPT WRAP end_POSTSUBSCRIPT</annotation></semantics></math> on the C4.
For meaningfully comparing models trained on the C4 and on its synthetic rephrases, we evaluate their generalization capability on 21 different domains of the Pile&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib24" title="">2020</a>)</cite>.
Results for each domain are presented in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S3.F2" title="Figure 2 ‣ Pre-training ‣ 3.2 Implementation Details ‣ 3 WRAP: Web Rephrase Augmented Pretraining ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data Complexity</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S1.F0.sf3" title="0(c) ‣ Figure 1 ‣ 1 Introduction ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">0(c)</span></a>, we show that models trained for fewer tokens (150B) and even smaller 350M models outperform training on the full C4 for 300B tokens indicating faster learning with synthetic rephrases.
On some domains such as ArXiv and HackerNews, we observe that training with synthetic data allows reducing the perplexity by nearly 3x of the perplexity of models trained on real data alone. This suggests that in many cases
it may not be possible to offset
the performance advantage of pre-training on synthetic data
by merely training on more real data.
Overall, on an average of multiple subsets of the Pile,
our models improve perplexity by 50% over models trained on real data alone.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Learning Speed</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">We observe that even at the first checkpoint (10B tokens) of <span class="ltx_text ltx_font_bold" id="S4.SS0.SSS0.Px2.p1.1.1">WRAP</span>&nbsp;training, the average perplexity of the LLM on the Pile is lower than that achieved by pre-training on C4 for 15 checkpoints. This suggests a 15x pre-training speed-up. We defer the discussion on learning speed to ‘zero-shot’ tasks in order to make more meaningful comparisons.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Zero-shot Tasks</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We now evaluate our pre-trained language models on various zero-shot question answering (QA) benchmarks
using the LLM Evaluation Harness<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We use git commit - <span class="ltx_text ltx_font_typewriter" id="footnote1.1">89618bf8</span> for consistency across all experiments with a batch size of 32.</span></span></span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib25" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We evaluate our models on a total of 13 different zero-shot benchmarks to assess their abilities across various natural language tasks like common sense reasoning, language and knowledge understanding and mathematical reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">General Understanding</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">The General Understanding category comprises datasets testing broader cognitive skills and language comprehension. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.1">ARC Easy (ARC-E)</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib15" title="">2018</a>)</cite> is the less challenging counterpart of ARC-C, featuring questions that require basic reasoning skills. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.2">BoolQ</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib14" title="">2019</a>)</cite> includes boolean questions that focus on reading comprehension and general language understanding. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.3">Winogrande (Wino.)</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(ai2, <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib1" title="">2019</a>)</cite> challenges models with common sense reasoning in language, particularly in pronoun disambiguation. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.4">PIQA</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bisk et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib9" title="">2020</a>)</cite> assesses understanding of physical processes, an essential part of practical common sense. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.5">HellaSwag</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zellers et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib72" title="">2019</a>)</cite> tests the ability to complete scenarios coherently, demanding both language understanding and common sense. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.6">TruthfulQA</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib42" title="">2021</a>)</cite> is centered on generating truthful, accurate answers, thus testing the model’s factual correctness. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.7">OpenBookQA (OBQA)</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mihaylov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib49" title="">2018</a>)</cite> evaluates the understanding of a broad range of facts and concepts. Finally, <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.8">LogiQA-2</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib44" title="">2023b</a>)</cite> assesses the model’s capacity to comprehend and apply logical principles.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Specialized Knowledge</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">In the Specialized Knowledge category, we include datasets that demand expertise in specific domains. The <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px2.p1.1.1">ARC Challenge (ARC-C)</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib15" title="">2018</a>)</cite> contains challenging science exam questions from grades 3 to 9, demanding advanced knowledge. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px2.p1.1.2">SciQ</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Johannes&nbsp;Welbl, <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib35" title="">2017</a>)</cite> provides science exam questions to test models’ understanding and reasoning in the scientific domain. <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px2.p1.1.3">PubMedQA</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib34" title="">2019</a>)</cite> focuses on biomedical literature, assessing comprehension in medical and health-related information.
<span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px2.p1.1.4">MathQA</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Amini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib4" title="">2019</a>)</cite> tests mathematical problem-solving, requiring both numerical comprehension and reasoning. Lastly, <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px2.p1.1.5">MMLU</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib30" title="">2021</a>)</cite> spans multiple domains, from professional to academic, testing the model on specialized subjects.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.3" style="width:307.0pt;height:129.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.4pt,16.2pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T1.3.1.1.1.1">Dataset (Real Tok.)</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.1.2">ARC-E</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.1.3">BoolQ</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.1.4">Wino.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.1.5">PIQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.1.6">HellaSwag</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.1.7">TruthfulQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.1.8">OBQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.1.9">LogiQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.3.1.1.1.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T1.3.1.1.1.10.1" style="background-color:#E0FFFF;">Avg</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.3.1.2.2.1">Half C4 (85B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.2.2.2">61.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.2.2.3">59.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.2.2.4">57.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.2.2.5">74.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.2.2.6">46.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.2.2.7">34.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.2.2.8">22.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.2.2.9">23.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.2.2.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T1.3.1.2.2.10.1" style="background-color:#E0FFFF;">47.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.3.1.3.3.1">Full C4 (170B)</th>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.3.3.2">61.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.3.3.3">54.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.3.3.4">59.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.3.3.5">74.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.3.3.6">46.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.3.3.7">33.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.3.3.8">25.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.3.3.9">23.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.3.3.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T1.3.1.3.3.10.1" style="background-color:#E0FFFF;">47.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.3.1.4.4.1">RW (160B)</th>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.4.4.2">61.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.4.4.3">60.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.4.4.4">57.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.4.4.5">74.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.4.4.6">45.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.4.4.7">36.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.4.4.8">21.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.4.4.9">23.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.4.4.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T1.3.1.4.4.10.1" style="background-color:#E0FFFF;">47.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.3.1.5.5.1">RW (320B)</th>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.5.5.2">60.7</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.5.5.3">61.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.5.5.4">57.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.5.5.5">74.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.5.5.6">45.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.5.5.7">36.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.5.5.8">22.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.5.5.9">22.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.5.5.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T1.3.1.5.5.10.1" style="background-color:#E0FFFF;">47.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.3.1.6.6.1">Pythia-Pile (300B)</th>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.6.6.2">60.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.6.6.3">63.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.6.6.4">57.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.6.6.5">70.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.6.6.6">40.4</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.6.6.7">38.9</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.6.6.8">22.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.6.6.9">22.2</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.6.6.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T1.3.1.6.6.10.1" style="background-color:#E0FFFF;">47.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.3.1.7.7.1">TinyLlama (1T)</th>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.7.7.2">60.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.7.7.3">57.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.7.7.4">59.1</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.7.7.5">73.3</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.7.7.6">45.0</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.7.7.7">37.6</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.7.7.8">21.8</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.7.7.9">24.5</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.1.7.7.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T1.3.1.7.7.10.1" style="background-color:#E0FFFF;">47.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.3.1.8.8.1">Synthetic (85B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.8.8.2">63.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.8.8.3">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.8.8.4">58.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.8.8.5">76.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.8.8.6">45.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.8.8.7">44.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.8.8.8">23.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.8.8.9">24.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.1.8.8.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T1.3.1.8.8.10.1" style="background-color:#E0FFFF;">49.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.3.1.9.9.1">Synthetic+C4 (85B)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.1.9.9.2">64.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.1.9.9.3">62.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.1.9.9.4">58.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.1.9.9.5">75.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.1.9.9.6">46.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.1.9.9.7">40.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.1.9.9.8">24.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.1.9.9.9">23.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.1.9.9.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T1.3.1.9.9.10.1" style="background-color:#E0FFFF;">49.4</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluation of <math alttext="\sim 1.3" class="ltx_Math" display="inline" id="S5.T1.2.m1.1"><semantics id="S5.T1.2.m1.1b"><mrow id="S5.T1.2.m1.1.1" xref="S5.T1.2.m1.1.1.cmml"><mi id="S5.T1.2.m1.1.1.2" xref="S5.T1.2.m1.1.1.2.cmml"></mi><mo id="S5.T1.2.m1.1.1.1" xref="S5.T1.2.m1.1.1.1.cmml">∼</mo><mn id="S5.T1.2.m1.1.1.3" xref="S5.T1.2.m1.1.1.3.cmml">1.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.m1.1c"><apply id="S5.T1.2.m1.1.1.cmml" xref="S5.T1.2.m1.1.1"><csymbol cd="latexml" id="S5.T1.2.m1.1.1.1.cmml" xref="S5.T1.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.T1.2.m1.1.1.2.cmml" xref="S5.T1.2.m1.1.1.2">absent</csymbol><cn id="S5.T1.2.m1.1.1.3.cmml" type="float" xref="S5.T1.2.m1.1.1.3">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.m1.1d">\sim 1.3</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.m1.1e">∼ 1.3</annotation></semantics></math>B parameter LLMs on ‘General Understanding Tasks’ on datasets focusing on general reasoning, language understanding, and common sense. Results for <span class="ltx_text ltx_font_bold" id="S5.T1.5.1">WRAP</span>are averaged over 3 runs</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.3" style="width:229.3pt;height:129.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.7pt,16.2pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.3.1.1.1.1">Dataset (Real Tok.)</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.1.1.2">ARC-C</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.1.1.3">SciQ</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.1.1.4">PubMedQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.1.1.5">MathQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.1.1.6">MMLU</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.1.1.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T2.3.1.1.1.7.1" style="background-color:#E0FFFF;">Avg</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.3.1.2.2.1">Half C4 (85B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.2.2.2">26.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.2.2.3">84.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.2.2.4">57.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.2.2.5">23.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.2.2.6">24.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.2.2.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T2.3.1.2.2.7.1" style="background-color:#E0FFFF;">43.1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.1.3.3.1">Full C4 (170B)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.3.3.2">26.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.3.3.3">85.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.3.3.4">57.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.3.3.5">24.3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.3.3.6">23.9</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.3.3.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T2.3.1.3.3.7.1" style="background-color:#E0FFFF;">43.5</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.1.4.4.1">RW (160B)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.4.4.2">27.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.4.4.3">87.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.4.4.4">56.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.4.4.5">24.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.4.4.6">25.9</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.4.4.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T2.3.1.4.4.7.1" style="background-color:#E0FFFF;">44.1</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.1.5.5.1">RW (320B)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.5.5.2">27.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.5.5.3">88.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.5.5.4">57.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.5.5.5">23.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.5.5.6">25.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.5.5.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T2.3.1.5.5.7.1" style="background-color:#E0FFFF;">44.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.1.6.6.1">Pythia-Pile (300B)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.6.6.2">26.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.6.6.3">86.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.6.6.4">60.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.6.6.5">25.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.6.6.6">24.3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.6.6.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T2.3.1.6.6.7.1" style="background-color:#E0FFFF;">44.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.3.1.7.7.1">TinyLlama (1T)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.7.7.2">27.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.7.7.3">88.9</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.7.7.4">61.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.7.7.5">24.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.7.7.6">25.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.7.7.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T2.3.1.7.7.7.1" style="background-color:#E0FFFF;">45.6</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.3.1.8.8.1">Synthetic (85B)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.8.8.2">29.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.8.8.3">87.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.8.8.4">60.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.8.8.5">23.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.8.8.6">24.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.8.8.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T2.3.1.8.8.7.1" style="background-color:#E0FFFF;">45.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.3.1.9.9.1">Synthetic+C4 (85B)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.9.9.2">29.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.9.9.3">87.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.9.9.4">61.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.9.9.5">23.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.9.9.6">24.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.9.9.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S5.T2.3.1.9.9.7.1" style="background-color:#E0FFFF;">45.5</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation of <math alttext="\sim 1.3" class="ltx_Math" display="inline" id="S5.T2.2.m1.1"><semantics id="S5.T2.2.m1.1b"><mrow id="S5.T2.2.m1.1.1" xref="S5.T2.2.m1.1.1.cmml"><mi id="S5.T2.2.m1.1.1.2" xref="S5.T2.2.m1.1.1.2.cmml"></mi><mo id="S5.T2.2.m1.1.1.1" xref="S5.T2.2.m1.1.1.1.cmml">∼</mo><mn id="S5.T2.2.m1.1.1.3" xref="S5.T2.2.m1.1.1.3.cmml">1.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.2.m1.1c"><apply id="S5.T2.2.m1.1.1.cmml" xref="S5.T2.2.m1.1.1"><csymbol cd="latexml" id="S5.T2.2.m1.1.1.1.cmml" xref="S5.T2.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.T2.2.m1.1.1.2.cmml" xref="S5.T2.2.m1.1.1.2">absent</csymbol><cn id="S5.T2.2.m1.1.1.3.cmml" type="float" xref="S5.T2.2.m1.1.1.3">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.m1.1d">\sim 1.3</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.m1.1e">∼ 1.3</annotation></semantics></math>B parameter LLMs on ‘Specialized Knowledge Tasks’ that require specific domain knowledge such as science, medicine, mathematics, and logic. Results for <span class="ltx_text ltx_font_bold" id="S5.T2.5.1">WRAP</span>are averaged over 3 runs.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We compare the performance of a model trained on a mixture of real and synthetic data with models trained on various splits of real data. In all our experiments, we use the C4&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib57" title="">2020</a>)</cite> dataset for rephrasing and producing splits of synthetic data.
We use the abbreviation ‘Real Tok.’ to denote the number of tokens of web data available for pre-training. In the ‘Synthetic + Real’ experiments, we augment the same number of synthetic rephrases. We choose ‘Real Tokens’ as the metric of comparison because we can potentially rephrase the same document multiple times, implying that the total corpus size is not meaningful, and corpus ‘knowledge’ is the actual currency of interest.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Baselines Methods</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">We pre-train LLMs of
(i) Half of C4, and the (ii) Full C4 corresponding to approximately
85 Billion and 170 Billion real tokens respectively&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib57" title="">2020</a>)</cite>. We also pre-train our own models on (iii) 160 Billion and (iv) 320 Billion tokens of the RefinedWeb Dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Penedo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib54" title="">2023</a>)</cite>. Additionally, we also compare with the (iv) Pythia-1.4B model that was trained on the Pile&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib24" title="">2020</a>)</cite>. This dataset is no longer publicly available, hence we utilize a pre-trained model. Finally, we also compare with the recent (v) TinyLlama model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib73" title="">2024</a>)</cite> that was trained for 3 epochs on 1 Trillion tokens of data from
SlimPajama <cite class="ltx_cite ltx_citemacro_citep">(Shen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib59" title="">2023</a>)</cite> and StarCoder <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib39" title="">2023a</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">General Improvements</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">Across all tasks in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.T1" title="Table 1 ‣ Specialized Knowledge ‣ 5.1 Datasets ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">1</span></a>, we observe that models trained on synthetic data combined with the C4 dataset (Synthetic+C4) exhibit an overall higher average performance of 49.4% as compared to those trained solely on the real C4 dataset with a 85B token split, which scored an average of 47.4%. This shows that the inclusion of synthetic data can enhance the general understanding capabilities of NLP models. Moreover, even the TinyLLama model trained for 10x compute and data, performs comparably to the other models trained on real data. This suggests that the gains from filtering out, or adding more real data are very low. As opposed to this, <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p1.1.1">WRAP</span>&nbsp;shows that pre-training on even small amounts of synthetic data can contribute to large performance gains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Specialized Knowledge Tasks</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p1.1">The key message from the results
in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.T2" title="Table 2 ‣ Specialized Knowledge ‣ 5.1 Datasets ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">2</span></a> is that synthetic data can not impart ‘new knowledge’. It can only help pre-train faster, which was also the premise of our work. In particular, we note several key findings:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Pre-training on larger datasets helps improve performance, by presumably exposing the LLM to more “knowledge”. For instance, the Pythia (300B) model achieves an average score of 44.6%, outperforming the smaller C4 (85B) dataset’s score of 43.5%.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Despite the advantages of a larger dataset, the improvements saturate. For example, RefinedWeb (320B) model outperforms the RefinedWeb (160B) model by only 0.2%. Similarly, the TinyLlama model (1T tokens) performs comparably to the <span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">WRAP</span>&nbsp; model, which only had 85B tokens of raw web data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Specific Improvements</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p1.1">We see maximum improvement in the TruthfulQA dataset, with the Synthetic (85B) model scoring 44.0%, which is significantly higher than any other model’s performance on this dataset. This is potentially because instruction-tuned LLMs already correct potential misconceptions while rephrasing the text. Interestingly, we notice how adding real data to the synthetic model (Synthetic+C4) reduces the performance on TruthfulQA by 4%, down to 40.5%, indicating a potential dilution of the benefits gained from synthetic data when combined with real data. Other datasets such as HellaSwag, and BoolQ, for which C4 trained models do well, continue to show the benefits of incorporating combinations of C4 and synthetic rephrases.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Analysis and Ablations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We further ask the following Research Questions (RQs) to investigate in a finer granularity how to enhance performance optimally.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Data Combination Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">RQ1: How important is it to have real C4 data?</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S6.F3.g1" src="x5.png" width="830">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S6.F3.2.1">Importance of Real Data:</span> Comparing perplexity on the Pile when pre-training on C4 with synthetic data vs. synthetic data only. Models are 1.3B parameters trained for a total of 150B tokens on a real data subset containing 35 Billion tokens of the C4.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.3" style="width:305.4pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.2pt,9.0pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T3.3.1.1.1.1">Dataset (Real Tok.)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.2">ARC-E</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.3">BoolQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.4">Wino.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.5">PIQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.6">HellaSwag</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.7">TruthfulQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.8">OBQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.9">LogiQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T3.3.1.1.1.10.1" style="background-color:#E0FFFF;">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.3.1.2.1.1">Med+C4-35B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.2.1.2">59.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.2.1.3">57.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.2.1.4">55.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.2.1.5">74.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.2.1.6">44.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.2.1.7">36.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.2.1.8">23.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.2.1.9">21.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.2.1.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T3.3.1.2.1.10.1" style="background-color:#E0FFFF;">46.7</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.3.2.1">QA+C4-35B</th>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.3.2.2">62.2</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.3.2.3">63.3</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.3.2.4">55.7</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.3.2.5">74.8</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.3.2.6">44.6</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.3.2.7">41.4</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.3.2.8">22.4</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.3.2.9">23.2</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.3.2.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T3.3.1.3.2.10.1" style="background-color:#E0FFFF;">48.4</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.4.3.1">Med-35B</th>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.4.3.2">56.6</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.4.3.3">59.5</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.4.3.4">53.4</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.4.3.5">74.0</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.4.3.6">41.9</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.4.3.7">36.3</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.4.3.8">22.2</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.4.3.9">22.7</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.1.4.3.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T3.3.1.4.3.10.1" style="background-color:#E0FFFF;">45.8</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T3.3.1.5.4.1">QA-35B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.5.4.2">61.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.5.4.3">62.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.5.4.4">53.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.5.4.5">75.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.5.4.6">43.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.5.4.7">43.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.5.4.8">22.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.5.4.9">23.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.5.4.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T3.3.1.5.4.10.1" style="background-color:#E0FFFF;">48.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="S6.T3.5.1">Importance of Real Data:</span> Evaluation of <math alttext="\sim 1.3" class="ltx_Math" display="inline" id="S6.T3.2.m1.1"><semantics id="S6.T3.2.m1.1b"><mrow id="S6.T3.2.m1.1.1" xref="S6.T3.2.m1.1.1.cmml"><mi id="S6.T3.2.m1.1.1.2" xref="S6.T3.2.m1.1.1.2.cmml"></mi><mo id="S6.T3.2.m1.1.1.1" xref="S6.T3.2.m1.1.1.1.cmml">∼</mo><mn id="S6.T3.2.m1.1.1.3" xref="S6.T3.2.m1.1.1.3.cmml">1.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.2.m1.1c"><apply id="S6.T3.2.m1.1.1.cmml" xref="S6.T3.2.m1.1.1"><csymbol cd="latexml" id="S6.T3.2.m1.1.1.1.cmml" xref="S6.T3.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S6.T3.2.m1.1.1.2.cmml" xref="S6.T3.2.m1.1.1.2">absent</csymbol><cn id="S6.T3.2.m1.1.1.3.cmml" type="float" xref="S6.T3.2.m1.1.1.3">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.m1.1d">\sim 1.3</annotation><annotation encoding="application/x-llamapun" id="S6.T3.2.m1.1e">∼ 1.3</annotation></semantics></math>B parameter LLMs trained for 150B tokens on General Understanding Tasks. Results show that adding real data helps improve model performance when pre-training on ‘Medium’ or ‘Wikipedia-style’ paraphrases.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T4.3" style="width:227.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.5pt,9.0pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T4.3.1.1.1.1">Dataset (Real Tok.)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.3.1.1.1.2">ARC-C</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.3.1.1.1.3">SciQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.3.1.1.1.4">PubMedQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.3.1.1.1.5">MathQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.3.1.1.1.6">MMLU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.3.1.1.1.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T4.3.1.1.1.7.1" style="background-color:#E0FFFF;">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.3.1.2.1.1">Med+C4-35B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.1.2.1.2">27.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.1.2.1.3">82.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.1.2.1.4">46.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.1.2.1.5">23.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.1.2.1.6">25.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.1.2.1.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T4.3.1.2.1.7.1" style="background-color:#E0FFFF;">40.8</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.1.3.2.1">QA+C4-35B</th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.3.2.2">29.0</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.3.2.3">85.1</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.3.2.4">62.2</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.3.2.5">22.5</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.3.2.6">26.1</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.3.2.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T4.3.1.3.2.7.1" style="background-color:#E0FFFF;">45.0</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.1.4.3.1">Med-35B</th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.4.3.2">27.0</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.4.3.3">80.0</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.4.3.4">59.4</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.4.3.5">22.5</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.4.3.6">24.7</td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.1.4.3.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T4.3.1.4.3.7.1" style="background-color:#E0FFFF;">42.7</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T4.3.1.5.4.1">QA-35B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.1.5.4.2">27.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.1.5.4.3">85.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.1.5.4.4">59.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.1.5.4.5">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.1.5.4.6">25.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.1.5.4.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T4.3.1.5.4.7.1" style="background-color:#E0FFFF;">43.8</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_bold" id="S6.T4.5.1">Importance of Real Data:</span> Evaluation of <math alttext="\sim 1.3" class="ltx_Math" display="inline" id="S6.T4.2.m1.1"><semantics id="S6.T4.2.m1.1b"><mrow id="S6.T4.2.m1.1.1" xref="S6.T4.2.m1.1.1.cmml"><mi id="S6.T4.2.m1.1.1.2" xref="S6.T4.2.m1.1.1.2.cmml"></mi><mo id="S6.T4.2.m1.1.1.1" xref="S6.T4.2.m1.1.1.1.cmml">∼</mo><mn id="S6.T4.2.m1.1.1.3" xref="S6.T4.2.m1.1.1.3.cmml">1.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T4.2.m1.1c"><apply id="S6.T4.2.m1.1.1.cmml" xref="S6.T4.2.m1.1.1"><csymbol cd="latexml" id="S6.T4.2.m1.1.1.1.cmml" xref="S6.T4.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S6.T4.2.m1.1.1.2.cmml" xref="S6.T4.2.m1.1.1.2">absent</csymbol><cn id="S6.T4.2.m1.1.1.3.cmml" type="float" xref="S6.T4.2.m1.1.1.3">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.m1.1d">\sim 1.3</annotation><annotation encoding="application/x-llamapun" id="S6.T4.2.m1.1e">∼ 1.3</annotation></semantics></math>B parameter LLMs on Specialized Knowledge Tasks.
Results show that adding real data helps improve model performance when pre-training on ‘Q/A-style’ paraphrases. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T5.3" style="width:227.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.5pt,9.0pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T5.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T5.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T5.3.1.1.1.1">Dataset (Real Tok.)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.3.1.1.1.2">ARC-C</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.3.1.1.1.3">SciQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.3.1.1.1.4">PubMedQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.3.1.1.1.5">MathQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.3.1.1.1.6">MMLU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.3.1.1.1.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T5.3.1.1.1.7.1" style="background-color:#E0FFFF;">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T5.3.1.2.1.1">Med+C4-35B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.1.2.1.2">27.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.1.2.1.3">82.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.1.2.1.4">46.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.1.2.1.5">23.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.1.2.1.6">25.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.1.2.1.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T5.3.1.2.1.7.1" style="background-color:#E0FFFF;">40.8</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.3.1.3.2.1">QA+C4-35B</th>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.3.2.2">29.0</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.3.2.3">85.1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.3.2.4">62.2</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.3.2.5">22.5</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.3.2.6">26.1</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.3.2.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T5.3.1.3.2.7.1" style="background-color:#E0FFFF;">45.0</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.3.1.4.3.1">Combined-1:1-35B</th>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.4.3.2">28.2</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.4.3.3">85.9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.4.3.4">61.2</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.4.3.5">23.2</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.4.3.6">23.9</td>
<td class="ltx_td ltx_align_center" id="S6.T5.3.1.4.3.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T5.3.1.4.3.7.1" style="background-color:#E0FFFF;">44.5</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T5.3.1.5.4.1">Combined-1:2-35B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.3.1.5.4.2">29.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.3.1.5.4.3">85.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.3.1.5.4.4">57.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.3.1.5.4.5">23.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.3.1.5.4.6">23.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.3.1.5.4.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T5.3.1.5.4.7.1" style="background-color:#E0FFFF;">43.7</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span><span class="ltx_text ltx_font_bold" id="S6.T5.5.1">Combining multiple styles:</span> Evaluation of <math alttext="\sim 1.3" class="ltx_Math" display="inline" id="S6.T5.2.m1.1"><semantics id="S6.T5.2.m1.1b"><mrow id="S6.T5.2.m1.1.1" xref="S6.T5.2.m1.1.1.cmml"><mi id="S6.T5.2.m1.1.1.2" xref="S6.T5.2.m1.1.1.2.cmml"></mi><mo id="S6.T5.2.m1.1.1.1" xref="S6.T5.2.m1.1.1.1.cmml">∼</mo><mn id="S6.T5.2.m1.1.1.3" xref="S6.T5.2.m1.1.1.3.cmml">1.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.2.m1.1c"><apply id="S6.T5.2.m1.1.1.cmml" xref="S6.T5.2.m1.1.1"><csymbol cd="latexml" id="S6.T5.2.m1.1.1.1.cmml" xref="S6.T5.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S6.T5.2.m1.1.1.2.cmml" xref="S6.T5.2.m1.1.1.2">absent</csymbol><cn id="S6.T5.2.m1.1.1.3.cmml" type="float" xref="S6.T5.2.m1.1.1.3">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.m1.1d">\sim 1.3</annotation><annotation encoding="application/x-llamapun" id="S6.T5.2.m1.1e">∼ 1.3</annotation></semantics></math>B parameter LLMs trained for 150B tokens on ‘Specialized Knowledge Tasks’. Results suggest that combining rephrasing styles does not yield performance benefit on zero-shot tasks compared to just Q/A style.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T6.3" style="width:305.4pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.2pt,9.0pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T6.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T6.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T6.3.1.1.1.1">Dataset (Real Tok.)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.3.1.1.1.2">ARC-E</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.3.1.1.1.3">BoolQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.3.1.1.1.4">Wino.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.3.1.1.1.5">PIQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.3.1.1.1.6">HellaSwag</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.3.1.1.1.7">TruthfulQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.3.1.1.1.8">OBQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.3.1.1.1.9">LogiQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.3.1.1.1.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T6.3.1.1.1.10.1" style="background-color:#E0FFFF;">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T6.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T6.3.1.2.1.1">Med+C4-35B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.1.2.1.2">59.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.1.2.1.3">57.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.1.2.1.4">55.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.1.2.1.5">74.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.1.2.1.6">44.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.1.2.1.7">36.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.1.2.1.8">23.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.1.2.1.9">21.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.1.2.1.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T6.3.1.2.1.10.1" style="background-color:#E0FFFF;">46.7</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.3.1.3.2.1">QA+C4-35B</th>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.3.2.2">62.2</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.3.2.3">63.3</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.3.2.4">55.7</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.3.2.5">74.8</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.3.2.6">44.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.3.2.7">41.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.3.2.8">22.4</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.3.2.9">23.2</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.3.2.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T6.3.1.3.2.10.1" style="background-color:#E0FFFF;">48.4</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.3.1.4.3.1">Combined-1:1-35B</th>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.4.3.2">60.6</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.4.3.3">60.2</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.4.3.4">57.7</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.4.3.5">73.8</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.4.3.6">43.7</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.4.3.7">40.2</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.4.3.8">22.0</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.4.3.9">22.1</td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.1.4.3.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T6.3.1.4.3.10.1" style="background-color:#E0FFFF;">47.5</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T6.3.1.5.4.1">Combined-1:2-35B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.1.5.4.2">61.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.1.5.4.3">62.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.1.5.4.4">57.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.1.5.4.5">74.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.1.5.4.6">44.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.1.5.4.7">39.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.1.5.4.8">23.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.1.5.4.9">21.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.1.5.4.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="S6.T6.3.1.5.4.10.1" style="background-color:#E0FFFF;">48.0</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span><span class="ltx_text ltx_font_bold" id="S6.T6.5.1">Combining multiple styles:</span> Evaluation of <math alttext="\sim 1.3" class="ltx_Math" display="inline" id="S6.T6.2.m1.1"><semantics id="S6.T6.2.m1.1b"><mrow id="S6.T6.2.m1.1.1" xref="S6.T6.2.m1.1.1.cmml"><mi id="S6.T6.2.m1.1.1.2" xref="S6.T6.2.m1.1.1.2.cmml"></mi><mo id="S6.T6.2.m1.1.1.1" xref="S6.T6.2.m1.1.1.1.cmml">∼</mo><mn id="S6.T6.2.m1.1.1.3" xref="S6.T6.2.m1.1.1.3.cmml">1.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T6.2.m1.1c"><apply id="S6.T6.2.m1.1.1.cmml" xref="S6.T6.2.m1.1.1"><csymbol cd="latexml" id="S6.T6.2.m1.1.1.1.cmml" xref="S6.T6.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S6.T6.2.m1.1.1.2.cmml" xref="S6.T6.2.m1.1.1.2">absent</csymbol><cn id="S6.T6.2.m1.1.1.3.cmml" type="float" xref="S6.T6.2.m1.1.1.3">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.2.m1.1d">\sim 1.3</annotation><annotation encoding="application/x-llamapun" id="S6.T6.2.m1.1e">∼ 1.3</annotation></semantics></math>B parameter LLMs trained for 150B tokens on General Understanding Tasks.
Results suggest that combining rephrasing styles does not yield performance benefit on zero-shot tasks compared to just Q/A style.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S6.F4.g1" src="x6.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S6.F4.2.1">Combining multiple styles:</span> Perplexity across all domains of the Pile comparing combining multiple styles of synthetic data. Models are 1.3B parameters trained for a total of 150B tokens. We see small perplexity improvements from combining multiple styles.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p1.1">Our findings in Tables&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.T1" title="Table 1 ‣ Specialized Knowledge ‣ 5.1 Datasets ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">1</span></a>–<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.T2" title="Table 2 ‣ Specialized Knowledge ‣ 5.1 Datasets ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">2</span></a>indicate that synthetic data using the QA prompt are sufficient for strong performance on QA tasks. However, when evaluated on Pile perplexity, we observe significant degradation in perplexity across many sub-domains in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.F3" title="Figure 3 ‣ RQ1: How important is it to have real C4 data? ‣ 6.1 Data Combination Analysis ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">3</span></a>. This is likely because synthetic data is very clean containing few special characters and being highly structured. In contrast several sub-domains of the Pile such as OWT, and Hackernews have such special tokens. On domains such as Philpapers and Gutenberg, we observe that dropping real C4 text from the pre-training data, and training on synthetic documents alone drops performance significantly (increase in perplexity). This is once again attributed to the fact that synthetic data does not contain certain ‘tags’ and ‘styles’ that are prevalent in real data scrapes, and emphasized how <span class="ltx_text ltx_font_bold" id="S6.SS1.SSS0.Px1.p1.1.1">WRAP</span>&nbsp;is a better strategy than pre-training on synthetic data alone.
In terms of performance on zero-shot tasks, we once again note that the presence of real data helps improve zero-shot performance in Tables&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.T3" title="Table 3 ‣ RQ1: How important is it to have real C4 data? ‣ 6.1 Data Combination Analysis ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">3</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.T4" title="Table 4 ‣ RQ1: How important is it to have real C4 data? ‣ 6.1 Data Combination Analysis ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">4</span></a>.
Since zero-shot tasks contain well-written Q/A pairs, this effect is not as evident as that for perplexities on real data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">RQ2: Does a combination of multiple synthetic datasets improve performance?</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px2.p1.1">We measure the impact of combining multiple synthetic styles with C4 for training. We consider two variants: combining in a 1:1 ratio meaning that there are two copies of C4 to match two synthetic styles (medium and QA), and 1:2 ratio which combines only one instance of the C4 dataset. For zero-shot QA tasks, our finding in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.T5" title="Table 5 ‣ RQ1: How important is it to have real C4 data? ‣ 6.1 Data Combination Analysis ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">5</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.T6" title="Table 6 ‣ RQ1: How important is it to have real C4 data? ‣ 6.1 Data Combination Analysis ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">6</span></a>
indicate lower performance than combining only QA and C4 data. Evaluations over the Pile are shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.F4" title="Figure 4 ‣ RQ1: How important is it to have real C4 data? ‣ 6.1 Data Combination Analysis ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">4</span></a>. We notice that both the ‘Q/A’ and ‘Wikipedia’ paraphrases help improve performance on certain domains. For example, ‘Stackexchange’, that has lots of question-answers benefits from the presence of synthetic data in Q/A style. Overall, we note that there is a small improvement on the average perplexity on the Pile by combining multiple styles.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Method Ablations</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">RQ3: How important is to have a high-quality re-phraser?</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S6.F5.g1" src="x7.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S6.F5.3.1">Importance of High Quality Paraphraser:</span> Perplexity across all the Pile domains for <span class="ltx_text ltx_font_bold" id="S6.F5.4.2">WRAP</span>&nbsp;on data generated by different LLMs.
Results show that even small models like Qwen-1.8B can generate paraphrases of high quality. Though, a low quality rephraser like our fine-tuned T5-base model leads to significantly worse language modeling.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S6.F6.g1" src="x8.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S6.F6.3.1">Is re-phrasing same as any augmentation?</span> We compare perplexity on the Pile for different augmemntation strategies. 350M parameter models are trained for a total of 15B tokens. <span class="ltx_text ltx_font_bold" id="S6.F6.4.2">WRAP</span>&nbsp;(Medium + C4) performs significantly better than traditional augmentations.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px1.p1.1">To answer this,
we use data from four distinct re-phrasing models (T5-base&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib57" title="">2020</a>)</cite>, Qwen-1.8B-chat&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib6" title="">2023a</a>)</cite>, Mistral-7B-chat&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib33" title="">2023</a>)</cite>, and Vicuna-13B-chat-v1.3&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib12" title="">2023</a>)</cite>) and train a 345M model for 30B tokens. We generate data from all models using the same prompt. In case of the T5-base model, we finetune the model for 1 epoch on re-phrase pairs from the Vicuna-13b-chat model.
We find that pre-training on data generated by smaller re-phrase models like Qwen-1.8B and Mistral-7B achieve lower perplexity than
Vicuna&nbsp;13B
(Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.F5" title="Figure 5 ‣ RQ3: How important is to have a high-quality re-phraser? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">5</span></a>).
At the same time, our fine-tuned T5-base model performs significantly worse than the rest.
Even then, all rephrase models reduce perplexity over only real C4 data.
It remains an open question to test the limits of how small can we train a paraphrase model that can generate high quality synthetic data to further scale the applicability of <span class="ltx_text ltx_font_bold" id="S6.SS2.SSS0.Px1.p1.1.1">WRAP</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">RQ4: Does synthetic data improve over augmentations?</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px2.p1.1">Are the gains observed by pre-training on synthetic data the same as pre-training with augmentations? In order to test this,
we consider two popular text augmentation baselines—synonym replacement and random deletion using the NL-Augmenter library&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dhole et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib18" title="">2021</a>)</cite>.
We pre-train a 350M parameter model for 15B tokens in order to conduct this set of experiments. The total pool size is only about 1.5B tokens, meaning that the model would have to repeat data around 10 times during the pre-training phase, unless augmented over.
As seen in the perplexity analysis in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.F6" title="Figure 6 ‣ RQ3: How important is to have a high-quality re-phraser? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">6</span></a>, the models trained on augmented data perform significantly worse than those trained on combinations of real and synthetic data. This suggests that synthetic data enhances the learning process, and is not merely another form of augmentation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">RQ5: How does the style of synthetic data impact performance on specialized domains?</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S6.F7.g1" src="x9.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="S6.F7.2.1">Impact of style of synthetic rephrases:</span> Perplexity across all domains of the Pile comparing different styles of synthetic data. We train 128M parameter models for 3B tokens.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px3.p1.1">We compare the performance of various models trained on different styles of synthetic data. In particular, we generate four styles of synthetic data (easy, medium, hard, and Q/A) and evaluate the performance of training on combinations of each style across Pile subsets. The prompts to generate these synthetic data styles are outlined in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A7" title="Appendix G Rephrase Prompt Templates ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">G</span></a>. Results corresponding to generations from a Vicuna-v1.3 model, and for a 128M model trained for 3B tokens are summarized in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.F7" title="Figure 7 ‣ RQ5: How does the style of synthetic data impact performance on specialized domains? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">7</span></a>.
We see that training with combinations of real C4 and synthetic data matching the style of the domain at evaluation improves performance. However, we find that no single synthetic data style performs the best across all domains, resulting in similar performance across training with combinations of real C4 data and each synthetic style variant. While knowing the best synthetic style to pre-train an LLM is impractical, an oracle that selects the best synthetic style across all domains will improve perplexity by 16%—indicating the importance of training with diverse data styles for LLM generalization, even when the underlying knowledge stays the same.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">RQ6: Is there data leakage from the rephrase model to the trained model?</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px4.p1.1">We investigate whether our synthetic data maintains similar semantic meaning while being stylistically different from the original C4 data and matching the style of different PILE domains. We start by comparing pairs of examples of synthetic and real data to confirm the performance gain is not attributed to knowledge leakage from the rephrase models. We take a subset of the first 1000 samples from each of the datasets.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS0.Px4.p2">
<p class="ltx_p" id="S6.SS2.SSS0.Px4.p2.1">We show the cosine similarity of the sentence embeddings from a pre-trained BERT model trained with SimCSE objective <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib26" title="">2021</a>)</cite> for medium and qa prompts in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.F8" title="Figure 8 ‣ RQ6: Is there data leakage from the rephrase model to the trained model? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">8</span></a>(a) and (b). When computing similarity, we remove outliers. Figures with distributions use a gaussian Kernel Density Estimator (KDE) to construct distributions for statistics from 1000 values. The cosine similarity of real-synthetic pairs is higher than several baselines including two random real samples from C4, a continuation baseline which computes cosine between the first half of a sample and the full sample, and cosine similarity between the first half and second half of the same sample. High similarity indicates that the re-phrases maintain similar meaning to their real counterparts without adding information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S6.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S6.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="553" id="S6.F7.sf1.g1" src="x10.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Cosine similarity Medium synthetic data</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S6.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="553" id="S6.F7.sf2.g1" src="x11.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Cosine similarity QA synthetic data</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Comparison between synthetic and real data from the C4 corpus showing that synthetic data maintains semantic meaning compared with the real C4 data and primarily changes style for (a) medium rephrases of C4, and (b) QA rephrases of C4.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations and Opportunities</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Cost Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1"><em class="ltx_emph ltx_font_italic" id="S7.SS1.p1.1.1">Should you generate synthetic data, or just train longer on real data?</em></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">The applications of <span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.1">WRAP</span>&nbsp;lies in both paradigms—(i) low-resourced data settings such as a language model for Finnish language&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Luukkonen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib46" title="">2023</a>)</cite>, and (ii) data-rich settings such as training on the common crawl.
In the former, there is no alternative option of naively gathering more data, and hence, synthetic data is a natural solution that should outperform training on in-domain data alone. However, there is a significant interest in training language models on English, or more broadly, general web data. Is using synthetic data a viable option even in this paradigm?</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.1">Before, we dive into the feasibility of pre-training on synthetic data, we should acknowledge the results of Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S5.T1" title="Table 1 ‣ Specialized Knowledge ‣ 5.1 Datasets ‣ 5 Zero-shot Tasks ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">1</span></a>. The TinyLlama model trained for 3 Trillion tokens also underperforms a model jointly trained on real and synthetic data. In fact, it performs quite comparably to the models that were trained for 300B tokens on just real data as well. This suggests that the ceiling for improvement by training for longer may not be that high (for a model of size 350M/1.3B parameters; larger models may benefit from training for longer).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.1">To analyze this cost trade-off, we compare the cost of generating synthetic data, versus that of training a language model on extra data.
For our synthetic data generation experiments, we use the vLLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kwon et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib38" title="">2023</a>)</cite> library for fast generation. In particular, we are able to generate 3M tokens per hour on a single A100 when using the Mistral-7B. Generating 85B tokens (as in our work) accounts for about 25K GPU hours.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p5">
<p class="ltx_p" id="S7.SS1.p5.1">In comparison, on 64 A100s, we achieve a throughput of 0.5M tokens per second. Assuming training for 300B tokens, would mean 256 GPU days, accounting for about 6k GPU hours to train a single model. On the contrary, training a 13B model would take about 30K GPU hours. At the scale of training a 13B model, reducing the training cost by 3-10x can incorporate the cost overhead of training with synthetic data in a single run.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p6">
<p class="ltx_p" id="S7.SS1.p6.1">While the cost of generating high quality data is still relatively high, two important sources of improvement impact this cost analysis. First, if we use the Qwen-1.8B model <cite class="ltx_cite ltx_citemacro_cite">Bai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib7" title="">2023b</a>)</cite> for rephrasing, we are able to get a 3x higher token throughput. As seen in our preliminary results in Fig&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.F5" title="Figure 5 ‣ RQ3: How important is to have a high-quality re-phraser? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">5</span></a>, the model pre-trained on rephrases generated by Qwen model performs comparably to that by the Mistral model. This reduces the cost of generation by 3x. More recent work in speculative decoding&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib45" title="">2023c</a>)</cite> and optimized inference&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Xia et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib68" title="">2024</a>)</cite> suggest that we can leverage another 3-5x improvement in the generation cost. Hence, indeed, even at the scale of just 1.3B parameter model training, we can already improve upon the cost of pre-training using just real data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.p7">
<p class="ltx_p" id="S7.SS1.p7.1">Two additional important advantages of synthetic data generation that could not be accounted for in the discussion above:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S7.I1">
<li class="ltx_item" id="S7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S7.I1.i1.p1">
<p class="ltx_p" id="S7.I1.i1.p1.1">The cost of synthetic data generation is a one-time investment, and we may train many models of varying scales once the data is generated.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para ltx_noindent" id="S7.I1.i2.p1">
<p class="ltx_p" id="S7.I1.i2.p1.1">Data generation is 100% parallelizable, whereas training requires the availability of a big cluster with fast inter-node connectivity. This is much more expensive. On the other hand, generation can be thought of as a side process that can fill in the empty GPUs in any large-scale compute cluster, and runs on single GPU machines.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Diversity of Synthetic Generations</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Another limitation is enforcing the diversity in the generated data. This diversity comes from both the “style” and the “knowledge” contained in the generated data. Recent works&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib40" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib41" title="">c</a>)</cite> used a selection of topics, or scenarios to seed the model to generate novel texts. Still, a recent study by&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Padmakumar et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib53" title="">2023</a>)</cite> showed that using language models for AI-assisted writing tends to reduce content diversity, particularly when using instruction-tuned models. While we used the paradigm of rephrasing specifically to mitigate the issues pertaining to the diversity of novel content generation, it remains for future work to assess the presence (or lack of) and impact of content diversity in paraphrase models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Strong language models are being pre-trained on combinations of real and synthetic data.
Using synthetic data enables baking in desirable attributes such as fairness, bias, and style (like instruction following) directly into the data, eliminating the need to adjust the training algorithm specifically. This offers an alternative approach to aligning language models to human values.
The recent uptick in interest around synthetic data, especially for instruction-tuning language models, is noteworthy, with concurrent researchers also leveraging it for pre-training.
As we transition into this paradigm, understanding the properties of the data fed to our models is paramount.
This paper aims to be a comprehensive guide on employing different synthetic style data in LLM pre-training. We delve into its significance from two vantage points: (1) In scenarios with scarce high-quality data, synthetic rephrases offer more value than mere repetition of existing data; (2) Synthetic data can be a boon for generalization on different text domains, and for generating text in styles that are underrepresented in the pre-training dataset.
As practitioners generate synthetic data for training models, they will be faced with important and expensive design choices—(i) How important is the quality of the synthetic data generator?; (ii) How to balance real and synthetic data? (iii) When does training on synthetic data reach a point of diminishing returns in terms of epochs? This work takes a first step towards answering these questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Conversely, it’s essential to note the inherent limitations, and opportunities with synthetic data. We highlight two limitations: (1) cost of generation is still large and requires strong LMs, and (2) enforcing the diversity in the generated data is challenging. In this work, we leverage the natural diversity of the web to generate synthetic “re-phrases”. This limits the model from learning new “knowledge” and enhances the learning process only via the provision of high-quality inputs. Whereas past work required a more intricate understanding of the blind spots of the model, potentially biasing the knowledge contained in the pre-training data distribution. Nonetheless, we demonstrate the potential of synthetic data to improve LLM training efficiency both in compute and data size.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ai2 (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbas et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari&nbsp;S. Morcos.

</span>
<span class="ltx_bibblock">Semdedup: Data-efficient learning at web-scale through semantic deduplication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">ArXiv</em>, abs/2303.09540, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257557221" title="">https://api.semanticscholar.org/CorpusID:257557221</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alemohammad et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed&nbsp;Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard&nbsp;G Baraniuk.

</span>
<span class="ltx_bibblock">Self-consuming generative models go mad.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2307.01850</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amini et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">MathQA: Towards interpretable math word problem solving with operation-based formalisms.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 2357–2367, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/N19-1245" title="">10.18653/v1/N19-1245</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1245" title="">https://aclanthology.org/N19-1245</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azizi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David&nbsp;J Fleet.

</span>
<span class="ltx_bibblock">Synthetic data from diffusion models improves imagenet classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2304.08466</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An&nbsp;Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2309.16609</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2309.16609</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal &amp; Grover (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hritik Bansal and Aditya Grover.

</span>
<span class="ltx_bibblock">Leaving reality to imagination: Robust classification via generated datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2302.02503</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Ronan&nbsp;Le Bras, Jianfeng Gao, and Yejin Choi.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Thirty-Fourth AAAI Conference on Artificial Intelligence</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In H.&nbsp;Larochelle, M.&nbsp;Ranzato, R.&nbsp;Hadsell, M.F. Balcan, and H.&nbsp;Lin (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</em>, volume&nbsp;33, pp.&nbsp; 1877–1901. Curran Associates, Inc., 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Alpagasus: Training a better alpaca with fewer data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2307.08701</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E. Gonzalez, Ion Stoica, and Eric&nbsp;P. Xing.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">https://lmsys.org/blog/2023-03-30-vicuna/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dami Choi, Alexandre Passos, Christopher&nbsp;J Shallue, and George&nbsp;E Dahl.

</span>
<span class="ltx_bibblock">Faster neural network training with data echoing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:1907.05550</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BoolQ: Exploring the surprising difficulty of natural yes/no questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pp.&nbsp; 2924–2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/N19-1300" title="">10.18653/v1/N19-1300</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1300" title="">https://aclanthology.org/N19-1300</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv:1803.05457v1</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Computer (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Together Computer.

</span>
<span class="ltx_bibblock">Redpajama: an open dataset for training large language models.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/togethercomputer/RedPajama-Data" title="">https://github.com/togethercomputer/RedPajama-Data</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cubuk et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ekin&nbsp;D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Randaugment: Practical automated data augmentation with a reduced search space.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</em>, pp.&nbsp; 702–703, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhole et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kaustubh&nbsp;D. Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood, Abinaya Mahendiran, Simon Mille, Ashish Srivastava, Samson Tan, Tongshuang Wu, Jascha Sohl-Dickstein, Jinho&nbsp;D. Choi, Eduard Hovy, Ondrej Dusek, Sebastian Ruder, Sajant Anand, Nagender Aneja, Rabin Banjade, Lisa Barthe, Hanna Behnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio&nbsp;Sobrevilla Cabezudo, Samuel Cahyawijaya, Emile Chapuis, Wanxiang Che, Mukund Choudhary, Christian Clauss, Pierre Colombo, Filip Cornell, Gautier Dagan, Mayukh Das, Tanay Dixit, Thomas Dopierre, Paul-Alexis Dray, Suchitra Dubey, Tatiana Ekeinhor, Marco&nbsp;Di Giovanni, Rishabh Gupta, Rishabh Gupta, Louanes Hamla, Sang Han, Fabrice Harel-Canada, Antoine Honore, Ishan Jindal, Przemyslaw&nbsp;K. Joniak, Denis Kleyko, Venelin Kovatchev, Kalpesh Krishna, Ashutosh Kumar, Stefan Langer, Seungjae&nbsp;Ryan Lee, Corey&nbsp;James Levinson, Hualou Liang, Kaizhao Liang, Zhexiong Liu, Andrey Lukyanenko, Vukosi Marivate, Gerard de&nbsp;Melo, Simon Meoni, Maxime
Meyer, Afnan Mir, Nafise&nbsp;Sadat Moosavi, Niklas Muennighoff, Timothy Sum&nbsp;Hon Mun, Kenton Murray, Marcin Namysl, Maria Obedkova, Priti Oli, Nivranshu Pasricha, Jan Pfister, Richard Plant, Vinay Prabhu, Vasile Pais, Libo Qin, Shahab Raji, Pawan&nbsp;Kumar Rajpoot, Vikas Raunak, Roy Rinberg, Nicolas Roberts, Juan&nbsp;Diego Rodriguez, Claude Roux, Vasconcellos P.&nbsp;H. S., Ananya&nbsp;B. Sai, Robin&nbsp;M. Schmidt, Thomas Scialom, Tshephisho Sefara, Saqib&nbsp;N. Shamsi, Xudong Shen, Haoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel, Damien Sileo, Jamie Simon, Chandan Singh, Roman Sitelew, Priyank Soni, Taylor Sorensen, William Soto, Aman Srivastava, KV&nbsp;Aditya Srivatsa, Tony Sun, Mukund&nbsp;Varma T, A&nbsp;Tabassum, Fiona&nbsp;Anting Tan, Ryan Teehan, Mo&nbsp;Tiwari, Marie Tolkiehn, Athena Wang, Zijian Wang, Gloria Wang, Zijie&nbsp;J. Wang, Fuxuan Wei, Bryan Wilie, Genta&nbsp;Indra Winata, Xinyi Wu, Witold Wydmański, Tianbao Xie, Usama Yaseen, M.&nbsp;Yee, Jing Zhang, and Yue Zhang.

</span>
<span class="ltx_bibblock">Nl-augmenter: A framework for task-sensitive natural language augmentation, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eisenstein (2013)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Eisenstein.

</span>
<span class="ltx_bibblock">What to do about bad language on the internet.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pp.&nbsp; 359–369, Atlanta, Georgia, June 2013. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N13-1037" title="">https://aclanthology.org/N13-1037</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eldan &amp; Li (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ronen Eldan and Yuanzhi Li.

</span>
<span class="ltx_bibblock">Tinystories: How small can language models be and still speak coherent english?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2305.07759</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fort et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, and Samuel&nbsp;L Smith.

</span>
<span class="ltx_bibblock">Drawing multiple augmentation samples per image during training efficiently decreases test error.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2105.13343</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Futrell et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Richard Futrell, Kyle Mahowald, and Edward Gibson.

</span>
<span class="ltx_bibblock">Large-scale evidence of dependency length minimization in 37 languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the National Academy of Sciences</em>, 112(33):10336–10341, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gadre et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Samir&nbsp;Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Datacomp: In search of the next generation of multimodal datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2304.14108</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et&nbsp;al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2101.00027</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation, 12 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/10256836" title="">https://zenodo.org/records/10256836</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianyu Gao, Xingcheng Yao, and Danqi Chen.

</span>
<span class="ltx_bibblock">Simcse: Simple contrastive learning of sentence embeddings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021</em>, pp.&nbsp; 6894–6910. Association for Computational Linguistics (ACL), 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gibson et&nbsp;al. (2000)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edward Gibson et&nbsp;al.

</span>
<span class="ltx_bibblock">The dependency locality theory: A distance-based theory of linguistic complexity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Image, language, brain</em>, 2000:95–126, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi&nbsp;Zhang, Jyoti Aneja, Caio César&nbsp;Teodoro Mendes, Allie Del&nbsp;Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de&nbsp;Rosa, Olli Saarikivi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Textbooks are all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2306.11644</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke Zettlemoyer, and Noah&nbsp;A. Smith.

</span>
<span class="ltx_bibblock">Whose language counts as high quality? measuring language ideologies in text data selection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pp.&nbsp; 2562–2580, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.emnlp-main.165" title="">10.18653/v1/2022.emnlp-main.165</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.165" title="">https://aclanthology.org/2022.emnlp-main.165</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the International Conference on Learning Representations (ICLR)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffer et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry.

</span>
<span class="ltx_bibblock">Augment your batch: Improving generalization through instance repetition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.&nbsp; 8129–8138, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.

</span>
<span class="ltx_bibblock">Pubmedqa: A dataset for biomedical research question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pp.&nbsp; 2567–2577, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johannes&nbsp;Welbl (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matt&nbsp;Gardner Johannes&nbsp;Welbl, Nelson F.&nbsp;Liu.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock">2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jung et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi.

</span>
<span class="ltx_bibblock">Impossible distillation: from low-quality model to high-quality dataset &amp; model for summarization and paraphrasing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2305.16635</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köksal et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich Schütze.

</span>
<span class="ltx_bibblock">Longform: Optimizing instruction tuning for long text generation with corpus extraction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2304.08460</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Raymond Li, Loubna&nbsp;Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry&nbsp;Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh&nbsp;Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva&nbsp;Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn&nbsp;Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos&nbsp;Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von
Werra, and Harm de&nbsp;Vries.

</span>
<span class="ltx_bibblock">Starcoder: may the source be with you!

</span>
<span class="ltx_bibblock">2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis.

</span>
<span class="ltx_bibblock">Self-alignment with instruction backtranslation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2308.06259</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del&nbsp;Giorno, Suriya Gunasekar, and Yin&nbsp;Tat Lee.

</span>
<span class="ltx_bibblock">Textbooks are all you need ii: phi-1.5 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2309.05463</em>, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, and Yi&nbsp;Zhang.

</span>
<span class="ltx_bibblock">Tinygsm: achieving¿ 80% on gsm8k with small language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2312.09241</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue Zhang.

</span>
<span class="ltx_bibblock">Logiqa 2.0 — an improved dataset for logical reasoning in natural language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, pp.&nbsp; 1–16, 2023b.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1109/TASLP.2023.3293046" title="">10.1109/TASLP.2023.3293046</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang.

</span>
<span class="ltx_bibblock">Online speculative decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2310.07177</em>, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luukkonen et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et&nbsp;al.

</span>
<span class="ltx_bibblock">Fingpt: Large generative models for a small language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2311.05640</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maini (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pratyush Maini.

</span>
<span class="ltx_bibblock">Phi-1.5 model: A case of comparing apples to oranges?

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pratyushmaini.github.io/phi-1_5/" title="">https://pratyushmaini.github.io/phi-1_5/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maini et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pratyush Maini, Sachin Goyal, Zachary&nbsp;C Lipton, J&nbsp;Zico Kolter, and Aditi Raghunathan.

</span>
<span class="ltx_bibblock">T-mars: Improving visual representations by circumventing text feature learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2307.03132</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">EMNLP</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Niklas Muennighoff, Alexander&nbsp;M Rush, Boaz Barak, Teven&nbsp;Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2305.16264</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Advances in Neural Information Processing Systems</em>, 35:27730–27744, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oya (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Masanori Oya.

</span>
<span class="ltx_bibblock">Three types of average dependency distances of sentences in a multilingual parallel corpus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation</em>, pp.&nbsp; 652–661, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Padmakumar et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vishakh Padmakumar, Behnam Hedayatnia, Di&nbsp;Jin, Patrick Lange, Seokhwan Kim, Nanyun Peng, Yang Liu, and Dilek Hakkani-Tur.

</span>
<span class="ltx_bibblock">Investigating the representation of open domain dialogue context for transformer models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue</em>, pp.&nbsp; 538–547, Prague, Czechia, September 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.sigdial-1.50" title="">https://aclanthology.org/2023.sigdial-1.50</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.

</span>
<span class="ltx_bibblock">The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2306.01116</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Jong&nbsp;Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">International conference on machine learning</em>, pp.&nbsp; 8748–8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jack&nbsp;W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et&nbsp;al.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training gopher.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2112.11446</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et&nbsp;al.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Advances in Neural Information Processing Systems</em>, 35:25278–25294, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing.

</span>
<span class="ltx_bibblock">Slimpajama-dc: Understanding data combinations for llm training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2309.10818</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shumailov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson.

</span>
<span class="ltx_bibblock">Model dementia: Generated data makes models forget.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2305.17493</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Solaiman &amp; Dennison (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Irene Solaiman and Christy Dennison.

</span>
<span class="ltx_bibblock">Process for adapting language models to society (palms) with values-targeted datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Advances in Neural Information Processing Systems</em>, 34:5861–5873, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trabucco et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov.

</span>
<span class="ltx_bibblock">Effective data augmentation with diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2302.07944</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan&nbsp;N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villalobos et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho.

</span>
<span class="ltx_bibblock">Will we run out of data? an analysis of the limits of scaling datasets in machine learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2211.04325</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang.

</span>
<span class="ltx_bibblock">Magicoder: Source code is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2312.02120</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzek et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.

</span>
<span class="ltx_bibblock">CCNet: Extracting high quality monolingual datasets from web crawl data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, pp.&nbsp; 4003–4012, Marseille, France, May 2020. European Language Resources Association.

</span>
<span class="ltx_bibblock">ISBN 979-10-95546-34-4.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.lrec-1.494" title="">https://aclanthology.org/2020.lrec-1.494</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui.

</span>
<span class="ltx_bibblock">Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sang&nbsp;Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc&nbsp;V Le, Tengyu Ma, and Adams&nbsp;Wei Yu.

</span>
<span class="ltx_bibblock">Doremi: Optimizing data mixtures speeds up language model pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2305.10429</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.

</span>
<span class="ltx_bibblock">To repeat or not to repeat: Insights from scaling llm under token-crisis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2305.13230</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haichao Yu, Yu&nbsp;Tian, Sateesh Kumar, Linjie Yang, and Heng Wang.

</span>
<span class="ltx_bibblock">The devil is in the details: A deep dive into the rabbit hole of data filtering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2309.15954</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.

</span>
<span class="ltx_bibblock">Tinyllama: An open-source small language model, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2305.11206</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Training Dataset</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">The primary pretraining corpus in our experiments is Colossal Clean Crawled Corpus (C4), a curated English text dataset comprising over 170 billion tokens. This corpus is derived from CommonCrawl, a common practice in the pretraining of LLMs <cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib10" title="">2020</a>); Raffel et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib57" title="">2020</a>); Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib62" title="">2023</a>)</cite>. This data source is also prominently featured in openly available LLM pretraining corpora, including The Pile <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib24" title="">2020</a>)</cite> and RedPajama <cite class="ltx_cite ltx_citemacro_cite">Computer (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib16" title="">2023</a>)</cite>.
There are different versions of CommonCrawl data and our selection of C4 for pretraining is driven by driven by its size and quality.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">We also compare with pre-training on the Refined Web corpus <cite class="ltx_cite ltx_citemacro_cite">Penedo et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib54" title="">2023</a>)</cite>. The dataset is also derived from the CommonCrawl, however has a more stringent filtering process. Our selection of Refined Web is for comparing synthetic rephrases to high quality subsets of web data, which were shown to achieve similar performance compared with curated datasets <cite class="ltx_cite ltx_citemacro_cite">Penedo et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib54" title="">2023</a>)</cite>. For our experiments we used the first <math alttext="3050" class="ltx_Math" display="inline" id="A1.SS1.p2.1.m1.1"><semantics id="A1.SS1.p2.1.m1.1a"><mn id="A1.SS1.p2.1.m1.1.1" xref="A1.SS1.p2.1.m1.1.1.cmml">3050</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.p2.1.m1.1b"><cn id="A1.SS1.p2.1.m1.1.1.cmml" type="integer" xref="A1.SS1.p2.1.m1.1.1">3050</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p2.1.m1.1c">3050</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p2.1.m1.1d">3050</annotation></semantics></math> files and train for 300B tokens to match training on C4. We aso conduct experiments with the first 1650 files to account for multiple epochs on the Refined Web dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Pile Perplexity Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">For the evaluation phase, we employed 20 subsets from the Pile corpus. We excluded the Europarl subset because it contained non-English language. The subsets used are:
CC, StackExchange, Wikipedia, GitHub, PubMed Abstracts, Openwebtext2, Freelaw, Math, NIH, USPTO, Hackernews, Enron, Books3, PubMed Central, Gutenberg, Arxiv, Bookcorpus2, Opensubtitles, Youtubesubtitles, Ubuntu, and Philpapers. We take the first 10000 samples from each subset and split into documents of maximum length 1024. The reported average in all perplexity plots is a weighted average over the perplexity of all domains according to the ratios in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A1.T7" title="Table 7 ‣ A.2.1 Pile Weighted Average Ratios ‣ A.2 Pile Perplexity Evaluation ‣ Appendix A Dataset Details ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">7</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="A1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Pile Weighted Average Ratios</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.SSS1.p1">
<p class="ltx_p" id="A1.SS2.SSS1.p1.1">We report the ratios for samples according to the first 10,000 documents from our Pile validation set in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A1.T7" title="Table 7 ‣ A.2.1 Pile Weighted Average Ratios ‣ A.2 Pile Perplexity Evaluation ‣ Appendix A Dataset Details ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">7</span></a>. Note that there are some slight variations in the ratios compared with those reported in <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib24" title="">2020</a>)</cite>, but most ratios are similar.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T7.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T7.1.1.1.1">Dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T7.1.1.1.2">Validation Ratio (%)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T7.1.1.1.3">Published Ratio (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T7.1.2.1.1">ArXiv</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.1.2">10.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.2.1.3">9.0</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.3.2.1">BookCorpus2</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.3.2.2">0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.3.2.3">0.8</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.4.3.1">Books3</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.3.2">11.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.4.3.3">12.1</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.5.4.1">Pile-CC</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.4.2">14.0</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.5.4.3">18.11</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.6.5.1">Enron</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.5.2">0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.6.5.3">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.7.6.1">EuroParl</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.7.6.2">1.1</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.7.6.3">0.7</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.8.7.1">FreeLaw</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.8.7.2">5.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.8.7.3">6.1</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.9.8.1">Github</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.9.8.2">10.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.9.8.3">7.6</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.10.9.1">Gutenberg</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.10.9.2">1.5</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.10.9.3">2.2</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.11.10.1">Hackernews</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.11.10.2">0.6</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.11.10.3">0.6</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.12.11.1">Dm Mathematics</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.12.11.2">2.0</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.12.11.3">1.2</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.13.12.1">NIH</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.13.12.2">0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.13.12.3">0.3</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.14.13.1">OpenSubtitles</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.14.13.2">1.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.14.13.3">1.6</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.15.14.1">OpenWebText2</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.15.14.2">8.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.15.14.3">10.0</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.16.15.1">PhilPapers</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.16.15.2">0.7</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.16.15.3">0.4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.17.16.1">PubMed Abstracts</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.17.16.2">0.7</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.17.16.3">3.1</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.18.17.1">PubMed Central</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.18.17.2">14.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.18.17.3">14.4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.19.18.1">StackExchange</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.19.18.2">5.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.19.18.3">5.1</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.20.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.20.19.1">Ubuntu</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.20.19.2">1.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.20.19.3">0.9</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.21.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.21.20.1">USPTO</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.21.20.2">2.7</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.21.20.3">3.7</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.22.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T7.1.22.21.1">Wikipedia</th>
<td class="ltx_td ltx_align_center" id="A1.T7.1.22.21.2">3.4</td>
<td class="ltx_td ltx_align_center" id="A1.T7.1.22.21.3">1.5</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.23.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T7.1.23.22.1">YoutubeSubtitles</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.23.22.2">0.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.1.23.22.3">0.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Pile ratios for our evaluation compared with published ratios</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Zero-shot Evaluation Dataset</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">We evaluate our models on a total of 13 different zero-shot benchmarks to assess their abilities across various natural language tasks. These benchmarks are categorized into two subsets: Specialized Knowledge and General Understanding.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Specialized Knowledge</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS3.SSS0.Px1.p1.1">This subset comprises datasets that focus on domain-specific knowledge and expertise.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px1.p2">
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">ARC Challenge (ARC-C)</span>: This dataset is part of the AI2 Reasoning Challenge (ARC)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib15" title="">2018</a>)</cite>, containing science exam questions from grades 3 to 9. The ARC Challenge set includes more difficult questions that necessitate higher-order reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">SciQ</span>: A dataset of science exam questions, specifically designed to evaluate the ability of NLP models in understanding and reasoning within the scientific domain&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Johannes&nbsp;Welbl, <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib35" title="">2017</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i3.p1.1.1">PubMedQA</span>: This dataset focuses on biomedical literature and is designed to evaluate the understanding of medical and healthcare-related information&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib34" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i4.p1.1.1">MathQA</span>: This dataset challenges models in mathematical problem-solving, requiring both numerical understanding and reasoning skills&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Amini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib4" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i5.p1.1.1">MMLU</span>: Multi-domain question answering, MMLU assesses the model’s expertise over a wide range of specialized subjects, from professional domains to academia&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib30" title="">2021</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">General Understanding</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS3.SSS0.Px2.p1.1">This subset contains datasets that test general cognitive skills, language understanding, and common sense reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px2.p2">
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i1.p1.1.1">ARC Easy (ARC-E)</span>: The Easy set of the AI2 Reasoning Challenge&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib15" title="">2018</a>)</cite> features questions from the same source as ARC-C but are considered less challenging and do not require as advanced reasoning skills.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i2.p1.1.1">BoolQ</span>: A dataset consisting of boolean (yes/no) questions, focusing on reading comprehension and general understanding of natural language text&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib14" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i3.p1.1.1">Winogrande (Wino.)</span>: This dataset challenges models on common sense reasoning in a language context, focusing on pronoun disambiguation tasks&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(ai2, <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib1" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i4.p1">
<p class="ltx_p" id="A1.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i4.p1.1.1">PIQA</span>: Physical Interaction Question Answering tests the understanding of everyday physical processes, an aspect of practical common sense&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bisk et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib9" title="">2020</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i5.p1">
<p class="ltx_p" id="A1.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i5.p1.1.1">HellaSwag</span>: This dataset evaluates a model’s ability to complete scenarios in a contextually and logically coherent manner, requiring both language understanding and common sense reasoning&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zellers et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib72" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i6.p1">
<p class="ltx_p" id="A1.I2.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i6.p1.1.1">TruthfulQA</span>: Focused on the generation of truthful, accurate answers, this dataset challenges models on their ability to discern and reproduce factually correct information&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib42" title="">2021</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i7.p1">
<p class="ltx_p" id="A1.I2.i7.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i7.p1.1.1">OpenBookQA (OBQA)</span>: OpenBookQA requires understanding a wide array of facts and concepts, thereby evaluating the model’s broader knowledge and reasoning skills&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mihaylov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib49" title="">2018</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A1.I2.i8.p1">
<p class="ltx_p" id="A1.I2.i8.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i8.p1.1.1">LogiQA-2</span>: This dataset involves logical reasoning, testing the model’s capability to understand and apply logical constructs and principles&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib44" title="">2023b</a>)</cite>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.SSS0.Px2.p3">
<p class="ltx_p" id="A1.SS3.SSS0.Px2.p3.1">Each dataset in these subsets is carefully selected to challenge and evaluate specific aspects of natural language processing models, ranging from domain-specific knowledge in science, medicine, and mathematics, to broader skills like common sense reasoning and general language understanding.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Filtering Details for Synthetic Data</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">When generating synthetic paraphrases using language models, we occasionally encounter the challenge of extraneous introductions in the generated outputs. Such paraphrases might commence with phrases like ”Here’s a paraphrase…”, ”The following…” or even contain keywords such as ”high-quality English”. To mitigate this, we’ve developed a method to filter and refine the synthetic outputs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Methodology</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">The primary function, <span class="ltx_text ltx_font_typewriter" id="A2.SS1.p1.1.1">remove_unwanted_part</span>, starts by splitting the input data into individual sentences. If the first sentence contains delimiters such as ”\n\n” (indicating a new paragraph) or ”:”, the function checks the segment preceding the delimiter for the aforementioned unwanted elements. If these elements are detected, the preceding segment is removed. The entire revised content is then reconstructed and returned. In cases where no modifications are applicable, but we still have the flagged keywords, we remove the paraphrase completely. To achieve this:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p2">
<ol class="ltx_enumerate" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1">Split the input data into individual sentences using the NLTK’s sentence splitter function.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1">Examine the first sentence for the presence of delimiters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p" id="A2.I1.i3.p1.1">If a delimiter is detected, check the preceding segment for unwanted elements.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A2.I1.i4.p1">
<p class="ltx_p" id="A2.I1.i4.p1.1">If unwanted elements are found, discard the preceding segment (before an occurrence of <span class="ltx_text ltx_font_typewriter" id="A2.I1.i4.p1.1.1">"\n\n"</span> or <span class="ltx_text ltx_font_typewriter" id="A2.I1.i4.p1.1.2">":"</span>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="A2.I1.i5.p1">
<p class="ltx_p" id="A2.I1.i5.p1.1">Modify and return the filtered paragraph.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p3">
<p class="ltx_p" id="A2.SS1.p3.1">Based on manual inspection we found that the error rate (occurrence of sentences with unwanted elements) after the modification is less than 0.1%.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Properties of Synthetic Corpus</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">To understand the properties of synthetic data generated from the rephrase model that lead to better pre-training performance, we compare the semantic similarity, syntactic complexity, and diversity between synthetic data, C4 data, and data from the Pile. Our primary focus is answering the following questions about synthetic data: (i) Do models trained on synthetic data perform better due to information leakage from the rephrase model? (ii) Does the rephrase model accurately capture multiple styles? (iii) What attributes of synthetic data make it high quality? Our investigation helps address what data is beneficial for better generalization to specific domains, and quantify the importance of data variability and quality.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Experimental Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">We take a subset of the first 1000 documents from each of the datasets. For synthetic comparisons with real C4 data, we take pairs of samples, while for Pile subsets, we take the first 1000 samples from the test subset. When computing dataset quality statistics, we remove outliers more than two standard deviations in metric value. When the number of samples from the Pile subset was fewer than 1000, we split samples. Figures with distributions use a Gaussian Kernel Density Estimator (KDE) to construct distributions for statistics from 1000 values.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Semantic Properties</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A3.F9">
<br class="ltx_break ltx_centering"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="A3.F9.g1" src="x12.png" width="622">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Cosine similarity medium synthetic MRPC rephrases</figcaption>
<br class="ltx_break ltx_centering">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">In Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6" title="6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">6</span></a>, we compared pairs of examples of synthetic and real data to confirm the performance gain is not attributed to knowledge leakage from the rephrase models using a pre-trained BERT model trained with SimCSE objective <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib26" title="">2021</a>)</cite> for medium and qa prompts in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S6.F8" title="Figure 8 ‣ RQ6: Is there data leakage from the rephrase model to the trained model? ‣ 6.2 Method Ablations ‣ 6 Analysis and Ablations ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">8</span></a>(a) and (b). We additionally compare the similarity of synthetic rephrases and actual rephrases using the MRPC corpus in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A3.F9" title="Figure 9 ‣ C.2 Semantic Properties ‣ Appendix C Properties of Synthetic Corpus ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">9</span></a>(c). We denote this additional comparison by RealP (real paraphrase), while maintaining comparison of splits of the sentence: R1 and R2. Synthetic rephrases have similar cosine similarity on average and lower spread compared with true rephrases according in the MRPC corpus.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A3.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A3.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="415" id="A3.F9.sf1.g1" src="x13.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Flesch-Kincaid Reading Level</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A3.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="415" id="A3.F9.sf2.g1" src="x14.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Type token ratio</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Comparison of readability and diversity (ttr) of synthetic data compared with C4 and different subsets of the Pile.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS2.p2">
<p class="ltx_p" id="A3.SS2.p2.1">As the semantic information is similar between C4 and our synthetic data, we further investigate stylistic differences in the data. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A3.F10" title="Figure 10 ‣ C.2 Semantic Properties ‣ Appendix C Properties of Synthetic Corpus ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">10</span></a>(a) shows the Flesch–Kincaid reading levels for different rephrase styles, and the Pile. Our findings indicate that C4 is on the low end of reading level (7-8). In contrast, medium increases the reading level to 10, and qa synthetic variants further reduces the reading level to 6. Medium synthetic data matches the reading level of Wikipedia, and other high reading level datasets yielding better performance on these domains. On QA synthetic data, we observe reduced reading level. This is because we observed that sentences are typically split into question and answer leading to shorter setnences compared with in the original text and medium style rephrases. This leads to lower metric values for many of the metrics. For type token ratio, we note that the diversity is quite similar between medium and most subsets of the Pile. The QA dataset has particularly low TTR matching ubuntu, github, and math as these are more similar to QA format datasets and have heavy repetition of the Question, and Answer format.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Syntactic Properties</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A3.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A3.F10.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="415" id="A3.F10.sf1.g1" src="x15.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Tree Depth</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="A3.F10.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="415" id="A3.F10.sf2.g1" src="x16.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Mean Dependency Distance</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Comparison between synthetic and real data from the C4 corpus showing that synthetic data have higher syntactic complexity indicated by higher average tree depth, and higher mean dependency distance (MDD).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">Finally, we compare the mean tree depth (measured by the mean over setences of the depth of the dependency tree), and mean dependency distance (measured as the average dependency distance of any pair of words within a sentence) in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A3.F11" title="Figure 11 ‣ C.3 Syntactic Properties ‣ Appendix C Properties of Synthetic Corpus ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">11</span></a>, which have been shown to be good measures of syntactic difficulty <cite class="ltx_cite ltx_citemacro_cite">Futrell et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib22" title="">2015</a>); Gibson et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib27" title="">2000</a>); Oya (<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#bib.bib52" title="">2021</a>)</cite>. We find similar trends as for reading level and TTR diversity where mediums tyle increase depth, mdd, and syntactic complexity in general. We find again that QA style reduces this complexity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Evaluation Metrics</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">The metric utilized for evaluation is the <span class="ltx_text ltx_font_italic" id="A4.p1.1.1">macro token level perplexity</span>. Given a batch of encoded texts, the perplexity at the token level was computed as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A4.p2">
<p class="ltx_p" id="A4.p2.3">Given the accumulated loss over the entire dataset, denoted as <math alttext="L" class="ltx_Math" display="inline" id="A4.p2.1.m1.1"><semantics id="A4.p2.1.m1.1a"><mi id="A4.p2.1.m1.1.1" xref="A4.p2.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="A4.p2.1.m1.1b"><ci id="A4.p2.1.m1.1.1.cmml" xref="A4.p2.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.1.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="A4.p2.1.m1.1d">italic_L</annotation></semantics></math>, and the total number of tokens, represented by <math alttext="T" class="ltx_Math" display="inline" id="A4.p2.2.m2.1"><semantics id="A4.p2.2.m2.1a"><mi id="A4.p2.2.m2.1.1" xref="A4.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="A4.p2.2.m2.1b"><ci id="A4.p2.2.m2.1.1.cmml" xref="A4.p2.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="A4.p2.2.m2.1d">italic_T</annotation></semantics></math>, the macro token-level perplexity, denoted as <math alttext="\mathcal{P}" class="ltx_Math" display="inline" id="A4.p2.3.m3.1"><semantics id="A4.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="A4.p2.3.m3.1.1" xref="A4.p2.3.m3.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="A4.p2.3.m3.1b"><ci id="A4.p2.3.m3.1.1.cmml" xref="A4.p2.3.m3.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.3.m3.1c">\mathcal{P}</annotation><annotation encoding="application/x-llamapun" id="A4.p2.3.m3.1d">caligraphic_P</annotation></semantics></math>, is calculated as:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A4.p3">
<table class="ltx_equation ltx_eqn_table" id="A4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{P}=\exp\left(\min\left(20,\frac{L}{T}\right)\right)" class="ltx_Math" display="block" id="A4.E3.m1.5"><semantics id="A4.E3.m1.5a"><mrow id="A4.E3.m1.5.5" xref="A4.E3.m1.5.5.cmml"><mi class="ltx_font_mathcaligraphic" id="A4.E3.m1.5.5.3" xref="A4.E3.m1.5.5.3.cmml">𝒫</mi><mo id="A4.E3.m1.5.5.2" xref="A4.E3.m1.5.5.2.cmml">=</mo><mrow id="A4.E3.m1.5.5.1.1" xref="A4.E3.m1.5.5.1.2.cmml"><mi id="A4.E3.m1.4.4" xref="A4.E3.m1.4.4.cmml">exp</mi><mo id="A4.E3.m1.5.5.1.1a" xref="A4.E3.m1.5.5.1.2.cmml">⁡</mo><mrow id="A4.E3.m1.5.5.1.1.1" xref="A4.E3.m1.5.5.1.2.cmml"><mo id="A4.E3.m1.5.5.1.1.1.2" xref="A4.E3.m1.5.5.1.2.cmml">(</mo><mrow id="A4.E3.m1.5.5.1.1.1.1.2" xref="A4.E3.m1.5.5.1.1.1.1.1.cmml"><mi id="A4.E3.m1.1.1" xref="A4.E3.m1.1.1.cmml">min</mi><mo id="A4.E3.m1.5.5.1.1.1.1.2a" xref="A4.E3.m1.5.5.1.1.1.1.1.cmml">⁡</mo><mrow id="A4.E3.m1.5.5.1.1.1.1.2.1" xref="A4.E3.m1.5.5.1.1.1.1.1.cmml"><mo id="A4.E3.m1.5.5.1.1.1.1.2.1.1" xref="A4.E3.m1.5.5.1.1.1.1.1.cmml">(</mo><mn id="A4.E3.m1.2.2" xref="A4.E3.m1.2.2.cmml">20</mn><mo id="A4.E3.m1.5.5.1.1.1.1.2.1.2" xref="A4.E3.m1.5.5.1.1.1.1.1.cmml">,</mo><mfrac id="A4.E3.m1.3.3" xref="A4.E3.m1.3.3.cmml"><mi id="A4.E3.m1.3.3.2" xref="A4.E3.m1.3.3.2.cmml">L</mi><mi id="A4.E3.m1.3.3.3" xref="A4.E3.m1.3.3.3.cmml">T</mi></mfrac><mo id="A4.E3.m1.5.5.1.1.1.1.2.1.3" xref="A4.E3.m1.5.5.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A4.E3.m1.5.5.1.1.1.3" xref="A4.E3.m1.5.5.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.E3.m1.5b"><apply id="A4.E3.m1.5.5.cmml" xref="A4.E3.m1.5.5"><eq id="A4.E3.m1.5.5.2.cmml" xref="A4.E3.m1.5.5.2"></eq><ci id="A4.E3.m1.5.5.3.cmml" xref="A4.E3.m1.5.5.3">𝒫</ci><apply id="A4.E3.m1.5.5.1.2.cmml" xref="A4.E3.m1.5.5.1.1"><exp id="A4.E3.m1.4.4.cmml" xref="A4.E3.m1.4.4"></exp><apply id="A4.E3.m1.5.5.1.1.1.1.1.cmml" xref="A4.E3.m1.5.5.1.1.1.1.2"><min id="A4.E3.m1.1.1.cmml" xref="A4.E3.m1.1.1"></min><cn id="A4.E3.m1.2.2.cmml" type="integer" xref="A4.E3.m1.2.2">20</cn><apply id="A4.E3.m1.3.3.cmml" xref="A4.E3.m1.3.3"><divide id="A4.E3.m1.3.3.1.cmml" xref="A4.E3.m1.3.3"></divide><ci id="A4.E3.m1.3.3.2.cmml" xref="A4.E3.m1.3.3.2">𝐿</ci><ci id="A4.E3.m1.3.3.3.cmml" xref="A4.E3.m1.3.3.3">𝑇</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.E3.m1.5c">\mathcal{P}=\exp\left(\min\left(20,\frac{L}{T}\right)\right)</annotation><annotation encoding="application/x-llamapun" id="A4.E3.m1.5d">caligraphic_P = roman_exp ( roman_min ( 20 , divide start_ARG italic_L end_ARG start_ARG italic_T end_ARG ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="A4.p4">
<p class="ltx_p" id="A4.p4.1">Where:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i1.p1.1"><math alttext="\exp" class="ltx_Math" display="inline" id="A4.I1.i1.p1.1.m1.1"><semantics id="A4.I1.i1.p1.1.m1.1a"><mi id="A4.I1.i1.p1.1.m1.1.1" xref="A4.I1.i1.p1.1.m1.1.1.cmml">exp</mi><annotation-xml encoding="MathML-Content" id="A4.I1.i1.p1.1.m1.1b"><exp id="A4.I1.i1.p1.1.m1.1.1.cmml" xref="A4.I1.i1.p1.1.m1.1.1"></exp></annotation-xml><annotation encoding="application/x-tex" id="A4.I1.i1.p1.1.m1.1c">\exp</annotation><annotation encoding="application/x-llamapun" id="A4.I1.i1.p1.1.m1.1d">roman_exp</annotation></semantics></math> is the exponential function.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i2.p1.1"><math alttext="L" class="ltx_Math" display="inline" id="A4.I1.i2.p1.1.m1.1"><semantics id="A4.I1.i2.p1.1.m1.1a"><mi id="A4.I1.i2.p1.1.m1.1.1" xref="A4.I1.i2.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="A4.I1.i2.p1.1.m1.1b"><ci id="A4.I1.i2.p1.1.m1.1.1.cmml" xref="A4.I1.i2.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.I1.i2.p1.1.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="A4.I1.i2.p1.1.m1.1d">italic_L</annotation></semantics></math> is the cumulative loss over all shifted logits and labels in the dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A4.I1.i3.p1">
<p class="ltx_p" id="A4.I1.i3.p1.1"><math alttext="T" class="ltx_Math" display="inline" id="A4.I1.i3.p1.1.m1.1"><semantics id="A4.I1.i3.p1.1.m1.1a"><mi id="A4.I1.i3.p1.1.m1.1.1" xref="A4.I1.i3.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="A4.I1.i3.p1.1.m1.1b"><ci id="A4.I1.i3.p1.1.m1.1.1.cmml" xref="A4.I1.i3.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.I1.i3.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="A4.I1.i3.p1.1.m1.1d">italic_T</annotation></semantics></math> is the total number of tokens in the dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A4.p5">
<p class="ltx_p" id="A4.p5.1">The value of 20 acts as an upper limit to stabilize the metric in cases of high loss values.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Additional Results for Smaller Model and Token Sizes</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Results for 350M Models Trained for 75B Tokens</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A5.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="A5.F12.g1" src="x17.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Perplexity across all domains of the Pile comparing combining multiple styles of synthetic data. Models are 350M parameters trained for a total of 75B tokens.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A5.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T8.1" style="width:227.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.5pt,9.0pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.T8.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T8.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A5.T8.1.1.1.1.1">Dataset (Real Tok.)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T8.1.1.1.1.2">ARC-C</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T8.1.1.1.1.3">SciQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T8.1.1.1.1.4">PubMedQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T8.1.1.1.1.5">MathQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T8.1.1.1.1.6">MMLU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T8.1.1.1.1.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T8.1.1.1.1.7.1" style="background-color:#E0FFFF;">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T8.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T8.1.1.2.1.1">C4-15B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.1.1.2.1.2">21.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.1.1.2.1.3">77.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.1.1.2.1.4">50.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.1.1.2.1.5">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.1.1.2.1.6">23.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.1.1.2.1.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T8.1.1.2.1.7.1" style="background-color:#E0FFFF;">38.8</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T8.1.1.3.2.1">C4-60B</th>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.3.2.2">23.4</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.3.2.3">76.2</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.3.2.4">46.4</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.3.2.5">22.0</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.3.2.6">23.0</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.3.2.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T8.1.1.3.2.7.1" style="background-color:#E0FFFF;">38.2</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T8.1.1.4.3.1">QA+C4-15B</th>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.4.3.2">24.4</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.4.3.3">79.8</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.4.3.4">56.0</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.4.3.5">21.7</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.4.3.6">22.9</td>
<td class="ltx_td ltx_align_center" id="A5.T8.1.1.4.3.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T8.1.1.4.3.7.1" style="background-color:#E0FFFF;">41.0</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A5.T8.1.1.5.4.1">Med+C4-15B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.1.1.5.4.2">22.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.1.1.5.4.3">74.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.1.1.5.4.4">53.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.1.1.5.4.5">22.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.1.1.5.4.6">23.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T8.1.1.5.4.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T8.1.1.5.4.7.1" style="background-color:#E0FFFF;">39.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Evaluation of 350M parameter LLMs trained for 75B tokens on Specialized Knowledge Tasks. This table presents the performance on tasks that require specific domain knowledge such as science, medicine, mathematics, and logic.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A5.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T9.1" style="width:305.4pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.2pt,9.0pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.T9.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T9.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A5.T9.1.1.1.1.1">Dataset (Real Tok.)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T9.1.1.1.1.2">ARC-E</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T9.1.1.1.1.3">BoolQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T9.1.1.1.1.4">Wino.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T9.1.1.1.1.5">PIQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T9.1.1.1.1.6">HellaSwag</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T9.1.1.1.1.7">TruthfulQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T9.1.1.1.1.8">OBQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T9.1.1.1.1.9">LogiQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T9.1.1.1.1.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T9.1.1.1.1.10.1" style="background-color:#E0FFFF;">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T9.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T9.1.1.2.1.1">C4-18B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T9.1.1.2.1.2">50.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T9.1.1.2.1.3">52.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T9.1.1.2.1.4">53.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T9.1.1.2.1.5">69.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T9.1.1.2.1.6">35.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T9.1.1.2.1.7">37.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T9.1.1.2.1.8">18.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T9.1.1.2.1.9">23.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T9.1.1.2.1.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T9.1.1.2.1.10.1" style="background-color:#E0FFFF;">42.6</span></td>
</tr>
<tr class="ltx_tr" id="A5.T9.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T9.1.1.3.2.1">C4-75B</th>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.3.2.2">51.4</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.3.2.3">53.4</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.3.2.4">51.6</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.3.2.5">70.3</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.3.2.6">36.1</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.3.2.7">39.0</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.3.2.8">17.4</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.3.2.9">22.6</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.3.2.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T9.1.1.3.2.10.1" style="background-color:#E0FFFF;">42.7</span></td>
</tr>
<tr class="ltx_tr" id="A5.T9.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T9.1.1.4.3.1">QA+C4-18B</th>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.4.3.2">53.4</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.4.3.3">60.7</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.4.3.4">52.2</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.4.3.5">70.0</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.4.3.6">36.3</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.4.3.7">40.0</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.4.3.8">17.6</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.4.3.9">22.3</td>
<td class="ltx_td ltx_align_center" id="A5.T9.1.1.4.3.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T9.1.1.4.3.10.1" style="background-color:#E0FFFF;">44.1</span></td>
</tr>
<tr class="ltx_tr" id="A5.T9.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A5.T9.1.1.5.4.1">Med+C4-18B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T9.1.1.5.4.2">50.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T9.1.1.5.4.3">57.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T9.1.1.5.4.4">53.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T9.1.1.5.4.5">70.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T9.1.1.5.4.6">36.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T9.1.1.5.4.7">36.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T9.1.1.5.4.8">18.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T9.1.1.5.4.9">22.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T9.1.1.5.4.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T9.1.1.5.4.10.1" style="background-color:#E0FFFF;">43.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Evaluation of 350M parameter LLMs trained for 75B tokens on General Understanding Tasks. This table shows the performance across various datasets, focusing on general reasoning, language understanding, and common sense comparing training .</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A5.SS1.p1">
<p class="ltx_p" id="A5.SS1.p1.2">We train models at smaller scales and demonstrate improvement. In particular we train a 350M GPT-2-medium architecture for a total of 75B tokens. We show Pile perplexity averaged across the 21 domains is much lower than for that of the model trained only on C4 in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A5.F12" title="Figure 12 ‣ E.1 Results for 350M Models Trained for 75B Tokens ‣ Appendix E Additional Results for Smaller Model and Token Sizes ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">12</span></a>, and even lower than 1.3B models trained only on C4 in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S1.F0.sf3" title="0(c) ‣ Figure 1 ‣ 1 Introduction ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">0(c)</span></a>. We also show an increase of <math alttext="1.5\%" class="ltx_Math" display="inline" id="A5.SS1.p1.1.m1.1"><semantics id="A5.SS1.p1.1.m1.1a"><mrow id="A5.SS1.p1.1.m1.1.1" xref="A5.SS1.p1.1.m1.1.1.cmml"><mn id="A5.SS1.p1.1.m1.1.1.2" xref="A5.SS1.p1.1.m1.1.1.2.cmml">1.5</mn><mo id="A5.SS1.p1.1.m1.1.1.1" xref="A5.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.SS1.p1.1.m1.1b"><apply id="A5.SS1.p1.1.m1.1.1.cmml" xref="A5.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="A5.SS1.p1.1.m1.1.1.1.cmml" xref="A5.SS1.p1.1.m1.1.1.1">percent</csymbol><cn id="A5.SS1.p1.1.m1.1.1.2.cmml" type="float" xref="A5.SS1.p1.1.m1.1.1.2">1.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.SS1.p1.1.m1.1c">1.5\%</annotation><annotation encoding="application/x-llamapun" id="A5.SS1.p1.1.m1.1d">1.5 %</annotation></semantics></math> across general understanding language tasks, and roughly <math alttext="3\%" class="ltx_Math" display="inline" id="A5.SS1.p1.2.m2.1"><semantics id="A5.SS1.p1.2.m2.1a"><mrow id="A5.SS1.p1.2.m2.1.1" xref="A5.SS1.p1.2.m2.1.1.cmml"><mn id="A5.SS1.p1.2.m2.1.1.2" xref="A5.SS1.p1.2.m2.1.1.2.cmml">3</mn><mo id="A5.SS1.p1.2.m2.1.1.1" xref="A5.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.SS1.p1.2.m2.1b"><apply id="A5.SS1.p1.2.m2.1.1.cmml" xref="A5.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="A5.SS1.p1.2.m2.1.1.1.cmml" xref="A5.SS1.p1.2.m2.1.1.1">percent</csymbol><cn id="A5.SS1.p1.2.m2.1.1.2.cmml" type="integer" xref="A5.SS1.p1.2.m2.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.SS1.p1.2.m2.1c">3\%</annotation><annotation encoding="application/x-llamapun" id="A5.SS1.p1.2.m2.1d">3 %</annotation></semantics></math> on specialized knowledge tasks in Tables&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A5.T8" title="Table 8 ‣ E.1 Results for 350M Models Trained for 75B Tokens ‣ Appendix E Additional Results for Smaller Model and Token Sizes ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">8</span></a>–<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A5.T9" title="Table 9 ‣ E.1 Results for 350M Models Trained for 75B Tokens ‣ Appendix E Additional Results for Smaller Model and Token Sizes ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">9</span></a> when adding QA rephrases. We also experimented with medium rephrases at this smaller scale. Our findings indicate that the high quality provided by medium rephrases improves performance over only C4, however matching the style as indicated by QA rephrase performance further improves performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Results for 1.3B Models Trained for 150B Tokens</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A5.SS2.p1">
<p class="ltx_p" id="A5.SS2.p1.2">We additionally train 1.3B GPT-2-XL models at 150B tokens, reducing the number of steps by half. We show Pile perplexity averaged across the 20 domains is much lower than for that of the model trained only on C4 in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A5.F13" title="Figure 13 ‣ E.2 Results for 1.3B Models Trained for 150B Tokens ‣ Appendix E Additional Results for Smaller Model and Token Sizes ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">13</span></a>, and even lower than 1.3B models trained only on C4 in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S1.F0.sf3" title="0(c) ‣ Figure 1 ‣ 1 Introduction ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">0(c)</span></a> for twice as long. We also show an increase of <math alttext="2\%" class="ltx_Math" display="inline" id="A5.SS2.p1.1.m1.1"><semantics id="A5.SS2.p1.1.m1.1a"><mrow id="A5.SS2.p1.1.m1.1.1" xref="A5.SS2.p1.1.m1.1.1.cmml"><mn id="A5.SS2.p1.1.m1.1.1.2" xref="A5.SS2.p1.1.m1.1.1.2.cmml">2</mn><mo id="A5.SS2.p1.1.m1.1.1.1" xref="A5.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.SS2.p1.1.m1.1b"><apply id="A5.SS2.p1.1.m1.1.1.cmml" xref="A5.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="A5.SS2.p1.1.m1.1.1.1.cmml" xref="A5.SS2.p1.1.m1.1.1.1">percent</csymbol><cn id="A5.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="A5.SS2.p1.1.m1.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.SS2.p1.1.m1.1c">2\%</annotation><annotation encoding="application/x-llamapun" id="A5.SS2.p1.1.m1.1d">2 %</annotation></semantics></math> across specialized knowledge tasks, and roughly <math alttext="2.5\%" class="ltx_Math" display="inline" id="A5.SS2.p1.2.m2.1"><semantics id="A5.SS2.p1.2.m2.1a"><mrow id="A5.SS2.p1.2.m2.1.1" xref="A5.SS2.p1.2.m2.1.1.cmml"><mn id="A5.SS2.p1.2.m2.1.1.2" xref="A5.SS2.p1.2.m2.1.1.2.cmml">2.5</mn><mo id="A5.SS2.p1.2.m2.1.1.1" xref="A5.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.SS2.p1.2.m2.1b"><apply id="A5.SS2.p1.2.m2.1.1.cmml" xref="A5.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="A5.SS2.p1.2.m2.1.1.1.cmml" xref="A5.SS2.p1.2.m2.1.1.1">percent</csymbol><cn id="A5.SS2.p1.2.m2.1.1.2.cmml" type="float" xref="A5.SS2.p1.2.m2.1.1.2">2.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.SS2.p1.2.m2.1c">2.5\%</annotation><annotation encoding="application/x-llamapun" id="A5.SS2.p1.2.m2.1d">2.5 %</annotation></semantics></math> on general understanding tasks in Tables&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A5.T10" title="Table 10 ‣ E.2 Results for 1.3B Models Trained for 150B Tokens ‣ Appendix E Additional Results for Smaller Model and Token Sizes ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">10</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#A5.T11" title="Table 11 ‣ E.2 Results for 1.3B Models Trained for 150B Tokens ‣ Appendix E Additional Results for Smaller Model and Token Sizes ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">11</span></a> when adding QA rephrases. We also experimented with medium rephrases at this smaller scale, and report similar findings consistent with other small-scale experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A5.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="A5.F13.g1" src="x18.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Perplexity across all domains of the Pile comparing combining multiple styles of synthetic data. Models are 350M parameters trained for a total of 75B tokens.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A5.T10">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T10.3" style="width:227.7pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.5pt,9.0pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.T10.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T10.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A5.T10.3.1.1.1.1">Dataset (Real Tok.)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T10.3.1.1.1.2">ARC-C</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T10.3.1.1.1.3">SciQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T10.3.1.1.1.4">PubMedQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T10.3.1.1.1.5">MathQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T10.3.1.1.1.6">MMLU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T10.3.1.1.1.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T10.3.1.1.1.7.1" style="background-color:#E0FFFF;">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T10.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T10.3.1.2.1.1">C4-35B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T10.3.1.2.1.2">27.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T10.3.1.2.1.3">83.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T10.3.1.2.1.4">55.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T10.3.1.2.1.5">22.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T10.3.1.2.1.6">24.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T10.3.1.2.1.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T10.3.1.2.1.7.1" style="background-color:#E0FFFF;">42.4</span></td>
</tr>
<tr class="ltx_tr" id="A5.T10.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T10.3.1.3.2.1">C4-150B</th>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.3.2.2">25.9</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.3.2.3">83.8</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.3.2.4">55.4</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.3.2.5">23.5</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.3.2.6">25.4</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.3.2.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T10.3.1.3.2.7.1" style="background-color:#E0FFFF;">42.8</span></td>
</tr>
<tr class="ltx_tr" id="A5.T10.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T10.3.1.4.3.1">Med+C4-35B</th>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.4.3.2">27.2</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.4.3.3">82.2</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.4.3.4">46.2</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.4.3.5">23.1</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.4.3.6">25.2</td>
<td class="ltx_td ltx_align_center" id="A5.T10.3.1.4.3.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T10.3.1.4.3.7.1" style="background-color:#E0FFFF;">40.8</span></td>
</tr>
<tr class="ltx_tr" id="A5.T10.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A5.T10.3.1.5.4.1">QA+C4-35B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T10.3.1.5.4.2">29.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T10.3.1.5.4.3">85.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T10.3.1.5.4.4">62.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T10.3.1.5.4.5">22.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T10.3.1.5.4.6">26.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T10.3.1.5.4.7" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T10.3.1.5.4.7.1" style="background-color:#E0FFFF;">45.0</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Evaluation of <math alttext="\sim 1.3" class="ltx_Math" display="inline" id="A5.T10.2.m1.1"><semantics id="A5.T10.2.m1.1b"><mrow id="A5.T10.2.m1.1.1" xref="A5.T10.2.m1.1.1.cmml"><mi id="A5.T10.2.m1.1.1.2" xref="A5.T10.2.m1.1.1.2.cmml"></mi><mo id="A5.T10.2.m1.1.1.1" xref="A5.T10.2.m1.1.1.1.cmml">∼</mo><mn id="A5.T10.2.m1.1.1.3" xref="A5.T10.2.m1.1.1.3.cmml">1.3</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.T10.2.m1.1c"><apply id="A5.T10.2.m1.1.1.cmml" xref="A5.T10.2.m1.1.1"><csymbol cd="latexml" id="A5.T10.2.m1.1.1.1.cmml" xref="A5.T10.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A5.T10.2.m1.1.1.2.cmml" xref="A5.T10.2.m1.1.1.2">absent</csymbol><cn id="A5.T10.2.m1.1.1.3.cmml" type="float" xref="A5.T10.2.m1.1.1.3">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T10.2.m1.1d">\sim 1.3</annotation><annotation encoding="application/x-llamapun" id="A5.T10.2.m1.1e">∼ 1.3</annotation></semantics></math>B parameter LLMs trained for 150B tokens on Specialized Knowledge Tasks. This table presents the performance on tasks that require specific domain knowledge such as science, medicine, mathematics, and logic.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A5.T11">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T11.3" style="width:305.4pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.2pt,9.0pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.T11.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T11.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A5.T11.3.1.1.1.1">Dataset (Real Tok.)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T11.3.1.1.1.2">ARC-E</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T11.3.1.1.1.3">BoolQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T11.3.1.1.1.4">Wino.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T11.3.1.1.1.5">PIQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T11.3.1.1.1.6">HellaSwag</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T11.3.1.1.1.7">TruthfulQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T11.3.1.1.1.8">OBQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T11.3.1.1.1.9">LogiQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A5.T11.3.1.1.1.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T11.3.1.1.1.10.1" style="background-color:#E0FFFF;">Avg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T11.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T11.3.1.2.1.1">C4-35B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T11.3.1.2.1.2">58.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T11.3.1.2.1.3">55.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T11.3.1.2.1.4">56.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T11.3.1.2.1.5">73.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T11.3.1.2.1.6">44.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T11.3.1.2.1.7">36.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T11.3.1.2.1.8">22.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T11.3.1.2.1.9">22.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T11.3.1.2.1.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T11.3.1.2.1.10.1" style="background-color:#E0FFFF;">46.2</span></td>
</tr>
<tr class="ltx_tr" id="A5.T11.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T11.3.1.3.2.1">C4-150B</th>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.3.2.2">59.1</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.3.2.3">54.4</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.3.2.4">56.4</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.3.2.5">74.5</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.3.2.6">44.9</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.3.2.7">34.3</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.3.2.8">22.2</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.3.2.9">22.1</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.3.2.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T11.3.1.3.2.10.1" style="background-color:#E0FFFF;">46.0</span></td>
</tr>
<tr class="ltx_tr" id="A5.T11.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T11.3.1.4.3.1">Med+C4-35B</th>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.4.3.2">59.8</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.4.3.3">57.0</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.4.3.4">55.7</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.4.3.5">74.6</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.4.3.6">44.5</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.4.3.7">36.5</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.4.3.8">23.8</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.4.3.9">21.5</td>
<td class="ltx_td ltx_align_center" id="A5.T11.3.1.4.3.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T11.3.1.4.3.10.1" style="background-color:#E0FFFF;">46.7</span></td>
</tr>
<tr class="ltx_tr" id="A5.T11.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A5.T11.3.1.5.4.1">QA+C4-35B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T11.3.1.5.4.2">62.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T11.3.1.5.4.3">63.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T11.3.1.5.4.4">55.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T11.3.1.5.4.5">74.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T11.3.1.5.4.6">44.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T11.3.1.5.4.7">41.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T11.3.1.5.4.8">22.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T11.3.1.5.4.9">23.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A5.T11.3.1.5.4.10" style="background-color:#E0FFFF;"><span class="ltx_text" id="A5.T11.3.1.5.4.10.1" style="background-color:#E0FFFF;">48.4</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Evaluation of <math alttext="\sim 1.3" class="ltx_Math" display="inline" id="A5.T11.2.m1.1"><semantics id="A5.T11.2.m1.1b"><mrow id="A5.T11.2.m1.1.1" xref="A5.T11.2.m1.1.1.cmml"><mi id="A5.T11.2.m1.1.1.2" xref="A5.T11.2.m1.1.1.2.cmml"></mi><mo id="A5.T11.2.m1.1.1.1" xref="A5.T11.2.m1.1.1.1.cmml">∼</mo><mn id="A5.T11.2.m1.1.1.3" xref="A5.T11.2.m1.1.1.3.cmml">1.3</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.T11.2.m1.1c"><apply id="A5.T11.2.m1.1.1.cmml" xref="A5.T11.2.m1.1.1"><csymbol cd="latexml" id="A5.T11.2.m1.1.1.1.cmml" xref="A5.T11.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A5.T11.2.m1.1.1.2.cmml" xref="A5.T11.2.m1.1.1.2">absent</csymbol><cn id="A5.T11.2.m1.1.1.3.cmml" type="float" xref="A5.T11.2.m1.1.1.3">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T11.2.m1.1d">\sim 1.3</annotation><annotation encoding="application/x-llamapun" id="A5.T11.2.m1.1e">∼ 1.3</annotation></semantics></math>B parameter LLMs trained for 150B tokens on General Understanding Tasks. This table shows the performance across various datasets, focusing on general reasoning, language understanding, and common sense comparing training .</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>LLM Leaderboard Few-shot Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">In our main experiments in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2401.16380v1#S4" title="4 Perplexity Evaluation ‣ Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"><span class="ltx_text ltx_ref_tag">4</span></a> we demonstrate that LLMs trained with synthetic rephrases are a better backbone for zero-shot question-answering tasks as the model learns the question-answer format and style during pre-training. In this section, we show that improvements from pre-training on synthetic rephrases are still present even in few-shot settings where the model has access to test samples. To study few-shot performance, we evaluate on six tasks present in the OpenLLMLeaderboard<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a></span></span></span>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A6.p2">
<ol class="ltx_enumerate" id="A6.I1">
<li class="ltx_item" id="A6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A6.I1.i1.p1">
<p class="ltx_p" id="A6.I1.i1.p1.1">ARC-Challenge (25 shot)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A6.I1.i2.p1">
<p class="ltx_p" id="A6.I1.i2.p1.1">HellaSwag (10 shot)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A6.I1.i3.p1">
<p class="ltx_p" id="A6.I1.i3.p1.1">MMLU (5 shot)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A6.I1.i4.p1">
<p class="ltx_p" id="A6.I1.i4.p1.1">Truthful-QA (5 shot)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A6.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="A6.I1.i5.p1">
<p class="ltx_p" id="A6.I1.i5.p1.1">Winogrande (5 shot)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A6.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para ltx_noindent" id="A6.I1.i6.p1">
<p class="ltx_p" id="A6.I1.i6.p1.1">GSM8k (5 shot)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="A6.p3">
<p class="ltx_p" id="A6.p3.1">We evaluate two models trained for 300B and 350B tokens corresponding to roughly 85B and 100B unique C4 tokens respectively. Our findings show substantial improvements on the ARC-challenge benchmark, and Truthful-QA conssitent in the zero-shot settings and comparable performance across other datasets. Our models also perform better than the publicly released Falcon-1.3B model trained on the Refined Web dataset, and the Pythia-1.4B model, which was trained on Pile.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A6.T12">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.T12.1" style="width:397.5pt;height:119.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(19.0pt,-5.7pt) scale(1.10548736362817,1.10548736362817) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A6.T12.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A6.T12.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A6.T12.1.1.1.1.1">Dataset</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.1.1.1.2">ARC</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.1.1.1.3">Hellaswag</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.1.1.1.4">MMLU</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.1.1.1.5">TruthfulQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.1.1.1.6">WinoGrande</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.1.1.1.7">GSM8K</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.1.1.1.8" style="background-color:#E0FFFF;"><span class="ltx_text" id="A6.T12.1.1.1.1.8.1" style="background-color:#E0FFFF;">Avg</span></td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A6.T12.1.1.2.2.1">C4</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.2.2.2">31.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.2.2.3">62.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.2.2.4">26.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.2.2.5">33.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.2.2.6">57.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.2.2.7">0.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.2.2.8" style="background-color:#E0FFFF;"><span class="ltx_text" id="A6.T12.1.1.2.2.8.1" style="background-color:#E0FFFF;">35.5</span></td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T12.1.1.3.3.1">Falcon-RW</th>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.3.3.2">35.1</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.3.3.3">63.6</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.3.3.4">25.3</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.3.3.5">36.0</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.3.3.6">62.0</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.3.3.7">0.5</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.3.3.8" style="background-color:#E0FFFF;"><span class="ltx_text" id="A6.T12.1.1.3.3.8.1" style="background-color:#E0FFFF;">37.1</span></td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A6.T12.1.1.4.4.1">Pythia-1.4b-Pile</th>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.4.4.2">32.7</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.4.4.3">55.0</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.4.4.4">25.6</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.4.4.5">38.7</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.4.4.6">57.3</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.4.4.7">0.8</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.4.4.8" style="background-color:#E0FFFF;"><span class="ltx_text" id="A6.T12.1.1.4.4.8.1" style="background-color:#E0FFFF;">35.0</span></td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A6.T12.1.1.5.5.1">QA+C4-85B (300K)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.5.5.2">36.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.5.5.3">60.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.5.5.4">25.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.5.5.5">40.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.5.5.6">59.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.5.5.7">0.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A6.T12.1.1.5.5.8" style="background-color:#E0FFFF;"><span class="ltx_text" id="A6.T12.1.1.5.5.8.1" style="background-color:#E0FFFF;">37.2</span></td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A6.T12.1.1.6.6.1">QA+C4-100B (350K)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.1.6.6.2">35.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.1.6.6.3">60.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.1.6.6.4">26.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.1.6.6.5">40.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.1.6.6.6">61.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.1.6.6.7">0.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.1.6.6.8" style="background-color:#E0FFFF;"><span class="ltx_text" id="A6.T12.1.1.6.6.8.1" style="background-color:#E0FFFF;">37.5</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>1.3B 300K LLM Leaderboard Eval. Evaluation is done on a single seed (1234).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Rephrase Prompt Templates</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">We detail the prompts given to the Mistral-7B model to generate synthetic versions of the C4 dataset in specific styles. <em class="ltx_emph ltx_font_italic" id="A7.p1.1.1">Note: there are slight variations in the prompt that were used for other frozen LLMs, and no prompt was used for the T5 model.</em></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A7.SSx1">
<h3 class="ltx_title ltx_title_subsection">Easy Style</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A7.SSx1.p1">
<p class="ltx_p" id="A7.SSx1.p1.1">A style designed to generate content understandable by toddlers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A7.SSx1.p2">
<svg class="ltx_picture" height="86.98" id="A7.SSx1.p2.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,86.98) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 81.08 C 0 84.34 2.64 86.98 5.91 86.98 L 600 86.98 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 81.08 C 1.97 83.25 3.73 85.01 5.91 85.01 L 598.03 85.01 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="59.42" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="A7.SSx1.p2.pic1.2.2.2.2.2" style="width:402.3pt;"><span class="ltx_text ltx_font_typewriter" id="A7.SSx1.p2.pic1.2.2.2.2.2.1">A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the questions. USER: For the following paragraph give me a paraphrase of the same using a very small vocabulary and extremely simple sentences that a toddler will understand:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A7.SSx2">
<h3 class="ltx_title ltx_title_subsection">Hard Style</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A7.SSx2.p1">
<p class="ltx_p" id="A7.SSx2.p1.1">A style designed to generate content comprehensible primarily to scholars using arcane language.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A7.SSx2.p2">
<svg class="ltx_picture" height="89.67" id="A7.SSx2.p2.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,89.67) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 83.77 C 0 87.03 2.64 89.67 5.91 89.67 L 600 89.67 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 83.77 C 1.97 85.94 3.73 87.7 5.91 87.7 L 598.03 87.7 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="A7.SSx2.p2.pic1.2.2.2.2.2" style="width:402.3pt;"><span class="ltx_text ltx_font_typewriter" id="A7.SSx2.p2.pic1.2.2.2.2.2.1">A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the questions. USER: For the following paragraph give me a paraphrase of the same using very terse and abstruse language that only an erudite scholar will understand. Replace simple words and phrases with rare and complex ones:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A7.SSx3">
<h3 class="ltx_title ltx_title_subsection">Medium Style</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A7.SSx3.p1">
<p class="ltx_p" id="A7.SSx3.p1.1">A style designed to generate content comparable to standard encyclopedic entries.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A7.SSx3.p2">
<svg class="ltx_picture" height="89.67" id="A7.SSx3.p2.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,89.67) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 83.77 C 0 87.03 2.64 89.67 5.91 89.67 L 600 89.67 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 83.77 C 1.97 85.94 3.73 87.7 5.91 87.7 L 598.03 87.7 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="A7.SSx3.p2.pic1.2.2.2.2.2" style="width:402.3pt;"><span class="ltx_text ltx_font_typewriter" id="A7.SSx3.p2.pic1.2.2.2.2.2.1">A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the questions. USER: For the following paragraph give me a diverse paraphrase of the same in high quality English language as in sentences on Wikipedia:</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A7.SSx4">
<h3 class="ltx_title ltx_title_subsection">Q/A Style</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A7.SSx4.p1">
<p class="ltx_p" id="A7.SSx4.p1.1">A style intended to convert narratives into a conversational format.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A7.SSx4.p2">
<svg class="ltx_picture" height="73.22" id="A7.SSx4.p2.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,73.22) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 67.32 C 0 70.58 2.64 73.22 5.91 73.22 L 600 73.22 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 67.32 C 1.97 69.49 3.73 71.25 5.91 71.25 L 598.03 71.25 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="45.66" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<p class="ltx_p ltx_minipage ltx_align_bottom" id="A7.SSx4.p2.pic1.2.2.2.2.2" style="width:402.3pt;"><span class="ltx_text ltx_font_typewriter" id="A7.SSx4.p2.pic1.2.2.2.2.2.1">A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the questions. USER: Convert the following paragraph into a conversational format with multiple tags of "Question:" followed by "Answer:":</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Rephrase Examples</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A8.p1">
<p class="ltx_p" id="A8.p1.1"><span class="ltx_text ltx_font_bold" id="A8.p1.1.1">Samplesfrom the MRPC corpus generated by the Mistral-7B model.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A8.SSx1">
<h3 class="ltx_title ltx_title_subsection">Original</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A8.SSx1.p1">
<svg class="ltx_picture" height="103.59" id="A8.SSx1.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,103.59) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 97.68 C 0 100.94 2.64 103.59 5.91 103.59 L 600 103.59 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 97.68 C 1.97 99.85 3.73 101.62 5.91 101.62 L 598.03 101.62 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="76.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="A8.SSx1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_para" id="A8.SSx1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="A8.SSx1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A8.SSx1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1">The stock rose $2.11, or about 11 percent, to close Friday at $21.51 on the New York Stock Exchange.</span></span>
<span class="ltx_p" id="A8.SSx1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text ltx_font_typewriter" id="A8.SSx1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2.1">Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.</span></span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A8.SSx2">
<h3 class="ltx_title ltx_title_subsection">Medium Style</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A8.SSx2.p1">
<svg class="ltx_picture" height="106.28" id="A8.SSx2.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,106.28) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 100.37 C 0 103.63 2.64 106.28 5.91 106.28 L 600 106.28 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 100.37 C 1.97 102.54 3.73 104.31 5.91 104.31 L 598.03 104.31 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="A8.SSx2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_para" id="A8.SSx2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="A8.SSx2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A8.SSx2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1">The stock experienced an increase of approximately 11 percent, closing at $21.51 on the New York Stock Exchange on Friday, with a rise of $2.11.</span></span>
<span class="ltx_p" id="A8.SSx2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text ltx_font_typewriter" id="A8.SSx2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2.1">During the initial three months of the current year, there was a 15 percent decrease in revenue compared to the corresponding quarter of the previous year.</span></span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A8.SSx3">
<h3 class="ltx_title ltx_title_subsection">Q/A Style</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A8.SSx3.p1">
<svg class="ltx_picture" height="106.28" id="A8.SSx3.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,106.28) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 100.37 C 0 103.63 2.64 106.28 5.91 106.28 L 600 106.28 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 100.37 C 1.97 102.54 3.73 104.31 5.91 104.31 L 598.03 104.31 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="A8.SSx3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_para" id="A8.SSx3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="A8.SSx3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A8.SSx3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1">Question: What was the stock’s closing price on Friday? Answer: $21.51 Question: How much did the stock rise on Friday? Answer: $2.11 or about 11 percent.</span></span>
<span class="ltx_p" id="A8.SSx3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text ltx_font_typewriter" id="A8.SSx3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2.1">Question: What was the revenue drop in the first quarter compared to the same period last year? Answer: The revenue dropped 15 percent.</span></span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_noindent" id="A8.SSx3.p2">
<p class="ltx_p" id="A8.SSx3.p2.1"><span class="ltx_text ltx_font_bold" id="A8.SSx3.p2.1.1">Samples from the C4 corpus generated by the Mistral-7B model.</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A8.SSx4">
<h3 class="ltx_title ltx_title_subsection">Original</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A8.SSx4.p1">
<svg class="ltx_picture" height="651.53" id="A8.SSx4.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,651.53) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 645.63 C 0 648.89 2.64 651.53 5.91 651.53 L 600 651.53 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 645.63 C 1.97 647.8 3.73 649.56 5.91 649.56 L 598.03 649.56 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="623.97" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_para" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1">First round on stress at work survey. Answering the questionnaire is voluntary and all answers will be saved anonymously. Please fill in this questionnaire only if you have some work experience, part-or full time. Otherwise, you will not be able to answer some of the questions! Here is a the link to all language version.</span></span>
<span class="ltx_p" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text ltx_font_typewriter" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2.1">Not that there’s a thing wrong with frozen burgers. The key here is the meat seasonings, which are pretty strong and spicy and just GOOD, something else I think is really necessary in a turkey burger because ground turkey otherwise can be kind of flavorless. You’ll need ground turkey, onion powder, chili powder, salt, pepper, and cayenne pepper for the burgers. Then the mayo takes garlic and onion. Then we need buns, clearly, swiss cheese, lettuce, and onion. I LOVE tomatoes but sometimes find that they get in the way of other flavors, so I left them off of this burger. Add them if you’d like to your array of toppings! First, we’ll make the mayo. Grate the garlic directly into the mayo, add a pinch of salt, and squeeze in the lemon juice. Stir. Done! I love this. Then, we’ll work on the burgers. Preheat a large skillet to medium-high heat with some olive oil, preheat the broiler to high, then add all the spices to the ground turkey.</span></span>
<span class="ltx_p" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.3"><span class="ltx_text ltx_font_typewriter" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.3.1">Whether you like your velvet crushed, vibrant or head-to-toe, there’s really no denying the sheer luxe and elegance of this timeless textile. Not only is it super stylish, it can actually be so wearable for day-to-day wear. Yes, really! This year it’s all about embracing fun gem-toned velvety pieces. Long gone are the days when velvet was solely associated with dark moody shades of navy and black. Below we’ve rounded up the most covetable velvet pieces on the high street right now. We’re already coming up with outfit ideas! Are you completely obsessed or beyond bored of it?</span></span>
<span class="ltx_p" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.4"><span class="ltx_text ltx_font_typewriter" id="A8.SSx4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.4.1">Save up to $8,086 on one of 1,258 Chrysler 200s near you. Find your perfect car with Edmunds expert and consumer car reviews, dealer reviews, car comparisons and pricing tools. We have 4,850,420. Research 2015 Chrysler 200 Sedan 4D 200C I4 prices, used values &amp; 200 Sedan 4D 200C I4 pricing, specs and more. Many years ago, we wrote about the stalling problem with the 2011 Chrysler 200, and believe it or not, we still receive an occasional call regarding the problem.However, a much larger issue has monopolized the phone lines as of late 2015 Chrysler 200 transmission problems leaving drivers with check engine lights, harsh shifting, and the occasional loss of power. The 2015 Chrysler 200 can fetch a premium for its style and its horsepower--but rear-seat room and handling are better bargains elsewhere. Find out why the 2015 Chrysler 200 is rated 8.4 by The. Don’t know where to find the perfect rims for your 2015 Chrysler 200 CARiD.com stores a massive selection of 2015 Chrysler 200 wheels offered in myriads of design and finish options, including chrome, black, silver, and so much more.</span></span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="A8.SSx5">
<h3 class="ltx_title ltx_title_subsection">Medium Style</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A8.SSx5.p1">
<svg class="ltx_picture" height="704.03" id="A8.SSx5.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,704.03) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 698.13 C 0 701.39 2.64 704.03 5.91 704.03 L 600 704.03 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 698.13 C 1.97 700.3 3.73 702.07 5.91 702.07 L 598.03 702.07 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="676.48" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_para" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1">In the initial stage of the survey on work-related stress, participation is optional, and all responses will be kept confidential. It is important to note that only those with work experience, whether part-time or full-time, will be able to accurately respond to certain questions. To complete the questionnaire, please use the following link, which is available in multiple languages.</span></span>
<span class="ltx_p" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text ltx_font_typewriter" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2.1">Frozen burgers are not inherently flawed, but the meat seasonings are a crucial element that elevates the flavor of the dish. Ground turkey can be quite bland without the right blend of spices, so it is essential to add onion powder, chili powder, salt, pepper, and cayenne pepper to the mixture. Additionally, garlic and onion should be used in the mayonnaise, which is necessary for the burger’s topping. To make the mayo, grate the garlic directly into the mixture, add a pinch of salt, and squeeze in lemon juice. Mix well. The mayonnaise is a delicious addition to the burger topping. Now, let’s work on the burgers. Heat a large skillet over medium-high heat with some olive oil, and preheat the broiler to high. Add the seasonings to the ground turkey and mix well. The burgers should be cooked on the skillet for about 5 minutes on each side, then transferred to the broiler to cook for an additional 2-3 minutes. The burgers are ready to be assembled on buns, along with swiss cheese, lettuce, and onion. Tomatoes can be added as a topping, but they may overpower other flavors. It is up to the individual to decide if they want to include them in their burger. In conclusion, frozen burgers can be tasty, but the meat seasonings are essential to add depth and flavor to the dish. The mayonnaise and toppings can also be customized to suit individual preferences.</span></span>
<span class="ltx_p" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.3"><span class="ltx_text ltx_font_typewriter" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.3.1">The luxurious and elegant nature of velvet cannot be denied, whether it is crushed, vibrant, or head-to-toe. This timeless textile is not only fashionable but also wearable for daily wear. In recent years, there has been a shift towards embracing fun gem-toned velvety pieces, breaking away from the traditional association of velvet with dark moody shades of navy and black. Below are some of the most desirable velvet pieces available on the high street today, and we are already envisioning outfits around them. Are you completely enamored with velvet or simply beyond bored with it?</span></span>
<span class="ltx_p" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.4"><span class="ltx_text ltx_font_typewriter" id="A8.SSx5.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.4.1">Discover savings up to $8,086 on one of 1,258 Chrysler 200s near you. Get expert and consumer car reviews, dealer reviews, car comparisons, and pricing tools from Edmunds. We have 4,850,420 listings for 2015 Chrysler 200 Sedan 4D 200C I4 prices, used values, and more. While we wrote about the stalling problem with the 2011 Chrysler 200, we still receive occasional calls regarding the issue. However, a more significant issue has taken over the phone lines recently - 2015 Chrysler 200 transmission problems that leave drivers with check engine lights, harsh shifting, and occasional loss of power. Despite the premium price for its style and horsepower, rear-seat room and handling are better bargains elsewhere. The 2015 Chrysler 200 is rated 8.4 by The Car Connection. If you’re looking for the perfect rims for your 2015 Chrysler 200 CARiD.com offers a vast selection of 2015 Chrysler 200 wheels in various design and finish options, including chrome, black, silver, and more</span></span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="A8.SSx6">
<h3 class="ltx_title ltx_title_subsection">Q/A Style</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A8.SSx6.p1">
<svg class="ltx_picture" height="770.45" id="A8.SSx6.p1.pic1" overflow="visible" version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,770.45) matrix(1 0 0 -1 0 0)"><g fill="#9F9F9F" fill-opacity="1.000000"><path d="M 0 0 L 0 764.55 C 0 767.81 2.64 770.45 5.91 770.45 L 600 770.45 L 600 5.91 C 600 2.64 597.36 0 594.09 0 Z" style="stroke:none"></path></g><g fill="#E6E6E6" fill-opacity="1.000000"><path d="M 1.97 1.97 L 1.97 764.55 C 1.97 766.72 3.73 768.48 5.91 768.48 L 598.03 768.48 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.000000" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject height="742.89" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69"><span class="ltx_inline-para ltx_minipage ltx_align_bottom" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_para" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1">
<span class="ltx_p" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.1.1">Question: Is participating in the stress at work survey mandatory? Answer: No, it is voluntary. Question: Will all answers be saved anonymously? Answer: Yes, they will. Question: Who is eligible to take the questionnaire? Answer: Only individuals with work experience, whether part-time or full-time. Question: What happens if someone is not eligible to take the questionnaire? Answer: They will not be able to answer some of the questions. Question: Can you provide a link to all language versions of the questionnaire? Answer: Yes, here is the link.</span></span>
<span class="ltx_p" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2"><span class="ltx_text ltx_font_typewriter" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.2.1">Question: What makes these frozen burgers so good? Answer: The meat seasonings are strong and spicy, and they give the burgers a good flavor. Question: Do you think ground turkey can be flavorless? Answer: Yes, ground turkey can be flavorless, but adding spices and seasonings can help. Question: What do you need to make these turkey burgers? Answer: Ground turkey, onion powder, chili powder, salt, pepper, and cayenne pepper for the burgers. Then the mayo takes garlic and onion. Buns, swiss cheese, lettuce, and onion are also needed. Question: What about toppings? Answer: Add tomatoes if you’d like to your array of toppings. Question: How do you make the mayo? Answer: Grate the garlic directly into the mayo, add a pinch of salt, and squeeze in the lemon juice. Stir. Done! Question: How do you work on the burgers? Answer: Preheat a large skillet to medium-high heat with some olive oil, preheat the broiler to high, then add all the spices to the ground turkey.</span></span>
<span class="ltx_p" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.3"><span class="ltx_text ltx_font_typewriter" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.3.1">Question: What do you think of velvet? Answer: Whether you like your velvet crushed, vibrant or head-to-toe, there’s really no denying the sheer luxe and elegance of this timeless textile. Question: Is velvet super stylish? Answer: Yes, really! This year it’s all about embracing fun gem-toned velvety pieces. Question: Was velvet solely associated with dark moody shades of navy and black? Answer: Long gone are the days when velvet was solely associated with dark moody shades of navy and black. Question: Do you have any covetable velvet pieces on the high street right now? Answer: Below we’ve rounded up the most covetable velvet pieces on the high street right now. Question: Are you completely obsessed or beyond bored of it? Answer: We’re already coming up with outfit ideas!</span></span>
<span class="ltx_p" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.4"><span class="ltx_text ltx_font_typewriter" id="A8.SSx6.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.p1.4.1">Question: How can I save money on a Chrysler 200? Answer: Save up to $8,086 on one of 1,258 Chrysler 200s near you. Find your perfect car with Edmunds expert and consumer car reviews, dealer reviews, car comparisons and pricing tools. Question: What are the problems with the 2015 Chrysler 200? Answer: We have 4,850,420. Research 2015 Chrysler 200 Sedan 4D 200C I4 prices, used values &amp; 200 Sedan 4D 200C I4 pricing, specs and more. Many years ago, we wrote about the stalling problem with the 2011 Chrysler 200, and believe it or not, we still receive an occasional call regarding the problem. However, a much larger issue has monopolized the phone lines as of late 2015 Chrysler 200 transmission problems leaving drivers with check engine lights, harsh shifting, and the occasional loss of power. Question: What are the benefits of buying a 2015 Chrysler 200? Answer: The 2015 Chrysler 200 can fetch a premium for its style and its horsepower--but rear-seat room and handling are better bargains elsewhere. Question: How is the 2015 Chrysler 200 rated? Answer: It’s rated 8.4 by The. Question: Where can I find the perfect rims for my 2015 Chrysler 200? Answer: CARiD.com stores a massive selection of 2015 Chrysler 200 wheels offered in myriads of design and finish options, including chrome, black, silver, and so much more.</span></span>
</span></span></foreignObject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
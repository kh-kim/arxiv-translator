<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Efficient and Effective Text Encoding for Chinese LLAMA and Alpaca\n' +
      '\n' +
      'Yiming Cui\n' +
      '\n' +
      'ymcui@ieee.org\n' +
      '\n' +
      'Equal contributions.\n' +
      '\n' +
      'Ziqing Yang\n' +
      '\n' +
      'ziqingyang@gmail.com\n' +
      '\n' +
      '&Xin Yao\n' +
      '\n' +
      'yaoxin94@foxmail.com\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'ChatGPT 및 GPT-4와 같은 대형 언어 모델(LLM)은 자연어 처리 연구를 극적으로 변화시켰고 인공 지능(AGI)에 대한 유망한 진전을 보여주었다. 그럼에도 불구하고 LLM 교육 및 배치와 관련된 높은 비용은 투명하고 접근 가능한 학술 연구에 상당한 장애물을 제공한다. LLaMA와 같은 여러 대형 언어 모델은 커뮤니티에 의해 공개되었지만 이들은 주로 영어 말뭉치에 초점을 맞추어 다른 언어에 대한 유용성을 제한한다. 본 논문에서는 중국어 텍스트를 이해하고 생성할 수 있는 능력과 명령어를 따를 수 있는 능력을 가진 LLaMA를 증강하는 방법을 제안한다. 우리는 LLaMA의 기존 어휘를 추가로 20,000개의 중국어 토큰으로 확장하여 중국어에 대한 인코딩 효율성과 의미론적 이해도를 향상시킴으로써 이를 달성한다. 중국어 데이터를 사용한 2차 사전 교육을 추가로 통합하고 중국어 명령어 데이터 세트로 모델을 미세 조정하여 모델의 명령어를 이해하고 실행하는 능력을 크게 향상시켰다. 실험 결과는 새롭게 제안된 모델이 LLaMA의 중국어 콘텐츠 이해 및 생성 능력을 현저하게 향상시킨다는 것을 나타낸다. 또한 C-Eval 데이터 세트에 대한 결과는 우리 모델의 몇 배 크기의 모델 사이에서 경쟁 성능을 산출한다. 우리는 미리 훈련된 모델, 훈련 스크립트 및 기타 리소스를 GitHub를 통해 사용할 수 있도록 하여 커뮤니티에 대한 개방형 연구를 육성했다.\n' +
      '\n' +
      '각주 1: GitHub 리포지토리: [https://github.com/ymcui/Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '자연어처리(Natural Language Processing, NLP) 분야는 대규모 언어모델(Large Language Model, LMM)의 출현과 함께 실질적인 패러다임의 변화를 목격하고 있다. 이 모델은 상당한 크기와 포괄적인 훈련 데이터로 구별되며 인간과 유사한 텍스트를 이해하고 생산하는 데 탁월한 능력을 보여주었다. BERT(Devlin et al., 2019), GPT 시리즈(Radford et al., 2018)와 같이 텍스트 이해에 전념하는 미리 훈련된 언어 모델과는 대조적으로, GPT 시리즈는 텍스트 생성을 강조하여, 상대방과 비교하여 창의성에 더 적합한 플랫폼으로 위치시킨다. 특히, GPT 가족의 최신 구성원인 ChatGPT와 GPT-4는 빠르게 진화하는 이 분야의 선도적인 사례로 자리매김하면서 상당한 관심을 받았다.\n' +
      '\n' +
      'InstructGPT(Ouyang et al., 2022)로부터 진화된 ChatGPT(OpenAI, 2022)는 상황 인식, 인간-유사 상호작용을 수행할 수 있는 진보된 대화형 AI 모델로서 기능한다. 그 성공은 보다 정교한 LLM인 GPT-4(OpenAI, 2023) 개발의 단계를 마련하여 특히 멀티모달 및 추론 능력으로 자연어 이해, 생성 및 다양한 NLP 작업에서 훨씬 더 큰 잠재력을 보여준다. 이러한 모델은 새로운 연구 방향과 응용 프로그램을 촉매하여 인공지능(AGI)의 잠재력을 탐구하는 데 대한 관심을 강화했다. 여러 벤치마크에 걸쳐 인상적인 성능을 발휘하면서 소수의 학습 능력과 새로운 작업에 대한 적응력을 입증하여 NLP 연구의 확장을 크게 주도했다. 결과적으로, 그들은 연구자 및 산업 전문가 모두에게 감성 분석, 기계 번역, 질문 응답 시스템 등을 포함한 광범위한 응용 프로그램에 걸쳐 잠재력을 더욱 활용하도록 영감을 주었다.\n' +
      '\n' +
      '그러나 LLM이 영향을 미쳤기 때문에 구현에는 투명하고 개방적인 연구를 방해하는 고유한 한계가 있다. 주요 관심사는 모델에 대한 접근을 제한하여 광범위한 연구 커뮤니티의 성공을 기반으로 하는 능력을 억제하는 독점적 특성이다. 또한 이러한 모델을 훈련하고 배치하는 데 필요한 방대한 계산 자원은 제한된 자원을 가진 연구자에게 어려움을 제공하여 접근성 문제를 더욱 복잡하게 만든다.\n' +
      '\n' +
      '이러한 한계를 해결하기 위해 NLP 연구 커뮤니티는 더 큰 투명성과 협력을 촉진하기 위해 오픈 소스 대안에 관심을 기울였다. LLaMA(Touvron et al., 2023) 및 알파카(Taori et al., 2023)는 이러한 이니셔티브의 주목할 만한 예로서 작용한다. 이러한 오픈 소스 LLM은 학술 연구를 촉진하고 NLP 분야 내에서 진전을 가속화하기 위한 것이다. 이러한 모델을 오픈소싱하는 목적은 모델 개발, 미세 조정 및 평가의 추가 발전에 도움이 되는 환경을 육성하여 궁극적으로 다양한 용도에 적용할 수 있는 강력하고 유능한 LLM을 만드는 것이다.\n' +
      '\n' +
      'NLP에서 LLaMA와 알파카가 상당히 발전했음에도 불구하고 중국어 작업에 대한 토착 지원과 관련하여 고유한 한계를 나타낸다. 그들의 어휘는 몇 백 개의 중국어 토큰만을 포함하고 있어 중국어 텍스트를 인코딩하고 디코딩하는 데 있어서 효율성을 실질적으로 저해한다. 본 기술 보고서에서는 중국어 BERT 시리즈(Cui et al., 2021)와 중국어 소수자 중심의 다국어 사전 훈련 모델(Yang et al., 2022)에 대한 이전 작업을 기반으로 중국어 콘텐츠를 이해하고 생성하는 기능이 강화된 중국어 LLaMA 및 알파카 모델 개발을 제안한다. 원본 LLaMA의 어휘를 추가 2만 개의 중국어 토큰으로 확장하여 중국어 텍스트 처리 및 생성 능력을 크게 향상시켰습니다. 이러한 모델의 효율적인 훈련 및 배치를 보장하기 위해 LoRA(Low-Rank Adaptation) 접근법(Hu et al., 2021)을 사용하여 과도한 계산 비용 없이 모델을 훈련하고 미세 조정할 수 있다. 우리는 LLaMA와 알파카의 중국 이해 및 생성 능력을 향상시키기 위한 예비 연구가 연구자들이 이러한 모델을 다른 언어에 적용하는 것을 목표로 하는 기초 역할을 할 것으로 기대한다. 제안된 방법의 타당성과 효율성을 보여줌으로써 다양한 언어로 어휘를 확장하고 LLaMA 및 알파카 모델의 성능을 향상시키는 데 사용할 수 있는 통찰력과 방법론을 제공한다. 요약하면, 본 기술 보고서의 기여도는 다음과 같다:\n' +
      '\n' +
      '* 중국어의 인코딩 및 디코딩 효율을 높이고, 추가로 20,000개의 중국어 토큰으로 원래의 LLaMA의 어휘를 확장함으로써 LLaMA의 중국어 이해 능력을 향상시킨다.\n' +
      '* 중국 LLaMA 및 Alpaca 모델의 효율적인 훈련 및 배포를 용이하게 하기 위해 Low-Rank Adaptation (LoRA) 접근법을 사용 하 여 연구자가 과도한 계산 비용을 발생 하지 않고 이러한 모델로 작업할 수 있습니다.\n' +
      '* 명령어 추종 태스크 및 자연어 이해 태스크에서 제안된 LLaMA 및 알파카 모델의 성능을 평가하여, 중국어 태스크의 맥락에서 원래 대응물에 비해 상당한 개선을 입증한다.\n' +
      '* 연구의 리소스 및 결과를 공개적으로 사용 하도록 설정 하 여 NLP 커뮤니티에서 추가 연구 및 협력을 촉진하고 LLaMA 및 Alpaca 모델을 다른 언어에 적용 하도록 권장 합니다.\n' +
      '\n' +
      '## 2 Chinese LLaMA and Chinese Alpaca\n' +
      '\n' +
      '### LLaMA\n' +
      '\n' +
      'LLaMA(Touvron et al., 2023)는 트랜스포머 아키텍처(Vaswani et al., 2017)를 기반으로 구축된 기초적, 디코더 전용 대형 언어 모델이다. GPT 시리즈 및 다른 변압기 기반 LLM과 유사하게 LLaMA는 임베딩 계층, 다중 변압기 블록 및 언어 모델 헤드로 구성된다. LLaMA는 또한 사전 정규화(Zhang and Sennrich, 2019), SwiGLU 활성화(Shazeer, 2020) 및 회전 임베딩(Su et al., 2021)과 같은 다양한 모델에서 활용되는 개선을 통합한다. LLaMA는 7B, 13B, 33B 및 65B의 4가지 다른 모델 크기로 사용할 수 있다.\n' +
      '\n' +
      'LLaMA는 크롤링된 웹 페이지, 책, 위키피디아 및 사전 인쇄 용지와 같은 공개적으로 이용 가능한 소스의 혼합물을 사용하여 표준 언어 모델링 작업(섹션 2.4 참조)으로 사전 훈련되었다. 실험 결과는 LLaMA가 더 작은 모델 크기이지만 GPT-3와 같은 다른 LLM에 비해 경쟁력 있는 성능을 제공한다는 것을 보여준다. 이러한 조밀성과 효과는 연구자들의 상당한 관심을 받아 LLaMA 기반 모델의 광범위한 사용으로 이어졌다.\n' +
      '\n' +
      '### 중국어 어휘 확장\n' +
      '\n' +
      'LLaMA의 트레이닝 세트는 대략 1.4T 토큰을 포함하며, 대다수는 영어이고 다른 유럽 언어는 라틴어 또는 키릴어 스크립트를 사용한다(Touvron et al., 2023). 따라서 LLaMA는 대부분 유럽 언어로 입증된 다국어 및 교차 언어 이해 능력을 가지고 있다. 흥미로운 사실은 우리의 이전 예비 연구는 LLaMA가 중국 텍스트를 생성하는 능력은 제한적이지만 기본적인 중국 이해 능력을 나타낸다는 것을 보여준다.\n' +
      '\n' +
      'LLaMA에 향상된 중국어 이해와 생성 능력을 갖추기 위해, 우리는 LLaMA 모델을 중국어 코포라와 함께 사전 훈련을 계속할 것을 제안한다. 그러나 중국어 코포라와 함께 지속적인 사전 교육을 직접 적용하는 것은 몇 가지 어려움을 겪는다. 첫째, 원본 LLaMA 어휘는 1,000자 미만의 한자를 포함하고 있어 일반 한문을 인코딩하기에는 부족하다. LLaMA 토큰화기는 알려지지 않은 UTF-8 문자를 바이트로 토큰화하여 이 문제를 우회하지만, 이 전략은 각 한자가 3-4바이트 토큰으로 분할됨에 따라 시퀀스 길이를 크게 확장하고 중국 텍스트의 인코딩 및 디코딩 효율성을 늦춘다. 둘째, 바이트 토큰은 한자를 나타내기 위해 독점적으로 설계되지 않습니다. 바이트 토큰은 또한 다른 언어로 UTF-8 토큰을 의미하기 때문에 바이트 토큰과 트랜스포머 인코더는 한자의 의미적 의미를 포착하는 표현을 효과적으로 학습하는 것이 어려워진다.\n' +
      '\n' +
      '이러한 문제점을 해결하고 인코딩 효율을 향상시키기 위해, 추가적인 중국어 토큰으로 LLaMA 어휘를 확장하고 확장 어휘에 대한 모델을 적응시키는 것을 제안한다(Yang et al., 2022). 확장 프로세스는 다음과 같이 진행된다:\n' +
      '\n' +
      '* 중국 텍스트에 대한 토큰라이저의 지원을 강화하기 위해 처음에는 단어 크기가 20,000인 중국 말뭉치 2에서 SentencePiece(Kudo and Richardson, 2018)를 사용하여 중국 토큰라이저를 학습합니다. 각주 2: 학습 데이터는 기본 버전의 모델을 학습하는 데이터와 동일합니다.\n' +
      '* 나중에 중국 토큰라이저를 해당 어휘의 결합을 사용하여 원래 LLaMA 토큰라이저로 병합합니다. 결과적으로, 어휘 크기가 49,953인 중국어 LLaMA 토큰라이저라고 하는 병합 토큰라이저를 얻는다.\n' +
      '* 중국어 LLaMA 토큰라이저를 위한 LLaMA 모델을 적용하기 위해 단어 임베딩과 언어 모델 헤드를 모양 \\(V\\times H\\)에서 \\(V^{\\prime}\\times H\\)으로 크기를 조정한다. 여기서 \\(V=32,000\\)은 원래 어휘 크기를 나타내고 \\(V^{\\prime}=49,953\\)은 중국어 LLaMA 토큰라이저의 새로운 어휘 크기를 나타낸다. 새로운 행들은 원래의 임베딩 행렬들의 끝에 부가되어, 원래의 어휘에서의 토큰들의 임베딩들이 영향을 받지 않고 유지되도록 보장한다.\n' +
      '\n' +
      '예비 실험에 따르면 중국 LLaMA 토큰화기에서 생성된 토큰의 수는 원래 LLaMA 토큰화기에서 생성된 토큰의 약 절반이다. 표 1은 원래 LLaMA 토큰화기와 우리의 중국 LLaMA 토큰화기를 비교한 것이다. 도시된 바와 같이, 중국 LLaMA 토큰라이저는 원본에 비해 인코딩 길이를 상당히 감소시킨다. 고정 컨텍스트 길이를 사용하면 모델은 약 2배의 정보를 수용할 수 있으며 생성 속도는 원래 LLaMA 토큰라이저보다 2배 빠르다. 이는 LLaMA 모델의 중국 이해 및 생성 능력을 향상시키는 데 있어 제안된 접근법의 효율성을 강조한다.\n' +
      '\n' +
      '### LoRA를 사용 하 여 매개 변수 효율적인 미세 조정\n' +
      '\n' +
      'LLM의 전체 파라미터를 업데이트하는 기존의 훈련 패러다임은 엄청나게 비싸고 대부분의 실험실이나 회사에는 시간이나 비용이 들지 않는다. Low-Rank Adaptation(LoRA)(Hu 등, 2021)은 훈련 가능한 랭크 분해 행렬을 도입하면서 미리 훈련된 모델 가중치를 유지하는 파라미터 효율적인 훈련 방법이다. LoRA는 미리 훈련된 모델 가중치를 동결하고 훈련 가능한 낮은 순위 행렬을 각 레이어에 주입한다. 이 접근법은 총 훈련 가능한 파라미터를 상당히 감소시켜 훨씬 적은 계산 리소스로 LLM을 훈련하는 것을 가능하게 한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:4]\n' +
      '\n' +
      '아래는 작업을 설명하는 지침입니다. 요청을 적절하게 완료하는 응답을 작성합니다.\n' +
      '\n' +
      '_### 명령: {instruction}_\n' +
      '\n' +
      '_### 응답: {output}_\n' +
      '\n' +
      '손실은 입력 시퀀스의 {_output_} 부분에서만 계산되며 다음과 같이 표현될 수 있다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{\\text{SFT}}(\\Theta)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}_{\\text{SFT}}} \\left[-\\sum_{i\\in\\{output\\}\\log p(x_{i}|x_{0},x_{1},\\ldots,x_{i-1};\\Theta)\\right] \\tag{3}\\]\n' +
      '\n' +
      '여기서, \\(\\Theta\\)은 모델 파라미터를 나타내고, \\(\\mathcal{D}_{\\text{SFT}}\\)은 미세 조정 데이터세트, \\(\\mathbf{x}=(x_{0},x_{1},\\ldots)\\)은 토큰화된 입력 시퀀스를 나타낸다.\n' +
      '\n' +
      '우리의 접근 방식과 Stanford Alpaca의 주요 차이점은 _입력_ 필드가 없는 예제에 대해 설계된 프롬프트 템플릿만 사용하는 반면 Stanford Alpaca는 _입력_ 필드가 있거나 없는 예제에 대해 두 개의 템플릿을 사용한다는 것입니다. 예제에 비어 있지 않은 _입력_ 필드가 포함 된 경우 _명령어_ 및 _입력_ 을 "\\(\\backslash n\\)" 와 연결 하 여 새 명령을 형성 합니다. 중국 알파카 모델에 대한 추가 패딩 토큰이 있어 어휘 크기가 49,954입니다.\n' +
      '\n' +
      '## 3 실험 설정\n' +
      '\n' +
      '### 사전 교육을 위한 실험 설정\n' +
      '\n' +
      '원래 LLaMA 가중치로 중국 LLaMA 모델을 초기화하고 7B 및 13B 모델에 fp16을 사용하여 사전 훈련을 수행한다. 또한, 33B 모델의 경우 비트 및 바이트3 라이브러리를 사용하여 8비트 형식으로 학습하여 효율성과 메모리 사용량을 향상시켰다. 임베딩과 LM 헤드를 트레이닝 가능으로 설정하면서 학습을 위한 주의사항과 MLP에 LoRA를 직접 적용한다.\n' +
      '\n' +
      '각주 3: [https://github.com/TimDetmers/bitsandbytes](https://github.com/TimDetmers/bitsandbytes)\n' +
      '\n' +
      '중국 LLaMA-7B의 기본 버전의 경우 2단계 사전 훈련 접근법을 활용한다. 단계 1에서는 모델 내에서 트랜스포머 인코더의 파라미터를 고정하고 임베딩만을 훈련시켜 외란을 최소화하면서 새로 추가된 중국어 단어 벡터를 원래 모델에 적응시킨다. 2단계에서는 어텐션 메커니즘에 LoRA 가중치(어댑터)를 추가하고 임베딩, LM 헤드 및 새로 추가된 LoRA 매개변수를 훈련한다. 2단계 훈련은 예비 연구에서 효율성이 낮기 때문에 다른 모델 훈련에는 적용되지 않는다.\n' +
      '\n' +
      '다른 중국어 LLaMA 모델(기본 버전)의 경우, 사전 훈련을 위해 20GB 일반 중국어 코퍼스를 활용하는데, 이는 중국어 BERT-wurm(Cui et al., 2021), MacBERT(Cui et al., 2020), LERT(Cui et al., 2022) 등이 사용하는 코퍼스와 일치한다. 또한 사전 학습 데이터를 120GB로 확장하여 커먼크롤(CC)과 백과사전 소스의 추가 데이터를 통합하여 기본 개념에 대한 모델의 이해도를 높이는 "플러스" 버전을 제공한다. 우리는 사전 훈련을 위해 모든 데이터 세트와 생성된 블록 크기 512의 청크를 연결한다.\n' +
      '\n' +
      '모델은 A40 GPU(48GB VRAM)에서 한 에포크 동안 학습되어 모델 크기에 따라 최대 48개의 GPU를 차지한다. LoRA를 이용한 파라미터 효율적인 학습은 PEFT library4를 이용하여 수행한다. 또한 DeepSpeed(Rasley et al., 2020)를 활용하여 학습 과정에서 메모리 효율을 최적화한다. 본 논문에서는 AdamW 옵티마이저(Loshchilov and Hutter, 2019)를 이용하여 2e-4의 최대 학습률과 5%의 워밍업 코사인 스케줄러를 사용하였다. 또한, 전위 구배 폭발을 완화하기 위해 1.0의 값을 갖는 구배 클리핑을 적용한다.\n' +
      '\n' +
      '각주 4: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)\n' +
      '\n' +
      '각 중국 LLaMA 모델에 대한 자세한 하이퍼파라미터는 표 2에 나열되어 있다.\n' +
      '\n' +
      '### 명령 미세 조정을 위한 실험 설정\n' +
      '\n' +
      '중국 LLaMA 모델을 얻은 후 섹션 2.5에 따라 미세 조정한다. 기본 모델의 모든 선형 레이어에 LoRA 모듈을 추가하여 효율적인 미세 조정을 위해 LoRA를 계속 사용한다. 기본 모델을 튜닝하기 위해 번역(Xu, 2019)(550K 샘플링), pCLUE5(250K 샘플링, "NLU 유사" 데이터를 제외), 스탠포드 알파카(원본 및 번역 데이터의 경우 50K+50K), 크롤링된 SFT 데이터를 포함하여 약 2M에서 3M 명령어 데이터를 활용한다. 플러스 버전에서는 STEM(Science, Technology, Engineering and Mathematics) 데이터와 물리학, 화학, 생물학, 의학, 지구 과학과 같은 여러 과학 분야를 통합하는 데 중점을 두고 데이터 세트를 약 4M에서 4.3M으로 확장한다. Alpaca-33B의 경우, OASST1 데이터세트(Kopf 등, 2023)를 추가로 추가하는데, 여기서 우리는 각 대화에서 첫 번째 쿼리-응답 쌍만을 추출하고 qpt-3.5-터보 API를 사용하여 번역하여 대략 20K 데이터(원본 및 번역된 데이터)를 생성한다. 최대 시퀀스 길이를 512로 설정하고 배치에서 최대 길이로 배치할 때 샘플을 동적으로 패딩한다.\n' +
      '\n' +
      '각주 5: [https://github.com/CLUEbenchmark/pCLUE](https://github.com/CLUEbenchmark/pCLUE)\n' +
      '\n' +
      '크롤링된 데이터는 Taori 등(2023a)에서 사용된 바와 같이 ChatGPT(gpt-3.5-turbo API)로부터 데이터를 자동으로 획득하기 위한 자체 지시(Wang 등, 2022) 방법을 참조한다. 구체적으로, 타겟 도메인 및 명령어 유형에 대한 요구 사항만 있는 시드 작업이 필요하지 않은 보다 단순화된 템플릿을 활용한다. 템플릿 및 코드 세부 정보는 GitHub.6에서 사용할 수 있습니다.\n' +
      '\n' +
      '각주 6: [https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/crawl_prompt.py)\n' +
      '\n' +
      '플러스 버전의 경우 기본 버전에 비해 더 큰 LoRA 순위를 사용합니다. 학습률 및 배치 크기를 조정하는 것 외에도 사전 훈련 단계에서 사용된 다른 하이퍼파라미터 및 설정과 일관성을 유지한다.\n' +
      '\n' +
      '명령어 미세조정을 위한 하이퍼파라미터는 표 3에 나열되어 있다. 모든 알파카 모델은 각각의 LLaMA 모델에 기초하여 트레이닝된다는 점에 유의한다. 예를 들어, 중국어 알파카-플러스-13B는 중국어 LLaMA-Plus-13B를 기반으로 훈련된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Settings** & **7B** & **Plus-7B** & **13B** & **Plus-13B** & **33B** \\\\ \\hline Training data & 20 GB & 120 GB & 20 GB & 120 GB & 20 GB \\\\ Batch size & 1,024 & 2,304 & 2,304 & 2,304 & 2,304 \\\\ Peak learning rate & 2e-4/1e-4 & 2e-4 & 2e-4 & 2e-4 & 2e-4 \\\\ Max sequence length & 512 & 512 & 512 & 512 & 512 \\\\ LoRA rank & -/8 & 8 & 8 & 8 & 8 \\\\ LoRA alpha & -/32 & 32 & 32 & 32 & 32 \\\\ LoRA weights & -/QKVO & QKVO, MLP & QKVO, MLP & QKVO, MLP & QKVO, MLP \\\\ Trainable params (\\%) & 2.97\\%/6.06\\% & 6.22\\% & 4.10\\% & 4.10\\% & 2.21\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: **중국 LLaMA에 대 한 하이퍼 매개 변수 사전 훈련** QKVO: 각 주의 모듈의 4개 행렬, 즉 쿼리, 키, 값 및 출력입니다. MLP: 각 MLP 계층에서 세 개의 매트릭스. 7B는 2단계 트레이닝 패러다임(설정은 ’/’에 의해 분리됨)을 사용하는데, 이는 다른 모델들에서는 더 이상 채택되지 않는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline\n' +
      '**Settings** & **7B** & **Plus-7B** & **13B** & **Plus-13B** & **33B** \\\\ \\hline Training data & 2M & 4M & 3M & 4.3M & 4.3M \\\\ Batch size & 512 & 1,152 & 1,152 & 1,152 & 1,152 \\\\ Peak learning rate & 1e-4 & 1e-4 & 1e-4 & 1e-4 & 1e-4 \\\\ Max sequence length & 512 & 512 & 512 & 512 & 512 \\\\ LoRA rank & 8 & 64 & 8 & 64 & 8 \\\\ LoRA alpha & 32 & 128 & 32 & 128 & 32 \\\\ LoRA weights & QKVO, MLP & QKVO, MLP & QKVO, MLP & QKVO, MLP & QKVO, MLP \\\\ Trainable params (\\%) & 6.22\\% & 8.08\\% & 4.10\\% & 5.66\\% & 2.21\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 중국 알파카에 대한 하이퍼파라미터 미세 조정 지시.\n' +
      '\n' +
      '## 4 결과 명령어 추적 작업\n' +
      '\n' +
      '### 작업 설계 및 평가 방법\n' +
      '\n' +
      '텍스트 생성 과제의 성능을 평가하는 것은 그 형태가 매우 다양하기 때문에 어려울 수 있으며, 텍스트 분류 및 추출 기계 독해와 같은 자연어 이해 과제와 크게 다르다. 채점 방법으로 GPT-4(OpenAI, 2023)를 활용하는 이전 작업에 이어 GPT-4를 채택하여 각 샘플에 대한 전체 점수(10점 척도)를 제공하여 인간 평가보다 효율적이다. 그러나 GPT-4는 항상 정확한 점수를 제공하지 않을 수 있으므로 등급에 대해 수동 검사를 수행하고 필요한 경우 조정한다. 수동 검사는 점수가 일관되고 평가되는 모델의 실제 성능을 반영하는지 확인합니다. 시스템의 두 출력(여러 시스템으로 조정할 수 있음)을 채점하기 위해 다음 프롬프트 템플릿을 사용합니다.\n' +
      '\n' +
      '_다음은 ChatGPT와 유사한 두 시스템의 출력입니다. 각 점수에 대해 전체 점수를 10점 척도로 평가하고 점수를 정당화하기 위해 설명을 해주십시오.\n' +
      '\n' +
      '_Prompt:_\n' +
      '\n' +
      '_{prompt-input_}\n' +
      '\n' +
      '_System1:_\n' +
      '\n' +
      '_{system1-output_}\n' +
      '\n' +
      '_System2:_\n' +
      '\n' +
      '_{system2-output_}\n' +
      '\n' +
      '수작업 검사와 함께 GPT-4를 채점 방법으로 사용하여 다양한 자연어 이해 및 생성 작업에 대한 중국 알파카 모델의 성능을 효과적으로 측정하는 신뢰할 수 있는 평가 프레임워크를 구축한다.\n' +
      '\n' +
      '우리의 평가 세트는 광범위한 자연어 이해 및 생성 작업에 걸쳐 중국 알파카 모델을 종합적으로 평가하도록 설계되었습니다. 이 세트는 질문 응답, 추론, 문학, 엔터테인먼트, 번역, 다중 전환 대화, 코딩 및 윤리 등을 포함한 10개의 별개의 작업을 포함하는 200개의 샘플로 구성된다. 특정 작업에 대한 전체 점수는 해당 작업 내의 모든 샘플에 대한 점수를 합산하고 총점을 100점 척도로 정규화하여 계산된다. 이 접근 방식은 평가 집합이 다양한 작업에 걸쳐 모델의 기능을 반영하도록 보장하여 균형 있고 강력한 성능 측정을 제공합니다.\n' +
      '\n' +
      '### 디코딩에 대한 실험 설정\n' +
      '\n' +
      'LLM의 디코딩 과정은 생성된 텍스트의 품질과 다양성을 결정하는 데 중요한 역할을 한다. 우리의 실험에서, 우리는 다음의 디코딩 하이퍼파라미터들을 사용한다:\n' +
      '\n' +
      '* 컨텍스트 크기: 컨텍스트 크기를 2048로 설정 하 여 모델이 텍스트를 생성할 때 동시에 고려할 수 있는 최대 토큰 수를 결정 합니다.\n' +
      '* 최대 시퀀스 길이: 생성 된 시퀀스 길이를 512 토큰으로 제한 하 여 출력에 포커스가 유지 되 고 입력 프롬프트와 관련이 있는지 확인 합니다.\n' +
      '* 온도: 샘플링 과정의 랜덤성을 제어하는 온도를 0.2로 설정하였다. 낮은 값은 모델이 더 집중적이고 결정론적 출력을 생성하는 반면, 높은 값은 일관성의 비용으로 다양성을 증가시킨다. 멀티턴 대화 및 생성 작업의 경우 온도를 0.5로 약간 조정하여 보다 다양한 출력을 허용합니다.\n' +
      '* Top-\\(k\\) 샘플링: \\(k=40\\)을 사용 하 여 Top-\\(k\\) 샘플링을 사용 합니다. 즉, 모델이 각 단계에서 가장 가능성이 높은 상위 40개 토큰에서 다음 토큰을 선택 하 여 생성 된 텍스트에 무작위성과 다양성의 요소를 추가 합니다.\n' +
      '* Top-\\(p\\) 샘플링: \\(p=0.9\\)을 사용 하 여 Top-\\(p\\) 샘플링을 사용 하 여 확률 질량의 90%를 집합적으로 설명 하는 동적 토큰 집합을 고려 하 여 다양성을 더욱 향상 합니다.\n' +
      '* 반복 패널티: 모델이 반복 텍스트를 생성 하지 않도록 하려면 이미 선택 된 토큰에 벌점을 부여 하는 1.1 인수의 반복 패널티를 적용 합니다.\n' +
      '\n' +
      '이러한 값은 각 테스트 시나리오에 대해 최적이 아닐 수 있습니다. 균형 잡힌 뷰를 유지하기 위해 각 작업에 대해 이러한 하이퍼파라미터에 대한 추가 튜닝을 수행하지 않았다.\n' +
      '\n' +
      '### Results\n' +
      '\n' +
      '중국 Alpaca-Plus-7B, Alpaca-Plus-13B 및 Alpaca-33B 모델에서 얻은 결과를 제시하고 분석한다. Alpaca-33B 결과는 원본 모델(FP16)에 의해 생성되는 반면, Alpaca-Plus-7B 및 Alpaca-Plus-13B는 8비트 양자화된 버전을 채택한다. 전체 결과는 표 4에 나와 있다. 평가는 총 200개의 샘플을 포함하는 10개의 별개의 NLP 작업에 걸쳐 GPT-4 등급 결과를 기반으로 한다. 제시된 점수는 서로 단독으로 비교할 수 있지만 시스템을 다시 조정해야 하는 다른 모델과는 비교할 수 없다는 점에 유의하는 것이 중요하다. 또한 우리의 모델은 원래 LLaMA를 기반으로 하기 때문에 이러한 관찰은 처음부터 훈련하기보다는 잘 확립된 모델을 기반으로 할 때 더 나은 성능을 달성하는 데 중요한 측면으로 간주할 수 있다. 우리는 몇 가지 주요 범주의 발견에 대해 자세히 설명한다.\n' +
      '\n' +
      '각주 7: 우리는 6절에서 양자화 효과에 대해 논의할 것이다.\n' +
      '\n' +
      '#### 4.3.1 다중 회전 대화\n' +
      '\n' +
      '챗GPT의 인상적인 성과 중 하나는 멀티턴 대화 인터페이스로 전달되는 풍부하고 유창한 맥락 이해 능력이다. 우리가 볼 수 있듯이 플러스 시리즈 모델은 기본 모델보다 일관된 개선을 산출하지만 후자의 크기는 형성기의 몇 배이다. 이것은 더 나은 대화 경험을 달성하기 위해 모델의 파라미터 크기를 단순히 확장하는 것보다 더 많은 트레이닝 데이터를 수집하는 것이 훨씬 더 중요하다는 것을 나타낼 수 있다. 특히 언어 지식을 직접 전달할 수 없는 원래의 LLaMA를 이용하여 모델을 구성하였다.\n' +
      '\n' +
      '#### 4.3.2 텍스트 생성\n' +
      '\n' +
      '텍스트 생성은 언어 모델의 가장 기본적인 능력 중 하나이다. 알파카-플러스-7B 및 알파카-플러스-13B와 비교하여 알파카-33B는 이 범주에서 열등한 결과를 보여준다. 표 5는 텍스트 생성 태스크의 일 예를 나타낸다. 알파카-플러스-7B와 알파카-플러스-13B는 모두 사용자의 프롬프트에 대한 요구 사항을 충족하는 올바른 문자 스타일을 제공한다는 것을 알 수 있다. 알파카플러스-13B는 신청자가 비자 신청을 위해 모든 재료를 철저히 준비했음을 표시해 가장 포괄적인 것을 제공하며, 세 시스템 중 최고의 세대 품질이다. 그러나, 알파카-33B는 글자 스타일을 따르지 않고, 내용이 다소 지나치게 단순화되어 다른 것들보다 분명히 좋지 않다. 이는 더 작은 모델을 가진 더 많은 데이터를 사용한 훈련이 더 적은 데이터를 가진 빅 모델보다 더 나은 성능을 제공할 수 있음을 보여준다.\n' +
      '\n' +
      '#### 4.3.3 수치 계산 및 추론\n' +
      '\n' +
      '수치 추론은 대용량 언어 모델의 추론 능력을 검토하는 데 있어 가장 중요한 작업 중 하나로 간주되어 왔다. 우리가 볼 수 있듯이 알파카-33B는 플러스-7B 및 플러스-13B 모델에 비해 상당한 개선을 달성한다. 표 6은 이 작업에 대한 예제 출력을 보여줍니다. 첫 번째 프롬프트\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline\n' +
      '**Task** & **Alpaca-Plus-7B** & **Alpaca-Plus-13B** & **Alpaca-33B** \\\\ \\hline Question Answering & 70.5 & 79.5 & **82.3** \\\\ Open-ended QA & **80.5** & 80.0 & 78.5 \\\\ Numerical Reasoning & 51.0 & 61.5 & **84.5** \\\\ Poetry, Literature, Philosophy & 78.5 & **81.3** & 76.0 \\\\ Music, Sports, Entertainment & 72.3 & **76.8** & 72.5 \\\\ Letters and Articles Writing & 81.0 & **86.5** & 79.0 \\\\ Translation & 86.8 & 89.3 & **92.3** \\\\ Multi-turn Dialogue & 80.3 & **81.3** & 78.0 \\\\ Coding & 62.5 & 67.5 & **84.0** \\\\ Ethics & 89.8 & 90.5 & **92.5** \\\\ \\hline\n' +
      '**Total** & 75.3 & 79.4 & **82.0** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: **중국 Alpaca-Plus-7B 및 Alpaca-Plus-13B 및 Alpaca-33B에 대한 GPT-4 등급 결과** 입니다. 결과는 이 모델 조합 내에서만 비교할 수 있습니다.**\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:9]\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:10]\n' +
      '\n' +
      '## 5 결과 자연어 이해 작업\n' +
      '\n' +
      '### Task Description\n' +
      '\n' +
      '또한 명령어 추적 태스크에 대한 생성 성능 테스트 외에도 다중 선택 질문 응답 데이터 세트인 C-Eval 데이터 세트(Huang et al., 2023)에 대해 모델을 테스트했다. C-Eval은 주로 STEM, 사회, 인문 및 기타의 4가지 범주를 다루며 52개 분야에 대한 거의 I4K 샘플로 구성된다. RACE(Lai et al., 2017)와 같은 다른 다중 선택 QA 데이터 세트와 유사하게, 주어진 질문에 기초하여 올바른 옵션 라벨을 생성하는 모델이 필요하다. 검증 분할(1,346개 샘플)과 테스트 분할(12,342개 샘플)에 대해 주로 모델을 테스트했으며, 여기서 모델의 예측 파일을 공식 리더보드에 제출하여 테스트 점수를 얻는다.\n' +
      '\n' +
      '### Decoding Strategy\n' +
      '\n' +
      '이 데이터 세트에서 LLaMA 모델을 평가하기 위해 이러한 모델에 예제를 직접 공급한다. 알파카 모델을 평가할 때 섹션 2.5에서 설명한 대로 프롬프트 템플릿에 예제를 포장한 다음 모델을 1단계 예측을 수행하고 확률을 제공하도록 요청한다.\n' +
      '\n' +
      '그림 1: **코딩 작업에 대한 예제 출력.** 33B 모델이 Alpaca-Plus-7B 및 Alpaca-Plus-13B보다 훨씬 성능이 뛰어납니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:12]\n' +
      '\n' +
      '**LLaMA는 일반적으로 몇 개의 샷 설정에서 더 나은 성능을 산출 하는 반면 Alpaca는 제로 샷을 선호 합니다.* * 일반적으로 5 개의 샷 설정이 있는 LLaMA는 제로 샷 설정보다 더 나은 성능을 나타내는 반면 0 개의 샷 설정이 있는 Alpaca는 5 개의 샷 설정보다 훨씬 더 좋습니다. LLaMA는 명령어 추종을 위해 설계되지 않았기 때문에 C-Eval에서 질문 응답 구조를 따르는 방법에 대한 귀중한 정보를 제공할 수 있는 샷 설정이 거의 없다. 그러나 반대로 알파카는 이미 수백만 개의 지시 데이터로 훈련된 만큼 추가 샷의 혜택을 받을 가능성이 적다. 또한 공식 5샷 설정은 모든 샘플에 동일한 프롬프트를 사용하여 알파카 모델의 주의를 분산시킵니다.\n' +
      '\n' +
      '이러한 관찰은 C-Eval 데이터 세트의 결과만을 기반으로 하며 다른 데이터 세트에 일반화할 수 있는지 여부는 추가 조사가 필요하다는 점을 강조하고 싶다. 향후 LLaMA 및 알파카 모델의 행동을 추가로 조사하기 위해 보다 포괄적인 테스트를 포함할 것이다.\n' +
      '\n' +
      '그림 2: **C-Eval 유효 집합에 대 한 결과** 입니다. 결과는 서로 다른 설정(제로샷 및 5샷) 및 모델 크기(7B 및 13B)별로 그룹화됩니다.**\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{2}{c}{**Valid Set**} & \\multicolumn{2}{c}{**Test Set**} \\\\  & **Zero-shot** & **5-shot** & **Zero-shot** & **5-shot** \\\\ \\hline _Random_ & _25.0_ & _25.0_ & _25.0_ & _25.0_ \\\\ \\hline LLaMA-65B & 37.2 & 41.2 & 33.4 & 38.8 \\\\ LLaMA-33B & 34.5 & 37.9 & 32.4 & 36.0 \\\\ LLaMA-13B & 27.8 & 30.9 & 28.5 & 29.6 \\\\ LLaMA-7B & 25.6 & 25.3 & 26.7 & 27.8 \\\\ \\hline Chinese-LLaMA-33B & 34.9 & 38.4 & 34.6 & 39.5 \\\\ Chinese-LLaMA-Plus-13B & 27.3 & 34.0 & 27.8 & 33.3 \\\\ Chinese-LLaMA-13B & 29.4 & 35.0 & 29.2 & 33.7 \\\\ Chinese-LLaMA-Plus-7B & 27.3 & 28.3 & 26.8 & 28.4 \\\\ Chinese-LLaMA-7B & 26.2 & 26.2 & 27.1 & 27.2 \\\\ \\hline Chinese-Alpaca-33B & 43.3 & 42.6 & 41.6 & 40.4 \\\\ Chinese-Alpaca-Plus-13B & 43.3 & 42.4 & 41.5 & 39.9 \\\\ Chinese-Alpaca-13B & 37.1 & 36.3 & 36.7 & 34.5 \\\\ Chinese-Alpaca-Plus-7B & 36.7 & 32.9 & 36.4 & 32.3 \\\\ Chinese-Alpaca-7B & 30.8 & 32.5 & 30.7 & 29.2 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: **C-Eval 유효 및 테스트 세트에 대한 결과**. 모든 예측 파일은 자체적으로 생성됩니다. 테스트 세트 점수는 예측 파일을 C-Eval 리더보드에 제출 하 여 얻습니다. **\n' +
      '\n' +
      '### 다른 모델과의 비교\n' +
      '\n' +
      '우리는 C-Eval 리더보드에 두 가지 가장 성능이 좋은 모델, 즉 중국-알파카-33B와 중국-알파카-플러스-13B를 포함하여 오픈 소스 및 비 오픈 소스 모델을 포함한 다른 LLM과 비교한다. C-Eval 리더보드(2023년 6월 9일 기준)에 대한 시험 결과는 표 9와 같다.\n' +
      '\n' +
      '당연히, 비 오픈 소스 LLM은 오픈 소스 LLM보다 훨씬 더 나은 성능을 가지고 있다. 모델의 경우, 중국-Alpaca-33B와 중국-Alpaca-Plus-13B가 모두 이 리더보드에서 오픈 소스 LLM 간의 경쟁 성능을 산출하여 Bloomz-mt-176B(Scao et al., 2022)와 GLM-130B(Zeng et al., 2023)에 중간 정도의 차이만 보이는 것을 볼 수 있으며, 후자의 모델이 몇 배의 크기를 가지고 있고 우리보다 훨씬 더 많은 데이터로 훈련되었음을 고려할 수 있다.\n' +
      '\n' +
      '또 다른 측면을 위해 중국-알파카-13B와 중국-LLMa-13B는 이전에 C-Eval에 의해 평가되었다. 또한 자체 구현에 의해 예측 파일을 리더보드에 수동으로 제출했다. 그 결과 두 모델 모두 C-Eval에 의해 평가된 모델보다 크게 개선되었으며, 특히 Alpaca-13B 모델의 경우 +5.8 평균 점수(30.9에서 36.7)를 얻었다. 또한 알파카-13B는 LLMa-13B보다 이점을 보여주며, 이는 이전 연구 결과와 일치한다. 이러한 관찰은 적절한 디코딩 전략과 신속한 템플릿을 채택하는 것이 개별 LLM, 특히 명령어 추적 모델의 경우 더 나은 성능을 달성하는 데 중요할 수 있음을 나타낸다.\n' +
      '\n' +
      '## 6 다른 양자화 방법의 효과\n' +
      '\n' +
      '개인용 컴퓨터, 특히 CPU에 대규모 언어 모델을 배포하는 것은 엄청난 계산 요구 사항으로 인해 역사적으로 어려운 일이었다. 그러나, llama.cpp(Gerganov, 2023)와 같은 많은 커뮤니티 노력의 도움으로, 사용자는 LLM을 효율적으로 양자화할 수 있고, 메모리 사용량 및 계산 요구를 상당히 감소시켜 개인용 컴퓨터에 LLM을 더 쉽게 배치할 수 있다. 이는 또한 모델과의 더 빠른 상호 작용을 가능하게 하고 로컬 데이터 처리를 용이하게 한다. LLM을 정량화하고 개인용 컴퓨터에 배포하면 몇 가지 이점이 있습니다. 첫째, 민감한 정보가 외부 서버로 전송되지 않고 로컬 환경 내에 유지되도록 함으로써 사용자가 데이터 프라이버시를 보호하는 데 도움이 됩니다. 둘째로\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l l l} \\hline \\hline\n' +
      '**Model** & **N-Shot** & **Open** & **Avg** & **Avg-H** & **STEM** & **Social** & **Human** & **Others** \\\\ \\hline GPT-4 & 5-shot & ✗ & 68.7 & 54.9 & 67.1 & 77.6 & 64.5 & 67.8 \\\\ IntermLM (104B) & few-shot & ✗ & 62.7 & 46.0 & 58.1 & 76.7 & 64.6 & 56.4 \\\\ ChatGPT & 5-shot & ✗ & 54.4 & 41.4 & 52.9 & 61.8 & 50.9 & 53.6 \\\\ Claude-v1.3 & 5-shot & ✗ & 54.2 & 39.0 & 51.9 & 61.7 & 52.1 & 53.7 \\\\ Claude-instant-v1.0 & 5-shot & ✗ & 45.9 & 35.5 & 43.1 & 53.8 & 44.2 & 45.4 \\\\ Bloomz-mt (176B) & 0-shot & ✓ & 44.3 & 30.8 & 39.0 & 53.0 & 47.7 & 42.7 \\\\ GLM-130B & 0-shot & ✓ & 44.0 & 30.7 & 36.7 & 55.8 & 47.7 & 43.0 \\\\\n' +
      '**Chinese-Alpaca-33B** & 0-shot & ✓ & 41.6 & 30.3 & 37.0 & 51.6 & 42.3 & 40.3 \\\\\n' +
      '**Chinese-Alpaca-Plus-13B** & 0-shot & ✓ & 41.5 & 30.5 & 36.6 & 49.7 & 43.1 & 41.2 \\\\ CubeLM (13B) & few-shot & ✗ & 40.2 & 27.3 & 34.1 & 49.7 & 43.4 & 39.6 \\\\ ChatGLM-6B & 0-shot & ✓ & 38.9 & 29.2 & 33.3 & 48.3 & 41.3 & 38.0 \\\\ LLMa-65B & 5-shot & ✓ & 38.8 & 31.7 & 37.8 & 45.6 & 36.1 & 37.1 \\\\\n' +
      '**Chinese-Alpaca-13B\\(\\dagger\\)** & 0-shot & ✓ & 36.7 & 28.4 & 33.1 & 43.7 & 38.4 & 35.0 \\\\\n' +
      '**Chinese-LLMa-13B\\(\\dagger\\)** & 5-shot & ✓ & 33.7 & 28.1 & 31.9 & 38.6 & 33.5 & 32.8 \\\\ Chinese-LLMaM-13B & 5-shot & ✓ & 33.3 & 27.3 & 31.6 & 37.2 & 33.6 & 32.8 \\\\ MOSS (16B) & 0-shot & ✓ & 33.1 & 28.4 & 31.6 & 37.0 & 33.4 & 32.1 \\\\ Chinese-Alpaca-13B & 0-shot & ✓ & 30.9 & 24.4 & 27.4 & 39.2 & 32.5 & 28.0 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: **C-Eval 리더 보드에 대 한 테스트 결과 (2023년 6월 9일 기준) 평균 점수로 정렬 됩니다.* * 볼드체로 된 모델 이름은 제출을 표시 하는 반면 다른 결과는 C-Eval 관리자에 의해 평가 됩니다. 추론 스크립트를 기반으로 \\(\\dagger\\) (이 점수들은 공개되지 않음)로 표시된 두 모델을 재평가하여 C-Eval에 의해 평가된 모델보다 훨씬 더 나은 성능을 달성했다. 모델의 매개변수 크기는 사용 가능한 경우 괄호 안에 표시된다. 오픈: 오픈 소스입니다. Avg-H: 평균(Hard).\n' +
      '\n' +
      'LLM에 대한 액세스를 제한된 계산 자원을 가진 사용자에게 더 쉽게 접근할 수 있게 함으로써 민주주의화한다. 마지막으로, 로컬 LLM 배치를 활용하는 새로운 애플리케이션 개발 및 연구 방향을 추진한다. 전반적으로, lama.cpp(또는 유사한)를 사용하여 개인용 컴퓨터에 LLM을 배치하는 능력은 다양한 도메인에서 LLM의 보다 다재다능하고 프라이버시를 의식하는 활용을 위한 길을 열어준다.\n' +
      '\n' +
      '이 섹션에서는 다양한 양자화 방법의 효과를 조사한다. lama.cpp를 사용하여 알파카-플러스-7B, 알파카-플러스-13B 및 알파카-33B를 양자화하고 중국 텍스트 말뭉치에서 복잡성을 계산한다. 이 모델을 2비트, 3비트, 4비트, 5비트, 6비트 및 8비트 형태로 양자화하여 원래 FP16 형태와 비교한다.9 결과는 그림 3에 나와 있다.\n' +
      '\n' +
      '각주 9: 구체적으로, 양자화된 모델마다 q2_K, q3_K, q4_0, q5_0, q6_K, q8_0 양자화 옵션을 사용한다.\n' +
      '\n' +
      '양자화 레벨은 메모리 사용량 및 추론 속도에 엄격하게 구속되므로, 적절한 양자화 레벨을 선택할 때 절충이 이루어져야 한다. 우리가 볼 수 있듯이 8비트 양자화 방법은 원래 FP16 모델과 거의 동일하거나 심지어 더 낮은 복잡도를 가지고 있어 FP16 모델의 절반 크기만으로 개인용 컴퓨터에 LLM을 배치하는 데 좋은 선택임을 보여준다. 6비트 모델은 또한 8비트 모델에 필적하는 괜찮은 PPL을 달성하여 속도와 성능의 더 나은 균형을 이룬다. 우리가 보다 공격적인 양자화 레벨을 사용할 때, 특히 3-비트 및 2-비트의 경우 성능이 급격히 감소(즉, 더 높은 PPL)한다. 또한 더 큰 모델이 더 작은 모델보다 양자화 방법에 덜 민감하다는 것을 발견했다. 예를 들어 33B 모델의 성능은 다른 모델보다 훨씬 더 완만하게 변화한다. Plus-7B와 Plus-13B 모델을 비교할 때도 유사한 결과가 관찰된다. 이는 2-비트 및 3-비트 양자화가 더 작은 모델에 대해 덜 효과적이지만 상당한 성능 손실 없이 더 큰 모델을 배포하는 유망한 방법일 수 있음을 나타낼 수 있다. 이것은 사용자가 제한된 컴퓨팅 자원만을 가지고 있고 여전히 큰 언어 모델을 시도하고 싶을 때 매우 도움이 된다. 이것은 또한 양자화된 훈련 방법이 특히 제한된 훈련 자원을 가진 사람들에게 큰 언어 모델을 훈련시키기 위한 주류 접근법이 될 수 있음을 의미할 수 있다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '이 기술 보고서에서는 LLAMA 모델의 중국 이해 및 생성 능력을 향상시키기 위한 접근법을 제시했다. 원작 LLAMA의 중국어 어휘의 한계를 인정받아 20K 추가 중국어 토큰을 편입해 확장해 중국어에 대한 인코딩 효율을 대폭 높였다. 중국인의 건축\n' +
      '\n' +
      '그림 3: **다른 양자화 방법에 대한 복잡성**. 33B 모델은 다른 모델보다 적은 데이터에 대해 학습되기 때문에 PPL이 더 높습니다.\n' +
      '\n' +
      'LLaMA는 명령어 데이터와 함께 감독된 미세 조정을 사용하여 향상된 명령어 추종 능력을 나타내는 중국 알파카 모델을 생성했다.\n' +
      '\n' +
      '모델을 효과적으로 평가하기 위해 10가지 다른 작업 유형에 걸쳐 200개의 샘플에 주석을 달았고 평가를 위해 GPT-4를 활용했다. 실험 결과, 제안된 모델은 중국어 이해 및 생성 작업에서 원래 LLaMA보다 훨씬 더 우수한 성능을 보였다. 또한 C-Eval 데이터 세트에서 모델을 테스트했다. 그 결과, 제안된 모델은 몇 배 더 큰 크기의 모델에 비해 상당한 개선을 달성하고 경쟁적인 성능을 보일 수 있음을 보여준다.\n' +
      '\n' +
      '앞으로는 인간 피드백(RLHF)에서 강화 학습( Reinforcement Learning from Human Feedback) 또는 AI 지시 피드백(RLAIF)에서 강화 학습( Reinforcement Learning from AI Instructed Feedback)을 탐색해 모델의 출력과 인간의 선호도를 더 맞출 계획이다. 더욱이, GPTQ(Frantar et al., 2022)와 같은 보다 진보되고 효과적인 양자화 방법을 채택하고자 한다. 또한, 대규모 언어 모델의 보다 효율적이고 효과적인 사전 훈련 및 미세 조정을 위해 LoRA의 대체 방법을 조사하여 궁극적으로 중국 NLP 커뮤니티 내의 다양한 작업에 걸쳐 성능과 적용 가능성을 높이는 것을 목표로 한다.\n' +
      '\n' +
      '## Limitations\n' +
      '\n' +
      '이 프로젝트는 LLaMA 및 알파카 모델의 중국 이해 및 생성 능력을 성공적으로 향상시켰지만 몇 가지 한계를 인정해야 한다.\n' +
      '\n' +
      '* [noitemsep,topsep=0pt]\n' +
      '* 유해하고 예측할 수 없는 콘텐츠: 모델이 비윤리적 쿼리를 거부할 수 있지만 이러한 모델은 여전히 인간의 선호도 및 값과 잘못 정렬되어 유해하거나 잘못 정렬될 수 있습니다. 이 문제는 훈련 데이터의 편향 또는 특정 컨텍스트에서 적절한 출력을 식별할 수 없는 모델의 무능력에서 발생할 수 있다.\n' +
      '* 불충분한 트레이닝: 컴퓨팅 파워 및 데이터 가용성의 제약들로 인해, 모델들의 트레이닝은 최적의 성능을 위해 충분하지 않을 수 있다. 그 결과 모델의 중국 이해 역량은 여전히 개선의 여지가 있다.\n' +
      '* 견고성 부족: 모델은 일부 상황에서 취성을 보여 적대적 입력 또는 희귀 언어 현상에 직면할 때 일관되지 않거나 무의미한 출력을 생성할 수 있습니다.\n' +
      '* 종합 평가: 대형 언어 모델 평가는 현재 시대에 중요한 주제입니다. LLM에 대한 많은 평가 벤치마크를 보았지만 LLM에 대한 포괄성과 적절성은 잘 연구되고 조사되어야 한다. 보다 다양하고 포괄적인 LLM 평가 데이터 세트 및 벤치마크는 LLM 연구의 미래를 형성하는 데 큰 긍정적인 영향을 미칠 것이다.\n' +
      '* 확장성 및 효율성: LoRA 및 양자화를 적용 하 여 모델을 보다 광범위한 커뮤니티에 더 쉽게 액세스할 수 있지만 원래 LLaMA와 결합 하는 경우 모델의 큰 크기와 복잡성으로 인해 특히 계산 리소스가 제한된 사용자의 배포에 어려움이 발생할 수 있습니다. 이 문제는 다양한 응용 프로그램에서 모델의 접근성과 광범위한 채택을 방해할 수 있다.\n' +
      '\n' +
      '향후 작업은 이러한 한계를 해결하여 모델의 기능을 더욱 강화하여 중국 NLP 커뮤니티의 광범위한 응용 프로그램에 대해 보다 강력하고 접근 가능하며 효과적이다.\n' +
      '\n' +
      '## Acknowledgments\n' +
      '\n' +
      '원안은 문법적 수정과 명확성 향상을 위해 OpenAI GPT-4에 의해 다듬어졌다. 오픈 소스 프로젝트에 기여한 커뮤니티 구성원에 감사드립니다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Cui 등(2020) Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 중국어 자연어 처리를 위해 미리 훈련된 모델을 다시 봅니다. _Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Processing: Findings_, pp. 657-668, Online, November 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.findings-emnlp.58](https://www.aclweb.org/anthology/2020.findings-emnlp.58).\n' +
      '* Cui 등(2021) Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang. Pre-training with whole word masking for chinese bert. _ IEEE/ACM Transactions on Audio, Speech and Language Processing_, 29:3504-3514, 2021. doi: 10.1109/TASLP.2021.3124365.\n' +
      '* Cui 등(2022) Yiming Cui, Wanxiang Che, Shijin Wang, and Ting Liu. Lert: 언어학적으로 동기부여된 사전 훈련된 언어 모델. _ arXiv preprint arXiv:2211.05344_, 2022.\n' +
      '* Dettmers 등(2023) Tim Dettmers, Arti doro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: 양자화된 llms의 효율적인 finetuning. _ arXiv preprint arXiv:2305.14314_, 2023.\n' +
      '* Devlin 등(2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: 언어 이해를 위한 심층 양방향 변압기의 사전 훈련. _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers)_, pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/N19-1423](https://www.aclweb.org/anthology/N19-1423).\n' +
      '* Frantar 등(2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: 생성 사전 훈련된 변압기에 대한 정확한 사후 훈련 압축. _ arXiv preprint arXiv:2210.17323_, 2022.\n' +
      '* Gerganov (2023) Georgi Gerganov. llama.cpp. [https://github.com/ggerganov/llama.cpp] (https://github.com/ggerganov/llama.cpp), 2023.\n' +
      '* Hu 등(2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: 대용량 언어 모델의 저순위 적응. _ arXiv e-prints_, art. arXiv:2106.09685, June 2021. doi: 10.48550/arXiv.2106.09685.\n' +
      '* Huang 등(2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: 기초 모델을 위한 다단계 다학제 중국어 평가 집합입니다. _ arXiv preprint arXiv:2305.08322_, 2023.\n' +
      '* 대규모 언어 모델 정렬을 민주화합니다. _ arXiv e-prints_, art. arXiv:2304.07327, 4 2023. doi: 10.48550/arXiv.2304.07327.\n' +
      '* Kudo and Richardson (2018) Taku Kudo and John Richardson. SentencePiece: 신경 텍스트 처리를 위한 간단하고 언어 독립적인 서브워드 토큰화기 및 디토크화기. _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL [https://aclanthology.org/D18-2012](https://aclanthology.org/D18-2012).\n' +
      '* Lai 등(2017) Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: 검사로부터의 대규모 재평가 이해 데이터세트. _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pp. 785-794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL [https://aclanthology.org/D17-1082](https://aclanthology.org/D17-1082).\n' +
      '* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 비결합 중량 감소 규칙화. _학습 표현에 대 한 국제 회의_ 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)입니다.\n' +
      '* OpenAI(2022) OpenAI. chatgpt를 소개합니다. [https://openai.com/blog/chatgpt] (https://openai.com/blog/chatgpt), 2022.\n' +
      '* OpenAI(2023) OpenAI. GPT-4 Technical Report. _ arXiv e-prints_, art. arXiv:2303.08774, March 2023. doi: 10.48550/arXiv.2303.08774.\n' +
      '\n' +
      '* Ouyang 등(2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 인간 피드백으로 지침을 따르도록 언어 모델을 훈련합니다. _ arXiv e-prints_, art. arXiv:2203.02155, March 2022. doi: 10.48550/arXiv.2203.02155.\n' +
      '* Radford 등(2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 생성 사전 교육을 통해 언어 이해를 향상시킵니다. 2018.\n' +
      '* Rasley 등(2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 딥스피드: 시스템 최적화를 통해 1000억 개 이상의 파라미터를 가진 딥러닝 모델을 학습할 수 있습니다. _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pp.3505-3506, 2020.\n' +
      '* Le Scao et al.(2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _ arXiv preprint arXiv:2211.05100_, 2022.\n' +
      '* Shazeer(2020) Noam Shazeer. Glu 변종은 2020년 변압기를 개선합니다.\n' +
      '* Su et al.(2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.\n' +
      '* Taori 등(2023a) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: 명령어 추종 llama 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca), 2023a.\n' +
      '* Taori 등(2023b) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: 명령어 추종 llama 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca), 2023b.\n' +
      '* Touvron 등(2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Bapiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: 오픈하고 효율적인 기초 언어 모델입니다. _ arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. 관심만 있으면 됩니다. 인구연 본룩스부르크 벵지오 퍼거스 Vishwanathan, R. Garnett(eds.), _Advances in Neural Information Processing Systems_, Volume 30. Curran Associates, Inc., 2017.\n' +
      '* Wang 등(2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: 언어 모델과 자체 생성 지침을 정렬합니다. _ arXiv e-prints_, art. arXiv: 2212.10560, December 2022. doi: 10.48550/arXiv.2212.10560.\n' +
      '* Xu(2019) Bright Xu. Nlp chinese corpus: Large scale chinese corpus for nlp, September 2019. URL [https://doi.org/10.5281/zenodo.3402023](https://doi.org/10.5281/zenodo.3402023).\n' +
      '* Yang 등(2022) Ziqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Daong Wu, and Zhigang Chen. CINO: 중국 소수 민족 사전 훈련 언어 모델. _Proceedings of the 29th International Conference on Computational Linguistics_, pp.3937-3949, 경주, 대한민국, October 2022. International Committee on Computational Linguistics. URL [https://aclanthology.org/2022.coling-1.346](https://aclanthology.org/2022.coling-1.346).\n' +
      '* Zeng 등(2023) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: 개방형 이중언어 사전 훈련 모델. _학습 표현에 대 한 11 번째 국제 회의_에서 2023. URL [https://openreview.net/forum?id=-Aw0rrrrPUF](https://openreview.net/forum?id=-Aw0rrrrPUF).\n' +
      '* Zhang and Sennrich (2019) Biao Zhang and Rico Sennrich. 루트 평균 제곱 레이어 정규화. _Neural Information Processing Systems 32_, Canada Vancouver, 2019. URL [https://openreview.net/references/pdf?id=SlqBAf6rr](https://openreview.net/references/pdf?id=SlqBAf6rr).\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
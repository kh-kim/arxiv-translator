[MISSING_PAGE_FAIL:1]

our framework can be flexibly applied to a wide range of LMs, we use FLAN-T5 (Longpre et al., 2023) models in this work because they are open-source and instruction-tuned on both QA and CoT data, enabling us to use it off-the-shelf without additional training or prompt engineering.1 Our experiment results show that LM-guided CoT prompting outperforms both the standard prompting and the original CoT prompting. More precisely, we find that (1) LM-guided CoT with KD and self-consistency (SC) decoding strategy (Wang et al., 2022b) maximizes the performance gain; (2) RL contributes to a slight increase in overall rationale quality and task performance; (3) choosing the highest-quality rationales for the large LM does not always guarantee improved task performance. This work presents a unique alternative to the direct optimization of the large LM through fine-tuning the comparatively smaller LM. Moreover, the clear separation of two fundamental sub-tasks within CoT reasoning grants practitioners greater control over each task.

Footnote 1: We also ran a small experiment with the instruction-tuned Open LLaMa (7B) model and confirmed that its performance is significantly worse than FLAN-T5 in a zero-shot setting.

## 2 Related Work

**Rationale Distillation.** For computation efficiency or task performance, recent literature has explored methods to improve small LMs' reasoning abilities. Li et al. (2023); Shridhar et al. (2023); Ma et al. (2023) have experimented with rationale distillation, where a small student LM learns from a large teacher LM to generate CoT rationales. While these studies have mainly concentrated on comparing its performance in downstream tasks against that of large LMs, there has been limited investigation into addressing errors in the generated rationales that might have been inherited from the teacher model.

**Rationale Evaluation and Refinement.** In contexts beyond rationale distillation, there have been growing efforts to unravel which aspects of the generated reasoning steps contribute to the downstream task performance. Wang et al. (2022a) report that rationales' logicality and relevance to the query are key factors in successful CoT reasoning. Few studies have measured the validity of reasoning steps from the lens of more diverse aspects like informativeness, coherence, and repetition, etc (Golovneva et al., 2022; Prasad et al., 2023). While RL has gained popularity as an approach for addressing misaligned behaviors in LMs, the field of rationale correction has seen limited research.

## 3 LM-guided Chain-of-Thought

The proposed framework consists of two LMs: a lightweight model \(M^{S}\) that focuses on generating the optimal rationale given an input instance, and a black-box large model \(M^{L}\) that predicts an output based on the rationale generated by \(M^{S}\).

### _Rationale Distillation_

**Rationale Generation.** In general, multi-hop extractive QA datasets contain a list of questions \(Q\), contexts \(C\), and corresponding ground truth answers \(A\). For each input (\(q\), \(c\))-output (\(a\)) pair, we need a corresponding ground truth rationale \(r\) to train \(M^{S}\) for rationale generation in a supervised manner. However, most QA benchmarks do not provide \(r\). Given that manual annotation for \(r\) is labor-intensive and time-consuming, we

Based on the provided context, answer the following question (Q) by reasoning step-by-step.

**Context**: \(c\)

**Q**: \(q\)

**A**: Let's think step by step.

For generation, we use greedy decoding; _i.e._, choosing the most plausible token at each generation step.

**Rationale Filtering and Training.** When it comes to knowledge distillation, data cleaning processes play a crucial role in preventing errors or noises included in the generation from the teacher LM being inherited to the student LM. Thus, we filter samples associated with unfaithful responses to the prompt (_i.e._, not providing rationales prior to providing a final answer) and inaccurate answer prediction. Finally, we instruction-tune \(M^{S}\) using the following prompt:

Given a question (Q) and a context, generate a chain of reasoning step by step to answer the question.

**Context**: \(c\)

**Q**: \(q\)

**Reasoning**: \(r^{\prime}\)

For the rest of the paper, we denote the rationale-distilled model as \(M^{*}\).

### _Rationale Refinement_

**Annotation for Rationale Quality Measurement.** Inspired by previous text and rationale generation evaluation metrics (Golovneva et al., 2022; Fu et al., 2023), we attempt to quantify 8 linguistic aspects (factuality, relevance, logicality, consistency, coherence, fluency, naturalness, readability) of rationales generated by \(M^{*}\) in SS3.1. Let us denote \(r^{*}\) as generated reasoning. Since there is no ground truth rationale \(r\) available for comparison, our metrics are reference-free, utilizing a pair of \(r^{*}\) and one of existing inputs (\(q\) or \(c\)). Table 1 describes each aspect type and input combinations. As we intend to use these metrics for reward scoring in RL, it is critical to have an accurate metric. Hence, we obtain a small set (n=100) of gold labels for all aspect types through human annotation. A detailed description of the annotation process and results is reported in SS11.1.

**Automatic Measurement for Rationale Quality.** Manual annotation offers substantial value, but within the context of RL, it becomes notably challenging due to frequent reward scoring. Therefore, we probe several ways to automate this process. Ye and Durrett (2022) introduced the simple yet effective approach for assessing factuality and relevance through token-level lexical overlap. We follow their method for factuality and relevance measurement. For the remaining 6 categories, two methods are considered. The first approach is to harness a large LM to function as reference-free NLG evaluators, inspired by recent works (_e.g._, Liu et al. (2023), Wang et al. (2023)). The second approach, in contrast, involves training a simple machine learning classifier using human-annotated data. Both approaches are comprehensively described in SS11.2. Due to inference time efficiency and higher alignment scores to human annotators (see Table 5), we resort to the second method for all our experiments.

**RL for Rationale Refinement.** We next detail how to utilize established evaluation metrics as reward signals to update the knowledge-distilled \(M^{*}\) with Proximal Policy Optimization (PPO) (Schulman et al., 2017). Given each input (\(q\), \(c\))-output (\(a\)) pair from training data, we first prompt \(M^{*}\) to generate a corresponding rationale \(r^{*}\). During the generation process, an aspect-specific reward (denoted as \(R_{aspect}\)) is measured by aggregating all values returned from automatic evaluation metrics.2 We then pass \(r^{*}\) to \(M^{L}\) to retrieve the answer prediction \(a^{*}\) and compute a task-specific reward (denoted as \(R_{taskAcc}\)). Specifically, we leverage the F1 score between the predicted answer and the ground truth answer:

Footnote 2: We also test with normalization or weighted summation for the scores, but they did not affect the performance.

\[R_{\text{taskAcc}}=\begin{cases}1&\text{if }F1(a,a^{*})>0.5,\\ 0&\text{else}.\end{cases}\]

A final reward score for model training is the summation of \(R_{aspect}\) and \(R_{taskAcc}\). Following Stiennon et al. (2020), we also incorporate penalties based on the Kullback Leibler (KL) divergence between the learned policy LM and \(M^{S}\).

## 4 Experiments and Results

### Experimental Setup

**Model and Dataset.** we utilize FLAN-T5 small (80M) for \(M^{S}\) and FLAN-T5 XXL (11B) for \(M^{L}\). Both HotpotQA (Yang et al., 2018) and 2Wiki-MultiHopQA (Ho et al., 2020) consist of an input question, and an answer, along with 9-10 context paragraphs with supportiveness labels indicating whether the paragraph contains supporting facts. Due to the input token size limitation of FLAN-T5, we only use supporting paragraphs as context.

**Training & Evaluation Setup.** We use a randomly sampled subset of training data from two datasets (15K samples per data) for model training. After filtering unqualified reasoning, it results in 23K samples. All training-related hyperparameters can be found in SS11.3. According to our preliminary experiments, the impact of CoT prompting appeared to be diminished due to two potential factors: (1) questions being overly simplistic, obscuring the significance of intermediate reasoning processes; (2) LMs already possessing pertinent background information (e.g., Flan-T5 is fine-tuned on various question-answering datasets). To prevent models from answering based on parametric memory, we attempt to make the existing evaluation data more challenging, similar to the approaches taken by Ye and Durrett (2022) and Zhao et al. (2023). For each dataset, we leverage the prediction outcomes of standard prompting and select 1000 input instances

\begin{table}
\begin{tabular}{|c|l|} \hline
**Aspects** & **Descriptions** \\ \hline Factuality & Percentage (0.0-1.0) measuring if the reasoning is grounded based on the context _(Input \(c\) \& \(r^{\prime}\))_ \\ \hline Relevance & Percentage (0.0-1.0) measuring if the reasoning is relevant to the question _(Input \(q\) \& \(r^{\prime}\))_ \\ \hline Logicality & Binary (0 or 1) measuring if the reasoning is logical and can reach a final answer \\ \hline Consistency & Binary (0 or 1) measuring if the reasoning remains consistent and coherent _(Input \(q\) \& \(r^{\prime}\))_ \\ \hline Coherence & Binary (0 or 1) measuring if the reasoning is without redundant information _(Input \(q\) \& \(r^{\prime}\))_ \\ \hline Fluency & Binary (0 or 1) measuring if the reasoning is well-written and grammatically correct _(Input \(r^{\prime}\))_ \\ \hline Naturalness & Binary (0 or 1) measuring if the reasoning is natural and human-like _(Input: \(r^{\prime}\))_ \\ \hline Readability & Binary (0 or 1) measuring if the reasoning is easy to follow and understandable _(Input: \(r^{\prime}\))_ \\ \hline \end{tabular}
\end{table}
Table 1: Descriptions of 8 rationale aspects used for evaluation. \(q\), \(c\), and \(r^{\prime}\) denote a question, context, and a corresponding rationale generated by the small LM, respectively.

that \(M^{L}\) answered correctly and an additional 1000 from questions where the model provided incorrect responses. This results in a total of 2000 samples for evaluation.

**Baselines and Evaluation Metrics.** We use standard prompting and CoT prompting as our baselines (see Table 3). We also experiment with the SC decoding strategy [22], which samples multiple reasoning paths (n=10) and selects the most consistent answer. For evaluation, we report three metrics for the answer prediction task: (1) exact match (EM), computing whether the prediction exactly matches the ground truth answer, (2) F1, computing the average word overlap between the prediction and ground truth answer, and (3) answer inclusion3, computing whether the ground truth answer is mentioned in the prediction.

Footnote 3: We included this measurement because models often provide more extensive responses (_e.g._, ground truth:

### Results

**Baseline Performance.** As shown in Table 2, we find that \(M^{L}\) (equivalent to FLAN-T5 XXL) does not benefit from the original CoT prompting, as its EM and F1 scores dropped in both datasets (except for answer inclusion score for HotpotQA) when compared to standard prompting. This is consistent with previous research findings that models with less than 50B parameters exhibit limited reasoning capabilities. We also observe that the performance drop is more significant with 2WikiMultihopQA (nearly 10% for EM and F1) than HotpotQA. Based on our manual inspection of incorrect predictions, \(M^{L}\) was prone to repeat sentences in the context and fail to provide a final answer to the questions. This hints that, when context gets too long, models face

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Prompt} & Rationale & \multicolumn{3}{c}{HotpotQA} & \multicolumn{3}{c}{2WikiMultiHopQA} \\ \cline{3-7}  & Provision? & & & & & \\ \cline{3-7}  & & EM & F1 & \begin{tabular}{c} Answer \\ Inclusion \\ \end{tabular} & EM & F1 & 
\begin{tabular}{c} Answer \\ Inclusion \\ \end{tabular} \\ \hline standard prompting & ✗ & 0.5 & **0.714** & 0.583 & 0.5 & 0.625 & 0.647 \\ CoT prompting & ✓ & 0.483 & 0.686 & 0.611 & 0.4 & 0.532 & 0.561 \\ CoT prompting + SC & ✗ & 0.503 & 0.70 & 0.624 & 0.471 & 0.603 & 0.625 \\ \hline LM-guided CoT prompting (KD) & ✓ & 0.507 & 0.702 & 0.625 & 0.506 & 0.626 & 0.661 \\ LM-guided CoT prompting (KD + SC) & ✗ & **0.513** & **0.714** & **0.635** & **0.524** & **0.644** & **0.679** \\ LM-guided CoT prompting & ✓ & 0.503 & 0.698 & 0.625 & 0.507 & 0.631 & 0.665 \\ (KD + \(R_{aspect}\)) & ✓ & 0.508 & 0.704 & 0.627 & 0.503 & 0.622 & 0.657 \\ LM-guided CoT prompting & & & & & & \\ (KD + \(R_{aspect}\) + ranking) & ✓ & 0.5 & 0.698 & 0.623 & 0.501 & 0.619 & 0.653 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Answer prediction performance results of baselines and our approach. We regard SC decoding as a non-rationale provision because this method can result in multiple variations of rationales, rather than a single one. Values in bold represent the highest scores and underlined values are the second highest scores.

\begin{table}
\begin{tabular}{|c|p{142.3pt}|p{142.3pt}|} \hline
**Type** & **Description** & **Template** \\ \hline Standard prompting & \begin{tabular}{c} Directly predicting the answer \\ based on input \\ \end{tabular} & \begin{tabular}{c} Based on the provided context, answer the following question (Q). \\ Context: \(c\) \\ \end{tabular} \\ \hline CoT prompting & \begin{tabular}{c} Predicting the answer after \\ generating the reasoning \\ \end{tabular} & \begin{tabular}{c} Based on the provided context, answer the following question (Q) \\ by reasoning step-by-step. \\ Context: \(c\) \\ \end{tabular} \\ \hline LM-guided (our method) & \begin{tabular}{c} Predicting the answer with \\ conditional generation upon \\ the LM-generated reasoning \\ \end{tabular} & 
\begin{tabular}{c} Based on the provided context, answer the following question (Q) \\ by reasoning step-by-step. \\ Context: \(c\) \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 3: Descriptions and templates of each prompt used for the answer prediction task. \(q\), \(c\), and \(r^{\prime}\) denote a question, context, and a corresponding rationale generated by the small LM, respectively.

difficulties in digesting the content and establishing valid reasoning steps. Overall, SC CoT prompting enables the model to noticeably recover from answer prediction errors, especially for 2WikiMulti-hopQA.

**LM-guided CoT Performance.** Table 2 shows a comprehensive breakdown of our method's performance. Additionally, we explore an extension of our approach, which involves sampling multiple reasoning paths and subsequently ranking the most optimal rationales. Our method with only KD outperforms the original CoT prompting with 2% gain for HotpotQA and 10% for 2WikiMultiHopQA, respectively. Figure 2 illustrates the respective rationale qualities of all prompting techniques, reinforcing the effectiveness of our method in enhancing both answer prediction and rationale qualities. When employing the original CoT prompting for questions with lengthy contexts, models frequently recycle sentences from the provided context and struggle to deliver a conclusive answer to the question. This trend is mitigated by our approach, resulting in a significant decrease in error rates. It also surpasses the performance of CoT prompting + SC and is on par with standard prompting in terms of EM and F1. For the answer inclusion score, LM-guided CoT prompting is slightly higher (1-2%) than standard prompting. Furthermore, LM-guided CoT prompting + SC achieves the highest performance across all settings.

As shown in Figure 2, the implementation of RL enables the model to achieve additional improvements in both rationale qualities and task performance. However, in line with Joshi et al. (2023)'s findings, a slight decrease in task performance is observed at the cost of maximized rationale qualities when selecting top-quality rationales. There may be several underlying factors involved (e.g., models' unfaithfulness), but this is not the scope of this work.

## 5 Conclusion

**LM-Guided CoT** is a novel framework that decomposes a conventional CoT prompting into two steps using two models: (1) rationale generation and (2) answer prediction. This includes distilling the reasoning ability from a large LM to a small LM and further optimizing it with RL. The results reveal that our method outperforms all baselines, highlighting its potential to serve as an effective and resource-efficient approach to tackle challenges within the CoT prompting paradigm. Meanwhile, we also find that selecting top-quality rationales for answer prediction may not consistently boost task performance. This prompts the need to explore a more harmonious balance between LM-generated rationale utilities and overall task performance.

## 6 Limitations

Although our framework can seamlessly accommodate various model combinations in a plug-and-play manner, we have restricted our experimental reporting to FLAN-T5. In a similar vein, this work only explores the task of multi-hop QA, leaving an open question about generalizability to other reasoning tasks. We anticipate future research endeavors to extend the application of our approach across diverse domains requiring sophisticated reasoning. Lastly, due to resource constraints, we were unable to collect extensive human annotations for established aspect evaluation metrics.

## 7 Ethical Considerations

All the datasets that we use in our work are publicly available, and we have given appropriate credit to the original authors throughout the paper. We acknowledge that occasionally, the generated rationales may include non-factual and offensive statements. As we do not plan on distributing these artifacts, it effectively reduces the potential for harm.

## 8 Acknowledgments

We thank all the reviewers for providing valuable feedback.

## References

* Fleiss (1971) Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. _Psychological

Figure 2: Average answer prediction performance (across three evaluation metrics) and average rationale quality scores (_i.e._, \(R_{aspect}\)) for HotpotQA (left) and 2WikiMultiHopQA (right). The right y-axis represents the mean answer prediction scores, and the left y-axis represents the mean rationale quality scores.

bullet_, 76(5):378.
* Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. _arXiv preprint arXiv:2302.04166_.
* Golovneva et al. (2022) Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning. _In The Eleventh International Conference on Learning Representations_.
* Joshi et al. (2023) Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Oifan Wang, Yejin Choi, and Xiang Ren. 2023. Are machine rationales (not) useful to humans? measuring and improving human utility of free-text rationales. _arXiv preprint arXiv:2305.07095_.
* Jung et al. (2022) Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maleutic prompting: Logically consistent reasoning with recursive explanations. _arXiv preprint arXiv:2205.11822_.
* Khot et al. (2022) Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. _arXiv preprint arXiv:2210.02406_.
* Lanham et al. (2023) Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. 2023. Measuring faithfulness in chain-of-thought reasoning. _arXiv preprint arXiv:2307.13702_.
* Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. _Advances in Neural Information Processing Systems_, 35:3843-3857.
* Li et al. (2023) Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. 2023. Symbolic chain-of-thought distillation: Small models can also" think" step-by-step. _arXiv preprint arXiv:2306.14050_.
* Liu et al. (2023) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gptsval: Nlg evaluation using gpt-4 with better human alignment. _arXiv preprint arXiv:2303.16634_.
* Longpre et al. (2023) Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The fian collection: Designing data and methods for effective instruction tuning. _arXiv preprint arXiv:2301.13688_.
* Ma et al. (2023) Yuhan Ma, Haiqi Jiang, and Chenyou Fan. 2023. Sci-cot: Leveraging large language models for enhanced knowledge distillation in small models for scientific qa. _arXiv preprint arXiv:2308.04679_.
* Prasad et al. (2023) Archiki Prasad, Swamadeep Saha, Xiang Zhou, and Mohit Bansal. 2023. Receval: Evaluating reasoning chains via correctness and informativeness. _arXiv preprint arXiv:2304.10703_.
* Schulman et al. (2017) John Schulman, Filip Wolski, Pratulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_.
* Shridhar et al. (2023) Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. 2023. Distilling reasoning capabilities into smaller language models. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 7059-7073.
* Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021.
* Turpin et al. (2023) Miles Turpin, Julian Michael, Ethan Perez, and Samuel R Bowman. 2023. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. _arXiv preprint arXiv:2305.04388_.
* Wang et al. (2022a) Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022a. Towards understanding chain-of-thought prompting: An empirical study of what matters. _arXiv preprint arXiv:2212.10001_.
* Wang et al. (2023) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. _arXiv preprint arXiv:2303.04048_.
* Wang et al. (2022b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. _arXiv preprint arXiv:2203.11171_.
* Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_.
* Wang et al. (2022b)Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837.
* Ye and Durrett (2022) Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot prompting for textual reasoning. _Advances in neural information processing systems_, 35:30378-30392.
* Zhao et al. (2023) Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023. Verify-and-edit: A knowledge-enhanced chain-of-thought framework. _arXiv preprint arXiv:2305.03268_.

## 10 Language Resource References

* Ho et al. (2020) Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko. 2020. _Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps_.
* Yang et al. (2018) Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D. 2018. _HotpotQA: A dataset for diverse, explainable multi-hop question answering_.

## 11 Appendices

### Human Annotation for Rationale Quality Measurement

**Annotation Details.** Three researchers manually inspected 100 instances randomly sampled from training data. To ensure the quality and consistency of the annotation process, one researcher designed the annotation instruction describing the definition of each aspect (see Table 1) as well as 4 demonstration examples for each aspect category. Figure 3 displays demonstration examples for logicality. These examples are chosen randomly from the remaining training set, which was not included in the initial set of 100 annotation samples, and are manually annotated based on aspect descriptions. Upon the completion of annotation, we gauged the inter-rater agreement rates to validate the reliability of submitted annotation results by computing the average Fleiss' Kappa coefficient (Fleiss, 1971). The mean Kappa score among the three annotators across all aspect categories was 0.56, indicating a moderate agreement rate. Ultimately, we take the mode of annotated labels submitted by three annotators and consider it as a ground truth.

**Analysis of Relationships between Aspect Types and Task Performance.** Based on 100 labeled instances, we perform a post-hoc analysis to understand how the proposed aspects are related to answer prediction performance. The label distributions are as follows: correct (n=78) vs. incorrect (n=22). We compare the mean difference of evaluation scores based on correct & incorrect responses. As shown in Figure 4, rationales associated with correct answer prediction are prone to have higher scores than those with incorrect prediction. In particular, coherence has the largest mean difference, followed by readability and logicality. For fluency and naturalness, the gap seems minimal. We further conduct statistical testing to validate their statistical significance. T-test results confirm that the mean difference observed in coherence (\(p=0.003\)), readability (\(p=0.0002\)), and logicality (\(p=0.05\)) are statistically significant with \(p<0.05\).

### Automatic Measurement for Rationale Quality

**Method 1: Self-Evaluation from Large LMs.** Fu et al. (2023) have demonstrated the emergent capabilities of large LMs in neural text evaluation, achieved through zero-shot instruction and in-context learning. The key idea is that, given the natural language description of desired task and evaluation aspects, large LMs can assess multi-dimensional text quality without any learning process. Motivated by this, we instruct FLAN-T5 XXL to evaluate 6 aspects of the machine-generated rationales. Similar to Fu et al. (2023)'s experiments, we investigate the performance of instruction-only (IST) prompting and instruction+demonstration (IDM) prompting. Let's say \(q\) and \(r^{\prime}\) denote a question and a machine-generated rationale that is yet to be evaluated. \(d\) represents the aspect definition of our interest from Table 1. A prompt template used for IST is:

Answer the question based on the provided information.Question: Can the given reasoning \(d\)? (a) Yes.(b) No.

**Information**:

Question: \(q\)Reasoning: \(r^{\prime}\)Answer :

A prompt template for IDM is equivalent to IST but with the inclusion of a few task demonstrations. The number of demonstrations ranges from 1 to 4. We use the same demonstrations included in the human annotation instruction (SS11.1). Lastly, we evaluate IST in combination with SC decoding, which involves sampling the prediction multiple times (n = 10) and taking the mode as a final prediction. To ensure that the rationale evaluation of FLAN-T5 XXL aligns closely with human annotation, we compute the macro F1 scores for 5 prompt-based experiments using 100 human-labeled examples (Table 4). In most cases except for fluency, IST prompting demonstrates the highest performance.

**Method 2: Supervised Training with Human-Annotated Data.** Here we train a logistic regression classifier using 100 ground truth data from SS11.1. This can be done by converting input data into TF-IDF vectors. Due to a small training data size, we resort to training two independent binary classifiers instead of having 6 models for each aspect type. While the first model is to predict if a given reasoning is logical, coherent, and consistent, the second model focuses on predicting whether the reasoning is fluent, natural, and readable. If at least one of the components is not satisfied, we consider it a negative label. The final label distribution is as follows: logicality & consistency & coherence (0: 60 vs. 1: 40), and fluency & naturalness & readability (0: 20 vs. 1: 80). We use 90% of the dataset (n=90) for training and the remaining 10% (n=10) for evaluation.

Figure 4: Mean scores of human annotation results by answer prediction correctness.

Figure 3: Demonstration example for “logicality” annotation.

**Method 1 vs. Method 2**. We attempt to assess which one is more suitable for providing rewards for RL. For a fair comparison, we use the same evaluation data that was used in Method 2. Since Method 1 has 6 aspect categories, we obtain individual results using the best-performing approach and group them into two as we did for Method 2. Table 5 reports the accuracy, macro precision, macro recall, and macro F1 for Method 1 and Method 2. The results indicate that, although models from Method 2 are trained on a relatively small dataset, Method 2 is more aligned with human judgments compared to Method 1. Additionally, inference time using Method 2 is significantly faster than Method 1, making it easier to retrieve rewards in the RL setting. As a result, we use Method 2 for all our experiments.

### Training Configuration

We provide a comprehensive description of the hyperparameters utilized in the model training process.

**Rationale Distillation.** For rationale generation from the teacher model, we randomly sampled 15K examples from each of the two datasets, resulting in a total of 30K examples. After filtering invalid rationales, the dataset was reduced to a total of 23K examples. 90% of data was used for training, and the remaining 10% was used for validation. For training, we used 8 NVIDIA Tesla V100 GPUs with 16GB configurations. Hyperparameters for training are as follows: 3e-3 for learning rates, 5 epochs, 64 batch size.

**RL for Rationale Refinement.** For RL, we utilized the Huggingface's TRL4 library that provides a set of tools to train transformer LMs with RL. Instead of using the same examples used for the rationale distillation, we selectively chose 5000 examples that FLAN-T5 XXL failed to answer the question correctly. The reason behind this choice was to increase the model's exposure to challenging questions, thus increasing the likelihood of receiving more learning signals. We used 90% of data for training and the remaining 10% for validation. Training hyperparameters are as follows: 1.4e-5 for learning rates, 1 epoch, 16 batch size. For generation configurations, we set top_k as 0.0, top_p as 1.0, and enabled sampling.

Footnote 4: [https://huggingface.co/docs/trl/index](https://huggingface.co/docs/trl/index)

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline \multicolumn{2}{c|}{Prompting} & Coherence & Consistency & Logicality & Fluency & Naturalness & Readability \\ \hline \multirow{2}{*}{IST} & IST & 0.52 & **0.70** & 0.73 & 0.91 & **0.52** & **0.86** \\  & IST + self-consistency & **0.53** & 0.67 & **0.74** & 0.90 & 0.50 & 0.85 \\ \hline \multirow{4}{*}{IDM} & IDM (1 shot) & **0.53** & 0.64 & 0.70 & **0.92** & 0.29 & 0.70 \\  & IDM (2 shot) & **0.53** & 0.62 & 0.73 & 0.91 & 0.24 & 0.69 \\  & IDM (3 shot) & 0.51 & 0.62 & 0.73 & 0.88 & 0.28 & 0.67 \\  & IDM (4 shot) & 0.52 & 0.62 & 0.70 & 0.89 & 0.36 & 0.50 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The macro F1 scores for 5 prompt-based experiments from Method 1, based on 100 human-labeled examples. Values in bold represent the best performance in each aspect.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c}{Coherence \& Consistency \& Logicality} & Fluency \& Naturalness \& Readability \\ \cline{2-9}  & Acc & Precision & Recall & F1 & Acc & Precision & Recall & F1 \\ \hline Method 1 & 0.62 & 0.62 & 0.6 & 0.6 & 0.7 & 0.7 & **0.81** & 0.67 \\ Method 2 & **0.8** & **0.79** & **0.79** & **0.79** & **0.9** & **0.94** & 0.75 & **0.9** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Evaluation results (Method 1 vs. Method 2). Values in bold represent the best performance in each aspect.
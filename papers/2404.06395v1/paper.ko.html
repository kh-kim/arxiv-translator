<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# MiniCPM: 확장 가능한 훈련 전략을 가진 소규모 언어 모델의 잠재력 분석\n' +
      '\n' +
      '성딩후1, 유게투2, 쉬한1, 차오쿤허1, 간쿠추이1, 샹룽2\n' +
      '\n' +
      '지정2, 예웨이팡2, 위샹황1, 웨일린자오1, 신롱장1\n' +
      '\n' +
      '정렝타이1,카이후장2,총이왕2,위안야오1\n' +
      '\n' +
      '천양 자오1, 제 주2, 제 차이2, 중우 자이2, 닝딩1\n' +
      '\n' +
      '조자2, 궈양정2, 다하이리2, 지위안리우1, 마오송순1\n' +
      '\n' +
      '칭화대학교 컴퓨터공학과\n' +
      '\n' +
      '2Modelbest Inc.\n' +
      '\n' +
      'shengdinghu@gmail.com\n' +
      '\n' +
      'Corresponding Authors.\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최대 조 개의 매개 변수를 가진 대규모 언어 모델(LLM) 개발에 대한 급증하는 관심은 특히 엄청난 실험 비용을 고려할 때 자원 효율성과 실제 비용에 대한 우려에 직면해 있다. 이 시나리오는 자원 효율적인 대안으로서 작은 언어 모델(SLM)의 잠재력을 탐색하는 것의 중요성을 강조한다. 이러한 맥락에서 MiniCPM, 특히 1.2B 및 2.4B 비 임베딩 매개변수 변형을 도입하여 각 범주에서 우수할 뿐만 아니라 7B-13B LLM과 동등한 기능을 보여준다. SLM에 초점을 맞추면서, 우리의 접근법은 향후 LLM 연구를 위한 모델 및 데이터 차원 모두에서 확장성을 나타낸다. 모델 스케일링과 관련하여, 우리는 안정적이고 최적의 스케일링을 위해 광범위한 모델 풍동 실험을 사용한다. 데이터 스케일링을 위해 Warmup-Stable-Decay(WSD) 학습률 스케줄러(LRS)를 도입하여 지속적인 훈련과 도메인 적응에 도움을 준다. 우리는 WSD LRS에서 발생한 흥미로운 훈련 역학에 대한 심층 분석을 제시한다. WSD LRS를 사용하여, 우리는 모델 및 데이터의 양 축에 대한 광범위한 재훈련 실험 없이 데이터-모델 스케일링 법칙을 효율적으로 연구할 수 있으며, 이로부터 친칠라 최적보다 훨씬 더 높은 계산 최적 데이터-모델 비율을 도출한다. 또한 MiniCPM-DPO, MiniCPM-MoE, MiniCPM-128K를 포함한 MiniCPM 계열을 소개하며, 다양한 SLM 응용에서 MiniCPM의 기초를 더욱 공고히 하는 우수한 성능을 보인다. MiniCPM 모델은 공개적으로 1을 사용할 수 있습니다.\n' +
      '\n' +
      '각주 1: [https://github.com/OpenBMB/MiniCPM](https://github.com/OpenBMB/MiniCPM)\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '스케일링 법칙 Kaplan et al.(2020)의 폭로에 이어, Large Language Models(LLMs) Hoffmann et al.(2022); Bai et al.(2023); Gemini et al.(2023); Chowdhery et al.(2023); Achiam et al.(2023)은 수조 페더스 et al.(2022)에서 놀라운 수의 파라미터를 갖는 모델을 포괄하는 분야에서 활발한 추구가 있었다. 이러한 모델들은 인공 지능의 진화에 중추적인 원동력으로 부상하였다.\n' +
      '\n' +
      '그럼에도 불구하고 이러한 대규모 모델의 교육은 재정적으로 부담스럽고 운영적으로 비효율적이다. 한편으로는 LLM의 훈련을 뒷받침하는 메커니즘에 대한 경험적 이해는 여전히 파악하기 어렵다. 상당한 경제적 및 환경적 비용을 감안할 때 LLM에 대한 실험은 대부분의 연구자와 기업에게 엄청나게 비싸다. 반면에 개인용 컴퓨터나 스마트폰과 같은 일상적인 시나리오에서 이러한 거대한 모델의 배포는 비효율적이거나 실행 불가능하다.\n' +
      '\n' +
      '두 측면 모두 더 작지만 강력한 언어 모델(SLM)을 포괄적으로 탐색하는 데 노력을 다시 집중해야 하는 필요성을 강조한다. 이러한 모델은 한편으로는 실제 배치에 대한 효율적인 솔루션을 제공하고 다른 한편으로는 확장 가능한 전략으로 훈련되면 잠재적으로 미래의 더 큰 모델의 개발을 안내할 수 있다.\n' +
      '\n' +
      '최근, Pi 시리즈(Gunasekar et al., 2023; Li et al., 2023; Javaheripi and Bubeck, 2023), TinyLlama(Zhang et al., 2024), MobileLLM(Liu et al., 2024), 및 Gemma(Banks and Warkentin, 2024) 등과 같은 일련의 혁신적인 모델의 출현에 의해 입증된, SLM의 영역에서 관심의 부활이 관찰되었다. 이러한 모델은 SLM 경관의 확장과 다양화에 크게 기여했지만 이러한 모델이 아직 일반적인 관심을 완전히 충족시키지 못한 두 가지 중추적인 영역이 남아 있다.\n' +
      '\n' +
      '본 논문에서는 2.4B와 1.2B non-embedding 파라미터를 각각 부여한 두 개의 모델을 기반으로 하는 SLM의 일종인 MiniCPM을 소개하고, 각각의 2B와 1B 스케일 카테고리에서 우선적으로 순위를 매긴다. MiniCPM은 또한 Llama2-7B (Touvron et al., 2023), Mistral-7B (Jiang et al., 2023), Gemma-7B (Banks and Warkentin, 2024), 및 Llama-13B (Touvron et al., 2023) 등과 같은 7B\\(\\sim\\)13B 언어 모델들과 비교가능한 능력을 나타낸다. 우리의 훈련 방법론은 작은 모델 크기에도 불구하고 모델 규모와 데이터 지평의 원활한 스케일링을 용이하게 하도록 세심하게 설계되었다. 이것은 포괄적인 하이퍼-파라미터 최적화(섹션 3)와 WSD(Warmup-Stable-Decay) 학습률 스케줄러(섹션 4)의 배치를 포괄하는 우리의 모델 풍동 실험을 통해 예시된다. 후자는 미리 정의되지 않은 사전 훈련 토큰 번호를 사용하여 지속적인 훈련에 맞게 조정되며 모델 중간 체크포인트의 재사용을 매우 실현 가능하게 한다. MiniCPM의 훈련 역학에 대한 상세한 분석이 제시되었으며, 이는 WSD 스케줄러가 모델 사전 훈련의 흥미로운 손실 환경을 입증함을 시사한다. WSD 스케줄러를 사용하면 모델 축에 대한 선형적 노력과 데이터 축에 대한 무시할 수 있는 노력을 가진 데이터 모델 스케일링 법칙을 연구할 수 있지만, 기존의 스케줄러는 모델 축과 데이터 축에 대한 스케일링을 고려하여 2차적 노력을 필요로 한다. 스케일링 법칙의 결과는 Chinchilla Optimal(Hoffmann et al., 2022)에 비해 훨씬 높은 데이터 크기/모델 크기 비율을 나타낸다.\n' +
      '\n' +
      '또한 MiniCPM-DPO, MiniCPM-128K, MiniCPM-MoE를 포함한 MiniCPM 계열을 소개한다. 기존의 벤치마크에 대해 MiniCPM 패밀리에 대한 평가를 수행하고 SLM으로서 그들의 인상적인 능력을 조명한다: (1) 기초 모델은 미스트랄-7B, L라마-13B를 능가한다. (2) dpo 모델은 MTBench(Zheng 등, 2024)에서 zephyr-7B(Tunstall 등, 2023)를 능가한다(3). 2.4B MiniCPM-128K 모델은 Yarn-Mistral-7B-128K(Peng 등, 2023) 및 ChatGLM3-6B-128K(Du 등, 2021)와 같은 모델을 능가하거나 매칭하는 성능을 보여준다. (4) 4B 활성화된 파라미터를 갖는 MiniCPM-MoE는 Llama2-34B와 동등하다(Touvron et al., 2023).\n' +
      '\n' +
      '요약하면, MiniCPM은 SLM 내의 잠재 잠재력을 예시하고 LLM을 확장하기 위한 보다 과학적이고 지속 가능한 접근법을 옹호하는 작은 언어 모델 개발의 새로운 단계를 제안한다.\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '**작은 언어 모델.** "작은 언어 모델"(SLMs)은 시간이 지남에 따라 상당한 변형을 겪은 진화하는 개념입니다. 현재, SLM은 일반적으로 잘 알려진 LLM에 비해 규모가 작은 모델로 해석되며, 일반적으로 70억 개의 매개변수를 초과하지 않는다. 이러한 모델은 GPU가 없는 경우에도 개인용 컴퓨터 및 스마트폰과 같은 최종 사용자 장치에 배포할 수 있는 용량으로 구별됩니다. SLM의 현재 풍경 내에서 주목할 만한 예들은 Phi 시리즈(Gunasekar et al., 2023; Li et al., 2023; Javaheripi and Bubeck, 2023), TinyLlama(Zhang et al., 2024), MobileLLM(Liu et al., 2024), 및 Gemma(Banks and Warkentin, 2024) 등을 포함한다. SLM의 효능을 높이기 위해 다양한 방법론이 탐구되었다. 이들은 고품질 데이터의 통합(Gunasekar et al., 2023; Li et al., 2023; Javaheripi and Bubeck, 2023), 구조 프루닝 기술들의 적용(Xia et al., 2023), 및 모델 아키텍처들의 재구성(Liu et al., 2024) 등을 포함한다. 미니CP멘은 하이퍼파라미터 최적화, 전략적 훈련 방법론, 아키텍처 설계 및 고품질 데이터의 세심한 통합을 통해 SLM의 기능을 조정합니다.\n' +
      '\n' +
      '**확장 가능한 사전 훈련 전략** 스케일링 법칙의 발견 이후(Kaplan et al., 2020; Rae et al., 2021; Aghajanyan et al., 2023), 과학적으로 그리고 예측 가능하게(Achiam et al., 2023; Hu et al., 2023; Du et al., 2024) LLMs 스케일 업은 특히 사전 훈련 단계에 대해 다양한 관점에서 추구되어 왔다. 훈련 안정성 측면에서, 텐서 프로그램 시리즈(Yang et al., 2022; 2023)는 훈련 CerebrasGPT(Dey et al., 2023)에서 채용된 기법인 다양한 모델 스케일들에 걸쳐 최적의 하이퍼-파라미터 일관성을 보장하기 위해 도입된다. 또한 Wortsman et al. (2023)은 더 큰 모델 훈련에서 불안정성을 예측하고 완화하기 위해 더 작은 모델을 활용하는 것을 제안한다. 훈련 데이터 관점에서, 다양한 데이터 중심 전략들이 주창되었다(Xie et al., 2024; Shi et al., 2023; Ye et al., 2024). 학습 방법론 분야에서 선행 연구는 다양한 학습률 스케줄러(LRS, Howard and Ruder, 2018; Raffel et al., 2020; Hundt et al., 2019)에 대해 조사했으며, LMs에서 코사인 LRS(Loshchilov and Hutter, 2016)가 주요 선택으로 부상했다. Kaplan et al.(2020)과 Hoffmann et al.(2022)은 코사인 LRS의 하이퍼 파라미터를 세밀하게 조사하여 후속 사전 훈련 작업을 위한 기초 기반을 마련하였다. 이 중 DeepSeek (Bi 등, 2024)는 제안된 WSD LRS와 가장 유사한 것을 가지고 있다. 배치 크기 스케줄링과 관련하여 Smith 등(2017)은 학습률 감소에 대한 대안으로 배치 크기를 증가시키는 것을 옹호하고 있으며, 최근 Yi-9B(Young 등, 2024)에 의해 채택된 전략이다.\n' +
      '\n' +
      '## 3 모델 풍동 실험\n' +
      '\n' +
      '최종 장치에 빠르게 배포될 수 있는 훈련 SLM을 목표로 하지만 모델 훈련의 많은 측면이 규모에 걸쳐 보편적이라고 상상한다. 경험을 LLM으로 옮기기 전에 SLM의 한계를 탐구하기 위해 SLM을 통해 광범위한 실험을 수행해야 한다. 이러한 실험들은 항공기 개발에 있어 풍동 시험의 정신을 취하므로, 이를 모델 풍동 실험(Model Wind Tunnel Experiment, MWTE)이라고 명명한다. 본 논문에서 MWTE는 (1) 하이퍼-파라미터, (2) 최적 배치 크기 스케일링, (3) 최적 학습 속도 안정성의 세 부분으로 구성된다.\n' +
      '\n' +
      '### Hyper-parameters 불변 LM 크기 조정\n' +
      '\n' +
      '하이퍼 매개변수는 모델의 성능에 상당한 영향을 미칩니다. 그러나, 전통적인 훈련에서 각 모델에 대한 하이퍼-파라미터 조정은 LLM에 대해 실현 가능하지 않다. MiniCPM과 같은 SLM의 경우에도 하이퍼-파라미터 탐색에 대한 광범위한 실험에는 많은 자원이 소요된다. 텐서 프로그램(Yang et al., 2022; 2023)은 상이한 스케일들을 갖는 모델들에 대한 하이퍼-파라미터들을 안정화시키기 위한 프레임워크를 제안한다. 텐서 프로그램의 주요 부분은 폭 스케일링(Yang et al., 2022) 및 깊이 스케일링(Yang et al., 2023)이다. 전자의 기술은 LLM의 손실을 보다 정확하게 예측하기 위해 CerebrasGPT(Dey et al., 2023)를 지원한다. MiniCPM에서는 두 가지 스케일링 기법을 모두 사용한다. 구체적인 스케일링 동작은 표 7에 나열되어 있다. 우리는 어텐션 소프트맥스 스케일링 기술을 적용하지 않는다(Yang et al., 2022). 블록 깊이가 2보다 큰 네트워크에 대한 깊이 스케일링이 만족스럽지 않음을 Yang et al. (2023)이 관찰했음에도 불구하고, 우리는 결과적인 최적 학습 속도가 경험적으로 안정적이라는 것을 발견한다. 하이퍼 매개 변수 및 텐서 프로그램 운영에 대한 자세한 내용은 부록 A.1에 나와 있습니다.\n' +
      '\n' +
      '### 최적 Batch 크기\n' +
      '\n' +
      '배치 크기는 모델의 수렴 속도와 계산 자원 소비 사이의 균형을 결정한다. 배치 크기가 너무 크면 상당한 양의 데이터 및 계산 비용이 발생합니다. 반면에 배치 크기가 너무 작으면 많은 수의 훈련 단계를 필요로 하고 손실 함수의 제한된 감소를 초래할 수 있다. 예상 손실에서 배치 크기를 결정하기 위해 Kaplan 등(2020)을 따르며 설정에서 약간 수정한다(부록 A.2 참조). 이 목표를 향해 각각 0.009B, 0.03B 및 0.17B 모델에 대한 실험을 수행한다. 각 모델 크기는 글로벌 학습률이 0.01이고 코사인 학습률 스케줄러가 있는 6개의 배치 크기로 훈련된다. 우리는 C4(Raffel et al., 2019) 데이터 세트(그림 1의 빨간색 선)에서 손실이 있는 최적의 배치 크기의 추세를 관찰한다.\n' +
      '\n' +
      '그림 1에서 볼 수 있듯이 배치 크기를 x축으로 표시하고 토큰 소비를 y축으로 표시하며 점의 색상은 손실을 나타낸다. 따라서, 컬러 포인트들에 의해 형성되는 수평 라인은 트레이닝 곡선을 나타낸다. 우리는 포물선을 사용하여 동일한 손실점을 맞추고 포물선의 최소값을 빨간색 선과 연결한다. 선들은 손실이 감소함에 따라 최적의 배치 크기가 크게 이동함을 보여준다. 그런 다음 세 개의 선(그림 2 참조)을 연결하고 선이 로그 공간에서 선형 관계로 잘 연결된다는 것을 발견하며, 이로부터 배치 크기 _bs_와 C4 손실 _L:_\\(bs=\\frac{1.21\\times 10^{9}}{L^{6.24}}}\\) 사이의 다음 관계를 얻는다.\n' +
      '\n' +
      '### 최적 학습 속도\n' +
      '\n' +
      '텐서 프로그램(Yang et al., 2022, 2023)의 사용으로 인해, 우리는 학습률이 모델 스케일링 동안 큰 변화를 겪지 않을 것으로 예상한다. 이를 검증하기 위해 0.04B, 0.1B, 0.3B, 0.5B에서 6개의 학습률 실험을 수행한다. 그림 3에서 모델 크기가 10배 증가했지만 최적의 기본 학습률 2는 눈에 띄는 변화를 보이지 않고 0.01 정도로 유지된다는 것을 발견했다. 또한 2.1B의 척도에서 간단한 검증을 수행하고 0.01의 학습률이 실제로 가장 낮은 손실을 달성하는지 확인한다.\n' +
      '\n' +
      '각주 2: 2-D 텐서의 실제 학습률은 텐서 프로그램에 따라 스케일링될 것이다.\n' +
      '\n' +
      '## 4 WSD 학습 속도 스케줄러\n' +
      '\n' +
      '### Cosine LRS 분석\n' +
      '\n' +
      '학습의 여러 단계에서 사용되는 학습률을 조절하는 학습률 스케줄러(LRS)는 모델 성능에 매우 중요하다. 현재 일반적으로 사용되는 학습률 전략은 코사인 LRS(Kaplan et al., 2020; Hoffmann et al., 2022; Rae et al., 2021; Touvron\n' +
      '\n' +
      '그림 1: 서로 다른 배치 크기를 사용하여 훈련된 세 가지 크기 모델의 손실 곡선을 보여준다. 그래디언트 컬러를 갖는 점들에 의해 형성된 각각의 수직선은 트레이닝 곡선을 나타낸다. 더 밝은 색은 더 높은 손실을 나타냅니다. 그림 2: 연결된 최적 배치 크기입니다.\n' +
      '\n' +
      'et al., 2023; Bai et al., 2023; Almazrouei et al., 2023)이고, 이는 워밍업 단계 이후 최대치에 도달한 후 코사인 곡선에 따라 학습률을 점진적으로 감소시키는 것을 특징으로 하는 학습 방법.\n' +
      '\n' +
      '코사인 LRS에서 핵심 하이퍼파라미터는 코사인이 처음으로 최소로 감소하는 단계 \\(T\\)이다. 전형적으로, \\(T\\)는 사전 정의된 트레이닝 단계를 갖는 트레이닝을 위해 총 트레이닝 단계 \\(S\\)로 설정된다. 일반적으로 충분한 탐구가 가능하도록 학습률이 높아야 할 것으로 판단된다. 예를 들어, Kaplan 등(2020)은 전체 훈련에 걸친 합산 학습률이 증가할 때 손실이 감소한다는 것을 입증한다(그들의 논문의 도 23 참조). 이것은 설정 \\(T<S\\)이 최적이 아님을 나타낸다. 한편, Hoffmann et al.(2022)은 설정 \\(T)이 최적이 아님을 핵심 관찰한다. S\\)를 설정하면 성능이 떨어지는 반면 \\(S=T\\)를 설정하면 훈련 효율이 향상되어 훈련 내내 학습률이 높게 유지되지 않아야 함을 확인할 수 있다. 이러한 관찰을 재현하기 위해 0.036B 모델에 대한 실험을 수행한다. 부록 B.1에 표시된 공식에 따라 \\(Cosine(T)\\) 및 \\(CosineLoop(T)\\) LRS를 시도합니다. 결과는 그림 4에서 볼 수 있습니다. 훈련 단계가 \\(S=20N,40N,60N,80N\\)일 때 가장 낮은 손실은 항상 \\(Cosine(T)\\) 여기서 \\(T=S\\)에 의해 달성됨을 알 수 있습니다. \\(T<S\\)와 \\(T>S\\) 모두 최적이 아닙니다.\n' +
      '\n' +
      '우리는 다음과 같은 두 가지 이유로 인해 코사인 LR이 \\(T=S\\)일 때 예외적으로 잘 수행된다고 가정한다. (1) \\(T=S\\)가 있는 코사인 LRS는 \\(T<S\\)에 비해 _높은 학습률_ 훈련 기간이 길며 선형 LRS와 같은 다른 LRS에 비해 이 높은 학습률은 모델이 더 나은 전역 최적을 찾는 데 도움이 될 수 있다. (2) \\(T=S\\)가 있는 코사인 LRS는 \\(T)가 있는 코사인 LRS에 비해 더 철저한 학습률 감쇠 단계를 갖는다. S\\) 및 Constant LRS. 이 학습 속도 감쇠는 모델이 더 나은 국소 최적을 찾을 수 있게 하는 고유한 훈련 역학을 포함할 수 있다.\n' +
      '\n' +
      '### Wsd Lrs\n' +
      '\n' +
      '이상의 관점에 비추어 훈련 단계를 고학습률 단계와 학습률 저하 단계로 명시적으로 구분할 것을 제안한다. Warmup-Stable-Decay(WSD) LRS라고 합니다. 특히 WSD LRS는 워밍업 단계(종료 단계를 \\(W\\)로 표시), 안정적인 훈련 단계(종료 단계를 \\(T\\)로 표시), 나머지 감쇠 단계의 세 단계로 구성된다. WSD의 함수 형태는:\n' +
      '\n' +
      '\\[WSD(T;s)=\\left\\{\\begin{array}{rl}\\frac{s}{W}\\eta,&s<W\\\\ \\eta,&W<s<T\\\\ f(s-T)\\eta,&T<s\\end{array}\\right. \\tag{1}\\]\n' +
      '\n' +
      '여기서, \\(0<f(s-T)\\leq 1\\)는 \\(s\\)에 대한 감소 함수이고, \\(\\eta\\)는 최대 학습률이다. 일반적으로 워밍업 단계가 충분한 한 성능에 거의 영향을 미치지 않으므로 후속 논의에서 \\(W\\)를 생략한다.\n' +
      '\n' +
      '### Experiments\n' +
      '\n' +
      '다음으로 WSD LRS의 몇 가지 실험 결과를 제시한다.\n' +
      '\n' +
      '**손실 손실 단계에서는 극적으로 감소 합니다.* * 0.036B 모델에서 WSD LRS를 시도 합니다. 그림 5에서 보는 바와 같이 붕괴 단계에서는 학습률이 감소하기 시작하면서 손실이 상당한 급격한 감소를 경험하고 단계 \\(T=S\\)에서 코사인 LRS와 같거나 더 낮아질 정도로 빠르게 감소한다. 동시에 붕괴 전에 모델을 재사용하고 이전의 높은 학습률로 훈련을 계속할 수 있다. 더 많은 훈련 단계 \\(S^{\\prime}\\) 후에, 우리는 또한 \\(Cosine(S^{\\prime})\\)에서 코사인 LRS와 동일한 손실을 달성하기 위해 어닐링을 수행할 수 있다. 이것은 훈련 단계가 안정 훈련 단계와 붕괴 단계로 명시적으로 분할될 수 있다는 우리의 가정을 검증한다.\n' +
      '\n' +
      '**10% 단계는 충분합니다.* * 두 단계 훈련 관점에서 붕괴 단계를 단축하면 안정적인 훈련의 서로 다른 모델 체크포인트의 빠른 테스트에 큰 도움이 됩니다. 따라서 우리는 동일한 안정적인 훈련 체크포인트에서 시작하여 감쇠 단계가 다른 실험을 수행한다. 그림 5에서도 볼 수 있듯이 40N, 60N, 80N 훈련 데이터의 세 가지 안정적인 훈련 체크포인트 중 총 토큰의 10%의 감쇠는 최상의 결과를 얻기에 충분하지만 총 토큰의 2.5%의 감쇠는 부족한다. 따라서, 후속 훈련 실험에서, 우리는 완전한 수렴을 보장하기 위해 약 10%의 감쇠를 사용한다.\n' +
      '\n' +
      '**WSD LRS를 사용한 효과적인 데이터 크기 조정** WSD LRS를 사용하면 LM을 극한 수렴으로 지속적으로 훈련할 수 있습니다. 고정 크기 모델을 융합으로 훈련하는 가능성을 추가로 입증하기 위해 40N 데이터를 사용하여 0.036B LM과 0.17B 모델을 지속적으로 훈련하는 것을 비교한다. 그림 6에서 녹색 선은 서로 다른 안정적인 훈련 토큰으로 훈련된 0.036B 모델을 나타낸다. 0.036B 시리즈의 마지막 포인트가 친칠라 최적(Hoffmann et al., 2022)보다 더 많은 토큰으로 훈련되었음에도 불구하고 여전히 성능 개선의 여지가 있다.\n' +
      '\n' +
      '이 고정된 크기의 LM을 지속적으로 훈련하는 것의 한계를 찾기 위해, 우리는 연속 훈련 동안 모델의 최적 성능이 계산에 따라 어떻게 변하는지를 추정한다. 최적의 성능으로 훈련 토큰의 손실 \\(D\\)은 \\(WSD(D,0.1D)\\)에 의해 달성됨을 의미한다. 일련의 \\(D\\)로 손실들은 최적의 손실 포락선을 형성할 것이다. 손실 포락선의 함수 형태에 대한 불확실성으로 인해, 우리는 (1) 지수: \\(L(C)=\\alpha e^{-\\beta C}+L_{0}\\)와 (2) 멱함수: \\(L(C)=\\beta C^{-\\alpha}+L_{0}\\)의 두 가지 피팅 공식을 시도한다. 두 함수에 대한 피팅 결과는 부록 B.2에 있다. 우리는 멱법칙 형태가 더 잘 맞는다는 것을 발견한다(Cosine LRS와 유사함(Kaplan et al., 2020)). 그림 6에서 적합 곡선은 녹색 점선으로 표시되어 있다. 이러한 고정 크기 모델의 지속적인 훈련 효과를 직관적으로 추정하고 이해하기 위해 그림 6에 분홍색으로 표시된 \\(WSD(40N,4N)\\)로 0.17B 모델을 훈련시켰다. 0.036B 모델은 추론 계산(Sardana and Frankle, 2023)을 많이 절약하면서 훈련 컴퓨팅에서 허용 가능한 증가(\\(\\sim4\\)배)와 0.17B 모델의 성능을 일치시킬 수 있음을 알 수 있다(\\(\\sim5\\)배) 추론 호출당 절약(\\(\\sim5\\)배). 이는 더 나은 추론-컴퓨팅-최적 설정 Sardana and Frankle(2023).\n' +
      '\n' +
      '### Decay Stage 분석\n' +
      '\n' +
      '이 절에서는 체크포인트 업데이트의 프리즘과 기울기 정보를 통해 감쇠 단계의 손실 감소에 대한 간략한 분석을 제공한다. MiniCPM-2.4B(6절에 소개됨)의 모든 가중치 행렬에서 최대 가중치 요소 업데이트 \\(max_{ij}(W_{ij}^{(t+1)}-W_{ij}^{(t)})\\)를 계산한다. 도 7에 도시된 바와 같이, 업데이트들은 학습 레이트의 크기와 강건한 상관관계를 나타낸다. 25번째 계층의 두 개의 서브 모듈(gate_proj 및 q_proj) 모듈의 예시에도 불구하고, 이 패턴은 네트워크 내의 모든 계층 및 서브 모듈에 걸쳐 널리 퍼져 있다. 이 관찰은 사소하지 않을 수 있으며, 모델 체크포인트는 학습 속도의 붕괴 이전에 상당한 업데이트를 경험하지만 손실은 최소 감소를 나타낸다. 반대로 부패 단계에서는 덜 뚜렷한 체중 변화에도 불구하고 손실이 가속화된다.\n' +
      '\n' +
      '구배 데이터의 추가 조사는 0.2B 모델을 훈련하고 모든 단계 구배 정보를 꼼꼼하게 기록하고 연속 단계 간의 차이를 평가하여 2차 구배 정보의 근사치를 제공함으로써 수행된다. 단계 \\(t\\)에서 기울기는 평탄화된 벡터 \\(\\mathbf{g}^{(t)}\\로 처리되며, 단계 \\(t\\)와 \\(t+1\\) 사이의 매개변수(벡터 \\(\\mathbf{x}^{(t)}\\) 업데이트는 \\(\\mathbf{v}^{(t)}=\\mathbf{x}^{(t+1)}-\\mathbf{x}^{(t)}\\로 처리된다. Gradient norm은 \\(\\|\\mathbf{g}^{(t)}\\|\\)의 \\(L2\\) norm을 취하고, Gradient inner product는 \\(\\mathbf{g}^{(t+1)}\\cdot\\mathbf{g}^{(t)}\\)의 cosine은 \\(\\frac{\\mathbf{g}^{(t+1)},\\mathbf{g}^{(t)}}{\\|\\mathbf{g}^{(t+1)}\\|\\|\\mathbf{g}^{(t)}\\|\\). 최적화 과정을 고차원 매니폴드를 지나는 궤적으로 영상화하면, 궤적에 따른 1차 방향미분은 \\(D_{1}=\\frac{\\mathbf{g}^{(t+1)},\\mathbf{v}^{(t)}}{\\|\\mathbf{v}^{(t)}\\|}\\로 계산되며, 2차 방향미분은 \\(D_{2}=\\frac{(\\mathbf{g}^{(t+1)}-\\mathbf{g}^{(t)}),\\mathbf{v}^{(t)}}{\\|\\mathbf{ v}^{(t)}\\|^{2}}\\로 계산된다. \\ (D_{1},D_{2}\\)는 궤적의 손실 곡률을 대략적으로 추정할 수 있다. \\(K=\\frac{|D_{2}|}{(1+D_{1}^{2})^{\\frac{1}{2}}}\\). 시간이 지남에 따라 이러한 통계의 결과는 그림 8에 나와 있다. 우리는 기울기 규범이 붕괴 단계에서 감소한다는 것을 알 수 있으며, 이 단계가 시작될 때 기울기 사이의 코사인은 주로 양의 값을 가정하며, 이는 붕괴 단계에서 모델 매개변수가 단계에 걸쳐 일관된 변화를 겪음을 시사한다. 방향성 도함수와 관련하여, 1차 방향 도함수는 각 단계에 따라 기하급수적으로 감소하여 학습률과 밀접하게 정렬되는 반면, 2차 방향 도함수는 크기가 약간 증가하는 것이 두드러진다. 손실 함수의 곡률은 또한 크기만큼 증가하여 국소 최적에 대한 근접성을 나타낸다. 이러한 발견은 잠재적으로 미래 탐색을 위해 예약된 주제인 형상 최적화 공간에 대한 더 깊은 통찰력을 제공한다.\n' +
      '\n' +
      '### WSD LRS를 사용 하 여 크기 조정 법칙 측정\n' +
      '\n' +
      '규모 조정법은 LLM 개발에 있어 기본 지침 원리 역할을 한다. 이러한 스케일링 법칙은 모델 시리즈 전반에 걸친 다양한 구성으로 인해 특정 계수의 변동성을 나타내지만, 계산 최적 데이터 대 모델 비율은 손실의 특정 값을 "한계화"하는 상이한 스케일링 법칙 함수에 걸쳐 의미 있는 메트릭으로 남아 있다. 이 비율과 관련하여 Kaplan 등(2020)은 모델 규모의 10배 증가는 데이터 규모의 단일배 증가와 동일해야 한다고 가정한다. 반대로, Hoffmann et al.(2022)\n' +
      '\n' +
      '그림 8: WSD LRS를 사용한 0.2B 모델의 훈련에 대한 기울기 통계량. 지수 붕괴 단계는 8000단계에서 시작된다.\n' +
      '\n' +
      '그림 7: 검문소의 최대 차이입니다.\n' +
      '\n' +
      '모델 크기와 데이터 크기 간에 동일한 크기 조정 비율에 대해 항의합니다. 더욱이, LLama 2(Touvron et al., 2023)와 같은 현재 모델은 Hoffmann et al.(2022)이 주장하는 것보다 훨씬 더 많은 데이터를 학습하여 여전히 상당한 성능 이득을 얻는다. 더 높은 데이터 대 모델 비율을 나타냅니다.\n' +
      '\n' +
      '이러한 해결되지 않은 불확실성은 전통적인 스케일링 실험에서 다양한 크기 및 데이터 크기의 여러 모델을 훈련하는 데 내재된 문제에서 비롯된다. 기존에는 하나의 데이터 크기에 대해 하나의 모델 크기를 훈련하는 평균 비용이 \\(C\\)일 경우, \\(m\\) 모델 크기와 \\(m\\) 데이터 크기로 스케일링 실험을 수행하는데 대략 \\(O(m^{2})C\\)가 소요된다.\n' +
      '\n' +
      '본 절에서는 선형 비용(\\(O(mC))을 갖는 스케일링 법칙을 탐색하기 위한 효과적인 접근법으로서 WSD 스케줄러의 활용을 소개한다. WSD 스케줄러는 모든 단계에서 안정된 단계의 체크포인트로부터 감쇠된 후 코사인 LRS의 최적 손실에 도달하는 장점이 있기 때문에, 우리는 이제 모델들을 처음부터 다른 양의 토큰들로 재트레이닝하지 않고 최적의 스케일링 특성들을 정확하게 측정할 수 있고, 따라서 스케일링 법칙 측정이 데이터 축을 따라 훨씬 더 효율적이다.\n' +
      '\n' +
      '안정적인 훈련 단계에서 \\(10N\\)에서 \\(60N\\) 데이터의 체크포인트부터 시작하여 각각 6개의 감쇠 모델을 갖는 0.04B에서 2B까지의 6가지 크기의 SLM을 훈련시켜 데이터와 모델 축을 따라 스케일링 법칙을 측정한다. 최종 손실은 5개의 보류된 평가 데이터 세트에 대해 평가된다. 모델이 다른 토큰화기를 사용할 때 손실을 잠재적으로 비교하기 위해 Achiam 등(2023)에 따라 토큰 수 대신 바이트 수만큼 손실의 평균을 취한다. 데이터 크기와 모델 크기의 각 쌍의 최종 손실은 그림 17의 파란색 선에 나와 있다.\n' +
      '\n' +
      '그런 다음 스키피 곡선 적합 함수를 사용하여 Hoffmann 등(2022)에 따라 모델 크기 \\(N\\) 및 데이터 크기 \\(D\\)에 손실을 적합시킵니다.\n' +
      '\n' +
      '\\[L(N,D)=C_{N}N^{-\\alpha}+C_{D}D^{-\\beta}+L_{0} \\tag{2}\\]\n' +
      '\n' +
      '각 데이터세트와 각 체크포인트에 대한 데이터 축을 따라 피팅된 곡선은 그림 17의 주황색 선으로 표시된다. 그런 다음 고정된 계산량이 주어진 최적의 모델 크기 \\(N_{opt}\\), 데이터세트 크기 \\(D_{opt}\\), 다음과 같이 \\(C=6ND\\)(Rae et al., 2021)를 갖는다.\n' +
      '\n' +
      '\\[\\frac{N_{opt}}{D_{opt}}=K^{2}\\left(\\frac{C}{6}\\right)^{\\eta}, \\tag{3}\\]\n' +
      '\n' +
      '여기서 \\(K=(\\frac{\\alpha C_{N}}{\\beta C_{D}})^{\\frac{1}{\\alpha+\\beta}}\\), \\(\\eta=\\frac{\\beta-\\alpha}{\\alpha+\\beta}\\). \\(N_{opt}\\)의 유도는 식 2에서 \\(D\\)를 \\(\\frac{C}{6N}\\)로 대입하고 \\(C\\)가 주어진 \\(L(N)\\)을 최소화함으로써 Hoffmann et al.(2022)를 밀접하게 따른다. \\(D_{opt}\\)에 대해서도 유사한 방식을 채택하고 있다. 식 3에서 \\(\\alpha=\\beta\\), \\(N_{opt}/D_{opt}\\)가 상수일 때 Hoffmann et al. (2022)의 주장을 지지하고, \\(\\alpha<\\beta\\)일 때 매개변수 스케일링에 대해 더 강조해야 하며(Kaplan et al., 2020), 그 반대도 마찬가지다.\n' +
      '\n' +
      '실험에서 손실과 \\(N\\), \\(D\\) 사이의 적합 관계는 그림 10의 등고선도에 나와 있다. 적합 스케일링 법칙 방정식은 각 서브플롯의 첫 번째 텍스트 상자에 나와 있다. 우리는 모든 평가 말뭉치에서 \\(\\beta<\\alpha\\)가 있음을 알 수 있다. 보다 구체적으로, 평균적으로 \\(\\alpha=0.29\\), \\(\\beta=0.23,K^{2}=0.01,\\eta=-0.10\\)를 갖는다. 이 결과는 \\(\\alpha\\)가 \\(\\beta\\)보다 약간 크기 때문에 계산 규모로서 Hoffmann 등(2022)과 일치하는 모델 스케일링보다 데이터 스케일링을 약간 강조해야 함을 보여준다.\n' +
      '\n' +
      '구체적인 데이터 대 모델 비율 \\(\\frac{D_{opt}}{N_{opt}}\\)의 경우, 계산 \\(\\frac{D_{opt}}{N_{opt}}\\)과 계산 \\(C\\)의 추세가 우리의 계산과 그들의 계산 사이에 정렬되어 있음에도 불구하고, 우리의 계산 최적 체제와 Hoffmann et al.(2022) 사이에 큰 격차가 있음을 알 수 있다. 구체적으로, 데이터 크기는 호프만 등(2022)의 20배에 비해 평균 192배가 되어야 한다. 우리는 이것이 섹션 4.3 및 그림 6의 관찰과 일치한다는 점에 주목한다.\n' +
      '\n' +
      '친칠라 최적 \\(\\frac{N_{opt}}{D_{opt}}\\)의 큰 편차에 대해, 우리는 그들의 스케일링 실험이 매우 최근의 구성이 아님을 주목한다. Llama2(Touvron et al., 2023)와 같은 최근의 구성과 비교하기 위해 부록 그림 18의 Llama2 논문(왼쪽 부분)에서 학습 손실 데이터를 추출하고 그림 18의 오른쪽 부분을 사용하여 그들의 논문에서 계산 최적 \\(\\frac{D_{opt}}{N_{opt}}}\\)을 추정한다. 그들은 코사인 LRS를 사용하기 때문에, 그림 18의 오른쪽 그림에서 훈련하는 동안 오목 곡선으로 묘사되는 훈련의 중간에서 손실이 최적이 아니다. WSD LRS를 사용했다면 최적의 손실 포락선을 추정하기 위해 오목 부분을 직선으로 채운다. 그 후, 계산 모델 크기는 대략 모델의 손실 곡선이 더 큰 모델의 손실 곡선과 교차하려는 체제여야 한다. 이러한 직관에 의해 13B 모델은 \\(10^{5}\\) EFlops (\\(10^{1}\\)s Flops)에서 34B 모델과 교차하게 되고, 34B 모델은 \\(5\\times 10^{5}\\) EFlops에서 70B 모델과 교차하게 된다. 따라서 \\(\\frac{D_{opt}}{N_{opt}}}\\)는 대략 \\(\\frac{5\\times 10^{5}}{6\\times 34^{2}}\\sim\\frac{10^{5}}{6\\times 13^{2}}\\)로 추정된다. 따라서 이 근사 비교에서 데이터 모델 비율은 우리와 더 가깝다. 그리고 우리의 구성은 이전 모델에 비해 더 적은 모델로 더 많은 데이터를 흡수할 수 있다. 그러나 위의 추정치는 대략적인 것에 불과합니다.\n' +
      '\n' +
      '데이터 대 모델 비율이 크다는 것은 우리가 이전에 생각했던 것보다 더 적은 모델에 더 많은 데이터를 흡수할 수 있다는 것을 의미하며, 이는 추론 및 배치에 더 효율적이다. 그리고 WSD LRS는 더 많은 연구자들이 적은 노력으로 \\(L(N,D)\\)을 탐색하고 LLMs에서 관계를 더 명확하게 만드는 데 도움이 되기를 바란다.\n' +
      '\n' +
      '## 5단계 사전 훈련 전략\n' +
      '\n' +
      '전형적으로, LLMs에 후속하는 명령의 훈련은 사전 훈련 단계 및 감독 미세 조정(supervised fine-tuning; SFT) 단계를 포함한다(Zhang 등, 2023; Wei 등, 2021). 사전 학습 단계에서는 데이터가 대규모의 레이블링되지 않은 데이터로 구성되는 반면, SFT 단계에서는 고품질의 레이블링된 데이터가 최적화 대상이 된다. WSD LRS의 붕괴 단계에서 관찰된 현저한 손실 감소에 비추어, 우리는 이 단계에서 고품질 라벨링된 데이터의 통합이 이중 이점을 제공한다고 가정한다:\n' +
      '\n' +
      '* SFT 단계 외에도 어닐링 단계에서 이 데이터를 도입하면 보다 포괄적인 모델 학습이 촉진됩니다. 구체적으로, 사전 트레이닝 데이터 분포보다는 SFT 데이터 분포와 관련하여 보다 현저한 손실 감소를 용이하게 한다. 이 접근법은 실제 사용자 시나리오와 더 일치한다.\n' +
      '* 전체 사전 학습 프로세스에 걸쳐 고품질 데이터의 균일한 분포와 달리 이 방법은 데이터에 집중하고 지속적인 사전 학습을 유지하여 학습을 강화합니다. 훈련 단계를 미리 결정하지 않으면 진행 중인 사전 훈련 프로세스 전반에 걸쳐 작은 데이터 세트를 반복하여 부정적인 영향을 미칠 수 있다.\n' +
      '\n' +
      '이 두 가지 가설을 바탕으로 다음과 같은 훈련 전략을 제안한다: 사전 훈련 단계에서는 대규모의 거친 품질의 사전 훈련 데이터만 사용하며, 이는 풍부하고 더 많은 계산 자원이 제공될 때 지속적인 훈련을 지원할 수 있다. 어닐링 단계에서는 다양하고 고품질의 지식과 능력 중심의 SFT 데이터를 사전 훈련 데이터에 혼합한다.\n' +
      '\n' +
      '훈련 전략의 장점을 검증하기 위해 (A) 안정 단계에서 MiniCPM-2.4B의 중간 체크포인트와 (B) 안정 단계에서 MiniCPM-1.2B의 마지막 체크포인트를 사용하여 비교 실험을 수행한다. 구체적으로, 다음과 같이 비교한다:\n' +
      '\n' +
      '1. A-1: 2.4B 모델, 사전 트레이닝 데이터만을 사용한 감쇠, 이어서 4B 토큰 SFT.\n' +
      '2. A-2: 2.4B 모델, 앞서 언급한 고품질 데이터 라벨링되지 않은 데이터 및 사전 트레이닝 데이터에 혼합된 SFT 데이터를 사용한 붕괴, 또한 4B 토큰 SFT가 뒤따른다.\n' +
      '3. B-1: 1.2B 모델, 사전 트레이닝 데이터만을 사용한 감쇠, 이어서 6B 토큰 SFT.\n' +
      '4. B-2: 1.2B 모델, 사전 트레이닝 데이터만을 사용한 감쇠, 이어서 12B 토큰 SFT.\n' +
      '5. B-3: 1.2B 모델, 앞서 언급한 고품질 데이터 + 사전 트레이닝 데이터에 혼합된 SFT 데이터를 사용하여 어닐링한 후, 6B 토큰 SFT도 뒤따른다.\n' +
      '\n' +
      '실험 결과는 표 1과 같다. A-2와 A-1이 동일한 SFT 분포를 거쳤음에도 불구하고 붕괴 단계에 SFT 데이터를 추가하면 경계를 밀어내는 것을 알 수 있다. B-2와 B-3의 비교는 SFT만의 결핍이 SFT 단계의 부족한 훈련 토큰 때문이 아님을 보여준다.\n' +
      '\n' +
      '그 결과 감쇠 초기에 고품질 데이터를 도입하면 SFT 단계에서 단순히 추가하는 것보다 훨씬 더 높은 이점이 있음을 나타낸다. 그러므로 우리는\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c c c c c c c c} \\hline \\hline  & **C-Eval** & **CMMLU** & **MMLU** & **GSM8K** & **MATH** & **HumanEval** & **MBPP** \\\\ \\hline A-1 & 40.0 & 41.5 & 44.6 & 27.7 & 5.1 & 27.7 & 24.4 \\\\ A-2 & **52.6** & **51.1** & **50.9** & **42.3** & **5.4** & **30.4** & **30.3** \\\\ \\hline B-1 & 40.9 & 41.5 & 47.9 & 34.2 & 7.9 & 43.9 & 30.5 \\\\ B-2 & 41.2 & 42.0 & 47.9 & **34.4** & 7.3 & 43.9 & 29.8 \\\\ B-3 & **49.1** & **46.8** & **49.6** & 31.8 & **10.5** & **44.5** & **32.8** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 다양한 훈련 전략의 절제 연구.\n' +
      '\n' +
      '그림 9: WSD Scheduler(위)와 fitting scaling curve(아래)를 이용한 scaling 실험 결과. x축은 계산 Flops \\(C=6ND\\)이며, 각 선의 색상은 계산 Flops가 다른 동일한 모델을 나타낸다. 우리는 작은 모델이 플롭스가 작을 때 큰 모델보다 더 좋고 플롭스가 클 때 더 나쁘다는 것을 알 수 있다. 따라서 차이 크기의 모델은 계산 최적 체제 주변의 그림에서 서로 교차합니다.\n' +
      '\n' +
      '모델 능력의 전문화 및 향상은 쇠퇴 단계부터 시작되어야 한다고 권고한다.\n' +
      '\n' +
      '## 6 Model\n' +
      '\n' +
      '이 섹션에서는 앞서 언급한 관찰 및 기술을 집계하는 MiniCPM 모델을 도입하기 시작한다.\n' +
      '\n' +
      '### Model Details\n' +
      '\n' +
      '**어휘.** MiniCPM-2.4B의 경우 122,753개의 어휘 크기와 MiniCPM-1.2B의 경우 73,440개의 어휘 크기의 두 개의 토큰라이저를 사용합니다. 1.2B를 위한 작은 어휘는 많은 성능을 해치지 않으면서 효율성을 선호한다. 토큰라이저에 대한 자세한 내용은 부록 C에 나와 있습니다.\n' +
      '\n' +
      '**공유 입력-출력 계층.** SLM의 경우 임베딩은 큰 매개 변수 공간을 차지합니다. 모델 파라미터를 작게 하기 위해 MiniCPM-2.4B와 MiniCPM-1.2B 모두에 대해 Embedding Sharing 기법을 사용한다.\n' +
      '\n' +
      '**Deep-and-thin Network.** MiniCPM-1.2B를 교육하기 전에 MiniCPM-2.4B를 교육합니다. MiniCPM-2.4B를 훈련할 때, 우리는 Phi-2(Javaherippi and Bubeck, 2023)에 비해 더 깊고 얇은 아키텍처를 채택한다(32개 층 대비 40개 층). 최근 Liu et al.(2024)\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c c} \\hline \\hline Model & N (B) & \\(d_{m}\\) & \\(d_{ff}\\) & \\(d_{h}\\) & \\(n_{q}\\) & \\(n_{ko}\\) & \\(L\\) & Batch size (M) & Tokens (T) \\\\ \\hline MiniCPM-1.2B & 1,247,442,432 & 1,536 & 3,840 & 64 & 24 & 8 & 52 & 2M \\(\\rightarrow\\) 4M & 1.1T \\\\ MiniCPM-2.4B & 2,442,057,984 & 2,304 & 5,760 & 64 & 36 & 36 & 40 & 4M & 1.1T \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: MiniCPM을 위한 모델 구성들. N(B), \\(d_{m}\\), \\(d_{ff}\\), \\(d_{h}\\), \\(n_{q}\\), \\(n_{ko}\\), \\(L\\), Batch size(M), Tokens(T)는 모델의 비 임베딩 파라미터 수, 모델 은닉 차원, 피드포워드 레이어 병목 차원, 어텐션 헤드 차원, 쿼리 수, 숫자 키/값, 레이어 수, 트레이닝 배치 크기, 총 트레이닝 토큰을 나타낸다.\n' +
      '\n' +
      '그림 10: WSD Scheduler를 이용한 스케일링 실험의 적합도 결과. 수평선의 검은색 점은 동일한 모델 크기 내에서 서로 다른 계산에서 감쇠된 체크포인트를 나타냅니다.\n' +
      '\n' +
      'SLM에 대해 깊고 얇은 네트워크를 훈련할 것을 제안하며, 이는 우리의 관점과 일치한다. 따라서 MiniCPM-1.2B를 위한 아키텍처를 더 깊고 얇게 만든다.\n' +
      '\n' +
      '**그룹 쿼리 주의력** 주의 계층에서 수정 없이 MiniCPM-2.4B를 학습합니다. Group Query Attention (Ainslie et al., 2023)을 MiniCPM-1.2B에 적용한 반면, Liu et al.(2024)에서 영감을 받아 파라미터 수를 줄였다.\n' +
      '\n' +
      '### Training Stages\n' +
      '\n' +
      'MiniCPM 기반 모델의 전체 훈련은 안정 훈련 단계, 붕괴 단계, SFT 단계의 세 단계로 구성된다(Zhang et al., 2023; Wei et al., 2021).\n' +
      '\n' +
      '**안정적인 교육 단계.** 1T 데이터(데이터 배포는 섹션 11 참조)를 활용하며 대부분의 데이터는 열린 데이터 세트에서 조달됩니다. 모델 풍동 실험 동안 발견된 최적 구성인 WSD LRS를 사용하여 배치 크기가 393만이고 최대 학습률이 0.01이다.\n' +
      '\n' +
      '**디케이 단계.** 사전 훈련 데이터와 고품질 SFT 데이터를 혼합하여 사용합니다. WSD 스케줄러의 특정 annealing 형태는 exponential annealing, 즉 \\(f(s-T)=\\times 0.5^{(s-S)/T}\\를 사용하며, 이때 \\(T\\)는 5000 단계(20B 토큰)로 설정된다.\n' +
      '\n' +
      '**SFT 단계.** 별도의 SFT 단계를 수행해야 합니다. 사전 훈련 데이터를 제외한 어닐링 단계와 유사한 SFT 데이터를 활용하고 약 60억 토큰으로 훈련한다. SFT에 대한 학습률은 어닐링이 끝날 때의 학습률과 정렬되며 지수 감쇠를 갖는 WSD 스케줄러도 사용된다.\n' +
      '\n' +
      '### Training Data Distribution\n' +
      '\n' +
      '우리는 그림 11에 우리의 훈련 데이터 분포를 소개한다. 그림에서 중국 말뭉치의 CommonCrawl_Chn은 CommonCrawl 원시 말뭉치에서 파생되어 철저한 청소를 거친다. 돌마(Soldaini et al., 2024), C4(Raffel et al., 2019), 및 Pile(Gao et al., 2020; Biderman et al., 2022)은 영어 코퍼라이다. 그들은 MinHash 알고리즘을 사용하여 중복 제거된 내부 말뭉치와 말뭉치 전체에 걸쳐 있다(Broder, 1997). 코드 사전 훈련 데이터는 스택(Kocetkov et al., 2022) 및 StarCoder Li et al.(2023)을 포함하며, 내부 중복 제거 및 교차 중복 제거가 있다. 붕괴 단계에서, 데이터 혼합물은 UltraChat(Ding et al., 2023), SlimOrca(Lian et al., 2023;3b), OssInstruct(Wei et al., 2023), EvolInstruct(Xu et al., 2023)를 포함하는 더 다양한 데이터 및 독점 데이터를 포함한다. 접미사 SFT가 있는 데이터는 렉트코드 문제, 유치원 12학년(K12) 교과서 및 문제 등을 포함한 당사의 독점 데이터입니다.\n' +
      '\n' +
      '도 11: 상이한 트레이닝 단계들의 데이터 혼합. 안정 단계는 왼쪽에, 붕괴 단계는 오른쪽에 표시됩니다.\n' +
      '\n' +
      '### Training Loss\n' +
      '\n' +
      'C4 데이터 세트의 전반적인 훈련 손실은 그림 12에 나와 있다. 우리는 예비 실험에서 예상한 바와 같이 감쇠 단계에서 손실이 급격히 감소한다는 것을 알 수 있다. 우리가 지수 감쇠를 사용하기 때문에, 학습률이 최대 학습률의 10% 이하로 떨어진 후에도 손실은 여전히 감소한다. 그러나 붕괴 단계 이후에 SFT 모델을 계속하기 때문에 최종 체크포인트를 활용하지 않는다. 우리가 미세 조정한 체크포인트는 짙은 녹색 세그먼트의 마지막 체크포인트에 표시된다. MiniCPM-1.2B의 첫 번째 하락은 배치 크기를 확대한 결과이며, 이는 학습률을 감소시키는 것과 유사한 효과를 가질 수 있다(Smith et al., 2017).\n' +
      '\n' +
      '### Evaluation\n' +
      '\n' +
      '전체적인 평가는 오픈소스 툴인 UltraEval3를 활용한다. UltraEval은 기초 모델의 성능을 평가하기 위한 오픈소스 프레임워크이다. 가볍고 사용자 친화적인 평가 시스템을 제공하여 주류 대형 모델에 대한 성능 평가를 지원하고 모델 훈련 팀의 신속한 평가 요구를 충족합니다. 기본 추론 및 가속은 오픈 소스 프레임워크 vLLM(Kwon et al., 2023)을 사용하고, 데이터세트는 일반적으로 사용되는 데이터세트: 영어 지식에 대해 MMLU(Hendrycks et al., 2020), 중국 지식에 대해 CMMLU(Li et al., 2024) 및 C-Eval(Huang et al., 2024), 코딩에 대해 HumanEval(Chen et al., 2021) 및 MBPP(Austin et al., 2021), 수학에 대해 GSM8K(Cobbe et al., 2021) 및 MATH(Hendrycks et al., 2021), 및 수학에 대해 HellaSwag(Zellers et al., 2019), ARC-e(Clark et al., 2018), 커먼센스 추론에 대해 ARC-c(Clark et al., 2018), 논리 추론에 대해 BBH(Suzgun et al., 2022)를 포함한다.\n' +
      '\n' +
      '각주 3: [https://ultraeval.openbmb.cn/home](https://ultraeval.openbmb.cn/home)\n' +
      '\n' +
      '대형 모델에 대한 평가 표준화의 어려움과 많은 모델의 평가에 대한 공개 프롬프트 및 테스트 코드의 부족으로 인해 다양한 모델 유형에 맞게 평가 방법을 적응시키기 위해 최선을 다하고 있다. 구체적으로 테스트 중 표준화된 입력 프롬프트에서 시작하여 각 모델의 적절한 입력 출력 템플릿에 따라 조정한다. 평가 스크립트와 프롬프트는 또한 리포지토리의 오픈 소스이며 개발자가 평가 방법을 지속적으로 개선하는 것을 환영합니다.\n' +
      '\n' +
      'QA 작업(ARC-e, ARC-c, HellaSwag)을 테스트할 때 일반적으로 두 가지 접근법이 사용된다. 첫 번째는 Perplexity(PPL)를 사용하는 것이다: 우리는 각 옵션을 질문의 연속으로 확장하고 옵션의 PPL을 선택 기준으로 사용한다. 두 번째는 직접 생성으로 모델이 답변 옵션을 직접 출력합니다. 이 두 가지 방법을 사용하여 얻은 결과에서 상당한 차이를 관찰한다. MiniCPM은 직접 생성 및 PPL 테스트에서도 유사하게 수행되며, 직접 생성에서는 더 나은 성능을 보인다. 반면에, 미스트랄-7B-v0.1은 PPL 테스트에서 더 나은 성능을 나타내지만 직접 생성에서는 더 낮은 성능을 나타낸다. 이러한 현상을 해결하기 위해 각 모델에 대한 점수를 보고할 때 가장 높은 점수를 산출하는 평가 방법에서 점수를 채택하여 비교의 공정성을 보장한다.\n' +
      '\n' +
      '그림 12: MiniCPM-1.2B(왼쪽) 및 MiniCPM-2.4B(오른쪽)에 대한 C4 데이터 세트의 손실 곡선입니다. 손실 곡선의 꼬리에 있는 연한 녹색 세그먼트는 나머지 붕괴 과정을 나타내며, 이는 출시된 버전의 MiniCPM에서는 활용되지 않는다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline\n' +
      '**Model** & **C-Eval** & **CMMLU** & **MMLU** & **HumanEval** & **MBPP** & **GSM8K** & **MATH** \\\\ \\hline Llama2-7B & 32.42 & 31.11 & 44.32 & 12.20 & 27.17 & 13.57 & 1.80 \\\\ Qwen-7B & 58.96 & 60.35 & 57.65 & 17.07 & 42.15 & 41.24 & 5.34 \\\\ Deepsek-7B & 42.82 & 44.45 & 47.82 & 20.12 & 41.45 & 15.85 & 1.53 \\\\ Mistral-7B & 46.12 & 42.96 & 62.69 & 27.44 & 45.20 & 33.13 & 5.00 \\\\ Gemma-7B & 42.57 & 44.20 & 60.83 & 38.41 & 50.12 & 47.31 & 6.18 \\\\ \\hline Llama2-13B & 37.32 & 37.06 & 54.71 & 17.07 & 32.55 & 21.15 & 2.25 \\\\ MPT-30B & 29.34 & 32.09 & 46.56 & 21.95 & 35.36 & 10.31 & 1.56 \\\\ Falcon-40B & 40.29 & 41.57 & 53.53 & 24.39 & 36.53 & 22.44 & 1.92 \\\\ \\hline TinyLlama-1.1B & 25.02 & 24.03 & 24.3 & 6.71 & 19.91 & 2.27 & 0.74 \\\\ Qwen-1.8B & 49.81 & 45.32 & 43.37 & 7.93 & 17.8 & 19.26 & 2.42 \\\\ Qwen1.5-1.8B & **55.00** & 50.85 & 43.81 & 5.49 & 24.82 & 26.16 & 3.25 \\\\ Gemini Nano-3B & - & - & - & - & 27.20 & 22.80 & - \\\\ StableLM-Zephyr-3B & 30.34 & 30.89 & 45.90 & 35.37 & 31.85 & 52.54 & 12.12 \\\\ Phi-2(2B) & 23.37 & 24.18 & 52.66 & 47.56 & **55.04** & **57.16** & 3.50 \\\\ Geamma-2B & 29.26 & 28.56 & 38.49 & 24.39 & 29.74 & 16.83 & 3.34 \\\\ \\hline\n' +
      '**MiniCPM-1.2B** & 49.14 & 46.81 & 49.63 & 44.51 & 32.75 & 31.77 & 10.60 \\\\\n' +
      '**MiniCPM-2.4B** & 51.13 & **51.07** & **53.46** & **50.00** & 47.31 & 53.83 & **10.24** \\\\ \\hline\n' +
      '**Model** & **BBH** & **ARC-e** & **ARC-c** & **Hella5wag** & **Avg\\({}_{\\textbf{en}}\\)** & **Avg\\({}_{\\textbf{chn}}\\)** \\\\ \\hline Llama2-7B & 33.23 & 75.25\\({}^{\\dagger}\\) & 42.75 & 75.62\\({}^{\\ddagger}\\) & 35.40 & 36.21 & 31.77 \\\\ Qwen-7B & 37.75 & 83.42 & 64.76 & 75.32\\({}^{\\ddagger}\\) & 49.46 & 47.19 & 59.66 \\\\ Deepsek-7B & 33.38 & 74.58\\({}^{\\ddagger}\\) & 42.15\\({}^{\\ddagger}\\) & 75.45\\({}^{\\ddagger}\\) & 39.96 & 39.15 & 43.64 \\\\ Mistral-7B & 41.06 & 83.92 & 70.73 & 80.43\\({}^{\\ddagger}\\) & 48.97 & 49.96 & 44.54 \\\\ Geamma-7B & 39.19 & 89.35 & 76.79 & 79.47 & 52.22 & 54.18 & 43.39 \\\\ \\hline Llama2-13B & 37.92 & 78.87\\({}^{\\ddagger}\\) & 58.19 & 79.23\\({}^{\\ddagger}\\) & 41.48 & 42.44 & 37.19 \\\\ MPT-30B & 38.22 & 78.66\\({}^{\\ddagger}\\) & 46.08\\({}^{\\daggeragger}\\) & 79.72\\({}^{\\ddagger}\\) & 38.17 & 39.82 & 30.72 \\\\ Falcon-40B & 36.24 & 81.94\\({}^{\\ddagger}\\) & 57.68 & 83.26\\({}^{\\ddagger}\\) & 43.62 & 44.21 & 40.93 \\\\ \\hline TinyLlama-1.1B & 28.78 & 60.77\\({}^{\\ddagger}\\) & 28.15\\({}^{\\ddagger}\\) & 58.33\\({}^{\\ddagger}\\) & 25.36 & 25.55 & 24.53 \\\\ Qwen-1.8B & 29.07 & 63.97\\({}^{\\ddagger}\\) & 43.69 & 59.28\\({}^{\\ddagger}\\) & 34.72 & 31.87 & 47.57 \\\\ Owen1.5-1.8B & 28.82 & 64.86 & 45.56 & 59.39 & 37.09 & 33.57 & **52.93** \\\\ Gemini Nano-3B & 42.40 & - & - & - & - & - & - \\\\ StableLM-Zephyr-3B & 37.68 & 73.78 & 55.38 & 71.87\\({}^{\\ddagger}\\) & 43.46 & 46.32 & 30.62 \\\\ Phi-2(2B) & **43.39** & **86.11** & **71.25** & **73.07\\({}^{\\ddagger}\\)** & 48.84 & **54.42** & 23.78 \\\\ Gemma-2B & 30.93 & 74.33 & 40.70 & 69.51 & 35.10 & 36.47 & 28.91 \\\\ \\hline\n' +
      '**MiniCPM-1.2B** & 34.70 & 80.93 & 66.81 & **54.72** & 45.67 & 45.16 & 47.98 \\\\\n' +
      '**MiniCPM-2.4B** & 36.87 & 85.44 & 68.00 & 68.25 & **52.33** & 52.60 & 51.10 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: MiniCPM-2.4B 및 MiniCPM-1.2B의 벤치마크 점수(둘 다 RLHF가 없음). 두 개의 테이블은 수평으로 연속되어 있습니다. **Avg\\({}_{\\textbf{en}}\\)** 는 테이블의 모든 데이터 세트에 대해, **Avg\\({}_{\\textbf{chn}}\\)** 는 C-Eval 및 CMMLU의 평균이고, **Avg\\({}_{\\textbf{en}}\\)** 는 나머지 데이터 세트의 평균입니다. \\ (\\ddagger\\)는 PPL 메트릭을 사용하여 결과를 테스트한다는 것을 의미합니다. **굵은** 숫자는 SLM 중 가장 좋은 점수를 나타냅니다. Gemini Nano-3B의 결과는 Gemini et al.(2023)로부터 차용되었다.\n' +
      '\n' +
      '전체 평가 결과는 표 4에 나와 있다. 전반적으로 언급된 데이터 세트에 대해 몇 가지 관찰이 있다. (1) 평균적으로 MiniCPM-2.4B가 전체 SLM 중에서 가장 높은 순위를 차지하고 있다. (2) MiniCPM-2.4B는 영어에서는 Mistral-7B-v0.1과 유사하게 수행되지만 중국어에서는 Mistral-7B-v0.1보다 상당히 우수하다. (3) MiniCPM-2.4B는 MMLU, BBH 및 HellaSwag를 제외하고 Llama2-13B를 능가하는 반면 MiniCPM-1.2B는 HellaSwag를 제외하고 Llama2-7B를 능가한다. (4) 일반적으로 BBH는 다른 지식 지향 데이터 세트에 비해 LLMs보다 SLMs에 대해 더 어려워 추론 능력이 지식보다 모델 크기에 더 의존할 수 있음을 보여준다. (5) SLM 중 Phi-2 성능은 학술 지향 데이터 세트에서 MiniCPM과 동등하다. 이는 그들의 교육 데이터가 주로 교육 및 학술 시나리오를 강조하는 텍스트 북 스타일 데이터를 포함하기 때문일 수 있다. 사전 훈련 데이터는 더 많은 분포를 커버하기 때문에 MiniCPM이 부록 G에서 볼 수 있는 지식과 능력 커버리지에 더 좋다고 생각한다.\n' +
      '\n' +
      '## 7 MiniCPM Family\n' +
      '\n' +
      '이 섹션에서는 MiniCPM 기반 모델을 기반으로 하는 다른 모델을 소개한다. 구체적으로 MiniCPM 2.4B를 대상으로 RLHF 모델, long-context 모델, MoE 모델을 학습하였다.\n' +
      '\n' +
      '### MiniCPM-DPO\n' +
      '\n' +
      'SFT 이후 모델의 인간 선호도 정렬을 위해 DPO(Rafailov et al., 2024)를 사용한다. 이 단계 동안, UltraFeedback(Cui et al., 2023)이 일차 정렬 데이터세트로 활용되며, 모델의 코드 및 수학적 능력을 향상시키기 위해 독점 선호도 데이터세트가 구축된다. 학습률이 \\(1\\times 10^{-5}\\)인 DPO 훈련의 한 단계를 수행하고 미리 정의된 훈련 단계가 있기 때문에 코사인 LRS를 활용한다.\n' +
      '\n' +
      '선호도 정렬을 위해 DPO를 적용한 후, MTBench(Zheng et al., 2024)에 대한 모델의 점수는 SFT 이후 6.89에서 7.25로 증가하여 Llama2-70B-Chat과 같은 대형 모델도 능가하였다(도 13 참조). 그러나 우리는 또한 벤치마크에 대한 성과가 약간 손상된다는 것을 발견했는데, 이는 정렬세(Askell et al., 2021)로 알려져 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline\n' +
      '**Model** & **C-Eval** & **CMMLU** & **MMLU** & **HumanEval** & **MBPP** & **GSM8K** & **MATH** \\\\ \\hline ChatGLM2-6B & 52.05 & 49.21 & 45.77 & 10.37 & 9.38 & 22.74 & 5.96 \\\\ Mistral-7B-Instruct-v0.1 & 38.06 & 36.96 & 53.56 & 29.27 & 39.34 & 28.73 & 3.48 \\\\ Mistral-7B-Instruct-v0.2 & 42.55 & 41.92 & 60.51 & 36.59 & 48.95 & 40.49 & 4.95 \\\\ Qwon-7B-Chat & 58.57 & 57.23 & 56.03 & 15.85 & 40.52 & 42.23 & 8.3 \\\\ Yi-6B-Chat & 70.88 & 71.11 & 62.95 & 14.02 & 28.34 & 36.54 & 3.88 \\\\ Baichuan-27B-Chat & 53.28 & 53.50 & 53.00 & 21.34 & 32.32 & 25.25 & 6.32 \\\\ Deepseek-7B-chat & 46.95 & 49.72 & 51.67 & 40.85 & 48.48 & 48.52 & 4.26 \\\\ Llama2-7B-Chat & 34.54 & 32.64 & 47.64 & 14.02 & 27.40 & 21.15 & 2.08 \\\\ \\hline\n' +
      '**MiniCPM-2.4B-DPO** & 48.64 & 48.37 & 53.05 & 51.22 & 48.01 & 53.37 & 9.86 \\\\ \\hline\n' +
      '**Model** & **BBH** & **ARC-e** & **ARC-c** & **HellaSwag** & **Avg** & **Avg\\({}_{\\textbf{Gen}}\\)** & **Avg\\({}_{\\textbf{Gen}}\\)** \\\\ \\hline ChatGLM2-6B & 32.60 & 74.45 & 56.82 & 58.48\\({}^{\\ddagger}\\) & 37.98 & 35.17 & 50.63 \\\\ Mistral-7B-Instruct-v0.1 & 39.52 & 81.61 & 63.99 & 73.47\\({}^{\\ddagger}\\) & 44.36 & 45.89 & 37.51 \\\\ Mistral-7B-Instruct-v0.2 & 39.81 & 86.28 & 73.38 & 84.55\\({}^{\\ddagger}\\) & 50.91 & 52.83 & 42.24 \\\\ Qwon-7B-Chat & 37.34 & 64.44\\({}^{\\ddagger}\\) & 39.25\\({}^{\\ddagger}\\) & 74.52\\({}^{\\ddagger}\\) & 44.93 & 42.05 & 57.90 \\\\ Yi-6B-Chat & 37.43 & 84.89 & 70.39 & 74.60\\({}^{\\ddagger}\\) & 50.46 & 45.89 & 71.00 \\\\ Baichuan-27B-Chat & 37.46 & 79.63 & 60.15 & 69.23\\({}^{\\ddagger}\\) & 44.68 & 42.74 & 53.39 \\\\ Deepseek-7B-chat & 35.70 & 76.85 & 63.05 & 76.68\\({}^{\\ddagger}\\) & 49.34 & 49.56 & 48.34 \\\\ Llama2-7B-Chat & 35.54 & 74.28 & 54.78 & 75.65\\({}^{\\ddagger}\\) & 38.16 & 39.17 & 33.59 \\\\ \\hline\n' +
      '**MiniCPM-2.4B-DPO** & 36.22 & 85.02 & 68.17 & 65.67 & 51.60 & 52.29 & 48.51 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 더 큰 채팅 모델과 비교하여 MiniCPM-2.4B-DPO에 대한 벤치마크 점수.\n' +
      '\n' +
      '### MiniCPM-128K\n' +
      '\n' +
      '긴 컨텍스트를 포함하는 작업은 이러한 컨텍스트 내의 암시적 정보에 의존하여 SLM에 종종 없는 광범위한 지식의 필요성을 우회한다. 이 섹션에서는 MiniCPM-2.4B의 컨텍스트 길이를 4,096에서 128,000 토큰으로 확장하여 긴 컨텍스트를 효과적으로 처리하는 SLM의 기능을 설명한다.\n' +
      '\n' +
      '**초기화** 초기화를 위해 주로 긴 컨텍스트를 가진 학습에 필수적인 어휘 병렬성을 수용하기 위해 입력과 출력 간의 공유 임베딩을 사용하지 않도록 설정합니다. LM 헤드는 입력 임베딩으로부터 초기화된다.\n' +
      '\n' +
      '**훈련.** MiniCPM과 마찬가지로 MiniCPM-2.4B-128K는 WSD를 학습률 스케줄러로 활용하고 MiniCPM-2.4B의 안정적인 훈련 단계의 마지막 체크포인트를 재사용합니다. 훈련 데이터와 관련하여 6.3절에 자세히 설명된 데이터 세트 분포를 "짧은 데이터"와 "긴 데이터"로 분류한다. 우리는 책, 위키, 논문을 "긴 데이터"로, 다른 하나는 "짧은 데이터"로 분류한다. 훈련은 연속 훈련을 위해 44%의 긴 데이터와 56%의 짧은 데이터로 구성된다. 긴 컨텍스트의 확장을 위해 4K에서 32K 범위의 ABF(Adjusted Base Frequency)(Xiong et al., 2023)를 적용하고 NTK-Aware RoPE Scaling(bloc97, 2023)과 32K에서 128K까지의 커리큘럼 학습을 사용한다. 두 단계 모두 미래의 훈련을 포함한다. 또한, Yi Tech Report (Young et al., 2024) 및 Zebra (Song et al., 2023)에 표시된 바와 같이, 상황 인지 태스크에서 모델 성능을 크게 향상시키는 합성 긴 QA 데이터를 사용한다.\n' +
      '\n' +
      '**평가.** 긴 컨텍스트 평가를 위한 선구적인 벤치마크인 \\(\\infty\\)Bench(Zhang 등, 2024b)에서 MiniCPM-2.4B-128K를 평가합니다. \\(\\infty\\)Bench (Zhang et al., 2024b)의 태스크는 일반적인 검색 태스크를 넘어 확장되며, 긴 컨텍스트 추론으로 모델에 도전한다. 표 5에서 볼 수 있듯이 미스트랄-7B-인스트럭트-v0.2(ABF1000w)에서 유사한 결과를 달성하고 2.5배 더 작음에도 불구하고 ChatGLM3-6B-128K를 능가한다.\n' +
      '\n' +
      '### MiniCPM-MoE\n' +
      '\n' +
      '우리는 Mixture-of-Expert를 사용하여 MiniCPM의 능력을 추가로 확장한다.\n' +
      '\n' +
      '**초기화.** MiniCPM-MoE는 Sparse Upcycling(Komatsuzaki et al., 2022)을 사용하여 초기화됩니다. MiniCPM의 안정적인 단계에서 파생된 조밀한 모델 체크포인트는 각 MLP 층이 MoE 층으로 대체되는 변환을 겪는다. 이러한 새로운 MoE 레이어는 조밀한 체크포인트에서 원래 MLP 레이어의 정확한 복제본이다. 라우터 매개변수는 평균이 0이고 분산이 0.01인 정규 분포에 따라 무작위로 초기화됩니다.\n' +
      '\n' +
      '**라우팅 메커니즘** MiniCPM-MoE의 총 비 임베딩 매개 변수 수는 13.6B입니다. 훈련과 추론 과정에서 토큰별로 전문가 8명 중 2명이 활성화된다.\n' +
      '\n' +
      '그림 13: MiniCPM-DPO-2.4B의 MTBench 점수는 더 큰 크기의 많은 모델을 능가한다.\n' +
      '\n' +
      '결과적으로 활성화된 매개변수의 수는 약 4B이다. 훈련이 붕괴되는 것을 방지하기 위해, 추가적인 부하 밸런싱 손실(Fedus et al., 2022)이 최종 훈련 목표에 적용된다. 이 보조 손실에는 0.01이 곱해져 서로 다른 전문가에게 할당된 토큰의 균형 잡힌 분포를 보장할 수 있을 만큼 충분히 크다.\n' +
      '\n' +
      '**훈련.** MiniCPM과 마찬가지로 WSD를 학습률 스케줄러로 사용합니다. 훈련 데이터와 관련하여 우리는 섹션 6.3에 명시된 분포를 엄격하게 준수한다. 훈련 배치 크기는 안정적인 훈련 및 감쇠 단계에서 4M 토큰으로 유지되며 STF 단계에서 2M 토큰으로 축소된다. 사전 훈련 단계(사전 훈련 및 붕괴 단계 계속 포함)는 130K 단계에 걸쳐 있으며, 그 후 개선 효과가 감소하는 것을 알 수 있다. 벤치마크 결과는 표 6에 자세히 나와 있다.\n' +
      '\n' +
      '## 8 Conclusion\n' +
      '\n' +
      '본 논문에서는 2.4 B와 1.2 B 비 임베딩 파라미터를 갖는 두 개의 SLM으로 구성된 MiniCPM을 소개한다. 이러한 모델은 더 큰 모델에 비해 우수한 성능을 보여줍니다. 학습 방법론은 모델 및 데이터 크기 측면에서 확장 가능하므로 LLM 개발에 잠재적으로 적용할 수 있다. WSD 스케줄러의 도입은 지속적인 훈련을 촉진하고, 강력한 훈련 역학을 보여주며, 스케일링 법칙에 대한 효율적인 연구를 가능하게 한다는 점에서 주목할 만하다. DPO, 긴 컨텍스트 및 MoE 버전을 포함한 MiniCPM 패밀리를 추가로 소개합니다. 향후 방향에는 감쇠 단계에서 손실 감소에 대한 심층 분석과 모델 크기와 데이터 크기 모두에서 스케일링을 통해 MiniCPM의 능력을 향상시키는 것이 포함된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c c c c c c c c} \\hline \\hline\n' +
      '**Model** & **C-Eval** & **CMMLU** & **MMLU** & **HumanEval** & **MBPP** & **GSM8K** & **MATH** & **BBH** \\\\ \\hline Llama2-34B & - & - & 62.6 & 22.6 & 33.0\\({}^{\\dagger}\\) & 42.2 & 6.24 & **44.1** \\\\ DeepSee-MoE (16B) & 40.6 & 42.5 & 45.0 & 26.8 & 39.2 & 18.8 & 4.3 & - \\\\ Mistral-7B & 46.12 & 42.96 & **62.69** & 27.44 & 45.20 & 33.13 & 5.0 & 41.06 \\\\ Gemma-7B & 42.57 & 44.20 & 60.83 & 38.41 & 50.12 & 47.31 & 6.18 & 39.19 \\\\ \\hline MiniCPM-2.4B & 51.13 & 51.07 & 53.46 & 50.00 & 47.31 & 53.83 & 10.24 & 36.87 \\\\\n' +
      '**MiniCPM-MoE (13.6B)** & **58.11** & **58.80** & 58.90 & **56.71** & **51.05** & **61.56** & **10.52** & 39.22 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: MiniCPM-MoE의 벤치마크 결과. \\ ({}^{\\dagger}\\)는 수동 검증 집합 대신 MBPP 전체 집합에 대한 평가 결과를 의미한다(Austin et al., 2021). Llama2-34B와 Qwen1.5-7B의 평가 결과는 기술 보고서에서 가져왔다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Passkey**} & \\multirow{2}{*}{\\begin{tabular}{c} **Number** \\\\ **String** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **KV Retrieval** \\\\ \\end{tabular} } & \\begin{tabular}{c} **Long** \\\\ **Book** \\\\ **Choice** \\\\ **Eng** \\\\ \\end{tabular} & \\begin{tabular}{c} **Long** \\\\ **Book** \\\\ **QA** \\\\ **Chn** \\\\ \\end{tabular} & \\begin{tabular}{c} **Long** \\\\ **Book** \\\\ **QA** \\\\ **Eng** \\\\ \\end{tabular} &\n' +
      '\\begin{tabular}{c} **Long** \\\\ **Book** \\\\ **Sum** \\\\ \\end{tabular} \\\\ \\hline LWM-Text-128K & 100 & 97.8 & 0.6 & 28.82 & 15.93 & 14.31 & 9.99 \\\\ Yarn-Mistral-7B-128K & 92.71 & 56.61 & 0 & 27.95 & 15.49 & 9.55 & 9.06 \\\\ Mistral-7B-Instruct- & 100 & 78.98 & 3.6 & 37.12 & 11.74 & 17.37 & 21.12 \\\\ v0.2(ARB1000w) & 100 & 94.92 & 0 & 36.68 & 15.07 & 9.2 & 0.92 \\\\ ChatGLM3-6B-128K & 89.93 & 99.66 & 5.2 & 46.29 & 10.7 & 8.38 & 25.91 \\\\ \\hline\n' +
      '**MiniCPM-2.4B-128K** & 98.31 & 99.83 & 9 & 29.69 & 23.06 & 16.33 & 15.73 \\\\ \\hline \\hline \\end{tabular} \\begin{tabular}{l c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multicolumn{2}{c}{\\begin{tabular}{c} **Long Di-** \\\\ **above** \\\\ **QA Eng** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Math** \\\\ **Calc** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Math** \\\\ **Find** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Code** \\\\ **Debug** \\\\ \\end{tabular} } & \\multirow{2}{*}{\\begin{tabular}{c} **Code** \\\\ **Run** \\\\ \\end{tabular} } & \\multirow{2}{*}{\n' +
      '\\begin{tabular}{c} **Avg** \\\\ **Code \\& **Math** \\\\ \\end{tabular} } \\\\ \\hline LWM-Text-128k & 1.5 & 0 & 3.43 & 20.05 & 1 & 24.45 & 33.62 \\\\ Yarn-Mistral-7b-128k & 7.5 & 0 & 17.14 & 0.76 & 1.25 & 19.84 & 27.36 \\\\ Mistral-7B-Instruct- & 9.5 & 0 & 29.43 & 17.51 & 0 & 27.75 & 36.9 \\\\ V0.2(ARB100w) & 3.5 & 0 & 4.29 & 0.51 & 0.75 & 22.15 & 32.54 \\\\ ChatGLM3-6B-128K & 6.5 & 0 & 8 & 5.33 & 1 & 25.58 & 36.57 \\\\ \\hline\n' +
      '**MiniCPM-2.4B-128K** & 9.5 & 0 & 4.29 & 22.08 & 0 & 27.32 & 37.68 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: MiniCPM-2.4B-128K 결과 in \\(\\infty\\)Bench (Zhang et al., 2024b)\n' +
      '\n' +
      '## Author Contributions\n' +
      '\n' +
      '모든 저자는 MiniCPM 프로젝트에 실질적으로 기여한다. 성딩 후가 주도하고 프로젝트의 모든 측면에 참여했습니다. 여기에는 스케일링 실험(유게 투와 함께 수행됨), 미니CPM 기본 모델의 훈련을 돌보는 것, 연구의 다양한 다른 부분에 기여하는 것이 포함되었다. 성딩 후가 쓴 거야 차오쿤 허는 미니CPM 평가를 담당했고 간쿠이는 RLHF 훈련을 담당했다. 상룽, 즈정, 신룽 장, 성딩 후는 문맥 창을 128K로 확장했다. MoE 연구는 예웨이팡과 지정이 수행했다. Weilin Zhao와 Kaihuo Zhang은 훈련 및 추론 인프라에 기여했습니다. MiniCPM의 오픈소싱은 위샹황과 성딩후가 준비하였다. 성딩 후는 첸양 자오와 함께 WSD 스케줄러의 훈련 역학에 대한 분석도 제공했다. 정렝 타이가 토나이저를 개발했어요 MiniCPM-V의 개발은 왕총이, 야오위안 등이 진행하였다. MiniCPM의 훈련 코퍼스는 Jie Zhou, Jie Cai, Shengding Hu, Zhi Zheng, Zhongwu Zhai가 준비하였다. 이 논문은 싱롱 장과 차오쿤 허에 의해 교정되었다. MiniCPM 교육에 대한 통찰력 있는 지침은 쉬한, 닝딩 및 지위안 류에 의해 제공되었다. 마지막으로 Zhiyuan Liu, Maosong Sun, Guoyang Zeng, Chao Jia, Dahai Li는 MiniCPM 훈련에 필수적인 자원을 제공했다.\n' +
      '\n' +
      '## Limitations\n' +
      '\n' +
      '우리는 SLM을 사용한 스케일링 법칙에 대한 철저한 연구를 제안했지만, 이 논문은 스케일링 법칙을 검증하기 위해 LLM을 훈련하는 것으로 확장되지 않는다. LLM에 대한 WSD LRS의 적용은 현재까지 완전히 탐구되지 않았다. 그러나, 우리는 그것의 잠재적인 장점에 대해 여전히 낙관적이다.\n' +
      '\n' +
      '## Acknowledgement\n' +
      '\n' +
      'MiniCPM은 2024년 2월 1일 처음 기술 블로그로 출판되었으며, 이후 커뮤니티로부터 많은 통찰력 있는 피드백을 받아 본 논문의 발전에 크게 기여하였다. 천팅 저우와 아르멘 아가잔얀의 귀중한 토론에 감사를 표합니다. 블로그에서 모호함을 명확히 하는 데 대한 세심한 피드백에 대해 페이긴 선과 옌 왕에게 특별한 감사를 드립니다. 또한 MiniCPM을 llama.cpp 등과 같은 추론 프레임워크에 통합하려는 오픈 소스 커뮤니티의 노력에 감사한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Achiam 등(2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _ arXiv preprint arXiv:2303.08774_, 2023.\n' +
      '* Aghajanyan et al. (2023) Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. 생성 혼합 모달 언어 모델에 대한 확장법. In _International Conference on Machine Learning_, pp. 265-279. PMLR, 2023.\n' +
      '* Ainslie et al. (2023) Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: 다중 헤드 체크포인트로부터 일반화된 다중 쿼리 변압기 모델을 트레이닝한다. Houda Boumor, Juan Pino, and Kalika Bali(eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 4895-4901, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emmlp-main.298. URL [https://aclanthology.org/2023.emmlp-main.298](https://aclanthology.org/2023.emmlp-main.298)\n' +
      '* Almazrouei 등(2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The Falcon series of open language models. _ arXiv preprint arXiv:2311.16867_, 2023.\n' +
      '* Askell et al.(2021) Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. _ arXiv preprint arXiv:2112.00861_, 2021.\n' +
      '* Austin 등(2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 대형 언어 모델을 사용한 프로그램 합성, 2021.\n' +
      '* Bai 등(2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _ arXiv preprint arXiv:2309.16609_, 2023.\n' +
      '* Banks and Warkentin (2024) Jeanine Banks and Tris Warkentin. Gemma: 최신 오픈 모델을 소개합니다. [https://blog.google/technology/developers/gemma-open-models/] (https://blog.google/technology/developers/gemma-open-models/), 2024. Accessed: date-of-access.\n' +
      '* Bi et al.(2024) Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. _ arXiv preprint arXiv:2401.02954_, 2024.\n' +
      '* Biderman 등 (2022) Stella Biderman, Kieran Bicheno, and Leo Gao. 파일의 데이터시트입니다. _ arXiv preprint arXiv:2201.07311_, 2022.\n' +
      '* bloc97 (8k+) bloc97. NTK 인식 크기 조정 RoPE를 사용 하면 LLaMA 모델이 미세 조정 및 최소 복잡도 저하 없이 확장 된 (8k+) 컨텍스트 크기를 가질 수 있습니다. [https://www.reddit.com/r/LocalLLaMA/comments/14l27j5/ntkaware_scaled_rope_allows_llama_models_to_have/] (https://www.reddit.com/r/LocalLLaMA/comments/14l27j5/ntkaware_scaled_rope_allows_llama_models_to_have/), 2023. Accessed: [Insert Date of Access]\n' +
      '* Broder (1997) Andrei Z Broder. 문서의 유사성과 포함에 대해. <프로시빙스>에서. SEQUENCES 1997 (Cat. No. 97TB100171)의 압축 및 복잡도 _, pp. 21-29. IEEE, 1997.\n' +
      '* Chen et al.(2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _ arXiv preprint arXiv:2107.03374_, 2021.\n' +
      '* Chowdhery et al.(2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _ Journal of Machine Learning Research_, 24(240):1-113, 2023.\n' +
      '* Chowdhery et al.(2021)* Clark et al.(2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문을 풀었다고 생각해? try arc, ai2 reasoning challenge. _ arXiv preprint arXiv:1803.05457_, 2018.\n' +
      '* Cobbe 등(2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _ arXiv preprint arXiv:2110.14168_, 2021.\n' +
      '* Cui et al. (2023) Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 울트라피드백: 고품질 피드백으로 언어 모델을 부스팅, 2023.\n' +
      '* Dey 등(2023) Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. Cerebras-gpt: Open compute-optimal language models trained on the cerebral swafer-scale cluster. _ arXiv preprint arXiv:2304.03208_, 2023.\n' +
      '* Ding 등 (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023년 고품질 교육 대화를 확장하여 채팅 언어 모델을 개선합니다.\n' +
      '* Du et al.(2021) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zilin Yang, and Jie Tang. GIm: 자기회귀 빈칸 채우기 일반 언어 모델 사전 훈련. _ arXiv preprint arXiv:2103.10360_, 2021.\n' +
      '* Du et al.(2024) Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. 손실 관점에서 언어 모델의 창발 능력을 이해합니다. _ arXiv preprint arXiv:2403.15796_, 2024.\n' +
      '* Fedus et al.(2022) William Fedus, Barret Zoph, and Noam Shazeer. 트랜스포머 전환: 간단하고 효율적인 희소성으로 조 단위 매개변수 모델로 확장합니다. _ Journal of Machine Learning Research_, 23(120):1-39, 2022.\n' +
      '* Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: 생성 미리 훈련된 변환기에 대한 정확한 훈련 후 양자화. _ CoRR_, abs/2210.17323, 2022. doi: 10.48550/ARXIV.2210.17323. URL [https://doi.org/10.48550/arXiv.2210.17323](https://doi.org/10.48550/arXiv.2210.17323).\n' +
      '* Gao 등 (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The File: An 800GB dataset of various text for language modeling _ arXiv preprint arXiv:2101.00027_, 2020.\n' +
      '* Gemini 등(2023) Team Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _ arXiv preprint arXiv:2312.11805_, 2023.\n' +
      '* Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Fiero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 교재만 있으면 됩니다. _ arXiv preprint arXiv:2306.11644_, 2023.\n' +
      '* Hendrycks 등(2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 대규모 멀티태스킹 언어 이해를 측정하는 중입니다. _ arXiv preprint arXiv:2009.03300_, 2020.\n' +
      '* Hendrycks 등(2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 수학 데이터 세트를 사용하여 수학 문제 풀이를 측정하는 중입니다. _ arXiv preprint arXiv:2103.03874_, 2021.\n' +
      '* Henry et al.(2020) Alex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. 변압기에 대한 쿼리 키 정규화 Trevor Cohn, Yulan He, and Yang Liu(eds.), _Findings of the Association for Computational Linguistics: EMNLP 2020_, pp. 4246-4253, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. findings-emnlp.379. URL [https://aclanthology.org/2020.findings-emnlp.379](https://aclanthology.org/2020.findings-emnlp.379)\n' +
      '* Held 등 (2020)Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _ arXiv preprint arXiv:2203.15556_, 2022.\n' +
      '* Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 텍스트 분류를 위한 범용 언어 모델 미세 조정입니다. _ arXiv preprint arXiv:1801.06146_, 2018.\n' +
      '* Hu et al.(2023) Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, et al. Unlock predictable scaling from emergent abilities _ arXiv preprint arXiv:2310.03262_, 2023.\n' +
      '* Huang et al. (2024) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-disc discipline chinese evaluation suite for foundation models. _ 신경 정보 처리 시스템_, 36, 2024에서의 진보.\n' +
      '* Hundt 등(2019) Andrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: 더 빠르고 정확한 차별화 가능한 아키텍처 검색 _ arXiv preprint arXiv:1903.09900_, 2019.\n' +
      '* Javaheripi and Bubeck (2023) Mojan Javaheripi and Sebastien Bubeck. Phi-2: 작은 언어 모델의 놀라운 힘. [https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/] (https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/), 2023. Accessed: date-of-access.\n' +
      '* Jiang 등(2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _ arXiv preprint arXiv:2310.06825_, 2023.\n' +
      '* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 신경 언어 모델의 법칙을 조정합니다. _ arXiv preprint arXiv:2001.08361_, 2020.\n' +
      '* Koetkov 등(2022) Denis Koetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Munoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. 스택: 허용 라이선스 소스 코드 3 tb입니다. _ Preprint_, 2022.\n' +
      '* Komatsuzaki et al.(2022) Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. 저렴한 업사이클링: 조밀한 검문소에서 전문가의 혼합물을 훈련합니다. _ arXiv preprint arXiv:2212.05055_, 2022.\n' +
      '* Kwon et al. (2023) 우석권, 주한리, 시위안장, 영성, 리안민정, 코디하오유, 조셉곤잘레스, 하오장, 이온스토이카. 페이지 주의와 함께 제공되는 대용량 언어 모델을 위한 효율적인 메모리 관리. In _Proceedings of the 29th Symposium on Operating Systems Principles_, pp. 611-626, 2023.\n' +
      '* Li 등(2024) Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2024년 중국어로 대량 멀티태스킹 언어 이해도 측정\n' +
      '* Li et al.(2023a) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Koetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may be source with you! _ arXiv preprint arXiv:2305.06161_, 2023a.\n' +
      '* Li et al.(2023b) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. ii: phi-1.5 기술 보고서만 있으면 됩니다. _ arXiv preprint arXiv:2309.05463_, 2023b.\n' +
      '* Lian 등(2023a) Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". 슬림코르카: 검증, 2023a와 함께 gpt-4 증강 플란 추론 추적의 열린 데이터 세트. URL https://[https://huggingface.co/Open-Orca/SlimOrca](https://huggingface.co/Open-Orca/SlimOrca).\n' +
      '\n' +
      '* Lian et al. (2023) Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, "Tekrium", and Nathan Hoos. Slimorca dedup: Slimorca, 2023b의 중복 제거된 부분집합. URL [https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup/](https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup/)\n' +
      '* Liu et al.(2024) Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobiellm: Optimizing sub-billion parameter language models for on-device use cases. _ arXiv preprint arXiv:2402.14905_, 2024.\n' +
      '* Loshchilov and Hutter (2016) Ilya Loshchilov and Frank Hutter. Sgdr: 따뜻한 재시작과 함께 확률적 기울기 하강. _ arXiv preprint arXiv:1608.03983_, 2016.\n' +
      '* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 비결합 중량 감쇠 규칙화. _ arXiv preprint arXiv:1711.05101_, 2017.\n' +
      '* Peng 등(2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarm: 대규모 언어 모델의 효율적인 컨텍스트 창 확장입니다. _ arXiv preprint arXiv:2309.00071_, 2023.\n' +
      '* Rae 등(2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _ arXiv preprint arXiv:2112.11446_, 2021.\n' +
      '* Rafailov 등(2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. _ 신경 정보 처리 시스템_, 36, 2024에서의 진보.\n' +
      '* Raffel 등(2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통합 텍스트 대 텍스트 변환기를 사용하여 전이 학습의 한계를 탐색합니다. _ arXiv e-prints_, 2019.\n' +
      '* Raffel 등(2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 통합 텍스트 대 텍스트 변환기를 사용하여 전이 학습의 한계를 탐색합니다. _ Journal of machine learning research_, 21(140):1-67, 2020.\n' +
      '* Sardana and Frankle (2023) Nikhil Sardana and Jonathan Frankle. 친칠라를 넘어 최적: 언어 모델 스케일링 법칙에서 추론에 대한 계산 _ arXiv preprint arXiv:2401.00448_, 2023.\n' +
      '* Sennrich et al.(2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 하위 단어 단위로 희귀 단어를 기계 번역합니다. In Katrin Erk and Noah A. Smith (eds.), _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1715-1725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL [https://aclanthology.org/P16-1162](https://aclanthology.org/P16-1162).\n' +
      '* Shi et al. (2023) Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. 컨텍스트 사전 훈련: 문서 경계를 넘어 언어 모델링. _ arXiv preprint arXiv:2310.10638_, 2023.\n' +
      '* Smith et al. (2017) Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. 학습률을 저하시키지 말고 배치 크기를 늘립니다. _ arXiv preprint arXiv:1711.00489_, 2017.\n' +
      '* Soldaini 등(2021) Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kylek Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 돌마: 언어 모델 사전 훈련 연구를 위한 3조 토큰의 공개 코퍼스입니다. _ arXiv preprint_, 2024. URL [https://arxiv.org/abs/2402.00159](https://arxiv.org/abs/2402.00159).\n' +
      '*Song et al. (2023) Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, and Dong Yu. Zebra: 계층별로 그룹화된 로컬 글로벌 주의를 사용하여 컨텍스트 창을 확장합니다. _ arXiv preprint arXiv:2312.08618_, 2023.\n' +
      '* Song et al.(2021)* Suzgun et al.(2022) Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. 레, 에드 치, 데니 저우, 제이슨 웨이 큰 벤치 과제들과 고민의 사슬이 그것들을 해결할 수 있는지 2022년.\n' +
      '* LMMFarm(2023a) LMMFarm팀. LLMFarm, 2023a URL [https://github.com/guinmoon/LLMFarm](https://github.com/guinmoon/LLMFarm).\n' +
      '* MLC팀(2023b) MLC팀. MLC-LLM, 2023b. URL [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm).\n' +
      '* Touvron 등(2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.\n' +
      '* Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sangeviero, Alexander M. 러쉬와 토마스 울프 제퍼: lm 정렬의 직접 증류, 2023.\n' +
      '* Wei et al.(2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 파인튜닝 언어 모델은 제로샷 학습자입니다. _ arXiv preprint arXiv:2109.01652_, 2021.\n' +
      '* Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 매직코더: 소스 코드만 있으면 됩니다. _ arXiv preprint arXiv:2312.02120_, 2023.\n' +
      '* Wortsman 등(2023) Mitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for large-scale transformer training instability. _ arXiv preprint arXiv:2309.14322_, 2023.\n' +
      '*Xia et al.(2023) Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 전단 라마: 구조화된 프루닝을 통해 언어 모델 사전 훈련을 가속화합니다. _ arXiv preprint arXiv:2310.06694_, 2023.\n' +
      '*Xie et al.(2024) Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. 도페르미: 데이터 혼합을 최적화하면 언어 모델 사전 훈련이 빨라집니다. _ 신경 정보 처리 시스템_, 36, 2024에서의 진보.\n' +
      '* Xiong 등(2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. _ arXiv preprint arXiv:2309.16039_, 2023.\n' +
      '* Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 마법사: 복잡한 지침을 따르도록 대규모 언어 모델에 권한을 부여합니다. _ arXiv preprint arXiv:2304.12244_, 2023.\n' +
      '* Yang et al.(2022) Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. 텐서 프로그램 v: 제로 샷 하이퍼파라미터 전송을 통해 큰 신경망을 조정합니다. _ arXiv preprint arXiv:2203.03466_, 2022.\n' +
      '* Yang et al. (2023) Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. 텐서 프로그램 vi: 무한 심도 신경망에서의 특징 학습 _ arXiv preprint arXiv:2310.02244_, 2023.\n' +
      '* Ye et al.(2024) Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. 데이터 혼합 법칙: 언어 모델링 성능을 예측하여 데이터 혼합을 최적화합니다. _ arXiv preprint arXiv:2403.16952_, 2024.\n' +
      '* Young et al.(2024) Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. _ arXiv preprint arXiv:2403.04652_, 2024.\n' +
      '* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yoatan Bisk, Ali Farhadi, and Yejin Choi. 헬라스왁: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? arXiv preprint arXiv:1905.07830_, 2019.\n' +
      '\n' +
      '* Zhang 등(2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: 오픈 소스 소형 언어 모델입니다. _ arXiv preprint arXiv:2401.02385_, 2024a.\n' +
      '* Zhang 등(2023) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey _ arXiv preprint arXiv:2308.10792_, 2023.\n' +
      '* Zhang 등 (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. obench: Extending long context evaluation beyond 100k token _ arXiv preprint arXiv:2402.13718_, 2024b.\n' +
      '* Zheng et al.(2024) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arenna. _ 신경 정보 처리 시스템_, 36, 2024에서의 진보.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:25]\n' +
      '\n' +
      '### 최적 Batchsize에 대한 설명\n' +
      '\n' +
      'Kaplan et al.(2020)에서 OpenAI는 손실 함수와 토큰 수의 관계를 연구한다. 그들의 실험에서, 그들은 더 많은 단계를 소비하는 것이 더 많은 시간을 소비하는 것과 동등하다고 가정한다. 이러한 가정 하에서 OpenAI는 너무 많은 단계 또는 토큰을 소비하지 않고 특정 손실을 달성하는 임계 배치 크기를 정의한다. 이 근거는 실험이 무제한 GPU(적어도 실험 범위 내에서)로 제공되는 경우 유효합니다. GPU는 무제한이므로 배치 크기를 확대하면 단일 단계 지속 시간이 증가하지 않고 전체 단계 수가 감소합니다. 그러나 우리의 실험에서는 고정된 자원(GPU 수)을 가지고 있기 때문에 배치 크기를 두 배로 하는 것이 단일 단계 시간을 두 배로 하는 것과 거의 같다는 것을 관찰한다. 따라서 총 훈련 단계를 줄이기 위해 배치 크기를 확대하는 것은 총 훈련 시간에 최소한의 영향을 미친다. 이러한 관찰에 비추어 "너무 많은 단계를 소비하지 않는다"는 목표를 포기하고 대신 가장 낮은 손실을 달성하기 위해 토큰 양을 최소화하는 쪽으로 방향을 돌린다.\n' +
      '\n' +
      '손실과 관련하여 최적의 배치 크기 추정에 관한 관찰은 "치킨 앤 계란" 역설과 유사하다. 실질적으로, 예비 실험에 대한 사전 지식에 의해 알려진, 주어진 모델 크기에 대해 달성 가능한 손실에 대한 예비 추정치가 종종 존재한다. 그러나 향후 보다 정교한 추정 절차의 개발 가능성이 있다.\n' +
      '\n' +
      '### 모델 풍동 실험에서의 모델 아키텍처\n' +
      '\n' +
      '모델 풍동 실험에 사용된 모델 구성을 표 8에 나열한다. 모델의 "형상", 즉 모델 깊이와 비교하여 모델 너비는 잠재적인 성능 변동을 피하기 위해 가능한 한 유사하게 유지된다.\n' +
      '\n' +
      '## 부록 B WSD LRS에 대한 추가 설명\n' +
      '\n' +
      '### LRS에 따라 학습 속도 패러다임\n' +
      '\n' +
      '본 논문에서는 LRS의 세 가지 종류, \\(Cosine(T)\\), \\(CosineLoop(T)\\), \\(WSD(T,D)\\에 대해 기술한다. 코사인 및 코사인 루프는 다음과 같은 형태를 취한다:\n' +
      '\n' +
      '\\[Cosine(T;s)=\\\\ \\left\\{\\begin{array}{ll}\\frac{s}{W}\\eta,&s<W\\\\0.9\\eta cos(\\pi\\frac{s}{T})+0.1\\eta,&W<s<T\\\\0.1\\eta,&s>T\\end{array}\\right.\\quad\\left\\{\\begin{array}{ll}\\frac{s}{W}\\eta,&s<W\\\\0.9\\eta cos(\\pi\\frac{s}{T})+0.1\\eta,&W<s\\end{array}\\right.\\]\n' +
      '\n' +
      'WSD 및 코사인 스케줄러에 대한 예시적인 학습률 다이어그램이 그림 15에 나와 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c c c c c c} \\hline \\hline\n' +
      '**Name** & **N (B)** & \\(d_{m}\\) & \\(d_{ff}\\) & \\(d_{h}\\) & \\(n_{h}\\) & \\(L\\) \\\\ \\hline\n' +
      '9M & 0.009 & 320 & 800 & 64 & 5 & 8 \\\\\n' +
      '30M & 0.036 & 512 & 1280 & 64 & 8 & 12 \\\\\n' +
      '70M & 0.066 & 640 & 1600 & 64 & 10 & 14 \\\\\n' +
      '0.1B & 0.109 & 768 & 1920 & 64 & 12 & 16 \\\\\n' +
      '0.17B & 0.166 & 896 & 2240 & 64 & 14 & 18 \\\\\n' +
      '0.2B & 0.241 & 1024 & 2560 & 64 & 16 & 20 \\\\\n' +
      '0.5B & 0.499 & 1344 & 3360 & 64 & 21 & 24 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 8: 스케일링 곡선에서의 모델 구성들 및 모델들의 트레이닝 구성들. N(B)는 수십억 개로 측정된 모델의 비포매 파라미터 수를 나타낸다.\n' +
      '\n' +
      '### 데이터 크기 조정 규칙 적합\n' +
      '\n' +
      '이 절에서는 WSD LRS를 사용하여 계속 훈련하기 위한 적합 데이터 스케일링 법칙에 대해 설명한다. 그림 16의 각 점은 다른 끝 단계를 가진 WSD LRS의 붕괴 단계의 끝이다. 우리는 지수식과 다항식의 두 가지 함수 형태를 시도한다. 적합된 결과는 다항식 스케일링 법칙이 여전히 계속 훈련에 가장 적합하다는 것을 보여준다.\n' +
      '\n' +
      '### 모델 데이터 크기 조정 법칙에 대한 개별 그림\n' +
      '\n' +
      '각 작업 및 모델에 대해 데이터 축을 따라 실제 손실 값을 갖는 스케일링 법칙 \\(L(N,D)\\)의 적합도는 그림 17에 나와 있다.\n' +
      '\n' +
      '### Llama2의 데이터 대 모델 비율 분석\n' +
      '\n' +
      '4.5절에서 언급한 바와 같이, 우리는 Llama2의 데이터 대 모델 비율을 훈련 손실 곡선을 기반으로 분석한다. 추출된 손실은 그림 18의 왼쪽에 표시된다. x축을 계산 Flops로 변환하여 그림의 오른쪽에 있는 계산 최적 체제를 비교한다.\n' +
      '\n' +
      '## 부록 C MiniCPM의 어휘\n' +
      '\n' +
      'MiniCPM은 모수 크기가 작음에도 불구하고 다양한 데이터 분포를 모델링하여 영어와 중국어에 탁월하다. 따라서, 우리의 어휘력은 상대적으로 크다. 2.4B 모델의 경우 122,753개의 토큰(MiniCPMTokenizer-120K로 표시됨)으로 구성된 토큰화기를 사용한다. 이 어휘는 바이트 쌍 인코딩(Byte Pair Encoding, BPE)을 위한 문장제 라이브러리 4(Sennrich et al., 2016)를 활용하여 광범위하고 다양한 언어 데이터로부터 구축되며, 전통적인 한자, 희귀 문자, 이모지와 같은 특수 기호 및 그리스 문자, 키릴 문자 등과 같은 특수 기호를 포함한다.\n' +
      '\n' +
      '각주 4: [https://github.com/google/sentencepiece](https://github.com/google/sentencepiece)\n' +
      '\n' +
      'SLM의 경우, 임베딩 파라미터는 어휘가 큰 경우 파라미터 공간을 많이 차지하게 된다. 따라서 1.2B 모델의 경우 더 작은 어휘 MiniCPMTokenizer-70K를 사용한다. MiniCPMTokenizer-120K 토큰화기와 비교하여 동일한 문서에 대해 토큰화를 재교육하고 최대 어휘 수를 64,000개로 설정했다. 특수문자는 전통적인 한자, 이모티콘, 특수 기호만 추가하고 중국어에서 희귀한 문자는 제외했다.\n' +
      '\n' +
      '토케나이저의 교육 세트에 없는 30만 개의 중국어, 영어, 코드, 학술 논문에 대한 평가를 진행합니다. MiniCPM-120K 토큰화기는 가장 높은 압축 비율(바이트/토큰)을 달성한다.\n' +
      '\n' +
      '그림 15: 그림 16: 코사인 LRS와 WSD LRS의 두 가지 다른 함수를 사용합니다. 데이터 스케일링 법칙에 적합한 형태들은 WSD LRS에 의해 서로 다른 최종 단계를 공유하는 WSD LRS를 달성하였고, 동일한 안정적인 훈련 단계로 멱법칙(power law)을 선택하였다. 최상의 핏.\n' +
      '\n' +
      '그림 17: 적합 스케일링 법칙은 각 모델 및 각 작업에 대한 데이터 양 축을 따라 표시된다. 적합된 결과는 0.11B 및 0.25B 모델의 마지막 체크포인트를 제외하고 만족한다.\n' +
      '\n' +
      '그림 18: Llama2 논문(왼쪽 부분)에서 훈련 손실 데이터를 추출하고 오른쪽 부분을 사용하여 그들의 논문에서 계산 최적 \\(\\frac{D_{opt}}{N_{opt}}}\\)을 추정한다. WSD Scheduler를 사용하여 최적의 손실 포락선을 추정하기 위해 직선이 표시된다.\n' +
      '\n' +
      '## 부록 D 양자화\n' +
      '\n' +
      '우리는 모델에 대해 4비트 양자화를 수행한다. 모델의 성능이 이러한 매개변수에 민감하기 때문에 임베딩 및 계층 정규화의 매개변수를 양자화하지 않는다. 따라서 각 가중치 매트릭스만 정량화하면 됩니다. 가중치 행렬을 \\(\\mathbf{W}\\in\\mathbb{R}^{d_{out}\\times d_{in}}\\)로 표현한다. 우리는 \\(d_{in}\\) 차원에서 모든 \\(G\\) 연속 매개 변수를 그룹화하고 \\(d_{in}/G\\) 그룹을 형성한다. 그런 다음 매개 변수의 각 그룹을 별도로 양자화한다. 각 그룹 파라미터 \\(\\mathbf{w}\\)에 대해 다음과 같이 양자화 스케일 및 영점을 계산한다:\n' +
      '\n' +
      '\\[\\text{scale}=\\frac{\\text{max}(\\mathbf{w})-\\text{min}(\\mathbf{w})}{2^{4}-1}, \\text{zero}=-\\frac{\\text{min}(\\mathbf{w})}{scale}-2^{3}.\\]\n' +
      '\n' +
      'Group parameter \\(\\mathbf{w}\\) then quantized to\n' +
      '\n' +
      '\\[\\hat{w}=quant(w)=round(\\frac{w}{\\text{scale}}+\\text{zero}),\\]\n' +
      '\n' +
      '여기서 \\(round\\) 연산은 부동 소수점을 가장 가까운 정수로 라운딩한다. 상기 역양자화 동작은 상기 양자화 방법의 대략 역인,\n' +
      '\n' +
      '\\[dequant(\\hat{w})=\\text{scale}(\\hat{w}-\\text{zero}).\\]\n' +
      '\n' +
      '마지막으로 행렬 \\(\\mathbf{W}\\in\\mathbb{R}^{d_{out}\\times d_{in}}\\)은 \\(\\text{int4}\\ \\hat{\\mathbf{W}\\in\\mathbb{R}^{d_{out}\\times d_{in}}\\), float \\(\\text{scale}\\in\\mathbb{R}^{d_{out}\\times\\frac{d_{in}}}}\\) 및 float \\(\\text{zero}\\in\\mathbb{R}^{d_{out}\\times\\frac{d_{in}}}}}}}\\)로 양자화된다.\n' +
      '\n' +
      '양자화 손실을 줄이기 위해 가중치 보정을 적용하기 위해 GPTQ(Frantar et al., 2022)를 채택한다. 우리는 SFT 데이터의 유사한 분포로부터 보정 데이터 \\(\\mathbf{X}\\)를 샘플링한다. 양자화 목적은 양자화 \\(\\|\\mathbf{W}\\mathbf{X}-dequant(\\hat{\\mathbf{W}})\\mathbf{X}\\|_{2}^{2}\\)의 교란을 최소화하는 것이다. 우리는 반복적으로 가중치를 양자화하고 나머지 비양자화된 가중치를 업데이트하기 위해 GPTQ를 따른다.\n' +
      '\n' +
      '\\[\\delta_{F}=-\\frac{w_{q}-dequant(quant(w_{q}))}{[H_{F}^{-1}]_{qq}}\\cdot(H_{F}^{- 1})_{:,q},\\]\n' +
      '\n' +
      '여기서, \\(q\\)는 현재 반복에서의 양자화 위치이고, \\(F\\)는 나머지 비양자화된 가중치들을 나타낸다. \\ (H_{F}\\)는 목적함수의 헤시안 행렬이다.\n' +
      '\n' +
      '## 부록 E Edge 장치 벤치마킹\n' +
      '\n' +
      '부록 D에서 Int4 양자화 후 MiniCPM-2.4B의 풋프린트는 2GB로 줄어들어 모바일 에지 장치에서의 배치를 용이하게 한다. MLC-LLM(팀, 2023b)을 사용하여 Android와 HarmonyOS의 모델을 적용하였고, 아이폰 시스템 적응을 위해 LLMFarm(팀, 2023a)을 사용하였다. 이 적응은 다양한 에지 모바일 장치에서 테스트되었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & **Baichuan2** & **ChatGLM2** & **Llama2** & **MiniCPM-120K** & **MiniCPM-70K** \\\\ \\hline Vocab Size & 125,696 & 64,794 & 32,000 & 122,753 & 73,440 \\\\ \\hline \\multicolumn{5}{c}{**Compression Rate (\\(\\text{Bytes}/\\text{Tokens}\\))**} \\\\ \\hline Chinese & 3.64 & 3.54 & 1.87 & **3.73** & 3.56 \\\\ English & 4.12 & 4.02 & 3.78 & **4.14** & 4.02 \\\\ Code & 2.71 & 2.71 & 2.74 & **2.81** & 2.76 \\\\ Paper & 2.74 & 2.88 & **2.97** & 2.93 & 2.88 \\\\ \\hline Average & 3.30 & 3.29 & 2.84 & **3.40** & 3.31 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 압축비 비교.\n' +
      '\n' +
      '우리의 노력이 모바일 배포를 위한 최적화에 초점을 맞춘 것이 아니라 모바일 플랫폼에서 미니CPM의 추론 능력의 실용성을 입증하는 데 중점을 두었다는 점을 강조하는 것이 중요하다. 모바일 컨텍스트에서 MiniCPM과 같은 대규모 모델의 성능을 향상시키기 위해 개발자 커뮤니티의 추가 최적화 및 업데이트를 권장합니다.\n' +
      '\n' +
      '그 결과는 표 10과 같으며, 우리는 가장 진보된 스마트폰 iPhone 15 Pro에서 추론 처리량이 초당 18 토큰만큼 높다는 것을 알 수 있다. 다른 디바이스들에서, 추론 처리량은 또한 허용가능하다.\n' +
      '\n' +
      '## 부록 F MiniCPM-V\n' +
      '\n' +
      '또한 MiniCPM의 멀티모달 버전, 즉 MiniCPM-V를 훈련한다. MiniCPM-V에 대한 자세한 내용은 별도의 논문에서 소개할 예정이다. 여기서는 표 11의 성능에 대한 스냅샷만 나열합니다.\n' +
      '\n' +
      '## 부록 G 사례 시연\n' +
      '\n' +
      '마이크로소프트의 Phi(Li et al., 2023)와 같은 다른 강력한 SLM과 MiniCPM 시리즈를 구별하는 주목할 만한 특징은 다양한 작업에 걸쳐 모델의 범용성과 일반화를 보장하는 범용 코퍼라에서 MiniCPM을 훈련시킨다는 것이다. 이 섹션에서는 MiniCPM의 기능을 입증하기 위해 일반적으로 더 큰 모델에 의해서만 달성할 수 있는 몇 가지 흥미로운 생성 인스턴스를 제시한다. 훈련 집합에 특정 사례가 존재하는지에 대한 광범위한 점검은 하지 않지만, 표현 방법이 다양하기 때문에 의도적으로 테스트 사례와 유사한 데이터가 추가되지 않도록 한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l} \\hline \\hline \\multirow{2}{*}{**SmartPhone**} & \\multirow{2}{*}{**Operating System**} & \\multirow{2}{*}{**\\begin{tabular}{} \\end{tabular}**} & \\multicolumn{2}{c}{\\begin{tabular}{} \\end{tabular} } & **\n' +
      '\\begin{tabular}{} \\end{tabular}** \\\\  & & & **(GB)** & **(token/s)** \\\\ \\hline OPPO Find N3 & Android 13 & snapdragon 8 Gen2 & 12 & 6.5 \\\\ Samsung S23 Ultra & Android 14 & snapdragon 8 Gen2 & 12 & 6.4 \\\\ Meizu M182Q & Android 11 & snapdragon & 8 & 3.7 \\\\ Xiaomi 12 Pro & Android 13 & snapdragon 8 Gen1 & 8+3 & 3.7 \\\\ Xiaomi Redmi K40 & Android 11 & snapdragon 870 & 8 & 3.5 \\\\ Oneplus LE 2100 & Android 13 & snapdragon 870 & 12 & 3.5 \\\\ Oneplus HD1900 & Android 11 & snapdragon 865 & 8 & 3.2 \\\\ Oneplus HD1900 & Android 11 & snapdragon 855 & 8 & 3.0 \\\\ Oneplus HD1900 & Android 11 & snapdragon 855 & 8 & 3.0 \\\\ Oneplus HD1900 & Android 11 & snapdragon 855 & 8 & 3.0 \\\\ Xiaomi M 8 & Android 9 & snapdragon 845 & 6 & 2.3 \\\\ Huawei Nova 11SE & Harmony 4.0.0 & snapdragon 778 & 12 & 1.9 \\\\ Xiaomi MX2 & Android 9 & snapdragon 835 & 6 & 1.3 \\\\ iPhone 15 Pro & 10S 17.2.1 & AI7 pro & 8 & 18.0 \\\\ iPhone 15 & iOS 17.2.1 & AI6 & 6 & 15.0 \\\\ iPhone 12 Pro & iOS 16.5.1 & AI4 & 6 & 5.8 \\\\ iPhone 12 & iOS 17.2.1 & AI4 & 4 & 5.8 \\\\ iPhone 11 & iOS 16.6 & A13 & 4 & 4.6 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 10: 상이한 엔드 디바이스 상의 MiniCPM-2.4B의 속도.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l} \\hline \\hline\n' +
      '**Model** & **Size** & **Tokens\\({}^{v}\\)** & **MME** & **MMB\\({}^{m}\\)** & **MMB\\({}^{ch}\\)** & **MMMU** & **CMMMU** \\\\ \\hline LLAvA-Phi & 38 & 576 & 1335 & 59.8 & - & - & - \\\\ MobileVLM & 3B & 144 & 1289 & 59.6 & - & - & - \\\\ Imp-v1 & 3B & 576 & 1434 & 66.5 & - & - \\\\ Owen-VL-Chat & 9.6B & 256 & 1487 & 60.6 & 56.7 & 35.9 & 30.7 \\\\ CogVLM & 17.4B & 1225 & 1438 & 63.7 & 53.8 & 32.1 & - \\\\ \\hline MiniCPM-V & 3B & 64 & 1452 & 67.9 & 65.3 & 37.2 & 32.1 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 11: MLLM 벤치마크에서 SOTA 모델과의 비교.\n' +
      '\n' +
      '도 21: 다음의 지시에 대한 예. MiniCPM-2.4B-DPO는 명령을 성공적으로 따르고, 정확한 성조 패턴과 각 문장의 첫 번째 문자를 가진 시를 생성한다. 그러나 MiniCPM-1.2B는 올바른 성조 패턴만을 생성한다.\n' +
      '\n' +
      '도 19: 번역에 대한 예. 우리는 MiniCPM-2.4B-DPO가 기술 부분을 번역하는 데 더 나은 성능을 보이지만, 두 모델 모두 “중국어를 배우기 시작한 것”을 표현하는 데 부자연스럽다. 그러나 두 모델 모두 함정으로 명령어를 성공적으로 이해하고 의도적으로 프랑스어 말뭉치를 포함하지 않았음에도 불구하고 프랑스어를 생성한다.\n' +
      '\n' +
      '그림 20: 수학과 추론에 대한 예. 두 모델 모두 기호 대입과 수학 계산에서 성공적이다.\n' +
      '\n' +
      '도 22: 이모지에 대한 예. 두 모델 모두 이모티콘과 정형 데이터를 잘 이해하고, 암묵적으로 항목 수를 정확하게 제어한다. DPO 모델은 더 많은 단어를 생성하는 경향이 있다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>The Unreasonable Ineffectiveness of the Deeper Layers</title>
<!--Generated on Tue Mar 26 17:20:28 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.17887v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2403.17887v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2403.17887v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2403.17887v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1" title="1 Introduction ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2" title="2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Literature Review</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS1" title="2.1 Pruning ‚Ä£ 2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Pruning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS2" title="2.2 Model distillation ‚Ä£ 2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Model distillation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS3" title="2.3 Efficient finetuning and inference acceleration ‚Ä£ 2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Efficient finetuning
and inference acceleration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS4" title="2.4 A breadth of depth-dependent studies ‚Ä£ 2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>A breadth of depth-dependent studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3" title="3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS1" title="3.1 Intuition ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Intuition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Layer-pruning algorithm(s)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4" title="4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS1" title="4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Accuracy on QA benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS2" title="4.2 Loss on next-token predictions ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Loss
on
next-token predictions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS3" title="4.3 Angular distances between representations ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Angular distances between representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS4" title="4.4 A simpler pruning strategy ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>A simpler pruning strategy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S5" title="5 Discussion and Future Directions ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Future Directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1" title="Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Model and healing details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS2" title="A.2 Evaluation details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Evaluation details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2" title="Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Ablations</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS1" title="B.1 Prompting ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS2" title="B.2 Finetuning seed ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Finetuning seed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS3" title="B.3 LoRA rank ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>LoRA rank</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2403.17887v1 [cs.CL] 26 Mar 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Unreasonable Ineffectiveness of the Deeper Layers</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrey Gromov 
<br class="ltx_break">Meta FAIR &amp; UMD 
<br class="ltx_break">&amp;Kushal Tirumala<math alttext="{}^{*}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><times id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break">Meta FAIR 
<br class="ltx_break">&amp;Hassan Shapourian 
<br class="ltx_break">Cisco
&amp;Paolo Glorioso 
<br class="ltx_break">Zyphra 
<br class="ltx_break">Daniel A. Roberts
<br class="ltx_break">MIT &amp; Sequoia Capital
</span><span class="ltx_author_notes">Co-first authors; direct correspondence to <span class="ltx_text ltx_font_typewriter" id="id2.2.id1">gromovand@meta.com</span>, <span class="ltx_text ltx_font_typewriter" id="id3.3.id2">ktirumala@meta.com</span>, and <span class="ltx_text ltx_font_typewriter" id="id4.4.id3">drob@mit.edu</span>. 
<br class="ltx_break"></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id5.id1">We
empirically study
a simple layer-pruning strategy
for
popular
families
of open-weight pretrained LLMs,
finding minimal degradation of performance on
different
question-answering benchmarks until
after
a large fraction (up to half) of the layers
are removed.
To prune these models,
we identify the optimal block of layers to prune
by considering
similarity
across
layers;
then,
to ‚Äúheal‚Äù the damage, we
perform a small amount of finetuning.
In particular, we use
parameter-efficient finetuning (PEFT) methods,
specifically quantization and Low Rank Adapters (QLoRA),
such that each of our experiments can be performed on a single A100 GPU.
From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand,
and can improve the
memory and latency of inference on the other hand.
From a scientific perspective,
the robustness of these LLMs to the
deletion of
layers
implies either that current pretraining methods
are
not properly
leveraging
the
parameters in the deeper layers of the network
or that the shallow layers play a critical role in storing knowledge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Over the last few years, large language models (LLMs) have evolved from mere research artifacts <cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib1" title="">2019</a>)</cite> into useful products <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib2" title="">2022</a>)</cite>. To a large extent this evolution can be attributed to a dramatic increase in scale of the resources used for training <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib3" title="">2023</a>); Gemini Team et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib4" title="">2023</a>)</cite>.
Since
these models will likely
see
most of their total lifetime FLOPs
in inference mode after training completes, the
pretraining of LLMs
requires
not only considerations for efficient, i.e. compute-optimal, training <cite class="ltx_cite ltx_citemacro_cite">Kaplan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib5" title="">2020</a>); Hoffmann et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib6" title="">2022</a>)</cite>, but
also requires inference awareness
<cite class="ltx_cite ltx_citemacro_cite">De&nbsp;Vries (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib7" title="">2023</a>); Sardana and Frankle (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib8" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">What about models that have already been trained?
Beyond the training considerations
indicated by
neural scaling laws,
there are also numerous post-training techniques for reducing the cost and time of finetuning and then inferencing LLMs. In particular,
<em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">quantization</em>
can be used to
reduce the memory footprint of models by
decreasing the precision of the model weights <cite class="ltx_cite ltx_citemacro_cite">Dettmers et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib9" title="">2022</a>); Frantar et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib10" title="">2022</a>); Dettmers and Zettlemoyer (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib11" title="">2023</a>); Xiao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib12" title="">2023</a>)</cite>,
Low Rank Adapters (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">LoRA</em>)
can be used to
reduce the cost of finetuning and customization
by only updating a small subset of the model parameters <cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib13" title="">2021</a>)</cite>,
or <em class="ltx_emph ltx_font_italic" id="S1.p2.1.3">pruning</em> can be used to
reduce the memory footprint and time for inference by
directly eliminating unnecessary parameters or connections <cite class="ltx_cite ltx_citemacro_cite">LeCun et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib14" title="">1989</a>); Hassibi and Stork (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib15" title="">1992</a>); Han et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib16" title="">2015</a>); Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib17" title="">2016</a>); Frankle and Carbin (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib18" title="">2018</a>)</cite>.
As these three
strategies
are more or less orthogonal,
in a resource constrained environment
ideally we would want to
leverage
all three of these post-training efficiency techniques in combination.
Towards that direction,
the
popular
method of <em class="ltx_emph ltx_font_italic" id="S1.p2.1.4">QLoRA</em> <cite class="ltx_cite ltx_citemacro_cite">Dettmers et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>
introduced a handful of innovations that
enabled
4-bit quantization of parameters and LoRA finetuning to work together.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Building on that combination,
in this work
we study
a very simple pruning strategy using
open-weight LLMs. In particular, we develop a
method
that
uses the similarity between the representations at different layers to
identify the optimal layers to prune for a given pruning fraction;
then,
after removing these layers we ‚Äúheal‚Äù the
pruning-induced
mismatch
with a small amount of fine tuning (using QLoRA).
Our main result is that we can remove a substantial fraction of the <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">deepest layers</em> from models with minimal degradation in downstream performance.
For example,
for Llama-2-70B <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib20" title="">2023a</a>)</cite>
we can eliminate up to roughly <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">half</em> of the layers
before the performance collapses.
An overview of our strategy and the results of pruning Llama-2-70B are shown in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1">
<p class="ltx_p ltx_align_center ltx_align_center" id="S1.F1.1"><span class="ltx_text" id="S1.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="548" id="S1.F1.1.1.g1" src="x1.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our layer-pruning strategy and example results:
<em class="ltx_emph ltx_font_italic" id="S1.F1.40.1">(a)</em> a flowchart describing the algorithm: if removing <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.19.m1.1"><semantics id="S1.F1.19.m1.1b"><mi id="S1.F1.19.m1.1.1" xref="S1.F1.19.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.19.m1.1c"><ci id="S1.F1.19.m1.1.1.cmml" xref="S1.F1.19.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.19.m1.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.19.m1.1e">italic_n</annotation></semantics></math> layers, we find the layer, <math alttext="\ell^{*}" class="ltx_Math" display="inline" id="S1.F1.20.m2.1"><semantics id="S1.F1.20.m2.1b"><msup id="S1.F1.20.m2.1.1" xref="S1.F1.20.m2.1.1.cmml"><mi id="S1.F1.20.m2.1.1.2" mathvariant="normal" xref="S1.F1.20.m2.1.1.2.cmml">‚Ñì</mi><mo id="S1.F1.20.m2.1.1.3" xref="S1.F1.20.m2.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F1.20.m2.1c"><apply id="S1.F1.20.m2.1.1.cmml" xref="S1.F1.20.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.20.m2.1.1.1.cmml" xref="S1.F1.20.m2.1.1">superscript</csymbol><ci id="S1.F1.20.m2.1.1.2.cmml" xref="S1.F1.20.m2.1.1.2">‚Ñì</ci><times id="S1.F1.20.m2.1.1.3.cmml" xref="S1.F1.20.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.20.m2.1d">\ell^{*}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.20.m2.1e">roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>, that minimizes the angular distance, <math alttext="d" class="ltx_Math" display="inline" id="S1.F1.21.m3.1"><semantics id="S1.F1.21.m3.1b"><mi id="S1.F1.21.m3.1.1" xref="S1.F1.21.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.F1.21.m3.1c"><ci id="S1.F1.21.m3.1.1.cmml" xref="S1.F1.21.m3.1.1">ùëë</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.21.m3.1d">d</annotation><annotation encoding="application/x-llamapun" id="S1.F1.21.m3.1e">italic_d</annotation></semantics></math>, between layers <math alttext="\ell" class="ltx_Math" display="inline" id="S1.F1.22.m4.1"><semantics id="S1.F1.22.m4.1b"><mi id="S1.F1.22.m4.1.1" mathvariant="normal" xref="S1.F1.22.m4.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S1.F1.22.m4.1c"><ci id="S1.F1.22.m4.1.1.cmml" xref="S1.F1.22.m4.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.22.m4.1d">\ell</annotation><annotation encoding="application/x-llamapun" id="S1.F1.22.m4.1e">roman_‚Ñì</annotation></semantics></math> and <math alttext="\ell\!+\!n" class="ltx_Math" display="inline" id="S1.F1.23.m5.1"><semantics id="S1.F1.23.m5.1b"><mrow id="S1.F1.23.m5.1.1" xref="S1.F1.23.m5.1.1.cmml"><mi id="S1.F1.23.m5.1.1.2" mathvariant="normal" xref="S1.F1.23.m5.1.1.2.cmml">‚Ñì</mi><mo id="S1.F1.23.m5.1.1.1" lspace="0.052em" rspace="0.052em" xref="S1.F1.23.m5.1.1.1.cmml">+</mo><mi id="S1.F1.23.m5.1.1.3" xref="S1.F1.23.m5.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.23.m5.1c"><apply id="S1.F1.23.m5.1.1.cmml" xref="S1.F1.23.m5.1.1"><plus id="S1.F1.23.m5.1.1.1.cmml" xref="S1.F1.23.m5.1.1.1"></plus><ci id="S1.F1.23.m5.1.1.2.cmml" xref="S1.F1.23.m5.1.1.2">‚Ñì</ci><ci id="S1.F1.23.m5.1.1.3.cmml" xref="S1.F1.23.m5.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.23.m5.1d">\ell\!+\!n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.23.m5.1e">roman_‚Ñì + italic_n</annotation></semantics></math>; we then remove the <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.24.m6.1"><semantics id="S1.F1.24.m6.1b"><mi id="S1.F1.24.m6.1.1" xref="S1.F1.24.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.24.m6.1c"><ci id="S1.F1.24.m6.1.1.cmml" xref="S1.F1.24.m6.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.24.m6.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.24.m6.1e">italic_n</annotation></semantics></math> layers beginning with layer <math alttext="\ell^{*}" class="ltx_Math" display="inline" id="S1.F1.25.m7.1"><semantics id="S1.F1.25.m7.1b"><msup id="S1.F1.25.m7.1.1" xref="S1.F1.25.m7.1.1.cmml"><mi id="S1.F1.25.m7.1.1.2" mathvariant="normal" xref="S1.F1.25.m7.1.1.2.cmml">‚Ñì</mi><mo id="S1.F1.25.m7.1.1.3" xref="S1.F1.25.m7.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F1.25.m7.1c"><apply id="S1.F1.25.m7.1.1.cmml" xref="S1.F1.25.m7.1.1"><csymbol cd="ambiguous" id="S1.F1.25.m7.1.1.1.cmml" xref="S1.F1.25.m7.1.1">superscript</csymbol><ci id="S1.F1.25.m7.1.1.2.cmml" xref="S1.F1.25.m7.1.1.2">‚Ñì</ci><times id="S1.F1.25.m7.1.1.3.cmml" xref="S1.F1.25.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.25.m7.1d">\ell^{*}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.25.m7.1e">roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>; finally, if necessary, we can ‚Äúheal‚Äù the damage with a small amount of (parameter-efficient) finetuning.
<em class="ltx_emph ltx_font_italic" id="S1.F1.41.2">(b)</em> a schematic depicting the removal of <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.26.m8.1"><semantics id="S1.F1.26.m8.1b"><mi id="S1.F1.26.m8.1.1" xref="S1.F1.26.m8.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.26.m8.1c"><ci id="S1.F1.26.m8.1.1.cmml" xref="S1.F1.26.m8.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.26.m8.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.26.m8.1e">italic_n</annotation></semantics></math> total layers, indexed from <math alttext="\ell^{*}\!" class="ltx_Math" display="inline" id="S1.F1.27.m9.1"><semantics id="S1.F1.27.m9.1b"><msup id="S1.F1.27.m9.1.1" xref="S1.F1.27.m9.1.1.cmml"><mi id="S1.F1.27.m9.1.1.2" mathvariant="normal" xref="S1.F1.27.m9.1.1.2.cmml">‚Ñì</mi><mo id="S1.F1.27.m9.1.1.3" xref="S1.F1.27.m9.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F1.27.m9.1c"><apply id="S1.F1.27.m9.1.1.cmml" xref="S1.F1.27.m9.1.1"><csymbol cd="ambiguous" id="S1.F1.27.m9.1.1.1.cmml" xref="S1.F1.27.m9.1.1">superscript</csymbol><ci id="S1.F1.27.m9.1.1.2.cmml" xref="S1.F1.27.m9.1.1.2">‚Ñì</ci><times id="S1.F1.27.m9.1.1.3.cmml" xref="S1.F1.27.m9.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.27.m9.1d">\ell^{*}\!</annotation><annotation encoding="application/x-llamapun" id="S1.F1.27.m9.1e">roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="\ell^{*}\!\!+\!n\!-\!1" class="ltx_Math" display="inline" id="S1.F1.28.m10.1"><semantics id="S1.F1.28.m10.1b"><mrow id="S1.F1.28.m10.1.1" xref="S1.F1.28.m10.1.1.cmml"><mrow id="S1.F1.28.m10.1.1.2" xref="S1.F1.28.m10.1.1.2.cmml"><msup id="S1.F1.28.m10.1.1.2.2" xref="S1.F1.28.m10.1.1.2.2.cmml"><mi id="S1.F1.28.m10.1.1.2.2.2" mathvariant="normal" xref="S1.F1.28.m10.1.1.2.2.2.cmml">‚Ñì</mi><mo id="S1.F1.28.m10.1.1.2.2.3" xref="S1.F1.28.m10.1.1.2.2.3.cmml">*</mo></msup><mo id="S1.F1.28.m10.1.1.2.1" rspace="0.052em" xref="S1.F1.28.m10.1.1.2.1.cmml">+</mo><mi id="S1.F1.28.m10.1.1.2.3" xref="S1.F1.28.m10.1.1.2.3.cmml">n</mi></mrow><mo id="S1.F1.28.m10.1.1.1" lspace="0.052em" rspace="0.052em" xref="S1.F1.28.m10.1.1.1.cmml">‚àí</mo><mn id="S1.F1.28.m10.1.1.3" xref="S1.F1.28.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.28.m10.1c"><apply id="S1.F1.28.m10.1.1.cmml" xref="S1.F1.28.m10.1.1"><minus id="S1.F1.28.m10.1.1.1.cmml" xref="S1.F1.28.m10.1.1.1"></minus><apply id="S1.F1.28.m10.1.1.2.cmml" xref="S1.F1.28.m10.1.1.2"><plus id="S1.F1.28.m10.1.1.2.1.cmml" xref="S1.F1.28.m10.1.1.2.1"></plus><apply id="S1.F1.28.m10.1.1.2.2.cmml" xref="S1.F1.28.m10.1.1.2.2"><csymbol cd="ambiguous" id="S1.F1.28.m10.1.1.2.2.1.cmml" xref="S1.F1.28.m10.1.1.2.2">superscript</csymbol><ci id="S1.F1.28.m10.1.1.2.2.2.cmml" xref="S1.F1.28.m10.1.1.2.2.2">‚Ñì</ci><times id="S1.F1.28.m10.1.1.2.2.3.cmml" xref="S1.F1.28.m10.1.1.2.2.3"></times></apply><ci id="S1.F1.28.m10.1.1.2.3.cmml" xref="S1.F1.28.m10.1.1.2.3">ùëõ</ci></apply><cn id="S1.F1.28.m10.1.1.3.cmml" type="integer" xref="S1.F1.28.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.28.m10.1d">\ell^{*}\!\!+\!n\!-\!1</annotation><annotation encoding="application/x-llamapun" id="S1.F1.28.m10.1e">roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT + italic_n - 1</annotation></semantics></math>. <em class="ltx_emph ltx_font_italic" id="S1.F1.42.3">(c)</em> angular distance, <math alttext="d" class="ltx_Math" display="inline" id="S1.F1.29.m11.1"><semantics id="S1.F1.29.m11.1b"><mi id="S1.F1.29.m11.1.1" xref="S1.F1.29.m11.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.F1.29.m11.1c"><ci id="S1.F1.29.m11.1.1.cmml" xref="S1.F1.29.m11.1.1">ùëë</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.29.m11.1d">d</annotation><annotation encoding="application/x-llamapun" id="S1.F1.29.m11.1e">italic_d</annotation></semantics></math>, between different numbers of layers, <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.30.m12.1"><semantics id="S1.F1.30.m12.1b"><mi id="S1.F1.30.m12.1.1" xref="S1.F1.30.m12.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.30.m12.1c"><ci id="S1.F1.30.m12.1.1.cmml" xref="S1.F1.30.m12.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.30.m12.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.30.m12.1e">italic_n</annotation></semantics></math>, vs. the layer number, <math alttext="\ell" class="ltx_Math" display="inline" id="S1.F1.31.m13.1"><semantics id="S1.F1.31.m13.1b"><mi id="S1.F1.31.m13.1.1" mathvariant="normal" xref="S1.F1.31.m13.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S1.F1.31.m13.1c"><ci id="S1.F1.31.m13.1.1.cmml" xref="S1.F1.31.m13.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.31.m13.1d">\ell</annotation><annotation encoding="application/x-llamapun" id="S1.F1.31.m13.1e">roman_‚Ñì</annotation></semantics></math>, that indexes the beginning of the block of <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.32.m14.1"><semantics id="S1.F1.32.m14.1b"><mi id="S1.F1.32.m14.1.1" xref="S1.F1.32.m14.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.32.m14.1c"><ci id="S1.F1.32.m14.1.1.cmml" xref="S1.F1.32.m14.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.32.m14.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.32.m14.1e">italic_n</annotation></semantics></math>; the bottom curve (darkest purple) represents <math alttext="n=1" class="ltx_Math" display="inline" id="S1.F1.33.m15.1"><semantics id="S1.F1.33.m15.1b"><mrow id="S1.F1.33.m15.1.1" xref="S1.F1.33.m15.1.1.cmml"><mi id="S1.F1.33.m15.1.1.2" xref="S1.F1.33.m15.1.1.2.cmml">n</mi><mo id="S1.F1.33.m15.1.1.1" xref="S1.F1.33.m15.1.1.1.cmml">=</mo><mn id="S1.F1.33.m15.1.1.3" xref="S1.F1.33.m15.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.33.m15.1c"><apply id="S1.F1.33.m15.1.1.cmml" xref="S1.F1.33.m15.1.1"><eq id="S1.F1.33.m15.1.1.1.cmml" xref="S1.F1.33.m15.1.1.1"></eq><ci id="S1.F1.33.m15.1.1.2.cmml" xref="S1.F1.33.m15.1.1.2">ùëõ</ci><cn id="S1.F1.33.m15.1.1.3.cmml" type="integer" xref="S1.F1.33.m15.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.33.m15.1d">n=1</annotation><annotation encoding="application/x-llamapun" id="S1.F1.33.m15.1e">italic_n = 1</annotation></semantics></math>, while the top curve (lightest yellow) represents <math alttext="n=64" class="ltx_Math" display="inline" id="S1.F1.34.m16.1"><semantics id="S1.F1.34.m16.1b"><mrow id="S1.F1.34.m16.1.1" xref="S1.F1.34.m16.1.1.cmml"><mi id="S1.F1.34.m16.1.1.2" xref="S1.F1.34.m16.1.1.2.cmml">n</mi><mo id="S1.F1.34.m16.1.1.1" xref="S1.F1.34.m16.1.1.1.cmml">=</mo><mn id="S1.F1.34.m16.1.1.3" xref="S1.F1.34.m16.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.34.m16.1c"><apply id="S1.F1.34.m16.1.1.cmml" xref="S1.F1.34.m16.1.1"><eq id="S1.F1.34.m16.1.1.1.cmml" xref="S1.F1.34.m16.1.1.1"></eq><ci id="S1.F1.34.m16.1.1.2.cmml" xref="S1.F1.34.m16.1.1.2">ùëõ</ci><cn id="S1.F1.34.m16.1.1.3.cmml" type="integer" xref="S1.F1.34.m16.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.34.m16.1d">n=64</annotation><annotation encoding="application/x-llamapun" id="S1.F1.34.m16.1e">italic_n = 64</annotation></semantics></math>; the black line traces <math alttext="\ell^{*}(n)" class="ltx_Math" display="inline" id="S1.F1.35.m17.1"><semantics id="S1.F1.35.m17.1b"><mrow id="S1.F1.35.m17.1.2" xref="S1.F1.35.m17.1.2.cmml"><msup id="S1.F1.35.m17.1.2.2" xref="S1.F1.35.m17.1.2.2.cmml"><mi id="S1.F1.35.m17.1.2.2.2" mathvariant="normal" xref="S1.F1.35.m17.1.2.2.2.cmml">‚Ñì</mi><mo id="S1.F1.35.m17.1.2.2.3" xref="S1.F1.35.m17.1.2.2.3.cmml">*</mo></msup><mo id="S1.F1.35.m17.1.2.1" xref="S1.F1.35.m17.1.2.1.cmml">‚Å¢</mo><mrow id="S1.F1.35.m17.1.2.3.2" xref="S1.F1.35.m17.1.2.cmml"><mo id="S1.F1.35.m17.1.2.3.2.1" stretchy="false" xref="S1.F1.35.m17.1.2.cmml">(</mo><mi id="S1.F1.35.m17.1.1" xref="S1.F1.35.m17.1.1.cmml">n</mi><mo id="S1.F1.35.m17.1.2.3.2.2" stretchy="false" xref="S1.F1.35.m17.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.35.m17.1c"><apply id="S1.F1.35.m17.1.2.cmml" xref="S1.F1.35.m17.1.2"><times id="S1.F1.35.m17.1.2.1.cmml" xref="S1.F1.35.m17.1.2.1"></times><apply id="S1.F1.35.m17.1.2.2.cmml" xref="S1.F1.35.m17.1.2.2"><csymbol cd="ambiguous" id="S1.F1.35.m17.1.2.2.1.cmml" xref="S1.F1.35.m17.1.2.2">superscript</csymbol><ci id="S1.F1.35.m17.1.2.2.2.cmml" xref="S1.F1.35.m17.1.2.2.2">‚Ñì</ci><times id="S1.F1.35.m17.1.2.2.3.cmml" xref="S1.F1.35.m17.1.2.2.3"></times></apply><ci id="S1.F1.35.m17.1.1.cmml" xref="S1.F1.35.m17.1.1">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.35.m17.1d">\ell^{*}(n)</annotation><annotation encoding="application/x-llamapun" id="S1.F1.35.m17.1e">roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math>, the minimum of the angular distance across the different sized layer blocks.
<em class="ltx_emph ltx_font_italic" id="S1.F1.43.4">(d)</em> results of pruning Llama-2-70B with healing (dark blue) and without healing (light blue) as a function of the fraction of layers removed: the top (middle) panel gives the accuracy on the MMLU (BoolQ) question-answering benchmark, while the bottom panel the autoregressive loss on a subset of the C4 validation set;
here, the dashed red lines (dashed gray lines) indicate the accuracy or loss of the original unpruned model (of random guessing);
these plots illustrate that typical behavior we find in which there are sharp transitions in performance for the accuracy of question-answering tasks (here between 40%-50% pruning fraction), but continuity and very slow growth in the healed loss (dark blue) up to at least to 80% pruning fraction.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Pruning is not only useful for reducing the footprint of inference, but also for understanding how a network uses its parameters: if you can remove large blocks of the network with minimal effect on its performance, then those blocks are likely not very important. In particular, our intuition for dropping layers comes considering the residual structure of the transformer architecture. In more detail, the output of the final layer can be decomposed as a sum over the outputs of all the model layers plus the
embedded input.
If such a sum
had numerous
and
independent terms, then removing a handful of
them
should not significantly change the output. However, since the terms are not independent
‚Äì each layer is
input to the following layer ‚Äì
we should expect to be able to remove terms if the residual contribution from a particular layer is small. In other words, if the output of each layer does not change too much from layer to layer.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This is strongly suggested
by
‚Äúlens‚Äù investigations
that studied the evolution of the token distribution
as a function of layer index
such as
the ‚Äúlogit lens‚Äù <cite class="ltx_cite ltx_citemacro_cite">nostalgebraist (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib21" title="">2020</a>)</cite> and the
‚Äútuned lens‚Äù <cite class="ltx_cite ltx_citemacro_cite">Belrose et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib22" title="">2023</a>)</cite>.
A separate line of
reasoning along these lines previously inspired neural ODEs <cite class="ltx_cite ltx_citemacro_cite">Chen et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib23" title="">2018</a>)</cite>,
and led Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Yang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib24" title="">2023</a>)</cite>
to argue
that
ideally representation should change substantially
from layer to layer
in order to most
effectively make use of the parameters
of a network.
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In conjunction with our layer pruning, we investigate
the similarity of layer representations at different separations
and find broadly that deeper layers are qualitatively more similar to neighboring layers than shallow layers (with the exception of the very final layer).
This suggests an even simpler pruning strategy: remove layers beginning at the penultimate layer and proceed from deep to shallow until the desired number of layers have been removed.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>This strategy is especially interesting in situations where resource constraints inhibit the full application of the similarity-informed pruning algorithm described in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‚Ä£ 4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>(a).</span></span></span>
In this case, we find that,
after healing the damage with a small amount of QLoRA finetuning,
we can achieve performance that nearly matches the more involved similarity-informed layer pruning strategy.
The effectiveness of this method is
evidence that LLMs might not properly leverage the parameters in the deeper layers of the network.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Overall, we hope you take these three bulleted points with you:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">The model‚Äôs memory footprint <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.1">and</em> inference
time
decreases linearly with the number of removed layers.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Contrast this with quantization: the memory footprint decreases with the quantization ratio, but the inference time remains approximately fixed since parameters are typically de-quantized before any FLOPs.</span></span></span> This
makes layer pruning a powerful tool, especially if the model‚Äôs performance is robust to dropping layers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">All the efficiency methods ‚Äì pruning, PEFT and quantization ‚Äì can be effectively combined with each other. Thus, in this work <em class="ltx_emph ltx_font_italic" id="S1.I1.i2.p1.1.1">each experiment was performed on a single A100 GPU</em> and is easily accessible to the open source and academic communities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">The robustness of models to removing the deeper layers,
the sharp transition in performance on downstream knowledge tasks (e.g. MMLU and BoolQ),
and the smooth behavior of the autoregressive loss
with respect to those
pruning fractions, altogether suggest that the shallow layers
may play a critical role in
storing knowledge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The structure of this paper is as follows. In ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2" title="2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>, we first perform a literature review of both practical post-training strategies and science-of-deep-learning investigations that motivate our work.
Then, in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3" title="3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a>, we give intuition for our layer pruning strategy
and explain our method in detail,
while in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4" title="4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4</span></a> we
iterate over all our experimental results. Finally, we conclude in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S5" title="5 Discussion and Future Directions ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">5</span></a>
by highlighting directions of future work. Specific model, finetuning, dataset, and evaluation details can be found in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1" title="Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A</span></a>, and evaluations ablations can be found in
Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2" title="Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B</span></a>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text ltx_font_bold" id="S1.p8.1.1">Note:</span> As we were finalizing this work,
a preprint of
Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Men et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib25" title="">2024</a>)</cite> was posted, which has a number of points of overlap with our work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Review</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we review practical strategies for post-training efficiency
and discuss
some
scientific investigations
that
provide motivation for, or insight into,
our approach:
in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS1" title="2.1 Pruning ‚Ä£ 2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2.1</span></a>, we first review the history of pruning and then discuss its modern
application to LLMs;
in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS2" title="2.2 Model distillation ‚Ä£ 2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2.2</span></a>, we contrast pruning with distillation, an alternative strategy for reducing the parameter count of LLMs;
then in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS3" title="2.3 Efficient finetuning and inference acceleration ‚Ä£ 2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2.3</span></a>, we discuss the various practical methods for efficient finetuning and inference acceleration that can be used in conjunction with our pruning strategy;
finally in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS4" title="2.4 A breadth of depth-dependent studies ‚Ä£ 2 Literature Review ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2.4</span></a> we highlight some scientific investigations into
some
depth-dependent statistical properties of LLMs that
are complementary to our results.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Pruning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">Pruning</em> is a method for reducing the size of a trained machine-learning model by removing unnecessary parameters, either individually or together as a group.
Pruning for neural networks has a long history <cite class="ltx_cite ltx_citemacro_citep">(LeCun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib14" title="">1989</a>; Hassibi and Stork, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib15" title="">1992</a>)</cite>,
and, as originally conceived,
<em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.2">unstructured pruning</em> techniques
sparsify networks by removing individual parameters based on pre-defined criteria.
For instance, if a parameter of the model has a very small value, then removing it ‚Äì i.e. by setting it to exactly zero ‚Äì will likely have minimal impact on
performance.
Inspired by this early work, modern researchers began exploring different criteria for
such
unstructured
pruning, focusing mostly on computer vision models <cite class="ltx_cite ltx_citemacro_citep">(Han et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib16" title="">2015</a>; Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib26" title="">2015</a>; Srinivas and Babu, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib27" title="">2015</a>)</cite>.
In particular, Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Han et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib16" title="">2015</a>)</cite> developed an <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.3">iterative pruning</em> method for alternatively pruning and finetuning a network in order to
reach better compression ratios and performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">While these models were smaller, they were not
necessarily
more efficient:
sparsifying networks by removing individual parameters according to
a
criterion leads to irregular or pseudorandom sparsification patterns that are difficult to accelerate without specialized hardware or libraries designed for sparsity <cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib17" title="">2016</a>)</cite>.
To that end, <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">structured pruning</em> techniques were developed to remove irrelevant groups of parameters together, such as particular channels or filters in convolutional networks.
As this increased their practical relevance,
researchers then began exploring structured pruning across computer vision <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib17" title="">2016</a>; Wen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib28" title="">2016</a>; Hu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib29" title="">2016</a>; He et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib30" title="">2017</a>; Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib31" title="">2018</a>)</cite> and pre-transformer NLP architectures <cite class="ltx_cite ltx_citemacro_citep">(Murray and Chiang, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib32" title="">2015</a>; See et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib33" title="">2016</a>; Kim and Rush, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib34" title="">2016</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Following
unprecedented progress in language modeling, recent work
has focused on
applying structured pruning methods to the Transformer <cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib35" title="">2017</a>)</cite>.
These studies consider nearly every possible component of the model architecture for elimination, with methods
ranging from dropping attention heads <cite class="ltx_cite ltx_citemacro_citep">(Voita et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib36" title="">2019</a>; Michel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib37" title="">2019</a>; Kim and Awadalla, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib38" title="">2020</a>)</cite>, to dropping layers <cite class="ltx_cite ltx_citemacro_citep">(Fan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib39" title="">2019</a>; Zhang and He, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib40" title="">2020</a>; Fan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib41" title="">2021</a>; Jha et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib42" title="">2023</a>; Sajjad et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib43" title="">2023</a>; Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib44" title="">2023a</a>)</cite>, to pruning hidden states <cite class="ltx_cite ltx_citemacro_citep">(Hou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib45" title="">2020</a>)</cite>,
to rank reducing large weight matrices
<cite class="ltx_cite ltx_citemacro_cite">Sharma et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib46" title="">2023</a>)</cite>, replacing sparse weight matrices with smaller dense ones <cite class="ltx_cite ltx_citemacro_cite">Ashkboos et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib47" title="">2024</a>)</cite>,
to
many
combinations of the aforementioned groups <cite class="ltx_cite ltx_citemacro_citep">(Xia et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib48" title="">2022</a>; Lagunas et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib49" title="">2021</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Of the prior work that also considers transformer layer dropping, most <cite class="ltx_cite ltx_citemacro_cite">Fan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib39" title="">2019</a>); Zhang and He (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib40" title="">2020</a>); Fan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib41" title="">2021</a>); Xia et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib48" title="">2022</a>); Sajjad et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib43" title="">2023</a>)</cite> study BERT-style models <cite class="ltx_cite ltx_citemacro_cite">Devlin et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib50" title="">2018</a>)</cite>, while we consider decoder-only GPT-style models <cite class="ltx_cite ltx_citemacro_cite">Radford et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib1" title="">2019</a>)</cite> that are most commonly used for large-scale language modeling and generation.
BERT-style models are
naturally
suited for
understanding tasks
due to their bidirectional masked language modeling (MLM) objective,
while GPT-style models are instead
suited for
generation,
due to their autoregressive
objective.
While this divide has been questioned in light of more powerful GPT-style models <cite class="ltx_cite ltx_citemacro_citep">(Zhong et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib51" title="">2023</a>)</cite>, previous work <cite class="ltx_cite ltx_citemacro_citep">(Ethayarajh, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib52" title="">2019</a>)</cite>
has found significant qualitative differences between BERT and GPT models in terms of the evolution of the layer-wise representation of words. Altogether, this
suggests that layer-dropping strategies will behave differently between the two families.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">One study
for BERT-style pre-trained models, Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Sajjad et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib43" title="">2023</a>)</cite>, concludes that the best layer-pruning strategy is
dropping the final layers;
this partially resonates with our results,
although
in contrast
we find that
<em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.1">(a)</em> for
some pruning sizes
keeping the last few layers of the model is actually beneficial, and
that
<em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.2">(b)</em> for all pruning sizes keeping the very last layer is essential.
Additionally,
while the authors also
study similarity between representations in different layers
‚Äì as in our approach ‚Äì
they actually found a higher similarity between representations in the shallow layers compared to the deeper ones ‚Äì which
very sharply disagrees
with our results.
Importantly, the models considered in Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Sajjad et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib43" title="">2023</a>)</cite> consist of a few hundred million parameters, which is much smaller than the model scales we consider in our work.
Perhaps as a consequence, the authors didn‚Äôt observe the
sharp transition
in downstream accuracies that we report in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS1" title="4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.1</span></a>, despite the fact that they also finetuned their pruned models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1">In contrast, while Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Jha et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib42" title="">2023</a>)</cite> does consider GPT-style models, the methodology is quite different from ours: <em class="ltx_emph ltx_font_italic" id="S2.SS1.p6.1.1">(i)</em> rather than pretraining first and then using a fixed layer-dropping strategy as we do, instead the authors incrementally drop layers in a modified pretraining procedure; and
<em class="ltx_emph ltx_font_italic" id="S2.SS1.p6.1.2">(ii)</em> the authors study their own sub-1B parameter models, while we focus on the families of readily available, open-weight, large-scale 2.7B-70B parameter models
that are commonly used and/or finetuned
for practical applications.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p7">
<p class="ltx_p" id="S2.SS1.p7.1">Finally, a systematic approach to layer dropping in transformers has also been
studied
in the context of <em class="ltx_emph ltx_font_italic" id="S2.SS1.p7.1.1">wav2vec</em> models,
which are
encoder-only models that map speech to embeddings and are sized
in the hundred-million parameter regime <cite class="ltx_cite ltx_citemacro_cite">Baevski et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib53" title="">2020</a>)</cite>.
With these models, Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib44" title="">2023a</a>)</cite> developed a layer-pruning algorithm
based on the correlation between layers
and downstream metrics.
Beyond the model architecture and domain, one significant difference between this and our work is that Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib44" title="">2023a</a>)</cite> considered non-contiguous pruning proposals, e.g. dropping alternate layers.
Our intuition for layer pruning predicts that
this shouldn‚Äôt work as well
‚Äì at least for decoder-only language models ‚Äì
as it creates multiple mismatches,
one with each block of layers removed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model distillation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">A completely different
method for reducing the size of a trained machine-learning model is
<em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">model distillation</em> <cite class="ltx_cite ltx_citemacro_cite">Hinton et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib54" title="">2015</a>)</cite>,
in which knowledge is
transferred
from a
large
‚Äúteacher‚Äù model
to a
smaller
‚Äústudent‚Äù
model by training the student on the distribution
predicted
by the teacher.
The essential insight
is
that this can
transform the very general knowledge and capabilities of the teacher into more streamlined, compressed, and possibly skill-specific representations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">While a very general technique, in the
setting of language models,
distillation has been implemented with
<em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1">(a)</em>
white-box approaches,
in which the
the student is trained to imitate the teacher‚Äôs logits <cite class="ltx_cite ltx_citemacro_cite">Gu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib55" title="">2023</a>)</cite> or hidden states <cite class="ltx_cite ltx_citemacro_cite">Jiao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib56" title="">2019</a>)</cite>; as well as with
<em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.2">(b)</em>
black-box approaches,
in which the student
only has access to the output tokens generated by the teacher.
This latter
approach
broadly
covers cases where the student is trained on text
that is augmented by the teacher in some way, such as
by adding
synthetic labels <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib57" title="">2021</a>)</cite>, generating high quality synthetic text <cite class="ltx_cite ltx_citemacro_cite">Eldan and Li (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib58" title="">2023</a>); Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib59" title="">2023a</a>); Gunasekar et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib60" title="">2023</a>)</cite> by providing chain of thought reasoning <cite class="ltx_cite ltx_citemacro_cite">Fu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib61" title="">2023</a>); Hsieh et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib62" title="">2023</a>)</cite>, which aims to enhance the student‚Äôs reasoning skills, or by annotating
instructions
that enhance
the student‚Äôs
instruction-following capabilities <cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib63" title="">2023a</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Compared to layer pruning, these distillation methods require
considerable computational resources due to the
reliance on the large teacher to process a big corpus of data.
Instead, our similarity-based pruning strategy only requires computing the similarity between representations at different layers on a small subset of a pretraining corpus, while our second simpler pruning strategy
only
uses
the reduced model post pruning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Efficient finetuning
and inference acceleration</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Complementary to directly reducing
size of a model,
<em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">parameter-efficient finetuning</em> (PEFT)
focuses on reducing the cost of specializing LLMs to certain tasks.
In particular, Low Rank Adapters (LoRA) reduce the memory and compute of fine tuning by freezing the pretrained model and introducing a parametrically small number of additional trainable weights
<cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib13" title="">2021</a>)</cite>.
We use its quantized cousin, QLoRA <cite class="ltx_cite ltx_citemacro_cite">Dettmers et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>, to keep our experiments cost efficient.
Other PEFT methods that can be combined with our work are Refs.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Li et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib64" title="">2023b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib65" title="">2023</a>)</cite>: in the first, the initialization of the LoRA matrices is adjusted to a quantization scheme; in the second, LoRA ranks for different LLM modules are chosen in an adaptive manner.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">For
additional
efficiency gains we could
combine our layer-pruned models with methods that further accelerate inference: with speculative decoding <cite class="ltx_cite ltx_citemacro_cite">Leviathan et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib66" title="">2023</a>)</cite>, tokens are rapidly generated from a smaller draft model and then evaluated in parallel by the main model; with Medusa <cite class="ltx_cite ltx_citemacro_cite">Cai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib67" title="">2024</a>)</cite> the draft model is discarded for extra decoding heads, but ultimately achieves a similar effect.
In particular, it could be interesting to consider highly-compressed layer-pruned models as
potential
draft models in a speculative decoding setup.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>A breadth of depth-dependent studies</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Finally, let us
highlight some scientific work that
study the depth-dependent properties of LLMs.
One relevant direction
considers
how
knowledge and linguistic properties are encoded in
language models.
On the one hand, Refs. <cite class="ltx_cite ltx_citemacro_cite">Meng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib68" title="">2022</a>); Dai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib69" title="">2021</a>)</cite> analyze the
<em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.1">storage
and recall</em>
of factual associations: these works
emphasize that knowledge localizes within the middle <cite class="ltx_cite ltx_citemacro_cite">Meng et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib68" title="">2022</a>)</cite> or final <cite class="ltx_cite ltx_citemacro_cite">Dai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib69" title="">2021</a>)</cite> layers,
which has implications for
directly
editing or erasing part of a model‚Äôs factual knowledge.
On the other hand,
attempts to perform such editing gives
evidence that information
may be
stored non-locally across layers <cite class="ltx_cite ltx_citemacro_cite">Hase et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib70" title="">2023</a>)</cite>.
Relatedly,
Ref. <cite class="ltx_cite ltx_citemacro_cite">Geva et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib71" title="">2023</a>)</cite> investigates
the way
facts are <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.2">processed</em>
during inference,
distinguishing between the role of attention heads,
for attribute extraction,
and the MLP blocks,
for subject enrichment:
both
are delocalized across several layers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Next, following the earlier ‚Äúlogic lens‚Äù <cite class="ltx_cite ltx_citemacro_cite">nostalgebraist (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib21" title="">2020</a>)</cite>,
Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Belrose et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib22" title="">2023</a>)</cite>
invented a technique they called ‚Äútuned lens‚Äù to
study the <em class="ltx_emph ltx_font_italic" id="S2.SS4.p2.1.1">trajectory of predictions</em> by
using a learnable affine transformation to convert intermediate representations into a distributions over tokens (see also <cite class="ltx_cite ltx_citemacro_cite">Din et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib72" title="">2023</a>)</cite>).
By studying the layer-to-layer dynamics of this distribution, the authors noted that it tended to converge.
This convergence is very suggestive that
that the deeper layers could be prunable, while the fact that they had to train an affine probe is likely related to our observation that the final layer cannot be pruned.
Somewhat relatedly,
Ref. <cite class="ltx_cite ltx_citemacro_cite">Gurnee and Tegmark (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib73" title="">2023</a>)</cite>
observed that
geographic features in the underlying text
can be determined from linear probes trained on
intermediate activations,
as long as the activations are deeper than halfway.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">More abstractly,
Refs. <cite class="ltx_cite ltx_citemacro_cite">Voita et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib74" title="">2023</a>); Liu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib75" title="">2023b</a>)</cite>
found that
the sparsity of activations
transitions
at around halfway through a network‚Äôs forward pass,
evolving from sparse to dense.
Perhaps relatedly,
Ref. <cite class="ltx_cite ltx_citemacro_cite">Panigrahi et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib76" title="">2023</a>)</cite>
investigated
which model weights update the most during finetuning,
finding that
it‚Äôs those in the mid-layers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS4.p4">
<p class="ltx_p" id="S2.SS4.p4.1">Altogether,
these deep studies are complementary to our work, which,
on the one hand,
provides evidence that removing the deepest layers of an LLM
does not significantly alter the model‚Äôs performance,
and,
on the other hand,
demonstrates
a sharp pruning transition after removing approximately half of an LLM‚Äôs deepest layers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we give intuition for why
we think layer pruning works
(¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS1" title="3.1 Intuition ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.1</span></a>)
and then we explain our method in detail (¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Intuition</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.8">Our intuition for layer dropping comes from thinking about
the
representations
as a slowly changing function of layer index. In particular, the
layer-to-layer evolution of representations for a transformer is given by a <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.8.1">residual</em> iteration equation</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(\ell+1)}=x^{(\ell)}+f(x^{(\ell)},\theta^{(\ell)})\,," class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><msup id="S3.E1.m1.5.5.1.1.4" xref="S3.E1.m1.5.5.1.1.4.cmml"><mi id="S3.E1.m1.5.5.1.1.4.2" xref="S3.E1.m1.5.5.1.1.4.2.cmml">x</mi><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.2" mathvariant="normal" xref="S3.E1.m1.1.1.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E1.m1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml"><msup id="S3.E1.m1.5.5.1.1.2.4" xref="S3.E1.m1.5.5.1.1.2.4.cmml"><mi id="S3.E1.m1.5.5.1.1.2.4.2" xref="S3.E1.m1.5.5.1.1.2.4.2.cmml">x</mi><mrow id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.5.5.1.1.2.4.cmml"><mo id="S3.E1.m1.2.2.1.3.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.4.cmml">(</mo><mi id="S3.E1.m1.2.2.1.1" mathvariant="normal" xref="S3.E1.m1.2.2.1.1.cmml">‚Ñì</mi><mo id="S3.E1.m1.2.2.1.3.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.4.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.5.5.1.1.2.3" xref="S3.E1.m1.5.5.1.1.2.3.cmml">+</mo><mrow id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2.4" xref="S3.E1.m1.5.5.1.1.2.2.4.cmml">f</mi><mo id="S3.E1.m1.5.5.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.2.2.3.cmml">‚Å¢</mo><mrow id="S3.E1.m1.5.5.1.1.2.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml"><mo id="S3.E1.m1.5.5.1.1.2.2.2.2.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml">(</mo><msup id="S3.E1.m1.5.5.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.3.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.3.3.1.1" mathvariant="normal" xref="S3.E1.m1.3.3.1.1.cmml">‚Ñì</mi><mo id="S3.E1.m1.3.3.1.3.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.5.5.1.1.2.2.2.2.4" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml">,</mo><msup id="S3.E1.m1.5.5.1.1.2.2.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2.2.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.2.cmml">Œ∏</mi><mrow id="S3.E1.m1.4.4.1.3" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml"><mo id="S3.E1.m1.4.4.1.3.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml">(</mo><mi id="S3.E1.m1.4.4.1.1" mathvariant="normal" xref="S3.E1.m1.4.4.1.1.cmml">‚Ñì</mi><mo id="S3.E1.m1.4.4.1.3.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.5.5.1.1.2.2.2.2.5" rspace="0.170em" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"></eq><apply id="S3.E1.m1.5.5.1.1.4.cmml" xref="S3.E1.m1.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.4.1.cmml" xref="S3.E1.m1.5.5.1.1.4">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.4.2.cmml" xref="S3.E1.m1.5.5.1.1.4.2">ùë•</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2">‚Ñì</ci><cn id="S3.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"><plus id="S3.E1.m1.5.5.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.3"></plus><apply id="S3.E1.m1.5.5.1.1.2.4.cmml" xref="S3.E1.m1.5.5.1.1.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.4.1.cmml" xref="S3.E1.m1.5.5.1.1.2.4">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.4.2.cmml" xref="S3.E1.m1.5.5.1.1.2.4.2">ùë•</ci><ci id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1">‚Ñì</ci></apply><apply id="S3.E1.m1.5.5.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><times id="S3.E1.m1.5.5.1.1.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.3"></times><ci id="S3.E1.m1.5.5.1.1.2.2.4.cmml" xref="S3.E1.m1.5.5.1.1.2.2.4">ùëì</ci><interval closure="open" id="S3.E1.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2"><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2">ùë•</ci><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">‚Ñì</ci></apply><apply id="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.2">ùúÉ</ci><ci id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1">‚Ñì</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">x^{(\ell+1)}=x^{(\ell)}+f(x^{(\ell)},\theta^{(\ell)})\,,</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + 1 ) end_POSTSUPERSCRIPT = italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT + italic_f ( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT , italic_Œ∏ start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.5">where <math alttext="(x^{(\ell)}" class="ltx_math_unparsed" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1b"><mo id="S3.SS1.p1.1.m1.1.2" stretchy="false">(</mo><msup id="S3.SS1.p1.1.m1.1.3"><mi id="S3.SS1.p1.1.m1.1.3.2">x</mi><mrow id="S3.SS1.p1.1.m1.1.1.1.3"><mo id="S3.SS1.p1.1.m1.1.1.1.3.1" stretchy="false">(</mo><mi id="S3.SS1.p1.1.m1.1.1.1.1" mathvariant="normal">‚Ñì</mi><mo id="S3.SS1.p1.1.m1.1.1.1.3.2" stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">(x^{(\ell)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\theta^{(\ell)})" class="ltx_math_unparsed" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1b"><msup id="S3.SS1.p1.2.m2.1.2"><mi id="S3.SS1.p1.2.m2.1.2.2">Œ∏</mi><mrow id="S3.SS1.p1.2.m2.1.1.1.3"><mo id="S3.SS1.p1.2.m2.1.1.1.3.1" stretchy="false">(</mo><mi id="S3.SS1.p1.2.m2.1.1.1.1" mathvariant="normal">‚Ñì</mi><mo id="S3.SS1.p1.2.m2.1.1.1.3.2" stretchy="false">)</mo></mrow></msup><mo id="S3.SS1.p1.2.m2.1.3" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\theta^{(\ell)})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_Œ∏ start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT )</annotation></semantics></math>, respectively, are the multi-dimensional input and parameter vectors for layer <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p1.3.m3.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">roman_‚Ñì</annotation></semantics></math>, and <math alttext="f(x,\theta)" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.2"><semantics id="S3.SS1.p1.4.m4.2a"><mrow id="S3.SS1.p1.4.m4.2.3" xref="S3.SS1.p1.4.m4.2.3.cmml"><mi id="S3.SS1.p1.4.m4.2.3.2" xref="S3.SS1.p1.4.m4.2.3.2.cmml">f</mi><mo id="S3.SS1.p1.4.m4.2.3.1" xref="S3.SS1.p1.4.m4.2.3.1.cmml">‚Å¢</mo><mrow id="S3.SS1.p1.4.m4.2.3.3.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml"><mo id="S3.SS1.p1.4.m4.2.3.3.2.1" stretchy="false" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">x</mi><mo id="S3.SS1.p1.4.m4.2.3.3.2.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml">Œ∏</mi><mo id="S3.SS1.p1.4.m4.2.3.3.2.3" stretchy="false" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.2b"><apply id="S3.SS1.p1.4.m4.2.3.cmml" xref="S3.SS1.p1.4.m4.2.3"><times id="S3.SS1.p1.4.m4.2.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.1"></times><ci id="S3.SS1.p1.4.m4.2.3.2.cmml" xref="S3.SS1.p1.4.m4.2.3.2">ùëì</ci><interval closure="open" id="S3.SS1.p1.4.m4.2.3.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.3.2"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ùë•</ci><ci id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2">ùúÉ</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.2c">f(x,\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.2d">italic_f ( italic_x , italic_Œ∏ )</annotation></semantics></math> describes the transformation of
one
multi-head self-attention <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.5.1">and</em> MLP
layer block. As for any residual network, if we unroll this iteration, we see that after <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ùêø</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_L</annotation></semantics></math> total layers the output is described as a sum over the transformations of all the layers</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(L)}=x^{(0)}+\sum_{\ell=0}^{L-1}f(x^{(\ell)},\theta^{(\ell)})\,." class="ltx_Math" display="block" id="S3.E2.m1.5"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1" xref="S3.E2.m1.5.5.1.1.cmml"><msup id="S3.E2.m1.5.5.1.1.4" xref="S3.E2.m1.5.5.1.1.4.cmml"><mi id="S3.E2.m1.5.5.1.1.4.2" xref="S3.E2.m1.5.5.1.1.4.2.cmml">x</mi><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.4.cmml"><mo id="S3.E2.m1.1.1.1.3.1" stretchy="false" xref="S3.E2.m1.5.5.1.1.4.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">L</mi><mo id="S3.E2.m1.1.1.1.3.2" stretchy="false" xref="S3.E2.m1.5.5.1.1.4.cmml">)</mo></mrow></msup><mo id="S3.E2.m1.5.5.1.1.3" xref="S3.E2.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.2" xref="S3.E2.m1.5.5.1.1.2.cmml"><msup id="S3.E2.m1.5.5.1.1.2.4" xref="S3.E2.m1.5.5.1.1.2.4.cmml"><mi id="S3.E2.m1.5.5.1.1.2.4.2" xref="S3.E2.m1.5.5.1.1.2.4.2.cmml">x</mi><mrow id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.5.5.1.1.2.4.cmml"><mo id="S3.E2.m1.2.2.1.3.1" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.4.cmml">(</mo><mn id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml">0</mn><mo id="S3.E2.m1.2.2.1.3.2" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.4.cmml">)</mo></mrow></msup><mo id="S3.E2.m1.5.5.1.1.2.3" rspace="0.055em" xref="S3.E2.m1.5.5.1.1.2.3.cmml">+</mo><mrow id="S3.E2.m1.5.5.1.1.2.2" xref="S3.E2.m1.5.5.1.1.2.2.cmml"><munderover id="S3.E2.m1.5.5.1.1.2.2.3" xref="S3.E2.m1.5.5.1.1.2.2.3.cmml"><mo id="S3.E2.m1.5.5.1.1.2.2.3.2.2" movablelimits="false" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2.cmml">‚àë</mo><mrow id="S3.E2.m1.5.5.1.1.2.2.3.2.3" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.3.2.3.2" mathvariant="normal" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.2.cmml">‚Ñì</mi><mo id="S3.E2.m1.5.5.1.1.2.2.3.2.3.1" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.1.cmml">=</mo><mn id="S3.E2.m1.5.5.1.1.2.2.3.2.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.3.cmml">0</mn></mrow><mrow id="S3.E2.m1.5.5.1.1.2.2.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.3.3.2" xref="S3.E2.m1.5.5.1.1.2.2.3.3.2.cmml">L</mi><mo id="S3.E2.m1.5.5.1.1.2.2.3.3.1" xref="S3.E2.m1.5.5.1.1.2.2.3.3.1.cmml">‚àí</mo><mn id="S3.E2.m1.5.5.1.1.2.2.3.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.3.3.cmml">1</mn></mrow></munderover><mrow id="S3.E2.m1.5.5.1.1.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.2.4" xref="S3.E2.m1.5.5.1.1.2.2.2.4.cmml">f</mi><mo id="S3.E2.m1.5.5.1.1.2.2.2.3" xref="S3.E2.m1.5.5.1.1.2.2.2.3.cmml">‚Å¢</mo><mrow id="S3.E2.m1.5.5.1.1.2.2.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml"><mo id="S3.E2.m1.5.5.1.1.2.2.2.2.2.3" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml">(</mo><msup id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E2.m1.3.3.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.1.3.1" stretchy="false" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.3.3.1.1" mathvariant="normal" xref="S3.E2.m1.3.3.1.1.cmml">‚Ñì</mi><mo id="S3.E2.m1.3.3.1.3.2" stretchy="false" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E2.m1.5.5.1.1.2.2.2.2.2.4" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml">,</mo><msup id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.2.cmml">Œ∏</mi><mrow id="S3.E2.m1.4.4.1.3" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml"><mo id="S3.E2.m1.4.4.1.3.1" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml">(</mo><mi id="S3.E2.m1.4.4.1.1" mathvariant="normal" xref="S3.E2.m1.4.4.1.1.cmml">‚Ñì</mi><mo id="S3.E2.m1.4.4.1.3.2" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml">)</mo></mrow></msup><mo id="S3.E2.m1.5.5.1.1.2.2.2.2.2.5" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.5.5.1.2" lspace="0.170em" xref="S3.E2.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.5.5.1"><eq id="S3.E2.m1.5.5.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.3"></eq><apply id="S3.E2.m1.5.5.1.1.4.cmml" xref="S3.E2.m1.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.4.1.cmml" xref="S3.E2.m1.5.5.1.1.4">superscript</csymbol><ci id="S3.E2.m1.5.5.1.1.4.2.cmml" xref="S3.E2.m1.5.5.1.1.4.2">ùë•</ci><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">ùêø</ci></apply><apply id="S3.E2.m1.5.5.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2"><plus id="S3.E2.m1.5.5.1.1.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.3"></plus><apply id="S3.E2.m1.5.5.1.1.2.4.cmml" xref="S3.E2.m1.5.5.1.1.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.4.1.cmml" xref="S3.E2.m1.5.5.1.1.2.4">superscript</csymbol><ci id="S3.E2.m1.5.5.1.1.2.4.2.cmml" xref="S3.E2.m1.5.5.1.1.2.4.2">ùë•</ci><cn id="S3.E2.m1.2.2.1.1.cmml" type="integer" xref="S3.E2.m1.2.2.1.1">0</cn></apply><apply id="S3.E2.m1.5.5.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2"><apply id="S3.E2.m1.5.5.1.1.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3">superscript</csymbol><apply id="S3.E2.m1.5.5.1.1.2.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.3.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3">subscript</csymbol><sum id="S3.E2.m1.5.5.1.1.2.2.3.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2"></sum><apply id="S3.E2.m1.5.5.1.1.2.2.3.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3"><eq id="S3.E2.m1.5.5.1.1.2.2.3.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.1"></eq><ci id="S3.E2.m1.5.5.1.1.2.2.3.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.2">‚Ñì</ci><cn id="S3.E2.m1.5.5.1.1.2.2.3.2.3.3.cmml" type="integer" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.3">0</cn></apply></apply><apply id="S3.E2.m1.5.5.1.1.2.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.3"><minus id="S3.E2.m1.5.5.1.1.2.2.3.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.3.1"></minus><ci id="S3.E2.m1.5.5.1.1.2.2.3.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.3.2">ùêø</ci><cn id="S3.E2.m1.5.5.1.1.2.2.3.3.3.cmml" type="integer" xref="S3.E2.m1.5.5.1.1.2.2.3.3.3">1</cn></apply></apply><apply id="S3.E2.m1.5.5.1.1.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2"><times id="S3.E2.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.3"></times><ci id="S3.E2.m1.5.5.1.1.2.2.2.4.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.4">ùëì</ci><interval closure="open" id="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2"><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2">ùë•</ci><ci id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">‚Ñì</ci></apply><apply id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.2">ùúÉ</ci><ci id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1.1">‚Ñì</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">x^{(L)}=x^{(0)}+\sum_{\ell=0}^{L-1}f(x^{(\ell)},\theta^{(\ell)})\,.</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.5d">italic_x start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT = italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT + ‚àë start_POSTSUBSCRIPT roman_‚Ñì = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT italic_f ( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT , italic_Œ∏ start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.7">If the terms in the sum were
<em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.7.1">numerous</em>, (<math alttext="L\gg 1" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m1.1"><semantics id="S3.SS1.p1.6.m1.1a"><mrow id="S3.SS1.p1.6.m1.1.1" xref="S3.SS1.p1.6.m1.1.1.cmml"><mi id="S3.SS1.p1.6.m1.1.1.2" xref="S3.SS1.p1.6.m1.1.1.2.cmml">L</mi><mo id="S3.SS1.p1.6.m1.1.1.1" xref="S3.SS1.p1.6.m1.1.1.1.cmml">‚â´</mo><mn id="S3.SS1.p1.6.m1.1.1.3" xref="S3.SS1.p1.6.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m1.1b"><apply id="S3.SS1.p1.6.m1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.6.m1.1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1.1">much-greater-than</csymbol><ci id="S3.SS1.p1.6.m1.1.1.2.cmml" xref="S3.SS1.p1.6.m1.1.1.2">ùêø</ci><cn id="S3.SS1.p1.6.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.6.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m1.1c">L\gg 1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m1.1d">italic_L ‚â´ 1</annotation></semantics></math>),
and
<em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.7.2">independent</em>, e.g. if the block functions were instead a function of the overall input as <math alttext="f(x^{(0)},\theta^{(\ell)})" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m2.4"><semantics id="S3.SS1.p1.7.m2.4a"><mrow id="S3.SS1.p1.7.m2.4.4" xref="S3.SS1.p1.7.m2.4.4.cmml"><mi id="S3.SS1.p1.7.m2.4.4.4" xref="S3.SS1.p1.7.m2.4.4.4.cmml">f</mi><mo id="S3.SS1.p1.7.m2.4.4.3" xref="S3.SS1.p1.7.m2.4.4.3.cmml">‚Å¢</mo><mrow id="S3.SS1.p1.7.m2.4.4.2.2" xref="S3.SS1.p1.7.m2.4.4.2.3.cmml"><mo id="S3.SS1.p1.7.m2.4.4.2.2.3" stretchy="false" xref="S3.SS1.p1.7.m2.4.4.2.3.cmml">(</mo><msup id="S3.SS1.p1.7.m2.3.3.1.1.1" xref="S3.SS1.p1.7.m2.3.3.1.1.1.cmml"><mi id="S3.SS1.p1.7.m2.3.3.1.1.1.2" xref="S3.SS1.p1.7.m2.3.3.1.1.1.2.cmml">x</mi><mrow id="S3.SS1.p1.7.m2.1.1.1.3" xref="S3.SS1.p1.7.m2.3.3.1.1.1.cmml"><mo id="S3.SS1.p1.7.m2.1.1.1.3.1" stretchy="false" xref="S3.SS1.p1.7.m2.3.3.1.1.1.cmml">(</mo><mn id="S3.SS1.p1.7.m2.1.1.1.1" xref="S3.SS1.p1.7.m2.1.1.1.1.cmml">0</mn><mo id="S3.SS1.p1.7.m2.1.1.1.3.2" stretchy="false" xref="S3.SS1.p1.7.m2.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.SS1.p1.7.m2.4.4.2.2.4" xref="S3.SS1.p1.7.m2.4.4.2.3.cmml">,</mo><msup id="S3.SS1.p1.7.m2.4.4.2.2.2" xref="S3.SS1.p1.7.m2.4.4.2.2.2.cmml"><mi id="S3.SS1.p1.7.m2.4.4.2.2.2.2" xref="S3.SS1.p1.7.m2.4.4.2.2.2.2.cmml">Œ∏</mi><mrow id="S3.SS1.p1.7.m2.2.2.1.3" xref="S3.SS1.p1.7.m2.4.4.2.2.2.cmml"><mo id="S3.SS1.p1.7.m2.2.2.1.3.1" stretchy="false" xref="S3.SS1.p1.7.m2.4.4.2.2.2.cmml">(</mo><mi id="S3.SS1.p1.7.m2.2.2.1.1" mathvariant="normal" xref="S3.SS1.p1.7.m2.2.2.1.1.cmml">‚Ñì</mi><mo id="S3.SS1.p1.7.m2.2.2.1.3.2" stretchy="false" xref="S3.SS1.p1.7.m2.4.4.2.2.2.cmml">)</mo></mrow></msup><mo id="S3.SS1.p1.7.m2.4.4.2.2.5" stretchy="false" xref="S3.SS1.p1.7.m2.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m2.4b"><apply id="S3.SS1.p1.7.m2.4.4.cmml" xref="S3.SS1.p1.7.m2.4.4"><times id="S3.SS1.p1.7.m2.4.4.3.cmml" xref="S3.SS1.p1.7.m2.4.4.3"></times><ci id="S3.SS1.p1.7.m2.4.4.4.cmml" xref="S3.SS1.p1.7.m2.4.4.4">ùëì</ci><interval closure="open" id="S3.SS1.p1.7.m2.4.4.2.3.cmml" xref="S3.SS1.p1.7.m2.4.4.2.2"><apply id="S3.SS1.p1.7.m2.3.3.1.1.1.cmml" xref="S3.SS1.p1.7.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m2.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.7.m2.3.3.1.1.1">superscript</csymbol><ci id="S3.SS1.p1.7.m2.3.3.1.1.1.2.cmml" xref="S3.SS1.p1.7.m2.3.3.1.1.1.2">ùë•</ci><cn id="S3.SS1.p1.7.m2.1.1.1.1.cmml" type="integer" xref="S3.SS1.p1.7.m2.1.1.1.1">0</cn></apply><apply id="S3.SS1.p1.7.m2.4.4.2.2.2.cmml" xref="S3.SS1.p1.7.m2.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m2.4.4.2.2.2.1.cmml" xref="S3.SS1.p1.7.m2.4.4.2.2.2">superscript</csymbol><ci id="S3.SS1.p1.7.m2.4.4.2.2.2.2.cmml" xref="S3.SS1.p1.7.m2.4.4.2.2.2.2">ùúÉ</ci><ci id="S3.SS1.p1.7.m2.2.2.1.1.cmml" xref="S3.SS1.p1.7.m2.2.2.1.1">‚Ñì</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m2.4c">f(x^{(0)},\theta^{(\ell)})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m2.4d">italic_f ( italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , italic_Œ∏ start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT )</annotation></semantics></math>,
then
perhaps any particular contribution to the sum (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E2" title="2 ‚Ä£ 3.1 Intuition ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>) could be neglected.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.3">Of course, they are not at all independent:
if we delete layer <math alttext="\ell-1" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" mathvariant="normal" xref="S3.SS1.p2.1.m1.1.1.2.cmml">‚Ñì</mi><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">‚àí</mo><mn id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><minus id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></minus><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">‚Ñì</ci><cn id="S3.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\ell-1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">roman_‚Ñì - 1</annotation></semantics></math>, then we must now
connect
the old input to that layer,
<math alttext="x^{(\ell-1)}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msup id="S3.SS1.p2.2.m2.1.2" xref="S3.SS1.p2.2.m2.1.2.cmml"><mi id="S3.SS1.p2.2.m2.1.2.2" xref="S3.SS1.p2.2.m2.1.2.2.cmml">x</mi><mrow id="S3.SS1.p2.2.m2.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.SS1.p2.2.m2.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml">‚àí</mo><mn id="S3.SS1.p2.2.m2.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.2.cmml" xref="S3.SS1.p2.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2">ùë•</ci><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1"><minus id="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1"></minus><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2">‚Ñì</ci><cn id="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">x^{(\ell-1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì - 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>,
into the block function of layer <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p2.3.m3.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">roman_‚Ñì</annotation></semantics></math> as</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(\ell+1)}=x^{(\ell-1)}+f(x^{(\ell-1)},\theta^{(\ell)})\,," class="ltx_Math" display="block" id="S3.E3.m1.5"><semantics id="S3.E3.m1.5a"><mrow id="S3.E3.m1.5.5.1" xref="S3.E3.m1.5.5.1.1.cmml"><mrow id="S3.E3.m1.5.5.1.1" xref="S3.E3.m1.5.5.1.1.cmml"><msup id="S3.E3.m1.5.5.1.1.4" xref="S3.E3.m1.5.5.1.1.4.cmml"><mi id="S3.E3.m1.5.5.1.1.4.2" xref="S3.E3.m1.5.5.1.1.4.2.cmml">x</mi><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2" mathvariant="normal" xref="S3.E3.m1.1.1.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E3.m1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E3.m1.5.5.1.1.3" xref="S3.E3.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.5.5.1.1.2" xref="S3.E3.m1.5.5.1.1.2.cmml"><msup id="S3.E3.m1.5.5.1.1.2.4" xref="S3.E3.m1.5.5.1.1.2.4.cmml"><mi id="S3.E3.m1.5.5.1.1.2.4.2" xref="S3.E3.m1.5.5.1.1.2.4.2.cmml">x</mi><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mo id="S3.E3.m1.2.2.1.1.2" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E3.m1.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E3.m1.2.2.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml">‚àí</mo><mn id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E3.m1.2.2.1.1.3" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E3.m1.5.5.1.1.2.3" xref="S3.E3.m1.5.5.1.1.2.3.cmml">+</mo><mrow id="S3.E3.m1.5.5.1.1.2.2" xref="S3.E3.m1.5.5.1.1.2.2.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.4" xref="S3.E3.m1.5.5.1.1.2.2.4.cmml">f</mi><mo id="S3.E3.m1.5.5.1.1.2.2.3" xref="S3.E3.m1.5.5.1.1.2.2.3.cmml">‚Å¢</mo><mrow id="S3.E3.m1.5.5.1.1.2.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml"><mo id="S3.E3.m1.5.5.1.1.2.2.2.2.3" stretchy="false" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml">(</mo><msup id="S3.E3.m1.5.5.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mo id="S3.E3.m1.3.3.1.1.2" stretchy="false" xref="S3.E3.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.2" mathvariant="normal" xref="S3.E3.m1.3.3.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E3.m1.3.3.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.cmml">‚àí</mo><mn id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E3.m1.3.3.1.1.3" stretchy="false" xref="S3.E3.m1.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E3.m1.5.5.1.1.2.2.2.2.4" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml">,</mo><msup id="S3.E3.m1.5.5.1.1.2.2.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.2.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.2.cmml">Œ∏</mi><mrow id="S3.E3.m1.4.4.1.3" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml"><mo id="S3.E3.m1.4.4.1.3.1" stretchy="false" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml">(</mo><mi id="S3.E3.m1.4.4.1.1" mathvariant="normal" xref="S3.E3.m1.4.4.1.1.cmml">‚Ñì</mi><mo id="S3.E3.m1.4.4.1.3.2" stretchy="false" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml">)</mo></mrow></msup><mo id="S3.E3.m1.5.5.1.1.2.2.2.2.5" rspace="0.170em" stretchy="false" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.5.5.1.2" xref="S3.E3.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.5b"><apply id="S3.E3.m1.5.5.1.1.cmml" xref="S3.E3.m1.5.5.1"><eq id="S3.E3.m1.5.5.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.3"></eq><apply id="S3.E3.m1.5.5.1.1.4.cmml" xref="S3.E3.m1.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.4.1.cmml" xref="S3.E3.m1.5.5.1.1.4">superscript</csymbol><ci id="S3.E3.m1.5.5.1.1.4.2.cmml" xref="S3.E3.m1.5.5.1.1.4.2">ùë•</ci><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"></plus><ci id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2">‚Ñì</ci><cn id="S3.E3.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S3.E3.m1.5.5.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.2"><plus id="S3.E3.m1.5.5.1.1.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.3"></plus><apply id="S3.E3.m1.5.5.1.1.2.4.cmml" xref="S3.E3.m1.5.5.1.1.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.4.1.cmml" xref="S3.E3.m1.5.5.1.1.2.4">superscript</csymbol><ci id="S3.E3.m1.5.5.1.1.2.4.2.cmml" xref="S3.E3.m1.5.5.1.1.2.4.2">ùë•</ci><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1"><minus id="S3.E3.m1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1"></minus><ci id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2">‚Ñì</ci><cn id="S3.E3.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.3">1</cn></apply></apply><apply id="S3.E3.m1.5.5.1.1.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2"><times id="S3.E3.m1.5.5.1.1.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.3"></times><ci id="S3.E3.m1.5.5.1.1.2.2.4.cmml" xref="S3.E3.m1.5.5.1.1.2.2.4">ùëì</ci><interval closure="open" id="S3.E3.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2"><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2">ùë•</ci><apply id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1"><minus id="S3.E3.m1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1"></minus><ci id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2">‚Ñì</ci><cn id="S3.E3.m1.3.3.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.3">1</cn></apply></apply><apply id="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2">superscript</csymbol><ci id="S3.E3.m1.5.5.1.1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.2">ùúÉ</ci><ci id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1.1">‚Ñì</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.5c">x^{(\ell+1)}=x^{(\ell-1)}+f(x^{(\ell-1)},\theta^{(\ell)})\,,</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.5d">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + 1 ) end_POSTSUPERSCRIPT = italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì - 1 ) end_POSTSUPERSCRIPT + italic_f ( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì - 1 ) end_POSTSUPERSCRIPT , italic_Œ∏ start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.11">where, for clarity, we are not relabeling layers or inputs despite the deletion.
In general, such a <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.11.1">mismatch</em> between the original input and new input should be very damaging for the network.
However, if, after some number of initial layers, the representations converge to a slowly changing function with respect to layer index,</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(\ell)}\approx x^{(\ell-1)}+\epsilon\,," class="ltx_Math" display="block" id="S3.E4.m1.3"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml"><msup id="S3.E4.m1.3.3.1.1.2" xref="S3.E4.m1.3.3.1.1.2.cmml"><mi id="S3.E4.m1.3.3.1.1.2.2" xref="S3.E4.m1.3.3.1.1.2.2.cmml">x</mi><mrow id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.3.1" stretchy="false" xref="S3.E4.m1.3.3.1.1.2.cmml">(</mo><mi id="S3.E4.m1.1.1.1.1" mathvariant="normal" xref="S3.E4.m1.1.1.1.1.cmml">‚Ñì</mi><mo id="S3.E4.m1.1.1.1.3.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.2.cmml">)</mo></mrow></msup><mo id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.cmml">‚âà</mo><mrow id="S3.E4.m1.3.3.1.1.3" xref="S3.E4.m1.3.3.1.1.3.cmml"><msup id="S3.E4.m1.3.3.1.1.3.2" xref="S3.E4.m1.3.3.1.1.3.2.cmml"><mi id="S3.E4.m1.3.3.1.1.3.2.2" xref="S3.E4.m1.3.3.1.1.3.2.2.cmml">x</mi><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.1.cmml"><mo id="S3.E4.m1.2.2.1.1.2" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E4.m1.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml">‚àí</mo><mn id="S3.E4.m1.2.2.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E4.m1.2.2.1.1.3" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E4.m1.3.3.1.1.3.1" xref="S3.E4.m1.3.3.1.1.3.1.cmml">+</mo><mi id="S3.E4.m1.3.3.1.1.3.3" xref="S3.E4.m1.3.3.1.1.3.3.cmml">œµ</mi></mrow></mrow><mo id="S3.E4.m1.3.3.1.2" lspace="0.170em" xref="S3.E4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1"><approx id="S3.E4.m1.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"></approx><apply id="S3.E4.m1.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.2">superscript</csymbol><ci id="S3.E4.m1.3.3.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.2.2">ùë•</ci><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">‚Ñì</ci></apply><apply id="S3.E4.m1.3.3.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.3"><plus id="S3.E4.m1.3.3.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.3.1"></plus><apply id="S3.E4.m1.3.3.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.3.2.1.cmml" xref="S3.E4.m1.3.3.1.1.3.2">superscript</csymbol><ci id="S3.E4.m1.3.3.1.1.3.2.2.cmml" xref="S3.E4.m1.3.3.1.1.3.2.2">ùë•</ci><apply id="S3.E4.m1.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1"><minus id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1"></minus><ci id="S3.E4.m1.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.2">‚Ñì</ci><cn id="S3.E4.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.E4.m1.2.2.1.1.1.3">1</cn></apply></apply><ci id="S3.E4.m1.3.3.1.1.3.3.cmml" xref="S3.E4.m1.3.3.1.1.3.3">italic-œµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">x^{(\ell)}\approx x^{(\ell-1)}+\epsilon\,,</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.3d">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT ‚âà italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì - 1 ) end_POSTSUPERSCRIPT + italic_œµ ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.10">with <math alttext="\epsilon\ll x^{(\ell)}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m1.1"><semantics id="S3.SS1.p2.4.m1.1a"><mrow id="S3.SS1.p2.4.m1.1.2" xref="S3.SS1.p2.4.m1.1.2.cmml"><mi id="S3.SS1.p2.4.m1.1.2.2" xref="S3.SS1.p2.4.m1.1.2.2.cmml">œµ</mi><mo id="S3.SS1.p2.4.m1.1.2.1" xref="S3.SS1.p2.4.m1.1.2.1.cmml">‚â™</mo><msup id="S3.SS1.p2.4.m1.1.2.3" xref="S3.SS1.p2.4.m1.1.2.3.cmml"><mi id="S3.SS1.p2.4.m1.1.2.3.2" xref="S3.SS1.p2.4.m1.1.2.3.2.cmml">x</mi><mrow id="S3.SS1.p2.4.m1.1.1.1.3" xref="S3.SS1.p2.4.m1.1.2.3.cmml"><mo id="S3.SS1.p2.4.m1.1.1.1.3.1" stretchy="false" xref="S3.SS1.p2.4.m1.1.2.3.cmml">(</mo><mi id="S3.SS1.p2.4.m1.1.1.1.1" mathvariant="normal" xref="S3.SS1.p2.4.m1.1.1.1.1.cmml">‚Ñì</mi><mo id="S3.SS1.p2.4.m1.1.1.1.3.2" stretchy="false" xref="S3.SS1.p2.4.m1.1.2.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m1.1b"><apply id="S3.SS1.p2.4.m1.1.2.cmml" xref="S3.SS1.p2.4.m1.1.2"><csymbol cd="latexml" id="S3.SS1.p2.4.m1.1.2.1.cmml" xref="S3.SS1.p2.4.m1.1.2.1">much-less-than</csymbol><ci id="S3.SS1.p2.4.m1.1.2.2.cmml" xref="S3.SS1.p2.4.m1.1.2.2">italic-œµ</ci><apply id="S3.SS1.p2.4.m1.1.2.3.cmml" xref="S3.SS1.p2.4.m1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m1.1.2.3.1.cmml" xref="S3.SS1.p2.4.m1.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.4.m1.1.2.3.2.cmml" xref="S3.SS1.p2.4.m1.1.2.3.2">ùë•</ci><ci id="S3.SS1.p2.4.m1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m1.1.1.1.1">‚Ñì</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m1.1c">\epsilon\ll x^{(\ell)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m1.1d">italic_œµ ‚â™ italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT</annotation></semantics></math> in some appropriate sense, then the effect of deleting a particular layer <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m2.1"><semantics id="S3.SS1.p2.5.m2.1a"><mi id="S3.SS1.p2.5.m2.1.1" mathvariant="normal" xref="S3.SS1.p2.5.m2.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m2.1b"><ci id="S3.SS1.p2.5.m2.1.1.cmml" xref="S3.SS1.p2.5.m2.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m2.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m2.1d">roman_‚Ñì</annotation></semantics></math>, e.g. making the replacement <math alttext="x^{(\ell)}\to x^{(\ell-1)}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m3.2"><semantics id="S3.SS1.p2.6.m3.2a"><mrow id="S3.SS1.p2.6.m3.2.3" xref="S3.SS1.p2.6.m3.2.3.cmml"><msup id="S3.SS1.p2.6.m3.2.3.2" xref="S3.SS1.p2.6.m3.2.3.2.cmml"><mi id="S3.SS1.p2.6.m3.2.3.2.2" xref="S3.SS1.p2.6.m3.2.3.2.2.cmml">x</mi><mrow id="S3.SS1.p2.6.m3.1.1.1.3" xref="S3.SS1.p2.6.m3.2.3.2.cmml"><mo id="S3.SS1.p2.6.m3.1.1.1.3.1" stretchy="false" xref="S3.SS1.p2.6.m3.2.3.2.cmml">(</mo><mi id="S3.SS1.p2.6.m3.1.1.1.1" mathvariant="normal" xref="S3.SS1.p2.6.m3.1.1.1.1.cmml">‚Ñì</mi><mo id="S3.SS1.p2.6.m3.1.1.1.3.2" stretchy="false" xref="S3.SS1.p2.6.m3.2.3.2.cmml">)</mo></mrow></msup><mo id="S3.SS1.p2.6.m3.2.3.1" stretchy="false" xref="S3.SS1.p2.6.m3.2.3.1.cmml">‚Üí</mo><msup id="S3.SS1.p2.6.m3.2.3.3" xref="S3.SS1.p2.6.m3.2.3.3.cmml"><mi id="S3.SS1.p2.6.m3.2.3.3.2" xref="S3.SS1.p2.6.m3.2.3.3.2.cmml">x</mi><mrow id="S3.SS1.p2.6.m3.2.2.1.1" xref="S3.SS1.p2.6.m3.2.2.1.1.1.cmml"><mo id="S3.SS1.p2.6.m3.2.2.1.1.2" stretchy="false" xref="S3.SS1.p2.6.m3.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.6.m3.2.2.1.1.1" xref="S3.SS1.p2.6.m3.2.2.1.1.1.cmml"><mi id="S3.SS1.p2.6.m3.2.2.1.1.1.2" mathvariant="normal" xref="S3.SS1.p2.6.m3.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.SS1.p2.6.m3.2.2.1.1.1.1" xref="S3.SS1.p2.6.m3.2.2.1.1.1.1.cmml">‚àí</mo><mn id="S3.SS1.p2.6.m3.2.2.1.1.1.3" xref="S3.SS1.p2.6.m3.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.6.m3.2.2.1.1.3" stretchy="false" xref="S3.SS1.p2.6.m3.2.2.1.1.1.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m3.2b"><apply id="S3.SS1.p2.6.m3.2.3.cmml" xref="S3.SS1.p2.6.m3.2.3"><ci id="S3.SS1.p2.6.m3.2.3.1.cmml" xref="S3.SS1.p2.6.m3.2.3.1">‚Üí</ci><apply id="S3.SS1.p2.6.m3.2.3.2.cmml" xref="S3.SS1.p2.6.m3.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m3.2.3.2.1.cmml" xref="S3.SS1.p2.6.m3.2.3.2">superscript</csymbol><ci id="S3.SS1.p2.6.m3.2.3.2.2.cmml" xref="S3.SS1.p2.6.m3.2.3.2.2">ùë•</ci><ci id="S3.SS1.p2.6.m3.1.1.1.1.cmml" xref="S3.SS1.p2.6.m3.1.1.1.1">‚Ñì</ci></apply><apply id="S3.SS1.p2.6.m3.2.3.3.cmml" xref="S3.SS1.p2.6.m3.2.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m3.2.3.3.1.cmml" xref="S3.SS1.p2.6.m3.2.3.3">superscript</csymbol><ci id="S3.SS1.p2.6.m3.2.3.3.2.cmml" xref="S3.SS1.p2.6.m3.2.3.3.2">ùë•</ci><apply id="S3.SS1.p2.6.m3.2.2.1.1.1.cmml" xref="S3.SS1.p2.6.m3.2.2.1.1"><minus id="S3.SS1.p2.6.m3.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.6.m3.2.2.1.1.1.1"></minus><ci id="S3.SS1.p2.6.m3.2.2.1.1.1.2.cmml" xref="S3.SS1.p2.6.m3.2.2.1.1.1.2">‚Ñì</ci><cn id="S3.SS1.p2.6.m3.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.6.m3.2.2.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m3.2c">x^{(\ell)}\to x^{(\ell-1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m3.2d">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT ‚Üí italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì - 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math> in going from (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E1" title="1 ‚Ä£ 3.1 Intuition ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>) to (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E3" title="3 ‚Ä£ 3.1 Intuition ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a>),
should
only change the representation in the subsequent layer, <math alttext="x^{(\ell+1)}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m4.1"><semantics id="S3.SS1.p2.7.m4.1a"><msup id="S3.SS1.p2.7.m4.1.2" xref="S3.SS1.p2.7.m4.1.2.cmml"><mi id="S3.SS1.p2.7.m4.1.2.2" xref="S3.SS1.p2.7.m4.1.2.2.cmml">x</mi><mrow id="S3.SS1.p2.7.m4.1.1.1.1" xref="S3.SS1.p2.7.m4.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.7.m4.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.7.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.7.m4.1.1.1.1.1" xref="S3.SS1.p2.7.m4.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.7.m4.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p2.7.m4.1.1.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.SS1.p2.7.m4.1.1.1.1.1.1" xref="S3.SS1.p2.7.m4.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS1.p2.7.m4.1.1.1.1.1.3" xref="S3.SS1.p2.7.m4.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.7.m4.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.7.m4.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m4.1b"><apply id="S3.SS1.p2.7.m4.1.2.cmml" xref="S3.SS1.p2.7.m4.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m4.1.2.1.cmml" xref="S3.SS1.p2.7.m4.1.2">superscript</csymbol><ci id="S3.SS1.p2.7.m4.1.2.2.cmml" xref="S3.SS1.p2.7.m4.1.2.2">ùë•</ci><apply id="S3.SS1.p2.7.m4.1.1.1.1.1.cmml" xref="S3.SS1.p2.7.m4.1.1.1.1"><plus id="S3.SS1.p2.7.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.7.m4.1.1.1.1.1.1"></plus><ci id="S3.SS1.p2.7.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.7.m4.1.1.1.1.1.2">‚Ñì</ci><cn id="S3.SS1.p2.7.m4.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.7.m4.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m4.1c">x^{(\ell+1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m4.1d">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>, by a small amount.
Similarly,
to
successfully prune the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m5.1"><semantics id="S3.SS1.p2.8.m5.1a"><mi id="S3.SS1.p2.8.m5.1.1" xref="S3.SS1.p2.8.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m5.1b"><ci id="S3.SS1.p2.8.m5.1.1.cmml" xref="S3.SS1.p2.8.m5.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m5.1d">italic_n</annotation></semantics></math> layers before layer <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m6.1"><semantics id="S3.SS1.p2.9.m6.1a"><mi id="S3.SS1.p2.9.m6.1.1" mathvariant="normal" xref="S3.SS1.p2.9.m6.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m6.1b"><ci id="S3.SS1.p2.9.m6.1.1.cmml" xref="S3.SS1.p2.9.m6.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m6.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m6.1d">roman_‚Ñì</annotation></semantics></math>, i.e. those indexed
from <math alttext="\ell-n,\ldots,\ell-1" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m7.3"><semantics id="S3.SS1.p2.10.m7.3a"><mrow id="S3.SS1.p2.10.m7.3.3.2" xref="S3.SS1.p2.10.m7.3.3.3.cmml"><mrow id="S3.SS1.p2.10.m7.2.2.1.1" xref="S3.SS1.p2.10.m7.2.2.1.1.cmml"><mi id="S3.SS1.p2.10.m7.2.2.1.1.2" mathvariant="normal" xref="S3.SS1.p2.10.m7.2.2.1.1.2.cmml">‚Ñì</mi><mo id="S3.SS1.p2.10.m7.2.2.1.1.1" xref="S3.SS1.p2.10.m7.2.2.1.1.1.cmml">‚àí</mo><mi id="S3.SS1.p2.10.m7.2.2.1.1.3" xref="S3.SS1.p2.10.m7.2.2.1.1.3.cmml">n</mi></mrow><mo id="S3.SS1.p2.10.m7.3.3.2.3" xref="S3.SS1.p2.10.m7.3.3.3.cmml">,</mo><mi id="S3.SS1.p2.10.m7.1.1" mathvariant="normal" xref="S3.SS1.p2.10.m7.1.1.cmml">‚Ä¶</mi><mo id="S3.SS1.p2.10.m7.3.3.2.4" xref="S3.SS1.p2.10.m7.3.3.3.cmml">,</mo><mrow id="S3.SS1.p2.10.m7.3.3.2.2" xref="S3.SS1.p2.10.m7.3.3.2.2.cmml"><mi id="S3.SS1.p2.10.m7.3.3.2.2.2" mathvariant="normal" xref="S3.SS1.p2.10.m7.3.3.2.2.2.cmml">‚Ñì</mi><mo id="S3.SS1.p2.10.m7.3.3.2.2.1" xref="S3.SS1.p2.10.m7.3.3.2.2.1.cmml">‚àí</mo><mn id="S3.SS1.p2.10.m7.3.3.2.2.3" xref="S3.SS1.p2.10.m7.3.3.2.2.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m7.3b"><list id="S3.SS1.p2.10.m7.3.3.3.cmml" xref="S3.SS1.p2.10.m7.3.3.2"><apply id="S3.SS1.p2.10.m7.2.2.1.1.cmml" xref="S3.SS1.p2.10.m7.2.2.1.1"><minus id="S3.SS1.p2.10.m7.2.2.1.1.1.cmml" xref="S3.SS1.p2.10.m7.2.2.1.1.1"></minus><ci id="S3.SS1.p2.10.m7.2.2.1.1.2.cmml" xref="S3.SS1.p2.10.m7.2.2.1.1.2">‚Ñì</ci><ci id="S3.SS1.p2.10.m7.2.2.1.1.3.cmml" xref="S3.SS1.p2.10.m7.2.2.1.1.3">ùëõ</ci></apply><ci id="S3.SS1.p2.10.m7.1.1.cmml" xref="S3.SS1.p2.10.m7.1.1">‚Ä¶</ci><apply id="S3.SS1.p2.10.m7.3.3.2.2.cmml" xref="S3.SS1.p2.10.m7.3.3.2.2"><minus id="S3.SS1.p2.10.m7.3.3.2.2.1.cmml" xref="S3.SS1.p2.10.m7.3.3.2.2.1"></minus><ci id="S3.SS1.p2.10.m7.3.3.2.2.2.cmml" xref="S3.SS1.p2.10.m7.3.3.2.2.2">‚Ñì</ci><cn id="S3.SS1.p2.10.m7.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.p2.10.m7.3.3.2.2.3">1</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m7.3c">\ell-n,\ldots,\ell-1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m7.3d">roman_‚Ñì - italic_n , ‚Ä¶ , roman_‚Ñì - 1</annotation></semantics></math>,
we‚Äôd want
that the input
to the pruned block
should be very similar to the output of the pruned block:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(\ell)}\approx x^{(\ell-n)}+\epsilon\,." class="ltx_Math" display="block" id="S3.E5.m1.3"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml"><msup id="S3.E5.m1.3.3.1.1.2" xref="S3.E5.m1.3.3.1.1.2.cmml"><mi id="S3.E5.m1.3.3.1.1.2.2" xref="S3.E5.m1.3.3.1.1.2.2.cmml">x</mi><mrow id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.2.cmml"><mo id="S3.E5.m1.1.1.1.3.1" stretchy="false" xref="S3.E5.m1.3.3.1.1.2.cmml">(</mo><mi id="S3.E5.m1.1.1.1.1" mathvariant="normal" xref="S3.E5.m1.1.1.1.1.cmml">‚Ñì</mi><mo id="S3.E5.m1.1.1.1.3.2" stretchy="false" xref="S3.E5.m1.3.3.1.1.2.cmml">)</mo></mrow></msup><mo id="S3.E5.m1.3.3.1.1.1" xref="S3.E5.m1.3.3.1.1.1.cmml">‚âà</mo><mrow id="S3.E5.m1.3.3.1.1.3" xref="S3.E5.m1.3.3.1.1.3.cmml"><msup id="S3.E5.m1.3.3.1.1.3.2" xref="S3.E5.m1.3.3.1.1.3.2.cmml"><mi id="S3.E5.m1.3.3.1.1.3.2.2" xref="S3.E5.m1.3.3.1.1.3.2.2.cmml">x</mi><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><mo id="S3.E5.m1.2.2.1.1.2" stretchy="false" xref="S3.E5.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E5.m1.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E5.m1.2.2.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.cmml">‚àí</mo><mi id="S3.E5.m1.2.2.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E5.m1.2.2.1.1.3" stretchy="false" xref="S3.E5.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E5.m1.3.3.1.1.3.1" xref="S3.E5.m1.3.3.1.1.3.1.cmml">+</mo><mi id="S3.E5.m1.3.3.1.1.3.3" xref="S3.E5.m1.3.3.1.1.3.3.cmml">œµ</mi></mrow></mrow><mo id="S3.E5.m1.3.3.1.2" lspace="0.170em" xref="S3.E5.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1"><approx id="S3.E5.m1.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1"></approx><apply id="S3.E5.m1.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.2">superscript</csymbol><ci id="S3.E5.m1.3.3.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2">ùë•</ci><ci id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1">‚Ñì</ci></apply><apply id="S3.E5.m1.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3"><plus id="S3.E5.m1.3.3.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3.1"></plus><apply id="S3.E5.m1.3.3.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.3.2.1.cmml" xref="S3.E5.m1.3.3.1.1.3.2">superscript</csymbol><ci id="S3.E5.m1.3.3.1.1.3.2.2.cmml" xref="S3.E5.m1.3.3.1.1.3.2.2">ùë•</ci><apply id="S3.E5.m1.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1"><minus id="S3.E5.m1.2.2.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1"></minus><ci id="S3.E5.m1.2.2.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2">‚Ñì</ci><ci id="S3.E5.m1.2.2.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3">ùëõ</ci></apply></apply><ci id="S3.E5.m1.3.3.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.3.3">italic-œµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">x^{(\ell)}\approx x^{(\ell-n)}+\epsilon\,.</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.3d">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT ‚âà italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì - italic_n ) end_POSTSUPERSCRIPT + italic_œµ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.3">Regardless, any layer removal
has a cascading effect:
since post pruning <math alttext="x^{(\ell+1)}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msup id="S3.SS1.p3.1.m1.1.2" xref="S3.SS1.p3.1.m1.1.2.cmml"><mi id="S3.SS1.p3.1.m1.1.2.2" xref="S3.SS1.p3.1.m1.1.2.2.cmml">x</mi><mrow id="S3.SS1.p3.1.m1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.1.m1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.SS1.p3.1.m1.1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.1.m1.1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p3.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.2">superscript</csymbol><ci id="S3.SS1.p3.1.m1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.2.2">ùë•</ci><apply id="S3.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1"><plus id="S3.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1"></plus><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2">‚Ñì</ci><cn id="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">x^{(\ell+1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math> is computed by a different function than before, cf. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E1" title="1 ‚Ä£ 3.1 Intuition ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>) vs. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E3" title="3 ‚Ä£ 3.1 Intuition ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a>), and since then <math alttext="x^{(\ell+1)}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><msup id="S3.SS1.p3.2.m2.1.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><mi id="S3.SS1.p3.2.m2.1.2.2" xref="S3.SS1.p3.2.m2.1.2.2.cmml">x</mi><mrow id="S3.SS1.p3.2.m2.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.SS1.p3.2.m2.1.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.2.m2.1.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p3.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.cmml" xref="S3.SS1.p3.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2">superscript</csymbol><ci id="S3.SS1.p3.2.m2.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2">ùë•</ci><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1"><plus id="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1"></plus><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2">‚Ñì</ci><cn id="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">x^{(\ell+1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math> is directly or indirectly input to subsequent layers,
<math alttext="\ell+2,\ldots,L" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.3"><semantics id="S3.SS1.p3.3.m3.3a"><mrow id="S3.SS1.p3.3.m3.3.3.1" xref="S3.SS1.p3.3.m3.3.3.2.cmml"><mrow id="S3.SS1.p3.3.m3.3.3.1.1" xref="S3.SS1.p3.3.m3.3.3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.3.3.1.1.2" mathvariant="normal" xref="S3.SS1.p3.3.m3.3.3.1.1.2.cmml">‚Ñì</mi><mo id="S3.SS1.p3.3.m3.3.3.1.1.1" xref="S3.SS1.p3.3.m3.3.3.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.3.m3.3.3.1.1.3" xref="S3.SS1.p3.3.m3.3.3.1.1.3.cmml">2</mn></mrow><mo id="S3.SS1.p3.3.m3.3.3.1.2" xref="S3.SS1.p3.3.m3.3.3.2.cmml">,</mo><mi id="S3.SS1.p3.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p3.3.m3.1.1.cmml">‚Ä¶</mi><mo id="S3.SS1.p3.3.m3.3.3.1.3" xref="S3.SS1.p3.3.m3.3.3.2.cmml">,</mo><mi id="S3.SS1.p3.3.m3.2.2" xref="S3.SS1.p3.3.m3.2.2.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.3b"><list id="S3.SS1.p3.3.m3.3.3.2.cmml" xref="S3.SS1.p3.3.m3.3.3.1"><apply id="S3.SS1.p3.3.m3.3.3.1.1.cmml" xref="S3.SS1.p3.3.m3.3.3.1.1"><plus id="S3.SS1.p3.3.m3.3.3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.3.3.1.1.1"></plus><ci id="S3.SS1.p3.3.m3.3.3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.3.3.1.1.2">‚Ñì</ci><cn id="S3.SS1.p3.3.m3.3.3.1.1.3.cmml" type="integer" xref="S3.SS1.p3.3.m3.3.3.1.1.3">2</cn></apply><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">‚Ä¶</ci><ci id="S3.SS1.p3.3.m3.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2">ùêø</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.3c">\ell+2,\ldots,L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.3d">roman_‚Ñì + 2 , ‚Ä¶ , italic_L</annotation></semantics></math>,
deleting a shallow layer
should have a much greater impact
than deleting a deeper layer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">From this, we have the following
hypotheses
that we will test experimentally:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I1.ix1.1.1.1">(0)</em></span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">We should be able to prune layers of a residual network.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I1.ix2.1.1.1">(1)</em></span>
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1">We should have greater success pruning deeper layers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I1.ix3.1.1.1">(2)</em></span>
<div class="ltx_para" id="S3.I1.ix3.p1">
<p class="ltx_p" id="S3.I1.ix3.p1.1">Blocks of layers
we successfully prune should have outputs that are
similar to their inputs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS1.p4.2">In the next subsection, ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.2</span></a> we will explain the details of our pruning algorithm and in the following section,
¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4" title="4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4</span></a>,
we will present experimental evidence for points&nbsp;<em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.2.1">(0)-(2)</em>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Layer-pruning algorithm(s)</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our principal layer pruning algorithm is very simple:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">0.</span>
<div class="ltx_para" id="S3.I2.ix1.p1">
<p class="ltx_p" id="S3.I2.ix1.p1.1">Pick a a number of layers to prune <math alttext="n" class="ltx_Math" display="inline" id="S3.I2.ix1.p1.1.m1.1"><semantics id="S3.I2.ix1.p1.1.m1.1a"><mi id="S3.I2.ix1.p1.1.m1.1.1" xref="S3.I2.ix1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.I2.ix1.p1.1.m1.1b"><ci id="S3.I2.ix1.p1.1.m1.1.1.cmml" xref="S3.I2.ix1.p1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.ix1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.I2.ix1.p1.1.m1.1d">italic_n</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.3">Compute the angular distance
<math alttext="d(x^{(\ell)},x^{(\ell+n)})" class="ltx_Math" display="inline" id="S3.I2.i1.p1.1.m1.4"><semantics id="S3.I2.i1.p1.1.m1.4a"><mrow id="S3.I2.i1.p1.1.m1.4.4" xref="S3.I2.i1.p1.1.m1.4.4.cmml"><mi id="S3.I2.i1.p1.1.m1.4.4.4" xref="S3.I2.i1.p1.1.m1.4.4.4.cmml">d</mi><mo id="S3.I2.i1.p1.1.m1.4.4.3" xref="S3.I2.i1.p1.1.m1.4.4.3.cmml">‚Å¢</mo><mrow id="S3.I2.i1.p1.1.m1.4.4.2.2" xref="S3.I2.i1.p1.1.m1.4.4.2.3.cmml"><mo id="S3.I2.i1.p1.1.m1.4.4.2.2.3" stretchy="false" xref="S3.I2.i1.p1.1.m1.4.4.2.3.cmml">(</mo><msup id="S3.I2.i1.p1.1.m1.3.3.1.1.1" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml"><mi id="S3.I2.i1.p1.1.m1.3.3.1.1.1.2" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.2.cmml">x</mi><mrow id="S3.I2.i1.p1.1.m1.1.1.1.3" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml"><mo id="S3.I2.i1.p1.1.m1.1.1.1.3.1" stretchy="false" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml">(</mo><mi id="S3.I2.i1.p1.1.m1.1.1.1.1" mathvariant="normal" xref="S3.I2.i1.p1.1.m1.1.1.1.1.cmml">‚Ñì</mi><mo id="S3.I2.i1.p1.1.m1.1.1.1.3.2" stretchy="false" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.I2.i1.p1.1.m1.4.4.2.2.4" xref="S3.I2.i1.p1.1.m1.4.4.2.3.cmml">,</mo><msup id="S3.I2.i1.p1.1.m1.4.4.2.2.2" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2.cmml"><mi id="S3.I2.i1.p1.1.m1.4.4.2.2.2.2" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2.2.cmml">x</mi><mrow id="S3.I2.i1.p1.1.m1.2.2.1.1" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml"><mo id="S3.I2.i1.p1.1.m1.2.2.1.1.2" stretchy="false" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.I2.i1.p1.1.m1.2.2.1.1.1" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.I2.i1.p1.1.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.I2.i1.p1.1.m1.2.2.1.1.1.1" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.1.cmml">+</mo><mi id="S3.I2.i1.p1.1.m1.2.2.1.1.1.3" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.I2.i1.p1.1.m1.2.2.1.1.3" stretchy="false" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.I2.i1.p1.1.m1.4.4.2.2.5" stretchy="false" xref="S3.I2.i1.p1.1.m1.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.m1.4b"><apply id="S3.I2.i1.p1.1.m1.4.4.cmml" xref="S3.I2.i1.p1.1.m1.4.4"><times id="S3.I2.i1.p1.1.m1.4.4.3.cmml" xref="S3.I2.i1.p1.1.m1.4.4.3"></times><ci id="S3.I2.i1.p1.1.m1.4.4.4.cmml" xref="S3.I2.i1.p1.1.m1.4.4.4">ùëë</ci><interval closure="open" id="S3.I2.i1.p1.1.m1.4.4.2.3.cmml" xref="S3.I2.i1.p1.1.m1.4.4.2.2"><apply id="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.m1.3.3.1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1">superscript</csymbol><ci id="S3.I2.i1.p1.1.m1.3.3.1.1.1.2.cmml" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.2">ùë•</ci><ci id="S3.I2.i1.p1.1.m1.1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1.1.1">‚Ñì</ci></apply><apply id="S3.I2.i1.p1.1.m1.4.4.2.2.2.cmml" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.m1.4.4.2.2.2.1.cmml" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2">superscript</csymbol><ci id="S3.I2.i1.p1.1.m1.4.4.2.2.2.2.cmml" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2.2">ùë•</ci><apply id="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.2.2.1.1"><plus id="S3.I2.i1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.1"></plus><ci id="S3.I2.i1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.2">‚Ñì</ci><ci id="S3.I2.i1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.3">ùëõ</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.m1.4c">d(x^{(\ell)},x^{(\ell+n)})</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.1.m1.4d">italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + italic_n ) end_POSTSUPERSCRIPT )</annotation></semantics></math>,
cf. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E7" title="7 ‚Ä£ 3.2 Layer-pruning algorithm(s) ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a>) below,
between the input to layer <math alttext="\ell" class="ltx_Math" display="inline" id="S3.I2.i1.p1.2.m2.1"><semantics id="S3.I2.i1.p1.2.m2.1a"><mi id="S3.I2.i1.p1.2.m2.1.1" mathvariant="normal" xref="S3.I2.i1.p1.2.m2.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.2.m2.1b"><ci id="S3.I2.i1.p1.2.m2.1.1.cmml" xref="S3.I2.i1.p1.2.m2.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.2.m2.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.2.m2.1d">roman_‚Ñì</annotation></semantics></math> and the input to layer <math alttext="\ell+n" class="ltx_Math" display="inline" id="S3.I2.i1.p1.3.m3.1"><semantics id="S3.I2.i1.p1.3.m3.1a"><mrow id="S3.I2.i1.p1.3.m3.1.1" xref="S3.I2.i1.p1.3.m3.1.1.cmml"><mi id="S3.I2.i1.p1.3.m3.1.1.2" mathvariant="normal" xref="S3.I2.i1.p1.3.m3.1.1.2.cmml">‚Ñì</mi><mo id="S3.I2.i1.p1.3.m3.1.1.1" xref="S3.I2.i1.p1.3.m3.1.1.1.cmml">+</mo><mi id="S3.I2.i1.p1.3.m3.1.1.3" xref="S3.I2.i1.p1.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.3.m3.1b"><apply id="S3.I2.i1.p1.3.m3.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1"><plus id="S3.I2.i1.p1.3.m3.1.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1.1"></plus><ci id="S3.I2.i1.p1.3.m3.1.1.2.cmml" xref="S3.I2.i1.p1.3.m3.1.1.2">‚Ñì</ci><ci id="S3.I2.i1.p1.3.m3.1.1.3.cmml" xref="S3.I2.i1.p1.3.m3.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.3.m3.1c">\ell+n</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.3.m3.1d">roman_‚Ñì + italic_n</annotation></semantics></math> on a neutral pretraining dataset or on a dataset representative of a downstream task of interest.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Find the layer, <math alttext="\ell^{*}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.m1.1"><semantics id="S3.I2.i2.p1.1.m1.1a"><msup id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml"><mi id="S3.I2.i2.p1.1.m1.1.1.2" mathvariant="normal" xref="S3.I2.i2.p1.1.m1.1.1.2.cmml">‚Ñì</mi><mo id="S3.I2.i2.p1.1.m1.1.1.3" xref="S3.I2.i2.p1.1.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.1b"><apply id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.1.m1.1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.I2.i2.p1.1.m1.1.1.2.cmml" xref="S3.I2.i2.p1.1.m1.1.1.2">‚Ñì</ci><times id="S3.I2.i2.p1.1.m1.1.1.3.cmml" xref="S3.I2.i2.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.1c">\ell^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.m1.1d">roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>, that minimizes that distance:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\ell^{\star}(n)\equiv\operatorname*{arg\,min}_{\ell}~{}d(x^{(\ell)},x^{(\ell+n%
)})\,." class="ltx_Math" display="block" id="S3.E6.m1.4"><semantics id="S3.E6.m1.4a"><mrow id="S3.E6.m1.4.4.1" xref="S3.E6.m1.4.4.1.1.cmml"><mrow id="S3.E6.m1.4.4.1.1" xref="S3.E6.m1.4.4.1.1.cmml"><mrow id="S3.E6.m1.4.4.1.1.4" xref="S3.E6.m1.4.4.1.1.4.cmml"><msup id="S3.E6.m1.4.4.1.1.4.2" xref="S3.E6.m1.4.4.1.1.4.2.cmml"><mi id="S3.E6.m1.4.4.1.1.4.2.2" mathvariant="normal" xref="S3.E6.m1.4.4.1.1.4.2.2.cmml">‚Ñì</mi><mo id="S3.E6.m1.4.4.1.1.4.2.3" xref="S3.E6.m1.4.4.1.1.4.2.3.cmml">‚ãÜ</mo></msup><mo id="S3.E6.m1.4.4.1.1.4.1" xref="S3.E6.m1.4.4.1.1.4.1.cmml">‚Å¢</mo><mrow id="S3.E6.m1.4.4.1.1.4.3.2" xref="S3.E6.m1.4.4.1.1.4.cmml"><mo id="S3.E6.m1.4.4.1.1.4.3.2.1" stretchy="false" xref="S3.E6.m1.4.4.1.1.4.cmml">(</mo><mi id="S3.E6.m1.3.3" xref="S3.E6.m1.3.3.cmml">n</mi><mo id="S3.E6.m1.4.4.1.1.4.3.2.2" stretchy="false" xref="S3.E6.m1.4.4.1.1.4.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.4.4.1.1.3" xref="S3.E6.m1.4.4.1.1.3.cmml">‚â°</mo><mrow id="S3.E6.m1.4.4.1.1.2" xref="S3.E6.m1.4.4.1.1.2.cmml"><mrow id="S3.E6.m1.4.4.1.1.2.4" xref="S3.E6.m1.4.4.1.1.2.4.cmml"><munder id="S3.E6.m1.4.4.1.1.2.4.1" xref="S3.E6.m1.4.4.1.1.2.4.1.cmml"><mrow id="S3.E6.m1.4.4.1.1.2.4.1.2" xref="S3.E6.m1.4.4.1.1.2.4.1.2.cmml"><mi id="S3.E6.m1.4.4.1.1.2.4.1.2.2" xref="S3.E6.m1.4.4.1.1.2.4.1.2.2.cmml">arg</mi><mo id="S3.E6.m1.4.4.1.1.2.4.1.2.1" lspace="0.170em" xref="S3.E6.m1.4.4.1.1.2.4.1.2.1.cmml">‚Å¢</mo><mi id="S3.E6.m1.4.4.1.1.2.4.1.2.3" xref="S3.E6.m1.4.4.1.1.2.4.1.2.3.cmml">min</mi></mrow><mi id="S3.E6.m1.4.4.1.1.2.4.1.3" mathvariant="normal" xref="S3.E6.m1.4.4.1.1.2.4.1.3.cmml">‚Ñì</mi></munder><mo id="S3.E6.m1.4.4.1.1.2.4a" xref="S3.E6.m1.4.4.1.1.2.4.cmml">‚Å°</mo><mi id="S3.E6.m1.4.4.1.1.2.4.2" xref="S3.E6.m1.4.4.1.1.2.4.2.cmml">d</mi></mrow><mo id="S3.E6.m1.4.4.1.1.2.3" xref="S3.E6.m1.4.4.1.1.2.3.cmml">‚Å¢</mo><mrow id="S3.E6.m1.4.4.1.1.2.2.2" xref="S3.E6.m1.4.4.1.1.2.2.3.cmml"><mo id="S3.E6.m1.4.4.1.1.2.2.2.3" stretchy="false" xref="S3.E6.m1.4.4.1.1.2.2.3.cmml">(</mo><msup id="S3.E6.m1.4.4.1.1.1.1.1.1" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.4.4.1.1.1.1.1.1.2" xref="S3.E6.m1.4.4.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E6.m1.1.1.1.3" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.1.1.1.3.1" stretchy="false" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E6.m1.1.1.1.1" mathvariant="normal" xref="S3.E6.m1.1.1.1.1.cmml">‚Ñì</mi><mo id="S3.E6.m1.1.1.1.3.2" stretchy="false" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E6.m1.4.4.1.1.2.2.2.4" xref="S3.E6.m1.4.4.1.1.2.2.3.cmml">,</mo><msup id="S3.E6.m1.4.4.1.1.2.2.2.2" xref="S3.E6.m1.4.4.1.1.2.2.2.2.cmml"><mi id="S3.E6.m1.4.4.1.1.2.2.2.2.2" xref="S3.E6.m1.4.4.1.1.2.2.2.2.2.cmml">x</mi><mrow id="S3.E6.m1.2.2.1.1" xref="S3.E6.m1.2.2.1.1.1.cmml"><mo id="S3.E6.m1.2.2.1.1.2" stretchy="false" xref="S3.E6.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.2.2.1.1.1" xref="S3.E6.m1.2.2.1.1.1.cmml"><mi id="S3.E6.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E6.m1.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E6.m1.2.2.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.cmml">+</mo><mi id="S3.E6.m1.2.2.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E6.m1.2.2.1.1.3" stretchy="false" xref="S3.E6.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E6.m1.4.4.1.1.2.2.2.5" stretchy="false" xref="S3.E6.m1.4.4.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6.m1.4.4.1.2" lspace="0.170em" xref="S3.E6.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.4b"><apply id="S3.E6.m1.4.4.1.1.cmml" xref="S3.E6.m1.4.4.1"><equivalent id="S3.E6.m1.4.4.1.1.3.cmml" xref="S3.E6.m1.4.4.1.1.3"></equivalent><apply id="S3.E6.m1.4.4.1.1.4.cmml" xref="S3.E6.m1.4.4.1.1.4"><times id="S3.E6.m1.4.4.1.1.4.1.cmml" xref="S3.E6.m1.4.4.1.1.4.1"></times><apply id="S3.E6.m1.4.4.1.1.4.2.cmml" xref="S3.E6.m1.4.4.1.1.4.2"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.4.2.1.cmml" xref="S3.E6.m1.4.4.1.1.4.2">superscript</csymbol><ci id="S3.E6.m1.4.4.1.1.4.2.2.cmml" xref="S3.E6.m1.4.4.1.1.4.2.2">‚Ñì</ci><ci id="S3.E6.m1.4.4.1.1.4.2.3.cmml" xref="S3.E6.m1.4.4.1.1.4.2.3">‚ãÜ</ci></apply><ci id="S3.E6.m1.3.3.cmml" xref="S3.E6.m1.3.3">ùëõ</ci></apply><apply id="S3.E6.m1.4.4.1.1.2.cmml" xref="S3.E6.m1.4.4.1.1.2"><times id="S3.E6.m1.4.4.1.1.2.3.cmml" xref="S3.E6.m1.4.4.1.1.2.3"></times><apply id="S3.E6.m1.4.4.1.1.2.4.cmml" xref="S3.E6.m1.4.4.1.1.2.4"><apply id="S3.E6.m1.4.4.1.1.2.4.1.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.2.4.1.1.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1">subscript</csymbol><apply id="S3.E6.m1.4.4.1.1.2.4.1.2.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.2"><times id="S3.E6.m1.4.4.1.1.2.4.1.2.1.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.2.1"></times><ci id="S3.E6.m1.4.4.1.1.2.4.1.2.2.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.2.2">arg</ci><ci id="S3.E6.m1.4.4.1.1.2.4.1.2.3.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.2.3">min</ci></apply><ci id="S3.E6.m1.4.4.1.1.2.4.1.3.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.3">‚Ñì</ci></apply><ci id="S3.E6.m1.4.4.1.1.2.4.2.cmml" xref="S3.E6.m1.4.4.1.1.2.4.2">ùëë</ci></apply><interval closure="open" id="S3.E6.m1.4.4.1.1.2.2.3.cmml" xref="S3.E6.m1.4.4.1.1.2.2.2"><apply id="S3.E6.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E6.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.2">ùë•</ci><ci id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1">‚Ñì</ci></apply><apply id="S3.E6.m1.4.4.1.1.2.2.2.2.cmml" xref="S3.E6.m1.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.2.2.2.2.1.cmml" xref="S3.E6.m1.4.4.1.1.2.2.2.2">superscript</csymbol><ci id="S3.E6.m1.4.4.1.1.2.2.2.2.2.cmml" xref="S3.E6.m1.4.4.1.1.2.2.2.2.2">ùë•</ci><apply id="S3.E6.m1.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1"><plus id="S3.E6.m1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1"></plus><ci id="S3.E6.m1.2.2.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.2">‚Ñì</ci><ci id="S3.E6.m1.2.2.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.1.3">ùëõ</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.4c">\ell^{\star}(n)\equiv\operatorname*{arg\,min}_{\ell}~{}d(x^{(\ell)},x^{(\ell+n%
)})\,.</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.4d">roman_‚Ñì start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ( italic_n ) ‚â° start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + italic_n ) end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.4">Drop layers <math alttext="\ell^{\star}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.1.m1.1"><semantics id="S3.I2.i3.p1.1.m1.1a"><msup id="S3.I2.i3.p1.1.m1.1.1" xref="S3.I2.i3.p1.1.m1.1.1.cmml"><mi id="S3.I2.i3.p1.1.m1.1.1.2" mathvariant="normal" xref="S3.I2.i3.p1.1.m1.1.1.2.cmml">‚Ñì</mi><mo id="S3.I2.i3.p1.1.m1.1.1.3" xref="S3.I2.i3.p1.1.m1.1.1.3.cmml">‚ãÜ</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.1.m1.1b"><apply id="S3.I2.i3.p1.1.m1.1.1.cmml" xref="S3.I2.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i3.p1.1.m1.1.1.1.cmml" xref="S3.I2.i3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.I2.i3.p1.1.m1.1.1.2.cmml" xref="S3.I2.i3.p1.1.m1.1.1.2">‚Ñì</ci><ci id="S3.I2.i3.p1.1.m1.1.1.3.cmml" xref="S3.I2.i3.p1.1.m1.1.1.3">‚ãÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.1.m1.1c">\ell^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.1.m1.1d">roman_‚Ñì start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="\ell^{\star}\!\!+\!n\!-\!1" class="ltx_Math" display="inline" id="S3.I2.i3.p1.2.m2.1"><semantics id="S3.I2.i3.p1.2.m2.1a"><mrow id="S3.I2.i3.p1.2.m2.1.1" xref="S3.I2.i3.p1.2.m2.1.1.cmml"><mrow id="S3.I2.i3.p1.2.m2.1.1.2" xref="S3.I2.i3.p1.2.m2.1.1.2.cmml"><msup id="S3.I2.i3.p1.2.m2.1.1.2.2" xref="S3.I2.i3.p1.2.m2.1.1.2.2.cmml"><mi id="S3.I2.i3.p1.2.m2.1.1.2.2.2" mathvariant="normal" xref="S3.I2.i3.p1.2.m2.1.1.2.2.2.cmml">‚Ñì</mi><mo id="S3.I2.i3.p1.2.m2.1.1.2.2.3" xref="S3.I2.i3.p1.2.m2.1.1.2.2.3.cmml">‚ãÜ</mo></msup><mo id="S3.I2.i3.p1.2.m2.1.1.2.1" rspace="0.052em" xref="S3.I2.i3.p1.2.m2.1.1.2.1.cmml">+</mo><mi id="S3.I2.i3.p1.2.m2.1.1.2.3" xref="S3.I2.i3.p1.2.m2.1.1.2.3.cmml">n</mi></mrow><mo id="S3.I2.i3.p1.2.m2.1.1.1" lspace="0.052em" rspace="0.052em" xref="S3.I2.i3.p1.2.m2.1.1.1.cmml">‚àí</mo><mn id="S3.I2.i3.p1.2.m2.1.1.3" xref="S3.I2.i3.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.2.m2.1b"><apply id="S3.I2.i3.p1.2.m2.1.1.cmml" xref="S3.I2.i3.p1.2.m2.1.1"><minus id="S3.I2.i3.p1.2.m2.1.1.1.cmml" xref="S3.I2.i3.p1.2.m2.1.1.1"></minus><apply id="S3.I2.i3.p1.2.m2.1.1.2.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2"><plus id="S3.I2.i3.p1.2.m2.1.1.2.1.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.1"></plus><apply id="S3.I2.i3.p1.2.m2.1.1.2.2.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.2"><csymbol cd="ambiguous" id="S3.I2.i3.p1.2.m2.1.1.2.2.1.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.2">superscript</csymbol><ci id="S3.I2.i3.p1.2.m2.1.1.2.2.2.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.2.2">‚Ñì</ci><ci id="S3.I2.i3.p1.2.m2.1.1.2.2.3.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.2.3">‚ãÜ</ci></apply><ci id="S3.I2.i3.p1.2.m2.1.1.2.3.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.3">ùëõ</ci></apply><cn id="S3.I2.i3.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.I2.i3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.2.m2.1c">\ell^{\star}\!\!+\!n\!-\!1</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.2.m2.1d">roman_‚Ñì start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT + italic_n - 1</annotation></semantics></math>; connect the old input to layer <math alttext="\ell^{\star}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.3.m3.1"><semantics id="S3.I2.i3.p1.3.m3.1a"><msup id="S3.I2.i3.p1.3.m3.1.1" xref="S3.I2.i3.p1.3.m3.1.1.cmml"><mi id="S3.I2.i3.p1.3.m3.1.1.2" mathvariant="normal" xref="S3.I2.i3.p1.3.m3.1.1.2.cmml">‚Ñì</mi><mo id="S3.I2.i3.p1.3.m3.1.1.3" xref="S3.I2.i3.p1.3.m3.1.1.3.cmml">‚ãÜ</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.3.m3.1b"><apply id="S3.I2.i3.p1.3.m3.1.1.cmml" xref="S3.I2.i3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I2.i3.p1.3.m3.1.1.1.cmml" xref="S3.I2.i3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.I2.i3.p1.3.m3.1.1.2.cmml" xref="S3.I2.i3.p1.3.m3.1.1.2">‚Ñì</ci><ci id="S3.I2.i3.p1.3.m3.1.1.3.cmml" xref="S3.I2.i3.p1.3.m3.1.1.3">‚ãÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.3.m3.1c">\ell^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.3.m3.1d">roman_‚Ñì start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT</annotation></semantics></math> to the old <math alttext="(\ell^{\star}\!\!+\!n)" class="ltx_Math" display="inline" id="S3.I2.i3.p1.4.m4.1"><semantics id="S3.I2.i3.p1.4.m4.1a"><mrow id="S3.I2.i3.p1.4.m4.1.1.1" xref="S3.I2.i3.p1.4.m4.1.1.1.1.cmml"><mo id="S3.I2.i3.p1.4.m4.1.1.1.2" stretchy="false" xref="S3.I2.i3.p1.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.I2.i3.p1.4.m4.1.1.1.1" xref="S3.I2.i3.p1.4.m4.1.1.1.1.cmml"><msup id="S3.I2.i3.p1.4.m4.1.1.1.1.2" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.cmml"><mi id="S3.I2.i3.p1.4.m4.1.1.1.1.2.2" mathvariant="normal" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.2.cmml">‚Ñì</mi><mo id="S3.I2.i3.p1.4.m4.1.1.1.1.2.3" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.3.cmml">‚ãÜ</mo></msup><mo id="S3.I2.i3.p1.4.m4.1.1.1.1.1" rspace="0.052em" xref="S3.I2.i3.p1.4.m4.1.1.1.1.1.cmml">+</mo><mi id="S3.I2.i3.p1.4.m4.1.1.1.1.3" xref="S3.I2.i3.p1.4.m4.1.1.1.1.3.cmml">n</mi></mrow><mo id="S3.I2.i3.p1.4.m4.1.1.1.3" stretchy="false" xref="S3.I2.i3.p1.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.4.m4.1b"><apply id="S3.I2.i3.p1.4.m4.1.1.1.1.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1"><plus id="S3.I2.i3.p1.4.m4.1.1.1.1.1.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.1"></plus><apply id="S3.I2.i3.p1.4.m4.1.1.1.1.2.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.I2.i3.p1.4.m4.1.1.1.1.2.1.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2">superscript</csymbol><ci id="S3.I2.i3.p1.4.m4.1.1.1.1.2.2.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.2">‚Ñì</ci><ci id="S3.I2.i3.p1.4.m4.1.1.1.1.2.3.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.3">‚ãÜ</ci></apply><ci id="S3.I2.i3.p1.4.m4.1.1.1.1.3.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.4.m4.1c">(\ell^{\star}\!\!+\!n)</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.4.m4.1d">( roman_‚Ñì start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT + italic_n )</annotation></semantics></math>th layer block.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Layers are often contained in a data structure, such a <span class="ltx_text ltx_font_typewriter" id="footnote4.1">ModuleList</span> in <em class="ltx_emph ltx_font_italic" id="footnote4.2">PyTorch</em>, so to drop these layers we would simply define a new <span class="ltx_text ltx_font_typewriter" id="footnote4.3">ModuleList</span> that removes the
layers from <math alttext="\ell^{\star}" class="ltx_Math" display="inline" id="footnote4.m1.1"><semantics id="footnote4.m1.1b"><msup id="footnote4.m1.1.1" xref="footnote4.m1.1.1.cmml"><mi id="footnote4.m1.1.1.2" mathvariant="normal" xref="footnote4.m1.1.1.2.cmml">‚Ñì</mi><mo id="footnote4.m1.1.1.3" xref="footnote4.m1.1.1.3.cmml">‚ãÜ</mo></msup><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><apply id="footnote4.m1.1.1.cmml" xref="footnote4.m1.1.1"><csymbol cd="ambiguous" id="footnote4.m1.1.1.1.cmml" xref="footnote4.m1.1.1">superscript</csymbol><ci id="footnote4.m1.1.1.2.cmml" xref="footnote4.m1.1.1.2">‚Ñì</ci><ci id="footnote4.m1.1.1.3.cmml" xref="footnote4.m1.1.1.3">‚ãÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m1.1d">\ell^{\star}</annotation><annotation encoding="application/x-llamapun" id="footnote4.m1.1e">roman_‚Ñì start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="\ell^{\star}+n-1" class="ltx_Math" display="inline" id="footnote4.m2.1"><semantics id="footnote4.m2.1b"><mrow id="footnote4.m2.1.1" xref="footnote4.m2.1.1.cmml"><mrow id="footnote4.m2.1.1.2" xref="footnote4.m2.1.1.2.cmml"><msup id="footnote4.m2.1.1.2.2" xref="footnote4.m2.1.1.2.2.cmml"><mi id="footnote4.m2.1.1.2.2.2" mathvariant="normal" xref="footnote4.m2.1.1.2.2.2.cmml">‚Ñì</mi><mo id="footnote4.m2.1.1.2.2.3" xref="footnote4.m2.1.1.2.2.3.cmml">‚ãÜ</mo></msup><mo id="footnote4.m2.1.1.2.1" xref="footnote4.m2.1.1.2.1.cmml">+</mo><mi id="footnote4.m2.1.1.2.3" xref="footnote4.m2.1.1.2.3.cmml">n</mi></mrow><mo id="footnote4.m2.1.1.1" xref="footnote4.m2.1.1.1.cmml">‚àí</mo><mn id="footnote4.m2.1.1.3" xref="footnote4.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="footnote4.m2.1c"><apply id="footnote4.m2.1.1.cmml" xref="footnote4.m2.1.1"><minus id="footnote4.m2.1.1.1.cmml" xref="footnote4.m2.1.1.1"></minus><apply id="footnote4.m2.1.1.2.cmml" xref="footnote4.m2.1.1.2"><plus id="footnote4.m2.1.1.2.1.cmml" xref="footnote4.m2.1.1.2.1"></plus><apply id="footnote4.m2.1.1.2.2.cmml" xref="footnote4.m2.1.1.2.2"><csymbol cd="ambiguous" id="footnote4.m2.1.1.2.2.1.cmml" xref="footnote4.m2.1.1.2.2">superscript</csymbol><ci id="footnote4.m2.1.1.2.2.2.cmml" xref="footnote4.m2.1.1.2.2.2">‚Ñì</ci><ci id="footnote4.m2.1.1.2.2.3.cmml" xref="footnote4.m2.1.1.2.2.3">‚ãÜ</ci></apply><ci id="footnote4.m2.1.1.2.3.cmml" xref="footnote4.m2.1.1.2.3">ùëõ</ci></apply><cn id="footnote4.m2.1.1.3.cmml" type="integer" xref="footnote4.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m2.1d">\ell^{\star}+n-1</annotation><annotation encoding="application/x-llamapun" id="footnote4.m2.1e">roman_‚Ñì start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT + italic_n - 1</annotation></semantics></math>.
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">(Optionally) heal the mismatch at layer <math alttext="\ell^{\star}\!+n" class="ltx_Math" display="inline" id="S3.I2.i4.p1.1.m1.1"><semantics id="S3.I2.i4.p1.1.m1.1a"><mrow id="S3.I2.i4.p1.1.m1.1.1" xref="S3.I2.i4.p1.1.m1.1.1.cmml"><msup id="S3.I2.i4.p1.1.m1.1.1.2" xref="S3.I2.i4.p1.1.m1.1.1.2.cmml"><mi id="S3.I2.i4.p1.1.m1.1.1.2.2" mathvariant="normal" xref="S3.I2.i4.p1.1.m1.1.1.2.2.cmml">‚Ñì</mi><mo id="S3.I2.i4.p1.1.m1.1.1.2.3" xref="S3.I2.i4.p1.1.m1.1.1.2.3.cmml">‚ãÜ</mo></msup><mo id="S3.I2.i4.p1.1.m1.1.1.1" lspace="0.052em" xref="S3.I2.i4.p1.1.m1.1.1.1.cmml">+</mo><mi id="S3.I2.i4.p1.1.m1.1.1.3" xref="S3.I2.i4.p1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i4.p1.1.m1.1b"><apply id="S3.I2.i4.p1.1.m1.1.1.cmml" xref="S3.I2.i4.p1.1.m1.1.1"><plus id="S3.I2.i4.p1.1.m1.1.1.1.cmml" xref="S3.I2.i4.p1.1.m1.1.1.1"></plus><apply id="S3.I2.i4.p1.1.m1.1.1.2.cmml" xref="S3.I2.i4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I2.i4.p1.1.m1.1.1.2.1.cmml" xref="S3.I2.i4.p1.1.m1.1.1.2">superscript</csymbol><ci id="S3.I2.i4.p1.1.m1.1.1.2.2.cmml" xref="S3.I2.i4.p1.1.m1.1.1.2.2">‚Ñì</ci><ci id="S3.I2.i4.p1.1.m1.1.1.2.3.cmml" xref="S3.I2.i4.p1.1.m1.1.1.2.3">‚ãÜ</ci></apply><ci id="S3.I2.i4.p1.1.m1.1.1.3.cmml" xref="S3.I2.i4.p1.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i4.p1.1.m1.1c">\ell^{\star}\!+n</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i4.p1.1.m1.1d">roman_‚Ñì start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT + italic_n</annotation></semantics></math> with a small amount of fine tuning on a neutral pretraining dataset or particular dataset of interest.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS2.p1.2">If fewer words inside of a figure are more helpful to you than the text in an enumerated list, then note that this algorithm is also depicted in panels (a)-(b) of Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Elaborating on the first step, the angular distance on a single sequence of length <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_T</annotation></semantics></math> is given by
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d(x^{(\ell)},x^{(\ell+n)})\equiv\frac{1}{\pi}\arccos\left(\frac{x^{(\ell)}_{T}%
\cdot x^{(\ell+n)}_{T}}{\left|\!\left|x^{(\ell)}_{T}\right|\!\right|\left|\!%
\left|x^{(\ell+n)}_{T}\right|\!\right|}\right)\,," class="ltx_Math" display="block" id="S3.E7.m1.10"><semantics id="S3.E7.m1.10a"><mrow id="S3.E7.m1.10.10.1" xref="S3.E7.m1.10.10.1.1.cmml"><mrow id="S3.E7.m1.10.10.1.1" xref="S3.E7.m1.10.10.1.1.cmml"><mrow id="S3.E7.m1.10.10.1.1.2" xref="S3.E7.m1.10.10.1.1.2.cmml"><mi id="S3.E7.m1.10.10.1.1.2.4" xref="S3.E7.m1.10.10.1.1.2.4.cmml">d</mi><mo id="S3.E7.m1.10.10.1.1.2.3" xref="S3.E7.m1.10.10.1.1.2.3.cmml">‚Å¢</mo><mrow id="S3.E7.m1.10.10.1.1.2.2.2" xref="S3.E7.m1.10.10.1.1.2.2.3.cmml"><mo id="S3.E7.m1.10.10.1.1.2.2.2.3" stretchy="false" xref="S3.E7.m1.10.10.1.1.2.2.3.cmml">(</mo><msup id="S3.E7.m1.10.10.1.1.1.1.1.1" xref="S3.E7.m1.10.10.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.10.10.1.1.1.1.1.1.2" xref="S3.E7.m1.10.10.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E7.m1.1.1.1.3" xref="S3.E7.m1.10.10.1.1.1.1.1.1.cmml"><mo id="S3.E7.m1.1.1.1.3.1" stretchy="false" xref="S3.E7.m1.10.10.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E7.m1.1.1.1.1" mathvariant="normal" xref="S3.E7.m1.1.1.1.1.cmml">‚Ñì</mi><mo id="S3.E7.m1.1.1.1.3.2" stretchy="false" xref="S3.E7.m1.10.10.1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E7.m1.10.10.1.1.2.2.2.4" xref="S3.E7.m1.10.10.1.1.2.2.3.cmml">,</mo><msup id="S3.E7.m1.10.10.1.1.2.2.2.2" xref="S3.E7.m1.10.10.1.1.2.2.2.2.cmml"><mi id="S3.E7.m1.10.10.1.1.2.2.2.2.2" xref="S3.E7.m1.10.10.1.1.2.2.2.2.2.cmml">x</mi><mrow id="S3.E7.m1.2.2.1.1" xref="S3.E7.m1.2.2.1.1.1.cmml"><mo id="S3.E7.m1.2.2.1.1.2" stretchy="false" xref="S3.E7.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.2.2.1.1.1" xref="S3.E7.m1.2.2.1.1.1.cmml"><mi id="S3.E7.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E7.m1.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E7.m1.2.2.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.cmml">+</mo><mi id="S3.E7.m1.2.2.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E7.m1.2.2.1.1.3" stretchy="false" xref="S3.E7.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E7.m1.10.10.1.1.2.2.2.5" stretchy="false" xref="S3.E7.m1.10.10.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.10.10.1.1.3" xref="S3.E7.m1.10.10.1.1.3.cmml">‚â°</mo><mrow id="S3.E7.m1.10.10.1.1.4" xref="S3.E7.m1.10.10.1.1.4.cmml"><mfrac id="S3.E7.m1.10.10.1.1.4.2" xref="S3.E7.m1.10.10.1.1.4.2.cmml"><mn id="S3.E7.m1.10.10.1.1.4.2.2" xref="S3.E7.m1.10.10.1.1.4.2.2.cmml">1</mn><mi id="S3.E7.m1.10.10.1.1.4.2.3" xref="S3.E7.m1.10.10.1.1.4.2.3.cmml">œÄ</mi></mfrac><mo id="S3.E7.m1.10.10.1.1.4.1" lspace="0.167em" xref="S3.E7.m1.10.10.1.1.4.1.cmml">‚Å¢</mo><mrow id="S3.E7.m1.10.10.1.1.4.3.2" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml"><mi id="S3.E7.m1.9.9" xref="S3.E7.m1.9.9.cmml">arccos</mi><mo id="S3.E7.m1.10.10.1.1.4.3.2a" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml">‚Å°</mo><mrow id="S3.E7.m1.10.10.1.1.4.3.2.1" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml"><mo id="S3.E7.m1.10.10.1.1.4.3.2.1.1" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml">(</mo><mfrac id="S3.E7.m1.8.8" xref="S3.E7.m1.8.8.cmml"><mrow id="S3.E7.m1.4.4.2" xref="S3.E7.m1.4.4.2.cmml"><msubsup id="S3.E7.m1.4.4.2.4" xref="S3.E7.m1.4.4.2.4.cmml"><mi id="S3.E7.m1.4.4.2.4.2.2" xref="S3.E7.m1.4.4.2.4.2.2.cmml">x</mi><mi id="S3.E7.m1.4.4.2.4.3" xref="S3.E7.m1.4.4.2.4.3.cmml">T</mi><mrow id="S3.E7.m1.3.3.1.1.1.3" xref="S3.E7.m1.4.4.2.4.cmml"><mo id="S3.E7.m1.3.3.1.1.1.3.1" stretchy="false" xref="S3.E7.m1.4.4.2.4.cmml">(</mo><mi id="S3.E7.m1.3.3.1.1.1.1" mathvariant="normal" xref="S3.E7.m1.3.3.1.1.1.1.cmml">‚Ñì</mi><mo id="S3.E7.m1.3.3.1.1.1.3.2" stretchy="false" xref="S3.E7.m1.4.4.2.4.cmml">)</mo></mrow></msubsup><mo id="S3.E7.m1.4.4.2.3" lspace="0.222em" rspace="0.222em" xref="S3.E7.m1.4.4.2.3.cmml">‚ãÖ</mo><msubsup id="S3.E7.m1.4.4.2.5" xref="S3.E7.m1.4.4.2.5.cmml"><mi id="S3.E7.m1.4.4.2.5.2.2" xref="S3.E7.m1.4.4.2.5.2.2.cmml">x</mi><mi id="S3.E7.m1.4.4.2.5.3" xref="S3.E7.m1.4.4.2.5.3.cmml">T</mi><mrow id="S3.E7.m1.4.4.2.2.1.1" xref="S3.E7.m1.4.4.2.2.1.1.1.cmml"><mo id="S3.E7.m1.4.4.2.2.1.1.2" stretchy="false" xref="S3.E7.m1.4.4.2.2.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.4.4.2.2.1.1.1" xref="S3.E7.m1.4.4.2.2.1.1.1.cmml"><mi id="S3.E7.m1.4.4.2.2.1.1.1.2" mathvariant="normal" xref="S3.E7.m1.4.4.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E7.m1.4.4.2.2.1.1.1.1" xref="S3.E7.m1.4.4.2.2.1.1.1.1.cmml">+</mo><mi id="S3.E7.m1.4.4.2.2.1.1.1.3" xref="S3.E7.m1.4.4.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E7.m1.4.4.2.2.1.1.3" stretchy="false" xref="S3.E7.m1.4.4.2.2.1.1.1.cmml">)</mo></mrow></msubsup></mrow><mrow id="S3.E7.m1.8.8.6" xref="S3.E7.m1.8.8.6.cmml"><mrow id="S3.E7.m1.7.7.5.3.1" xref="S3.E7.m1.7.7.5.3.2.cmml"><mo id="S3.E7.m1.7.7.5.3.1.2" stretchy="false" xref="S3.E7.m1.7.7.5.3.2.1.cmml">‚Äñ</mo><msubsup id="S3.E7.m1.7.7.5.3.1.1" xref="S3.E7.m1.7.7.5.3.1.1.cmml"><mi id="S3.E7.m1.7.7.5.3.1.1.2.2" xref="S3.E7.m1.7.7.5.3.1.1.2.2.cmml">x</mi><mi id="S3.E7.m1.7.7.5.3.1.1.3" xref="S3.E7.m1.7.7.5.3.1.1.3.cmml">T</mi><mrow id="S3.E7.m1.5.5.3.1.1.3" xref="S3.E7.m1.7.7.5.3.1.1.cmml"><mo id="S3.E7.m1.5.5.3.1.1.3.1" stretchy="false" xref="S3.E7.m1.7.7.5.3.1.1.cmml">(</mo><mi id="S3.E7.m1.5.5.3.1.1.1" mathvariant="normal" xref="S3.E7.m1.5.5.3.1.1.1.cmml">‚Ñì</mi><mo id="S3.E7.m1.5.5.3.1.1.3.2" stretchy="false" xref="S3.E7.m1.7.7.5.3.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E7.m1.7.7.5.3.1.3" stretchy="false" xref="S3.E7.m1.7.7.5.3.2.1.cmml">‚Äñ</mo></mrow><mo id="S3.E7.m1.8.8.6.5" xref="S3.E7.m1.8.8.6.5.cmml">‚Å¢</mo><mrow id="S3.E7.m1.8.8.6.4.1" xref="S3.E7.m1.8.8.6.4.2.cmml"><mo id="S3.E7.m1.8.8.6.4.1.2" stretchy="false" xref="S3.E7.m1.8.8.6.4.2.1.cmml">‚Äñ</mo><msubsup id="S3.E7.m1.8.8.6.4.1.1" xref="S3.E7.m1.8.8.6.4.1.1.cmml"><mi id="S3.E7.m1.8.8.6.4.1.1.2.2" xref="S3.E7.m1.8.8.6.4.1.1.2.2.cmml">x</mi><mi id="S3.E7.m1.8.8.6.4.1.1.3" xref="S3.E7.m1.8.8.6.4.1.1.3.cmml">T</mi><mrow id="S3.E7.m1.6.6.4.2.1.1" xref="S3.E7.m1.6.6.4.2.1.1.1.cmml"><mo id="S3.E7.m1.6.6.4.2.1.1.2" stretchy="false" xref="S3.E7.m1.6.6.4.2.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.6.6.4.2.1.1.1" xref="S3.E7.m1.6.6.4.2.1.1.1.cmml"><mi id="S3.E7.m1.6.6.4.2.1.1.1.2" mathvariant="normal" xref="S3.E7.m1.6.6.4.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S3.E7.m1.6.6.4.2.1.1.1.1" xref="S3.E7.m1.6.6.4.2.1.1.1.1.cmml">+</mo><mi id="S3.E7.m1.6.6.4.2.1.1.1.3" xref="S3.E7.m1.6.6.4.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E7.m1.6.6.4.2.1.1.3" stretchy="false" xref="S3.E7.m1.6.6.4.2.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E7.m1.8.8.6.4.1.3" stretchy="false" xref="S3.E7.m1.8.8.6.4.2.1.cmml">‚Äñ</mo></mrow></mrow></mfrac><mo id="S3.E7.m1.10.10.1.1.4.3.2.1.2" rspace="0.170em" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E7.m1.10.10.1.2" xref="S3.E7.m1.10.10.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.10b"><apply id="S3.E7.m1.10.10.1.1.cmml" xref="S3.E7.m1.10.10.1"><equivalent id="S3.E7.m1.10.10.1.1.3.cmml" xref="S3.E7.m1.10.10.1.1.3"></equivalent><apply id="S3.E7.m1.10.10.1.1.2.cmml" xref="S3.E7.m1.10.10.1.1.2"><times id="S3.E7.m1.10.10.1.1.2.3.cmml" xref="S3.E7.m1.10.10.1.1.2.3"></times><ci id="S3.E7.m1.10.10.1.1.2.4.cmml" xref="S3.E7.m1.10.10.1.1.2.4">ùëë</ci><interval closure="open" id="S3.E7.m1.10.10.1.1.2.2.3.cmml" xref="S3.E7.m1.10.10.1.1.2.2.2"><apply id="S3.E7.m1.10.10.1.1.1.1.1.1.cmml" xref="S3.E7.m1.10.10.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.10.10.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.10.10.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E7.m1.10.10.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.10.10.1.1.1.1.1.1.2">ùë•</ci><ci id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1">‚Ñì</ci></apply><apply id="S3.E7.m1.10.10.1.1.2.2.2.2.cmml" xref="S3.E7.m1.10.10.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.10.10.1.1.2.2.2.2.1.cmml" xref="S3.E7.m1.10.10.1.1.2.2.2.2">superscript</csymbol><ci id="S3.E7.m1.10.10.1.1.2.2.2.2.2.cmml" xref="S3.E7.m1.10.10.1.1.2.2.2.2.2">ùë•</ci><apply id="S3.E7.m1.2.2.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1"><plus id="S3.E7.m1.2.2.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1"></plus><ci id="S3.E7.m1.2.2.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1.2">‚Ñì</ci><ci id="S3.E7.m1.2.2.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.1.3">ùëõ</ci></apply></apply></interval></apply><apply id="S3.E7.m1.10.10.1.1.4.cmml" xref="S3.E7.m1.10.10.1.1.4"><times id="S3.E7.m1.10.10.1.1.4.1.cmml" xref="S3.E7.m1.10.10.1.1.4.1"></times><apply id="S3.E7.m1.10.10.1.1.4.2.cmml" xref="S3.E7.m1.10.10.1.1.4.2"><divide id="S3.E7.m1.10.10.1.1.4.2.1.cmml" xref="S3.E7.m1.10.10.1.1.4.2"></divide><cn id="S3.E7.m1.10.10.1.1.4.2.2.cmml" type="integer" xref="S3.E7.m1.10.10.1.1.4.2.2">1</cn><ci id="S3.E7.m1.10.10.1.1.4.2.3.cmml" xref="S3.E7.m1.10.10.1.1.4.2.3">ùúã</ci></apply><apply id="S3.E7.m1.10.10.1.1.4.3.1.cmml" xref="S3.E7.m1.10.10.1.1.4.3.2"><arccos id="S3.E7.m1.9.9.cmml" xref="S3.E7.m1.9.9"></arccos><apply id="S3.E7.m1.8.8.cmml" xref="S3.E7.m1.8.8"><divide id="S3.E7.m1.8.8.7.cmml" xref="S3.E7.m1.8.8"></divide><apply id="S3.E7.m1.4.4.2.cmml" xref="S3.E7.m1.4.4.2"><ci id="S3.E7.m1.4.4.2.3.cmml" xref="S3.E7.m1.4.4.2.3">‚ãÖ</ci><apply id="S3.E7.m1.4.4.2.4.cmml" xref="S3.E7.m1.4.4.2.4"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.4.1.cmml" xref="S3.E7.m1.4.4.2.4">subscript</csymbol><apply id="S3.E7.m1.4.4.2.4.2.cmml" xref="S3.E7.m1.4.4.2.4"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.4.2.1.cmml" xref="S3.E7.m1.4.4.2.4">superscript</csymbol><ci id="S3.E7.m1.4.4.2.4.2.2.cmml" xref="S3.E7.m1.4.4.2.4.2.2">ùë•</ci><ci id="S3.E7.m1.3.3.1.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1">‚Ñì</ci></apply><ci id="S3.E7.m1.4.4.2.4.3.cmml" xref="S3.E7.m1.4.4.2.4.3">ùëá</ci></apply><apply id="S3.E7.m1.4.4.2.5.cmml" xref="S3.E7.m1.4.4.2.5"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.5.1.cmml" xref="S3.E7.m1.4.4.2.5">subscript</csymbol><apply id="S3.E7.m1.4.4.2.5.2.cmml" xref="S3.E7.m1.4.4.2.5"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.5.2.1.cmml" xref="S3.E7.m1.4.4.2.5">superscript</csymbol><ci id="S3.E7.m1.4.4.2.5.2.2.cmml" xref="S3.E7.m1.4.4.2.5.2.2">ùë•</ci><apply id="S3.E7.m1.4.4.2.2.1.1.1.cmml" xref="S3.E7.m1.4.4.2.2.1.1"><plus id="S3.E7.m1.4.4.2.2.1.1.1.1.cmml" xref="S3.E7.m1.4.4.2.2.1.1.1.1"></plus><ci id="S3.E7.m1.4.4.2.2.1.1.1.2.cmml" xref="S3.E7.m1.4.4.2.2.1.1.1.2">‚Ñì</ci><ci id="S3.E7.m1.4.4.2.2.1.1.1.3.cmml" xref="S3.E7.m1.4.4.2.2.1.1.1.3">ùëõ</ci></apply></apply><ci id="S3.E7.m1.4.4.2.5.3.cmml" xref="S3.E7.m1.4.4.2.5.3">ùëá</ci></apply></apply><apply id="S3.E7.m1.8.8.6.cmml" xref="S3.E7.m1.8.8.6"><times id="S3.E7.m1.8.8.6.5.cmml" xref="S3.E7.m1.8.8.6.5"></times><apply id="S3.E7.m1.7.7.5.3.2.cmml" xref="S3.E7.m1.7.7.5.3.1"><csymbol cd="latexml" id="S3.E7.m1.7.7.5.3.2.1.cmml" xref="S3.E7.m1.7.7.5.3.1.2">norm</csymbol><apply id="S3.E7.m1.7.7.5.3.1.1.cmml" xref="S3.E7.m1.7.7.5.3.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.7.7.5.3.1.1.1.cmml" xref="S3.E7.m1.7.7.5.3.1.1">subscript</csymbol><apply id="S3.E7.m1.7.7.5.3.1.1.2.cmml" xref="S3.E7.m1.7.7.5.3.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.7.7.5.3.1.1.2.1.cmml" xref="S3.E7.m1.7.7.5.3.1.1">superscript</csymbol><ci id="S3.E7.m1.7.7.5.3.1.1.2.2.cmml" xref="S3.E7.m1.7.7.5.3.1.1.2.2">ùë•</ci><ci id="S3.E7.m1.5.5.3.1.1.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1">‚Ñì</ci></apply><ci id="S3.E7.m1.7.7.5.3.1.1.3.cmml" xref="S3.E7.m1.7.7.5.3.1.1.3">ùëá</ci></apply></apply><apply id="S3.E7.m1.8.8.6.4.2.cmml" xref="S3.E7.m1.8.8.6.4.1"><csymbol cd="latexml" id="S3.E7.m1.8.8.6.4.2.1.cmml" xref="S3.E7.m1.8.8.6.4.1.2">norm</csymbol><apply id="S3.E7.m1.8.8.6.4.1.1.cmml" xref="S3.E7.m1.8.8.6.4.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.6.4.1.1.1.cmml" xref="S3.E7.m1.8.8.6.4.1.1">subscript</csymbol><apply id="S3.E7.m1.8.8.6.4.1.1.2.cmml" xref="S3.E7.m1.8.8.6.4.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.6.4.1.1.2.1.cmml" xref="S3.E7.m1.8.8.6.4.1.1">superscript</csymbol><ci id="S3.E7.m1.8.8.6.4.1.1.2.2.cmml" xref="S3.E7.m1.8.8.6.4.1.1.2.2">ùë•</ci><apply id="S3.E7.m1.6.6.4.2.1.1.1.cmml" xref="S3.E7.m1.6.6.4.2.1.1"><plus id="S3.E7.m1.6.6.4.2.1.1.1.1.cmml" xref="S3.E7.m1.6.6.4.2.1.1.1.1"></plus><ci id="S3.E7.m1.6.6.4.2.1.1.1.2.cmml" xref="S3.E7.m1.6.6.4.2.1.1.1.2">‚Ñì</ci><ci id="S3.E7.m1.6.6.4.2.1.1.1.3.cmml" xref="S3.E7.m1.6.6.4.2.1.1.1.3">ùëõ</ci></apply></apply><ci id="S3.E7.m1.8.8.6.4.1.1.3.cmml" xref="S3.E7.m1.8.8.6.4.1.1.3">ùëá</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.10c">d(x^{(\ell)},x^{(\ell+n)})\equiv\frac{1}{\pi}\arccos\left(\frac{x^{(\ell)}_{T}%
\cdot x^{(\ell+n)}_{T}}{\left|\!\left|x^{(\ell)}_{T}\right|\!\right|\left|\!%
\left|x^{(\ell+n)}_{T}\right|\!\right|}\right)\,,</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.10d">italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + italic_n ) end_POSTSUPERSCRIPT ) ‚â° divide start_ARG 1 end_ARG start_ARG italic_œÄ end_ARG roman_arccos ( divide start_ARG italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ‚ãÖ italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG start_ARG | | italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT | | | | italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT | | end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p2.5">where the inner product is
over the hidden dimension of the model
for the final token <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m1.1"><semantics id="S3.SS2.p2.2.m1.1a"><mi id="S3.SS2.p2.2.m1.1.1" xref="S3.SS2.p2.2.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m1.1b"><ci id="S3.SS2.p2.2.m1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m1.1d">italic_T</annotation></semantics></math> of the sequence,
<math alttext="|\!|\cdot|\!|" class="ltx_math_unparsed" display="inline" id="S3.SS2.p2.3.m2.1"><semantics id="S3.SS2.p2.3.m2.1a"><mrow id="S3.SS2.p2.3.m2.1b"><mo fence="false" id="S3.SS2.p2.3.m2.1.1" stretchy="false">|</mo><mo fence="false" id="S3.SS2.p2.3.m2.1.2" stretchy="false">|</mo><mo id="S3.SS2.p2.3.m2.1.3" lspace="0em" rspace="0em">‚ãÖ</mo><mo fence="false" id="S3.SS2.p2.3.m2.1.4" stretchy="false">|</mo><mo fence="false" id="S3.SS2.p2.3.m2.1.5" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m2.1c">|\!|\cdot|\!|</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m2.1d">| | ‚ãÖ | |</annotation></semantics></math> denotes the <math alttext="L^{2}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m3.1"><semantics id="S3.SS2.p2.4.m3.1a"><msup id="S3.SS2.p2.4.m3.1.1" xref="S3.SS2.p2.4.m3.1.1.cmml"><mi id="S3.SS2.p2.4.m3.1.1.2" xref="S3.SS2.p2.4.m3.1.1.2.cmml">L</mi><mn id="S3.SS2.p2.4.m3.1.1.3" xref="S3.SS2.p2.4.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m3.1b"><apply id="S3.SS2.p2.4.m3.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m3.1.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1">superscript</csymbol><ci id="S3.SS2.p2.4.m3.1.1.2.cmml" xref="S3.SS2.p2.4.m3.1.1.2">ùêø</ci><cn id="S3.SS2.p2.4.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p2.4.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m3.1c">L^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m3.1d">italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm,
and the factor of <math alttext="1/\pi" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m4.1"><semantics id="S3.SS2.p2.5.m4.1a"><mrow id="S3.SS2.p2.5.m4.1.1" xref="S3.SS2.p2.5.m4.1.1.cmml"><mn id="S3.SS2.p2.5.m4.1.1.2" xref="S3.SS2.p2.5.m4.1.1.2.cmml">1</mn><mo id="S3.SS2.p2.5.m4.1.1.1" xref="S3.SS2.p2.5.m4.1.1.1.cmml">/</mo><mi id="S3.SS2.p2.5.m4.1.1.3" xref="S3.SS2.p2.5.m4.1.1.3.cmml">œÄ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m4.1b"><apply id="S3.SS2.p2.5.m4.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1"><divide id="S3.SS2.p2.5.m4.1.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1.1"></divide><cn id="S3.SS2.p2.5.m4.1.1.2.cmml" type="integer" xref="S3.SS2.p2.5.m4.1.1.2">1</cn><ci id="S3.SS2.p2.5.m4.1.1.3.cmml" xref="S3.SS2.p2.5.m4.1.1.3">ùúã</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m4.1c">1/\pi</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m4.1d">1 / italic_œÄ</annotation></semantics></math> is a
convention.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Two comments: <em class="ltx_emph ltx_font_italic" id="footnote5.1">(i)</em>, we do not expect our choice of angular distance ‚Äì in lieu of any other reasonable metric, e.g., such as cosine similarity ‚Äì
to be particular significant; and <em class="ltx_emph ltx_font_italic" id="footnote5.2">(ii)</em>,
we chose to focus on the final token
since,
due to the causal attention mask,
its embedding is the only one that depends on the entire sequence.
</span></span></span>
This distance should then be summed over a number of examples that is large enough to get a low-fluctuation estimate but overall should be quite small.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Elaborating on the ‚Äúoptionality‚Äù of the final step, we find that the near-lack of performance degradation on question-answering benchmarks, cf. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>(d) and others in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS1" title="4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.1</span></a>, can be extended to greater pruning fractions with a small amount of finetuning. Depending on resource constraints and intended application of the pruned model, this may not be necessary. However,
the healing procedure
does have
a substantial impact on perplexity, cf. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>(d) and others in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS2" title="4.2 Loss on next-token predictions ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">For both the angular distance measuring and the healing,
if the ultimate goal is to supervise finetune (SFT) a model for a downstream task, it could be useful to evaluate the distance of a sample from that dataset and
then
combine the healing process with the SFT.
In contrast, for the greatest generality, it‚Äôs most natural to measure distance and heal with a pretraining dataset that approximates the statistics under which the model was originally pretrained.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.4">Finally, we also investigated an even simpler pruning strategy inspired
by
analyzing
the
angular distances across different model families:
drop
the deepest layers,
excluding
the final layer before the LLM head, and then (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p5.4.1">non-optionally</em>) heal the damage. For complete clarity,
this means that
if
we are
pruning <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_n</annotation></semantics></math> layers from an <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">ùêø</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_L</annotation></semantics></math>-layer model,
then we would remove layers <math alttext="(L-n)" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3.1"><semantics id="S3.SS2.p5.3.m3.1a"><mrow id="S3.SS2.p5.3.m3.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.1.cmml"><mo id="S3.SS2.p5.3.m3.1.1.1.2" stretchy="false" xref="S3.SS2.p5.3.m3.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p5.3.m3.1.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.1.cmml"><mi id="S3.SS2.p5.3.m3.1.1.1.1.2" xref="S3.SS2.p5.3.m3.1.1.1.1.2.cmml">L</mi><mo id="S3.SS2.p5.3.m3.1.1.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.1.1.cmml">‚àí</mo><mi id="S3.SS2.p5.3.m3.1.1.1.1.3" xref="S3.SS2.p5.3.m3.1.1.1.1.3.cmml">n</mi></mrow><mo id="S3.SS2.p5.3.m3.1.1.1.3" stretchy="false" xref="S3.SS2.p5.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1"><minus id="S3.SS2.p5.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1"></minus><ci id="S3.SS2.p5.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.2">ùêø</ci><ci id="S3.SS2.p5.3.m3.1.1.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">(L-n)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m3.1d">( italic_L - italic_n )</annotation></semantics></math> to <math alttext="(L-1)" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m4.1"><semantics id="S3.SS2.p5.4.m4.1a"><mrow id="S3.SS2.p5.4.m4.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.1.cmml"><mo id="S3.SS2.p5.4.m4.1.1.1.2" stretchy="false" xref="S3.SS2.p5.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p5.4.m4.1.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.1.cmml"><mi id="S3.SS2.p5.4.m4.1.1.1.1.2" xref="S3.SS2.p5.4.m4.1.1.1.1.2.cmml">L</mi><mo id="S3.SS2.p5.4.m4.1.1.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.1.1.cmml">‚àí</mo><mn id="S3.SS2.p5.4.m4.1.1.1.1.3" xref="S3.SS2.p5.4.m4.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p5.4.m4.1.1.1.3" stretchy="false" xref="S3.SS2.p5.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1"><minus id="S3.SS2.p5.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1"></minus><ci id="S3.SS2.p5.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.2">ùêø</ci><cn id="S3.SS2.p5.4.m4.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.p5.4.m4.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">(L-1)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.4.m4.1d">( italic_L - 1 )</annotation></semantics></math>, inclusive.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we demonstrate the effectiveness of our pruning strategy
on different
question-answering (QA) benchmarks
and highlight
a robust
pruning-driven transition in performance
(¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS1" title="4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.1</span></a>),
while,
in contrast,
we
find
that
the autoregressive perplexities of the healed pruned models
are
continuous across their transition points (¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS2" title="4.2 Loss on next-token predictions ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.2</span></a>);
then, after comparing the similarity statistics between different layers across model sizes and families (¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS3" title="4.3 Angular distances between representations ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.3</span></a>), we contrast our principal similarity-informed pruning strategy with a simpler remove-the-deepest-layers strategy (¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS4" title="4.4 A simpler pruning strategy ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">For our experiments,
we pruned a wide variety of large-scale LLMs from 2.7B to 70B parameters spanning 32 to 80 total unpruned layers. Specifically, we used models in
the Llama-2 family
<cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib20" title="">2023a</a>)</cite>,
the Qwen family <cite class="ltx_cite ltx_citemacro_cite">Bai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib77" title="">2023</a>)</cite>,
Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">Jiang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib78" title="">2023b</a>)</cite>,
and Phi-2 <cite class="ltx_cite ltx_citemacro_cite">Javaheripi and Bubeck (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib79" title="">2023</a>)</cite>.
For these models, we
executed
the ‚Äúhealing‚Äù step using
QLoRA <cite class="ltx_cite ltx_citemacro_cite">Dettmers et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>: our models were quantized to 4-bit precision and then finetuned, using QLoRA for efficient training,
on
either
164M or 328M
tokens
from
the Colossal Clean Crawled Corpus (C4) <cite class="ltx_cite ltx_citemacro_cite">Raffel et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib80" title="">2020</a>)</cite>, a common pretraining dataset.
As a result,
<em class="ltx_emph ltx_font_italic" id="S4.p2.1.1">each experiment of ours was performed on a single A<math alttext="100" class="ltx_Math" display="inline" id="S4.p2.1.1.m1.1"><semantics id="S4.p2.1.1.m1.1a"><mn id="S4.p2.1.1.m1.1.1" mathvariant="normal" xref="S4.p2.1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.1.m1.1b"><cn id="S4.p2.1.1.m1.1.1.cmml" type="integer" xref="S4.p2.1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.1.m1.1d">100</annotation></semantics></math> GPU</em>.
For our QA evals, we used
Massive Multitask Language Understanding (MMLU) <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib81" title="">2020</a>)</cite>,
a common world-knowledge and problem solving benchmark,
and
BoolQ <cite class="ltx_cite ltx_citemacro_cite">Clark et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib82" title="">2019</a>)</cite>, a common
yes/no
reading comprehension
benchmark
where the answer has to be inferred from the text itself.
The specifics
of our
models,
healing
procedure, dataset choices, and evaluation details
can be found across Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1" title="Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A</span></a>;
ablations of different hyperparameter choices can be found across
Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2" title="Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Accuracy on QA benchmarks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">Our first set of results are shown in
Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‚Ä£ 4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>,
where
we plot
<math alttext="5" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn id="S4.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">5</annotation></semantics></math>-shot
MMLU accuracy as a function of the fraction of
layers removed:
in the left panel
we present the Llama-2 family,
in the middle panel
we present models from the Qwen family,
and in the right panel
we show Mistral-7B and Phi-2.
In order to better compare models of different total number of layers,
in these plots we opted to
normalize the <math alttext="x" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_x</annotation></semantics></math>-axis by the
fraction of layers removed (rather than the absolute number of layers removed).
Note that since MMLU contains multiple choice questions with four possible responses, the expected accuracy of random guessing is 25%.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F2">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F2.1"><span class="ltx_text" id="S4.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="204" id="S4.F2.1.1.g1" src="x2.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>MMLU accuracy (5-shot) vs. fraction of layers
dropped
for different model families.
(<em class="ltx_emph ltx_font_italic" id="S4.F2.5.1">Left:</em> Llama-2 family; <em class="ltx_emph ltx_font_italic" id="S4.F2.6.2">Middle:</em> Qwen family; <em class="ltx_emph ltx_font_italic" id="S4.F2.7.3">Right:</em> Mistral-7B and Phi-2.)
The
solid lines
represent
performance
after dropping layers and healing,
dotted lines
show
performance after dropping layers only (no healing),
and the dashed gray line is the score for guessing randomly.
For these models, healing leads to modest improvements, and performances
are quite robust
until 20%-55% pruning fractions, depending on model family and size, at which point they transitions to random guessing.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Importantly, we see a characteristic flat region of robust performance followed by a sharp transition to random accuracy at a pruning fraction around 45%-55%
for models in the Llama-2 family, 35% for Mistral 7B, 25% for Phi-2, and 20% for models from the Qwen family.
This implies that the essential knowledge required to achieve a model‚Äôs top score
isn‚Äôt removed by significant layer
removal
‚Äì even
though the fraction can be quite
large(!) ‚Äì
until eventually
that knowledge is lost
at a critical model-dependent threshold.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>This effect
is rather
robust
to choice of QA benchmark: in Appendix Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.F6" title="Figure 6 ‚Ä£ A.2 Evaluation details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">6</span></a> we plot the average 0-shot BoolQ accuracy for our model families and observe analogous behavior.</span></span></span>
Contrasting the curves with and without healing, we see that finetuning offers a modest improvement by better preserving the unpruned performance and pushing the phase transition to random guessing to slightly larger pruning fractions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Broadly we see that layer pruning is more robust for the larger and deeper models, e.g. Llama-2-13B and Llama-2-70B, which we hypothesize could be related to the fact that either the smaller models are more overtrained, making parameters less redundant, or that the deeper models can afford to lose more layers in an absolute sense. Also, the Qwen family is strange, a fact we will further elaborate on in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS3" title="4.3 Angular distances between representations ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Loss
on
next-token predictions</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">In this section,
we
look at
the effect of layer pruning on
the pretraining optimization objective
‚Äì the cross-entropy loss of next-token prediction ‚Äì
when evaluated on a subset of
the C4 validation dataset.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We make sure that none of the validation data are seen during the healing
stage.</span></span></span>
In order to have a fair comparison across models with different sized vocabularies <math alttext="V" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">V</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_V</annotation></semantics></math>, we normalize the loss by <math alttext="\log V" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">log</mi><mo id="S4.SS2.p1.2.m2.1.1a" lspace="0.167em" xref="S4.SS2.p1.2.m2.1.1.cmml">‚Å°</mo><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><log id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></log><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\log V</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">roman_log italic_V</annotation></semantics></math>, which corresponds to the loss of sampling tokens randomly with uniform probability.
(See Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS2" title="A.2 Evaluation details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.2</span></a> for more details.)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F3" title="Figure 3 ‚Ä£ 4.2 Loss on next-token predictions ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a> ,
we plot the normalized C4 validation loss for
all seven of our models,
after healing (left panel) and before healing (right panel),
as a
function of the fraction layers removed. Without healing, we see that there is a somewhat sharp(ish) transition to random guessing for each model at
approximately
the pruning fraction that the QA benchmark accuracies also sharply transition to random guessing, suggesting that models are hopelessly harmed at this point, cf. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‚Ä£ 4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>.
Next, contrasting the scales of both plots, we see that healing significantly restores the next-token prediction ability of all the models to near-unpruned levels,
with
the loss
increasing slowly and linearly with layer dropping.
Most strikingly ‚Äì from a scientific perspective ‚Äì is
the post-healing
continuity
through the pruning fractions where we previously found
sharp transitions
for
the
QA benchmarks:
this decoupling
illustrates
one way of
disconnecting (or creating a miscalibration) between performance on downstream tasks ‚Äì such as MMLU and BoolQ ‚Äì and continuous measures of performance ‚Äì such as the cross-entropy loss.
<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>This
is consistent with Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Schaeffer et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib83" title="">2023</a>)</cite>
that
argued
jumps in one kind of metric may not be visible in others.
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F3">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F3.1"><span class="ltx_text" id="S4.F3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="224" id="S4.F3.1.1.g1" src="x3.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Normalized C4 validation loss vs. fraction of layers dropped
before healing (<em class="ltx_emph ltx_font_italic" id="S4.F3.4.1">left</em>) and after healing (<em class="ltx_emph ltx_font_italic" id="S4.F3.5.2">right</em>);
each curve is normalized by the cross-entropy loss of sampling uniformly from the model‚Äôs vocabulary.
For the experiments before healing, the loss for each model transitions to random guessing (gray dashed line) at approximately the same pruning fractions that the QA benchmarks transition to random guessing; after healing, there is continuity through the regions of sharp transition on QA tasks, cf. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‚Ä£ 4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>. Contrasting the overall scale of both plots, it‚Äôs clear that healing significantly restores the performance on next-token prediction to near-unpruned levels.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Angular distances between representations</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Given the central role the angular distance (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E7" title="7 ‚Ä£ 3.2 Layer-pruning algorithm(s) ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a>) plays in our pruning strategy,
let‚Äôs take a subsection to look at
these distances across our seven models.
For this analysis, the angular distances for each model were averaged over 10k samples from the C4 validation set.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.13">Recall from earlier Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>(c): for Llama-2-70B this plotted the angular distance <math alttext="d(x^{(\ell)},x^{(\ell+n)})" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.4"><semantics id="S4.SS3.p2.1.m1.4a"><mrow id="S4.SS3.p2.1.m1.4.4" xref="S4.SS3.p2.1.m1.4.4.cmml"><mi id="S4.SS3.p2.1.m1.4.4.4" xref="S4.SS3.p2.1.m1.4.4.4.cmml">d</mi><mo id="S4.SS3.p2.1.m1.4.4.3" xref="S4.SS3.p2.1.m1.4.4.3.cmml">‚Å¢</mo><mrow id="S4.SS3.p2.1.m1.4.4.2.2" xref="S4.SS3.p2.1.m1.4.4.2.3.cmml"><mo id="S4.SS3.p2.1.m1.4.4.2.2.3" stretchy="false" xref="S4.SS3.p2.1.m1.4.4.2.3.cmml">(</mo><msup id="S4.SS3.p2.1.m1.3.3.1.1.1" xref="S4.SS3.p2.1.m1.3.3.1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.3.3.1.1.1.2" xref="S4.SS3.p2.1.m1.3.3.1.1.1.2.cmml">x</mi><mrow id="S4.SS3.p2.1.m1.1.1.1.3" xref="S4.SS3.p2.1.m1.3.3.1.1.1.cmml"><mo id="S4.SS3.p2.1.m1.1.1.1.3.1" stretchy="false" xref="S4.SS3.p2.1.m1.3.3.1.1.1.cmml">(</mo><mi id="S4.SS3.p2.1.m1.1.1.1.1" mathvariant="normal" xref="S4.SS3.p2.1.m1.1.1.1.1.cmml">‚Ñì</mi><mo id="S4.SS3.p2.1.m1.1.1.1.3.2" stretchy="false" xref="S4.SS3.p2.1.m1.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="S4.SS3.p2.1.m1.4.4.2.2.4" xref="S4.SS3.p2.1.m1.4.4.2.3.cmml">,</mo><msup id="S4.SS3.p2.1.m1.4.4.2.2.2" xref="S4.SS3.p2.1.m1.4.4.2.2.2.cmml"><mi id="S4.SS3.p2.1.m1.4.4.2.2.2.2" xref="S4.SS3.p2.1.m1.4.4.2.2.2.2.cmml">x</mi><mrow id="S4.SS3.p2.1.m1.2.2.1.1" xref="S4.SS3.p2.1.m1.2.2.1.1.1.cmml"><mo id="S4.SS3.p2.1.m1.2.2.1.1.2" stretchy="false" xref="S4.SS3.p2.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S4.SS3.p2.1.m1.2.2.1.1.1" xref="S4.SS3.p2.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.2.2.1.1.1.2" mathvariant="normal" xref="S4.SS3.p2.1.m1.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S4.SS3.p2.1.m1.2.2.1.1.1.1" xref="S4.SS3.p2.1.m1.2.2.1.1.1.1.cmml">+</mo><mi id="S4.SS3.p2.1.m1.2.2.1.1.1.3" xref="S4.SS3.p2.1.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S4.SS3.p2.1.m1.2.2.1.1.3" stretchy="false" xref="S4.SS3.p2.1.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S4.SS3.p2.1.m1.4.4.2.2.5" stretchy="false" xref="S4.SS3.p2.1.m1.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.4b"><apply id="S4.SS3.p2.1.m1.4.4.cmml" xref="S4.SS3.p2.1.m1.4.4"><times id="S4.SS3.p2.1.m1.4.4.3.cmml" xref="S4.SS3.p2.1.m1.4.4.3"></times><ci id="S4.SS3.p2.1.m1.4.4.4.cmml" xref="S4.SS3.p2.1.m1.4.4.4">ùëë</ci><interval closure="open" id="S4.SS3.p2.1.m1.4.4.2.3.cmml" xref="S4.SS3.p2.1.m1.4.4.2.2"><apply id="S4.SS3.p2.1.m1.3.3.1.1.1.cmml" xref="S4.SS3.p2.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.3.3.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.3.3.1.1.1">superscript</csymbol><ci id="S4.SS3.p2.1.m1.3.3.1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.3.3.1.1.1.2">ùë•</ci><ci id="S4.SS3.p2.1.m1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1">‚Ñì</ci></apply><apply id="S4.SS3.p2.1.m1.4.4.2.2.2.cmml" xref="S4.SS3.p2.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.4.4.2.2.2.1.cmml" xref="S4.SS3.p2.1.m1.4.4.2.2.2">superscript</csymbol><ci id="S4.SS3.p2.1.m1.4.4.2.2.2.2.cmml" xref="S4.SS3.p2.1.m1.4.4.2.2.2.2">ùë•</ci><apply id="S4.SS3.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS3.p2.1.m1.2.2.1.1"><plus id="S4.SS3.p2.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.2.2.1.1.1.1"></plus><ci id="S4.SS3.p2.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.2.2.1.1.1.2">‚Ñì</ci><ci id="S4.SS3.p2.1.m1.2.2.1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.2.2.1.1.1.3">ùëõ</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.4c">d(x^{(\ell)},x^{(\ell+n)})</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.4d">italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + italic_n ) end_POSTSUPERSCRIPT )</annotation></semantics></math>
that compared the <math alttext="\ell" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mi id="S4.SS3.p2.2.m2.1.1" mathvariant="normal" xref="S4.SS3.p2.2.m2.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><ci id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">roman_‚Ñì</annotation></semantics></math>-th layer to the <math alttext="(\ell+n)" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.1.cmml"><mo id="S4.SS3.p2.3.m3.1.1.1.2" stretchy="false" xref="S4.SS3.p2.3.m3.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.p2.3.m3.1.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.1.1.2" mathvariant="normal" xref="S4.SS3.p2.3.m3.1.1.1.1.2.cmml">‚Ñì</mi><mo id="S4.SS3.p2.3.m3.1.1.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.1.1.cmml">+</mo><mi id="S4.SS3.p2.3.m3.1.1.1.1.3" xref="S4.SS3.p2.3.m3.1.1.1.1.3.cmml">n</mi></mrow><mo id="S4.SS3.p2.3.m3.1.1.1.3" stretchy="false" xref="S4.SS3.p2.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1"><plus id="S4.SS3.p2.3.m3.1.1.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1.1.1"></plus><ci id="S4.SS3.p2.3.m3.1.1.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.1.1.2">‚Ñì</ci><ci id="S4.SS3.p2.3.m3.1.1.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">(\ell+n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">( roman_‚Ñì + italic_n )</annotation></semantics></math>-th layer, across all initial indexes <math alttext="\ell" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mi id="S4.SS3.p2.4.m4.1.1" mathvariant="normal" xref="S4.SS3.p2.4.m4.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><ci id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">roman_‚Ñì</annotation></semantics></math> for block sizes from <math alttext="n=1" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mrow id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml"><mi id="S4.SS3.p2.5.m5.1.1.2" xref="S4.SS3.p2.5.m5.1.1.2.cmml">n</mi><mo id="S4.SS3.p2.5.m5.1.1.1" xref="S4.SS3.p2.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.5.m5.1.1.3" xref="S4.SS3.p2.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><apply id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1"><eq id="S4.SS3.p2.5.m5.1.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1.1"></eq><ci id="S4.SS3.p2.5.m5.1.1.2.cmml" xref="S4.SS3.p2.5.m5.1.1.2">ùëõ</ci><cn id="S4.SS3.p2.5.m5.1.1.3.cmml" type="integer" xref="S4.SS3.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">n=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">italic_n = 1</annotation></semantics></math> to <math alttext="n=64" class="ltx_Math" display="inline" id="S4.SS3.p2.6.m6.1"><semantics id="S4.SS3.p2.6.m6.1a"><mrow id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml"><mi id="S4.SS3.p2.6.m6.1.1.2" xref="S4.SS3.p2.6.m6.1.1.2.cmml">n</mi><mo id="S4.SS3.p2.6.m6.1.1.1" xref="S4.SS3.p2.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.6.m6.1.1.3" xref="S4.SS3.p2.6.m6.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><apply id="S4.SS3.p2.6.m6.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1"><eq id="S4.SS3.p2.6.m6.1.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1.1"></eq><ci id="S4.SS3.p2.6.m6.1.1.2.cmml" xref="S4.SS3.p2.6.m6.1.1.2">ùëõ</ci><cn id="S4.SS3.p2.6.m6.1.1.3.cmml" type="integer" xref="S4.SS3.p2.6.m6.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m6.1c">n=64</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.6.m6.1d">italic_n = 64</annotation></semantics></math>;
the minimum of the curves, <math alttext="\ell^{\star}(n)" class="ltx_Math" display="inline" id="S4.SS3.p2.7.m7.1"><semantics id="S4.SS3.p2.7.m7.1a"><mrow id="S4.SS3.p2.7.m7.1.2" xref="S4.SS3.p2.7.m7.1.2.cmml"><msup id="S4.SS3.p2.7.m7.1.2.2" xref="S4.SS3.p2.7.m7.1.2.2.cmml"><mi id="S4.SS3.p2.7.m7.1.2.2.2" mathvariant="normal" xref="S4.SS3.p2.7.m7.1.2.2.2.cmml">‚Ñì</mi><mo id="S4.SS3.p2.7.m7.1.2.2.3" xref="S4.SS3.p2.7.m7.1.2.2.3.cmml">‚ãÜ</mo></msup><mo id="S4.SS3.p2.7.m7.1.2.1" xref="S4.SS3.p2.7.m7.1.2.1.cmml">‚Å¢</mo><mrow id="S4.SS3.p2.7.m7.1.2.3.2" xref="S4.SS3.p2.7.m7.1.2.cmml"><mo id="S4.SS3.p2.7.m7.1.2.3.2.1" stretchy="false" xref="S4.SS3.p2.7.m7.1.2.cmml">(</mo><mi id="S4.SS3.p2.7.m7.1.1" xref="S4.SS3.p2.7.m7.1.1.cmml">n</mi><mo id="S4.SS3.p2.7.m7.1.2.3.2.2" stretchy="false" xref="S4.SS3.p2.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.7.m7.1b"><apply id="S4.SS3.p2.7.m7.1.2.cmml" xref="S4.SS3.p2.7.m7.1.2"><times id="S4.SS3.p2.7.m7.1.2.1.cmml" xref="S4.SS3.p2.7.m7.1.2.1"></times><apply id="S4.SS3.p2.7.m7.1.2.2.cmml" xref="S4.SS3.p2.7.m7.1.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.7.m7.1.2.2.1.cmml" xref="S4.SS3.p2.7.m7.1.2.2">superscript</csymbol><ci id="S4.SS3.p2.7.m7.1.2.2.2.cmml" xref="S4.SS3.p2.7.m7.1.2.2.2">‚Ñì</ci><ci id="S4.SS3.p2.7.m7.1.2.2.3.cmml" xref="S4.SS3.p2.7.m7.1.2.2.3">‚ãÜ</ci></apply><ci id="S4.SS3.p2.7.m7.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.7.m7.1c">\ell^{\star}(n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.7.m7.1d">roman_‚Ñì start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math>, gave the optimal block to prune for a given <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.p2.8.m8.1"><semantics id="S4.SS3.p2.8.m8.1a"><mi id="S4.SS3.p2.8.m8.1.1" xref="S4.SS3.p2.8.m8.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.8.m8.1b"><ci id="S4.SS3.p2.8.m8.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.8.m8.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.8.m8.1d">italic_n</annotation></semantics></math>, cf. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E6" title="6 ‚Ä£ item 2 ‚Ä£ 3.2 Layer-pruning algorithm(s) ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">6</span></a>).
A more compact way
to display this
same data
is shown in the heat maps of Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F4" title="Figure 4 ‚Ä£ 4.3 Angular distances between representations ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4</span></a>: each square is colored to depict the row-normalized angular distance
between layer <math alttext="\ell" class="ltx_Math" display="inline" id="S4.SS3.p2.9.m9.1"><semantics id="S4.SS3.p2.9.m9.1a"><mi id="S4.SS3.p2.9.m9.1.1" mathvariant="normal" xref="S4.SS3.p2.9.m9.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.9.m9.1b"><ci id="S4.SS3.p2.9.m9.1.1.cmml" xref="S4.SS3.p2.9.m9.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.9.m9.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.9.m9.1d">roman_‚Ñì</annotation></semantics></math> and <math alttext="\ell+n" class="ltx_Math" display="inline" id="S4.SS3.p2.10.m10.1"><semantics id="S4.SS3.p2.10.m10.1a"><mrow id="S4.SS3.p2.10.m10.1.1" xref="S4.SS3.p2.10.m10.1.1.cmml"><mi id="S4.SS3.p2.10.m10.1.1.2" mathvariant="normal" xref="S4.SS3.p2.10.m10.1.1.2.cmml">‚Ñì</mi><mo id="S4.SS3.p2.10.m10.1.1.1" xref="S4.SS3.p2.10.m10.1.1.1.cmml">+</mo><mi id="S4.SS3.p2.10.m10.1.1.3" xref="S4.SS3.p2.10.m10.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.10.m10.1b"><apply id="S4.SS3.p2.10.m10.1.1.cmml" xref="S4.SS3.p2.10.m10.1.1"><plus id="S4.SS3.p2.10.m10.1.1.1.cmml" xref="S4.SS3.p2.10.m10.1.1.1"></plus><ci id="S4.SS3.p2.10.m10.1.1.2.cmml" xref="S4.SS3.p2.10.m10.1.1.2">‚Ñì</ci><ci id="S4.SS3.p2.10.m10.1.1.3.cmml" xref="S4.SS3.p2.10.m10.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.10.m10.1c">\ell+n</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.10.m10.1d">roman_‚Ñì + italic_n</annotation></semantics></math>
across all possible <math alttext="\ell" class="ltx_Math" display="inline" id="S4.SS3.p2.11.m11.1"><semantics id="S4.SS3.p2.11.m11.1a"><mi id="S4.SS3.p2.11.m11.1.1" mathvariant="normal" xref="S4.SS3.p2.11.m11.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.11.m11.1b"><ci id="S4.SS3.p2.11.m11.1.1.cmml" xref="S4.SS3.p2.11.m11.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.11.m11.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.11.m11.1d">roman_‚Ñì</annotation></semantics></math>,
and <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.p2.12.m12.1"><semantics id="S4.SS3.p2.12.m12.1a"><mi id="S4.SS3.p2.12.m12.1.1" xref="S4.SS3.p2.12.m12.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.12.m12.1b"><ci id="S4.SS3.p2.12.m12.1.1.cmml" xref="S4.SS3.p2.12.m12.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.12.m12.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.12.m12.1d">italic_n</annotation></semantics></math> up to very large fractions of the total number of layers;
the optimal layer to prune for a given block size, <math alttext="\ell^{*}(n)" class="ltx_Math" display="inline" id="S4.SS3.p2.13.m13.1"><semantics id="S4.SS3.p2.13.m13.1a"><mrow id="S4.SS3.p2.13.m13.1.2" xref="S4.SS3.p2.13.m13.1.2.cmml"><msup id="S4.SS3.p2.13.m13.1.2.2" xref="S4.SS3.p2.13.m13.1.2.2.cmml"><mi id="S4.SS3.p2.13.m13.1.2.2.2" mathvariant="normal" xref="S4.SS3.p2.13.m13.1.2.2.2.cmml">‚Ñì</mi><mo id="S4.SS3.p2.13.m13.1.2.2.3" xref="S4.SS3.p2.13.m13.1.2.2.3.cmml">*</mo></msup><mo id="S4.SS3.p2.13.m13.1.2.1" xref="S4.SS3.p2.13.m13.1.2.1.cmml">‚Å¢</mo><mrow id="S4.SS3.p2.13.m13.1.2.3.2" xref="S4.SS3.p2.13.m13.1.2.cmml"><mo id="S4.SS3.p2.13.m13.1.2.3.2.1" stretchy="false" xref="S4.SS3.p2.13.m13.1.2.cmml">(</mo><mi id="S4.SS3.p2.13.m13.1.1" xref="S4.SS3.p2.13.m13.1.1.cmml">n</mi><mo id="S4.SS3.p2.13.m13.1.2.3.2.2" stretchy="false" xref="S4.SS3.p2.13.m13.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.13.m13.1b"><apply id="S4.SS3.p2.13.m13.1.2.cmml" xref="S4.SS3.p2.13.m13.1.2"><times id="S4.SS3.p2.13.m13.1.2.1.cmml" xref="S4.SS3.p2.13.m13.1.2.1"></times><apply id="S4.SS3.p2.13.m13.1.2.2.cmml" xref="S4.SS3.p2.13.m13.1.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.13.m13.1.2.2.1.cmml" xref="S4.SS3.p2.13.m13.1.2.2">superscript</csymbol><ci id="S4.SS3.p2.13.m13.1.2.2.2.cmml" xref="S4.SS3.p2.13.m13.1.2.2.2">‚Ñì</ci><times id="S4.SS3.p2.13.m13.1.2.2.3.cmml" xref="S4.SS3.p2.13.m13.1.2.2.3"></times></apply><ci id="S4.SS3.p2.13.m13.1.1.cmml" xref="S4.SS3.p2.13.m13.1.1">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.13.m13.1c">\ell^{*}(n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.13.m13.1d">roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math>, corresponds to the minimal distance in each row.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Across models, we
make two generalizations:
<em class="ltx_emph ltx_font_italic" id="S4.SS3.p3.1.1">(i)</em> the smallest distances
are found across the deeper blocks,
meaning
deeper layers are typically quite similar to each other and can be more easily dropped;
<em class="ltx_emph ltx_font_italic" id="S4.SS3.p3.1.2">(ii)</em> the distances across the deepest blocks ‚Äì the blocks that include the last layer
‚Äì take either maximal or nearly-maximal values,
meaning one should never drop the final layer.
While broadly true, there
are a few exceptions.
For some models, e.g. Phi-2-2.7B, or for the largest blocks in some models, e.g. Llama-2-7B, final <em class="ltx_emph ltx_font_italic" id="S4.SS3.p3.1.3">few</em> layers seem important.
As previously noted, the Qwen family is somewhat unusual: here we see
that
there are a few
odd ‚Äúislands‚Äù of high similarity for shallow blocks;
this
likely
explains the shorter region of robust performance in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‚Ä£ 4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F4">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F4.1"><span class="ltx_text" id="S4.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="484" id="S4.F4.1.1.g1" src="x4.png" width="831"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Normalized angular distance (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E7" title="7 ‚Ä£ 3.2 Layer-pruning algorithm(s) ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a>)
from initial layer <math alttext="\ell" class="ltx_Math" display="inline" id="S4.F4.7.m1.1"><semantics id="S4.F4.7.m1.1b"><mi id="S4.F4.7.m1.1.1" mathvariant="normal" xref="S4.F4.7.m1.1.1.cmml">‚Ñì</mi><annotation-xml encoding="MathML-Content" id="S4.F4.7.m1.1c"><ci id="S4.F4.7.m1.1.1.cmml" xref="S4.F4.7.m1.1.1">‚Ñì</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.7.m1.1d">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.F4.7.m1.1e">roman_‚Ñì</annotation></semantics></math> (x-axis) with block size <math alttext="n" class="ltx_Math" display="inline" id="S4.F4.8.m2.1"><semantics id="S4.F4.8.m2.1b"><mi id="S4.F4.8.m2.1.1" xref="S4.F4.8.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F4.8.m2.1c"><ci id="S4.F4.8.m2.1.1.cmml" xref="S4.F4.8.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.8.m2.1d">n</annotation><annotation encoding="application/x-llamapun" id="S4.F4.8.m2.1e">italic_n</annotation></semantics></math> (y-axis) for each of the seven models we evaluated;
the distance
for each
<math alttext="n" class="ltx_Math" display="inline" id="S4.F4.9.m3.1"><semantics id="S4.F4.9.m3.1b"><mi id="S4.F4.9.m3.1.1" xref="S4.F4.9.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F4.9.m3.1c"><ci id="S4.F4.9.m3.1.1.cmml" xref="S4.F4.9.m3.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.9.m3.1d">n</annotation><annotation encoding="application/x-llamapun" id="S4.F4.9.m3.1e">italic_n</annotation></semantics></math>
is shifted and rescaled
to span the same range, <math alttext="[0,1]" class="ltx_Math" display="inline" id="S4.F4.10.m4.2"><semantics id="S4.F4.10.m4.2b"><mrow id="S4.F4.10.m4.2.3.2" xref="S4.F4.10.m4.2.3.1.cmml"><mo id="S4.F4.10.m4.2.3.2.1" stretchy="false" xref="S4.F4.10.m4.2.3.1.cmml">[</mo><mn id="S4.F4.10.m4.1.1" xref="S4.F4.10.m4.1.1.cmml">0</mn><mo id="S4.F4.10.m4.2.3.2.2" xref="S4.F4.10.m4.2.3.1.cmml">,</mo><mn id="S4.F4.10.m4.2.2" xref="S4.F4.10.m4.2.2.cmml">1</mn><mo id="S4.F4.10.m4.2.3.2.3" stretchy="false" xref="S4.F4.10.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.10.m4.2c"><interval closure="closed" id="S4.F4.10.m4.2.3.1.cmml" xref="S4.F4.10.m4.2.3.2"><cn id="S4.F4.10.m4.1.1.cmml" type="integer" xref="S4.F4.10.m4.1.1">0</cn><cn id="S4.F4.10.m4.2.2.cmml" type="integer" xref="S4.F4.10.m4.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.10.m4.2d">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S4.F4.10.m4.2e">[ 0 , 1 ]</annotation></semantics></math>
(yellow to purple): the optimal block to prune, <math alttext="\ell^{*}(n)" class="ltx_Math" display="inline" id="S4.F4.11.m5.1"><semantics id="S4.F4.11.m5.1b"><mrow id="S4.F4.11.m5.1.2" xref="S4.F4.11.m5.1.2.cmml"><msup id="S4.F4.11.m5.1.2.2" xref="S4.F4.11.m5.1.2.2.cmml"><mi id="S4.F4.11.m5.1.2.2.2" mathvariant="normal" xref="S4.F4.11.m5.1.2.2.2.cmml">‚Ñì</mi><mo id="S4.F4.11.m5.1.2.2.3" xref="S4.F4.11.m5.1.2.2.3.cmml">*</mo></msup><mo id="S4.F4.11.m5.1.2.1" xref="S4.F4.11.m5.1.2.1.cmml">‚Å¢</mo><mrow id="S4.F4.11.m5.1.2.3.2" xref="S4.F4.11.m5.1.2.cmml"><mo id="S4.F4.11.m5.1.2.3.2.1" stretchy="false" xref="S4.F4.11.m5.1.2.cmml">(</mo><mi id="S4.F4.11.m5.1.1" xref="S4.F4.11.m5.1.1.cmml">n</mi><mo id="S4.F4.11.m5.1.2.3.2.2" stretchy="false" xref="S4.F4.11.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.11.m5.1c"><apply id="S4.F4.11.m5.1.2.cmml" xref="S4.F4.11.m5.1.2"><times id="S4.F4.11.m5.1.2.1.cmml" xref="S4.F4.11.m5.1.2.1"></times><apply id="S4.F4.11.m5.1.2.2.cmml" xref="S4.F4.11.m5.1.2.2"><csymbol cd="ambiguous" id="S4.F4.11.m5.1.2.2.1.cmml" xref="S4.F4.11.m5.1.2.2">superscript</csymbol><ci id="S4.F4.11.m5.1.2.2.2.cmml" xref="S4.F4.11.m5.1.2.2.2">‚Ñì</ci><times id="S4.F4.11.m5.1.2.2.3.cmml" xref="S4.F4.11.m5.1.2.2.3"></times></apply><ci id="S4.F4.11.m5.1.1.cmml" xref="S4.F4.11.m5.1.1">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.11.m5.1d">\ell^{*}(n)</annotation><annotation encoding="application/x-llamapun" id="S4.F4.11.m5.1e">roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math>,
corresponds to
the deepest yellow for each row.
Across models,
the
deeper layers tend
to be very similar,
though
the deepest blocks that include the final layer
(squares along the outer diagonal)
are
(near-)maximally dissimilar.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>A simpler pruning strategy</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.4">Inspired by
our recent conclusions,
we experiment with a very simple heuristic pruning strategy: <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.4.1">(1)</em> if pruning <math alttext="n" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">italic_n</annotation></semantics></math> layers from an <math alttext="L" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><mi id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><ci id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">ùêø</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">L</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">italic_L</annotation></semantics></math>-layer model, drop layers <math alttext="(L-n)" class="ltx_Math" display="inline" id="S4.SS4.p1.3.m3.1"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.1.cmml"><mo id="S4.SS4.p1.3.m3.1.1.1.2" stretchy="false" xref="S4.SS4.p1.3.m3.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p1.3.m3.1.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.1.1.2" xref="S4.SS4.p1.3.m3.1.1.1.1.2.cmml">L</mi><mo id="S4.SS4.p1.3.m3.1.1.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.1.1.cmml">‚àí</mo><mi id="S4.SS4.p1.3.m3.1.1.1.1.3" xref="S4.SS4.p1.3.m3.1.1.1.1.3.cmml">n</mi></mrow><mo id="S4.SS4.p1.3.m3.1.1.1.3" stretchy="false" xref="S4.SS4.p1.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1"><minus id="S4.SS4.p1.3.m3.1.1.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1.1.1"></minus><ci id="S4.SS4.p1.3.m3.1.1.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.1.1.2">ùêø</ci><ci id="S4.SS4.p1.3.m3.1.1.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">(L-n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.3.m3.1d">( italic_L - italic_n )</annotation></semantics></math> to <math alttext="(L-1)" class="ltx_Math" display="inline" id="S4.SS4.p1.4.m4.1"><semantics id="S4.SS4.p1.4.m4.1a"><mrow id="S4.SS4.p1.4.m4.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.1.cmml"><mo id="S4.SS4.p1.4.m4.1.1.1.2" stretchy="false" xref="S4.SS4.p1.4.m4.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p1.4.m4.1.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.1.1.2" xref="S4.SS4.p1.4.m4.1.1.1.1.2.cmml">L</mi><mo id="S4.SS4.p1.4.m4.1.1.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.1.1.cmml">‚àí</mo><mn id="S4.SS4.p1.4.m4.1.1.1.1.3" xref="S4.SS4.p1.4.m4.1.1.1.1.3.cmml">1</mn></mrow><mo id="S4.SS4.p1.4.m4.1.1.1.3" stretchy="false" xref="S4.SS4.p1.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1"><minus id="S4.SS4.p1.4.m4.1.1.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1.1.1"></minus><ci id="S4.SS4.p1.4.m4.1.1.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.1.1.2">ùêø</ci><cn id="S4.SS4.p1.4.m4.1.1.1.1.3.cmml" type="integer" xref="S4.SS4.p1.4.m4.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">(L-1)</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.4.m4.1d">( italic_L - 1 )</annotation></semantics></math>
so as to remove
the deepest block that excludes the final layer;
then <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.4.2">(2)</em> heal with a small amount of finetuning as before.
Compared with our principal similarity-informed pruning strategy, this simpler heuristic algorithm has the advantage of
never
requiring
practitioners
to
load onto a GPU or inference the unpruned model.
It also provides a meaningful ablation of the importance
of optimizing the
block to prune.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F5" title="Figure 5 ‚Ä£ 4.4 A simpler pruning strategy ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">5</span></a>, we contrast our two pruning strategies, both before healing (left panels) and after healing (right panels), for the QA benchmarks (MMLU/BoolQ, top/middle panels) and the autoregressive loss (C4 validation, bottom panels). On the one hand,
the simple heuristic performs quite poorly without healing the damage incurred by pruning: accuracy on the QA benchmarks decays rapidly to (near-) random with increased pruning fraction, and the loss begins to increase very rapidly even with small amounts of pruning.
On the other hand,
the results for the two pruning strategies across evaluations are quite comparable after healing: for the QA benchmarks, the similarity-informed algorithm slightly better preserves the accuracy before the phase transition,
though the simple algorithm perhaps pushes the phase transition
to slightly
greater pruning factions;
and
for the loss,
the curves nearly lie on top of each other,
though the similarity-informed strategy
does marginally outperform
for all amounts of pruning.
These experiments are strong evidence that the purpose of
post-pruning finetuning is
the
healing
of damage
at the pruning interface
and not
the
acquisition of additional knowledge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F5">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.1"><span class="ltx_text" id="S4.F5.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="S4.F5.1.1.g1" src="x5.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Evaluation of Llama-2-70B
with
the
simple pruning heuristic (solid red line),
shown along with
scores
for
the similarity-informed pruning strategy (solid blue line),
scores of the unpruned Llama-2-70B (red dashed line),
and
scores for
randomly guessing (gray dashed line).
(<em class="ltx_emph ltx_font_italic" id="S4.F5.7.1">Left:</em> before healing, <em class="ltx_emph ltx_font_italic" id="S4.F5.8.2">Right:</em> after healing; <em class="ltx_emph ltx_font_italic" id="S4.F5.9.3">Top:</em> MMLU, <em class="ltx_emph ltx_font_italic" id="S4.F5.10.4">Middle:</em> BoolQ, <em class="ltx_emph ltx_font_italic" id="S4.F5.11.5">Bottom:</em> C4 Validation Loss.)
Without healing, the simple heuristic performs poorly across all evals; with healing, the scores of both methods are quite similar.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Directions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Beginning with the release of the open-weight LLaMA family <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib84" title="">2023b</a>)</cite>, the open-source machine-learning community has rallied around the philosophy of making LLMs accessible to everyone.
This has engendered many innovations around efficiency,
such as LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib13" title="">2021</a>)</cite> and
quantization (with LoRA) <cite class="ltx_cite ltx_citemacro_cite">Dettmers et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>,
allowing large (near-)state-of-the-art 70B models to be finetuned on only
single 80GB A100 GPUs.
In conjunction with these other tools,
our work enables further efficiency gains via a simple-to-implement layer-pruning technique.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.4">In particular,
the released version of Llama-2-70B
spans <math alttext="140" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mn id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">140</mn><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><cn id="S5.p2.1.m1.1.1.cmml" type="integer" xref="S5.p2.1.m1.1.1">140</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">140</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">140</annotation></semantics></math> GB of memory
and consumes approximately <math alttext="3\times 10^{10}" class="ltx_Math" display="inline" id="S5.p2.2.m2.1"><semantics id="S5.p2.2.m2.1a"><mrow id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mn id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">3</mn><mo id="S5.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.p2.2.m2.1.1.1.cmml">√ó</mo><msup id="S5.p2.2.m2.1.1.3" xref="S5.p2.2.m2.1.1.3.cmml"><mn id="S5.p2.2.m2.1.1.3.2" xref="S5.p2.2.m2.1.1.3.2.cmml">10</mn><mn id="S5.p2.2.m2.1.1.3.3" xref="S5.p2.2.m2.1.1.3.3.cmml">10</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><times id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1.1"></times><cn id="S5.p2.2.m2.1.1.2.cmml" type="integer" xref="S5.p2.2.m2.1.1.2">3</cn><apply id="S5.p2.2.m2.1.1.3.cmml" xref="S5.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.p2.2.m2.1.1.3.1.cmml" xref="S5.p2.2.m2.1.1.3">superscript</csymbol><cn id="S5.p2.2.m2.1.1.3.2.cmml" type="integer" xref="S5.p2.2.m2.1.1.3.2">10</cn><cn id="S5.p2.2.m2.1.1.3.3.cmml" type="integer" xref="S5.p2.2.m2.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">3\times 10^{10}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.2.m2.1d">3 √ó 10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOPs per token.
With 4-bit quantization
and
a layer-pruning fraction of 50%, the model fits in approximately <math alttext="17.5" class="ltx_Math" display="inline" id="S5.p2.3.m3.1"><semantics id="S5.p2.3.m3.1a"><mn id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml">17.5</mn><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><cn id="S5.p2.3.m3.1.1.cmml" type="float" xref="S5.p2.3.m3.1.1">17.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">17.5</annotation><annotation encoding="application/x-llamapun" id="S5.p2.3.m3.1d">17.5</annotation></semantics></math> GB of memory and requires roughly <math alttext="1.5\times 10^{10}" class="ltx_Math" display="inline" id="S5.p2.4.m4.1"><semantics id="S5.p2.4.m4.1a"><mrow id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml"><mn id="S5.p2.4.m4.1.1.2" xref="S5.p2.4.m4.1.1.2.cmml">1.5</mn><mo id="S5.p2.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.p2.4.m4.1.1.1.cmml">√ó</mo><msup id="S5.p2.4.m4.1.1.3" xref="S5.p2.4.m4.1.1.3.cmml"><mn id="S5.p2.4.m4.1.1.3.2" xref="S5.p2.4.m4.1.1.3.2.cmml">10</mn><mn id="S5.p2.4.m4.1.1.3.3" xref="S5.p2.4.m4.1.1.3.3.cmml">10</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><apply id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1"><times id="S5.p2.4.m4.1.1.1.cmml" xref="S5.p2.4.m4.1.1.1"></times><cn id="S5.p2.4.m4.1.1.2.cmml" type="float" xref="S5.p2.4.m4.1.1.2">1.5</cn><apply id="S5.p2.4.m4.1.1.3.cmml" xref="S5.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S5.p2.4.m4.1.1.3.1.cmml" xref="S5.p2.4.m4.1.1.3">superscript</csymbol><cn id="S5.p2.4.m4.1.1.3.2.cmml" type="integer" xref="S5.p2.4.m4.1.1.3.2">10</cn><cn id="S5.p2.4.m4.1.1.3.3.cmml" type="integer" xref="S5.p2.4.m4.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">1.5\times 10^{10}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.4.m4.1d">1.5 √ó 10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOPs per token:
quantization from 16-bit <span class="ltx_text ltx_font_typewriter" id="S5.p2.4.1">bfloats</span> to
4-bit QLoRA precision reduces
model memory by a factor of 4, but keeps
FLOPs more or less the same, since calculations are performed in 16-bit precision;
layer pruning will additionally reduce both memory and FLOPs by an amount equal to the layer-pruning fraction.These memory and compute requirements enable open-weight state-of-the-art models to be run and even finetuned efficiently on consumer-level GPUs without any CPU off-loading and with only minor performance trade-offs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">At the conclusion of the work, we are left with the following questions:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p4">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">What are
better layer-pruning strategies?
What are
better approaches to healing?<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>At the cost of introducing another hyperparameter and requiring both pruned and unpruned models to fit in memory during finetuning, one natural way to improve healing
is
by adding
an auxiliary student-teacher loss that explicitly addresses the pruning mismatch (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E5" title="5 ‚Ä£ 3.1 Intuition ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">5</span></a>), such as

<span class="ltx_equation ltx_eqn_table" id="S5.E8">
<span><span class="ltx_equation ltx_eqn_row ltx_align_baseline">
<span class="ltx_eqn_cell ltx_eqn_center_padleft"></span>
<span class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{aux}}\sim\left(x^{(\ell^{*}\!+n)}(\theta_{0})-x^{(\ell^{*})%
}(\theta)\right)^{2}\,," class="ltx_Math" display="block" id="S5.E8.m1.4"><semantics id="S5.E8.m1.4b"><mrow id="S5.E8.m1.4.4.1" xref="S5.E8.m1.4.4.1.1.cmml"><mrow id="S5.E8.m1.4.4.1.1" xref="S5.E8.m1.4.4.1.1.cmml"><msub id="S5.E8.m1.4.4.1.1.3" xref="S5.E8.m1.4.4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E8.m1.4.4.1.1.3.2" xref="S5.E8.m1.4.4.1.1.3.2.cmml">‚Ñí</mi><mtext id="S5.E8.m1.4.4.1.1.3.3" xref="S5.E8.m1.4.4.1.1.3.3a.cmml">aux</mtext></msub><mo id="S5.E8.m1.4.4.1.1.2" xref="S5.E8.m1.4.4.1.1.2.cmml">‚àº</mo><msup id="S5.E8.m1.4.4.1.1.1" xref="S5.E8.m1.4.4.1.1.1.cmml"><mrow id="S5.E8.m1.4.4.1.1.1.1.1" xref="S5.E8.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S5.E8.m1.4.4.1.1.1.1.1.2" xref="S5.E8.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E8.m1.4.4.1.1.1.1.1.1" xref="S5.E8.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S5.E8.m1.4.4.1.1.1.1.1.1.1" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.cmml"><msup id="S5.E8.m1.4.4.1.1.1.1.1.1.1.3" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E8.m1.4.4.1.1.1.1.1.1.1.3.2" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.3.2.cmml">x</mi><mrow id="S5.E8.m1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.cmml"><mo id="S5.E8.m1.1.1.1.1.2" stretchy="false" xref="S5.E8.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E8.m1.1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.cmml"><msup id="S5.E8.m1.1.1.1.1.1.2" xref="S5.E8.m1.1.1.1.1.1.2.cmml"><mi id="S5.E8.m1.1.1.1.1.1.2.2" mathvariant="normal" xref="S5.E8.m1.1.1.1.1.1.2.2.cmml">‚Ñì</mi><mo id="S5.E8.m1.1.1.1.1.1.2.3" xref="S5.E8.m1.1.1.1.1.1.2.3.cmml">*</mo></msup><mo id="S5.E8.m1.1.1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.1.1.cmml">+</mo><mi id="S5.E8.m1.1.1.1.1.1.3" xref="S5.E8.m1.1.1.1.1.1.3.cmml">n</mi></mrow><mo id="S5.E8.m1.1.1.1.1.3" stretchy="false" xref="S5.E8.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S5.E8.m1.4.4.1.1.1.1.1.1.1.2" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml">Œ∏</mi><mn id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S5.E8.m1.4.4.1.1.1.1.1.1.2" xref="S5.E8.m1.4.4.1.1.1.1.1.1.2.cmml">‚àí</mo><mrow id="S5.E8.m1.4.4.1.1.1.1.1.1.3" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.cmml"><msup id="S5.E8.m1.4.4.1.1.1.1.1.1.3.2" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.2.cmml"><mi id="S5.E8.m1.4.4.1.1.1.1.1.1.3.2.2" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.2.2.cmml">x</mi><mrow id="S5.E8.m1.2.2.1.1" xref="S5.E8.m1.2.2.1.1.1.cmml"><mo id="S5.E8.m1.2.2.1.1.2" stretchy="false" xref="S5.E8.m1.2.2.1.1.1.cmml">(</mo><msup id="S5.E8.m1.2.2.1.1.1" xref="S5.E8.m1.2.2.1.1.1.cmml"><mi id="S5.E8.m1.2.2.1.1.1.2" mathvariant="normal" xref="S5.E8.m1.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="S5.E8.m1.2.2.1.1.1.3" xref="S5.E8.m1.2.2.1.1.1.3.cmml">*</mo></msup><mo id="S5.E8.m1.2.2.1.1.3" stretchy="false" xref="S5.E8.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S5.E8.m1.4.4.1.1.1.1.1.1.3.1" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.1.cmml">‚Å¢</mo><mrow id="S5.E8.m1.4.4.1.1.1.1.1.1.3.3.2" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.cmml"><mo id="S5.E8.m1.4.4.1.1.1.1.1.1.3.3.2.1" stretchy="false" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.cmml">(</mo><mi id="S5.E8.m1.3.3" xref="S5.E8.m1.3.3.cmml">Œ∏</mi><mo id="S5.E8.m1.4.4.1.1.1.1.1.1.3.3.2.2" stretchy="false" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S5.E8.m1.4.4.1.1.1.1.1.3" xref="S5.E8.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S5.E8.m1.4.4.1.1.1.3" xref="S5.E8.m1.4.4.1.1.1.3.cmml">2</mn></msup></mrow><mo id="S5.E8.m1.4.4.1.2" xref="S5.E8.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E8.m1.4c"><apply id="S5.E8.m1.4.4.1.1.cmml" xref="S5.E8.m1.4.4.1"><csymbol cd="latexml" id="S5.E8.m1.4.4.1.1.2.cmml" xref="S5.E8.m1.4.4.1.1.2">similar-to</csymbol><apply id="S5.E8.m1.4.4.1.1.3.cmml" xref="S5.E8.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S5.E8.m1.4.4.1.1.3.1.cmml" xref="S5.E8.m1.4.4.1.1.3">subscript</csymbol><ci id="S5.E8.m1.4.4.1.1.3.2.cmml" xref="S5.E8.m1.4.4.1.1.3.2">‚Ñí</ci><ci id="S5.E8.m1.4.4.1.1.3.3a.cmml" xref="S5.E8.m1.4.4.1.1.3.3"><mtext id="S5.E8.m1.4.4.1.1.3.3.cmml" mathsize="70%" xref="S5.E8.m1.4.4.1.1.3.3">aux</mtext></ci></apply><apply id="S5.E8.m1.4.4.1.1.1.cmml" xref="S5.E8.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S5.E8.m1.4.4.1.1.1.2.cmml" xref="S5.E8.m1.4.4.1.1.1">superscript</csymbol><apply id="S5.E8.m1.4.4.1.1.1.1.1.1.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1"><minus id="S5.E8.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.2"></minus><apply id="S5.E8.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1"><times id="S5.E8.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.2"></times><apply id="S5.E8.m1.4.4.1.1.1.1.1.1.1.3.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E8.m1.4.4.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S5.E8.m1.4.4.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.3.2">ùë•</ci><apply id="S5.E8.m1.1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1"><plus id="S5.E8.m1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1.1.1"></plus><apply id="S5.E8.m1.1.1.1.1.1.2.cmml" xref="S5.E8.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E8.m1.1.1.1.1.1.2.1.cmml" xref="S5.E8.m1.1.1.1.1.1.2">superscript</csymbol><ci id="S5.E8.m1.1.1.1.1.1.2.2.cmml" xref="S5.E8.m1.1.1.1.1.1.2.2">‚Ñì</ci><times id="S5.E8.m1.1.1.1.1.1.2.3.cmml" xref="S5.E8.m1.1.1.1.1.1.2.3"></times></apply><ci id="S5.E8.m1.1.1.1.1.1.3.cmml" xref="S5.E8.m1.1.1.1.1.1.3">ùëõ</ci></apply></apply><apply id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.2">ùúÉ</ci><cn id="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S5.E8.m1.4.4.1.1.1.1.1.1.1.1.1.1.3">0</cn></apply></apply><apply id="S5.E8.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3"><times id="S5.E8.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.1"></times><apply id="S5.E8.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S5.E8.m1.4.4.1.1.1.1.1.1.3.2.1.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.2">superscript</csymbol><ci id="S5.E8.m1.4.4.1.1.1.1.1.1.3.2.2.cmml" xref="S5.E8.m1.4.4.1.1.1.1.1.1.3.2.2">ùë•</ci><apply id="S5.E8.m1.2.2.1.1.1.cmml" xref="S5.E8.m1.2.2.1.1"><csymbol cd="ambiguous" id="S5.E8.m1.2.2.1.1.1.1.cmml" xref="S5.E8.m1.2.2.1.1">superscript</csymbol><ci id="S5.E8.m1.2.2.1.1.1.2.cmml" xref="S5.E8.m1.2.2.1.1.1.2">‚Ñì</ci><times id="S5.E8.m1.2.2.1.1.1.3.cmml" xref="S5.E8.m1.2.2.1.1.1.3"></times></apply></apply><ci id="S5.E8.m1.3.3.cmml" xref="S5.E8.m1.3.3">ùúÉ</ci></apply></apply><cn id="S5.E8.m1.4.4.1.1.1.3.cmml" type="integer" xref="S5.E8.m1.4.4.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E8.m1.4d">\mathcal{L}_{\text{aux}}\sim\left(x^{(\ell^{*}\!+n)}(\theta_{0})-x^{(\ell^{*})%
}(\theta)\right)^{2}\,,</annotation><annotation encoding="application/x-llamapun" id="S5.E8.m1.4e">caligraphic_L start_POSTSUBSCRIPT aux end_POSTSUBSCRIPT ‚àº ( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT + italic_n ) end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) - italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT ( italic_Œ∏ ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></span>
<span class="ltx_eqn_cell ltx_eqn_center_padright"></span>
<span class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></span></span></span>
</span>
where <math alttext="\theta_{0}" class="ltx_Math" display="inline" id="footnote9.m1.1"><semantics id="footnote9.m1.1b"><msub id="footnote9.m1.1.1" xref="footnote9.m1.1.1.cmml"><mi id="footnote9.m1.1.1.2" xref="footnote9.m1.1.1.2.cmml">Œ∏</mi><mn id="footnote9.m1.1.1.3" xref="footnote9.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="footnote9.m1.1c"><apply id="footnote9.m1.1.1.cmml" xref="footnote9.m1.1.1"><csymbol cd="ambiguous" id="footnote9.m1.1.1.1.cmml" xref="footnote9.m1.1.1">subscript</csymbol><ci id="footnote9.m1.1.1.2.cmml" xref="footnote9.m1.1.1.2">ùúÉ</ci><cn id="footnote9.m1.1.1.3.cmml" type="integer" xref="footnote9.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote9.m1.1d">\theta_{0}</annotation><annotation encoding="application/x-llamapun" id="footnote9.m1.1e">italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> are the frozen parameters of the unpruned model, and <math alttext="\theta" class="ltx_Math" display="inline" id="footnote9.m2.1"><semantics id="footnote9.m2.1b"><mi id="footnote9.m2.1.1" xref="footnote9.m2.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="footnote9.m2.1c"><ci id="footnote9.m2.1.1.cmml" xref="footnote9.m2.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote9.m2.1d">\theta</annotation><annotation encoding="application/x-llamapun" id="footnote9.m2.1e">italic_Œ∏</annotation></semantics></math> are the parameters of the pruned model to be healed; thus, <math alttext="x^{(\ell^{*}\!+n)}(\theta_{0})" class="ltx_Math" display="inline" id="footnote9.m3.2"><semantics id="footnote9.m3.2b"><mrow id="footnote9.m3.2.2" xref="footnote9.m3.2.2.cmml"><msup id="footnote9.m3.2.2.3" xref="footnote9.m3.2.2.3.cmml"><mi id="footnote9.m3.2.2.3.2" xref="footnote9.m3.2.2.3.2.cmml">x</mi><mrow id="footnote9.m3.1.1.1.1" xref="footnote9.m3.1.1.1.1.1.cmml"><mo id="footnote9.m3.1.1.1.1.2" stretchy="false" xref="footnote9.m3.1.1.1.1.1.cmml">(</mo><mrow id="footnote9.m3.1.1.1.1.1" xref="footnote9.m3.1.1.1.1.1.cmml"><msup id="footnote9.m3.1.1.1.1.1.2" xref="footnote9.m3.1.1.1.1.1.2.cmml"><mi id="footnote9.m3.1.1.1.1.1.2.2" mathvariant="normal" xref="footnote9.m3.1.1.1.1.1.2.2.cmml">‚Ñì</mi><mo id="footnote9.m3.1.1.1.1.1.2.3" xref="footnote9.m3.1.1.1.1.1.2.3.cmml">*</mo></msup><mo id="footnote9.m3.1.1.1.1.1.1" xref="footnote9.m3.1.1.1.1.1.1.cmml">+</mo><mi id="footnote9.m3.1.1.1.1.1.3" xref="footnote9.m3.1.1.1.1.1.3.cmml">n</mi></mrow><mo id="footnote9.m3.1.1.1.1.3" stretchy="false" xref="footnote9.m3.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="footnote9.m3.2.2.2" xref="footnote9.m3.2.2.2.cmml">‚Å¢</mo><mrow id="footnote9.m3.2.2.1.1" xref="footnote9.m3.2.2.1.1.1.cmml"><mo id="footnote9.m3.2.2.1.1.2" stretchy="false" xref="footnote9.m3.2.2.1.1.1.cmml">(</mo><msub id="footnote9.m3.2.2.1.1.1" xref="footnote9.m3.2.2.1.1.1.cmml"><mi id="footnote9.m3.2.2.1.1.1.2" xref="footnote9.m3.2.2.1.1.1.2.cmml">Œ∏</mi><mn id="footnote9.m3.2.2.1.1.1.3" xref="footnote9.m3.2.2.1.1.1.3.cmml">0</mn></msub><mo id="footnote9.m3.2.2.1.1.3" stretchy="false" xref="footnote9.m3.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote9.m3.2c"><apply id="footnote9.m3.2.2.cmml" xref="footnote9.m3.2.2"><times id="footnote9.m3.2.2.2.cmml" xref="footnote9.m3.2.2.2"></times><apply id="footnote9.m3.2.2.3.cmml" xref="footnote9.m3.2.2.3"><csymbol cd="ambiguous" id="footnote9.m3.2.2.3.1.cmml" xref="footnote9.m3.2.2.3">superscript</csymbol><ci id="footnote9.m3.2.2.3.2.cmml" xref="footnote9.m3.2.2.3.2">ùë•</ci><apply id="footnote9.m3.1.1.1.1.1.cmml" xref="footnote9.m3.1.1.1.1"><plus id="footnote9.m3.1.1.1.1.1.1.cmml" xref="footnote9.m3.1.1.1.1.1.1"></plus><apply id="footnote9.m3.1.1.1.1.1.2.cmml" xref="footnote9.m3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="footnote9.m3.1.1.1.1.1.2.1.cmml" xref="footnote9.m3.1.1.1.1.1.2">superscript</csymbol><ci id="footnote9.m3.1.1.1.1.1.2.2.cmml" xref="footnote9.m3.1.1.1.1.1.2.2">‚Ñì</ci><times id="footnote9.m3.1.1.1.1.1.2.3.cmml" xref="footnote9.m3.1.1.1.1.1.2.3"></times></apply><ci id="footnote9.m3.1.1.1.1.1.3.cmml" xref="footnote9.m3.1.1.1.1.1.3">ùëõ</ci></apply></apply><apply id="footnote9.m3.2.2.1.1.1.cmml" xref="footnote9.m3.2.2.1.1"><csymbol cd="ambiguous" id="footnote9.m3.2.2.1.1.1.1.cmml" xref="footnote9.m3.2.2.1.1">subscript</csymbol><ci id="footnote9.m3.2.2.1.1.1.2.cmml" xref="footnote9.m3.2.2.1.1.1.2">ùúÉ</ci><cn id="footnote9.m3.2.2.1.1.1.3.cmml" type="integer" xref="footnote9.m3.2.2.1.1.1.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote9.m3.2d">x^{(\ell^{*}\!+n)}(\theta_{0})</annotation><annotation encoding="application/x-llamapun" id="footnote9.m3.2e">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT + italic_n ) end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )</annotation></semantics></math> is the input to
the <math alttext="(\ell^{*}\!+n)" class="ltx_Math" display="inline" id="footnote9.m4.1"><semantics id="footnote9.m4.1b"><mrow id="footnote9.m4.1.1.1" xref="footnote9.m4.1.1.1.1.cmml"><mo id="footnote9.m4.1.1.1.2" stretchy="false" xref="footnote9.m4.1.1.1.1.cmml">(</mo><mrow id="footnote9.m4.1.1.1.1" xref="footnote9.m4.1.1.1.1.cmml"><msup id="footnote9.m4.1.1.1.1.2" xref="footnote9.m4.1.1.1.1.2.cmml"><mi id="footnote9.m4.1.1.1.1.2.2" mathvariant="normal" xref="footnote9.m4.1.1.1.1.2.2.cmml">‚Ñì</mi><mo id="footnote9.m4.1.1.1.1.2.3" xref="footnote9.m4.1.1.1.1.2.3.cmml">*</mo></msup><mo id="footnote9.m4.1.1.1.1.1" xref="footnote9.m4.1.1.1.1.1.cmml">+</mo><mi id="footnote9.m4.1.1.1.1.3" xref="footnote9.m4.1.1.1.1.3.cmml">n</mi></mrow><mo id="footnote9.m4.1.1.1.3" stretchy="false" xref="footnote9.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="footnote9.m4.1c"><apply id="footnote9.m4.1.1.1.1.cmml" xref="footnote9.m4.1.1.1"><plus id="footnote9.m4.1.1.1.1.1.cmml" xref="footnote9.m4.1.1.1.1.1"></plus><apply id="footnote9.m4.1.1.1.1.2.cmml" xref="footnote9.m4.1.1.1.1.2"><csymbol cd="ambiguous" id="footnote9.m4.1.1.1.1.2.1.cmml" xref="footnote9.m4.1.1.1.1.2">superscript</csymbol><ci id="footnote9.m4.1.1.1.1.2.2.cmml" xref="footnote9.m4.1.1.1.1.2.2">‚Ñì</ci><times id="footnote9.m4.1.1.1.1.2.3.cmml" xref="footnote9.m4.1.1.1.1.2.3"></times></apply><ci id="footnote9.m4.1.1.1.1.3.cmml" xref="footnote9.m4.1.1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote9.m4.1d">(\ell^{*}\!+n)</annotation><annotation encoding="application/x-llamapun" id="footnote9.m4.1e">( roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT + italic_n )</annotation></semantics></math>-th layer
in the unpruned model,
<math alttext="x^{(\ell^{*})}(\theta)" class="ltx_Math" display="inline" id="footnote9.m5.2"><semantics id="footnote9.m5.2b"><mrow id="footnote9.m5.2.3" xref="footnote9.m5.2.3.cmml"><msup id="footnote9.m5.2.3.2" xref="footnote9.m5.2.3.2.cmml"><mi id="footnote9.m5.2.3.2.2" xref="footnote9.m5.2.3.2.2.cmml">x</mi><mrow id="footnote9.m5.1.1.1.1" xref="footnote9.m5.1.1.1.1.1.cmml"><mo id="footnote9.m5.1.1.1.1.2" stretchy="false" xref="footnote9.m5.1.1.1.1.1.cmml">(</mo><msup id="footnote9.m5.1.1.1.1.1" xref="footnote9.m5.1.1.1.1.1.cmml"><mi id="footnote9.m5.1.1.1.1.1.2" mathvariant="normal" xref="footnote9.m5.1.1.1.1.1.2.cmml">‚Ñì</mi><mo id="footnote9.m5.1.1.1.1.1.3" xref="footnote9.m5.1.1.1.1.1.3.cmml">*</mo></msup><mo id="footnote9.m5.1.1.1.1.3" stretchy="false" xref="footnote9.m5.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="footnote9.m5.2.3.1" xref="footnote9.m5.2.3.1.cmml">‚Å¢</mo><mrow id="footnote9.m5.2.3.3.2" xref="footnote9.m5.2.3.cmml"><mo id="footnote9.m5.2.3.3.2.1" stretchy="false" xref="footnote9.m5.2.3.cmml">(</mo><mi id="footnote9.m5.2.2" xref="footnote9.m5.2.2.cmml">Œ∏</mi><mo id="footnote9.m5.2.3.3.2.2" stretchy="false" xref="footnote9.m5.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote9.m5.2c"><apply id="footnote9.m5.2.3.cmml" xref="footnote9.m5.2.3"><times id="footnote9.m5.2.3.1.cmml" xref="footnote9.m5.2.3.1"></times><apply id="footnote9.m5.2.3.2.cmml" xref="footnote9.m5.2.3.2"><csymbol cd="ambiguous" id="footnote9.m5.2.3.2.1.cmml" xref="footnote9.m5.2.3.2">superscript</csymbol><ci id="footnote9.m5.2.3.2.2.cmml" xref="footnote9.m5.2.3.2.2">ùë•</ci><apply id="footnote9.m5.1.1.1.1.1.cmml" xref="footnote9.m5.1.1.1.1"><csymbol cd="ambiguous" id="footnote9.m5.1.1.1.1.1.1.cmml" xref="footnote9.m5.1.1.1.1">superscript</csymbol><ci id="footnote9.m5.1.1.1.1.1.2.cmml" xref="footnote9.m5.1.1.1.1.1.2">‚Ñì</ci><times id="footnote9.m5.1.1.1.1.1.3.cmml" xref="footnote9.m5.1.1.1.1.1.3"></times></apply></apply><ci id="footnote9.m5.2.2.cmml" xref="footnote9.m5.2.2">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote9.m5.2d">x^{(\ell^{*})}(\theta)</annotation><annotation encoding="application/x-llamapun" id="footnote9.m5.2e">italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT ( italic_Œ∏ )</annotation></semantics></math> is the input to that same layer after pruning,
and <math alttext="\mathcal{L}_{\text{aux}}" class="ltx_Math" display="inline" id="footnote9.m6.1"><semantics id="footnote9.m6.1b"><msub id="footnote9.m6.1.1" xref="footnote9.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="footnote9.m6.1.1.2" xref="footnote9.m6.1.1.2.cmml">‚Ñí</mi><mtext id="footnote9.m6.1.1.3" xref="footnote9.m6.1.1.3a.cmml">aux</mtext></msub><annotation-xml encoding="MathML-Content" id="footnote9.m6.1c"><apply id="footnote9.m6.1.1.cmml" xref="footnote9.m6.1.1"><csymbol cd="ambiguous" id="footnote9.m6.1.1.1.cmml" xref="footnote9.m6.1.1">subscript</csymbol><ci id="footnote9.m6.1.1.2.cmml" xref="footnote9.m6.1.1.2">‚Ñí</ci><ci id="footnote9.m6.1.1.3a.cmml" xref="footnote9.m6.1.1.3"><mtext id="footnote9.m6.1.1.3.cmml" mathsize="70%" xref="footnote9.m6.1.1.3">aux</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote9.m6.1d">\mathcal{L}_{\text{aux}}</annotation><annotation encoding="application/x-llamapun" id="footnote9.m6.1e">caligraphic_L start_POSTSUBSCRIPT aux end_POSTSUBSCRIPT</annotation></semantics></math> minimizes their mismatch. We thank Sho Yaida for
this observation.
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">Why does healing
eliminate the phase transition in the loss but not in the QA accuracies?</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">With more comprehensive evals, will accuracy on different tasks degrade at different depths?
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">Relatedly, is knowledge generally
stored in
shallow or middle layers,
or is it delocalized?</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">Do pretraining details
affect
the
ability to prune, e.g., are scaling-law
over-trained or distilled models more difficult to prune?</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.1">How can we enable
LLMs
to more effectively use the parameters in their deepest layers?</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.p4.1">Some of these questions would benefit from studying both layer similarity and pruning across different pretraining checkpoints; for instance, at what point does the sharp phase transition and critical depth in the QA accuracies emerge,
and does more training lead to better use of the prunable parameters?
Others
suggest explorations with different pretraining architectures and objectives, e.g. in order
better make use of the deeper layers.
With more comprehensive evaluations, if different kinds of tasks degrade at very different depths, then
this
might indicate that
the knowledge required to complete those tasks
is
stored
at different depths.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Alternatively, one could measure <math alttext="d(x^{(\ell)},x^{(\ell+n)})" class="ltx_Math" display="inline" id="footnote10.m1.4"><semantics id="footnote10.m1.4b"><mrow id="footnote10.m1.4.4" xref="footnote10.m1.4.4.cmml"><mi id="footnote10.m1.4.4.4" xref="footnote10.m1.4.4.4.cmml">d</mi><mo id="footnote10.m1.4.4.3" xref="footnote10.m1.4.4.3.cmml">‚Å¢</mo><mrow id="footnote10.m1.4.4.2.2" xref="footnote10.m1.4.4.2.3.cmml"><mo id="footnote10.m1.4.4.2.2.3" stretchy="false" xref="footnote10.m1.4.4.2.3.cmml">(</mo><msup id="footnote10.m1.3.3.1.1.1" xref="footnote10.m1.3.3.1.1.1.cmml"><mi id="footnote10.m1.3.3.1.1.1.2" xref="footnote10.m1.3.3.1.1.1.2.cmml">x</mi><mrow id="footnote10.m1.1.1.1.3" xref="footnote10.m1.3.3.1.1.1.cmml"><mo id="footnote10.m1.1.1.1.3.1" stretchy="false" xref="footnote10.m1.3.3.1.1.1.cmml">(</mo><mi id="footnote10.m1.1.1.1.1" mathvariant="normal" xref="footnote10.m1.1.1.1.1.cmml">‚Ñì</mi><mo id="footnote10.m1.1.1.1.3.2" stretchy="false" xref="footnote10.m1.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="footnote10.m1.4.4.2.2.4" xref="footnote10.m1.4.4.2.3.cmml">,</mo><msup id="footnote10.m1.4.4.2.2.2" xref="footnote10.m1.4.4.2.2.2.cmml"><mi id="footnote10.m1.4.4.2.2.2.2" xref="footnote10.m1.4.4.2.2.2.2.cmml">x</mi><mrow id="footnote10.m1.2.2.1.1" xref="footnote10.m1.2.2.1.1.1.cmml"><mo id="footnote10.m1.2.2.1.1.2" stretchy="false" xref="footnote10.m1.2.2.1.1.1.cmml">(</mo><mrow id="footnote10.m1.2.2.1.1.1" xref="footnote10.m1.2.2.1.1.1.cmml"><mi id="footnote10.m1.2.2.1.1.1.2" mathvariant="normal" xref="footnote10.m1.2.2.1.1.1.2.cmml">‚Ñì</mi><mo id="footnote10.m1.2.2.1.1.1.1" xref="footnote10.m1.2.2.1.1.1.1.cmml">+</mo><mi id="footnote10.m1.2.2.1.1.1.3" xref="footnote10.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="footnote10.m1.2.2.1.1.3" stretchy="false" xref="footnote10.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="footnote10.m1.4.4.2.2.5" stretchy="false" xref="footnote10.m1.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote10.m1.4c"><apply id="footnote10.m1.4.4.cmml" xref="footnote10.m1.4.4"><times id="footnote10.m1.4.4.3.cmml" xref="footnote10.m1.4.4.3"></times><ci id="footnote10.m1.4.4.4.cmml" xref="footnote10.m1.4.4.4">ùëë</ci><interval closure="open" id="footnote10.m1.4.4.2.3.cmml" xref="footnote10.m1.4.4.2.2"><apply id="footnote10.m1.3.3.1.1.1.cmml" xref="footnote10.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="footnote10.m1.3.3.1.1.1.1.cmml" xref="footnote10.m1.3.3.1.1.1">superscript</csymbol><ci id="footnote10.m1.3.3.1.1.1.2.cmml" xref="footnote10.m1.3.3.1.1.1.2">ùë•</ci><ci id="footnote10.m1.1.1.1.1.cmml" xref="footnote10.m1.1.1.1.1">‚Ñì</ci></apply><apply id="footnote10.m1.4.4.2.2.2.cmml" xref="footnote10.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="footnote10.m1.4.4.2.2.2.1.cmml" xref="footnote10.m1.4.4.2.2.2">superscript</csymbol><ci id="footnote10.m1.4.4.2.2.2.2.cmml" xref="footnote10.m1.4.4.2.2.2.2">ùë•</ci><apply id="footnote10.m1.2.2.1.1.1.cmml" xref="footnote10.m1.2.2.1.1"><plus id="footnote10.m1.2.2.1.1.1.1.cmml" xref="footnote10.m1.2.2.1.1.1.1"></plus><ci id="footnote10.m1.2.2.1.1.1.2.cmml" xref="footnote10.m1.2.2.1.1.1.2">‚Ñì</ci><ci id="footnote10.m1.2.2.1.1.1.3.cmml" xref="footnote10.m1.2.2.1.1.1.3">ùëõ</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote10.m1.4d">d(x^{(\ell)},x^{(\ell+n)})</annotation><annotation encoding="application/x-llamapun" id="footnote10.m1.4e">italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_‚Ñì + italic_n ) end_POSTSUPERSCRIPT )</annotation></semantics></math> or find
<math alttext="\ell^{*}(n)" class="ltx_Math" display="inline" id="footnote10.m2.1"><semantics id="footnote10.m2.1b"><mrow id="footnote10.m2.1.2" xref="footnote10.m2.1.2.cmml"><msup id="footnote10.m2.1.2.2" xref="footnote10.m2.1.2.2.cmml"><mi id="footnote10.m2.1.2.2.2" mathvariant="normal" xref="footnote10.m2.1.2.2.2.cmml">‚Ñì</mi><mo id="footnote10.m2.1.2.2.3" xref="footnote10.m2.1.2.2.3.cmml">*</mo></msup><mo id="footnote10.m2.1.2.1" xref="footnote10.m2.1.2.1.cmml">‚Å¢</mo><mrow id="footnote10.m2.1.2.3.2" xref="footnote10.m2.1.2.cmml"><mo id="footnote10.m2.1.2.3.2.1" stretchy="false" xref="footnote10.m2.1.2.cmml">(</mo><mi id="footnote10.m2.1.1" xref="footnote10.m2.1.1.cmml">n</mi><mo id="footnote10.m2.1.2.3.2.2" stretchy="false" xref="footnote10.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote10.m2.1c"><apply id="footnote10.m2.1.2.cmml" xref="footnote10.m2.1.2"><times id="footnote10.m2.1.2.1.cmml" xref="footnote10.m2.1.2.1"></times><apply id="footnote10.m2.1.2.2.cmml" xref="footnote10.m2.1.2.2"><csymbol cd="ambiguous" id="footnote10.m2.1.2.2.1.cmml" xref="footnote10.m2.1.2.2">superscript</csymbol><ci id="footnote10.m2.1.2.2.2.cmml" xref="footnote10.m2.1.2.2.2">‚Ñì</ci><times id="footnote10.m2.1.2.2.3.cmml" xref="footnote10.m2.1.2.2.3"></times></apply><ci id="footnote10.m2.1.1.cmml" xref="footnote10.m2.1.1">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote10.m2.1d">\ell^{*}(n)</annotation><annotation encoding="application/x-llamapun" id="footnote10.m2.1e">roman_‚Ñì start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math> as a function of
different eval datasets. </span></span></span>
It would be very interesting to use pruning to systematically study these kind of interpretability questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments and Disclosure of Funding</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.3">We thank Aaron Schwartz for his initial collaboration,
Aaditya Singh
and
Sho Yaida for discussions,
and
Aaditya Singh
for comments on the draft.
We would also like to acknowledge the 2023 NeurIPS Large Language Model Efficiency Challenge
for initializing us
for
work on this project.
A.G. is supported by the NSF CAREER grant DMR-2045181, the Sloan Foundation, and
by the Laboratory for Physical Sciences through the Condensed Matter Theory Center.
D.R. acknowledges support from the National Science Foundation under Cooperative Agreement PHY-2019786 (the NSF AI Institute for Artificial Intelligence
and Fundamental Interactions, <span class="ltx_text ltx_font_typewriter" id="Sx1.p1.3.1">http://iaifi.org/</span>) and appreciates both the sanction and support of Sequoia Capital.
This paper has been
brought to you residually by the letters <math alttext="G" class="ltx_Math" display="inline" id="Sx1.p1.1.m1.1"><semantics id="Sx1.p1.1.m1.1a"><mi id="Sx1.p1.1.m1.1.1" xref="Sx1.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="Sx1.p1.1.m1.1b"><ci id="Sx1.p1.1.m1.1.1.cmml" xref="Sx1.p1.1.m1.1.1">ùê∫</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="Sx1.p1.1.m1.1d">italic_G</annotation></semantics></math>, <math alttext="P" class="ltx_Math" display="inline" id="Sx1.p1.2.m2.1"><semantics id="Sx1.p1.2.m2.1a"><mi id="Sx1.p1.2.m2.1.1" xref="Sx1.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="Sx1.p1.2.m2.1b"><ci id="Sx1.p1.2.m2.1.1.cmml" xref="Sx1.p1.2.m2.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="Sx1.p1.2.m2.1d">italic_P</annotation></semantics></math>, and <math alttext="U" class="ltx_Math" display="inline" id="Sx1.p1.3.m3.1"><semantics id="Sx1.p1.3.m3.1a"><mi id="Sx1.p1.3.m3.1.1" xref="Sx1.p1.3.m3.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="Sx1.p1.3.m3.1b"><ci id="Sx1.p1.3.m3.1.1.cmml" xref="Sx1.p1.3.m3.1.1">ùëà</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.3.m3.1c">U</annotation><annotation encoding="application/x-llamapun" id="Sx1.p1.3.m3.1d">italic_U</annotation></semantics></math>, after summing over many layers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" title="">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing chatgpt, Nov 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini Team et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De&nbsp;Vries (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Harm De&nbsp;Vries.

</span>
<span class="ltx_bibblock">Go smol or go home, July 2023.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.harmdevries.com/post/model-size-vs-compute-overhead/" title="">https://www.harmdevries.com/post/model-size-vs-compute-overhead/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sardana and Frankle (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Nikhil Sardana and Jonathan Frankle.

</span>
<span class="ltx_bibblock">Beyond chinchilla-optimal: Accounting for inference in language model
scaling laws.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2401.00448</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">Gptq: Accurate post-training quantization for generative pre-trained
transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers and Zettlemoyer (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Tim Dettmers and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">The case for 4-bit precision: k-bit inference scaling laws.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">International Conference on Machine Learning</em>, pages
7750‚Äì7774. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Guangxuan Xiao, Ji&nbsp;Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.

</span>
<span class="ltx_bibblock">Smoothquant: Accurate and efficient post-training quantization for
large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International Conference on Machine Learning</em>, pages
38087‚Äì38099. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Edward&nbsp;J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu&nbsp;Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et&nbsp;al. (1989)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yann LeCun, John Denker, and Sara Solla.

</span>
<span class="ltx_bibblock">Optimal brain damage.

</span>
<span class="ltx_bibblock">In D.&nbsp;Touretzky, editor, <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in Neural Information
Processing Systems</em>, volume&nbsp;2. Morgan-Kaufmann, 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassibi and Stork (1992)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Babak Hassibi and David Stork.

</span>
<span class="ltx_bibblock">Second order derivatives for network pruning: Optimal brain surgeon.

</span>
<span class="ltx_bibblock">In S.&nbsp;Hanson, J.&nbsp;Cowan, and C.&nbsp;Giles, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in
Neural Information Processing Systems</em>, volume&nbsp;5. Morgan-Kaufmann, 1992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Song Han, Jeff Pool, John Tran, and William Dally.

</span>
<span class="ltx_bibblock">Learning both weights and connections for efficient neural network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Advances in neural information processing systems</em>, 28, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans&nbsp;Peter Graf.

</span>
<span class="ltx_bibblock">Pruning filters for efficient convnets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1608.08710</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankle and Carbin (2018)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jonathan Frankle and Michael Carbin.

</span>
<span class="ltx_bibblock">The lottery ticket hypothesis: Finding sparse, trainable neural
networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1803.03635</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2305.14314</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2307.09288</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">nostalgebraist (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
nostalgebraist.

</span>
<span class="ltx_bibblock">interpreting gpt: the logit lens.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" title="">https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens</a>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belrose et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev
McKinney, Stella Biderman, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Eliciting latent predictions from transformers with the tuned lens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2303.08112</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Ricky&nbsp;TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David&nbsp;K Duvenaud.

</span>
<span class="ltx_bibblock">Neural ordinary differential equations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Advances in neural information processing systems</em>, 31, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou.

</span>
<span class="ltx_bibblock">Tensor programs vi: Feature learning in infinite-depth neural
networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2310.02244</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Men et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei
Han, and Weipeng Chen.

</span>
<span class="ltx_bibblock">Shortgpt: Layers in large language models are more redundant than you
expect.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2403.03853</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.

</span>
<span class="ltx_bibblock">Compressing neural networks with the hashing trick.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">International conference on machine learning</em>, pages
2285‚Äì2294. PMLR, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srinivas and Babu (2015)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Suraj Srinivas and R&nbsp;Venkatesh Babu.

</span>
<span class="ltx_bibblock">Data-free parameter pruning for deep neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:1507.06149</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.

</span>
<span class="ltx_bibblock">Learning structured sparsity in deep neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Advances in neural information processing systems</em>, 29, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.

</span>
<span class="ltx_bibblock">Network trimming: A data-driven neuron pruning approach towards
efficient deep architectures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:1607.03250</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yihui He, Xiangyu Zhang, and Jian Sun.

</span>
<span class="ltx_bibblock">Channel pruning for accelerating very deep neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE international conference on computer
vision</em>, pages 1389‚Äì1397, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Gao Huang, Shichen Liu, Laurens Van&nbsp;der Maaten, and Kilian&nbsp;Q Weinberger.

</span>
<span class="ltx_bibblock">Condensenet: An efficient densenet using learned group convolutions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 2752‚Äì2761, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murray and Chiang (2015)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Kenton Murray and David Chiang.

</span>
<span class="ltx_bibblock">Auto-sizing neural networks: With applications to n-gram language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:1508.05051</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">See et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Abigail See, Minh-Thang Luong, and Christopher&nbsp;D Manning.

</span>
<span class="ltx_bibblock">Compression of neural machine translation models via pruning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1606.09274</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Rush (2016)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yoon Kim and Alexander&nbsp;M Rush.

</span>
<span class="ltx_bibblock">Sequence-level knowledge distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:1606.07947</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.

</span>
<span class="ltx_bibblock">Analyzing multi-head self-attention: Specialized heads do the heavy
lifting, the rest can be pruned.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:1905.09418</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michel et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Paul Michel, Omer Levy, and Graham Neubig.

</span>
<span class="ltx_bibblock">Are sixteen heads really better than one?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Awadalla (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Young&nbsp;Jin Kim and Hany&nbsp;Hassan Awadalla.

</span>
<span class="ltx_bibblock">Fastformers: Highly efficient transformer models for natural language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2010.13382</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Angela Fan, Edouard Grave, and Armand Joulin.

</span>
<span class="ltx_bibblock">Reducing transformer depth on demand with structured dropout.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:1909.11556</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and He (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Minjia Zhang and Yuxiong He.

</span>
<span class="ltx_bibblock">Accelerating training of transformer-based language models with
progressive layer dropping.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in Neural Information Processing Systems</em>,
33:14011‚Äì14023, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Chun Fan, Jiwei Li, Xiang Ao, Fei Wu, Yuxian Meng, and Xiaofei Sun.

</span>
<span class="ltx_bibblock">Layer-wise model pruning based on mutual information.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2108.12594</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jha et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Ananya&nbsp;Harsh Jha, Dirk Groeneveld, Emma Strubell, and Iz&nbsp;Beltagy.

</span>
<span class="ltx_bibblock">Large language model distillation doesn‚Äôt need a teacher.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2305.14864</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sajjad et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov.

</span>
<span class="ltx_bibblock">On the effect of dropping layers of pre-trained transformer models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Computer Speech &amp; Language</em>, 77:101429, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Wei Liu, Zhiyuan Peng, and Tan Lee.

</span>
<span class="ltx_bibblock">Comflp: Correlation measure based fast search on asr layer pruning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2309.11768</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Lu&nbsp;Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu.

</span>
<span class="ltx_bibblock">Dynabert: Dynamic bert with adaptive width and depth.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Advances in Neural Information Processing Systems</em>,
33:9782‚Äì9793, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Pratyusha Sharma, Jordan&nbsp;T Ash, and Dipendra Misra.

</span>
<span class="ltx_bibblock">The truth is in there: Improving reasoning in language models with
layer-selective rank reduction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2312.13558</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashkboos et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Saleh Ashkboos, Maximilian&nbsp;L. Croci, Marcelo Gennari&nbsp;do Nascimento, Torsten
Hoefler, and James Hensman.

</span>
<span class="ltx_bibblock">Slicegpt: Compress large language models by deleting rows and
columns.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2401.15024</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Mengzhou Xia, Zexuan Zhong, and Danqi Chen.

</span>
<span class="ltx_bibblock">Structured pruning learns compact and accurate models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2204.00408</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lagunas et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Fran√ßois Lagunas, Ella Charlaix, Victor Sanh, and Alexander&nbsp;M Rush.

</span>
<span class="ltx_bibblock">Block pruning for faster transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2109.04838</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Qihuang Zhong, Liang Ding, Juhua Liu, Bo&nbsp;Du, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Can chatgpt understand too? a comparative study on chatgpt and
fine-tuned bert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2302.10198</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Kawin Ethayarajh.

</span>
<span class="ltx_bibblock">How contextual are contextualized word representations? comparing the
geometry of bert, elmo, and gpt-2 embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:1909.00512</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech
representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Advances in neural information processing systems</em>,
33:12449‚Äì12460, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:1503.02531</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yuxian Gu, Li&nbsp;Dong, Furu Wei, and Minlie Huang.

</span>
<span class="ltx_bibblock">Knowledge distillation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2306.08543</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
Wang, and Qun Liu.

</span>
<span class="ltx_bibblock">Tinybert: Distilling bert for natural language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:1909.10351</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng.

</span>
<span class="ltx_bibblock">Want to reduce labeling cost? gpt-3 can help.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2108.13487</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eldan and Li (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Ronen Eldan and Yuanzhi Li.

</span>
<span class="ltx_bibblock">Tinystories: How small can language models be and still speak
coherent english?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2305.07759</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del&nbsp;Giorno, Suriya
Gunasekar, and Yin&nbsp;Tat Lee.

</span>
<span class="ltx_bibblock">Textbooks are all you need ii: phi-1.5 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2309.05463</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi&nbsp;Zhang, Jyoti Aneja, Caio C√©sar&nbsp;Teodoro Mendes, Allie
Del&nbsp;Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
de&nbsp;Rosa, Olli Saarikivi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Textbooks are all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2306.11644</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot.

</span>
<span class="ltx_bibblock">Specializing smaller language models towards multi-step reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2301.12726</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii,
Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.

</span>
<span class="ltx_bibblock">Distilling step-by-step! outperforming larger language models with
less training data and smaller model sizes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2305.02301</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang.

</span>
<span class="ltx_bibblock">Lion: Adversarial distillation of closed-source large language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2305.12870</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu
Chen, and Tuo Zhao.

</span>
<span class="ltx_bibblock">Loftq: Lora-fine-tuning-aware quantization for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2310.08659</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu&nbsp;Cheng, Weizhu
Chen, and Tuo Zhao.

</span>
<span class="ltx_bibblock">Adaptive budget allocation for parameter-efficient fine-tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2303.10512</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leviathan et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Yaniv Leviathan, Matan Kalman, and Yossi Matias.

</span>
<span class="ltx_bibblock">Fast inference from transformers via speculative decoding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">International Conference on Machine Learning</em>, pages
19274‚Äì19286. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason&nbsp;D Lee, Deming Chen,
and Tri Dao.

</span>
<span class="ltx_bibblock">Medusa: Simple llm inference acceleration framework with multiple
decoding heads.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2401.10774</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.

</span>
<span class="ltx_bibblock">Locating and editing factual associations in gpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Advances in Neural Information Processing Systems</em>,
35:17359‚Äì17372, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Damai Dai, Li&nbsp;Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.

</span>
<span class="ltx_bibblock">Knowledge neurons in pretrained transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2104.08696</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hase et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun.

</span>
<span class="ltx_bibblock">Does localization inform editing? surprising differences in
causality-based localization vs. knowledge editing in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2301.04213</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.

</span>
<span class="ltx_bibblock">Dissecting recall of factual associations in auto-regressive language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2304.14767</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Din et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Alexander&nbsp;Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva.

</span>
<span class="ltx_bibblock">Jump to conclusions: Short-cutting transformers with linear
transformations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2303.09435</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurnee and Tegmark (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Wes Gurnee and Max Tegmark.

</span>
<span class="ltx_bibblock">Language models represent space and time.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2310.02207</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Elena Voita, Javier Ferrando, and Christoforos Nalmpantis.

</span>
<span class="ltx_bibblock">Neurons in large language models: Dead, n-gram, positional.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2309.04827</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali
Shrivastava, Ce&nbsp;Zhang, Yuandong Tian, Christopher Re, et&nbsp;al.

</span>
<span class="ltx_bibblock">Deja vu: Contextual sparsity for efficient llms at inference time.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">International Conference on Machine Learning</em>, pages
22137‚Äì22176. PMLR, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panigrahi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora.

</span>
<span class="ltx_bibblock">Task-specific skill localization in fine-tuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">arXiv preprint arXiv:2302.06600</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2310.06825</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Javaheripi and Bubeck (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Mojan Javaheripi and S√©bastien Bubeck.

</span>
<span class="ltx_bibblock">Phi-2: The surprising power of small language models, Dec 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">The Journal of Machine Learning Research</em>, 21(1):5485‚Äì5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">arXiv preprint arXiv:2009.03300</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">arXiv preprint arXiv:1905.10044</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.

</span>
<span class="ltx_bibblock">Are emergent abilities of large language models a mirage?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2304.15004</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric
Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2302.13971</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven&nbsp;Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander&nbsp;M. Rush.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 38‚Äì45, Online,
October 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/2020.emnlp-demos.6" title="">https://www.aclweb.org/anthology/2020.emnlp-demos.6</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">arXiv e-prints</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangrulkar et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul,
and Benjamin Bossan.

</span>
<span class="ltx_bibblock">Peft: State-of-the-art parameter-efficient fine-tuning methods.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/peft" title="">https://github.com/huggingface/peft</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Ariel&nbsp;N Lee, Cole&nbsp;J Hunter, and Nataniel Ruiz.

</span>
<span class="ltx_bibblock">Platypus: Quick, cheap, and powerful refinement of llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2308.07317</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experimental Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Here
we explain various
details
of
models
and healing
(¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.1</span></a>) and of evaluations (¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS2" title="A.2 Evaluation details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.2</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Model and healing details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">All models in this paper were fine-tuned using the Hugging Face <span class="ltx_text ltx_font_typewriter" id="A1.SS1.p1.1.1">Trainer API</span> <cite class="ltx_cite ltx_citemacro_citep">(Wolf et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib85" title="">2020</a>)</cite>.
A list of models and their paths on Hugging Face
are as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.SS1.p1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS1.p1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.SS1.p1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p1.2.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p1.2.1.1.2">Repository Path</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS1.p1.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.p1.2.2.1.1">Llama-2 7B</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.p1.2.2.1.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.2.1.2.1">meta-llama/Llama-2-7b-hf</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.3.2.1">Llama-2 13B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.3.2.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.3.2.2.1">meta-llama/Llama-2-13b-hf</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.4.3.1">Llama-2 70B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.4.3.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.4.3.2.1">meta-llama/Llama-2-70b-hf</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.5.4.1">Mistral 7B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.5.4.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.5.4.2.1">mistralai/Mistral-7B-v0.1</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.6.5.1">Phi-2 (2.7B)</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.6.5.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.6.5.2.1">microsoft/phi-2</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.7.6.1">Qwen 7B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.7.6.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.7.6.2.1">Qwen/Qwen-7B</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.8.7.1">Qwen 14B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.8.7.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.8.7.2.1">Qwen/Qwen-14B</code></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">For healing,
we
used
the version of the Colossal Clean Crawled Corpus (C4) <cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib86" title="">2019</a>)</cite> from Hugging Face: <code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p2.1.1">data = load_dataset("c4", ‚Äôen‚Äô)</code>.
We truncated long examples as described
later in the
paragraph and added special tokens when available.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>N.B. the Qwen tokenizer from Hugging Face does not include any special tokens; in this case, it was essential to add a default padding token.
</span></span></span>
Models were finetuned
for 5000 steps with
a global batch size of 16: this corresponds to total finetuning tokens of <math alttext="16\times 5000\times[\text{{max\_seq\_length}}]" class="ltx_Math" display="inline" id="A1.SS1.p2.1.m1.1"><semantics id="A1.SS1.p2.1.m1.1a"><mrow id="A1.SS1.p2.1.m1.1.2" xref="A1.SS1.p2.1.m1.1.2.cmml"><mn id="A1.SS1.p2.1.m1.1.2.2" xref="A1.SS1.p2.1.m1.1.2.2.cmml">16</mn><mo id="A1.SS1.p2.1.m1.1.2.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p2.1.m1.1.2.1.cmml">√ó</mo><mn id="A1.SS1.p2.1.m1.1.2.3" xref="A1.SS1.p2.1.m1.1.2.3.cmml">5000</mn><mo id="A1.SS1.p2.1.m1.1.2.1a" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p2.1.m1.1.2.1.cmml">√ó</mo><mrow id="A1.SS1.p2.1.m1.1.2.4.2" xref="A1.SS1.p2.1.m1.1.2.4.1.cmml"><mo id="A1.SS1.p2.1.m1.1.2.4.2.1" stretchy="false" xref="A1.SS1.p2.1.m1.1.2.4.1.1.cmml">[</mo><mtext id="A1.SS1.p2.1.m1.1.1" mathvariant="monospace" xref="A1.SS1.p2.1.m1.1.1a.cmml">max_seq_length</mtext><mo id="A1.SS1.p2.1.m1.1.2.4.2.2" stretchy="false" xref="A1.SS1.p2.1.m1.1.2.4.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p2.1.m1.1b"><apply id="A1.SS1.p2.1.m1.1.2.cmml" xref="A1.SS1.p2.1.m1.1.2"><times id="A1.SS1.p2.1.m1.1.2.1.cmml" xref="A1.SS1.p2.1.m1.1.2.1"></times><cn id="A1.SS1.p2.1.m1.1.2.2.cmml" type="integer" xref="A1.SS1.p2.1.m1.1.2.2">16</cn><cn id="A1.SS1.p2.1.m1.1.2.3.cmml" type="integer" xref="A1.SS1.p2.1.m1.1.2.3">5000</cn><apply id="A1.SS1.p2.1.m1.1.2.4.1.cmml" xref="A1.SS1.p2.1.m1.1.2.4.2"><csymbol cd="latexml" id="A1.SS1.p2.1.m1.1.2.4.1.1.cmml" xref="A1.SS1.p2.1.m1.1.2.4.2.1">delimited-[]</csymbol><ci id="A1.SS1.p2.1.m1.1.1a.cmml" xref="A1.SS1.p2.1.m1.1.1"><mtext id="A1.SS1.p2.1.m1.1.1.cmml" mathvariant="monospace" xref="A1.SS1.p2.1.m1.1.1">max_seq_length</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p2.1.m1.1c">16\times 5000\times[\text{{max\_seq\_length}}]</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p2.1.m1.1d">16 √ó 5000 √ó [ max_seq_length ]</annotation></semantics></math> for each model.
We used
a cosine-annealed learning rate schedule, with a warmup of 100 steps.
When possible, the peak learning rate was
set to the peak learning rate from the model‚Äôs pretraining;
in practice,
this
means all models were
trained with a peak LR of 3e-4, with the exceptions of Phi-2 <cite class="ltx_cite ltx_citemacro_cite">Javaheripi and Bubeck (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib79" title="">2023</a>)</cite>, which was trained with a peak LR of 2e-4 during pre-training, Llama-2-70B, which
was trained with a peak LR of 3e-5 (a value that resulted from a sweep), and Mistral-7B which was trained with a peak LR of 3e-6 (also a value that resulted from a sweep).
All models 7B parameters or smaller
were trained with a max sequence length of 2048 tokens, while
all models
13B parameters or greater
were trained with a max sequence length of 4096 tokens.
While we realize that some models may have been pretrained on longer
sequences, e.g. Qwen<em class="ltx_emph ltx_font_italic" id="A1.SS1.p2.1.2">-the-outlier</em> <cite class="ltx_cite ltx_citemacro_citep">(Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib77" title="">2023</a>)</cite>,
we decided to
the max sequence length consistent across models of similar size to allow fairer comparisons across model families.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1">On top of the
Hugging Face Trainer API,
we used quantization and Low-Rank Adapters (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib13" title="">2021</a>)</cite> for all of our finetuning:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">For quantization, we used the <span class="ltx_text ltx_font_typewriter" id="A1.I1.i1.p1.1.1">bitsandbytes</span> library for QLoRA&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Dettmers et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>
to
quantize our models to 4 bits.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">For LoRA, we used the Hugging Face <span class="ltx_text ltx_font_typewriter" id="A1.I1.i2.p1.1.1">peft</span> library <cite class="ltx_cite ltx_citemacro_citep">(Mangrulkar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib87" title="">2022</a>)</cite>. We set the LoRA dropout to 0.05 and kept the LoRA <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.I1.i2.p1.1.m1.1"><semantics id="A1.I1.i2.p1.1.m1.1a"><mi id="A1.I1.i2.p1.1.m1.1.1" xref="A1.I1.i2.p1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.1.m1.1b"><ci id="A1.I1.i2.p1.1.m1.1.1.cmml" xref="A1.I1.i2.p1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i2.p1.1.m1.1d">italic_Œ±</annotation></semantics></math> equivalent to the LoRA rank, following <cite class="ltx_cite ltx_citemacro_citep">(Lee et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib88" title="">2023</a>)</cite>. Aside from two exceptions,
discussed below, models are trained with LoRA rank 64.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">Also following Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib88" title="">2023</a>)</cite>, we only applied LoRA to FFN modules: <code class="ltx_verbatim ltx_font_typewriter" id="A1.I1.i3.p1.1.1">["gate_proj", "down_proj", "up_proj"]</code> for Llama-2 and Mistral models, <code class="ltx_verbatim ltx_font_typewriter" id="A1.I1.i3.p1.1.2">["fc1", "fc2"]</code> for Phi-2, and <code class="ltx_verbatim ltx_font_typewriter" id="A1.I1.i3.p1.1.3">["w1", "w2", "c_proj"]</code> for Qwen models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS1.p4">
<p class="ltx_p" id="A1.SS1.p4.2">The large majority of these hyperparameter choices are standard and found in previous works, e.g. Refs.&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lee et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib88" title="">2023</a>; Dettmers et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib9" title="">2022</a>)</cite>. For absolute clarity, we list display all the model specific architecture and healing details below:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.SS1.p4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS1.p4.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.SS1.p4.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p4.3.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.2"># Layers</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.3">Vocab Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.4">Max Seq. Len.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.5">FT Tokens</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.6">Peak LR</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.7">LoRA Rank</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS1.p4.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.SS1.p4.3.2.1.1">Llama-2 7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS1.p4.3.2.1.2">32</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.SS1.p4.3.2.1.3">32,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS1.p4.3.2.1.4">2048</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS1.p4.3.2.1.5">164M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS1.p4.3.2.1.6">3e-4</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.SS1.p4.3.2.1.7">2</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.3.2.1">Llama-2 13B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.3.2.2">40</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.3.2.3">32,000</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.3.2.4">4096</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.3.2.5">328M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.3.2.6">3e-4</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.3.2.7">64</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.4.3.1">Llama-2 70B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.4.3.2">80</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.4.3.3">32,000</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.4.3.4">4096</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.4.3.5">328M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.4.3.6">3e-5</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.4.3.7">8</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.5.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.5.4.1">Qwen 7B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.5.4.2">32</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.5.4.3">151,936</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.5.4.4">2048</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.5.4.5">164M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.5.4.6">3e-4</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.5.4.7">64</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.6.5.1">Qwen 14B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.6.5.2">40</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.6.5.3">151,936</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.6.5.4">4096</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.6.5.5">328M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.6.5.6">3e-4</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.6.5.7">64</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.7.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.7.6.1">Mistral 7B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.7.6.2">32</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.7.6.3">32,000</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.7.6.4">2048</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.7.6.5">164M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.7.6.6">3e-6</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.7.6.7">4</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.8.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.8.7.1">Phi-2 2.7B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.8.7.2">32</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.8.7.3">51,200</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.8.7.4">2048</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.8.7.5">164M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.8.7.6">2e-4</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.8.7.7">64</td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="A1.SS1.p4.4">We also have the following hyperparameters common between all models:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.SS1.p4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS1.p4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.SS1.p4.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p4.1.2.1.1.1">Config</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.SS1.p4.1.2.1.2">Value</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.p4.1.3.2.1">Finetuning dataset</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.p4.1.3.2.2">C4</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.4.3.1">Batch size</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.4.3.2">16</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.1.1">LoRA <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.SS1.p4.1.1.1.m1.1"><semantics id="A1.SS1.p4.1.1.1.m1.1a"><mi id="A1.SS1.p4.1.1.1.m1.1.1" xref="A1.SS1.p4.1.1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p4.1.1.1.m1.1b"><ci id="A1.SS1.p4.1.1.1.m1.1.1.cmml" xref="A1.SS1.p4.1.1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p4.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p4.1.1.1.m1.1d">italic_Œ±</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.1.2">LoRA rank</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.5.4.1">LoRA dropout</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.5.4.2">0.05</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.6.5.1">LoRA targets</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.6.5.2">FFN modules</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.7.6.1">LR scheduler</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.7.6.2">Cosine</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.8.7.1">Warmup steps</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.8.7.2">100</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.9.8.1">Total steps</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.9.8.2">5000</td>
</tr>
</tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Evaluation details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We performed three
principal
evaluations:
accuracy on <em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.1.1">MMLU</em>, accuracy on <em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.1.2">BoolQ</em>, and
loss on <em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.1.3">C4</em>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1">For <span class="ltx_text ltx_font_bold" id="A1.SS2.p2.1.1">MMLU accuracy</span>:
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1">We use the <code class="ltx_verbatim ltx_font_typewriter" id="A1.I2.i1.p1.1.1">cais/mmlu</code> version of the dataset from Hugging Face.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1">We
follow
the formatting suggested in the original reference <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib81" title="">2020</a>)</cite>
without further prompt engineering.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1">For constructing few-shot examples, we use the <code class="ltx_verbatim ltx_font_typewriter" id="A1.I2.i3.p1.1.1">dev</code> set from <code class="ltx_verbatim ltx_font_typewriter" id="A1.I2.i3.p1.1.2">cais/mmlu</code>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I2.i4.p1">
<p class="ltx_p" id="A1.I2.i4.p1.1">For our experiments, we use <math alttext="0" class="ltx_Math" display="inline" id="A1.I2.i4.p1.1.m1.1"><semantics id="A1.I2.i4.p1.1.m1.1a"><mn id="A1.I2.i4.p1.1.m1.1.1" xref="A1.I2.i4.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="A1.I2.i4.p1.1.m1.1b"><cn id="A1.I2.i4.p1.1.m1.1.1.cmml" type="integer" xref="A1.I2.i4.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> few-shot examples; our results and analysis are robust to this choice, cf. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F7" title="Figure 7 ‚Ä£ B.1 Prompting ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I2.i5.p1">
<p class="ltx_p" id="A1.I2.i5.p1.1">We report average accuracy across all subjects.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1">For <span class="ltx_text ltx_font_bold" id="A1.SS2.p3.1.1">BoolQ accuracy</span>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A1.I3">
<li class="ltx_item" id="A1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I3.i1.p1">
<p class="ltx_p" id="A1.I3.i1.p1.1">We used the <code class="ltx_verbatim ltx_font_typewriter" id="A1.I3.i1.p1.1.1">hassansh/boolq_n_shot</code> version from Hugging Face.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I3.i2.p1">
<p class="ltx_p" id="A1.I3.i2.p1.1">For our experiments, we use <math alttext="0" class="ltx_Math" display="inline" id="A1.I3.i2.p1.1.m1.1"><semantics id="A1.I3.i2.p1.1.m1.1a"><mn id="A1.I3.i2.p1.1.m1.1.1" xref="A1.I3.i2.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="A1.I3.i2.p1.1.m1.1b"><cn id="A1.I3.i2.p1.1.m1.1.1.cmml" type="integer" xref="A1.I3.i2.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> few-shot examples.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I3.i3.p1">
<p class="ltx_p" id="A1.I3.i3.p1.1">The complete BoolQ results ‚Äì truncated from the main text ‚Äì are shown here in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.F6" title="Figure 6 ‚Ä£ A.2 Evaluation details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">6</span></a>: in the left panel we present the Llama-2 family, in the middle panel we present models from the Qwen family, and in the right panel we should Mistral-7B and Phi-2; we also make the experiments without healing semi-transparent in order to better display the results from the complete similarity-informed pruning method. Importantly, while we see here that healing plays a more important role than it did
for MMLU in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‚Ä£ 4.1 Accuracy on QA benchmarks ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>, after healing we still have a characteristic flat region of robust performance;
as before, the capabilities required to achieve a model‚Äôs top score isn‚Äôt removed by significant layer pruning until a critical model-dependent threshold.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="A1.F6">
<p class="ltx_p ltx_align_center ltx_align_center" id="A1.F6.1"><span class="ltx_text" id="A1.F6.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="305" id="A1.F6.1.1.g1" src="x6.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>BoolQ accuracy (0-shot)
vs. fraction of layers
dropped
for different model families.
(<em class="ltx_emph ltx_font_italic" id="A1.F6.5.1">Left:</em> Llama-2 family; <em class="ltx_emph ltx_font_italic" id="A1.F6.6.2">Middle:</em> Qwen family; <em class="ltx_emph ltx_font_italic" id="A1.F6.7.3">Right:</em> Mistral-7B and Phi-2.)
The
solid lines
represent
performance
after dropping layers and healing,
and the (semi-transparent) dotted lines
show
performance after dropping layers only (no healing),
and the dashed gray line is the score for guessing randomly.
For BoolQ, healing leads to important improvements such that performances; then, across all models, performances
are quite robust
until 20%-55% pruning fractions, depending on model family and size, at which point they transitions to random guessing.
</figcaption>
<br class="ltx_break ltx_centering">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.p4">
<p class="ltx_p" id="A1.SS2.p4.1">For <span class="ltx_text ltx_font_bold" id="A1.SS2.p4.1.1">C4 Validation Loss</span>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A1.I4">
<li class="ltx_item" id="A1.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I4.i1.p1">
<p class="ltx_p" id="A1.I4.i1.p1.1">We used the <code class="ltx_verbatim ltx_font_typewriter" id="A1.I4.i1.p1.1.1">c4</code> version from Hugging Face (soon be deprecated in favor of <code class="ltx_verbatim ltx_font_typewriter" id="A1.I4.i1.p1.1.2">allenai/c4</code>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I4.i2.p1">
<p class="ltx_p" id="A1.I4.i2.p1.1">We evaluated using the <em class="ltx_emph ltx_font_italic" id="A1.I4.i2.p1.1.1">validation</em> split as we healed with the train split.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I4.i3.p1">
<p class="ltx_p" id="A1.I4.i3.p1.1">Given its size, we randomly sampled 60k sequences and held them fixed across all models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A1.I4.i4.p1">
<p class="ltx_p" id="A1.I4.i4.p1.3">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F3" title="Figure 3 ‚Ä£ 4.2 Loss on next-token predictions ‚Ä£ 4 Results ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a> we normalized the
loss to facilitate fair comparison across model families that employ different vocab sizes:
to normalize, we divided by
<math alttext="\log V" class="ltx_Math" display="inline" id="A1.I4.i4.p1.1.m1.1"><semantics id="A1.I4.i4.p1.1.m1.1a"><mrow id="A1.I4.i4.p1.1.m1.1.1" xref="A1.I4.i4.p1.1.m1.1.1.cmml"><mi id="A1.I4.i4.p1.1.m1.1.1.1" xref="A1.I4.i4.p1.1.m1.1.1.1.cmml">log</mi><mo id="A1.I4.i4.p1.1.m1.1.1a" lspace="0.167em" xref="A1.I4.i4.p1.1.m1.1.1.cmml">‚Å°</mo><mi id="A1.I4.i4.p1.1.m1.1.1.2" xref="A1.I4.i4.p1.1.m1.1.1.2.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.I4.i4.p1.1.m1.1b"><apply id="A1.I4.i4.p1.1.m1.1.1.cmml" xref="A1.I4.i4.p1.1.m1.1.1"><log id="A1.I4.i4.p1.1.m1.1.1.1.cmml" xref="A1.I4.i4.p1.1.m1.1.1.1"></log><ci id="A1.I4.i4.p1.1.m1.1.1.2.cmml" xref="A1.I4.i4.p1.1.m1.1.1.2">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i4.p1.1.m1.1c">\log V</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i4.p1.1.m1.1d">roman_log italic_V</annotation></semantics></math>, where <math alttext="V" class="ltx_Math" display="inline" id="A1.I4.i4.p1.2.m2.1"><semantics id="A1.I4.i4.p1.2.m2.1a"><mi id="A1.I4.i4.p1.2.m2.1.1" xref="A1.I4.i4.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="A1.I4.i4.p1.2.m2.1b"><ci id="A1.I4.i4.p1.2.m2.1.1.cmml" xref="A1.I4.i4.p1.2.m2.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i4.p1.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i4.p1.2.m2.1d">italic_V</annotation></semantics></math> is the <em class="ltx_emph ltx_font_italic" id="A1.I4.i4.p1.3.1">per-model</em> vocab size (listed in a table in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.1</span></a>). This, <math alttext="\log V" class="ltx_Math" display="inline" id="A1.I4.i4.p1.3.m3.1"><semantics id="A1.I4.i4.p1.3.m3.1a"><mrow id="A1.I4.i4.p1.3.m3.1.1" xref="A1.I4.i4.p1.3.m3.1.1.cmml"><mi id="A1.I4.i4.p1.3.m3.1.1.1" xref="A1.I4.i4.p1.3.m3.1.1.1.cmml">log</mi><mo id="A1.I4.i4.p1.3.m3.1.1a" lspace="0.167em" xref="A1.I4.i4.p1.3.m3.1.1.cmml">‚Å°</mo><mi id="A1.I4.i4.p1.3.m3.1.1.2" xref="A1.I4.i4.p1.3.m3.1.1.2.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.I4.i4.p1.3.m3.1b"><apply id="A1.I4.i4.p1.3.m3.1.1.cmml" xref="A1.I4.i4.p1.3.m3.1.1"><log id="A1.I4.i4.p1.3.m3.1.1.1.cmml" xref="A1.I4.i4.p1.3.m3.1.1.1"></log><ci id="A1.I4.i4.p1.3.m3.1.1.2.cmml" xref="A1.I4.i4.p1.3.m3.1.1.2">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i4.p1.3.m3.1c">\log V</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i4.p1.3.m3.1d">roman_log italic_V</annotation></semantics></math>, corresponds to the loss of sampling tokens uniformly, which naturally sets the scale for a given model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Ablations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Here
we detail ablations of various hyperparameters: prompting (¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS1" title="B.1 Prompting ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B.1</span></a>), finetuning seed (¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS2" title="B.2 Finetuning seed ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B.2</span></a>),
LoRA rank (¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS3" title="B.3 LoRA rank ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B.3</span></a>). Qualitatively, the results of the paper are quite robust to the variation of any of these.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Prompting</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">It‚Äôs common knowledge that altering the prompt on QA evaluations can significantly impact results. To control for prompting, we ablate the MMLU accuracy
for our principal similarity-informed pruning described in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.2</span></a> when applied to
Llama-2-13B:
in the left panel of Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F7" title="Figure 7 ‚Ä£ B.1 Prompting ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a>, we show results for
changing the ordering of the few-shot examples in the prompt,
and in the right panel the same figure, we show results for changing the number of few-shot examples. Broadly we see that the layer-pruning method is robust to these changes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A2.F7">
<p class="ltx_p ltx_align_center ltx_align_center" id="A2.F7.1"><span class="ltx_text" id="A2.F7.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="307" id="A2.F7.1.1.g1" src="x7.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Effect of prompt ablations on MMLU accuracy vs. fraction of layers dropped for
Llama-2-13B.
<em class="ltx_emph ltx_font_italic" id="A2.F7.6.1">Left:</em> We vary the ordering of the few-shot examples and see it does not have any impact.
<em class="ltx_emph ltx_font_italic" id="A2.F7.7.2">Right:</em> We very the number <math alttext="n" class="ltx_Math" display="inline" id="A2.F7.3.m1.1"><semantics id="A2.F7.3.m1.1b"><mi id="A2.F7.3.m1.1.1" xref="A2.F7.3.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A2.F7.3.m1.1c"><ci id="A2.F7.3.m1.1.1.cmml" xref="A2.F7.3.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.F7.3.m1.1d">n</annotation><annotation encoding="application/x-llamapun" id="A2.F7.3.m1.1e">italic_n</annotation></semantics></math> of few-shot examples; while careful study of the flat region suggests increasing the number of few-shot examples marginally improves performance,
regardless,
the layer-pruning strategy is robust to
this kind of variation.
</figcaption>
<br class="ltx_break ltx_centering">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Finetuning seed</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">Here we vary the finetuning seed.
For all of our experiments, we use the following code snippet to ensure reproducibility:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS2.p1.2">SEED_VAL = 0
transformers.enable_full_determinism(SEED_VAL)
</pre>
<p class="ltx_p" id="A2.SS2.p1.3">Since we begin with a pretrained model, the finetuning seed doesn‚Äôt affect initialization, but it will impact the stochastic aspects of further training such as data order.
To control for this, we ablate the
finetuning seed
for our principal similarity-informed pruning described in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‚Ä£ 3 Method ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.2</span></a> when applied to
Llama-2-13B: in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F8" title="Figure 8 ‚Ä£ B.2 Finetuning seed ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">8</span></a> we observe that the layer-pruning method is robust to the choice of seed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A2.F8">
<p class="ltx_p ltx_align_center ltx_align_center" id="A2.F8.1"><span class="ltx_text" id="A2.F8.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="330" id="A2.F8.1.1.g1" src="x8.png" width="415"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Effect of varying the finetuning seed on MMLU accuracy vs. fraction of layers dropped for Llama-2-13B: there is no meaningful effect.
</figcaption>
<br class="ltx_break ltx_centering">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>LoRA rank</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">Here we vary the LoRA rank used for healing. Unfortunately, our compute budget did not allow us to make an exhaustive sweep across all of our experimental configurations. In lieu of that, we employed the following protocol for our main experiments:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1">Begin with rank 64,
following the QLoRA setup (see, e.g. Appendix B.2 of Ref.&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dettmers et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1">If healing with that rank significantly harms the performance compared to no healing, then
sweep LoRA ranks for that model and, for the other evaluations, pick the best performing LoRA rank according to its MMLU accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
<p class="ltx_p" id="A2.SS3.p1.2">This protocol is designed to maximize the chance that healing will improve performance across all of our evaluations. For simplicity, we ran this rank-picking protocol using the simple pruning heuristic, with the exception of Llama-2-70B.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS3.p2">
<p class="ltx_p" id="A2.SS3.p2.1">In practice, this led to us using rank 64 for every model with the exceptions of
Mistral-7B, with rank 4,
Llama-2-7B, with rank 2,
and
Llama-2-70B, with rank 8.
(To review
this same information
in tabular form,
see the second Table in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.1</span></a>.)
Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F9" title="Figure 9 ‚Ä£ B.3 LoRA rank ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">9</span></a>
displays the sweeps over MMLU accuracy
supporting these choices
for Mistral-7B (bottom left panel),
Llama-2-7B (bottom middle panel),
and
Llama-2-70B (top right panel):
overall, while the LoRA rank does not have a significant impact on the qualitative behavior of the healed model,
decreasing the LoRA rank
generally
improves performance. In the top left and middle panels of Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F9" title="Figure 9 ‚Ä£ B.3 LoRA rank ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">9</span></a>, we show corresponding sweeps for Mistral-7B (top) and Llama-2-7B (middle) using the similarity-informed pruning strategy:
we see that for this pruning method both models are much more robust,
though rank 2 is still the top performing rank for Llama-2-7B.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A2.F9">
<p class="ltx_p ltx_align_center ltx_align_center" id="A2.F9.1"><span class="ltx_text" id="A2.F9.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="279" id="A2.F9.1.1.g1" src="x9.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Effect of varying the LoRA rank.
<span class="ltx_text ltx_font_bold" id="A2.F9.10.1">Top</span>: 5-shot MMLU accuracy vs. fraction of layers dropped
using the similarity-informed pruning strategy on
Mistral-7B (<em class="ltx_emph ltx_font_italic" id="A2.F9.11.2">left</em>), Llama-2-7B (<span class="ltx_text ltx_font_italic" id="A2.F9.12.3">middle</span>), and Llama-2-70B (<span class="ltx_text ltx_font_italic" id="A2.F9.13.4">right</span>).
Across all ranks we observe
similar behavior, though
there‚Äôs a small effect of
decreasing
rank
improving
overall performance.
<span class="ltx_text ltx_font_bold" id="A2.F9.14.5">Bottom, left and middle</span>: 5-shot MMLU accuracy vs. fraction of layers dropped
using the simple pruning heuristic on
Mistral-7B (<em class="ltx_emph ltx_font_italic" id="A2.F9.15.6">left</em>) and Llama-2-7B (<span class="ltx_text ltx_font_italic" id="A2.F9.16.7">middle</span>). As before, qualitative behavior is similar across ranks, though
in this case
it‚Äôs much clearer that decreasing rank improves performance.
<span class="ltx_text ltx_font_bold" id="A2.F9.17.8">Bottom, right</span>: C4 validation loss vs. fraction of layers dropped
using the similarity-informed pruning strategy on
Mistral-7B.
In contrast to MMLU, decreasing rank
harms performance; together, these results suggest that larger ranks may be overfitting.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS3.p3">
<p class="ltx_p" id="A2.SS3.p3.1">The characteristic improvement of MMLU accuracy with decreasing LoRA rank ‚Äì even for extremely low ranks(!) ‚Äì deserves an explanation. One possibility is that lowering the LoRA rank can better regularize finetuning against overfitting. In particular, astute readers may have been surprised at the discussion of peak learning rates in ¬ß<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‚Ä£ Appendix A Experimental Details ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.1</span></a>: models were finetuned with the same peak used in pretraining;
a ‚Äúlarge‚Äù LoRA rank of 64 introduces a number of additional parameters that may overfit to C4. This overfitting would certainly be harmful, since
the actual pretraining datasets for the models we consider are <em class="ltx_emph ltx_font_italic" id="A2.SS3.p3.1.1">(a)</em> unknown to us, and <em class="ltx_emph ltx_font_italic" id="A2.SS3.p3.1.2">(b)</em>, likely to be of significantly higher quality than C4.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS3.p4">
<p class="ltx_p" id="A2.SS3.p4.1">We investigate this directly for Mistral-7B. In the bottom right panel of Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F9" title="Figure 9 ‚Ä£ B.3 LoRA rank ‚Ä£ Appendix B Ablations ‚Ä£ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">9</span></a> we plot the C4 validation loss
across different LoRA ranks: we see that while decreasing the LoRA rank generally improves MMLU accuracy (cf. left-most panels), at the same time it harms the C4 validation loss.
This supports our overfitting hypothesis. In a greater-resourced future, it would be interesting to improve the healing process by considering other forms of regularization and learning rate tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
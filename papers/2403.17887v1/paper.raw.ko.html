<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>The Unreasonable Ineffectiveness of the Deeper Layers</title>
<!--Generated on Tue Mar 26 17:20:28 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2403.17887v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2403.17887v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2403.17887v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2403.17887v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1" title="1 Introduction ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2" title="2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Literature Review</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS1" title="2.1 Pruning ‣ 2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Pruning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS2" title="2.2 Model distillation ‣ 2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Model distillation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS3" title="2.3 Efficient finetuning and inference acceleration ‣ 2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Efficient finetuning
and inference acceleration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS4" title="2.4 A breadth of depth-dependent studies ‣ 2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>A breadth of depth-dependent studies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3" title="3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS1" title="3.1 Intuition ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Intuition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Layer-pruning algorithm(s)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4" title="4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS1" title="4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Accuracy on QA benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS2" title="4.2 Loss on next-token predictions ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Loss
on
next-token predictions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS3" title="4.3 Angular distances between representations ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Angular distances between representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS4" title="4.4 A simpler pruning strategy ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>A simpler pruning strategy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S5" title="5 Discussion and Future Directions ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Future Directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1" title="Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Model and healing details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS2" title="A.2 Evaluation details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Evaluation details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2" title="Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Ablations</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS1" title="B.1 Prompting ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS2" title="B.2 Finetuning seed ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Finetuning seed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS3" title="B.3 LoRA rank ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>LoRA rank</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2403.17887v1 [cs.CL] 26 Mar 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Unreasonable Ineffectiveness of the Deeper Layers</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrey Gromov 
<br class="ltx_break">Meta FAIR &amp; UMD 
<br class="ltx_break">&amp;Kushal Tirumala<math alttext="{}^{*}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><times id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{*}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT * end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break">Meta FAIR 
<br class="ltx_break">&amp;Hassan Shapourian 
<br class="ltx_break">Cisco
&amp;Paolo Glorioso 
<br class="ltx_break">Zyphra 
<br class="ltx_break">Daniel A. Roberts
<br class="ltx_break">MIT &amp; Sequoia Capital
</span><span class="ltx_author_notes">Co-first authors; direct correspondence to <span class="ltx_text ltx_font_typewriter" id="id2.2.id1">gromovand@meta.com</span>, <span class="ltx_text ltx_font_typewriter" id="id3.3.id2">ktirumala@meta.com</span>, and <span class="ltx_text ltx_font_typewriter" id="id4.4.id3">drob@mit.edu</span>. 
<br class="ltx_break"></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id5.id1">본 논문에서는 오픈웨이트 사전학습 LLM(Open-weight pretrained LLM)의 인기 있는 패밀리에 대한 간단한 레이어 프루닝 전략을 경험적으로 연구하여, 레이어의 많은 부분(최대 절반)이 제거될 때까지 서로 다른 질의응답 벤치마크에서 성능의 최소 저하를 발견한다. 이러한 모델을 가지치기하기 위해, 우리는 층들 간의 유사성을 고려하여 가지치기할 층들의 최적 블록을 식별하고, 그리고 손상을 "치유"하기 위해 소량의 미세조정을 수행한다. 특히, PEFT(parameter-efficient finetuning) 방법, 특히 양자화 및QLoRA(Low Rank Adapters) 방법을 사용하여 각 실험을 단일 A100 GPU에서 수행할 수 있다. 이러한 결과는 실용적인 관점에서, 레이어 프루닝 방법이 한편으로는 미세조정의 계산 자원을 더욱 줄이기 위해 다른 PEFT 전략을 보완할 수 있고, 다른 한편으로는 추론의 메모리 및 지연 시간을 개선할 수 있음을 시사한다. 과학적 관점에서 이러한 LLM의 계층 삭제에 대한 견고성은 현재 사전 훈련 방법이 네트워크의 더 깊은 계층에서 매개변수를 적절하게 활용하지 않거나 얕은 계층이 지식을 저장하는 데 중요한 역할을 한다는 것을 의미한다.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">지난 몇 년 동안 대규모 언어 모델(LLM)은 단순한 연구 인공물 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib1" title="">2019</a>)</cite>에서 유용한 제품 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib2" title="">2022</a>)</cite>로 진화했다. 이 진화는 대부분 <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib3" title="">2023</a>); Gemini Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib4" title="">2023</a>)</cite>를 훈련하는 데 사용되는 리소스의 규모가 극적으로 증가했기 때문일 수 있다. 이러한 모델들은 트레이닝이 완료된 후에 추론 모드에서 전체 수명 FLOP의 대부분을 볼 가능성이 있기 때문에, LLM의 사전 트레이닝은 효율적인, 즉 컴퓨팅-최적, 트레이닝 <cite class="ltx_cite ltx_citemacro_cite">Kaplan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib5" title="">2020</a>); Hoffmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib6" title="">2022</a>)</cite>에 대한 고려를 필요로 할 뿐만 아니라 추론 인식 <cite class="ltx_cite ltx_citemacro_cite">De Vries (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib7" title="">2023</a>); Sardana and Frankle (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib8" title="">2023</a>)</cite>도 필요로 한다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">이미 교육을 받은 모델은 어떤가요? 신경 스케일링 법칙에 의해 지시되는 트레이닝 고려사항들 외에도, 미세조정 및 그 후 LLMs를 추론하는 비용 및 시간을 감소시키기 위한 수많은 사후 트레이닝 기술들이 또한 존재한다. 특히, <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">quantization</em>은 모델 가중치 <cite class="ltx_cite ltx_citemacro_cite">Dettmers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib9" title="">2022</a>); Frantar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib10" title="">2022</a>); Dettmers and Zettlemoyer (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib11" title="">2023</a>); Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib12" title="">2023</a>)</cite>, Low Rank Adapters(<em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">LoRA</em>)는 모델 파라미터 <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib13" title="">2021</a>)</cite> 또는 <em class="ltx_emph ltx_font_italic" id="S1.p2.1.3">pruning</em>은 불필요한 파라미터 또는 연결 <cite class="ltx_cite ltx_citemacro_cite">LeCun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib14" title="">1989</a>); Hassibi and Stork (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib15" title="">1992</a>); Han et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib16" title="">2015</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib17" title="">2016</a>); Frankle and Carbin (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib18" title="">2018</a>)</cite>를 직접 제거하여 추론용 메모리 풋프린트 및 시간을 줄이는 데 사용할 수 있다. 이 세 가지 전략은 다소 직교적이므로 이상적으로는 자원 제약 환경에서 이러한 세 가지 훈련 후 효율성 기술을 모두 조합하여 활용하고자 한다. 이러한 방향으로 <em class="ltx_emph ltx_font_italic" id="S1.p2.1.4">QLoRA</em> <cite class="ltx_cite ltx_citemacro_cite">Dettmers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>는 매개 변수의 4비트 양자화와 LoRA finetuning이 함께 작동할 수 있도록 하는 몇 가지 혁신을 도입했습니다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">이러한 조합을 기반으로 이 작업에서는 개방형 가중치 LLM을 사용하여 매우 간단한 가지치기 전략을 연구한다. 특히, 주어진 가지치기 분수에 대해 가지치기하기 위한 최적의 층들을 식별하기 위해 서로 다른 층들에서의 표현들 사이의 유사성을 이용하는 방법을 개발하고, 이러한 층들을 제거한 후, 작은 양의 미세 조정(QLoRA를 사용)으로 가지치기-유도된 불일치를 "힐링"한다. 우리의 주요 결과는 다운스트림 성능 저하가 최소인 모델에서 <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">deepest layers</em>의 상당 부분을 제거할 수 있다는 것이다. 예를 들어, Llama-2-70B <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib20" title="">2023a</a>)</cite>의 경우 성능이 붕괴되기 전에 레이어의 대략 <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">half</em>까지 제거할 수 있다. 우리의 전략에 대한 개요와 Llama-2-70B를 가지치기한 결과는 그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>에 나와 있다.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<p class="ltx_p ltx_align_center ltx_align_center" id="S1.F1.1"><span class="ltx_text" id="S1.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="548" id="S1.F1.1.1.g1" src="https://arxiv.org/html/2403.17887v1/x1.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 1:</span>Overview of our layer-pruning strategy and example results:</figcaption>
<em class="ltx_emph ltx_font_italic" id="S1.F1.40.1">(a)</em> a flowchart describing the algorithm: if removing <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.19.m1.1"><semantics id="S1.F1.19.m1.1b"><mi id="S1.F1.19.m1.1.1" xref="S1.F1.19.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.19.m1.1c"><ci id="S1.F1.19.m1.1.1.cmml" xref="S1.F1.19.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.19.m1.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.19.m1.1e">italic_n</annotation></semantics></math> layers, we find the layer, <math alttext="\ell^{*}" class="ltx_Math" display="inline" id="S1.F1.20.m2.1"><semantics id="S1.F1.20.m2.1b"><msup id="S1.F1.20.m2.1.1" xref="S1.F1.20.m2.1.1.cmml"><mi id="S1.F1.20.m2.1.1.2" mathvariant="normal" xref="S1.F1.20.m2.1.1.2.cmml">ℓ</mi><mo id="S1.F1.20.m2.1.1.3" xref="S1.F1.20.m2.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F1.20.m2.1c"><apply id="S1.F1.20.m2.1.1.cmml" xref="S1.F1.20.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.20.m2.1.1.1.cmml" xref="S1.F1.20.m2.1.1">superscript</csymbol><ci id="S1.F1.20.m2.1.1.2.cmml" xref="S1.F1.20.m2.1.1.2">ℓ</ci><times id="S1.F1.20.m2.1.1.3.cmml" xref="S1.F1.20.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.20.m2.1d">\ell^{*}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.20.m2.1e">roman_ℓ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>, that minimizes the angular distance, <math alttext="d" class="ltx_Math" display="inline" id="S1.F1.21.m3.1"><semantics id="S1.F1.21.m3.1b"><mi id="S1.F1.21.m3.1.1" xref="S1.F1.21.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.F1.21.m3.1c"><ci id="S1.F1.21.m3.1.1.cmml" xref="S1.F1.21.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.21.m3.1d">d</annotation><annotation encoding="application/x-llamapun" id="S1.F1.21.m3.1e">italic_d</annotation></semantics></math>, between layers <math alttext="\ell" class="ltx_Math" display="inline" id="S1.F1.22.m4.1"><semantics id="S1.F1.22.m4.1b"><mi id="S1.F1.22.m4.1.1" mathvariant="normal" xref="S1.F1.22.m4.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S1.F1.22.m4.1c"><ci id="S1.F1.22.m4.1.1.cmml" xref="S1.F1.22.m4.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.22.m4.1d">\ell</annotation><annotation encoding="application/x-llamapun" id="S1.F1.22.m4.1e">roman_ℓ</annotation></semantics></math> and <math alttext="\ell\!+\!n" class="ltx_Math" display="inline" id="S1.F1.23.m5.1"><semantics id="S1.F1.23.m5.1b"><mrow id="S1.F1.23.m5.1.1" xref="S1.F1.23.m5.1.1.cmml"><mi id="S1.F1.23.m5.1.1.2" mathvariant="normal" xref="S1.F1.23.m5.1.1.2.cmml">ℓ</mi><mo id="S1.F1.23.m5.1.1.1" lspace="0.052em" rspace="0.052em" xref="S1.F1.23.m5.1.1.1.cmml">+</mo><mi id="S1.F1.23.m5.1.1.3" xref="S1.F1.23.m5.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.23.m5.1c"><apply id="S1.F1.23.m5.1.1.cmml" xref="S1.F1.23.m5.1.1"><plus id="S1.F1.23.m5.1.1.1.cmml" xref="S1.F1.23.m5.1.1.1"></plus><ci id="S1.F1.23.m5.1.1.2.cmml" xref="S1.F1.23.m5.1.1.2">ℓ</ci><ci id="S1.F1.23.m5.1.1.3.cmml" xref="S1.F1.23.m5.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.23.m5.1d">\ell\!+\!n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.23.m5.1e">roman_ℓ + italic_n</annotation></semantics></math>; we then remove the <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.24.m6.1"><semantics id="S1.F1.24.m6.1b"><mi id="S1.F1.24.m6.1.1" xref="S1.F1.24.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.24.m6.1c"><ci id="S1.F1.24.m6.1.1.cmml" xref="S1.F1.24.m6.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.24.m6.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.24.m6.1e">italic_n</annotation></semantics></math> layers beginning with layer <math alttext="\ell^{*}" class="ltx_Math" display="inline" id="S1.F1.25.m7.1"><semantics id="S1.F1.25.m7.1b"><msup id="S1.F1.25.m7.1.1" xref="S1.F1.25.m7.1.1.cmml"><mi id="S1.F1.25.m7.1.1.2" mathvariant="normal" xref="S1.F1.25.m7.1.1.2.cmml">ℓ</mi><mo id="S1.F1.25.m7.1.1.3" xref="S1.F1.25.m7.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F1.25.m7.1c"><apply id="S1.F1.25.m7.1.1.cmml" xref="S1.F1.25.m7.1.1"><csymbol cd="ambiguous" id="S1.F1.25.m7.1.1.1.cmml" xref="S1.F1.25.m7.1.1">superscript</csymbol><ci id="S1.F1.25.m7.1.1.2.cmml" xref="S1.F1.25.m7.1.1.2">ℓ</ci><times id="S1.F1.25.m7.1.1.3.cmml" xref="S1.F1.25.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.25.m7.1d">\ell^{*}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.25.m7.1e">roman_ℓ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>; finally, if necessary, we can “heal” the damage with a small amount of (parameter-efficient) finetuning.
<em class="ltx_emph ltx_font_italic" id="S1.F1.41.2">(b)</em> a schematic depicting the removal of <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.26.m8.1"><semantics id="S1.F1.26.m8.1b"><mi id="S1.F1.26.m8.1.1" xref="S1.F1.26.m8.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.26.m8.1c"><ci id="S1.F1.26.m8.1.1.cmml" xref="S1.F1.26.m8.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.26.m8.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.26.m8.1e">italic_n</annotation></semantics></math> total layers, indexed from <math alttext="\ell^{*}\!" class="ltx_Math" display="inline" id="S1.F1.27.m9.1"><semantics id="S1.F1.27.m9.1b"><msup id="S1.F1.27.m9.1.1" xref="S1.F1.27.m9.1.1.cmml"><mi id="S1.F1.27.m9.1.1.2" mathvariant="normal" xref="S1.F1.27.m9.1.1.2.cmml">ℓ</mi><mo id="S1.F1.27.m9.1.1.3" xref="S1.F1.27.m9.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F1.27.m9.1c"><apply id="S1.F1.27.m9.1.1.cmml" xref="S1.F1.27.m9.1.1"><csymbol cd="ambiguous" id="S1.F1.27.m9.1.1.1.cmml" xref="S1.F1.27.m9.1.1">superscript</csymbol><ci id="S1.F1.27.m9.1.1.2.cmml" xref="S1.F1.27.m9.1.1.2">ℓ</ci><times id="S1.F1.27.m9.1.1.3.cmml" xref="S1.F1.27.m9.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.27.m9.1d">\ell^{*}\!</annotation><annotation encoding="application/x-llamapun" id="S1.F1.27.m9.1e">roman_ℓ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="\ell^{*}\!\!+\!n\!-\!1" class="ltx_Math" display="inline" id="S1.F1.28.m10.1"><semantics id="S1.F1.28.m10.1b"><mrow id="S1.F1.28.m10.1.1" xref="S1.F1.28.m10.1.1.cmml"><mrow id="S1.F1.28.m10.1.1.2" xref="S1.F1.28.m10.1.1.2.cmml"><msup id="S1.F1.28.m10.1.1.2.2" xref="S1.F1.28.m10.1.1.2.2.cmml"><mi id="S1.F1.28.m10.1.1.2.2.2" mathvariant="normal" xref="S1.F1.28.m10.1.1.2.2.2.cmml">ℓ</mi><mo id="S1.F1.28.m10.1.1.2.2.3" xref="S1.F1.28.m10.1.1.2.2.3.cmml">*</mo></msup><mo id="S1.F1.28.m10.1.1.2.1" rspace="0.052em" xref="S1.F1.28.m10.1.1.2.1.cmml">+</mo><mi id="S1.F1.28.m10.1.1.2.3" xref="S1.F1.28.m10.1.1.2.3.cmml">n</mi></mrow><mo id="S1.F1.28.m10.1.1.1" lspace="0.052em" rspace="0.052em" xref="S1.F1.28.m10.1.1.1.cmml">−</mo><mn id="S1.F1.28.m10.1.1.3" xref="S1.F1.28.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.28.m10.1c"><apply id="S1.F1.28.m10.1.1.cmml" xref="S1.F1.28.m10.1.1"><minus id="S1.F1.28.m10.1.1.1.cmml" xref="S1.F1.28.m10.1.1.1"></minus><apply id="S1.F1.28.m10.1.1.2.cmml" xref="S1.F1.28.m10.1.1.2"><plus id="S1.F1.28.m10.1.1.2.1.cmml" xref="S1.F1.28.m10.1.1.2.1"></plus><apply id="S1.F1.28.m10.1.1.2.2.cmml" xref="S1.F1.28.m10.1.1.2.2"><csymbol cd="ambiguous" id="S1.F1.28.m10.1.1.2.2.1.cmml" xref="S1.F1.28.m10.1.1.2.2">superscript</csymbol><ci id="S1.F1.28.m10.1.1.2.2.2.cmml" xref="S1.F1.28.m10.1.1.2.2.2">ℓ</ci><times id="S1.F1.28.m10.1.1.2.2.3.cmml" xref="S1.F1.28.m10.1.1.2.2.3"></times></apply><ci id="S1.F1.28.m10.1.1.2.3.cmml" xref="S1.F1.28.m10.1.1.2.3">𝑛</ci></apply><cn id="S1.F1.28.m10.1.1.3.cmml" type="integer" xref="S1.F1.28.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.28.m10.1d">\ell^{*}\!\!+\!n\!-\!1</annotation><annotation encoding="application/x-llamapun" id="S1.F1.28.m10.1e">roman_ℓ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT + italic_n - 1</annotation></semantics></math>. <em class="ltx_emph ltx_font_italic" id="S1.F1.42.3">(c)</em> angular distance, <math alttext="d" class="ltx_Math" display="inline" id="S1.F1.29.m11.1"><semantics id="S1.F1.29.m11.1b"><mi id="S1.F1.29.m11.1.1" xref="S1.F1.29.m11.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.F1.29.m11.1c"><ci id="S1.F1.29.m11.1.1.cmml" xref="S1.F1.29.m11.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.29.m11.1d">d</annotation><annotation encoding="application/x-llamapun" id="S1.F1.29.m11.1e">italic_d</annotation></semantics></math>, between different numbers of layers, <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.30.m12.1"><semantics id="S1.F1.30.m12.1b"><mi id="S1.F1.30.m12.1.1" xref="S1.F1.30.m12.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.30.m12.1c"><ci id="S1.F1.30.m12.1.1.cmml" xref="S1.F1.30.m12.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.30.m12.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.30.m12.1e">italic_n</annotation></semantics></math>, vs. the layer number, <math alttext="\ell" class="ltx_Math" display="inline" id="S1.F1.31.m13.1"><semantics id="S1.F1.31.m13.1b"><mi id="S1.F1.31.m13.1.1" mathvariant="normal" xref="S1.F1.31.m13.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S1.F1.31.m13.1c"><ci id="S1.F1.31.m13.1.1.cmml" xref="S1.F1.31.m13.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.31.m13.1d">\ell</annotation><annotation encoding="application/x-llamapun" id="S1.F1.31.m13.1e">roman_ℓ</annotation></semantics></math>, that indexes the beginning of the block of <math alttext="n" class="ltx_Math" display="inline" id="S1.F1.32.m14.1"><semantics id="S1.F1.32.m14.1b"><mi id="S1.F1.32.m14.1.1" xref="S1.F1.32.m14.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.F1.32.m14.1c"><ci id="S1.F1.32.m14.1.1.cmml" xref="S1.F1.32.m14.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.32.m14.1d">n</annotation><annotation encoding="application/x-llamapun" id="S1.F1.32.m14.1e">italic_n</annotation></semantics></math>; the bottom curve (darkest purple) represents <math alttext="n=1" class="ltx_Math" display="inline" id="S1.F1.33.m15.1"><semantics id="S1.F1.33.m15.1b"><mrow id="S1.F1.33.m15.1.1" xref="S1.F1.33.m15.1.1.cmml"><mi id="S1.F1.33.m15.1.1.2" xref="S1.F1.33.m15.1.1.2.cmml">n</mi><mo id="S1.F1.33.m15.1.1.1" xref="S1.F1.33.m15.1.1.1.cmml">=</mo><mn id="S1.F1.33.m15.1.1.3" xref="S1.F1.33.m15.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.33.m15.1c"><apply id="S1.F1.33.m15.1.1.cmml" xref="S1.F1.33.m15.1.1"><eq id="S1.F1.33.m15.1.1.1.cmml" xref="S1.F1.33.m15.1.1.1"></eq><ci id="S1.F1.33.m15.1.1.2.cmml" xref="S1.F1.33.m15.1.1.2">𝑛</ci><cn id="S1.F1.33.m15.1.1.3.cmml" type="integer" xref="S1.F1.33.m15.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.33.m15.1d">n=1</annotation><annotation encoding="application/x-llamapun" id="S1.F1.33.m15.1e">italic_n = 1</annotation></semantics></math>, while the top curve (lightest yellow) represents <math alttext="n=64" class="ltx_Math" display="inline" id="S1.F1.34.m16.1"><semantics id="S1.F1.34.m16.1b"><mrow id="S1.F1.34.m16.1.1" xref="S1.F1.34.m16.1.1.cmml"><mi id="S1.F1.34.m16.1.1.2" xref="S1.F1.34.m16.1.1.2.cmml">n</mi><mo id="S1.F1.34.m16.1.1.1" xref="S1.F1.34.m16.1.1.1.cmml">=</mo><mn id="S1.F1.34.m16.1.1.3" xref="S1.F1.34.m16.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.34.m16.1c"><apply id="S1.F1.34.m16.1.1.cmml" xref="S1.F1.34.m16.1.1"><eq id="S1.F1.34.m16.1.1.1.cmml" xref="S1.F1.34.m16.1.1.1"></eq><ci id="S1.F1.34.m16.1.1.2.cmml" xref="S1.F1.34.m16.1.1.2">𝑛</ci><cn id="S1.F1.34.m16.1.1.3.cmml" type="integer" xref="S1.F1.34.m16.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.34.m16.1d">n=64</annotation><annotation encoding="application/x-llamapun" id="S1.F1.34.m16.1e">italic_n = 64</annotation></semantics></math>; the black line traces <math alttext="\ell^{*}(n)" class="ltx_Math" display="inline" id="S1.F1.35.m17.1"><semantics id="S1.F1.35.m17.1b"><mrow id="S1.F1.35.m17.1.2" xref="S1.F1.35.m17.1.2.cmml"><msup id="S1.F1.35.m17.1.2.2" xref="S1.F1.35.m17.1.2.2.cmml"><mi id="S1.F1.35.m17.1.2.2.2" mathvariant="normal" xref="S1.F1.35.m17.1.2.2.2.cmml">ℓ</mi><mo id="S1.F1.35.m17.1.2.2.3" xref="S1.F1.35.m17.1.2.2.3.cmml">*</mo></msup><mo id="S1.F1.35.m17.1.2.1" xref="S1.F1.35.m17.1.2.1.cmml">⁢</mo><mrow id="S1.F1.35.m17.1.2.3.2" xref="S1.F1.35.m17.1.2.cmml"><mo id="S1.F1.35.m17.1.2.3.2.1" stretchy="false" xref="S1.F1.35.m17.1.2.cmml">(</mo><mi id="S1.F1.35.m17.1.1" xref="S1.F1.35.m17.1.1.cmml">n</mi><mo id="S1.F1.35.m17.1.2.3.2.2" stretchy="false" xref="S1.F1.35.m17.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.35.m17.1c"><apply id="S1.F1.35.m17.1.2.cmml" xref="S1.F1.35.m17.1.2"><times id="S1.F1.35.m17.1.2.1.cmml" xref="S1.F1.35.m17.1.2.1"></times><apply id="S1.F1.35.m17.1.2.2.cmml" xref="S1.F1.35.m17.1.2.2"><csymbol cd="ambiguous" id="S1.F1.35.m17.1.2.2.1.cmml" xref="S1.F1.35.m17.1.2.2">superscript</csymbol><ci id="S1.F1.35.m17.1.2.2.2.cmml" xref="S1.F1.35.m17.1.2.2.2">ℓ</ci><times id="S1.F1.35.m17.1.2.2.3.cmml" xref="S1.F1.35.m17.1.2.2.3"></times></apply><ci id="S1.F1.35.m17.1.1.cmml" xref="S1.F1.35.m17.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.35.m17.1d">\ell^{*}(n)</annotation><annotation encoding="application/x-llamapun" id="S1.F1.35.m17.1e">roman_ℓ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math>, the minimum of the angular distance across the different sized layer blocks.
<em class="ltx_emph ltx_font_italic" id="S1.F1.43.4">(d)</em> results of pruning Llama-2-70B with healing (dark blue) and without healing (light blue) as a function of the fraction of layers removed: the top (middle) panel gives the accuracy on the MMLU (BoolQ) question-answering benchmark, while the bottom panel the autoregressive loss on a subset of the C4 validation set;
here, the dashed red lines (dashed gray lines) indicate the accuracy or loss of the original unpruned model (of random guessing);
these plots illustrate that typical behavior we find in which there are sharp transitions in performance for the accuracy of question-answering tasks (here between 40%-50% pruning fraction), but continuity and very slow growth in the healed loss (dark blue) up to at least to 80% pruning fraction.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">프루닝은 추론 공간을 줄이는 데 유용할 뿐만 아니라 네트워크가 매개 변수를 사용하는 방법을 이해하는 데 유용합니다. 네트워크의 성능에 미치는 영향을 최소화하면서 네트워크의 큰 블록을 제거할 수 있다면 해당 블록은 그다지 중요하지 않을 수 있습니다. 특히, 계층 드롭에 대한 우리의 직관은 변압기 아키텍처의 잔류 구조를 고려하게 된다. 보다 상세하게, 최종 레이어의 출력은 모든 모델 레이어의 출력과 임베디드 입력에 대한 합으로 분해될 수 있다. 그러한 합이 많고 독립적인 항을 가지고 있다면, 그 중 몇 개를 제거하는 것은 산출량을 크게 변화시키지 않아야 한다. 그러나 항이 독립적이지 않기 때문에 각 계층은 다음 계층에 입력됩니다. 특정 계층의 잔류 기여도가 작은 경우 항을 제거할 수 있을 것으로 예상해야 합니다. 즉, 각 계층의 출력이 계층에서 계층으로 너무 많이 변화하지 않는 경우. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This is strongly suggested by “lens” investigations that studied the evolution of the token distribution as a function of layer index such as the “logit lens” <cite class="ltx_cite ltx_citemacro_cite">nostalgebraist (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib21" title="">2020</a>)</cite> and the “tuned lens” <cite class="ltx_cite ltx_citemacro_cite">Belrose et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib22" title="">2023</a>)</cite>. A separate line of reasoning along these lines previously inspired neural ODEs <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib23" title="">2018</a>)</cite>, and led Ref. <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib24" title="">2023</a>)</cite> to argue that ideally representation should change substantially from layer to layer in order to most effectively make use of the parameters of a network. </span></span></span></p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">레이어 프루닝과 함께, 서로 다른 분리에서 레이어 표현들의 유사성을 조사하고, 더 깊은 레이어들이 (매우 최종 레이어를 제외하고) 얕은 레이어들보다 이웃 레이어들과 질적으로 더 유사하다는 것을 광범위하게 발견한다. 이것은 더 간단한 가지치기 전략을 제안한다: 끝에서 두 번째 층에서 시작하는 층을 제거하고 원하는 수의 층이 제거될 때까지 깊은 층에서 얕은 층으로 진행한다. <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>This strategy is especially interesting in situations where resource constraints inhibit the full application of the similarity-informed pruning algorithm described in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‣ 4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>(a).</span></span></span> 이 경우, 적은 양의 QLoRA 미세 조정으로 손상을 치유한 후, 더 관련된 유사성 정보 레이어 가지치기 전략과 거의 일치하는 성능을 얻을 수 있음을 발견했다. 이 방법의 효과는 LLM이 네트워크의 더 깊은 계층에서 매개변수를 적절하게 활용하지 못할 수 있다는 증거이다.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">전반적으로 이 세 가지 불렛 포인트를 가져가시기 바랍니다.</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">모델의 메모리 풋프린트 <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.1"> 및</em> 추론 시간은 제거된 레이어의 수에 따라 선형적으로 감소한다. <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Contrast this with quantization: the memory footprint decreases with the quantization ratio, but the inference time remains approximately fixed since parameters are typically de-quantized before any FLOPs.</span></span></span> 이것은 특히 모델의 성능이 레이어 드롭에 강한 경우 레이어 프루닝을 강력한 도구로 만듭니다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">가지치기, PEFT 및 양자화의 모든 효율성 방법은 서로 효과적으로 결합될 수 있다. 따라서 이 작업에서 <em class="ltx_emph ltx_font_italic" id="S1.I1.i2.p1.1.1">각 실험은 단일 A100 GPU</em>에서 수행되었으며 오픈 소스 및 학술 커뮤니티에 쉽게 액세스할 수 있다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">심층 제거에 대한 모델의 견고성, 다운스트림 지식 태스크(예: MMLU 및 BoolQ)에 대한 성능의 급격한 전환, 그리고 이러한 가지치기 분수에 대한 자기회귀 손실의 부드러운 행동은 모두 얕은 층이 지식을 저장하는 데 중요한 역할을 할 수 있음을 시사한다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">본 논문의 구성은 다음과 같다. §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2" title="2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>에서 우리는 먼저 우리의 작업에 동기를 부여하는 실용적인 사후 훈련 전략과 과학 심화 학습 조사에 대한 문헌 검토를 수행한다. 그런 다음, §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3" title="3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a>에서는 계층 가지치기 전략에 대한 직관을 부여하고 방법을 자세히 설명하는 반면, §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4" title="4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4</span></a>에서는 모든 실험 결과를 반복한다. 마지막으로 향후 작업의 방향을 강조하여 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S5" title="5 Discussion and Future Directions ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">5</span></a>로 결론을 맺는다. 특정 모델, 미세 조정, 데이터 세트 및 평가 세부 사항은 부록 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1" title="Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A</span></a>에서 찾을 수 있으며, 평가 삭제는 부록 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2" title="Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B</span></a>에서 찾을 수 있다.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text ltx_font_bold" id="S1.p8.1.1">Note:</span> 우리가 이 작업을 마무리하면서 Ref의 preprint. <cite class="ltx_cite ltx_citemacro_cite">Men et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib25" title="">2024</a>)</cite>가 게시되었는데, 이는 우리의 작업과 겹치는 점이 많다.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Review</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">이 섹션에서는 훈련 후 효율성을 위한 실용적인 전략을 검토하고 접근법에 대한 동기 부여 또는 통찰력을 제공하는 몇 가지 과학적 조사에 대해 논의합니다. 먼저 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS1" title="2.1 Pruning ‣ 2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2.1</span></a>에서 가지치기 역사를 검토한 다음 LLMs에 대한 현대적 적용에 대해 논의합니다. §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS2" title="2.2 Model distillation ‣ 2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2.2</span></a>에서 가지치기를 LLM의 매개변수 수를 줄이기 위한 대안 전략인 증류와 대조합니다. 그리고 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS3" title="2.3 Efficient finetuning and inference acceleration ‣ 2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2.3</span></a>에서 가지치기 전략과 함께 사용할 수 있는 효율적인 미세 조정 및 추론 가속을 위한 다양한 실용적인 방법에 대해 논의합니다. 마지막으로 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S2.SS4" title="2.4 A breadth of depth-dependent studies ‣ 2 Literature Review ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2.4</span></a>에서 결과에 보완적인 LLM의 깊이 의존적 통계적 특성에 대한 몇 가지 과학적 조사를 강조합니다.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Pruning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">Pruning</em>은 그룹으로서 개별적으로 또는 함께 불필요한 파라미터를 제거함으로써 학습된 머신-러닝 모델의 크기를 줄이는 방법이다. 신경망을 위한 프루닝은 긴 이력 <cite class="ltx_cite ltx_citemacro_citep">(LeCun et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib14" title="">1989</a>; Hassibi and Stork, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib15" title="">1992</a>)</cite>를 가지며, 원래 구상된 바와 같이 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.2">unstructured pruning</em> 기법은 미리 정의된 기준에 따라 개별 파라미터를 제거함으로써 네트워크를 희소화한다. 예를 들어 모델의 매개 변수가 매우 작은 값을 갖는 경우 제거(즉, 정확히 0으로 설정)하면 성능에 최소한의 영향을 미칠 수 있습니다. 이 초기 작업에 영감을 받은 현대 연구자들은 대부분 컴퓨터 비전 모델 <cite class="ltx_cite ltx_citemacro_citep">(Han et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib16" title="">2015</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib26" title="">2015</a>; Srinivas and Babu, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib27" title="">2015</a>)</cite>에 초점을 맞추어 이러한 비정형 가지치기에 대한 다양한 기준을 탐구하기 시작했다. 특히, 심판. <cite class="ltx_cite ltx_citemacro_cite">Han et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib16" title="">2015</a>)</cite>는 더 나은 압축 비율과 성능에 도달하기 위해 네트워크를 대안적으로 프루닝하고 미세 튜닝하기 위한 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.3">iterative pruning</em> 방법을 개발했다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">이러한 모델은 더 작았지만 반드시 더 효율적이지는 않았다: 기준에 따라 개별 매개변수를 제거하여 네트워크를 희소화하는 것은 희소성 <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib17" title="">2016</a>)</cite>를 위해 설계된 특수 하드웨어 또는 라이브러리 없이는 가속화하기 어려운 불규칙하거나 의사 무작위 희소화 패턴으로 이어진다. 이를 위해 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">structured pruning</em> 기술은 컨볼루션 네트워크의 특정 채널 또는 필터와 같은 관련 없는 매개 변수 그룹을 함께 제거하기 위해 개발되었다. 이를 통해 실제 관련성이 증가함에 따라 연구자들은 컴퓨터 비전 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib17" title="">2016</a>; Wen et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib28" title="">2016</a>; Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib29" title="">2016</a>; He et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib30" title="">2017</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib31" title="">2018</a>)</cite> 및 변환기 이전 NLP 아키텍처 <cite class="ltx_cite ltx_citemacro_citep">(Murray and Chiang, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib32" title="">2015</a>; See et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib33" title="">2016</a>; Kim and Rush, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib34" title="">2016</a>)</cite>에 걸쳐 구조화된 가지치기를 탐구하기 시작했다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">언어 모델링의 전례 없는 발전에 따라 최근 연구는 트랜스포머 <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib35" title="">2017</a>)</cite>에 구조화된 가지치기 방법을 적용하는 데 중점을 두었다. 이들 연구는 제거를 위한 모델 아키텍처의 거의 모든 가능한 구성 요소를 고려하는데, 드롭 어텐션 헤드 <cite class="ltx_cite ltx_citemacro_citep">(Voita et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib36" title="">2019</a>; Michel et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib37" title="">2019</a>; Kim and Awadalla, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib38" title="">2020</a>)</cite>, 드롭 층 <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib39" title="">2019</a>; Zhang and He, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib40" title="">2020</a>; Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib41" title="">2021</a>; Jha et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib42" title="">2023</a>; Sajjad et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib43" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib44" title="">2023a</a>)</cite>, 프루닝 히든 스테이트 <cite class="ltx_cite ltx_citemacro_citep">(Hou et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib45" title="">2020</a>)</cite>, 순위 감소 큰 가중치 매트릭스 <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib46" title="">2023</a>)</cite>, 희소 가중치 매트릭스를 더 작은 조밀한 것 <cite class="ltx_cite ltx_citemacro_cite">Ashkboos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib47" title="">2024</a>)</cite>, 전술한 그룹 <cite class="ltx_cite ltx_citemacro_citep">(Xia et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib48" title="">2022</a>; Lagunas et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib49" title="">2021</a>)</cite>의 많은 조합에 이르는 방법을 포함한다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">트랜스포머 레이어 드롭을 고려하는 선행 연구 중, 대부분의 <cite class="ltx_cite ltx_citemacro_cite">Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib39" title="">2019</a>); Zhang and He (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib40" title="">2020</a>); Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib41" title="">2021</a>); Xia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib48" title="">2022</a>); Sajjad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib43" title="">2023</a>)</cite> 연구에서는 BERT 스타일 모델 <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib50" title="">2018</a>)</cite>를 고려하고, 대규모 언어 모델링 및 생성에 가장 일반적으로 사용되는 디코더 전용 GPT 스타일 모델 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib1" title="">2019</a>)</cite>를 고려한다. BERT-스타일 모델은 양방향 마스킹 언어 모델링(MLM) 목적으로 인해 태스크를 이해하는 데 자연스럽게 적합하지만 GPT-스타일 모델은 자기회귀 목적으로 인해 생성에 적합하다. 이 분열은 더 강력한 GPT 스타일 모델 <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib51" title="">2023</a>)</cite>에 비추어 의문을 제기했지만 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">(Ethayarajh, <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib52" title="">2019</a>)</cite>는 단어의 계층별 표현의 진화 측면에서 BERT와 GPT 모델 간의 상당한 질적 차이를 발견했다. 전체적으로 이것은 레이어 드롭 전략이 두 가족 간에 다르게 행동할 것임을 시사한다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">BERT 스타일의 사전 훈련 모델에 대한 한 연구, Ref. <cite class="ltx_cite ltx_citemacro_cite">Sajjad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib43" title="">2023</a>)</cite>는 최상의 계층 프루닝 전략이 최종 계층을 드롭하는 것이라고 결론짓고, 이는 부분적으로 우리의 결과와 공명하지만, 대조적으로 모델의 마지막 몇 계층을 유지하는 일부 프루닝 크기에 대한 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.1">(a)</em>은 실제로 유익하고, 가장 마지막 계층을 유지하는 모든 프루닝 크기에 대한 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.2">(b)</em>은 필수적이다. 또한 저자는 접근법과 같이 다른 층의 표현 간의 유사성을 연구하지만 실제로 깊은 층에 비해 얕은 층의 표현 간에 더 높은 유사성을 발견했으며 이는 우리의 결과와 매우 첨예하게 일치하지 않는다. 중요하게도, 그 모델들은 Ref.에서 고려되었다. <cite class="ltx_cite ltx_citemacro_cite">Sajjad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib43" title="">2023</a>)</cite>는 몇억 개의 매개 변수로 구성되며, 이는 우리가 작업에서 고려하는 모델 규모보다 훨씬 작다. 아마도 결과적으로 저자는 가지치기된 모델도 미세 조정했음에도 불구하고 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS1" title="4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.1</span></a>에서 보고하는 다운스트림 정확도의 급격한 전환을 관찰하지 못했다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1">이와는 대조적으로, Ref. <cite class="ltx_cite ltx_citemacro_cite">Jha et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib42" title="">2023</a>)</cite>는 GPT 스타일 모델을 고려하지만, 방법론은 우리와 상당히 다르다: <em class="ltx_emph ltx_font_italic" id="S2.SS1.p6.1.1">(i)</em> 먼저 사전 훈련한 다음 고정 계층 드롭 전략을 사용하는 대신 저자는 수정된 사전 훈련 절차에서 층을 점진적으로 드롭합니다. 그리고 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p6.1.2">(ii)</em> 저자는 자체 하위 1B 매개 변수 모델을 연구하지만 실제 응용 프로그램에 일반적으로 사용 및/또는 미세 조정되는 쉽게 사용할 수 있는 개방형 가중치, 대규모 2.7B-70B 매개 변수 모델의 패밀리에 중점을 둡니다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p7">
<p class="ltx_p" id="S2.SS1.p7.1">마지막으로, 트랜스포머에서 계층 드롭에 대한 체계적인 접근법이 또한 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p7.1.1">wav2vec</em> 모델의 맥락에서 연구되었는데, 이는 음성을 임베딩에 매핑하는 인코더 전용 모델이며 수억-백만 파라미터 체제 <cite class="ltx_cite ltx_citemacro_cite">Baevski et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib53" title="">2020</a>)</cite>의 크기이다. 이 모델들로요, 재판장님 <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib44" title="">2023a</a>)</cite>는 레이어와 다운스트림 메트릭 간의 상관 관계를 기반으로 레이어 프루닝 알고리즘을 개발했다. 모델 아키텍처와 도메인 외에도, 이것과 우리의 작업 사이의 한 가지 중요한 차이점은 Ref입니다. <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib44" title="">2023a</a>)</cite>는 연속되지 않는 가지치기 제안, 예를 들어 대체 층을 떨어뜨리는 것으로 간주되었다. 레이어 프루닝에 대한 우리의 직관은 이것이 각 레이어 블록이 제거된 여러 불일치를 생성하기 때문에 적어도 디코더 전용 언어 모델에서도 작동하지 않아야 한다고 예측한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model distillation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">훈련된 기계-학습 모델의 크기를 감소시키는 완전히 다른 방법은 <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">모델 증류</em> <cite class="ltx_cite ltx_citemacro_cite">Hinton et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib54" title="">2015</a>)</cite>로, 교사가 예측한 분포에 대해 학생을 훈련시킴으로써 지식이 큰 "교사" 모델에서 작은 "학생" 모델로 전달된다. 본질적인 통찰은 이것이 교사의 매우 일반적인 지식과 능력을 보다 간소화되고 압축되며 아마도 기술 특유의 표상으로 변형시킬 수 있다는 것이다.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">매우 일반적인 기술이지만 언어 모델의 설정에서 증류는 <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1">(a)</em> white-box 접근법으로 구현되었으며, 이 접근법에서는 학생이 교사의 로짓 <cite class="ltx_cite ltx_citemacro_cite">Gu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib55" title="">2023</a>)</cite> 또는 숨겨진 상태 <cite class="ltx_cite ltx_citemacro_cite">Jiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib56" title="">2019</a>)</cite>를 모방하도록 훈련되며, 또한 <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.2">(b)</em> black-box 접근법에서는 학생이 교사에 의해 생성된 출력 토큰에만 액세스할 수 있다. 이 후자의 접근법은 합성 라벨 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib57" title="">2021</a>)</cite>를 추가하고, 학생의 추론 기술을 향상시키는 것을 목표로 하는 사고 추론 <cite class="ltx_cite ltx_citemacro_cite">Fu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib61" title="">2023</a>); Hsieh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib62" title="">2023</a>)</cite>의 체인을 제공하여 고품질의 합성 텍스트 <cite class="ltx_cite ltx_citemacro_cite">Eldan and Li (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib58" title="">2023</a>); Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib59" title="">2023a</a>); Gunasekar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib60" title="">2023</a>)</cite>를 생성하거나, 학생의 명령어 추종 능력 <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib63" title="">2023a</a>)</cite>를 향상시키는 명령어에 주석을 달아서 학생이 어떤 방식으로든 교사에 의해 증강되는 텍스트에 대해 훈련되는 경우를 광범위하게 다룬다.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">레이어 프루닝에 비해, 이러한 증류 방법은 데이터의 큰 코퍼스를 처리하기 위해 큰 교사에 의존하기 때문에 상당한 계산 자원이 필요하다. 대신, 유사성 기반 가지치기 전략은 사전 훈련 말뭉치의 작은 하위 집합에 있는 다른 계층에서 표현 간의 유사성을 계산하기만 하면 되는 반면, 두 번째 간단한 가지치기 전략은 가지치기 후 축소된 모델만 사용한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Efficient finetuning
and inference acceleration</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">모델의 크기를 직접 줄이는 데 보완적인 <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">parameter-efficient finetuning</em> (PEFT)는 특정 작업에 LLM을 전문화하는 비용을 줄이는 데 중점을 둡니다. 특히 LoRA(Low Rank Adapters)는 사전 훈련된 모델을 동결하고 매개변수적으로 적은 수의 추가 훈련 가능한 가중치 <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib13" title="">2021</a>)</cite>를 도입함으로써 메모리 및 미세 튜닝의 계산을 감소시킨다. QLoRA <cite class="ltx_cite ltx_citemacro_cite">Dettmers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>의 양자화된 사촌을 사용하여 실험 비용을 효율적으로 유지한다. 우리의 작업과 결합될 수 있는 다른 PEFT 방법은 Refs이다. <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib64" title="">2023b</a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib65" title="">2023</a>)</cite>: 첫째, LoRA 행렬들의 초기화는 양자화 방식으로 조정되고, 둘째, 서로 다른 LLM 모듈들에 대한 LoRA 순위들은 적응적 방식으로 선택된다.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">추가적인 효율성 향상을 위해 레이어 프루닝 모델을 추론을 더욱 가속화하는 방법과 결합할 수 있습니다. 추측적 디코딩 <cite class="ltx_cite ltx_citemacro_cite">Leviathan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib66" title="">2023</a>)</cite>에서는 토큰이 더 작은 드래프트 모델에서 빠르게 생성된 다음 메인 모델에 의해 병렬로 평가되고, 메두사 <cite class="ltx_cite ltx_citemacro_cite">Cai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib67" title="">2024</a>)</cite>에서는 드래프트 모델이 추가 디코딩 헤드에 대해 폐기되지만 궁극적으로 유사한 효과를 달성합니다. 특히, 고도로 압축된 계층 프루닝된 모델들을 추측적 디코딩 셋업에서 잠재적인 드래프트 모델들로서 고려하는 것은 흥미로울 수 있다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>A breadth of depth-dependent studies</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">마지막으로 LLM의 깊이 의존적 특성을 연구하는 몇 가지 과학적 작업을 강조하겠습니다. 한 가지 관련 방향은 지식과 언어 속성이 언어 모델에서 인코딩되는 방식을 고려한다. 한편으론 심판관님 <cite class="ltx_cite ltx_citemacro_cite">Meng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib68" title="">2022</a>); Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib69" title="">2021</a>)</cite> <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.1">storage and recall</em> factual associations: 이러한 작업은 지식이 중간 <cite class="ltx_cite ltx_citemacro_cite">Meng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib68" title="">2022</a>)</cite> 또는 최종 <cite class="ltx_cite ltx_citemacro_cite">Dai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib69" title="">2021</a>)</cite> 레이어 내에서 로컬화된다는 점을 강조하며, 이는 모델의 사실적 지식의 일부를 직접 편집하거나 지우는 데 영향을 미친다. 한편, 이러한 편집을 수행하려는 시도는 정보가 계층 <cite class="ltx_cite ltx_citemacro_cite">Hase et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib70" title="">2023</a>)</cite>에 걸쳐 비지역적으로 저장될 수 있다는 증거를 제공한다. 관련해서요, 재판장님 <cite class="ltx_cite ltx_citemacro_cite">Geva et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib71" title="">2023</a>)</cite>는 사실들이 <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.2">processed</em> during inference, distinguish the role of attention head, for attribute extraction, and the MLP block, for subject enrichment: both are delocalized across several layers.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">다음으로, 앞의 "논리 렌즈" <cite class="ltx_cite ltx_citemacro_cite">nostalgebraist (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib21" title="">2020</a>)</cite>, Ref. <cite class="ltx_cite ltx_citemacro_cite">Belrose et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib22" title="">2023</a>)</cite>는 중간 표현을 토큰에 대한 분포로 변환하기 위해 학습 가능한 어파인 변환을 사용하여 <em class="ltx_emph ltx_font_italic" id="S2.SS4.p2.1.1">예측의 궤적</em>을 연구하기 위해 "튜닝 렌즈"라고 불리는 기술을 발명했다(또한 <cite class="ltx_cite ltx_citemacro_cite">Din et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib72" title="">2023</a>)</cite> 참조). 이 분포의 층간 역학을 연구함으로써 저자는 수렴하는 경향이 있다고 언급했다. 이 수렴은 더 깊은 층이 가지치기할 수 있다는 것을 매우 암시하는 반면, 아핀 프로브를 훈련시켜야 한다는 사실은 최종 층을 가지치기할 수 없다는 우리의 관찰과 관련이 있을 수 있다. 그런 셈이죠, 재판장님 <cite class="ltx_cite ltx_citemacro_cite">Gurnee and Tegmark (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib73" title="">2023</a>)</cite>는 활성화가 중간보다 깊은 한, 기본 텍스트의 지리적 특징이 중간 활성화에 대해 훈련된 선형 프로브로부터 결정될 수 있음을 관찰했다.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">좀 더 추상적으로요, 심판님 <cite class="ltx_cite ltx_citemacro_cite">Voita et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib74" title="">2023</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib75" title="">2023b</a>)</cite>는 활성화의 희소성이 네트워크의 전진 패스를 중간쯤에서 전환하여 희소성에서 조밀성으로 진화하는 것을 발견했다. 관련이 있을 겁니다, 재판장님 <cite class="ltx_cite ltx_citemacro_cite">Panigrahi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib76" title="">2023</a>)</cite>는 미세 조정 중에 어떤 모델 가중치가 가장 많이 업데이트되는지 조사하여 중간층에 있는 가중치임을 발견했다.</p>
</div>
<div class="ltx_para" id="S2.SS4.p4">
<p class="ltx_p" id="S2.SS4.p4.1">전체적으로 이러한 깊은 연구는 우리의 연구와 보완적이며, 한편으로는 LLM의 가장 깊은 층을 제거하는 것이 모델의 성능을 크게 변경하지 않는다는 증거를 제공하고 다른 한편으로는 LLM의 가장 깊은 층의 약 절반을 제거한 후 날카로운 가지치기 전환을 보여준다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">이 절에서는 계층 가지치기(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS1" title="3.1 Intuition ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.1</span></a>)가 작동하는 이유에 대한 직관을 제시하고, 그 방법을 자세히 설명한다(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Intuition</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.8">레이어 드롭에 대한 우리의 직관은 표현들을 천천히 변화하는 레이어 인덱스의 함수로 생각하는 데서 비롯된다. 특히, 트랜스포머에 대한 표현의 계층 간 진화는 <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.8.1">residual</em>iteration equation에 의해 주어진다.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(\ell+1)}=x^{(\ell)}+f(x^{(\ell)},\theta^{(\ell)})\,," class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.1.cmml"><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><msup id="S3.E1.m1.5.5.1.1.4" xref="S3.E1.m1.5.5.1.1.4.cmml"><mi id="S3.E1.m1.5.5.1.1.4.2" xref="S3.E1.m1.5.5.1.1.4.2.cmml">x</mi><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.2" mathvariant="normal" xref="S3.E1.m1.1.1.1.1.1.2.cmml">ℓ</mi><mo id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E1.m1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml"><msup id="S3.E1.m1.5.5.1.1.2.4" xref="S3.E1.m1.5.5.1.1.2.4.cmml"><mi id="S3.E1.m1.5.5.1.1.2.4.2" xref="S3.E1.m1.5.5.1.1.2.4.2.cmml">x</mi><mrow id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.5.5.1.1.2.4.cmml"><mo id="S3.E1.m1.2.2.1.3.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.4.cmml">(</mo><mi id="S3.E1.m1.2.2.1.1" mathvariant="normal" xref="S3.E1.m1.2.2.1.1.cmml">ℓ</mi><mo id="S3.E1.m1.2.2.1.3.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.4.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.5.5.1.1.2.3" xref="S3.E1.m1.5.5.1.1.2.3.cmml">+</mo><mrow id="S3.E1.m1.5.5.1.1.2.2" xref="S3.E1.m1.5.5.1.1.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2.4" xref="S3.E1.m1.5.5.1.1.2.2.4.cmml">f</mi><mo id="S3.E1.m1.5.5.1.1.2.2.3" xref="S3.E1.m1.5.5.1.1.2.2.3.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.1.1.2.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml"><mo id="S3.E1.m1.5.5.1.1.2.2.2.2.3" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml">(</mo><msup id="S3.E1.m1.5.5.1.1.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.3.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.3.3.1.1" mathvariant="normal" xref="S3.E1.m1.3.3.1.1.cmml">ℓ</mi><mo id="S3.E1.m1.3.3.1.3.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.5.5.1.1.2.2.2.2.4" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml">,</mo><msup id="S3.E1.m1.5.5.1.1.2.2.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.5.5.1.1.2.2.2.2.2.2" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.2.cmml">θ</mi><mrow id="S3.E1.m1.4.4.1.3" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml"><mo id="S3.E1.m1.4.4.1.3.1" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml">(</mo><mi id="S3.E1.m1.4.4.1.1" mathvariant="normal" xref="S3.E1.m1.4.4.1.1.cmml">ℓ</mi><mo id="S3.E1.m1.4.4.1.3.2" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.5.5.1.1.2.2.2.2.5" rspace="0.170em" stretchy="false" xref="S3.E1.m1.5.5.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1"><eq id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"></eq><apply id="S3.E1.m1.5.5.1.1.4.cmml" xref="S3.E1.m1.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.4.1.cmml" xref="S3.E1.m1.5.5.1.1.4">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.4.2.cmml" xref="S3.E1.m1.5.5.1.1.4.2">𝑥</ci><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.2">ℓ</ci><cn id="S3.E1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"><plus id="S3.E1.m1.5.5.1.1.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.3"></plus><apply id="S3.E1.m1.5.5.1.1.2.4.cmml" xref="S3.E1.m1.5.5.1.1.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.4.1.cmml" xref="S3.E1.m1.5.5.1.1.2.4">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.4.2.cmml" xref="S3.E1.m1.5.5.1.1.2.4.2">𝑥</ci><ci id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1">ℓ</ci></apply><apply id="S3.E1.m1.5.5.1.1.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2"><times id="S3.E1.m1.5.5.1.1.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.3"></times><ci id="S3.E1.m1.5.5.1.1.2.2.4.cmml" xref="S3.E1.m1.5.5.1.1.2.2.4">𝑓</ci><interval closure="open" id="S3.E1.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2"><apply id="S3.E1.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">ℓ</ci></apply><apply id="S3.E1.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.5.5.1.1.2.2.2.2.2.2">𝜃</ci><ci id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1">ℓ</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">x^{(\ell+1)}=x^{(\ell)}+f(x^{(\ell)},\theta^{(\ell)})\,,</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_x start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT = italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT + italic_f ( italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT , italic_θ start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.5">여기서, 각각 <math alttext="(x^{(\ell)}" class="ltx_math_unparsed" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1b"><mo id="S3.SS1.p1.1.m1.1.2" stretchy="false">(</mo><msup id="S3.SS1.p1.1.m1.1.3"><mi id="S3.SS1.p1.1.m1.1.3.2">x</mi><mrow id="S3.SS1.p1.1.m1.1.1.1.3"><mo id="S3.SS1.p1.1.m1.1.1.1.3.1" stretchy="false">(</mo><mi id="S3.SS1.p1.1.m1.1.1.1.1" mathvariant="normal">ℓ</mi><mo id="S3.SS1.p1.1.m1.1.1.1.3.2" stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">(x^{(\ell)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">( italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\theta^{(\ell)})" class="ltx_math_unparsed" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1b"><msup id="S3.SS1.p1.2.m2.1.2"><mi id="S3.SS1.p1.2.m2.1.2.2">θ</mi><mrow id="S3.SS1.p1.2.m2.1.1.1.3"><mo id="S3.SS1.p1.2.m2.1.1.1.3.1" stretchy="false">(</mo><mi id="S3.SS1.p1.2.m2.1.1.1.1" mathvariant="normal">ℓ</mi><mo id="S3.SS1.p1.2.m2.1.1.1.3.2" stretchy="false">)</mo></mrow></msup><mo id="S3.SS1.p1.2.m2.1.3" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\theta^{(\ell)})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_θ start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT )</annotation></semantics></math>는 레이어 <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p1.3.m3.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">roman_ℓ</annotation></semantics></math>, <math alttext="f(x,\theta)" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.2"><semantics id="S3.SS1.p1.4.m4.2a"><mrow id="S3.SS1.p1.4.m4.2.3" xref="S3.SS1.p1.4.m4.2.3.cmml"><mi id="S3.SS1.p1.4.m4.2.3.2" xref="S3.SS1.p1.4.m4.2.3.2.cmml">f</mi><mo id="S3.SS1.p1.4.m4.2.3.1" xref="S3.SS1.p1.4.m4.2.3.1.cmml">⁢</mo><mrow id="S3.SS1.p1.4.m4.2.3.3.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml"><mo id="S3.SS1.p1.4.m4.2.3.3.2.1" stretchy="false" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">(</mo><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">x</mi><mo id="S3.SS1.p1.4.m4.2.3.3.2.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">,</mo><mi id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml">θ</mi><mo id="S3.SS1.p1.4.m4.2.3.3.2.3" stretchy="false" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.2b"><apply id="S3.SS1.p1.4.m4.2.3.cmml" xref="S3.SS1.p1.4.m4.2.3"><times id="S3.SS1.p1.4.m4.2.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.1"></times><ci id="S3.SS1.p1.4.m4.2.3.2.cmml" xref="S3.SS1.p1.4.m4.2.3.2">𝑓</ci><interval closure="open" id="S3.SS1.p1.4.m4.2.3.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.3.2"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑥</ci><ci id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2">𝜃</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.2c">f(x,\theta)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.2d">italic_f ( italic_x , italic_θ )</annotation></semantics></math>는 하나의 멀티 헤드 셀프 어텐션 <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.5.1"> 및</em>MLP 레이어 블록의 변환을 설명한다. 모든 잔차 네트워크에 대해, 만약 우리가 이 반복을 언롤하면, 우리는 <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_L</annotation></semantics></math> 전체 계층 이후에 출력이 모든 계층의 변환에 대한 합으로 기술된다는 것을 알 수 있다.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(L)}=x^{(0)}+\sum_{\ell=0}^{L-1}f(x^{(\ell)},\theta^{(\ell)})\,." class="ltx_Math" display="block" id="S3.E2.m1.5"><semantics id="S3.E2.m1.5a"><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.1.cmml"><mrow id="S3.E2.m1.5.5.1.1" xref="S3.E2.m1.5.5.1.1.cmml"><msup id="S3.E2.m1.5.5.1.1.4" xref="S3.E2.m1.5.5.1.1.4.cmml"><mi id="S3.E2.m1.5.5.1.1.4.2" xref="S3.E2.m1.5.5.1.1.4.2.cmml">x</mi><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.5.5.1.1.4.cmml"><mo id="S3.E2.m1.1.1.1.3.1" stretchy="false" xref="S3.E2.m1.5.5.1.1.4.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">L</mi><mo id="S3.E2.m1.1.1.1.3.2" stretchy="false" xref="S3.E2.m1.5.5.1.1.4.cmml">)</mo></mrow></msup><mo id="S3.E2.m1.5.5.1.1.3" xref="S3.E2.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.5.5.1.1.2" xref="S3.E2.m1.5.5.1.1.2.cmml"><msup id="S3.E2.m1.5.5.1.1.2.4" xref="S3.E2.m1.5.5.1.1.2.4.cmml"><mi id="S3.E2.m1.5.5.1.1.2.4.2" xref="S3.E2.m1.5.5.1.1.2.4.2.cmml">x</mi><mrow id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.5.5.1.1.2.4.cmml"><mo id="S3.E2.m1.2.2.1.3.1" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.4.cmml">(</mo><mn id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml">0</mn><mo id="S3.E2.m1.2.2.1.3.2" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.4.cmml">)</mo></mrow></msup><mo id="S3.E2.m1.5.5.1.1.2.3" rspace="0.055em" xref="S3.E2.m1.5.5.1.1.2.3.cmml">+</mo><mrow id="S3.E2.m1.5.5.1.1.2.2" xref="S3.E2.m1.5.5.1.1.2.2.cmml"><munderover id="S3.E2.m1.5.5.1.1.2.2.3" xref="S3.E2.m1.5.5.1.1.2.2.3.cmml"><mo id="S3.E2.m1.5.5.1.1.2.2.3.2.2" movablelimits="false" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E2.m1.5.5.1.1.2.2.3.2.3" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.3.2.3.2" mathvariant="normal" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.2.cmml">ℓ</mi><mo id="S3.E2.m1.5.5.1.1.2.2.3.2.3.1" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.1.cmml">=</mo><mn id="S3.E2.m1.5.5.1.1.2.2.3.2.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.3.cmml">0</mn></mrow><mrow id="S3.E2.m1.5.5.1.1.2.2.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.3.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.3.3.2" xref="S3.E2.m1.5.5.1.1.2.2.3.3.2.cmml">L</mi><mo id="S3.E2.m1.5.5.1.1.2.2.3.3.1" xref="S3.E2.m1.5.5.1.1.2.2.3.3.1.cmml">−</mo><mn id="S3.E2.m1.5.5.1.1.2.2.3.3.3" xref="S3.E2.m1.5.5.1.1.2.2.3.3.3.cmml">1</mn></mrow></munderover><mrow id="S3.E2.m1.5.5.1.1.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.2.4" xref="S3.E2.m1.5.5.1.1.2.2.2.4.cmml">f</mi><mo id="S3.E2.m1.5.5.1.1.2.2.2.3" xref="S3.E2.m1.5.5.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S3.E2.m1.5.5.1.1.2.2.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml"><mo id="S3.E2.m1.5.5.1.1.2.2.2.2.2.3" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml">(</mo><msup id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E2.m1.3.3.1.3" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.3.3.1.3.1" stretchy="false" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.3.3.1.1" mathvariant="normal" xref="S3.E2.m1.3.3.1.1.cmml">ℓ</mi><mo id="S3.E2.m1.3.3.1.3.2" stretchy="false" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E2.m1.5.5.1.1.2.2.2.2.2.4" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml">,</mo><msup id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.2" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.2.cmml">θ</mi><mrow id="S3.E2.m1.4.4.1.3" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml"><mo id="S3.E2.m1.4.4.1.3.1" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml">(</mo><mi id="S3.E2.m1.4.4.1.1" mathvariant="normal" xref="S3.E2.m1.4.4.1.1.cmml">ℓ</mi><mo id="S3.E2.m1.4.4.1.3.2" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml">)</mo></mrow></msup><mo id="S3.E2.m1.5.5.1.1.2.2.2.2.2.5" stretchy="false" xref="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.5.5.1.2" lspace="0.170em" xref="S3.E2.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.5b"><apply id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.5.5.1"><eq id="S3.E2.m1.5.5.1.1.3.cmml" xref="S3.E2.m1.5.5.1.1.3"></eq><apply id="S3.E2.m1.5.5.1.1.4.cmml" xref="S3.E2.m1.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.4.1.cmml" xref="S3.E2.m1.5.5.1.1.4">superscript</csymbol><ci id="S3.E2.m1.5.5.1.1.4.2.cmml" xref="S3.E2.m1.5.5.1.1.4.2">𝑥</ci><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝐿</ci></apply><apply id="S3.E2.m1.5.5.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.2"><plus id="S3.E2.m1.5.5.1.1.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.3"></plus><apply id="S3.E2.m1.5.5.1.1.2.4.cmml" xref="S3.E2.m1.5.5.1.1.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.4.1.cmml" xref="S3.E2.m1.5.5.1.1.2.4">superscript</csymbol><ci id="S3.E2.m1.5.5.1.1.2.4.2.cmml" xref="S3.E2.m1.5.5.1.1.2.4.2">𝑥</ci><cn id="S3.E2.m1.2.2.1.1.cmml" type="integer" xref="S3.E2.m1.2.2.1.1">0</cn></apply><apply id="S3.E2.m1.5.5.1.1.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2"><apply id="S3.E2.m1.5.5.1.1.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3">superscript</csymbol><apply id="S3.E2.m1.5.5.1.1.2.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.3.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3">subscript</csymbol><sum id="S3.E2.m1.5.5.1.1.2.2.3.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.2"></sum><apply id="S3.E2.m1.5.5.1.1.2.2.3.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3"><eq id="S3.E2.m1.5.5.1.1.2.2.3.2.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.1"></eq><ci id="S3.E2.m1.5.5.1.1.2.2.3.2.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.2">ℓ</ci><cn id="S3.E2.m1.5.5.1.1.2.2.3.2.3.3.cmml" type="integer" xref="S3.E2.m1.5.5.1.1.2.2.3.2.3.3">0</cn></apply></apply><apply id="S3.E2.m1.5.5.1.1.2.2.3.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.3"><minus id="S3.E2.m1.5.5.1.1.2.2.3.3.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.3.1"></minus><ci id="S3.E2.m1.5.5.1.1.2.2.3.3.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.3.3.2">𝐿</ci><cn id="S3.E2.m1.5.5.1.1.2.2.3.3.3.cmml" type="integer" xref="S3.E2.m1.5.5.1.1.2.2.3.3.3">1</cn></apply></apply><apply id="S3.E2.m1.5.5.1.1.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2"><times id="S3.E2.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.3"></times><ci id="S3.E2.m1.5.5.1.1.2.2.2.4.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.4">𝑓</ci><interval closure="open" id="S3.E2.m1.5.5.1.1.2.2.2.2.3.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2"><apply id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.5.5.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1">ℓ</ci></apply><apply id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2">superscript</csymbol><ci id="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.5.5.1.1.2.2.2.2.2.2.2">𝜃</ci><ci id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1.1">ℓ</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.5c">x^{(L)}=x^{(0)}+\sum_{\ell=0}^{L-1}f(x^{(\ell)},\theta^{(\ell)})\,.</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.5d">italic_x start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT = italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT + ∑ start_POSTSUBSCRIPT roman_ℓ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT italic_f ( italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT , italic_θ start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.7">합계의 용어가 <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.7.1">numerous</em>, (<math alttext="L\gg 1" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m1.1"><semantics id="S3.SS1.p1.6.m1.1a"><mrow id="S3.SS1.p1.6.m1.1.1" xref="S3.SS1.p1.6.m1.1.1.cmml"><mi id="S3.SS1.p1.6.m1.1.1.2" xref="S3.SS1.p1.6.m1.1.1.2.cmml">L</mi><mo id="S3.SS1.p1.6.m1.1.1.1" xref="S3.SS1.p1.6.m1.1.1.1.cmml">≫</mo><mn id="S3.SS1.p1.6.m1.1.1.3" xref="S3.SS1.p1.6.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m1.1b"><apply id="S3.SS1.p1.6.m1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.6.m1.1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1.1">much-greater-than</csymbol><ci id="S3.SS1.p1.6.m1.1.1.2.cmml" xref="S3.SS1.p1.6.m1.1.1.2">𝐿</ci><cn id="S3.SS1.p1.6.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.6.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m1.1c">L\gg 1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m1.1d">italic_L ≫ 1</annotation></semantics></math>)이고, <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.7.2">independent</em>, 예를 들어 블록 함수가 대신 전체 입력의 함수인 경우 <math alttext="f(x^{(0)},\theta^{(\ell)})" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m2.4"><semantics id="S3.SS1.p1.7.m2.4a"><mrow id="S3.SS1.p1.7.m2.4.4" xref="S3.SS1.p1.7.m2.4.4.cmml"><mi id="S3.SS1.p1.7.m2.4.4.4" xref="S3.SS1.p1.7.m2.4.4.4.cmml">f</mi><mo id="S3.SS1.p1.7.m2.4.4.3" xref="S3.SS1.p1.7.m2.4.4.3.cmml">⁢</mo><mrow id="S3.SS1.p1.7.m2.4.4.2.2" xref="S3.SS1.p1.7.m2.4.4.2.3.cmml"><mo id="S3.SS1.p1.7.m2.4.4.2.2.3" stretchy="false" xref="S3.SS1.p1.7.m2.4.4.2.3.cmml">(</mo><msup id="S3.SS1.p1.7.m2.3.3.1.1.1" xref="S3.SS1.p1.7.m2.3.3.1.1.1.cmml"><mi id="S3.SS1.p1.7.m2.3.3.1.1.1.2" xref="S3.SS1.p1.7.m2.3.3.1.1.1.2.cmml">x</mi><mrow id="S3.SS1.p1.7.m2.1.1.1.3" xref="S3.SS1.p1.7.m2.3.3.1.1.1.cmml"><mo id="S3.SS1.p1.7.m2.1.1.1.3.1" stretchy="false" xref="S3.SS1.p1.7.m2.3.3.1.1.1.cmml">(</mo><mn id="S3.SS1.p1.7.m2.1.1.1.1" xref="S3.SS1.p1.7.m2.1.1.1.1.cmml">0</mn><mo id="S3.SS1.p1.7.m2.1.1.1.3.2" stretchy="false" xref="S3.SS1.p1.7.m2.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.SS1.p1.7.m2.4.4.2.2.4" xref="S3.SS1.p1.7.m2.4.4.2.3.cmml">,</mo><msup id="S3.SS1.p1.7.m2.4.4.2.2.2" xref="S3.SS1.p1.7.m2.4.4.2.2.2.cmml"><mi id="S3.SS1.p1.7.m2.4.4.2.2.2.2" xref="S3.SS1.p1.7.m2.4.4.2.2.2.2.cmml">θ</mi><mrow id="S3.SS1.p1.7.m2.2.2.1.3" xref="S3.SS1.p1.7.m2.4.4.2.2.2.cmml"><mo id="S3.SS1.p1.7.m2.2.2.1.3.1" stretchy="false" xref="S3.SS1.p1.7.m2.4.4.2.2.2.cmml">(</mo><mi id="S3.SS1.p1.7.m2.2.2.1.1" mathvariant="normal" xref="S3.SS1.p1.7.m2.2.2.1.1.cmml">ℓ</mi><mo id="S3.SS1.p1.7.m2.2.2.1.3.2" stretchy="false" xref="S3.SS1.p1.7.m2.4.4.2.2.2.cmml">)</mo></mrow></msup><mo id="S3.SS1.p1.7.m2.4.4.2.2.5" stretchy="false" xref="S3.SS1.p1.7.m2.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m2.4b"><apply id="S3.SS1.p1.7.m2.4.4.cmml" xref="S3.SS1.p1.7.m2.4.4"><times id="S3.SS1.p1.7.m2.4.4.3.cmml" xref="S3.SS1.p1.7.m2.4.4.3"></times><ci id="S3.SS1.p1.7.m2.4.4.4.cmml" xref="S3.SS1.p1.7.m2.4.4.4">𝑓</ci><interval closure="open" id="S3.SS1.p1.7.m2.4.4.2.3.cmml" xref="S3.SS1.p1.7.m2.4.4.2.2"><apply id="S3.SS1.p1.7.m2.3.3.1.1.1.cmml" xref="S3.SS1.p1.7.m2.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m2.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.7.m2.3.3.1.1.1">superscript</csymbol><ci id="S3.SS1.p1.7.m2.3.3.1.1.1.2.cmml" xref="S3.SS1.p1.7.m2.3.3.1.1.1.2">𝑥</ci><cn id="S3.SS1.p1.7.m2.1.1.1.1.cmml" type="integer" xref="S3.SS1.p1.7.m2.1.1.1.1">0</cn></apply><apply id="S3.SS1.p1.7.m2.4.4.2.2.2.cmml" xref="S3.SS1.p1.7.m2.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m2.4.4.2.2.2.1.cmml" xref="S3.SS1.p1.7.m2.4.4.2.2.2">superscript</csymbol><ci id="S3.SS1.p1.7.m2.4.4.2.2.2.2.cmml" xref="S3.SS1.p1.7.m2.4.4.2.2.2.2">𝜃</ci><ci id="S3.SS1.p1.7.m2.2.2.1.1.cmml" xref="S3.SS1.p1.7.m2.2.2.1.1">ℓ</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m2.4c">f(x^{(0)},\theta^{(\ell)})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m2.4d">italic_f ( italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT , italic_θ start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT )</annotation></semantics></math>, 아마도 합계에 대한 특정 기여(<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E2" title="2 ‣ 3.1 Intuition ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>)는 무시될 수 있다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.3">물론, 그들은 전혀 독립적이지 않다: 만약 우리가 층 <math alttext="\ell-1" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" mathvariant="normal" xref="S3.SS1.p2.1.m1.1.1.2.cmml">ℓ</mi><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">−</mo><mn id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><minus id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></minus><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ℓ</ci><cn id="S3.SS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\ell-1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">roman_ℓ - 1</annotation></semantics></math>를 삭제한다면, 우리는 이제 그 층 <math alttext="x^{(\ell-1)}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msup id="S3.SS1.p2.2.m2.1.2" xref="S3.SS1.p2.2.m2.1.2.cmml"><mi id="S3.SS1.p2.2.m2.1.2.2" xref="S3.SS1.p2.2.m2.1.2.2.cmml">x</mi><mrow id="S3.SS1.p2.2.m2.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.2.m2.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml">ℓ</mi><mo id="S3.SS1.p2.2.m2.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.p2.2.m2.1.1.1.1.1.3" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.2.cmml" xref="S3.SS1.p2.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.2">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.2.2">𝑥</ci><apply id="S3.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1"><minus id="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1"></minus><ci id="S3.SS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.1.1.1.2">ℓ</ci><cn id="S3.SS1.p2.2.m2.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">x^{(\ell-1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_x start_POSTSUPERSCRIPT ( roman_ℓ - 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>에 이전 입력을 연결해야 한다.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(\ell+1)}=x^{(\ell-1)}+f(x^{(\ell-1)},\theta^{(\ell)})\,," class="ltx_Math" display="block" id="S3.E3.m1.5"><semantics id="S3.E3.m1.5a"><mrow id="S3.E3.m1.5.5.1" xref="S3.E3.m1.5.5.1.1.cmml"><mrow id="S3.E3.m1.5.5.1.1" xref="S3.E3.m1.5.5.1.1.cmml"><msup id="S3.E3.m1.5.5.1.1.4" xref="S3.E3.m1.5.5.1.1.4.cmml"><mi id="S3.E3.m1.5.5.1.1.4.2" xref="S3.E3.m1.5.5.1.1.4.2.cmml">x</mi><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2" mathvariant="normal" xref="S3.E3.m1.1.1.1.1.1.2.cmml">ℓ</mi><mo id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E3.m1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E3.m1.5.5.1.1.3" xref="S3.E3.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.5.5.1.1.2" xref="S3.E3.m1.5.5.1.1.2.cmml"><msup id="S3.E3.m1.5.5.1.1.2.4" xref="S3.E3.m1.5.5.1.1.2.4.cmml"><mi id="S3.E3.m1.5.5.1.1.2.4.2" xref="S3.E3.m1.5.5.1.1.2.4.2.cmml">x</mi><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mo id="S3.E3.m1.2.2.1.1.2" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E3.m1.2.2.1.1.1.2.cmml">ℓ</mi><mo id="S3.E3.m1.2.2.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.cmml">−</mo><mn id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E3.m1.2.2.1.1.3" stretchy="false" xref="S3.E3.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E3.m1.5.5.1.1.2.3" xref="S3.E3.m1.5.5.1.1.2.3.cmml">+</mo><mrow id="S3.E3.m1.5.5.1.1.2.2" xref="S3.E3.m1.5.5.1.1.2.2.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.4" xref="S3.E3.m1.5.5.1.1.2.2.4.cmml">f</mi><mo id="S3.E3.m1.5.5.1.1.2.2.3" xref="S3.E3.m1.5.5.1.1.2.2.3.cmml">⁢</mo><mrow id="S3.E3.m1.5.5.1.1.2.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml"><mo id="S3.E3.m1.5.5.1.1.2.2.2.2.3" stretchy="false" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml">(</mo><msup id="S3.E3.m1.5.5.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mo id="S3.E3.m1.3.3.1.1.2" stretchy="false" xref="S3.E3.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml"><mi id="S3.E3.m1.3.3.1.1.1.2" mathvariant="normal" xref="S3.E3.m1.3.3.1.1.1.2.cmml">ℓ</mi><mo id="S3.E3.m1.3.3.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.cmml">−</mo><mn id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E3.m1.3.3.1.1.3" stretchy="false" xref="S3.E3.m1.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E3.m1.5.5.1.1.2.2.2.2.4" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml">,</mo><msup id="S3.E3.m1.5.5.1.1.2.2.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.2.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.2.cmml">θ</mi><mrow id="S3.E3.m1.4.4.1.3" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml"><mo id="S3.E3.m1.4.4.1.3.1" stretchy="false" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml">(</mo><mi id="S3.E3.m1.4.4.1.1" mathvariant="normal" xref="S3.E3.m1.4.4.1.1.cmml">ℓ</mi><mo id="S3.E3.m1.4.4.1.3.2" stretchy="false" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml">)</mo></mrow></msup><mo id="S3.E3.m1.5.5.1.1.2.2.2.2.5" rspace="0.170em" stretchy="false" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.5.5.1.2" xref="S3.E3.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.5b"><apply id="S3.E3.m1.5.5.1.1.cmml" xref="S3.E3.m1.5.5.1"><eq id="S3.E3.m1.5.5.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.3"></eq><apply id="S3.E3.m1.5.5.1.1.4.cmml" xref="S3.E3.m1.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.4.1.cmml" xref="S3.E3.m1.5.5.1.1.4">superscript</csymbol><ci id="S3.E3.m1.5.5.1.1.4.2.cmml" xref="S3.E3.m1.5.5.1.1.4.2">𝑥</ci><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><plus id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"></plus><ci id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2">ℓ</ci><cn id="S3.E3.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S3.E3.m1.5.5.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.2"><plus id="S3.E3.m1.5.5.1.1.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.3"></plus><apply id="S3.E3.m1.5.5.1.1.2.4.cmml" xref="S3.E3.m1.5.5.1.1.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.4.1.cmml" xref="S3.E3.m1.5.5.1.1.2.4">superscript</csymbol><ci id="S3.E3.m1.5.5.1.1.2.4.2.cmml" xref="S3.E3.m1.5.5.1.1.2.4.2">𝑥</ci><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1"><minus id="S3.E3.m1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1"></minus><ci id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2">ℓ</ci><cn id="S3.E3.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.2.2.1.1.1.3">1</cn></apply></apply><apply id="S3.E3.m1.5.5.1.1.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2"><times id="S3.E3.m1.5.5.1.1.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.3"></times><ci id="S3.E3.m1.5.5.1.1.2.2.4.cmml" xref="S3.E3.m1.5.5.1.1.2.2.4">𝑓</ci><interval closure="open" id="S3.E3.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2"><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1"><minus id="S3.E3.m1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1"></minus><ci id="S3.E3.m1.3.3.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.2">ℓ</ci><cn id="S3.E3.m1.3.3.1.1.1.3.cmml" type="integer" xref="S3.E3.m1.3.3.1.1.1.3">1</cn></apply></apply><apply id="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2">superscript</csymbol><ci id="S3.E3.m1.5.5.1.1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.2">𝜃</ci><ci id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1.1">ℓ</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.5c">x^{(\ell+1)}=x^{(\ell-1)}+f(x^{(\ell-1)},\theta^{(\ell)})\,,</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.5d">italic_x start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT = italic_x start_POSTSUPERSCRIPT ( roman_ℓ - 1 ) end_POSTSUPERSCRIPT + italic_f ( italic_x start_POSTSUPERSCRIPT ( roman_ℓ - 1 ) end_POSTSUPERSCRIPT , italic_θ start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.11">여기서 명확성을 위해 삭제에도 불구하고 레이어 또는 입력에 레이블을 다시 지정하지 않습니다. 일반적으로 원본 입력과 새 입력 사이의 이러한 <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.11.1">mismatch</em>은 네트워크에 매우 손상되어야 합니다. 그러나, 일부 수의 초기 레이어들 후에, 표현들이 레이어 인덱스에 관하여 천천히 변화하는 함수로 수렴하는 경우,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(\ell)}\approx x^{(\ell-1)}+\epsilon\,," class="ltx_Math" display="block" id="S3.E4.m1.3"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.1.cmml"><mrow id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml"><msup id="S3.E4.m1.3.3.1.1.2" xref="S3.E4.m1.3.3.1.1.2.cmml"><mi id="S3.E4.m1.3.3.1.1.2.2" xref="S3.E4.m1.3.3.1.1.2.2.cmml">x</mi><mrow id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.3.1" stretchy="false" xref="S3.E4.m1.3.3.1.1.2.cmml">(</mo><mi id="S3.E4.m1.1.1.1.1" mathvariant="normal" xref="S3.E4.m1.1.1.1.1.cmml">ℓ</mi><mo id="S3.E4.m1.1.1.1.3.2" stretchy="false" xref="S3.E4.m1.3.3.1.1.2.cmml">)</mo></mrow></msup><mo id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.cmml">≈</mo><mrow id="S3.E4.m1.3.3.1.1.3" xref="S3.E4.m1.3.3.1.1.3.cmml"><msup id="S3.E4.m1.3.3.1.1.3.2" xref="S3.E4.m1.3.3.1.1.3.2.cmml"><mi id="S3.E4.m1.3.3.1.1.3.2.2" xref="S3.E4.m1.3.3.1.1.3.2.2.cmml">x</mi><mrow id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.1.cmml"><mo id="S3.E4.m1.2.2.1.1.2" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E4.m1.2.2.1.1.1.2.cmml">ℓ</mi><mo id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml">−</mo><mn id="S3.E4.m1.2.2.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E4.m1.2.2.1.1.3" stretchy="false" xref="S3.E4.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E4.m1.3.3.1.1.3.1" xref="S3.E4.m1.3.3.1.1.3.1.cmml">+</mo><mi id="S3.E4.m1.3.3.1.1.3.3" xref="S3.E4.m1.3.3.1.1.3.3.cmml">ϵ</mi></mrow></mrow><mo id="S3.E4.m1.3.3.1.2" lspace="0.170em" xref="S3.E4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1"><approx id="S3.E4.m1.3.3.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"></approx><apply id="S3.E4.m1.3.3.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.2.1.cmml" xref="S3.E4.m1.3.3.1.1.2">superscript</csymbol><ci id="S3.E4.m1.3.3.1.1.2.2.cmml" xref="S3.E4.m1.3.3.1.1.2.2">𝑥</ci><ci id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">ℓ</ci></apply><apply id="S3.E4.m1.3.3.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.3"><plus id="S3.E4.m1.3.3.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.3.1"></plus><apply id="S3.E4.m1.3.3.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.3.2.1.cmml" xref="S3.E4.m1.3.3.1.1.3.2">superscript</csymbol><ci id="S3.E4.m1.3.3.1.1.3.2.2.cmml" xref="S3.E4.m1.3.3.1.1.3.2.2">𝑥</ci><apply id="S3.E4.m1.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1"><minus id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1"></minus><ci id="S3.E4.m1.2.2.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.2">ℓ</ci><cn id="S3.E4.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.E4.m1.2.2.1.1.1.3">1</cn></apply></apply><ci id="S3.E4.m1.3.3.1.1.3.3.cmml" xref="S3.E4.m1.3.3.1.1.3.3">italic-ϵ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">x^{(\ell)}\approx x^{(\ell-1)}+\epsilon\,,</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.3d">italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ≈ italic_x start_POSTSUPERSCRIPT ( roman_ℓ - 1 ) end_POSTSUPERSCRIPT + italic_ϵ ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.10">어떤 적절한 의미에서 <math alttext="\epsilon\ll x^{(\ell)}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m1.1"><semantics id="S3.SS1.p2.4.m1.1a"><mrow id="S3.SS1.p2.4.m1.1.2" xref="S3.SS1.p2.4.m1.1.2.cmml"><mi id="S3.SS1.p2.4.m1.1.2.2" xref="S3.SS1.p2.4.m1.1.2.2.cmml">ϵ</mi><mo id="S3.SS1.p2.4.m1.1.2.1" xref="S3.SS1.p2.4.m1.1.2.1.cmml">≪</mo><msup id="S3.SS1.p2.4.m1.1.2.3" xref="S3.SS1.p2.4.m1.1.2.3.cmml"><mi id="S3.SS1.p2.4.m1.1.2.3.2" xref="S3.SS1.p2.4.m1.1.2.3.2.cmml">x</mi><mrow id="S3.SS1.p2.4.m1.1.1.1.3" xref="S3.SS1.p2.4.m1.1.2.3.cmml"><mo id="S3.SS1.p2.4.m1.1.1.1.3.1" stretchy="false" xref="S3.SS1.p2.4.m1.1.2.3.cmml">(</mo><mi id="S3.SS1.p2.4.m1.1.1.1.1" mathvariant="normal" xref="S3.SS1.p2.4.m1.1.1.1.1.cmml">ℓ</mi><mo id="S3.SS1.p2.4.m1.1.1.1.3.2" stretchy="false" xref="S3.SS1.p2.4.m1.1.2.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m1.1b"><apply id="S3.SS1.p2.4.m1.1.2.cmml" xref="S3.SS1.p2.4.m1.1.2"><csymbol cd="latexml" id="S3.SS1.p2.4.m1.1.2.1.cmml" xref="S3.SS1.p2.4.m1.1.2.1">much-less-than</csymbol><ci id="S3.SS1.p2.4.m1.1.2.2.cmml" xref="S3.SS1.p2.4.m1.1.2.2">italic-ϵ</ci><apply id="S3.SS1.p2.4.m1.1.2.3.cmml" xref="S3.SS1.p2.4.m1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m1.1.2.3.1.cmml" xref="S3.SS1.p2.4.m1.1.2.3">superscript</csymbol><ci id="S3.SS1.p2.4.m1.1.2.3.2.cmml" xref="S3.SS1.p2.4.m1.1.2.3.2">𝑥</ci><ci id="S3.SS1.p2.4.m1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m1.1.1.1.1">ℓ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m1.1c">\epsilon\ll x^{(\ell)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m1.1d">italic_ϵ ≪ italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT</annotation></semantics></math>로, 그 다음 특정 계층 <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m2.1"><semantics id="S3.SS1.p2.5.m2.1a"><mi id="S3.SS1.p2.5.m2.1.1" mathvariant="normal" xref="S3.SS1.p2.5.m2.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m2.1b"><ci id="S3.SS1.p2.5.m2.1.1.cmml" xref="S3.SS1.p2.5.m2.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m2.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m2.1d">roman_ℓ</annotation></semantics></math>, 예를 들어 (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E1" title="1 ‣ 3.1 Intuition ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>)에서 (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E3" title="3 ‣ 3.1 Intuition ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a>)로 가면서 대체 <math alttext="x^{(\ell)}\to x^{(\ell-1)}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m3.2"><semantics id="S3.SS1.p2.6.m3.2a"><mrow id="S3.SS1.p2.6.m3.2.3" xref="S3.SS1.p2.6.m3.2.3.cmml"><msup id="S3.SS1.p2.6.m3.2.3.2" xref="S3.SS1.p2.6.m3.2.3.2.cmml"><mi id="S3.SS1.p2.6.m3.2.3.2.2" xref="S3.SS1.p2.6.m3.2.3.2.2.cmml">x</mi><mrow id="S3.SS1.p2.6.m3.1.1.1.3" xref="S3.SS1.p2.6.m3.2.3.2.cmml"><mo id="S3.SS1.p2.6.m3.1.1.1.3.1" stretchy="false" xref="S3.SS1.p2.6.m3.2.3.2.cmml">(</mo><mi id="S3.SS1.p2.6.m3.1.1.1.1" mathvariant="normal" xref="S3.SS1.p2.6.m3.1.1.1.1.cmml">ℓ</mi><mo id="S3.SS1.p2.6.m3.1.1.1.3.2" stretchy="false" xref="S3.SS1.p2.6.m3.2.3.2.cmml">)</mo></mrow></msup><mo id="S3.SS1.p2.6.m3.2.3.1" stretchy="false" xref="S3.SS1.p2.6.m3.2.3.1.cmml">→</mo><msup id="S3.SS1.p2.6.m3.2.3.3" xref="S3.SS1.p2.6.m3.2.3.3.cmml"><mi id="S3.SS1.p2.6.m3.2.3.3.2" xref="S3.SS1.p2.6.m3.2.3.3.2.cmml">x</mi><mrow id="S3.SS1.p2.6.m3.2.2.1.1" xref="S3.SS1.p2.6.m3.2.2.1.1.1.cmml"><mo id="S3.SS1.p2.6.m3.2.2.1.1.2" stretchy="false" xref="S3.SS1.p2.6.m3.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.6.m3.2.2.1.1.1" xref="S3.SS1.p2.6.m3.2.2.1.1.1.cmml"><mi id="S3.SS1.p2.6.m3.2.2.1.1.1.2" mathvariant="normal" xref="S3.SS1.p2.6.m3.2.2.1.1.1.2.cmml">ℓ</mi><mo id="S3.SS1.p2.6.m3.2.2.1.1.1.1" xref="S3.SS1.p2.6.m3.2.2.1.1.1.1.cmml">−</mo><mn id="S3.SS1.p2.6.m3.2.2.1.1.1.3" xref="S3.SS1.p2.6.m3.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.6.m3.2.2.1.1.3" stretchy="false" xref="S3.SS1.p2.6.m3.2.2.1.1.1.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m3.2b"><apply id="S3.SS1.p2.6.m3.2.3.cmml" xref="S3.SS1.p2.6.m3.2.3"><ci id="S3.SS1.p2.6.m3.2.3.1.cmml" xref="S3.SS1.p2.6.m3.2.3.1">→</ci><apply id="S3.SS1.p2.6.m3.2.3.2.cmml" xref="S3.SS1.p2.6.m3.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m3.2.3.2.1.cmml" xref="S3.SS1.p2.6.m3.2.3.2">superscript</csymbol><ci id="S3.SS1.p2.6.m3.2.3.2.2.cmml" xref="S3.SS1.p2.6.m3.2.3.2.2">𝑥</ci><ci id="S3.SS1.p2.6.m3.1.1.1.1.cmml" xref="S3.SS1.p2.6.m3.1.1.1.1">ℓ</ci></apply><apply id="S3.SS1.p2.6.m3.2.3.3.cmml" xref="S3.SS1.p2.6.m3.2.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m3.2.3.3.1.cmml" xref="S3.SS1.p2.6.m3.2.3.3">superscript</csymbol><ci id="S3.SS1.p2.6.m3.2.3.3.2.cmml" xref="S3.SS1.p2.6.m3.2.3.3.2">𝑥</ci><apply id="S3.SS1.p2.6.m3.2.2.1.1.1.cmml" xref="S3.SS1.p2.6.m3.2.2.1.1"><minus id="S3.SS1.p2.6.m3.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.6.m3.2.2.1.1.1.1"></minus><ci id="S3.SS1.p2.6.m3.2.2.1.1.1.2.cmml" xref="S3.SS1.p2.6.m3.2.2.1.1.1.2">ℓ</ci><cn id="S3.SS1.p2.6.m3.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.6.m3.2.2.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m3.2c">x^{(\ell)}\to x^{(\ell-1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m3.2d">italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT → italic_x start_POSTSUPERSCRIPT ( roman_ℓ - 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>를 만드는 것은 후속 계층인 <math alttext="x^{(\ell+1)}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m4.1"><semantics id="S3.SS1.p2.7.m4.1a"><msup id="S3.SS1.p2.7.m4.1.2" xref="S3.SS1.p2.7.m4.1.2.cmml"><mi id="S3.SS1.p2.7.m4.1.2.2" xref="S3.SS1.p2.7.m4.1.2.2.cmml">x</mi><mrow id="S3.SS1.p2.7.m4.1.1.1.1" xref="S3.SS1.p2.7.m4.1.1.1.1.1.cmml"><mo id="S3.SS1.p2.7.m4.1.1.1.1.2" stretchy="false" xref="S3.SS1.p2.7.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.7.m4.1.1.1.1.1" xref="S3.SS1.p2.7.m4.1.1.1.1.1.cmml"><mi id="S3.SS1.p2.7.m4.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p2.7.m4.1.1.1.1.1.2.cmml">ℓ</mi><mo id="S3.SS1.p2.7.m4.1.1.1.1.1.1" xref="S3.SS1.p2.7.m4.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS1.p2.7.m4.1.1.1.1.1.3" xref="S3.SS1.p2.7.m4.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p2.7.m4.1.1.1.1.3" stretchy="false" xref="S3.SS1.p2.7.m4.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m4.1b"><apply id="S3.SS1.p2.7.m4.1.2.cmml" xref="S3.SS1.p2.7.m4.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m4.1.2.1.cmml" xref="S3.SS1.p2.7.m4.1.2">superscript</csymbol><ci id="S3.SS1.p2.7.m4.1.2.2.cmml" xref="S3.SS1.p2.7.m4.1.2.2">𝑥</ci><apply id="S3.SS1.p2.7.m4.1.1.1.1.1.cmml" xref="S3.SS1.p2.7.m4.1.1.1.1"><plus id="S3.SS1.p2.7.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.7.m4.1.1.1.1.1.1"></plus><ci id="S3.SS1.p2.7.m4.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.7.m4.1.1.1.1.1.2">ℓ</ci><cn id="S3.SS1.p2.7.m4.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p2.7.m4.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m4.1c">x^{(\ell+1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m4.1d">italic_x start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>에서의 표현만 소량으로 변경해야 한다. 유사하게, 계층 이전에 <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m5.1"><semantics id="S3.SS1.p2.8.m5.1a"><mi id="S3.SS1.p2.8.m5.1.1" xref="S3.SS1.p2.8.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m5.1b"><ci id="S3.SS1.p2.8.m5.1.1.cmml" xref="S3.SS1.p2.8.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m5.1d">italic_n</annotation></semantics></math> 계층들을 성공적으로 프루닝하기 위해, 즉 <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m6.1"><semantics id="S3.SS1.p2.9.m6.1a"><mi id="S3.SS1.p2.9.m6.1.1" mathvariant="normal" xref="S3.SS1.p2.9.m6.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m6.1b"><ci id="S3.SS1.p2.9.m6.1.1.cmml" xref="S3.SS1.p2.9.m6.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m6.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m6.1d">roman_ℓ</annotation></semantics></math>, 즉 <math alttext="\ell-n,\ldots,\ell-1" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m7.3"><semantics id="S3.SS1.p2.10.m7.3a"><mrow id="S3.SS1.p2.10.m7.3.3.2" xref="S3.SS1.p2.10.m7.3.3.3.cmml"><mrow id="S3.SS1.p2.10.m7.2.2.1.1" xref="S3.SS1.p2.10.m7.2.2.1.1.cmml"><mi id="S3.SS1.p2.10.m7.2.2.1.1.2" mathvariant="normal" xref="S3.SS1.p2.10.m7.2.2.1.1.2.cmml">ℓ</mi><mo id="S3.SS1.p2.10.m7.2.2.1.1.1" xref="S3.SS1.p2.10.m7.2.2.1.1.1.cmml">−</mo><mi id="S3.SS1.p2.10.m7.2.2.1.1.3" xref="S3.SS1.p2.10.m7.2.2.1.1.3.cmml">n</mi></mrow><mo id="S3.SS1.p2.10.m7.3.3.2.3" xref="S3.SS1.p2.10.m7.3.3.3.cmml">,</mo><mi id="S3.SS1.p2.10.m7.1.1" mathvariant="normal" xref="S3.SS1.p2.10.m7.1.1.cmml">…</mi><mo id="S3.SS1.p2.10.m7.3.3.2.4" xref="S3.SS1.p2.10.m7.3.3.3.cmml">,</mo><mrow id="S3.SS1.p2.10.m7.3.3.2.2" xref="S3.SS1.p2.10.m7.3.3.2.2.cmml"><mi id="S3.SS1.p2.10.m7.3.3.2.2.2" mathvariant="normal" xref="S3.SS1.p2.10.m7.3.3.2.2.2.cmml">ℓ</mi><mo id="S3.SS1.p2.10.m7.3.3.2.2.1" xref="S3.SS1.p2.10.m7.3.3.2.2.1.cmml">−</mo><mn id="S3.SS1.p2.10.m7.3.3.2.2.3" xref="S3.SS1.p2.10.m7.3.3.2.2.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m7.3b"><list id="S3.SS1.p2.10.m7.3.3.3.cmml" xref="S3.SS1.p2.10.m7.3.3.2"><apply id="S3.SS1.p2.10.m7.2.2.1.1.cmml" xref="S3.SS1.p2.10.m7.2.2.1.1"><minus id="S3.SS1.p2.10.m7.2.2.1.1.1.cmml" xref="S3.SS1.p2.10.m7.2.2.1.1.1"></minus><ci id="S3.SS1.p2.10.m7.2.2.1.1.2.cmml" xref="S3.SS1.p2.10.m7.2.2.1.1.2">ℓ</ci><ci id="S3.SS1.p2.10.m7.2.2.1.1.3.cmml" xref="S3.SS1.p2.10.m7.2.2.1.1.3">𝑛</ci></apply><ci id="S3.SS1.p2.10.m7.1.1.cmml" xref="S3.SS1.p2.10.m7.1.1">…</ci><apply id="S3.SS1.p2.10.m7.3.3.2.2.cmml" xref="S3.SS1.p2.10.m7.3.3.2.2"><minus id="S3.SS1.p2.10.m7.3.3.2.2.1.cmml" xref="S3.SS1.p2.10.m7.3.3.2.2.1"></minus><ci id="S3.SS1.p2.10.m7.3.3.2.2.2.cmml" xref="S3.SS1.p2.10.m7.3.3.2.2.2">ℓ</ci><cn id="S3.SS1.p2.10.m7.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.p2.10.m7.3.3.2.2.3">1</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m7.3c">\ell-n,\ldots,\ell-1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m7.3d">roman_ℓ - italic_n , … , roman_ℓ - 1</annotation></semantics></math>로부터 인덱싱된 것들, 우리는 프루닝된 블록에 대한 입력이 프루닝된 블록의 출력과 매우 유사해야 한다고 원할 것이다:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{(\ell)}\approx x^{(\ell-n)}+\epsilon\,." class="ltx_Math" display="block" id="S3.E5.m1.3"><semantics id="S3.E5.m1.3a"><mrow id="S3.E5.m1.3.3.1" xref="S3.E5.m1.3.3.1.1.cmml"><mrow id="S3.E5.m1.3.3.1.1" xref="S3.E5.m1.3.3.1.1.cmml"><msup id="S3.E5.m1.3.3.1.1.2" xref="S3.E5.m1.3.3.1.1.2.cmml"><mi id="S3.E5.m1.3.3.1.1.2.2" xref="S3.E5.m1.3.3.1.1.2.2.cmml">x</mi><mrow id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.3.3.1.1.2.cmml"><mo id="S3.E5.m1.1.1.1.3.1" stretchy="false" xref="S3.E5.m1.3.3.1.1.2.cmml">(</mo><mi id="S3.E5.m1.1.1.1.1" mathvariant="normal" xref="S3.E5.m1.1.1.1.1.cmml">ℓ</mi><mo id="S3.E5.m1.1.1.1.3.2" stretchy="false" xref="S3.E5.m1.3.3.1.1.2.cmml">)</mo></mrow></msup><mo id="S3.E5.m1.3.3.1.1.1" xref="S3.E5.m1.3.3.1.1.1.cmml">≈</mo><mrow id="S3.E5.m1.3.3.1.1.3" xref="S3.E5.m1.3.3.1.1.3.cmml"><msup id="S3.E5.m1.3.3.1.1.3.2" xref="S3.E5.m1.3.3.1.1.3.2.cmml"><mi id="S3.E5.m1.3.3.1.1.3.2.2" xref="S3.E5.m1.3.3.1.1.3.2.2.cmml">x</mi><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><mo id="S3.E5.m1.2.2.1.1.2" stretchy="false" xref="S3.E5.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E5.m1.2.2.1.1.1.2.cmml">ℓ</mi><mo id="S3.E5.m1.2.2.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.cmml">−</mo><mi id="S3.E5.m1.2.2.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E5.m1.2.2.1.1.3" stretchy="false" xref="S3.E5.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E5.m1.3.3.1.1.3.1" xref="S3.E5.m1.3.3.1.1.3.1.cmml">+</mo><mi id="S3.E5.m1.3.3.1.1.3.3" xref="S3.E5.m1.3.3.1.1.3.3.cmml">ϵ</mi></mrow></mrow><mo id="S3.E5.m1.3.3.1.2" lspace="0.170em" xref="S3.E5.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.3b"><apply id="S3.E5.m1.3.3.1.1.cmml" xref="S3.E5.m1.3.3.1"><approx id="S3.E5.m1.3.3.1.1.1.cmml" xref="S3.E5.m1.3.3.1.1.1"></approx><apply id="S3.E5.m1.3.3.1.1.2.cmml" xref="S3.E5.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.2.1.cmml" xref="S3.E5.m1.3.3.1.1.2">superscript</csymbol><ci id="S3.E5.m1.3.3.1.1.2.2.cmml" xref="S3.E5.m1.3.3.1.1.2.2">𝑥</ci><ci id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1">ℓ</ci></apply><apply id="S3.E5.m1.3.3.1.1.3.cmml" xref="S3.E5.m1.3.3.1.1.3"><plus id="S3.E5.m1.3.3.1.1.3.1.cmml" xref="S3.E5.m1.3.3.1.1.3.1"></plus><apply id="S3.E5.m1.3.3.1.1.3.2.cmml" xref="S3.E5.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.3.3.1.1.3.2.1.cmml" xref="S3.E5.m1.3.3.1.1.3.2">superscript</csymbol><ci id="S3.E5.m1.3.3.1.1.3.2.2.cmml" xref="S3.E5.m1.3.3.1.1.3.2.2">𝑥</ci><apply id="S3.E5.m1.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1"><minus id="S3.E5.m1.2.2.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1"></minus><ci id="S3.E5.m1.2.2.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2">ℓ</ci><ci id="S3.E5.m1.2.2.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3">𝑛</ci></apply></apply><ci id="S3.E5.m1.3.3.1.1.3.3.cmml" xref="S3.E5.m1.3.3.1.1.3.3">italic-ϵ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.3c">x^{(\ell)}\approx x^{(\ell-n)}+\epsilon\,.</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.3d">italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT ≈ italic_x start_POSTSUPERSCRIPT ( roman_ℓ - italic_n ) end_POSTSUPERSCRIPT + italic_ϵ .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.3">그럼에도 불구하고, 임의의 층 제거는 캐스케이딩 효과를 갖는다 : 포스트 프루닝 <math alttext="x^{(\ell+1)}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msup id="S3.SS1.p3.1.m1.1.2" xref="S3.SS1.p3.1.m1.1.2.cmml"><mi id="S3.SS1.p3.1.m1.1.2.2" xref="S3.SS1.p3.1.m1.1.2.2.cmml">x</mi><mrow id="S3.SS1.p3.1.m1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.1.m1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.1.m1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml">ℓ</mi><mo id="S3.SS1.p3.1.m1.1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.1.m1.1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p3.1.m1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.2">superscript</csymbol><ci id="S3.SS1.p3.1.m1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.2.2">𝑥</ci><apply id="S3.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1"><plus id="S3.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1"></plus><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2">ℓ</ci><cn id="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">x^{(\ell+1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_x start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>는 이전과 다른 함수에 의해 계산되기 때문에, cf. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E1" title="1 ‣ 3.1 Intuition ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>) vs. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E3" title="3 ‣ 3.1 Intuition ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a>), 이후 <math alttext="x^{(\ell+1)}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><msup id="S3.SS1.p3.2.m2.1.2" xref="S3.SS1.p3.2.m2.1.2.cmml"><mi id="S3.SS1.p3.2.m2.1.2.2" xref="S3.SS1.p3.2.m2.1.2.2.cmml">x</mi><mrow id="S3.SS1.p3.2.m2.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mo id="S3.SS1.p3.2.m2.1.1.1.1.2" stretchy="false" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.2.m2.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.2" mathvariant="normal" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml">ℓ</mi><mo id="S3.SS1.p3.2.m2.1.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.2.m2.1.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS1.p3.2.m2.1.1.1.1.3" stretchy="false" xref="S3.SS1.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.2.cmml" xref="S3.SS1.p3.2.m2.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.2.1.cmml" xref="S3.SS1.p3.2.m2.1.2">superscript</csymbol><ci id="S3.SS1.p3.2.m2.1.2.2.cmml" xref="S3.SS1.p3.2.m2.1.2.2">𝑥</ci><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1"><plus id="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1"></plus><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.2">ℓ</ci><cn id="S3.SS1.p3.2.m2.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.2.m2.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">x^{(\ell+1)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_x start_POSTSUPERSCRIPT ( roman_ℓ + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>는 후속 레이어에 직간접적으로 입력되기 때문에, <math alttext="\ell+2,\ldots,L" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.3"><semantics id="S3.SS1.p3.3.m3.3a"><mrow id="S3.SS1.p3.3.m3.3.3.1" xref="S3.SS1.p3.3.m3.3.3.2.cmml"><mrow id="S3.SS1.p3.3.m3.3.3.1.1" xref="S3.SS1.p3.3.m3.3.3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.3.3.1.1.2" mathvariant="normal" xref="S3.SS1.p3.3.m3.3.3.1.1.2.cmml">ℓ</mi><mo id="S3.SS1.p3.3.m3.3.3.1.1.1" xref="S3.SS1.p3.3.m3.3.3.1.1.1.cmml">+</mo><mn id="S3.SS1.p3.3.m3.3.3.1.1.3" xref="S3.SS1.p3.3.m3.3.3.1.1.3.cmml">2</mn></mrow><mo id="S3.SS1.p3.3.m3.3.3.1.2" xref="S3.SS1.p3.3.m3.3.3.2.cmml">,</mo><mi id="S3.SS1.p3.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p3.3.m3.1.1.cmml">…</mi><mo id="S3.SS1.p3.3.m3.3.3.1.3" xref="S3.SS1.p3.3.m3.3.3.2.cmml">,</mo><mi id="S3.SS1.p3.3.m3.2.2" xref="S3.SS1.p3.3.m3.2.2.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.3b"><list id="S3.SS1.p3.3.m3.3.3.2.cmml" xref="S3.SS1.p3.3.m3.3.3.1"><apply id="S3.SS1.p3.3.m3.3.3.1.1.cmml" xref="S3.SS1.p3.3.m3.3.3.1.1"><plus id="S3.SS1.p3.3.m3.3.3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.3.3.1.1.1"></plus><ci id="S3.SS1.p3.3.m3.3.3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.3.3.1.1.2">ℓ</ci><cn id="S3.SS1.p3.3.m3.3.3.1.1.3.cmml" type="integer" xref="S3.SS1.p3.3.m3.3.3.1.1.3">2</cn></apply><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">…</ci><ci id="S3.SS1.p3.3.m3.2.2.cmml" xref="S3.SS1.p3.3.m3.2.2">𝐿</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.3c">\ell+2,\ldots,L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.3d">roman_ℓ + 2 , … , italic_L</annotation></semantics></math>는 얕은 레이어를 삭제하는 것이 더 깊은 레이어를 삭제하는 것보다 훨씬 더 큰 영향을 미쳐야 한다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">이로부터 우리는 실험적으로 테스트할 다음과 같은 가설을 가지고 있다:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I1.ix1.1.1.1">(0)</em></span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">잔여 네트워크의 계층을 가지치기할 수 있어야 합니다.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I1.ix2.1.1.1">(1)</em></span>
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1">우리는 더 깊은 층을 가지치기하는 데 더 큰 성공을 거두어야 한다.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I1.ix3.1.1.1">(2)</em></span>
<div class="ltx_para" id="S3.I1.ix3.p1">
<p class="ltx_p" id="S3.I1.ix3.p1.1">성공적으로 프루닝한 레이어 블록에는 입력과 유사한 출력이 있어야 합니다.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS1.p4.2">다음 하위 섹션인 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.2</span></a>에서는 프루닝 알고리즘의 세부 사항을 설명하고 다음 섹션인 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4" title="4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4</span></a>에서는 점 <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.2.1">(0)-(2)</em>에 대한 실험적 증거를 제시할 것이다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Layer-pruning algorithm(s)</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">우리의 주요 계층 가지치기 알고리즘은 매우 간단하다.</p>
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">0.</span>
<div class="ltx_para" id="S3.I2.ix1.p1">
<p class="ltx_p" id="S3.I2.ix1.p1.1"><math alttext="n" class="ltx_Math" display="inline" id="S3.I2.ix1.p1.1.m1.1"><semantics id="S3.I2.ix1.p1.1.m1.1a"><mi id="S3.I2.ix1.p1.1.m1.1.1" xref="S3.I2.ix1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.I2.ix1.p1.1.m1.1b"><ci id="S3.I2.ix1.p1.1.m1.1.1.cmml" xref="S3.I2.ix1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.ix1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.I2.ix1.p1.1.m1.1d">italic_n</annotation></semantics></math>를 프루닝할 다수의 레이어를 선택한다.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.3">각도 거리 <math alttext="d(x^{(\ell)},x^{(\ell+n)})" class="ltx_Math" display="inline" id="S3.I2.i1.p1.1.m1.4"><semantics id="S3.I2.i1.p1.1.m1.4a"><mrow id="S3.I2.i1.p1.1.m1.4.4" xref="S3.I2.i1.p1.1.m1.4.4.cmml"><mi id="S3.I2.i1.p1.1.m1.4.4.4" xref="S3.I2.i1.p1.1.m1.4.4.4.cmml">d</mi><mo id="S3.I2.i1.p1.1.m1.4.4.3" xref="S3.I2.i1.p1.1.m1.4.4.3.cmml">⁢</mo><mrow id="S3.I2.i1.p1.1.m1.4.4.2.2" xref="S3.I2.i1.p1.1.m1.4.4.2.3.cmml"><mo id="S3.I2.i1.p1.1.m1.4.4.2.2.3" stretchy="false" xref="S3.I2.i1.p1.1.m1.4.4.2.3.cmml">(</mo><msup id="S3.I2.i1.p1.1.m1.3.3.1.1.1" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml"><mi id="S3.I2.i1.p1.1.m1.3.3.1.1.1.2" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.2.cmml">x</mi><mrow id="S3.I2.i1.p1.1.m1.1.1.1.3" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml"><mo id="S3.I2.i1.p1.1.m1.1.1.1.3.1" stretchy="false" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml">(</mo><mi id="S3.I2.i1.p1.1.m1.1.1.1.1" mathvariant="normal" xref="S3.I2.i1.p1.1.m1.1.1.1.1.cmml">ℓ</mi><mo id="S3.I2.i1.p1.1.m1.1.1.1.3.2" stretchy="false" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.I2.i1.p1.1.m1.4.4.2.2.4" xref="S3.I2.i1.p1.1.m1.4.4.2.3.cmml">,</mo><msup id="S3.I2.i1.p1.1.m1.4.4.2.2.2" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2.cmml"><mi id="S3.I2.i1.p1.1.m1.4.4.2.2.2.2" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2.2.cmml">x</mi><mrow id="S3.I2.i1.p1.1.m1.2.2.1.1" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml"><mo id="S3.I2.i1.p1.1.m1.2.2.1.1.2" stretchy="false" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.I2.i1.p1.1.m1.2.2.1.1.1" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.I2.i1.p1.1.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.2.cmml">ℓ</mi><mo id="S3.I2.i1.p1.1.m1.2.2.1.1.1.1" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.1.cmml">+</mo><mi id="S3.I2.i1.p1.1.m1.2.2.1.1.1.3" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.I2.i1.p1.1.m1.2.2.1.1.3" stretchy="false" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.I2.i1.p1.1.m1.4.4.2.2.5" stretchy="false" xref="S3.I2.i1.p1.1.m1.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.m1.4b"><apply id="S3.I2.i1.p1.1.m1.4.4.cmml" xref="S3.I2.i1.p1.1.m1.4.4"><times id="S3.I2.i1.p1.1.m1.4.4.3.cmml" xref="S3.I2.i1.p1.1.m1.4.4.3"></times><ci id="S3.I2.i1.p1.1.m1.4.4.4.cmml" xref="S3.I2.i1.p1.1.m1.4.4.4">𝑑</ci><interval closure="open" id="S3.I2.i1.p1.1.m1.4.4.2.3.cmml" xref="S3.I2.i1.p1.1.m1.4.4.2.2"><apply id="S3.I2.i1.p1.1.m1.3.3.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.m1.3.3.1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1">superscript</csymbol><ci id="S3.I2.i1.p1.1.m1.3.3.1.1.1.2.cmml" xref="S3.I2.i1.p1.1.m1.3.3.1.1.1.2">𝑥</ci><ci id="S3.I2.i1.p1.1.m1.1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1.1.1">ℓ</ci></apply><apply id="S3.I2.i1.p1.1.m1.4.4.2.2.2.cmml" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.m1.4.4.2.2.2.1.cmml" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2">superscript</csymbol><ci id="S3.I2.i1.p1.1.m1.4.4.2.2.2.2.cmml" xref="S3.I2.i1.p1.1.m1.4.4.2.2.2.2">𝑥</ci><apply id="S3.I2.i1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.2.2.1.1"><plus id="S3.I2.i1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.1"></plus><ci id="S3.I2.i1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.2">ℓ</ci><ci id="S3.I2.i1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.I2.i1.p1.1.m1.2.2.1.1.1.3">𝑛</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.m1.4c">d(x^{(\ell)},x^{(\ell+n)})</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.1.m1.4d">italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_ℓ + italic_n ) end_POSTSUPERSCRIPT )</annotation></semantics></math>, cf를 계산한다. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E7" title="7 ‣ 3.2 Layer-pruning algorithm(s) ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a>) 이하의, 계층 <math alttext="\ell" class="ltx_Math" display="inline" id="S3.I2.i1.p1.2.m2.1"><semantics id="S3.I2.i1.p1.2.m2.1a"><mi id="S3.I2.i1.p1.2.m2.1.1" mathvariant="normal" xref="S3.I2.i1.p1.2.m2.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.2.m2.1b"><ci id="S3.I2.i1.p1.2.m2.1.1.cmml" xref="S3.I2.i1.p1.2.m2.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.2.m2.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.2.m2.1d">roman_ℓ</annotation></semantics></math>에 대한 입력과 계층 <math alttext="\ell+n" class="ltx_Math" display="inline" id="S3.I2.i1.p1.3.m3.1"><semantics id="S3.I2.i1.p1.3.m3.1a"><mrow id="S3.I2.i1.p1.3.m3.1.1" xref="S3.I2.i1.p1.3.m3.1.1.cmml"><mi id="S3.I2.i1.p1.3.m3.1.1.2" mathvariant="normal" xref="S3.I2.i1.p1.3.m3.1.1.2.cmml">ℓ</mi><mo id="S3.I2.i1.p1.3.m3.1.1.1" xref="S3.I2.i1.p1.3.m3.1.1.1.cmml">+</mo><mi id="S3.I2.i1.p1.3.m3.1.1.3" xref="S3.I2.i1.p1.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.3.m3.1b"><apply id="S3.I2.i1.p1.3.m3.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1"><plus id="S3.I2.i1.p1.3.m3.1.1.1.cmml" xref="S3.I2.i1.p1.3.m3.1.1.1"></plus><ci id="S3.I2.i1.p1.3.m3.1.1.2.cmml" xref="S3.I2.i1.p1.3.m3.1.1.2">ℓ</ci><ci id="S3.I2.i1.p1.3.m3.1.1.3.cmml" xref="S3.I2.i1.p1.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.3.m3.1c">\ell+n</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.3.m3.1d">roman_ℓ + italic_n</annotation></semantics></math>에 대한 입력 사이에서, 중립 프리트레이닝 데이터세트 상에서 또는 관심 있는 다운스트림 태스크를 대표하는 데이터세트 상에서.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">그 거리를 최소화하는 레이어를 찾아, <math alttext="\ell^{*}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.m1.1"><semantics id="S3.I2.i2.p1.1.m1.1a"><msup id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml"><mi id="S3.I2.i2.p1.1.m1.1.1.2" mathvariant="normal" xref="S3.I2.i2.p1.1.m1.1.1.2.cmml">ℓ</mi><mo id="S3.I2.i2.p1.1.m1.1.1.3" xref="S3.I2.i2.p1.1.m1.1.1.3.cmml">*</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.1b"><apply id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i2.p1.1.m1.1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.I2.i2.p1.1.m1.1.1.2.cmml" xref="S3.I2.i2.p1.1.m1.1.1.2">ℓ</ci><times id="S3.I2.i2.p1.1.m1.1.1.3.cmml" xref="S3.I2.i2.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.1c">\ell^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.m1.1d">roman_ℓ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\ell^{\star}(n)\equiv\operatorname*{arg\,min}_{\ell}~{}d(x^{(\ell)},x^{(\ell+n%
)})\,." class="ltx_Math" display="block" id="S3.E6.m1.4"><semantics id="S3.E6.m1.4a"><mrow id="S3.E6.m1.4.4.1" xref="S3.E6.m1.4.4.1.1.cmml"><mrow id="S3.E6.m1.4.4.1.1" xref="S3.E6.m1.4.4.1.1.cmml"><mrow id="S3.E6.m1.4.4.1.1.4" xref="S3.E6.m1.4.4.1.1.4.cmml"><msup id="S3.E6.m1.4.4.1.1.4.2" xref="S3.E6.m1.4.4.1.1.4.2.cmml"><mi id="S3.E6.m1.4.4.1.1.4.2.2" mathvariant="normal" xref="S3.E6.m1.4.4.1.1.4.2.2.cmml">ℓ</mi><mo id="S3.E6.m1.4.4.1.1.4.2.3" xref="S3.E6.m1.4.4.1.1.4.2.3.cmml">⋆</mo></msup><mo id="S3.E6.m1.4.4.1.1.4.1" xref="S3.E6.m1.4.4.1.1.4.1.cmml">⁢</mo><mrow id="S3.E6.m1.4.4.1.1.4.3.2" xref="S3.E6.m1.4.4.1.1.4.cmml"><mo id="S3.E6.m1.4.4.1.1.4.3.2.1" stretchy="false" xref="S3.E6.m1.4.4.1.1.4.cmml">(</mo><mi id="S3.E6.m1.3.3" xref="S3.E6.m1.3.3.cmml">n</mi><mo id="S3.E6.m1.4.4.1.1.4.3.2.2" stretchy="false" xref="S3.E6.m1.4.4.1.1.4.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.4.4.1.1.3" xref="S3.E6.m1.4.4.1.1.3.cmml">≡</mo><mrow id="S3.E6.m1.4.4.1.1.2" xref="S3.E6.m1.4.4.1.1.2.cmml"><mrow id="S3.E6.m1.4.4.1.1.2.4" xref="S3.E6.m1.4.4.1.1.2.4.cmml"><munder id="S3.E6.m1.4.4.1.1.2.4.1" xref="S3.E6.m1.4.4.1.1.2.4.1.cmml"><mrow id="S3.E6.m1.4.4.1.1.2.4.1.2" xref="S3.E6.m1.4.4.1.1.2.4.1.2.cmml"><mi id="S3.E6.m1.4.4.1.1.2.4.1.2.2" xref="S3.E6.m1.4.4.1.1.2.4.1.2.2.cmml">arg</mi><mo id="S3.E6.m1.4.4.1.1.2.4.1.2.1" lspace="0.170em" xref="S3.E6.m1.4.4.1.1.2.4.1.2.1.cmml">⁢</mo><mi id="S3.E6.m1.4.4.1.1.2.4.1.2.3" xref="S3.E6.m1.4.4.1.1.2.4.1.2.3.cmml">min</mi></mrow><mi id="S3.E6.m1.4.4.1.1.2.4.1.3" mathvariant="normal" xref="S3.E6.m1.4.4.1.1.2.4.1.3.cmml">ℓ</mi></munder><mo id="S3.E6.m1.4.4.1.1.2.4a" xref="S3.E6.m1.4.4.1.1.2.4.cmml">⁡</mo><mi id="S3.E6.m1.4.4.1.1.2.4.2" xref="S3.E6.m1.4.4.1.1.2.4.2.cmml">d</mi></mrow><mo id="S3.E6.m1.4.4.1.1.2.3" xref="S3.E6.m1.4.4.1.1.2.3.cmml">⁢</mo><mrow id="S3.E6.m1.4.4.1.1.2.2.2" xref="S3.E6.m1.4.4.1.1.2.2.3.cmml"><mo id="S3.E6.m1.4.4.1.1.2.2.2.3" stretchy="false" xref="S3.E6.m1.4.4.1.1.2.2.3.cmml">(</mo><msup id="S3.E6.m1.4.4.1.1.1.1.1.1" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.4.4.1.1.1.1.1.1.2" xref="S3.E6.m1.4.4.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E6.m1.1.1.1.3" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml"><mo id="S3.E6.m1.1.1.1.3.1" stretchy="false" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E6.m1.1.1.1.1" mathvariant="normal" xref="S3.E6.m1.1.1.1.1.cmml">ℓ</mi><mo id="S3.E6.m1.1.1.1.3.2" stretchy="false" xref="S3.E6.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E6.m1.4.4.1.1.2.2.2.4" xref="S3.E6.m1.4.4.1.1.2.2.3.cmml">,</mo><msup id="S3.E6.m1.4.4.1.1.2.2.2.2" xref="S3.E6.m1.4.4.1.1.2.2.2.2.cmml"><mi id="S3.E6.m1.4.4.1.1.2.2.2.2.2" xref="S3.E6.m1.4.4.1.1.2.2.2.2.2.cmml">x</mi><mrow id="S3.E6.m1.2.2.1.1" xref="S3.E6.m1.2.2.1.1.1.cmml"><mo id="S3.E6.m1.2.2.1.1.2" stretchy="false" xref="S3.E6.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.2.2.1.1.1" xref="S3.E6.m1.2.2.1.1.1.cmml"><mi id="S3.E6.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E6.m1.2.2.1.1.1.2.cmml">ℓ</mi><mo id="S3.E6.m1.2.2.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.cmml">+</mo><mi id="S3.E6.m1.2.2.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E6.m1.2.2.1.1.3" stretchy="false" xref="S3.E6.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E6.m1.4.4.1.1.2.2.2.5" stretchy="false" xref="S3.E6.m1.4.4.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E6.m1.4.4.1.2" lspace="0.170em" xref="S3.E6.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.4b"><apply id="S3.E6.m1.4.4.1.1.cmml" xref="S3.E6.m1.4.4.1"><equivalent id="S3.E6.m1.4.4.1.1.3.cmml" xref="S3.E6.m1.4.4.1.1.3"></equivalent><apply id="S3.E6.m1.4.4.1.1.4.cmml" xref="S3.E6.m1.4.4.1.1.4"><times id="S3.E6.m1.4.4.1.1.4.1.cmml" xref="S3.E6.m1.4.4.1.1.4.1"></times><apply id="S3.E6.m1.4.4.1.1.4.2.cmml" xref="S3.E6.m1.4.4.1.1.4.2"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.4.2.1.cmml" xref="S3.E6.m1.4.4.1.1.4.2">superscript</csymbol><ci id="S3.E6.m1.4.4.1.1.4.2.2.cmml" xref="S3.E6.m1.4.4.1.1.4.2.2">ℓ</ci><ci id="S3.E6.m1.4.4.1.1.4.2.3.cmml" xref="S3.E6.m1.4.4.1.1.4.2.3">⋆</ci></apply><ci id="S3.E6.m1.3.3.cmml" xref="S3.E6.m1.3.3">𝑛</ci></apply><apply id="S3.E6.m1.4.4.1.1.2.cmml" xref="S3.E6.m1.4.4.1.1.2"><times id="S3.E6.m1.4.4.1.1.2.3.cmml" xref="S3.E6.m1.4.4.1.1.2.3"></times><apply id="S3.E6.m1.4.4.1.1.2.4.cmml" xref="S3.E6.m1.4.4.1.1.2.4"><apply id="S3.E6.m1.4.4.1.1.2.4.1.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.2.4.1.1.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1">subscript</csymbol><apply id="S3.E6.m1.4.4.1.1.2.4.1.2.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.2"><times id="S3.E6.m1.4.4.1.1.2.4.1.2.1.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.2.1"></times><ci id="S3.E6.m1.4.4.1.1.2.4.1.2.2.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.2.2">arg</ci><ci id="S3.E6.m1.4.4.1.1.2.4.1.2.3.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.2.3">min</ci></apply><ci id="S3.E6.m1.4.4.1.1.2.4.1.3.cmml" xref="S3.E6.m1.4.4.1.1.2.4.1.3">ℓ</ci></apply><ci id="S3.E6.m1.4.4.1.1.2.4.2.cmml" xref="S3.E6.m1.4.4.1.1.2.4.2">𝑑</ci></apply><interval closure="open" id="S3.E6.m1.4.4.1.1.2.2.3.cmml" xref="S3.E6.m1.4.4.1.1.2.2.2"><apply id="S3.E6.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E6.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.4.4.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1">ℓ</ci></apply><apply id="S3.E6.m1.4.4.1.1.2.2.2.2.cmml" xref="S3.E6.m1.4.4.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.4.4.1.1.2.2.2.2.1.cmml" xref="S3.E6.m1.4.4.1.1.2.2.2.2">superscript</csymbol><ci id="S3.E6.m1.4.4.1.1.2.2.2.2.2.cmml" xref="S3.E6.m1.4.4.1.1.2.2.2.2.2">𝑥</ci><apply id="S3.E6.m1.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1"><plus id="S3.E6.m1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1"></plus><ci id="S3.E6.m1.2.2.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.2">ℓ</ci><ci id="S3.E6.m1.2.2.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.1.3">𝑛</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.4c">\ell^{\star}(n)\equiv\operatorname*{arg\,min}_{\ell}~{}d(x^{(\ell)},x^{(\ell+n%
)})\,.</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.4d">roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( italic_n ) ≡ start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_ℓ + italic_n ) end_POSTSUPERSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.4">레이어 <math alttext="\ell^{\star}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.1.m1.1"><semantics id="S3.I2.i3.p1.1.m1.1a"><msup id="S3.I2.i3.p1.1.m1.1.1" xref="S3.I2.i3.p1.1.m1.1.1.cmml"><mi id="S3.I2.i3.p1.1.m1.1.1.2" mathvariant="normal" xref="S3.I2.i3.p1.1.m1.1.1.2.cmml">ℓ</mi><mo id="S3.I2.i3.p1.1.m1.1.1.3" xref="S3.I2.i3.p1.1.m1.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.1.m1.1b"><apply id="S3.I2.i3.p1.1.m1.1.1.cmml" xref="S3.I2.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I2.i3.p1.1.m1.1.1.1.cmml" xref="S3.I2.i3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.I2.i3.p1.1.m1.1.1.2.cmml" xref="S3.I2.i3.p1.1.m1.1.1.2">ℓ</ci><ci id="S3.I2.i3.p1.1.m1.1.1.3.cmml" xref="S3.I2.i3.p1.1.m1.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.1.m1.1c">\ell^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.1.m1.1d">roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>를 <math alttext="\ell^{\star}\!\!+\!n\!-\!1" class="ltx_Math" display="inline" id="S3.I2.i3.p1.2.m2.1"><semantics id="S3.I2.i3.p1.2.m2.1a"><mrow id="S3.I2.i3.p1.2.m2.1.1" xref="S3.I2.i3.p1.2.m2.1.1.cmml"><mrow id="S3.I2.i3.p1.2.m2.1.1.2" xref="S3.I2.i3.p1.2.m2.1.1.2.cmml"><msup id="S3.I2.i3.p1.2.m2.1.1.2.2" xref="S3.I2.i3.p1.2.m2.1.1.2.2.cmml"><mi id="S3.I2.i3.p1.2.m2.1.1.2.2.2" mathvariant="normal" xref="S3.I2.i3.p1.2.m2.1.1.2.2.2.cmml">ℓ</mi><mo id="S3.I2.i3.p1.2.m2.1.1.2.2.3" xref="S3.I2.i3.p1.2.m2.1.1.2.2.3.cmml">⋆</mo></msup><mo id="S3.I2.i3.p1.2.m2.1.1.2.1" rspace="0.052em" xref="S3.I2.i3.p1.2.m2.1.1.2.1.cmml">+</mo><mi id="S3.I2.i3.p1.2.m2.1.1.2.3" xref="S3.I2.i3.p1.2.m2.1.1.2.3.cmml">n</mi></mrow><mo id="S3.I2.i3.p1.2.m2.1.1.1" lspace="0.052em" rspace="0.052em" xref="S3.I2.i3.p1.2.m2.1.1.1.cmml">−</mo><mn id="S3.I2.i3.p1.2.m2.1.1.3" xref="S3.I2.i3.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.2.m2.1b"><apply id="S3.I2.i3.p1.2.m2.1.1.cmml" xref="S3.I2.i3.p1.2.m2.1.1"><minus id="S3.I2.i3.p1.2.m2.1.1.1.cmml" xref="S3.I2.i3.p1.2.m2.1.1.1"></minus><apply id="S3.I2.i3.p1.2.m2.1.1.2.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2"><plus id="S3.I2.i3.p1.2.m2.1.1.2.1.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.1"></plus><apply id="S3.I2.i3.p1.2.m2.1.1.2.2.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.2"><csymbol cd="ambiguous" id="S3.I2.i3.p1.2.m2.1.1.2.2.1.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.2">superscript</csymbol><ci id="S3.I2.i3.p1.2.m2.1.1.2.2.2.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.2.2">ℓ</ci><ci id="S3.I2.i3.p1.2.m2.1.1.2.2.3.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.2.3">⋆</ci></apply><ci id="S3.I2.i3.p1.2.m2.1.1.2.3.cmml" xref="S3.I2.i3.p1.2.m2.1.1.2.3">𝑛</ci></apply><cn id="S3.I2.i3.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.I2.i3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.2.m2.1c">\ell^{\star}\!\!+\!n\!-\!1</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.2.m2.1d">roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT + italic_n - 1</annotation></semantics></math>에 드롭하고, 레이어 <math alttext="\ell^{\star}" class="ltx_Math" display="inline" id="S3.I2.i3.p1.3.m3.1"><semantics id="S3.I2.i3.p1.3.m3.1a"><msup id="S3.I2.i3.p1.3.m3.1.1" xref="S3.I2.i3.p1.3.m3.1.1.cmml"><mi id="S3.I2.i3.p1.3.m3.1.1.2" mathvariant="normal" xref="S3.I2.i3.p1.3.m3.1.1.2.cmml">ℓ</mi><mo id="S3.I2.i3.p1.3.m3.1.1.3" xref="S3.I2.i3.p1.3.m3.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.3.m3.1b"><apply id="S3.I2.i3.p1.3.m3.1.1.cmml" xref="S3.I2.i3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I2.i3.p1.3.m3.1.1.1.cmml" xref="S3.I2.i3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.I2.i3.p1.3.m3.1.1.2.cmml" xref="S3.I2.i3.p1.3.m3.1.1.2">ℓ</ci><ci id="S3.I2.i3.p1.3.m3.1.1.3.cmml" xref="S3.I2.i3.p1.3.m3.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.3.m3.1c">\ell^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.3.m3.1d">roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>에 구 입력을 구 <math alttext="(\ell^{\star}\!\!+\!n)" class="ltx_Math" display="inline" id="S3.I2.i3.p1.4.m4.1"><semantics id="S3.I2.i3.p1.4.m4.1a"><mrow id="S3.I2.i3.p1.4.m4.1.1.1" xref="S3.I2.i3.p1.4.m4.1.1.1.1.cmml"><mo id="S3.I2.i3.p1.4.m4.1.1.1.2" stretchy="false" xref="S3.I2.i3.p1.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.I2.i3.p1.4.m4.1.1.1.1" xref="S3.I2.i3.p1.4.m4.1.1.1.1.cmml"><msup id="S3.I2.i3.p1.4.m4.1.1.1.1.2" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.cmml"><mi id="S3.I2.i3.p1.4.m4.1.1.1.1.2.2" mathvariant="normal" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.2.cmml">ℓ</mi><mo id="S3.I2.i3.p1.4.m4.1.1.1.1.2.3" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.3.cmml">⋆</mo></msup><mo id="S3.I2.i3.p1.4.m4.1.1.1.1.1" rspace="0.052em" xref="S3.I2.i3.p1.4.m4.1.1.1.1.1.cmml">+</mo><mi id="S3.I2.i3.p1.4.m4.1.1.1.1.3" xref="S3.I2.i3.p1.4.m4.1.1.1.1.3.cmml">n</mi></mrow><mo id="S3.I2.i3.p1.4.m4.1.1.1.3" stretchy="false" xref="S3.I2.i3.p1.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.4.m4.1b"><apply id="S3.I2.i3.p1.4.m4.1.1.1.1.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1"><plus id="S3.I2.i3.p1.4.m4.1.1.1.1.1.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.1"></plus><apply id="S3.I2.i3.p1.4.m4.1.1.1.1.2.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.I2.i3.p1.4.m4.1.1.1.1.2.1.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2">superscript</csymbol><ci id="S3.I2.i3.p1.4.m4.1.1.1.1.2.2.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.2">ℓ</ci><ci id="S3.I2.i3.p1.4.m4.1.1.1.1.2.3.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.2.3">⋆</ci></apply><ci id="S3.I2.i3.p1.4.m4.1.1.1.1.3.cmml" xref="S3.I2.i3.p1.4.m4.1.1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.4.m4.1c">(\ell^{\star}\!\!+\!n)</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.4.m4.1d">( roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT + italic_n )</annotation></semantics></math>번째 레이어 블록에 연결한다. <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Layers는 종종 데이터 구조에 포함되어 있으며, 이러한 <span class="ltx_text ltx_font_typewriter" id="footnote4.1">ModuleList</span> in <em class="ltx_emph ltx_font_italic" id="footnote4.2">PyTorch</em> so to drop these layers we'll define a new <span class="ltx_text ltx_font_typewriter" id="footnote4.3">ModuleList</span> that remove the layers from <math alttext="\ell^{\star}" class="ltx_Math" display="inline" id="footnote4.m1.1"><semantics id="footnote4.m1.1b"><msup id="footnote4.m1.1.1" xref="footnote4.m1.1.1.cmml"><mi id="footnote4.m1.1.1.2" mathvariant="normal" xref="footnote4.m1.1.1.2.cmml">ℓ</mi><mo id="footnote4.m1.1.1.3" xref="footnote4.m1.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><apply id="footnote4.m1.1.1.cmml" xref="footnote4.m1.1.1"><csymbol cd="ambiguous" id="footnote4.m1.1.1.1.cmml" xref="footnote4.m1.1.1">superscript</csymbol><ci id="footnote4.m1.1.1.2.cmml" xref="footnote4.m1.1.1.2">ℓ</ci><ci id="footnote4.m1.1.1.3.cmml" xref="footnote4.m1.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m1.1d">\ell^{\star}</annotation><annotation encoding="application/x-llamapun" id="footnote4.m1.1e">roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="\ell^{\star}+n-1" class="ltx_Math" display="inline" id="footnote4.m2.1"><semantics id="footnote4.m2.1b"><mrow id="footnote4.m2.1.1" xref="footnote4.m2.1.1.cmml"><mrow id="footnote4.m2.1.1.2" xref="footnote4.m2.1.1.2.cmml"><msup id="footnote4.m2.1.1.2.2" xref="footnote4.m2.1.1.2.2.cmml"><mi id="footnote4.m2.1.1.2.2.2" mathvariant="normal" xref="footnote4.m2.1.1.2.2.2.cmml">ℓ</mi><mo id="footnote4.m2.1.1.2.2.3" xref="footnote4.m2.1.1.2.2.3.cmml">⋆</mo></msup><mo id="footnote4.m2.1.1.2.1" xref="footnote4.m2.1.1.2.1.cmml">+</mo><mi id="footnote4.m2.1.1.2.3" xref="footnote4.m2.1.1.2.3.cmml">n</mi></mrow><mo id="footnote4.m2.1.1.1" xref="footnote4.m2.1.1.1.cmml">−</mo><mn id="footnote4.m2.1.1.3" xref="footnote4.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="footnote4.m2.1c"><apply id="footnote4.m2.1.1.cmml" xref="footnote4.m2.1.1"><minus id="footnote4.m2.1.1.1.cmml" xref="footnote4.m2.1.1.1"></minus><apply id="footnote4.m2.1.1.2.cmml" xref="footnote4.m2.1.1.2"><plus id="footnote4.m2.1.1.2.1.cmml" xref="footnote4.m2.1.1.2.1"></plus><apply id="footnote4.m2.1.1.2.2.cmml" xref="footnote4.m2.1.1.2.2"><csymbol cd="ambiguous" id="footnote4.m2.1.1.2.2.1.cmml" xref="footnote4.m2.1.1.2.2">superscript</csymbol><ci id="footnote4.m2.1.1.2.2.2.cmml" xref="footnote4.m2.1.1.2.2.2">ℓ</ci><ci id="footnote4.m2.1.1.2.2.3.cmml" xref="footnote4.m2.1.1.2.2.3">⋆</ci></apply><ci id="footnote4.m2.1.1.2.3.cmml" xref="footnote4.m2.1.1.2.3">𝑛</ci></apply><cn id="footnote4.m2.1.1.3.cmml" type="integer" xref="footnote4.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m2.1d">\ell^{\star}+n-1</annotation><annotation encoding="application/x-llamapun" id="footnote4.m2.1e">roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT + italic_n - 1</annotation></semantics></math>. </span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">(선택적으로) 계층 <math alttext="\ell^{\star}\!+n" class="ltx_Math" display="inline" id="S3.I2.i4.p1.1.m1.1"><semantics id="S3.I2.i4.p1.1.m1.1a"><mrow id="S3.I2.i4.p1.1.m1.1.1" xref="S3.I2.i4.p1.1.m1.1.1.cmml"><msup id="S3.I2.i4.p1.1.m1.1.1.2" xref="S3.I2.i4.p1.1.m1.1.1.2.cmml"><mi id="S3.I2.i4.p1.1.m1.1.1.2.2" mathvariant="normal" xref="S3.I2.i4.p1.1.m1.1.1.2.2.cmml">ℓ</mi><mo id="S3.I2.i4.p1.1.m1.1.1.2.3" xref="S3.I2.i4.p1.1.m1.1.1.2.3.cmml">⋆</mo></msup><mo id="S3.I2.i4.p1.1.m1.1.1.1" lspace="0.052em" xref="S3.I2.i4.p1.1.m1.1.1.1.cmml">+</mo><mi id="S3.I2.i4.p1.1.m1.1.1.3" xref="S3.I2.i4.p1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i4.p1.1.m1.1b"><apply id="S3.I2.i4.p1.1.m1.1.1.cmml" xref="S3.I2.i4.p1.1.m1.1.1"><plus id="S3.I2.i4.p1.1.m1.1.1.1.cmml" xref="S3.I2.i4.p1.1.m1.1.1.1"></plus><apply id="S3.I2.i4.p1.1.m1.1.1.2.cmml" xref="S3.I2.i4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I2.i4.p1.1.m1.1.1.2.1.cmml" xref="S3.I2.i4.p1.1.m1.1.1.2">superscript</csymbol><ci id="S3.I2.i4.p1.1.m1.1.1.2.2.cmml" xref="S3.I2.i4.p1.1.m1.1.1.2.2">ℓ</ci><ci id="S3.I2.i4.p1.1.m1.1.1.2.3.cmml" xref="S3.I2.i4.p1.1.m1.1.1.2.3">⋆</ci></apply><ci id="S3.I2.i4.p1.1.m1.1.1.3.cmml" xref="S3.I2.i4.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i4.p1.1.m1.1c">\ell^{\star}\!+n</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i4.p1.1.m1.1d">roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT + italic_n</annotation></semantics></math>에서 불일치를 중립 프리트레이닝 데이터세트 또는 관심있는 특정 데이터세트 상의 소량의 미세 조정으로 치유한다.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS2.p1.2">도형 내부의 단어가 열거된 목록의 텍스트보다 당신에게 더 도움이 된다면, 이 알고리즘은 그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>의 패널(a)-(b)에도 묘사된다는 점에 유의하라.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">첫 번째 단계에서, 길이 <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_T</annotation></semantics></math>의 단일 시퀀스 상의 각도 거리는 다음과 같이 주어진다.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d(x^{(\ell)},x^{(\ell+n)})\equiv\frac{1}{\pi}\arccos\left(\frac{x^{(\ell)}_{T}%
\cdot x^{(\ell+n)}_{T}}{\left|\!\left|x^{(\ell)}_{T}\right|\!\right|\left|\!%
\left|x^{(\ell+n)}_{T}\right|\!\right|}\right)\,," class="ltx_Math" display="block" id="S3.E7.m1.10"><semantics id="S3.E7.m1.10a"><mrow id="S3.E7.m1.10.10.1" xref="S3.E7.m1.10.10.1.1.cmml"><mrow id="S3.E7.m1.10.10.1.1" xref="S3.E7.m1.10.10.1.1.cmml"><mrow id="S3.E7.m1.10.10.1.1.2" xref="S3.E7.m1.10.10.1.1.2.cmml"><mi id="S3.E7.m1.10.10.1.1.2.4" xref="S3.E7.m1.10.10.1.1.2.4.cmml">d</mi><mo id="S3.E7.m1.10.10.1.1.2.3" xref="S3.E7.m1.10.10.1.1.2.3.cmml">⁢</mo><mrow id="S3.E7.m1.10.10.1.1.2.2.2" xref="S3.E7.m1.10.10.1.1.2.2.3.cmml"><mo id="S3.E7.m1.10.10.1.1.2.2.2.3" stretchy="false" xref="S3.E7.m1.10.10.1.1.2.2.3.cmml">(</mo><msup id="S3.E7.m1.10.10.1.1.1.1.1.1" xref="S3.E7.m1.10.10.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.10.10.1.1.1.1.1.1.2" xref="S3.E7.m1.10.10.1.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E7.m1.1.1.1.3" xref="S3.E7.m1.10.10.1.1.1.1.1.1.cmml"><mo id="S3.E7.m1.1.1.1.3.1" stretchy="false" xref="S3.E7.m1.10.10.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E7.m1.1.1.1.1" mathvariant="normal" xref="S3.E7.m1.1.1.1.1.cmml">ℓ</mi><mo id="S3.E7.m1.1.1.1.3.2" stretchy="false" xref="S3.E7.m1.10.10.1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E7.m1.10.10.1.1.2.2.2.4" xref="S3.E7.m1.10.10.1.1.2.2.3.cmml">,</mo><msup id="S3.E7.m1.10.10.1.1.2.2.2.2" xref="S3.E7.m1.10.10.1.1.2.2.2.2.cmml"><mi id="S3.E7.m1.10.10.1.1.2.2.2.2.2" xref="S3.E7.m1.10.10.1.1.2.2.2.2.2.cmml">x</mi><mrow id="S3.E7.m1.2.2.1.1" xref="S3.E7.m1.2.2.1.1.1.cmml"><mo id="S3.E7.m1.2.2.1.1.2" stretchy="false" xref="S3.E7.m1.2.2.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.2.2.1.1.1" xref="S3.E7.m1.2.2.1.1.1.cmml"><mi id="S3.E7.m1.2.2.1.1.1.2" mathvariant="normal" xref="S3.E7.m1.2.2.1.1.1.2.cmml">ℓ</mi><mo id="S3.E7.m1.2.2.1.1.1.1" xref="S3.E7.m1.2.2.1.1.1.1.cmml">+</mo><mi id="S3.E7.m1.2.2.1.1.1.3" xref="S3.E7.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E7.m1.2.2.1.1.3" stretchy="false" xref="S3.E7.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E7.m1.10.10.1.1.2.2.2.5" stretchy="false" xref="S3.E7.m1.10.10.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.10.10.1.1.3" xref="S3.E7.m1.10.10.1.1.3.cmml">≡</mo><mrow id="S3.E7.m1.10.10.1.1.4" xref="S3.E7.m1.10.10.1.1.4.cmml"><mfrac id="S3.E7.m1.10.10.1.1.4.2" xref="S3.E7.m1.10.10.1.1.4.2.cmml"><mn id="S3.E7.m1.10.10.1.1.4.2.2" xref="S3.E7.m1.10.10.1.1.4.2.2.cmml">1</mn><mi id="S3.E7.m1.10.10.1.1.4.2.3" xref="S3.E7.m1.10.10.1.1.4.2.3.cmml">π</mi></mfrac><mo id="S3.E7.m1.10.10.1.1.4.1" lspace="0.167em" xref="S3.E7.m1.10.10.1.1.4.1.cmml">⁢</mo><mrow id="S3.E7.m1.10.10.1.1.4.3.2" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml"><mi id="S3.E7.m1.9.9" xref="S3.E7.m1.9.9.cmml">arccos</mi><mo id="S3.E7.m1.10.10.1.1.4.3.2a" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml">⁡</mo><mrow id="S3.E7.m1.10.10.1.1.4.3.2.1" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml"><mo id="S3.E7.m1.10.10.1.1.4.3.2.1.1" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml">(</mo><mfrac id="S3.E7.m1.8.8" xref="S3.E7.m1.8.8.cmml"><mrow id="S3.E7.m1.4.4.2" xref="S3.E7.m1.4.4.2.cmml"><msubsup id="S3.E7.m1.4.4.2.4" xref="S3.E7.m1.4.4.2.4.cmml"><mi id="S3.E7.m1.4.4.2.4.2.2" xref="S3.E7.m1.4.4.2.4.2.2.cmml">x</mi><mi id="S3.E7.m1.4.4.2.4.3" xref="S3.E7.m1.4.4.2.4.3.cmml">T</mi><mrow id="S3.E7.m1.3.3.1.1.1.3" xref="S3.E7.m1.4.4.2.4.cmml"><mo id="S3.E7.m1.3.3.1.1.1.3.1" stretchy="false" xref="S3.E7.m1.4.4.2.4.cmml">(</mo><mi id="S3.E7.m1.3.3.1.1.1.1" mathvariant="normal" xref="S3.E7.m1.3.3.1.1.1.1.cmml">ℓ</mi><mo id="S3.E7.m1.3.3.1.1.1.3.2" stretchy="false" xref="S3.E7.m1.4.4.2.4.cmml">)</mo></mrow></msubsup><mo id="S3.E7.m1.4.4.2.3" lspace="0.222em" rspace="0.222em" xref="S3.E7.m1.4.4.2.3.cmml">⋅</mo><msubsup id="S3.E7.m1.4.4.2.5" xref="S3.E7.m1.4.4.2.5.cmml"><mi id="S3.E7.m1.4.4.2.5.2.2" xref="S3.E7.m1.4.4.2.5.2.2.cmml">x</mi><mi id="S3.E7.m1.4.4.2.5.3" xref="S3.E7.m1.4.4.2.5.3.cmml">T</mi><mrow id="S3.E7.m1.4.4.2.2.1.1" xref="S3.E7.m1.4.4.2.2.1.1.1.cmml"><mo id="S3.E7.m1.4.4.2.2.1.1.2" stretchy="false" xref="S3.E7.m1.4.4.2.2.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.4.4.2.2.1.1.1" xref="S3.E7.m1.4.4.2.2.1.1.1.cmml"><mi id="S3.E7.m1.4.4.2.2.1.1.1.2" mathvariant="normal" xref="S3.E7.m1.4.4.2.2.1.1.1.2.cmml">ℓ</mi><mo id="S3.E7.m1.4.4.2.2.1.1.1.1" xref="S3.E7.m1.4.4.2.2.1.1.1.1.cmml">+</mo><mi id="S3.E7.m1.4.4.2.2.1.1.1.3" xref="S3.E7.m1.4.4.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E7.m1.4.4.2.2.1.1.3" stretchy="false" xref="S3.E7.m1.4.4.2.2.1.1.1.cmml">)</mo></mrow></msubsup></mrow><mrow id="S3.E7.m1.8.8.6" xref="S3.E7.m1.8.8.6.cmml"><mrow id="S3.E7.m1.7.7.5.3.1" xref="S3.E7.m1.7.7.5.3.2.cmml"><mo id="S3.E7.m1.7.7.5.3.1.2" stretchy="false" xref="S3.E7.m1.7.7.5.3.2.1.cmml">‖</mo><msubsup id="S3.E7.m1.7.7.5.3.1.1" xref="S3.E7.m1.7.7.5.3.1.1.cmml"><mi id="S3.E7.m1.7.7.5.3.1.1.2.2" xref="S3.E7.m1.7.7.5.3.1.1.2.2.cmml">x</mi><mi id="S3.E7.m1.7.7.5.3.1.1.3" xref="S3.E7.m1.7.7.5.3.1.1.3.cmml">T</mi><mrow id="S3.E7.m1.5.5.3.1.1.3" xref="S3.E7.m1.7.7.5.3.1.1.cmml"><mo id="S3.E7.m1.5.5.3.1.1.3.1" stretchy="false" xref="S3.E7.m1.7.7.5.3.1.1.cmml">(</mo><mi id="S3.E7.m1.5.5.3.1.1.1" mathvariant="normal" xref="S3.E7.m1.5.5.3.1.1.1.cmml">ℓ</mi><mo id="S3.E7.m1.5.5.3.1.1.3.2" stretchy="false" xref="S3.E7.m1.7.7.5.3.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E7.m1.7.7.5.3.1.3" stretchy="false" xref="S3.E7.m1.7.7.5.3.2.1.cmml">‖</mo></mrow><mo id="S3.E7.m1.8.8.6.5" xref="S3.E7.m1.8.8.6.5.cmml">⁢</mo><mrow id="S3.E7.m1.8.8.6.4.1" xref="S3.E7.m1.8.8.6.4.2.cmml"><mo id="S3.E7.m1.8.8.6.4.1.2" stretchy="false" xref="S3.E7.m1.8.8.6.4.2.1.cmml">‖</mo><msubsup id="S3.E7.m1.8.8.6.4.1.1" xref="S3.E7.m1.8.8.6.4.1.1.cmml"><mi id="S3.E7.m1.8.8.6.4.1.1.2.2" xref="S3.E7.m1.8.8.6.4.1.1.2.2.cmml">x</mi><mi id="S3.E7.m1.8.8.6.4.1.1.3" xref="S3.E7.m1.8.8.6.4.1.1.3.cmml">T</mi><mrow id="S3.E7.m1.6.6.4.2.1.1" xref="S3.E7.m1.6.6.4.2.1.1.1.cmml"><mo id="S3.E7.m1.6.6.4.2.1.1.2" stretchy="false" xref="S3.E7.m1.6.6.4.2.1.1.1.cmml">(</mo><mrow id="S3.E7.m1.6.6.4.2.1.1.1" xref="S3.E7.m1.6.6.4.2.1.1.1.cmml"><mi id="S3.E7.m1.6.6.4.2.1.1.1.2" mathvariant="normal" xref="S3.E7.m1.6.6.4.2.1.1.1.2.cmml">ℓ</mi><mo id="S3.E7.m1.6.6.4.2.1.1.1.1" xref="S3.E7.m1.6.6.4.2.1.1.1.1.cmml">+</mo><mi id="S3.E7.m1.6.6.4.2.1.1.1.3" xref="S3.E7.m1.6.6.4.2.1.1.1.3.cmml">n</mi></mrow><mo id="S3.E7.m1.6.6.4.2.1.1.3" stretchy="false" xref="S3.E7.m1.6.6.4.2.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E7.m1.8.8.6.4.1.3" stretchy="false" xref="S3.E7.m1.8.8.6.4.2.1.cmml">‖</mo></mrow></mrow></mfrac><mo id="S3.E7.m1.10.10.1.1.4.3.2.1.2" rspace="0.170em" xref="S3.E7.m1.10.10.1.1.4.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E7.m1.10.10.1.2" xref="S3.E7.m1.10.10.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.10b"><apply id="S3.E7.m1.10.10.1.1.cmml" xref="S3.E7.m1.10.10.1"><equivalent id="S3.E7.m1.10.10.1.1.3.cmml" xref="S3.E7.m1.10.10.1.1.3"></equivalent><apply id="S3.E7.m1.10.10.1.1.2.cmml" xref="S3.E7.m1.10.10.1.1.2"><times id="S3.E7.m1.10.10.1.1.2.3.cmml" xref="S3.E7.m1.10.10.1.1.2.3"></times><ci id="S3.E7.m1.10.10.1.1.2.4.cmml" xref="S3.E7.m1.10.10.1.1.2.4">𝑑</ci><interval closure="open" id="S3.E7.m1.10.10.1.1.2.2.3.cmml" xref="S3.E7.m1.10.10.1.1.2.2.2"><apply id="S3.E7.m1.10.10.1.1.1.1.1.1.cmml" xref="S3.E7.m1.10.10.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.10.10.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.10.10.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E7.m1.10.10.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.10.10.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1">ℓ</ci></apply><apply id="S3.E7.m1.10.10.1.1.2.2.2.2.cmml" xref="S3.E7.m1.10.10.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.10.10.1.1.2.2.2.2.1.cmml" xref="S3.E7.m1.10.10.1.1.2.2.2.2">superscript</csymbol><ci id="S3.E7.m1.10.10.1.1.2.2.2.2.2.cmml" xref="S3.E7.m1.10.10.1.1.2.2.2.2.2">𝑥</ci><apply id="S3.E7.m1.2.2.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1"><plus id="S3.E7.m1.2.2.1.1.1.1.cmml" xref="S3.E7.m1.2.2.1.1.1.1"></plus><ci id="S3.E7.m1.2.2.1.1.1.2.cmml" xref="S3.E7.m1.2.2.1.1.1.2">ℓ</ci><ci id="S3.E7.m1.2.2.1.1.1.3.cmml" xref="S3.E7.m1.2.2.1.1.1.3">𝑛</ci></apply></apply></interval></apply><apply id="S3.E7.m1.10.10.1.1.4.cmml" xref="S3.E7.m1.10.10.1.1.4"><times id="S3.E7.m1.10.10.1.1.4.1.cmml" xref="S3.E7.m1.10.10.1.1.4.1"></times><apply id="S3.E7.m1.10.10.1.1.4.2.cmml" xref="S3.E7.m1.10.10.1.1.4.2"><divide id="S3.E7.m1.10.10.1.1.4.2.1.cmml" xref="S3.E7.m1.10.10.1.1.4.2"></divide><cn id="S3.E7.m1.10.10.1.1.4.2.2.cmml" type="integer" xref="S3.E7.m1.10.10.1.1.4.2.2">1</cn><ci id="S3.E7.m1.10.10.1.1.4.2.3.cmml" xref="S3.E7.m1.10.10.1.1.4.2.3">𝜋</ci></apply><apply id="S3.E7.m1.10.10.1.1.4.3.1.cmml" xref="S3.E7.m1.10.10.1.1.4.3.2"><arccos id="S3.E7.m1.9.9.cmml" xref="S3.E7.m1.9.9"></arccos><apply id="S3.E7.m1.8.8.cmml" xref="S3.E7.m1.8.8"><divide id="S3.E7.m1.8.8.7.cmml" xref="S3.E7.m1.8.8"></divide><apply id="S3.E7.m1.4.4.2.cmml" xref="S3.E7.m1.4.4.2"><ci id="S3.E7.m1.4.4.2.3.cmml" xref="S3.E7.m1.4.4.2.3">⋅</ci><apply id="S3.E7.m1.4.4.2.4.cmml" xref="S3.E7.m1.4.4.2.4"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.4.1.cmml" xref="S3.E7.m1.4.4.2.4">subscript</csymbol><apply id="S3.E7.m1.4.4.2.4.2.cmml" xref="S3.E7.m1.4.4.2.4"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.4.2.1.cmml" xref="S3.E7.m1.4.4.2.4">superscript</csymbol><ci id="S3.E7.m1.4.4.2.4.2.2.cmml" xref="S3.E7.m1.4.4.2.4.2.2">𝑥</ci><ci id="S3.E7.m1.3.3.1.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1">ℓ</ci></apply><ci id="S3.E7.m1.4.4.2.4.3.cmml" xref="S3.E7.m1.4.4.2.4.3">𝑇</ci></apply><apply id="S3.E7.m1.4.4.2.5.cmml" xref="S3.E7.m1.4.4.2.5"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.5.1.cmml" xref="S3.E7.m1.4.4.2.5">subscript</csymbol><apply id="S3.E7.m1.4.4.2.5.2.cmml" xref="S3.E7.m1.4.4.2.5"><csymbol cd="ambiguous" id="S3.E7.m1.4.4.2.5.2.1.cmml" xref="S3.E7.m1.4.4.2.5">superscript</csymbol><ci id="S3.E7.m1.4.4.2.5.2.2.cmml" xref="S3.E7.m1.4.4.2.5.2.2">𝑥</ci><apply id="S3.E7.m1.4.4.2.2.1.1.1.cmml" xref="S3.E7.m1.4.4.2.2.1.1"><plus id="S3.E7.m1.4.4.2.2.1.1.1.1.cmml" xref="S3.E7.m1.4.4.2.2.1.1.1.1"></plus><ci id="S3.E7.m1.4.4.2.2.1.1.1.2.cmml" xref="S3.E7.m1.4.4.2.2.1.1.1.2">ℓ</ci><ci id="S3.E7.m1.4.4.2.2.1.1.1.3.cmml" xref="S3.E7.m1.4.4.2.2.1.1.1.3">𝑛</ci></apply></apply><ci id="S3.E7.m1.4.4.2.5.3.cmml" xref="S3.E7.m1.4.4.2.5.3">𝑇</ci></apply></apply><apply id="S3.E7.m1.8.8.6.cmml" xref="S3.E7.m1.8.8.6"><times id="S3.E7.m1.8.8.6.5.cmml" xref="S3.E7.m1.8.8.6.5"></times><apply id="S3.E7.m1.7.7.5.3.2.cmml" xref="S3.E7.m1.7.7.5.3.1"><csymbol cd="latexml" id="S3.E7.m1.7.7.5.3.2.1.cmml" xref="S3.E7.m1.7.7.5.3.1.2">norm</csymbol><apply id="S3.E7.m1.7.7.5.3.1.1.cmml" xref="S3.E7.m1.7.7.5.3.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.7.7.5.3.1.1.1.cmml" xref="S3.E7.m1.7.7.5.3.1.1">subscript</csymbol><apply id="S3.E7.m1.7.7.5.3.1.1.2.cmml" xref="S3.E7.m1.7.7.5.3.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.7.7.5.3.1.1.2.1.cmml" xref="S3.E7.m1.7.7.5.3.1.1">superscript</csymbol><ci id="S3.E7.m1.7.7.5.3.1.1.2.2.cmml" xref="S3.E7.m1.7.7.5.3.1.1.2.2">𝑥</ci><ci id="S3.E7.m1.5.5.3.1.1.1.cmml" xref="S3.E7.m1.5.5.3.1.1.1">ℓ</ci></apply><ci id="S3.E7.m1.7.7.5.3.1.1.3.cmml" xref="S3.E7.m1.7.7.5.3.1.1.3">𝑇</ci></apply></apply><apply id="S3.E7.m1.8.8.6.4.2.cmml" xref="S3.E7.m1.8.8.6.4.1"><csymbol cd="latexml" id="S3.E7.m1.8.8.6.4.2.1.cmml" xref="S3.E7.m1.8.8.6.4.1.2">norm</csymbol><apply id="S3.E7.m1.8.8.6.4.1.1.cmml" xref="S3.E7.m1.8.8.6.4.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.6.4.1.1.1.cmml" xref="S3.E7.m1.8.8.6.4.1.1">subscript</csymbol><apply id="S3.E7.m1.8.8.6.4.1.1.2.cmml" xref="S3.E7.m1.8.8.6.4.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.8.8.6.4.1.1.2.1.cmml" xref="S3.E7.m1.8.8.6.4.1.1">superscript</csymbol><ci id="S3.E7.m1.8.8.6.4.1.1.2.2.cmml" xref="S3.E7.m1.8.8.6.4.1.1.2.2">𝑥</ci><apply id="S3.E7.m1.6.6.4.2.1.1.1.cmml" xref="S3.E7.m1.6.6.4.2.1.1"><plus id="S3.E7.m1.6.6.4.2.1.1.1.1.cmml" xref="S3.E7.m1.6.6.4.2.1.1.1.1"></plus><ci id="S3.E7.m1.6.6.4.2.1.1.1.2.cmml" xref="S3.E7.m1.6.6.4.2.1.1.1.2">ℓ</ci><ci id="S3.E7.m1.6.6.4.2.1.1.1.3.cmml" xref="S3.E7.m1.6.6.4.2.1.1.1.3">𝑛</ci></apply></apply><ci id="S3.E7.m1.8.8.6.4.1.1.3.cmml" xref="S3.E7.m1.8.8.6.4.1.1.3">𝑇</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.10c">d(x^{(\ell)},x^{(\ell+n)})\equiv\frac{1}{\pi}\arccos\left(\frac{x^{(\ell)}_{T}%
\cdot x^{(\ell+n)}_{T}}{\left|\!\left|x^{(\ell)}_{T}\right|\!\right|\left|\!%
\left|x^{(\ell+n)}_{T}\right|\!\right|}\right)\,,</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.10d">italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_ℓ + italic_n ) end_POSTSUPERSCRIPT ) ≡ divide start_ARG 1 end_ARG start_ARG italic_π end_ARG roman_arccos ( divide start_ARG italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ⋅ italic_x start_POSTSUPERSCRIPT ( roman_ℓ + italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG start_ARG | | italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT | | | | italic_x start_POSTSUPERSCRIPT ( roman_ℓ + italic_n ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT | | end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p2.5">여기서 내부 곱은 시퀀스의 최종 토큰 <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m1.1"><semantics id="S3.SS2.p2.2.m1.1a"><mi id="S3.SS2.p2.2.m1.1.1" xref="S3.SS2.p2.2.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m1.1b"><ci id="S3.SS2.p2.2.m1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m1.1d">italic_T</annotation></semantics></math>에 대한 모델의 숨겨진 차원을 넘고, <math alttext="|\!|\cdot|\!|" class="ltx_math_unparsed" display="inline" id="S3.SS2.p2.3.m2.1"><semantics id="S3.SS2.p2.3.m2.1a"><mrow id="S3.SS2.p2.3.m2.1b"><mo fence="false" id="S3.SS2.p2.3.m2.1.1" stretchy="false">|</mo><mo fence="false" id="S3.SS2.p2.3.m2.1.2" stretchy="false">|</mo><mo id="S3.SS2.p2.3.m2.1.3" lspace="0em" rspace="0em">⋅</mo><mo fence="false" id="S3.SS2.p2.3.m2.1.4" stretchy="false">|</mo><mo fence="false" id="S3.SS2.p2.3.m2.1.5" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m2.1c">|\!|\cdot|\!|</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m2.1d">| | ⋅ | |</annotation></semantics></math>는 <math alttext="L^{2}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m3.1"><semantics id="S3.SS2.p2.4.m3.1a"><msup id="S3.SS2.p2.4.m3.1.1" xref="S3.SS2.p2.4.m3.1.1.cmml"><mi id="S3.SS2.p2.4.m3.1.1.2" xref="S3.SS2.p2.4.m3.1.1.2.cmml">L</mi><mn id="S3.SS2.p2.4.m3.1.1.3" xref="S3.SS2.p2.4.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m3.1b"><apply id="S3.SS2.p2.4.m3.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m3.1.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1">superscript</csymbol><ci id="S3.SS2.p2.4.m3.1.1.2.cmml" xref="S3.SS2.p2.4.m3.1.1.2">𝐿</ci><cn id="S3.SS2.p2.4.m3.1.1.3.cmml" type="integer" xref="S3.SS2.p2.4.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m3.1c">L^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m3.1d">italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm을 나타내고, <math alttext="1/\pi" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m4.1"><semantics id="S3.SS2.p2.5.m4.1a"><mrow id="S3.SS2.p2.5.m4.1.1" xref="S3.SS2.p2.5.m4.1.1.cmml"><mn id="S3.SS2.p2.5.m4.1.1.2" xref="S3.SS2.p2.5.m4.1.1.2.cmml">1</mn><mo id="S3.SS2.p2.5.m4.1.1.1" xref="S3.SS2.p2.5.m4.1.1.1.cmml">/</mo><mi id="S3.SS2.p2.5.m4.1.1.3" xref="S3.SS2.p2.5.m4.1.1.3.cmml">π</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m4.1b"><apply id="S3.SS2.p2.5.m4.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1"><divide id="S3.SS2.p2.5.m4.1.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1.1"></divide><cn id="S3.SS2.p2.5.m4.1.1.2.cmml" type="integer" xref="S3.SS2.p2.5.m4.1.1.2">1</cn><ci id="S3.SS2.p2.5.m4.1.1.3.cmml" xref="S3.SS2.p2.5.m4.1.1.3">𝜋</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m4.1c">1/\pi</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m4.1d">1 / italic_π</annotation></semantics></math>의 인자는 관례이다. <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Two comments: <em class="ltx_emph ltx_font_italic" id="footnote5.1">(i)</em>, we do not expect our choice of angular distance – in lieu of any other reasonable metric, e.g., such as cosine similarity – to be particular significant; and <em class="ltx_emph ltx_font_italic" id="footnote5.2">(ii)</em>, we chose to focus on the final token since, due to the causal attention mask, its embedding is the only one that depends on the entire sequence. </span></span></span> 이 거리는 낮은 변동 추정치를 얻을 수 있을 만큼 충분히 크지만 전체적으로 상당히 작아야 하는 다수의 예에 걸쳐 합산되어야 한다.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">최종 단계의 "선택성"에 대해 자세히 살펴보면, 질의 응답 벤치마크인 cf에서 성능 저하가 거의 없음을 알 수 있다. <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>(d) 및 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS1" title="4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.1</span></a>의 다른 것들은 소량의 미세 조정으로 더 큰 가지치기 분수로 확장될 수 있다. 자원 제약들 및 프루닝된 모델의 의도된 적용에 따라, 이것은 필요하지 않을 수 있다. 그러나 치유 절차는 당혹감, cf에 상당한 영향을 미친다. <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>(d) and others in §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS2" title="4.2 Loss on next-token predictions ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">각 거리 측정과 치유의 경우 최종 목표가 다운스트림 작업에 대한 모델을 감독하는 것이라면 해당 데이터 세트에서 샘플의 거리를 평가한 다음 치유 프로세스를 SFT와 결합하는 것이 유용할 수 있다. 대조적으로, 가장 일반적인 경우에는 모델이 원래 사전 훈련된 통계에 근사하는 사전 훈련 데이터 세트로 거리를 측정하고 치유하는 것이 가장 자연스럽다.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.4">마지막으로, LLM 헤드 앞에 최종 레이어를 제외하고 가장 깊은 레이어를 떨어뜨린 다음 (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p5.4.1">non-optionally</em>) 손상을 치유하는 다른 모델 패밀리에 대한 각도 거리를 분석하여 영감을 얻은 훨씬 더 간단한 가지치기 전략을 조사했다. 완전한 명확성을 위해, 이것은 우리가 <math alttext="L" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_L</annotation></semantics></math>-계층 모델로부터 <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_n</annotation></semantics></math> 계층들을 프루닝하고 있다면, 우리는 계층들 <math alttext="(L-n)" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3.1"><semantics id="S3.SS2.p5.3.m3.1a"><mrow id="S3.SS2.p5.3.m3.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.1.cmml"><mo id="S3.SS2.p5.3.m3.1.1.1.2" stretchy="false" xref="S3.SS2.p5.3.m3.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p5.3.m3.1.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.1.cmml"><mi id="S3.SS2.p5.3.m3.1.1.1.1.2" xref="S3.SS2.p5.3.m3.1.1.1.1.2.cmml">L</mi><mo id="S3.SS2.p5.3.m3.1.1.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.1.1.cmml">−</mo><mi id="S3.SS2.p5.3.m3.1.1.1.1.3" xref="S3.SS2.p5.3.m3.1.1.1.1.3.cmml">n</mi></mrow><mo id="S3.SS2.p5.3.m3.1.1.1.3" stretchy="false" xref="S3.SS2.p5.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1"><minus id="S3.SS2.p5.3.m3.1.1.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.1"></minus><ci id="S3.SS2.p5.3.m3.1.1.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.2">𝐿</ci><ci id="S3.SS2.p5.3.m3.1.1.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">(L-n)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m3.1d">( italic_L - italic_n )</annotation></semantics></math> 내지 <math alttext="(L-1)" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m4.1"><semantics id="S3.SS2.p5.4.m4.1a"><mrow id="S3.SS2.p5.4.m4.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.1.cmml"><mo id="S3.SS2.p5.4.m4.1.1.1.2" stretchy="false" xref="S3.SS2.p5.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p5.4.m4.1.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.1.cmml"><mi id="S3.SS2.p5.4.m4.1.1.1.1.2" xref="S3.SS2.p5.4.m4.1.1.1.1.2.cmml">L</mi><mo id="S3.SS2.p5.4.m4.1.1.1.1.1" xref="S3.SS2.p5.4.m4.1.1.1.1.1.cmml">−</mo><mn id="S3.SS2.p5.4.m4.1.1.1.1.3" xref="S3.SS2.p5.4.m4.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p5.4.m4.1.1.1.3" stretchy="false" xref="S3.SS2.p5.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1"><minus id="S3.SS2.p5.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.1"></minus><ci id="S3.SS2.p5.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.1.1.2">𝐿</ci><cn id="S3.SS2.p5.4.m4.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.p5.4.m4.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">(L-1)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.4.m4.1d">( italic_L - 1 )</annotation></semantics></math>를 제거할 것이다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">이 절에서는 서로 다른 질의응답(QA) 벤치마크에 대한 가지치기 전략의 효과를 입증하고 성능에서 강력한 가지치기 기반 전환(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS1" title="4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.1</span></a>)을 강조하지만, 대조적으로 치유된 가지치기 모델의 자기회귀적 복잡성은 전이점(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS2" title="4.2 Loss on next-token predictions ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.2</span></a>)에 걸쳐 연속적이라는 것을 발견했으며, 모델 크기와 패밀리에 걸쳐 서로 다른 계층 간의 유사성 통계를 비교한 후(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS3" title="4.3 Angular distances between representations ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.3</span></a>), 주요 유사성 정보 가지치기 전략을 더 간단한 제거 계층 전략(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS4" title="4.4 A simpler pruning strategy ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.4</span></a>)과 대조한다.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">실험을 위해 32~80개의 프루닝되지 않은 총 레이어에 걸쳐 2.7B에서 70B 매개변수로 다양한 대규모 LLM을 프루닝했다. 구체적으로, Llama-2 계열 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib20" title="">2023a</a>)</cite>, Qwen 계열 <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib77" title="">2023</a>)</cite>, Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib78" title="">2023b</a>)</cite>, Phi-2 <cite class="ltx_cite ltx_citemacro_cite">Javaheripi and Bubeck (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib79" title="">2023</a>)</cite>의 모델을 사용하였다. 이러한 모델의 경우 QLoRA <cite class="ltx_cite ltx_citemacro_cite">Dettmers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>를 사용하여 "힐링" 단계를 실행했다. 우리의 모델은 4비트 정밀도로 양자화된 다음 효율적인 훈련을 위해 QLoRA를 사용하여 공통 사전 훈련 데이터 세트인 거대 클린 크롤드 코퍼스(C4) <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib80" title="">2020</a>)</cite>의 164M 또는 328M 토큰에 대해 미세 조정되었다. 그 결과 <em class="ltx_emph ltx_font_italic" id="S4.p2.1.1">each experiments of ours is performed a single A<math alttext="100" class="ltx_Math" display="inline" id="S4.p2.1.1.m1.1"><semantics id="S4.p2.1.1.m1.1a"><mn id="S4.p2.1.1.m1.1.1" mathvariant="normal" xref="S4.p2.1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.1.m1.1b"><cn id="S4.p2.1.1.m1.1.1.cmml" type="integer" xref="S4.p2.1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.1.m1.1d">100</annotation></semantics></math> GPU</em>. QA 평가에는 일반적인 세계 지식 및 문제 해결 벤치마크인 MMLU <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib81" title="">2020</a>)</cite>와 텍스트 자체에서 답을 추론해야 하는 일반적인 예/아니오 읽기 이해 벤치마크인 BoolQ <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib82" title="">2019</a>)</cite>를 사용하였다. 모델의 세부 사항, 치유 절차, 데이터 세트 선택 및 평가 세부 사항은 부록 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1" title="Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A</span></a>에서 찾을 수 있으며, 다른 하이퍼파라미터 선택의 삭제는 부록 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2" title="Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B</span></a>에서 찾을 수 있다.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Accuracy on QA benchmarks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.2">첫 번째 결과 세트는 그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‣ 4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>에 나와 있으며, 여기서 제거되는 레이어의 비율에 대한 함수로서 <math alttext="5" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn id="S4.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">5</annotation></semantics></math>-shot MMLU 정확도를 플롯한다: 왼쪽 패널에서 Llama-2 패밀리를 제시하고, 중간 패널에서 Qwen 패밀리의 모델을 제시하고, 오른쪽 패널에서 Mistral-7B와 Phi-2를 보여준다. 다른 총 레이어 수의 모델을 더 잘 비교하기 위해, 이러한 플롯에서 제거되는 레이어의 비율만큼 <math alttext="x" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_x</annotation></semantics></math> 축을 정규화하는 것을 선택했다. MLU에는 4개의 가능한 응답이 있는 객관식 질문이 포함되어 있기 때문에 무작위 추측의 예상 정확도는 25%이다.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F2.1"><span class="ltx_text" id="S4.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="204" id="S4.F2.1.1.g1" src="https://arxiv.org/html/2403.17887v1/x2.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2:</span>MMLU accuracy (5-shot) vs. 겹겹이 갈라진 층들</figcaption>
dropped
for different model families.
(<em class="ltx_emph ltx_font_italic" id="S4.F2.5.1">Left:</em> Llama-2 family; <em class="ltx_emph ltx_font_italic" id="S4.F2.6.2">Middle:</em> Qwen family; <em class="ltx_emph ltx_font_italic" id="S4.F2.7.3">Right:</em> Mistral-7B and Phi-2.)
The
solid lines
represent
performance
after dropping layers and healing,
dotted lines
show
performance after dropping layers only (no healing),
and the dashed gray line is the score for guessing randomly.
For these models, healing leads to modest improvements, and performances
are quite robust
until 20%-55% pruning fractions, depending on model family and size, at which point they transitions to random guessing.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">중요하게도, 우리는 라마-2 계열의 모델의 경우 45%-55%, 미스트랄 7B의 경우 35%, Phi-2의 경우 25%, Qwen 계열의 모델의 경우 20% 정도의 가지치기 부분에서 무작위 정확도로 급격한 전환이 뒤따르는 강력한 성능의 특징적인 평평한 영역을 볼 수 있다. 이것은 모델의 최고 점수를 달성하는 데 필요한 필수 지식이 중요한 모델 의존적 임계값에서 지식이 손실될 때까지 상당한 층 제거에 의해 제거되지 않는다는 것을 의미한다. <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>This effect is rather robust to choice of QA benchmark: in Appendix Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.F6" title="Figure 6 ‣ A.2 Evaluation details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">6</span></a> we plot the average 0-shot BoolQ accuracy for our model families and observe analogous behavior.</span></span></span> 힐링이 있는 곡선과 없는 곡선을 비교하여, 우리는 미세 조정이 가지치기되지 않은 성능을 더 잘 보존하고 무작위 추측으로 위상 전이를 약간 더 큰 가지치기 분수로 밀어냄으로써 약간의 개선을 제공한다는 것을 알 수 있다.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">일반적으로 레이어 프루닝이 더 크고 더 깊은 모델, 예를 들어 라마-2-13B 및 라마-2-70B에 대해 더 강력하다는 것을 알 수 있으며, 이는 더 작은 모델이 더 많이 훈련되어 매개 변수가 덜 중복되거나 더 깊은 모델이 절대적인 의미에서 더 많은 레이어를 잃을 수 있다는 사실과 관련될 수 있다고 가정한다. 또한 Qwen 계열은 이상한데, 우리는 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.SS3" title="4.3 Angular distances between representations ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4.3</span></a>에서 더 자세히 설명할 것이다.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Loss
on
next-token predictions</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">이 섹션에서는 C4 검증 데이터 세트의 하위 집합에서 평가할 때 사전 훈련 최적화 목표인 다음 토큰 예측의 교차 엔트로피 손실에 대한 레이어 가지치기 효과를 살펴본다. <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We make sure that none of the validation data are seen during the healing stage.</span></span></span> 서로 다른 크기의 어휘 <math alttext="V" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">V</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_V</annotation></semantics></math>를 가진 모델에 걸쳐 공정한 비교를 하기 위해, 균일한 확률로 랜덤하게 샘플링 토큰의 손실에 해당하는 <math alttext="\log V" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">log</mi><mo id="S4.SS2.p1.2.m2.1.1a" lspace="0.167em" xref="S4.SS2.p1.2.m2.1.1.cmml">⁡</mo><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><log id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></log><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\log V</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">roman_log italic_V</annotation></semantics></math>에 의해 손실을 정규화한다. (자세한 내용은 부록 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS2" title="A.2 Evaluation details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.2</span></a> 참조)</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F3" title="Figure 3 ‣ 4.2 Loss on next-token predictions ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a>에서, 우리는 치유 후(왼쪽 패널) 및 치유 전(오른쪽 패널)의 7개 모델 모두에 대한 정규화된 C4 검증 손실을 제거된 분획 층의 함수로 플롯한다. 치유가 없으면 QA 벤치마크 정확도도 무작위 추측으로 급격히 전환되는 대략 가지치기 부분에서 각 모델에 대해 무작위 추측으로의 다소 날카로운(약간) 전환이 있음을 알 수 있으며, 이는 모델이 이 시점 cf에서 절망적으로 해를 입음을 시사한다. <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‣ 4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a> 그림. 다음으로, 두 도표의 척도를 대조하여 치유가 모든 모델의 다음 토큰 예측 능력을 거의 가지치 않은 수준으로 크게 회복하고 손실이 층 탈락에 따라 천천히 선형적으로 증가한다는 것을 알 수 있다. 과학적 관점에서 가장 놀라운 것은 이전에 QA 벤치마크에 대한 날카로운 전환을 발견한 가지치기 분수를 통한 치유 후 연속성이다. 이 디커플링은 MMLU 및 BoolQ와 같은 다운스트림 태스크의 성능과 교차 엔트로피 손실과 같은 지속적인 성능 측정 간의 연결을 끊는 한 가지 방법을 보여준다. <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>This is consistent with Ref. <cite class="ltx_cite ltx_citemacro_cite">Schaeffer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib83" title="">2023</a>)</cite> that argued jumps in one kind of metric may not be visible in others. </span></span></span></p>
</div>
<figure class="ltx_figure" id="S4.F3">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F3.1"><span class="ltx_text" id="S4.F3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="224" id="S4.F3.1.1.g1" src="https://arxiv.org/html/2403.17887v1/x3.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span>Normalized C4 validation loss vs. 떨어진 층의 분율.</figcaption>
before healing (<em class="ltx_emph ltx_font_italic" id="S4.F3.4.1">left</em>) and after healing (<em class="ltx_emph ltx_font_italic" id="S4.F3.5.2">right</em>);
each curve is normalized by the cross-entropy loss of sampling uniformly from the model’s vocabulary.
For the experiments before healing, the loss for each model transitions to random guessing (gray dashed line) at approximately the same pruning fractions that the QA benchmarks transition to random guessing; after healing, there is continuity through the regions of sharp transition on QA tasks, cf. Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‣ 4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>. Contrasting the overall scale of both plots, it’s clear that healing significantly restores the performance on next-token prediction to near-unpruned levels.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Angular distances between representations</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">프루닝 전략에서 각도 거리(<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E7" title="7 ‣ 3.2 Layer-pruning algorithm(s) ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a>)의 중심 역할을 감안할 때, 7개 모델에 걸쳐 이러한 거리를 살펴보기 위해 하위 섹션을 취하자. 이 분석을 위해 각 모델의 각도 거리는 C4 검증 세트의 10k 샘플에 대해 평균을 냈다.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.13">Recall from earlier Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">1</span></a>(c): for Llama-2-70B this plotted the angular distance <math alttext="d(x^{(\ell)},x^{(\ell+n)})" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.4"><semantics id="S4.SS3.p2.1.m1.4a"><mrow id="S4.SS3.p2.1.m1.4.4" xref="S4.SS3.p2.1.m1.4.4.cmml"><mi id="S4.SS3.p2.1.m1.4.4.4" xref="S4.SS3.p2.1.m1.4.4.4.cmml">d</mi><mo id="S4.SS3.p2.1.m1.4.4.3" xref="S4.SS3.p2.1.m1.4.4.3.cmml">⁢</mo><mrow id="S4.SS3.p2.1.m1.4.4.2.2" xref="S4.SS3.p2.1.m1.4.4.2.3.cmml"><mo id="S4.SS3.p2.1.m1.4.4.2.2.3" stretchy="false" xref="S4.SS3.p2.1.m1.4.4.2.3.cmml">(</mo><msup id="S4.SS3.p2.1.m1.3.3.1.1.1" xref="S4.SS3.p2.1.m1.3.3.1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.3.3.1.1.1.2" xref="S4.SS3.p2.1.m1.3.3.1.1.1.2.cmml">x</mi><mrow id="S4.SS3.p2.1.m1.1.1.1.3" xref="S4.SS3.p2.1.m1.3.3.1.1.1.cmml"><mo id="S4.SS3.p2.1.m1.1.1.1.3.1" stretchy="false" xref="S4.SS3.p2.1.m1.3.3.1.1.1.cmml">(</mo><mi id="S4.SS3.p2.1.m1.1.1.1.1" mathvariant="normal" xref="S4.SS3.p2.1.m1.1.1.1.1.cmml">ℓ</mi><mo id="S4.SS3.p2.1.m1.1.1.1.3.2" stretchy="false" xref="S4.SS3.p2.1.m1.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="S4.SS3.p2.1.m1.4.4.2.2.4" xref="S4.SS3.p2.1.m1.4.4.2.3.cmml">,</mo><msup id="S4.SS3.p2.1.m1.4.4.2.2.2" xref="S4.SS3.p2.1.m1.4.4.2.2.2.cmml"><mi id="S4.SS3.p2.1.m1.4.4.2.2.2.2" xref="S4.SS3.p2.1.m1.4.4.2.2.2.2.cmml">x</mi><mrow id="S4.SS3.p2.1.m1.2.2.1.1" xref="S4.SS3.p2.1.m1.2.2.1.1.1.cmml"><mo id="S4.SS3.p2.1.m1.2.2.1.1.2" stretchy="false" xref="S4.SS3.p2.1.m1.2.2.1.1.1.cmml">(</mo><mrow id="S4.SS3.p2.1.m1.2.2.1.1.1" xref="S4.SS3.p2.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.2.2.1.1.1.2" mathvariant="normal" xref="S4.SS3.p2.1.m1.2.2.1.1.1.2.cmml">ℓ</mi><mo id="S4.SS3.p2.1.m1.2.2.1.1.1.1" xref="S4.SS3.p2.1.m1.2.2.1.1.1.1.cmml">+</mo><mi id="S4.SS3.p2.1.m1.2.2.1.1.1.3" xref="S4.SS3.p2.1.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="S4.SS3.p2.1.m1.2.2.1.1.3" stretchy="false" xref="S4.SS3.p2.1.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S4.SS3.p2.1.m1.4.4.2.2.5" stretchy="false" xref="S4.SS3.p2.1.m1.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.4b"><apply id="S4.SS3.p2.1.m1.4.4.cmml" xref="S4.SS3.p2.1.m1.4.4"><times id="S4.SS3.p2.1.m1.4.4.3.cmml" xref="S4.SS3.p2.1.m1.4.4.3"></times><ci id="S4.SS3.p2.1.m1.4.4.4.cmml" xref="S4.SS3.p2.1.m1.4.4.4">𝑑</ci><interval closure="open" id="S4.SS3.p2.1.m1.4.4.2.3.cmml" xref="S4.SS3.p2.1.m1.4.4.2.2"><apply id="S4.SS3.p2.1.m1.3.3.1.1.1.cmml" xref="S4.SS3.p2.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.3.3.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.3.3.1.1.1">superscript</csymbol><ci id="S4.SS3.p2.1.m1.3.3.1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.3.3.1.1.1.2">𝑥</ci><ci id="S4.SS3.p2.1.m1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1">ℓ</ci></apply><apply id="S4.SS3.p2.1.m1.4.4.2.2.2.cmml" xref="S4.SS3.p2.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.4.4.2.2.2.1.cmml" xref="S4.SS3.p2.1.m1.4.4.2.2.2">superscript</csymbol><ci id="S4.SS3.p2.1.m1.4.4.2.2.2.2.cmml" xref="S4.SS3.p2.1.m1.4.4.2.2.2.2">𝑥</ci><apply id="S4.SS3.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS3.p2.1.m1.2.2.1.1"><plus id="S4.SS3.p2.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.2.2.1.1.1.1"></plus><ci id="S4.SS3.p2.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.2.2.1.1.1.2">ℓ</ci><ci id="S4.SS3.p2.1.m1.2.2.1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.2.2.1.1.1.3">𝑛</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.4c">d(x^{(\ell)},x^{(\ell+n)})</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.4d">italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_ℓ + italic_n ) end_POSTSUPERSCRIPT )</annotation></semantics></math> that compared the <math alttext="\ell" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mi id="S4.SS3.p2.2.m2.1.1" mathvariant="normal" xref="S4.SS3.p2.2.m2.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><ci id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">roman_ℓ</annotation></semantics></math>-th layer to the <math alttext="(\ell+n)" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.1.cmml"><mo id="S4.SS3.p2.3.m3.1.1.1.2" stretchy="false" xref="S4.SS3.p2.3.m3.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.p2.3.m3.1.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.1.1.2" mathvariant="normal" xref="S4.SS3.p2.3.m3.1.1.1.1.2.cmml">ℓ</mi><mo id="S4.SS3.p2.3.m3.1.1.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.1.1.cmml">+</mo><mi id="S4.SS3.p2.3.m3.1.1.1.1.3" xref="S4.SS3.p2.3.m3.1.1.1.1.3.cmml">n</mi></mrow><mo id="S4.SS3.p2.3.m3.1.1.1.3" stretchy="false" xref="S4.SS3.p2.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1"><plus id="S4.SS3.p2.3.m3.1.1.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1.1.1"></plus><ci id="S4.SS3.p2.3.m3.1.1.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.1.1.2">ℓ</ci><ci id="S4.SS3.p2.3.m3.1.1.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">(\ell+n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">( roman_ℓ + italic_n )</annotation></semantics></math>-th layer, across all initial indexes <math alttext="\ell" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mi id="S4.SS3.p2.4.m4.1.1" mathvariant="normal" xref="S4.SS3.p2.4.m4.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><ci id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">roman_ℓ</annotation></semantics></math> for block sizes from <math alttext="n=1" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mrow id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml"><mi id="S4.SS3.p2.5.m5.1.1.2" xref="S4.SS3.p2.5.m5.1.1.2.cmml">n</mi><mo id="S4.SS3.p2.5.m5.1.1.1" xref="S4.SS3.p2.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.5.m5.1.1.3" xref="S4.SS3.p2.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><apply id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1"><eq id="S4.SS3.p2.5.m5.1.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1.1"></eq><ci id="S4.SS3.p2.5.m5.1.1.2.cmml" xref="S4.SS3.p2.5.m5.1.1.2">𝑛</ci><cn id="S4.SS3.p2.5.m5.1.1.3.cmml" type="integer" xref="S4.SS3.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">n=1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">italic_n = 1</annotation></semantics></math> to <math alttext="n=64" class="ltx_Math" display="inline" id="S4.SS3.p2.6.m6.1"><semantics id="S4.SS3.p2.6.m6.1a"><mrow id="S4.SS3.p2.6.m6.1.1" xref="S4.SS3.p2.6.m6.1.1.cmml"><mi id="S4.SS3.p2.6.m6.1.1.2" xref="S4.SS3.p2.6.m6.1.1.2.cmml">n</mi><mo id="S4.SS3.p2.6.m6.1.1.1" xref="S4.SS3.p2.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS3.p2.6.m6.1.1.3" xref="S4.SS3.p2.6.m6.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.6.m6.1b"><apply id="S4.SS3.p2.6.m6.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1"><eq id="S4.SS3.p2.6.m6.1.1.1.cmml" xref="S4.SS3.p2.6.m6.1.1.1"></eq><ci id="S4.SS3.p2.6.m6.1.1.2.cmml" xref="S4.SS3.p2.6.m6.1.1.2">𝑛</ci><cn id="S4.SS3.p2.6.m6.1.1.3.cmml" type="integer" xref="S4.SS3.p2.6.m6.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.6.m6.1c">n=64</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.6.m6.1d">italic_n = 64</annotation></semantics></math>; the minimum of the curves, <math alttext="\ell^{\star}(n)" class="ltx_Math" display="inline" id="S4.SS3.p2.7.m7.1"><semantics id="S4.SS3.p2.7.m7.1a"><mrow id="S4.SS3.p2.7.m7.1.2" xref="S4.SS3.p2.7.m7.1.2.cmml"><msup id="S4.SS3.p2.7.m7.1.2.2" xref="S4.SS3.p2.7.m7.1.2.2.cmml"><mi id="S4.SS3.p2.7.m7.1.2.2.2" mathvariant="normal" xref="S4.SS3.p2.7.m7.1.2.2.2.cmml">ℓ</mi><mo id="S4.SS3.p2.7.m7.1.2.2.3" xref="S4.SS3.p2.7.m7.1.2.2.3.cmml">⋆</mo></msup><mo id="S4.SS3.p2.7.m7.1.2.1" xref="S4.SS3.p2.7.m7.1.2.1.cmml">⁢</mo><mrow id="S4.SS3.p2.7.m7.1.2.3.2" xref="S4.SS3.p2.7.m7.1.2.cmml"><mo id="S4.SS3.p2.7.m7.1.2.3.2.1" stretchy="false" xref="S4.SS3.p2.7.m7.1.2.cmml">(</mo><mi id="S4.SS3.p2.7.m7.1.1" xref="S4.SS3.p2.7.m7.1.1.cmml">n</mi><mo id="S4.SS3.p2.7.m7.1.2.3.2.2" stretchy="false" xref="S4.SS3.p2.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.7.m7.1b"><apply id="S4.SS3.p2.7.m7.1.2.cmml" xref="S4.SS3.p2.7.m7.1.2"><times id="S4.SS3.p2.7.m7.1.2.1.cmml" xref="S4.SS3.p2.7.m7.1.2.1"></times><apply id="S4.SS3.p2.7.m7.1.2.2.cmml" xref="S4.SS3.p2.7.m7.1.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.7.m7.1.2.2.1.cmml" xref="S4.SS3.p2.7.m7.1.2.2">superscript</csymbol><ci id="S4.SS3.p2.7.m7.1.2.2.2.cmml" xref="S4.SS3.p2.7.m7.1.2.2.2">ℓ</ci><ci id="S4.SS3.p2.7.m7.1.2.2.3.cmml" xref="S4.SS3.p2.7.m7.1.2.2.3">⋆</ci></apply><ci id="S4.SS3.p2.7.m7.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.7.m7.1c">\ell^{\star}(n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.7.m7.1d">roman_ℓ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math>, gave the optimal block to prune for a given <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.p2.8.m8.1"><semantics id="S4.SS3.p2.8.m8.1a"><mi id="S4.SS3.p2.8.m8.1.1" xref="S4.SS3.p2.8.m8.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.8.m8.1b"><ci id="S4.SS3.p2.8.m8.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.8.m8.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.8.m8.1d">italic_n</annotation></semantics></math>, cf. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E6" title="6 ‣ item 2 ‣ 3.2 Layer-pruning algorithm(s) ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">6</span></a>). A more compact way to display this same data is shown in the heat maps of Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F4" title="Figure 4 ‣ 4.3 Angular distances between representations ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">4</span></a>: each square is colored to depict the row-normalized angular distance between layer <math alttext="\ell" class="ltx_Math" display="inline" id="S4.SS3.p2.9.m9.1"><semantics id="S4.SS3.p2.9.m9.1a"><mi id="S4.SS3.p2.9.m9.1.1" mathvariant="normal" xref="S4.SS3.p2.9.m9.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.9.m9.1b"><ci id="S4.SS3.p2.9.m9.1.1.cmml" xref="S4.SS3.p2.9.m9.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.9.m9.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.9.m9.1d">roman_ℓ</annotation></semantics></math> and <math alttext="\ell+n" class="ltx_Math" display="inline" id="S4.SS3.p2.10.m10.1"><semantics id="S4.SS3.p2.10.m10.1a"><mrow id="S4.SS3.p2.10.m10.1.1" xref="S4.SS3.p2.10.m10.1.1.cmml"><mi id="S4.SS3.p2.10.m10.1.1.2" mathvariant="normal" xref="S4.SS3.p2.10.m10.1.1.2.cmml">ℓ</mi><mo id="S4.SS3.p2.10.m10.1.1.1" xref="S4.SS3.p2.10.m10.1.1.1.cmml">+</mo><mi id="S4.SS3.p2.10.m10.1.1.3" xref="S4.SS3.p2.10.m10.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.10.m10.1b"><apply id="S4.SS3.p2.10.m10.1.1.cmml" xref="S4.SS3.p2.10.m10.1.1"><plus id="S4.SS3.p2.10.m10.1.1.1.cmml" xref="S4.SS3.p2.10.m10.1.1.1"></plus><ci id="S4.SS3.p2.10.m10.1.1.2.cmml" xref="S4.SS3.p2.10.m10.1.1.2">ℓ</ci><ci id="S4.SS3.p2.10.m10.1.1.3.cmml" xref="S4.SS3.p2.10.m10.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.10.m10.1c">\ell+n</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.10.m10.1d">roman_ℓ + italic_n</annotation></semantics></math> across all possible <math alttext="\ell" class="ltx_Math" display="inline" id="S4.SS3.p2.11.m11.1"><semantics id="S4.SS3.p2.11.m11.1a"><mi id="S4.SS3.p2.11.m11.1.1" mathvariant="normal" xref="S4.SS3.p2.11.m11.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.11.m11.1b"><ci id="S4.SS3.p2.11.m11.1.1.cmml" xref="S4.SS3.p2.11.m11.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.11.m11.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.11.m11.1d">roman_ℓ</annotation></semantics></math>, and <math alttext="n" class="ltx_Math" display="inline" id="S4.SS3.p2.12.m12.1"><semantics id="S4.SS3.p2.12.m12.1a"><mi id="S4.SS3.p2.12.m12.1.1" xref="S4.SS3.p2.12.m12.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.12.m12.1b"><ci id="S4.SS3.p2.12.m12.1.1.cmml" xref="S4.SS3.p2.12.m12.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.12.m12.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.12.m12.1d">italic_n</annotation></semantics></math> up to very large fractions of the total number of layers; the optimal layer to prune for a given block size, <math alttext="\ell^{*}(n)" class="ltx_Math" display="inline" id="S4.SS3.p2.13.m13.1"><semantics id="S4.SS3.p2.13.m13.1a"><mrow id="S4.SS3.p2.13.m13.1.2" xref="S4.SS3.p2.13.m13.1.2.cmml"><msup id="S4.SS3.p2.13.m13.1.2.2" xref="S4.SS3.p2.13.m13.1.2.2.cmml"><mi id="S4.SS3.p2.13.m13.1.2.2.2" mathvariant="normal" xref="S4.SS3.p2.13.m13.1.2.2.2.cmml">ℓ</mi><mo id="S4.SS3.p2.13.m13.1.2.2.3" xref="S4.SS3.p2.13.m13.1.2.2.3.cmml">*</mo></msup><mo id="S4.SS3.p2.13.m13.1.2.1" xref="S4.SS3.p2.13.m13.1.2.1.cmml">⁢</mo><mrow id="S4.SS3.p2.13.m13.1.2.3.2" xref="S4.SS3.p2.13.m13.1.2.cmml"><mo id="S4.SS3.p2.13.m13.1.2.3.2.1" stretchy="false" xref="S4.SS3.p2.13.m13.1.2.cmml">(</mo><mi id="S4.SS3.p2.13.m13.1.1" xref="S4.SS3.p2.13.m13.1.1.cmml">n</mi><mo id="S4.SS3.p2.13.m13.1.2.3.2.2" stretchy="false" xref="S4.SS3.p2.13.m13.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.13.m13.1b"><apply id="S4.SS3.p2.13.m13.1.2.cmml" xref="S4.SS3.p2.13.m13.1.2"><times id="S4.SS3.p2.13.m13.1.2.1.cmml" xref="S4.SS3.p2.13.m13.1.2.1"></times><apply id="S4.SS3.p2.13.m13.1.2.2.cmml" xref="S4.SS3.p2.13.m13.1.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.13.m13.1.2.2.1.cmml" xref="S4.SS3.p2.13.m13.1.2.2">superscript</csymbol><ci id="S4.SS3.p2.13.m13.1.2.2.2.cmml" xref="S4.SS3.p2.13.m13.1.2.2.2">ℓ</ci><times id="S4.SS3.p2.13.m13.1.2.2.3.cmml" xref="S4.SS3.p2.13.m13.1.2.2.3"></times></apply><ci id="S4.SS3.p2.13.m13.1.1.cmml" xref="S4.SS3.p2.13.m13.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.13.m13.1c">\ell^{*}(n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.13.m13.1d">roman_ℓ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math>, corresponds to the minimal distance in each row.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">모델 전반에 걸쳐 두 가지 일반화를 수행합니다. <em class="ltx_emph ltx_font_italic" id="S4.SS3.p3.1.1">(i)</em> 가장 작은 거리는 더 깊은 블록에서 발견되며, 이는 더 깊은 계층이 일반적으로 서로 매우 유사하고 더 쉽게 드롭될 수 있음을 의미합니다. <em class="ltx_emph ltx_font_italic" id="S4.SS3.p3.1.2">(ii)</em> 가장 깊은 블록 간의 거리 – 마지막 계층을 포함하는 블록 – 최대 또는 거의 최대 값을 취합니다. 대체로 사실이지만 몇 가지 예외가 있다. 일부 모델, 예를 들어 Phi-2-2.7B 또는 일부 모델에서 가장 큰 블록, 예를 들어 Llama-2-7B의 경우 최종 <em class="ltx_emph ltx_font_italic" id="S4.SS3.p3.1.3">few</em> 계층이 중요한 것 같습니다. 이전에 언급했듯이 Qwen 계열은 다소 이례적이다. 여기서는 얕은 블록에 대해 유사성이 높은 홀수 "섬"이 몇 개 있음을 알 수 있으며, 이는 그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‣ 4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>에서 강력한 성능의 더 짧은 영역을 설명할 가능성이 있다.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F4.1"><span class="ltx_text" id="S4.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="484" id="S4.F4.1.1.g1" src="https://arxiv.org/html/2403.17887v1/x4.png" width="831"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 4:</span></figcaption>
Normalized angular distance (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.E7" title="7 ‣ 3.2 Layer-pruning algorithm(s) ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a>)
from initial layer <math alttext="\ell" class="ltx_Math" display="inline" id="S4.F4.7.m1.1"><semantics id="S4.F4.7.m1.1b"><mi id="S4.F4.7.m1.1.1" mathvariant="normal" xref="S4.F4.7.m1.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S4.F4.7.m1.1c"><ci id="S4.F4.7.m1.1.1.cmml" xref="S4.F4.7.m1.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.7.m1.1d">\ell</annotation><annotation encoding="application/x-llamapun" id="S4.F4.7.m1.1e">roman_ℓ</annotation></semantics></math> (x-axis) with block size <math alttext="n" class="ltx_Math" display="inline" id="S4.F4.8.m2.1"><semantics id="S4.F4.8.m2.1b"><mi id="S4.F4.8.m2.1.1" xref="S4.F4.8.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F4.8.m2.1c"><ci id="S4.F4.8.m2.1.1.cmml" xref="S4.F4.8.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.8.m2.1d">n</annotation><annotation encoding="application/x-llamapun" id="S4.F4.8.m2.1e">italic_n</annotation></semantics></math> (y-axis) for each of the seven models we evaluated;
the distance
for each
<math alttext="n" class="ltx_Math" display="inline" id="S4.F4.9.m3.1"><semantics id="S4.F4.9.m3.1b"><mi id="S4.F4.9.m3.1.1" xref="S4.F4.9.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F4.9.m3.1c"><ci id="S4.F4.9.m3.1.1.cmml" xref="S4.F4.9.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.9.m3.1d">n</annotation><annotation encoding="application/x-llamapun" id="S4.F4.9.m3.1e">italic_n</annotation></semantics></math>
is shifted and rescaled
to span the same range, <math alttext="[0,1]" class="ltx_Math" display="inline" id="S4.F4.10.m4.2"><semantics id="S4.F4.10.m4.2b"><mrow id="S4.F4.10.m4.2.3.2" xref="S4.F4.10.m4.2.3.1.cmml"><mo id="S4.F4.10.m4.2.3.2.1" stretchy="false" xref="S4.F4.10.m4.2.3.1.cmml">[</mo><mn id="S4.F4.10.m4.1.1" xref="S4.F4.10.m4.1.1.cmml">0</mn><mo id="S4.F4.10.m4.2.3.2.2" xref="S4.F4.10.m4.2.3.1.cmml">,</mo><mn id="S4.F4.10.m4.2.2" xref="S4.F4.10.m4.2.2.cmml">1</mn><mo id="S4.F4.10.m4.2.3.2.3" stretchy="false" xref="S4.F4.10.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.10.m4.2c"><interval closure="closed" id="S4.F4.10.m4.2.3.1.cmml" xref="S4.F4.10.m4.2.3.2"><cn id="S4.F4.10.m4.1.1.cmml" type="integer" xref="S4.F4.10.m4.1.1">0</cn><cn id="S4.F4.10.m4.2.2.cmml" type="integer" xref="S4.F4.10.m4.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.10.m4.2d">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S4.F4.10.m4.2e">[ 0 , 1 ]</annotation></semantics></math>
(yellow to purple): the optimal block to prune, <math alttext="\ell^{*}(n)" class="ltx_Math" display="inline" id="S4.F4.11.m5.1"><semantics id="S4.F4.11.m5.1b"><mrow id="S4.F4.11.m5.1.2" xref="S4.F4.11.m5.1.2.cmml"><msup id="S4.F4.11.m5.1.2.2" xref="S4.F4.11.m5.1.2.2.cmml"><mi id="S4.F4.11.m5.1.2.2.2" mathvariant="normal" xref="S4.F4.11.m5.1.2.2.2.cmml">ℓ</mi><mo id="S4.F4.11.m5.1.2.2.3" xref="S4.F4.11.m5.1.2.2.3.cmml">*</mo></msup><mo id="S4.F4.11.m5.1.2.1" xref="S4.F4.11.m5.1.2.1.cmml">⁢</mo><mrow id="S4.F4.11.m5.1.2.3.2" xref="S4.F4.11.m5.1.2.cmml"><mo id="S4.F4.11.m5.1.2.3.2.1" stretchy="false" xref="S4.F4.11.m5.1.2.cmml">(</mo><mi id="S4.F4.11.m5.1.1" xref="S4.F4.11.m5.1.1.cmml">n</mi><mo id="S4.F4.11.m5.1.2.3.2.2" stretchy="false" xref="S4.F4.11.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.11.m5.1c"><apply id="S4.F4.11.m5.1.2.cmml" xref="S4.F4.11.m5.1.2"><times id="S4.F4.11.m5.1.2.1.cmml" xref="S4.F4.11.m5.1.2.1"></times><apply id="S4.F4.11.m5.1.2.2.cmml" xref="S4.F4.11.m5.1.2.2"><csymbol cd="ambiguous" id="S4.F4.11.m5.1.2.2.1.cmml" xref="S4.F4.11.m5.1.2.2">superscript</csymbol><ci id="S4.F4.11.m5.1.2.2.2.cmml" xref="S4.F4.11.m5.1.2.2.2">ℓ</ci><times id="S4.F4.11.m5.1.2.2.3.cmml" xref="S4.F4.11.m5.1.2.2.3"></times></apply><ci id="S4.F4.11.m5.1.1.cmml" xref="S4.F4.11.m5.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.11.m5.1d">\ell^{*}(n)</annotation><annotation encoding="application/x-llamapun" id="S4.F4.11.m5.1e">roman_ℓ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math>,
corresponds to
the deepest yellow for each row.
Across models,
the
deeper layers tend
to be very similar,
though
the deepest blocks that include the final layer
(squares along the outer diagonal)
are
(near-)maximally dissimilar.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>A simpler pruning strategy</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.4">우리의 최근 결론에 영감을 받아, 우리는 매우 간단한 휴리스틱 프루닝 전략을 실험했다: <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.4.1">(1)</em> if pruning <math alttext="n" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">italic_n</annotation></semantics></math> layers from a <math alttext="L" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><mi id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><ci id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">L</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">italic_L</annotation></semantics></math>-layer model, drop layers <math alttext="(L-n)" class="ltx_Math" display="inline" id="S4.SS4.p1.3.m3.1"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.1.cmml"><mo id="S4.SS4.p1.3.m3.1.1.1.2" stretchy="false" xref="S4.SS4.p1.3.m3.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p1.3.m3.1.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.1.1.2" xref="S4.SS4.p1.3.m3.1.1.1.1.2.cmml">L</mi><mo id="S4.SS4.p1.3.m3.1.1.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.1.1.cmml">−</mo><mi id="S4.SS4.p1.3.m3.1.1.1.1.3" xref="S4.SS4.p1.3.m3.1.1.1.1.3.cmml">n</mi></mrow><mo id="S4.SS4.p1.3.m3.1.1.1.3" stretchy="false" xref="S4.SS4.p1.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1"><minus id="S4.SS4.p1.3.m3.1.1.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1.1.1"></minus><ci id="S4.SS4.p1.3.m3.1.1.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.1.1.2">𝐿</ci><ci id="S4.SS4.p1.3.m3.1.1.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">(L-n)</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.3.m3.1d">( italic_L - italic_n )</annotation></semantics></math> to <math alttext="(L-1)" class="ltx_Math" display="inline" id="S4.SS4.p1.4.m4.1"><semantics id="S4.SS4.p1.4.m4.1a"><mrow id="S4.SS4.p1.4.m4.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.1.cmml"><mo id="S4.SS4.p1.4.m4.1.1.1.2" stretchy="false" xref="S4.SS4.p1.4.m4.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.p1.4.m4.1.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.1.1.2" xref="S4.SS4.p1.4.m4.1.1.1.1.2.cmml">L</mi><mo id="S4.SS4.p1.4.m4.1.1.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.4.m4.1.1.1.1.3" xref="S4.SS4.p1.4.m4.1.1.1.1.3.cmml">1</mn></mrow><mo id="S4.SS4.p1.4.m4.1.1.1.3" stretchy="false" xref="S4.SS4.p1.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1"><minus id="S4.SS4.p1.4.m4.1.1.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1.1.1"></minus><ci id="S4.SS4.p1.4.m4.1.1.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.1.1.2">𝐿</ci><cn id="S4.SS4.p1.4.m4.1.1.1.1.3.cmml" type="integer" xref="S4.SS4.p1.4.m4.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">(L-1)</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.4.m4.1d">( italic_L - 1 )</annotation></semantics></math> so to remove the deep block of the final layer; then <em class="ltx_emph ltx_font_italic" id="S4.SS4.p1.4.2">(2)</em> heal with a small amount finetuning with the previous. 이 간단한 휴리스틱 알고리즘은 우리의 주요 유사성 정보 가지치기 전략과 비교할 때, 의사들이 GPU에 로드하거나 가지치기되지 않은 모델을 추론할 필요가 없다는 장점이 있다. 또한 가지를 치기 위해 블록을 최적화하는 것의 중요성에 대한 의미 있는 절제를 제공한다.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F5" title="Figure 5 ‣ 4.4 A simpler pruning strategy ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">5</span></a>에서 QA 벤치마크(MMLU/BoolQ, 상단/중간 패널)와 자기회귀 손실(C4 검증, 하단 패널)에 대해 치유 전(왼쪽 패널)과 치유 후(오른쪽 패널)의 두 가지 가지치기 전략을 대조한다. 한편, 간단한 휴리스틱은 가지치기에 의해 발생하는 손상을 치유하지 않고 매우 저조하게 수행하는데, QA 벤치마크에 대한 정확도는 가지치기의 분율이 증가함에 따라 (근) 랜덤으로 빠르게 붕괴되고, 적은 양의 가지치기를 하더라도 손실이 매우 빠르게 증가하기 시작한다. 반면에 평가 전반에 걸친 두 가지 가지치기 전략에 대한 결과는 치유 후 상당히 비슷합니다. QA 벤치마크의 경우 유사성 정보 알고리즘이 상전이 전에 정확도를 약간 더 잘 보존하지만 단순 알고리즘은 상전이를 약간 더 큰 가지치기 파벌로 밀어내고 손실에 대해서는 유사성 정보 전략이 모든 가지치기 양에 대해 약간 더 능가하지만 곡선이 거의 서로 위에 있다. 이러한 실험은 프루닝 후 미세조정의 목적이 추가 지식의 획득이 아니라 프루닝 인터페이스에서 손상의 치유라는 강력한 증거이다.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.1"><span class="ltx_text" id="S4.F5.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="S4.F5.1.1.g1" src="https://arxiv.org/html/2403.17887v1/x5.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 5:</span>Llama-2-70B 평가</figcaption>
with
the
simple pruning heuristic (solid red line),
shown along with
scores
for
the similarity-informed pruning strategy (solid blue line),
scores of the unpruned Llama-2-70B (red dashed line),
and
scores for
randomly guessing (gray dashed line).
(<em class="ltx_emph ltx_font_italic" id="S4.F5.7.1">Left:</em> before healing, <em class="ltx_emph ltx_font_italic" id="S4.F5.8.2">Right:</em> after healing; <em class="ltx_emph ltx_font_italic" id="S4.F5.9.3">Top:</em> MMLU, <em class="ltx_emph ltx_font_italic" id="S4.F5.10.4">Middle:</em> BoolQ, <em class="ltx_emph ltx_font_italic" id="S4.F5.11.5">Bottom:</em> C4 Validation Loss.)
Without healing, the simple heuristic performs poorly across all evals; with healing, the scores of both methods are quite similar.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Directions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">오픈-웨이트 LLaMA 패밀리 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib84" title="">2023b</a>)</cite>의 출시를 시작으로 오픈 소스 머신 러닝 커뮤니티는 LLMs를 모든 사람이 액세스할 수 있도록 하는 철학을 중심으로 모였다. 이는 LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib13" title="">2021</a>)</cite> 및 양자화(with LoRA) <cite idx=2></cite)와 같은 효율성에 대한 많은 혁신을 일으켜 단일 80GB A100 GPU에서만 큰(근) 최첨단 70B 모델을 미세 조정할 수 있게 했다. 이러한 다른 도구와 함께 우리의 작업은 간단한 구현 계층 가지치기 기술을 통해 추가 효율성 향상을 가능하게 한다.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.4">특히, 릴리즈된 버전의 Llama-2-70B는 <math alttext="140" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mn id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">140</mn><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><cn id="S5.p2.1.m1.1.1.cmml" type="integer" xref="S5.p2.1.m1.1.1">140</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">140</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">140</annotation></semantics></math>GB의 메모리에 걸쳐 있으며, 토큰당 대략 <math alttext="3\times 10^{10}" class="ltx_Math" display="inline" id="S5.p2.2.m2.1"><semantics id="S5.p2.2.m2.1a"><mrow id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mn id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">3</mn><mo id="S5.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.p2.2.m2.1.1.1.cmml">×</mo><msup id="S5.p2.2.m2.1.1.3" xref="S5.p2.2.m2.1.1.3.cmml"><mn id="S5.p2.2.m2.1.1.3.2" xref="S5.p2.2.m2.1.1.3.2.cmml">10</mn><mn id="S5.p2.2.m2.1.1.3.3" xref="S5.p2.2.m2.1.1.3.3.cmml">10</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><times id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1.1"></times><cn id="S5.p2.2.m2.1.1.2.cmml" type="integer" xref="S5.p2.2.m2.1.1.2">3</cn><apply id="S5.p2.2.m2.1.1.3.cmml" xref="S5.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.p2.2.m2.1.1.3.1.cmml" xref="S5.p2.2.m2.1.1.3">superscript</csymbol><cn id="S5.p2.2.m2.1.1.3.2.cmml" type="integer" xref="S5.p2.2.m2.1.1.3.2">10</cn><cn id="S5.p2.2.m2.1.1.3.3.cmml" type="integer" xref="S5.p2.2.m2.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">3\times 10^{10}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.2.m2.1d">3 × 10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOPs를 소비한다. 4-비트 양자화 및 계층 프루닝 분율이 50%인 경우, 모델은 대략 <math alttext="17.5" class="ltx_Math" display="inline" id="S5.p2.3.m3.1"><semantics id="S5.p2.3.m3.1a"><mn id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml">17.5</mn><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><cn id="S5.p2.3.m3.1.1.cmml" type="float" xref="S5.p2.3.m3.1.1">17.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">17.5</annotation><annotation encoding="application/x-llamapun" id="S5.p2.3.m3.1d">17.5</annotation></semantics></math>GB의 메모리에 적합하고 토큰당 대략 <math alttext="1.5\times 10^{10}" class="ltx_Math" display="inline" id="S5.p2.4.m4.1"><semantics id="S5.p2.4.m4.1a"><mrow id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml"><mn id="S5.p2.4.m4.1.1.2" xref="S5.p2.4.m4.1.1.2.cmml">1.5</mn><mo id="S5.p2.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.p2.4.m4.1.1.1.cmml">×</mo><msup id="S5.p2.4.m4.1.1.3" xref="S5.p2.4.m4.1.1.3.cmml"><mn id="S5.p2.4.m4.1.1.3.2" xref="S5.p2.4.m4.1.1.3.2.cmml">10</mn><mn id="S5.p2.4.m4.1.1.3.3" xref="S5.p2.4.m4.1.1.3.3.cmml">10</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><apply id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1"><times id="S5.p2.4.m4.1.1.1.cmml" xref="S5.p2.4.m4.1.1.1"></times><cn id="S5.p2.4.m4.1.1.2.cmml" type="float" xref="S5.p2.4.m4.1.1.2">1.5</cn><apply id="S5.p2.4.m4.1.1.3.cmml" xref="S5.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S5.p2.4.m4.1.1.3.1.cmml" xref="S5.p2.4.m4.1.1.3">superscript</csymbol><cn id="S5.p2.4.m4.1.1.3.2.cmml" type="integer" xref="S5.p2.4.m4.1.1.3.2">10</cn><cn id="S5.p2.4.m4.1.1.3.3.cmml" type="integer" xref="S5.p2.4.m4.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">1.5\times 10^{10}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.4.m4.1d">1.5 × 10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOPs를 요구한다: 16-비트 <span class="ltx_text ltx_font_typewriter" id="S5.p2.4.1">bfloats</span>에서 4-비트 QLoRA 정밀도로 양자화: 16-비트 정밀도로 계산이 수행되기 때문에 모델 메모리를 4배 감소시키지만 FLOPs는 거의 동일하게 유지한다; 계층 프루닝은 추가적으로 메모리 및 FLOPs를 계층 프루닝 분율과 동일한 양만큼 감소시킬 것이다. 이러한 메모리 및 컴퓨팅 요구 사항은 오픈-웨이트 최첨단 모델이 CPU 오프로딩 없이 그리고 단지 마이너 성능 트레이드오프만으로 소비자 레벨 GPU 상에서 효율적으로 실행되고 심지어 미세 튜닝될 수 있게 한다.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">작품이 끝나면 다음과 같은 질문을 남긴다.</p>
</div>
<div class="ltx_para" id="S5.p4">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">더 나은 레이어 프루닝 전략은 무엇입니까? 치유에 더 좋은 방법은 무엇일까요? <span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_mark">9</sup><span class="ltx_note_mark">9</sup><span class="ltx_eqn_row ltx_align_baseline"><span class="ltx_eqn_cell ltx_eqn_center_padright"><span class="ltx_eqn_cell ltx_eqn_center_padright"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></ 쇼 야에다에게 감사를 표합니다 </span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">왜 치유는 손실에서 상전이를 제거하지만 QA 정확도는 제거하지 않는가?</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">더 포괄적인 평가를 통해 다양한 작업에 대한 정확도가 다른 깊이에서 저하되나요?</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">이와 관련하여 지식은 일반적으로 얕은 층이나 중간 층에 저장되어 있는가, 아니면 비편재화되어 있는가?</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">사전 훈련 세부 정보가 자두 제거 능력에 영향을 미치는가, 예를 들어 스케일링 법칙 과잉 훈련 또는 증류 모델이 자두 제거가 더 어려운가?</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.1">LLM이 가장 깊은 층에서 매개 변수를 더 효과적으로 사용할 수 있도록 하려면 어떻게 해야 합니까?</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.p4.1">이러한 질문 중 일부는 다른 사전 훈련 체크포인트에 걸쳐 레이어 유사성과 가지치기 모두를 연구함으로써 이점을 얻을 수 있으며, 예를 들어 QA 정확도의 급격한 위상 전환 및 임계 깊이가 어느 시점에서 나타나고 더 많은 훈련이 가지치기 가능한 매개변수의 더 나은 사용으로 이어지는가? 다른 사람들은 예를 들어 더 깊은 층을 더 잘 사용하기 위해 다른 사전 훈련 아키텍처 및 목표를 가진 탐색을 제안한다. 보다 포괄적인 평가를 통해 서로 다른 종류의 작업이 매우 다른 깊이에서 저하되면 이러한 작업을 완료하는 데 필요한 지식이 서로 다른 깊이에 저장된다는 것을 나타낼 수 있다. <span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content">10</sup><span class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Alternatively, one could measure <math alttext="d(x^{(\ell)},x^{(\ell+n)})" class="ltx_Math" display="inline" id="footnote10.m1.4"><semantics id="footnote10.m1.4b"><mrow id="footnote10.m1.4.4" xref="footnote10.m1.4.4.cmml"><mi id="footnote10.m1.4.4.4" xref="footnote10.m1.4.4.4.cmml">d</mi><mo id="footnote10.m1.4.4.3" xref="footnote10.m1.4.4.3.cmml">⁢</mo><mrow id="footnote10.m1.4.4.2.2" xref="footnote10.m1.4.4.2.3.cmml"><mo id="footnote10.m1.4.4.2.2.3" stretchy="false" xref="footnote10.m1.4.4.2.3.cmml">(</mo><msup id="footnote10.m1.3.3.1.1.1" xref="footnote10.m1.3.3.1.1.1.cmml"><mi id="footnote10.m1.3.3.1.1.1.2" xref="footnote10.m1.3.3.1.1.1.2.cmml">x</mi><mrow id="footnote10.m1.1.1.1.3" xref="footnote10.m1.3.3.1.1.1.cmml"><mo id="footnote10.m1.1.1.1.3.1" stretchy="false" xref="footnote10.m1.3.3.1.1.1.cmml">(</mo><mi id="footnote10.m1.1.1.1.1" mathvariant="normal" xref="footnote10.m1.1.1.1.1.cmml">ℓ</mi><mo id="footnote10.m1.1.1.1.3.2" stretchy="false" xref="footnote10.m1.3.3.1.1.1.cmml">)</mo></mrow></msup><mo id="footnote10.m1.4.4.2.2.4" xref="footnote10.m1.4.4.2.3.cmml">,</mo><msup id="footnote10.m1.4.4.2.2.2" xref="footnote10.m1.4.4.2.2.2.cmml"><mi id="footnote10.m1.4.4.2.2.2.2" xref="footnote10.m1.4.4.2.2.2.2.cmml">x</mi><mrow id="footnote10.m1.2.2.1.1" xref="footnote10.m1.2.2.1.1.1.cmml"><mo id="footnote10.m1.2.2.1.1.2" stretchy="false" xref="footnote10.m1.2.2.1.1.1.cmml">(</mo><mrow id="footnote10.m1.2.2.1.1.1" xref="footnote10.m1.2.2.1.1.1.cmml"><mi id="footnote10.m1.2.2.1.1.1.2" mathvariant="normal" xref="footnote10.m1.2.2.1.1.1.2.cmml">ℓ</mi><mo id="footnote10.m1.2.2.1.1.1.1" xref="footnote10.m1.2.2.1.1.1.1.cmml">+</mo><mi id="footnote10.m1.2.2.1.1.1.3" xref="footnote10.m1.2.2.1.1.1.3.cmml">n</mi></mrow><mo id="footnote10.m1.2.2.1.1.3" stretchy="false" xref="footnote10.m1.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="footnote10.m1.4.4.2.2.5" stretchy="false" xref="footnote10.m1.4.4.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote10.m1.4c"><apply id="footnote10.m1.4.4.cmml" xref="footnote10.m1.4.4"><times id="footnote10.m1.4.4.3.cmml" xref="footnote10.m1.4.4.3"></times><ci id="footnote10.m1.4.4.4.cmml" xref="footnote10.m1.4.4.4">𝑑</ci><interval closure="open" id="footnote10.m1.4.4.2.3.cmml" xref="footnote10.m1.4.4.2.2"><apply id="footnote10.m1.3.3.1.1.1.cmml" xref="footnote10.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="footnote10.m1.3.3.1.1.1.1.cmml" xref="footnote10.m1.3.3.1.1.1">superscript</csymbol><ci id="footnote10.m1.3.3.1.1.1.2.cmml" xref="footnote10.m1.3.3.1.1.1.2">𝑥</ci><ci id="footnote10.m1.1.1.1.1.cmml" xref="footnote10.m1.1.1.1.1">ℓ</ci></apply><apply id="footnote10.m1.4.4.2.2.2.cmml" xref="footnote10.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="footnote10.m1.4.4.2.2.2.1.cmml" xref="footnote10.m1.4.4.2.2.2">superscript</csymbol><ci id="footnote10.m1.4.4.2.2.2.2.cmml" xref="footnote10.m1.4.4.2.2.2.2">𝑥</ci><apply id="footnote10.m1.2.2.1.1.1.cmml" xref="footnote10.m1.2.2.1.1"><plus id="footnote10.m1.2.2.1.1.1.1.cmml" xref="footnote10.m1.2.2.1.1.1.1"></plus><ci id="footnote10.m1.2.2.1.1.1.2.cmml" xref="footnote10.m1.2.2.1.1.1.2">ℓ</ci><ci id="footnote10.m1.2.2.1.1.1.3.cmml" xref="footnote10.m1.2.2.1.1.1.3">𝑛</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote10.m1.4d">d(x^{(\ell)},x^{(\ell+n)})</annotation><annotation encoding="application/x-llamapun" id="footnote10.m1.4e">italic_d ( italic_x start_POSTSUPERSCRIPT ( roman_ℓ ) end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT ( roman_ℓ + italic_n ) end_POSTSUPERSCRIPT )</annotation></semantics></math> or find <math alttext="\ell^{*}(n)" class="ltx_Math" display="inline" id="footnote10.m2.1"><semantics id="footnote10.m2.1b"><mrow id="footnote10.m2.1.2" xref="footnote10.m2.1.2.cmml"><msup id="footnote10.m2.1.2.2" xref="footnote10.m2.1.2.2.cmml"><mi id="footnote10.m2.1.2.2.2" mathvariant="normal" xref="footnote10.m2.1.2.2.2.cmml">ℓ</mi><mo id="footnote10.m2.1.2.2.3" xref="footnote10.m2.1.2.2.3.cmml">*</mo></msup><mo id="footnote10.m2.1.2.1" xref="footnote10.m2.1.2.1.cmml">⁢</mo><mrow id="footnote10.m2.1.2.3.2" xref="footnote10.m2.1.2.cmml"><mo id="footnote10.m2.1.2.3.2.1" stretchy="false" xref="footnote10.m2.1.2.cmml">(</mo><mi id="footnote10.m2.1.1" xref="footnote10.m2.1.1.cmml">n</mi><mo id="footnote10.m2.1.2.3.2.2" stretchy="false" xref="footnote10.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote10.m2.1c"><apply id="footnote10.m2.1.2.cmml" xref="footnote10.m2.1.2"><times id="footnote10.m2.1.2.1.cmml" xref="footnote10.m2.1.2.1"></times><apply id="footnote10.m2.1.2.2.cmml" xref="footnote10.m2.1.2.2"><csymbol cd="ambiguous" id="footnote10.m2.1.2.2.1.cmml" xref="footnote10.m2.1.2.2">superscript</csymbol><ci id="footnote10.m2.1.2.2.2.cmml" xref="footnote10.m2.1.2.2.2">ℓ</ci><times id="footnote10.m2.1.2.2.3.cmml" xref="footnote10.m2.1.2.2.3"></times></apply><ci id="footnote10.m2.1.1.cmml" xref="footnote10.m2.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote10.m2.1d">\ell^{*}(n)</annotation><annotation encoding="application/x-llamapun" id="footnote10.m2.1e">roman_ℓ start_POSTSUPERSCRIPT * end_POSTSUPERSCRIPT ( italic_n )</annotation></semantics></math> as a function of different eval datasets. 이러한 종류의 해석 가능성 질문을 체계적으로 연구하기 위해 가지치기를 사용하는 것은 매우 흥미로울 것이다.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments and Disclosure of Funding</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.3">우리는 애런 슈워츠의 초기 협업에 감사하고, 토론에 아디티아 싱과 쇼 야에다, 초안에 대한 논평에 아디티아 싱에게 감사한다. 우리는 또한 이 프로젝트에 대한 작업을 위해 우리를 초기화하기 위한 2023 NeurIPS 대규모 언어 모델 효율성 챌린지를 인정하고 싶습니다. A.G는 NSF CAREER grant DMR-2045181, 슬론 재단 및 Condensed Matter Theory Center를 통한 물리 과학 연구소의 지원을 받습니다. D.R은 협력 협정 PHY-2019786(NSF AI Institute for Artificial Intelligence and Fundamental Interactions, <span class="ltx_text ltx_font_typewriter" id="Sx1.p1.3.1">http://iaifi.org/</span>)에 따라 국립 과학 재단의 지원을 인정하고 Sequoia Capital의 제재와 지원을 모두 감사한다. 이 논문은 많은 계층에 걸쳐 합산한 후 <math alttext="G" class="ltx_Math" display="inline" id="Sx1.p1.1.m1.1"><semantics id="Sx1.p1.1.m1.1a"><mi id="Sx1.p1.1.m1.1.1" xref="Sx1.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="Sx1.p1.1.m1.1b"><ci id="Sx1.p1.1.m1.1.1.cmml" xref="Sx1.p1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.1.m1.1c">G</annotation><annotation encoding="application/x-llamapun" id="Sx1.p1.1.m1.1d">italic_G</annotation></semantics></math>, <math alttext="P" class="ltx_Math" display="inline" id="Sx1.p1.2.m2.1"><semantics id="Sx1.p1.2.m2.1a"><mi id="Sx1.p1.2.m2.1.1" xref="Sx1.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="Sx1.p1.2.m2.1b"><ci id="Sx1.p1.2.m2.1.1.cmml" xref="Sx1.p1.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="Sx1.p1.2.m2.1d">italic_P</annotation></semantics></math>, <math alttext="U" class="ltx_Math" display="inline" id="Sx1.p1.3.m3.1"><semantics id="Sx1.p1.3.m3.1a"><mi id="Sx1.p1.3.m3.1.1" xref="Sx1.p1.3.m3.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="Sx1.p1.3.m3.1b"><ci id="Sx1.p1.3.m3.1.1.cmml" xref="Sx1.p1.3.m3.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.3.m3.1c">U</annotation><annotation encoding="application/x-llamapun" id="Sx1.p1.3.m3.1d">italic_U</annotation></semantics></math>라는 글자에 의해 레지던트하게 가져왔다.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" title="">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing chatgpt, Nov 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemini Team et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew&nbsp;M Dai, Anja Hauth,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom&nbsp;B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de&nbsp;Las Casas, Lisa&nbsp;Anne Hendricks, Johannes
Welbl, Aidan Clark, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De&nbsp;Vries (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Harm De&nbsp;Vries.

</span>
<span class="ltx_bibblock">Go smol or go home, July 2023.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.harmdevries.com/post/model-size-vs-compute-overhead/" title="">https://www.harmdevries.com/post/model-size-vs-compute-overhead/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sardana and Frankle (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nikhil Sardana and Jonathan Frankle.

</span>
<span class="ltx_bibblock">Beyond chinchilla-optimal: Accounting for inference in language model
scaling laws.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2401.00448</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Llm. int8 (): 8-bit matrix multiplication for transformers at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2208.07339</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frantar et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">Gptq: Accurate post-training quantization for generative pre-trained
transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2210.17323</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers and Zettlemoyer (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Dettmers and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">The case for 4-bit precision: k-bit inference scaling laws.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">International Conference on Machine Learning</em>, pages
7750–7774. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guangxuan Xiao, Ji&nbsp;Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.

</span>
<span class="ltx_bibblock">Smoothquant: Accurate and efficient post-training quantization for
large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International Conference on Machine Learning</em>, pages
38087–38099. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edward&nbsp;J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu&nbsp;Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et&nbsp;al. (1989)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yann LeCun, John Denker, and Sara Solla.

</span>
<span class="ltx_bibblock">Optimal brain damage.

</span>
<span class="ltx_bibblock">In D.&nbsp;Touretzky, editor, <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in Neural Information
Processing Systems</em>, volume&nbsp;2. Morgan-Kaufmann, 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassibi and Stork (1992)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Babak Hassibi and David Stork.

</span>
<span class="ltx_bibblock">Second order derivatives for network pruning: Optimal brain surgeon.

</span>
<span class="ltx_bibblock">In S.&nbsp;Hanson, J.&nbsp;Cowan, and C.&nbsp;Giles, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in
Neural Information Processing Systems</em>, volume&nbsp;5. Morgan-Kaufmann, 1992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Song Han, Jeff Pool, John Tran, and William Dally.

</span>
<span class="ltx_bibblock">Learning both weights and connections for efficient neural network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Advances in neural information processing systems</em>, 28, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans&nbsp;Peter Graf.

</span>
<span class="ltx_bibblock">Pruning filters for efficient convnets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1608.08710</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankle and Carbin (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jonathan Frankle and Michael Carbin.

</span>
<span class="ltx_bibblock">The lottery ticket hypothesis: Finding sparse, trainable neural
networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1803.03635</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2305.14314</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2307.09288</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">nostalgebraist (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
nostalgebraist.

</span>
<span class="ltx_bibblock">interpreting gpt: the logit lens.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" title="">https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens</a>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belrose et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev
McKinney, Stella Biderman, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Eliciting latent predictions from transformers with the tuned lens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2303.08112</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ricky&nbsp;TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David&nbsp;K Duvenaud.

</span>
<span class="ltx_bibblock">Neural ordinary differential equations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Advances in neural information processing systems</em>, 31, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou.

</span>
<span class="ltx_bibblock">Tensor programs vi: Feature learning in infinite-depth neural
networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2310.02244</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Men et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei
Han, and Weipeng Chen.

</span>
<span class="ltx_bibblock">Shortgpt: Layers in large language models are more redundant than you
expect.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2403.03853</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.

</span>
<span class="ltx_bibblock">Compressing neural networks with the hashing trick.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">International conference on machine learning</em>, pages
2285–2294. PMLR, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srinivas and Babu (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suraj Srinivas and R&nbsp;Venkatesh Babu.

</span>
<span class="ltx_bibblock">Data-free parameter pruning for deep neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:1507.06149</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.

</span>
<span class="ltx_bibblock">Learning structured sparsity in deep neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Advances in neural information processing systems</em>, 29, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.

</span>
<span class="ltx_bibblock">Network trimming: A data-driven neuron pruning approach towards
efficient deep architectures.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:1607.03250</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yihui He, Xiangyu Zhang, and Jian Sun.

</span>
<span class="ltx_bibblock">Channel pruning for accelerating very deep neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE international conference on computer
vision</em>, pages 1389–1397, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gao Huang, Shichen Liu, Laurens Van&nbsp;der Maaten, and Kilian&nbsp;Q Weinberger.

</span>
<span class="ltx_bibblock">Condensenet: An efficient densenet using learned group convolutions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 2752–2761, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murray and Chiang (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kenton Murray and David Chiang.

</span>
<span class="ltx_bibblock">Auto-sizing neural networks: With applications to n-gram language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:1508.05051</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">See et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Abigail See, Minh-Thang Luong, and Christopher&nbsp;D Manning.

</span>
<span class="ltx_bibblock">Compression of neural machine translation models via pruning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1606.09274</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Rush (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yoon Kim and Alexander&nbsp;M Rush.

</span>
<span class="ltx_bibblock">Sequence-level knowledge distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:1606.07947</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.

</span>
<span class="ltx_bibblock">Analyzing multi-head self-attention: Specialized heads do the heavy
lifting, the rest can be pruned.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:1905.09418</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Michel et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Paul Michel, Omer Levy, and Graham Neubig.

</span>
<span class="ltx_bibblock">Are sixteen heads really better than one?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Awadalla (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Young&nbsp;Jin Kim and Hany&nbsp;Hassan Awadalla.

</span>
<span class="ltx_bibblock">Fastformers: Highly efficient transformer models for natural language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2010.13382</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Angela Fan, Edouard Grave, and Armand Joulin.

</span>
<span class="ltx_bibblock">Reducing transformer depth on demand with structured dropout.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:1909.11556</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and He (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Minjia Zhang and Yuxiong He.

</span>
<span class="ltx_bibblock">Accelerating training of transformer-based language models with
progressive layer dropping.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in Neural Information Processing Systems</em>,
33:14011–14023, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chun Fan, Jiwei Li, Xiang Ao, Fei Wu, Yuxian Meng, and Xiaofei Sun.

</span>
<span class="ltx_bibblock">Layer-wise model pruning based on mutual information.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2108.12594</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jha et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ananya&nbsp;Harsh Jha, Dirk Groeneveld, Emma Strubell, and Iz&nbsp;Beltagy.

</span>
<span class="ltx_bibblock">Large language model distillation doesn’t need a teacher.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2305.14864</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sajjad et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov.

</span>
<span class="ltx_bibblock">On the effect of dropping layers of pre-trained transformer models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Computer Speech &amp; Language</em>, 77:101429, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei Liu, Zhiyuan Peng, and Tan Lee.

</span>
<span class="ltx_bibblock">Comflp: Correlation measure based fast search on asr layer pruning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2309.11768</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lu&nbsp;Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu.

</span>
<span class="ltx_bibblock">Dynabert: Dynamic bert with adaptive width and depth.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Advances in Neural Information Processing Systems</em>,
33:9782–9793, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pratyusha Sharma, Jordan&nbsp;T Ash, and Dipendra Misra.

</span>
<span class="ltx_bibblock">The truth is in there: Improving reasoning in language models with
layer-selective rank reduction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2312.13558</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashkboos et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Saleh Ashkboos, Maximilian&nbsp;L. Croci, Marcelo Gennari&nbsp;do Nascimento, Torsten
Hoefler, and James Hensman.

</span>
<span class="ltx_bibblock">Slicegpt: Compress large language models by deleting rows and
columns.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:2401.15024</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mengzhou Xia, Zexuan Zhong, and Danqi Chen.

</span>
<span class="ltx_bibblock">Structured pruning learns compact and accurate models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2204.00408</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lagunas et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
François Lagunas, Ella Charlaix, Victor Sanh, and Alexander&nbsp;M Rush.

</span>
<span class="ltx_bibblock">Block pruning for faster transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2109.04838</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qihuang Zhong, Liang Ding, Juhua Liu, Bo&nbsp;Du, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Can chatgpt understand too? a comparative study on chatgpt and
fine-tuned bert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2302.10198</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kawin Ethayarajh.

</span>
<span class="ltx_bibblock">How contextual are contextualized word representations? comparing the
geometry of bert, elmo, and gpt-2 embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:1909.00512</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech
representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Advances in neural information processing systems</em>,
33:12449–12460, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et&nbsp;al. (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:1503.02531</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuxian Gu, Li&nbsp;Dong, Furu Wei, and Minlie Huang.

</span>
<span class="ltx_bibblock">Knowledge distillation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2306.08543</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
Wang, and Qun Liu.

</span>
<span class="ltx_bibblock">Tinybert: Distilling bert for natural language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:1909.10351</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng.

</span>
<span class="ltx_bibblock">Want to reduce labeling cost? gpt-3 can help.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2108.13487</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eldan and Li (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ronen Eldan and Yuanzhi Li.

</span>
<span class="ltx_bibblock">Tinystories: How small can language models be and still speak
coherent english?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2305.07759</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del&nbsp;Giorno, Suriya
Gunasekar, and Yin&nbsp;Tat Lee.

</span>
<span class="ltx_bibblock">Textbooks are all you need ii: phi-1.5 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2309.05463</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi&nbsp;Zhang, Jyoti Aneja, Caio César&nbsp;Teodoro Mendes, Allie
Del&nbsp;Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
de&nbsp;Rosa, Olli Saarikivi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Textbooks are all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2306.11644</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot.

</span>
<span class="ltx_bibblock">Specializing smaller language models towards multi-step reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2301.12726</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii,
Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.

</span>
<span class="ltx_bibblock">Distilling step-by-step! outperforming larger language models with
less training data and smaller model sizes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2305.02301</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang.

</span>
<span class="ltx_bibblock">Lion: Adversarial distillation of closed-source large language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2305.12870</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu
Chen, and Tuo Zhao.

</span>
<span class="ltx_bibblock">Loftq: Lora-fine-tuning-aware quantization for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2310.08659</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu&nbsp;Cheng, Weizhu
Chen, and Tuo Zhao.

</span>
<span class="ltx_bibblock">Adaptive budget allocation for parameter-efficient fine-tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2303.10512</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leviathan et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yaniv Leviathan, Matan Kalman, and Yossi Matias.

</span>
<span class="ltx_bibblock">Fast inference from transformers via speculative decoding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">International Conference on Machine Learning</em>, pages
19274–19286. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason&nbsp;D Lee, Deming Chen,
and Tri Dao.

</span>
<span class="ltx_bibblock">Medusa: Simple llm inference acceleration framework with multiple
decoding heads.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">arXiv preprint arXiv:2401.10774</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.

</span>
<span class="ltx_bibblock">Locating and editing factual associations in gpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Advances in Neural Information Processing Systems</em>,
35:17359–17372, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Damai Dai, Li&nbsp;Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.

</span>
<span class="ltx_bibblock">Knowledge neurons in pretrained transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2104.08696</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hase et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun.

</span>
<span class="ltx_bibblock">Does localization inform editing? surprising differences in
causality-based localization vs. knowledge editing in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2301.04213</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson.

</span>
<span class="ltx_bibblock">Dissecting recall of factual associations in auto-regressive language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2304.14767</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Din et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexander&nbsp;Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva.

</span>
<span class="ltx_bibblock">Jump to conclusions: Short-cutting transformers with linear
transformations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2303.09435</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurnee and Tegmark (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wes Gurnee and Max Tegmark.

</span>
<span class="ltx_bibblock">Language models represent space and time.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2310.02207</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elena Voita, Javier Ferrando, and Christoforos Nalmpantis.

</span>
<span class="ltx_bibblock">Neurons in large language models: Dead, n-gram, positional.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2309.04827</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali
Shrivastava, Ce&nbsp;Zhang, Yuandong Tian, Christopher Re, et&nbsp;al.

</span>
<span class="ltx_bibblock">Deja vu: Contextual sparsity for efficient llms at inference time.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">International Conference on Machine Learning</em>, pages
22137–22176. PMLR, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panigrahi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora.

</span>
<span class="ltx_bibblock">Task-specific skill localization in fine-tuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">arXiv preprint arXiv:2302.06600</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2310.06825</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Javaheripi and Bubeck (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mojan Javaheripi and Sébastien Bubeck.

</span>
<span class="ltx_bibblock">Phi-2: The surprising power of small language models, Dec 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">The Journal of Machine Learning Research</em>, 21(1):5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">arXiv preprint arXiv:2009.03300</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">arXiv preprint arXiv:1905.10044</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.

</span>
<span class="ltx_bibblock">Are emergent abilities of large language models a mirage?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2304.15004</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2302.13971</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven&nbsp;Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander&nbsp;M. Rush.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 38–45, Online,
October 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/2020.emnlp-demos.6" title="">https://www.aclweb.org/anthology/2020.emnlp-demos.6</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">arXiv e-prints</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangrulkar et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul,
and Benjamin Bossan.

</span>
<span class="ltx_bibblock">Peft: State-of-the-art parameter-efficient fine-tuning methods.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/peft" title="">https://github.com/huggingface/peft</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ariel&nbsp;N Lee, Cole&nbsp;J Hunter, and Nataniel Ruiz.

</span>
<span class="ltx_bibblock">Platypus: Quick, cheap, and powerful refinement of llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2308.07317</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experimental Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">여기서는 모델 및 치유(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.1</span></a>) 및 평가(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS2" title="A.2 Evaluation details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.2</span></a>)의 다양한 세부 사항을 설명한다.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Model and healing details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">본 논문의 모든 모델은 Hugging Face <span class="ltx_text ltx_font_typewriter" id="A1.SS1.p1.1.1">Trainer API</span> <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib85" title="">2020</a>)</cite>를 사용하여 미세 조정되었다. 휴징 페이스의 모델 및 경로 목록은 다음과 같습니다.</p>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.SS1.p1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS1.p1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.SS1.p1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p1.2.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p1.2.1.1.2">Repository Path</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS1.p1.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.p1.2.2.1.1">Llama-2 7B</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.p1.2.2.1.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.2.1.2.1">meta-llama/Llama-2-7b-hf</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.3.2.1">Llama-2 13B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.3.2.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.3.2.2.1">meta-llama/Llama-2-13b-hf</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.4.3.1">Llama-2 70B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.4.3.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.4.3.2.1">meta-llama/Llama-2-70b-hf</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.5.4.1">Mistral 7B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.5.4.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.5.4.2.1">mistralai/Mistral-7B-v0.1</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.6.5.1">Phi-2 (2.7B)</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.6.5.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.6.5.2.1">microsoft/phi-2</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.7.6.1">Qwen 7B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.7.6.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.7.6.2.1">Qwen/Qwen-7B</code></td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p1.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p1.2.8.7.1">Qwen 14B</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p1.2.8.7.2"><code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p1.2.8.7.2.1">Qwen/Qwen-14B</code></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">치유를 위해 Colossal Clean Crawled Corpus (C4) <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib86" title="">2019</a>)</cite> from Hugging Face: <code class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p2.1.1">data = load_dataset("c4", ’en')</code> 버전을 사용했다. 우리는 단락에 설명된 대로 긴 예를 잘랐고 사용 가능한 경우 특수 토큰을 추가했다. <span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>N.B. the Qwen tokenizer from Hugging Face does not include any special tokens; in this case, it was essential to add a default padding token. </span></span></span> 모델은 전역 배치 크기가 16인 5000 단계에 대해 미세 조정되었습니다. 이는 각 모델에 대한 <math alttext="16\times 5000\times[\text{{max\_seq\_length}}]" class="ltx_Math" display="inline" id="A1.SS1.p2.1.m1.1"><semantics id="A1.SS1.p2.1.m1.1a"><mrow id="A1.SS1.p2.1.m1.1.2" xref="A1.SS1.p2.1.m1.1.2.cmml"><mn id="A1.SS1.p2.1.m1.1.2.2" xref="A1.SS1.p2.1.m1.1.2.2.cmml">16</mn><mo id="A1.SS1.p2.1.m1.1.2.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p2.1.m1.1.2.1.cmml">×</mo><mn id="A1.SS1.p2.1.m1.1.2.3" xref="A1.SS1.p2.1.m1.1.2.3.cmml">5000</mn><mo id="A1.SS1.p2.1.m1.1.2.1a" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p2.1.m1.1.2.1.cmml">×</mo><mrow id="A1.SS1.p2.1.m1.1.2.4.2" xref="A1.SS1.p2.1.m1.1.2.4.1.cmml"><mo id="A1.SS1.p2.1.m1.1.2.4.2.1" stretchy="false" xref="A1.SS1.p2.1.m1.1.2.4.1.1.cmml">[</mo><mtext id="A1.SS1.p2.1.m1.1.1" mathvariant="monospace" xref="A1.SS1.p2.1.m1.1.1a.cmml">max_seq_length</mtext><mo id="A1.SS1.p2.1.m1.1.2.4.2.2" stretchy="false" xref="A1.SS1.p2.1.m1.1.2.4.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p2.1.m1.1b"><apply id="A1.SS1.p2.1.m1.1.2.cmml" xref="A1.SS1.p2.1.m1.1.2"><times id="A1.SS1.p2.1.m1.1.2.1.cmml" xref="A1.SS1.p2.1.m1.1.2.1"></times><cn id="A1.SS1.p2.1.m1.1.2.2.cmml" type="integer" xref="A1.SS1.p2.1.m1.1.2.2">16</cn><cn id="A1.SS1.p2.1.m1.1.2.3.cmml" type="integer" xref="A1.SS1.p2.1.m1.1.2.3">5000</cn><apply id="A1.SS1.p2.1.m1.1.2.4.1.cmml" xref="A1.SS1.p2.1.m1.1.2.4.2"><csymbol cd="latexml" id="A1.SS1.p2.1.m1.1.2.4.1.1.cmml" xref="A1.SS1.p2.1.m1.1.2.4.2.1">delimited-[]</csymbol><ci id="A1.SS1.p2.1.m1.1.1a.cmml" xref="A1.SS1.p2.1.m1.1.1"><mtext id="A1.SS1.p2.1.m1.1.1.cmml" mathvariant="monospace" xref="A1.SS1.p2.1.m1.1.1">max_seq_length</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p2.1.m1.1c">16\times 5000\times[\text{{max\_seq\_length}}]</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p2.1.m1.1d">16 × 5000 × [ max_seq_length ]</annotation></semantics></math>의 총 미세 조정 토큰에 해당합니다. 우리는 100단계의 준비운동과 함께 코사인 어닐링 학습률 스케줄을 사용했다. 피크 학습률은 가능한 경우 모델의 사전 훈련에서 피크 학습률로 설정되었으며, 실제로 이는 모든 모델이 사전 훈련 동안 2e-4의 피크 LR로 훈련된 Phi-2 <cite class="ltx_cite ltx_citemacro_cite">Javaheripi and Bubeck (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib79" title="">2023</a>)</cite>, 3e-5의 피크 LR로 훈련된 Llama-2-70B(스윕으로 인한 값), 3e-6의 피크 LR로 훈련된 Mistral-7B(스윕으로 인한 값)를 제외하고 3e-4의 피크 LR로 훈련되었음을 의미한다. 모든 모델 7B 파라미터 또는 그 이상은 2048 토큰의 최대 시퀀스 길이로 트레이닝된 반면, 모든 모델 13B 파라미터 또는 그 이상은 4096 토큰의 최대 시퀀스 길이로 트레이닝되었다. 우리는 일부 모델이 더 긴 시퀀스에 대해 사전 훈련되었을 수 있음을 깨닫지만, 예를 들어 Qwen<em class="ltx_emph ltx_font_italic" id="A1.SS1.p2.1.2">-the-outlier</em> <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib77" title="">2023</a>)</cite>와 같이 모델 패밀리에 걸쳐 더 공정한 비교를 허용하기 위해 유사한 크기의 모델에 걸쳐 일관된 최대 시퀀스 길이로 결정했다.</p>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1">휴징 페이스 트레이너 API 위에서 모든 미세 조정에 양자화 및 LoRA(Low-Rank Adapters) <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib13" title="">2021</a>)</cite>를 사용했습니다.</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">양자화를 위해 <span class="ltx_text ltx_font_typewriter" id="A1.I1.i1.p1.1.1">bitsandbytes</span> library for QLoRA <cite class="ltx_cite ltx_citemacro_cite">Dettmers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>를 사용하여 모델을 4비트로 양자화했다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">LoRA의 경우 Hugging Face <span class="ltx_text ltx_font_typewriter" id="A1.I1.i2.p1.1.1">peft</span> library <cite class="ltx_cite ltx_citemacro_citep">(Mangrulkar et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib87" title="">2022</a>)</cite>를 사용하였다. LoRA 드롭아웃을 0.05로 설정하고 <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib88" title="">2023</a>)</cite>에 이어 LoRA <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.I1.i2.p1.1.m1.1"><semantics id="A1.I1.i2.p1.1.m1.1a"><mi id="A1.I1.i2.p1.1.m1.1.1" xref="A1.I1.i2.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.1.m1.1b"><ci id="A1.I1.i2.p1.1.m1.1.1.cmml" xref="A1.I1.i2.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i2.p1.1.m1.1d">italic_α</annotation></semantics></math>를 LoRA 순위에 동일하게 유지했다. 아래에서 논의되는 두 가지 예외를 제외하고, 모델은 LoRA 순위 64로 훈련된다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">심판도 따르고요 <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib88" title="">2023</a>)</cite>, FFN 모듈에는 LoRA만 적용했습니다. <code class="ltx_verbatim ltx_font_typewriter" id="A1.I1.i3.p1.1.1">["gate_proj", "down_proj", "up_proj"]</code>는 Llama-2 및 Mistral 모델, <code class="ltx_verbatim ltx_font_typewriter" id="A1.I1.i3.p1.1.2">["fc1", "fc2"]</code>는 Phi-2, <code class="ltx_verbatim ltx_font_typewriter" id="A1.I1.i3.p1.1.3">["w1", "w2", "c_proj"]</code>는 Qwen 모델입니다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS1.p4">
<p class="ltx_p" id="A1.SS1.p4.2">이러한 하이퍼파라미터 선택의 대부분은 표준이며 참조와 같은 이전 작업에서 찾을 수 있다. <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib88" title="">2023</a>; Dettmers et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib9" title="">2022</a>)</cite>. 절대적 명확성을 위해, 우리는 아래의 모든 모델 특정 아키텍처 및 치유 세부사항을 나열한다:</p>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.SS1.p4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.SS1.p4.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.SS1.p4.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p4.3.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.2"># Layers</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.3">Vocab Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.4">Max Seq. Len.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.5">FT Tokens</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.6">Peak LR</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.SS1.p4.3.1.1.7">LoRA Rank</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS1.p4.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.SS1.p4.3.2.1.1">Llama-2 7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS1.p4.3.2.1.2">32</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.SS1.p4.3.2.1.3">32,000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS1.p4.3.2.1.4">2048</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS1.p4.3.2.1.5">164M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.SS1.p4.3.2.1.6">3e-4</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.SS1.p4.3.2.1.7">2</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.3.2.1">Llama-2 13B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.3.2.2">40</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.3.2.3">32,000</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.3.2.4">4096</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.3.2.5">328M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.3.2.6">3e-4</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.3.2.7">64</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.4.3.1">Llama-2 70B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.4.3.2">80</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.4.3.3">32,000</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.4.3.4">4096</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.4.3.5">328M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.4.3.6">3e-5</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.4.3.7">8</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.5.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.5.4.1">Qwen 7B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.5.4.2">32</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.5.4.3">151,936</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.5.4.4">2048</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.5.4.5">164M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.5.4.6">3e-4</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.5.4.7">64</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.6.5.1">Qwen 14B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.6.5.2">40</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.6.5.3">151,936</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.6.5.4">4096</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.6.5.5">328M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.6.5.6">3e-4</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.6.5.7">64</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.7.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.7.6.1">Mistral 7B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.7.6.2">32</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.7.6.3">32,000</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.7.6.4">2048</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.7.6.5">164M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.7.6.6">3e-6</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.7.6.7">4</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.3.8.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.SS1.p4.3.8.7.1">Phi-2 2.7B</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.8.7.2">32</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.8.7.3">51,200</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.8.7.4">2048</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.8.7.5">164M</td>
<td class="ltx_td ltx_align_center" id="A1.SS1.p4.3.8.7.6">2e-4</td>
<td class="ltx_td ltx_align_right" id="A1.SS1.p4.3.8.7.7">64</td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="A1.SS1.p4.4">또한 모든 모델 간에 공통되는 다음 하이퍼 매개 변수가 있습니다.</p>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.SS1.p4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.SS1.p4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.SS1.p4.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p4.1.2.1.1.1">Config</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.SS1.p4.1.2.1.2">Value</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.SS1.p4.1.3.2.1">Finetuning dataset</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.SS1.p4.1.3.2.2">C4</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.4.3.1">Batch size</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.4.3.2">16</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.1.1">LoRA <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.SS1.p4.1.1.1.m1.1"><semantics id="A1.SS1.p4.1.1.1.m1.1a"><mi id="A1.SS1.p4.1.1.1.m1.1.1" xref="A1.SS1.p4.1.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p4.1.1.1.m1.1b"><ci id="A1.SS1.p4.1.1.1.m1.1.1.cmml" xref="A1.SS1.p4.1.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p4.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p4.1.1.1.m1.1d">italic_α</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.1.2">LoRA rank</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.5.4.1">LoRA dropout</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.5.4.2">0.05</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.6.5.1">LoRA targets</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.6.5.2">FFN modules</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.7.6.1">LR scheduler</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.7.6.2">Cosine</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.8.7.1">Warmup steps</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.8.7.2">100</td>
</tr>
<tr class="ltx_tr" id="A1.SS1.p4.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.SS1.p4.1.9.8.1">Total steps</th>
<td class="ltx_td ltx_align_left" id="A1.SS1.p4.1.9.8.2">5000</td>
</tr>
</tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Evaluation details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">세 가지 주요 평가를 수행했다: <em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.1.1">MMLU</em>, <em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.1.2">BoolQ</em>, <em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.1.3">C4</em>.</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p2.1.1">MMLU accuracy</span></p>
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1"><code class="ltx_verbatim ltx_font_typewriter" id="A1.I2.i1.p1.1.1">cais/mmlu</code> 버전의 Hugging Face 데이터셋을 사용합니다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1">추가 프롬프트 엔지니어링 없이 원본 참조 <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib81" title="">2020</a>)</cite>에서 제안한 형식을 따른다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1">소샷 예제 구성을 위해 <code class="ltx_verbatim ltx_font_typewriter" id="A1.I2.i3.p1.1.1">dev</code> <code class="ltx_verbatim ltx_font_typewriter" id="A1.I2.i3.p1.1.2">cais/mmlu</code>의 집합을 사용합니다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i4.p1">
<p class="ltx_p" id="A1.I2.i4.p1.1">실험은 <math alttext="0" class="ltx_Math" display="inline" id="A1.I2.i4.p1.1.m1.1"><semantics id="A1.I2.i4.p1.1.m1.1a"><mn id="A1.I2.i4.p1.1.m1.1.1" xref="A1.I2.i4.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="A1.I2.i4.p1.1.m1.1b"><cn id="A1.I2.i4.p1.1.m1.1.1.cmml" type="integer" xref="A1.I2.i4.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> few-shot 예제를 사용하였으며, 본 논문의 결과와 분석은 이 선택인 cf에 강건하다. <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F7" title="Figure 7 ‣ B.1 Prompting ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a> 그림.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i5.p1">
<p class="ltx_p" id="A1.I2.i5.p1.1">우리는 모든 피험자에 걸쳐 평균 정확도를 보고한다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p3.1.1">BoolQ accuracy</span></p>
<ul class="ltx_itemize" id="A1.I3">
<li class="ltx_item" id="A1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I3.i1.p1">
<p class="ltx_p" id="A1.I3.i1.p1.1">우리는 Hugging Face의 <code class="ltx_verbatim ltx_font_typewriter" id="A1.I3.i1.p1.1.1">hassansh/boolq_n_shot</code> 버전을 사용했다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I3.i2.p1">
<p class="ltx_p" id="A1.I3.i2.p1.1">실험을 위해 <math alttext="0" class="ltx_Math" display="inline" id="A1.I3.i2.p1.1.m1.1"><semantics id="A1.I3.i2.p1.1.m1.1a"><mn id="A1.I3.i2.p1.1.m1.1.1" xref="A1.I3.i2.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="A1.I3.i2.p1.1.m1.1b"><cn id="A1.I3.i2.p1.1.m1.1.1.cmml" type="integer" xref="A1.I3.i2.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> few-shot 예제를 사용한다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I3.i3.p1">
<p class="ltx_p" id="A1.I3.i3.p1.1">본문에서 잘린 완전한 BoolQ 결과는 그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.F6" title="Figure 6 ‣ A.2 Evaluation details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">6</span></a>에 나와 있다: 왼쪽 패널에서 라마-2 패밀리를 제시하고, 중간 패널에서 Qwen 패밀리의 모델을 제시하고, 오른쪽 패널에서 미스트랄-7B 및 Phi-2를 제시해야 하며, 완전한 유사성 정보 가지치기 방법의 결과를 더 잘 표시하기 위해 반투명 치유 없이 실험을 한다. 중요한 것은 여기서 힐링이 그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F2" title="Figure 2 ‣ 4.1 Accuracy on QA benchmarks ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">2</span></a>에서 MMLU보다 더 중요한 역할을 한다는 것을 알 수 있지만, 힐링 후에도 우리는 여전히 강력한 성능의 특징적인 평평한 영역을 가지고 있으며, 이전과 같이 모델의 최고 점수를 달성하는 데 필요한 기능은 중요한 모델 의존적 임계값까지 중요한 레이어 프루닝에 의해 제거되지 않는다.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="A1.F6">
<p class="ltx_p ltx_align_center ltx_align_center" id="A1.F6.1"><span class="ltx_text" id="A1.F6.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="305" id="A1.F6.1.1.g1" src="https://arxiv.org/html/2403.17887v1/x6.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 6:</span>BoolQ accuracy (0-shot)</figcaption>
vs. fraction of layers
dropped
for different model families.
(<em class="ltx_emph ltx_font_italic" id="A1.F6.5.1">Left:</em> Llama-2 family; <em class="ltx_emph ltx_font_italic" id="A1.F6.6.2">Middle:</em> Qwen family; <em class="ltx_emph ltx_font_italic" id="A1.F6.7.3">Right:</em> Mistral-7B and Phi-2.)
The
solid lines
represent
performance
after dropping layers and healing,
and the (semi-transparent) dotted lines
show
performance after dropping layers only (no healing),
and the dashed gray line is the score for guessing randomly.
For BoolQ, healing leads to important improvements such that performances; then, across all models, performances
are quite robust
until 20%-55% pruning fractions, depending on model family and size, at which point they transitions to random guessing.
</figcaption>
<br class="ltx_break ltx_centering">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.p4">
<p class="ltx_p" id="A1.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p4.1.1">C4 Validation Loss</span></p>
<ul class="ltx_itemize" id="A1.I4">
<li class="ltx_item" id="A1.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i1.p1">
<p class="ltx_p" id="A1.I4.i1.p1.1">우리는 Hugging Face의 <code class="ltx_verbatim ltx_font_typewriter" id="A1.I4.i1.p1.1.1">c4</code> 버전을 사용했습니다 (곧 <code class="ltx_verbatim ltx_font_typewriter" id="A1.I4.i1.p1.1.2">allenai/c4</code>를 선호하여 사용되지 않습니다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i2.p1">
<p class="ltx_p" id="A1.I4.i2.p1.1">우리는 열차 분할로 치유된 <em class="ltx_emph ltx_font_italic" id="A1.I4.i2.p1.1.1">validation</em> split을 사용하여 평가했다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i3.p1">
<p class="ltx_p" id="A1.I4.i3.p1.1">크기를 감안할 때 우리는 무작위로 60k 서열을 샘플링하고 모든 모델에 걸쳐 고정했다.</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i4.p1">
<p class="ltx_p" id="A1.I4.i4.p1.3"><a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S4.F3" title="Figure 3 ‣ 4.2 Loss on next-token predictions ‣ 4 Results ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3</span></a>에서 우리는 서로 다른 어휘 크기를 사용하는 모델 패밀리에 걸쳐 공정한 비교를 용이하게 하기 위해 손실을 정규화했다: 정규화하기 위해 <math alttext="\log V" class="ltx_Math" display="inline" id="A1.I4.i4.p1.1.m1.1"><semantics id="A1.I4.i4.p1.1.m1.1a"><mrow id="A1.I4.i4.p1.1.m1.1.1" xref="A1.I4.i4.p1.1.m1.1.1.cmml"><mi id="A1.I4.i4.p1.1.m1.1.1.1" xref="A1.I4.i4.p1.1.m1.1.1.1.cmml">log</mi><mo id="A1.I4.i4.p1.1.m1.1.1a" lspace="0.167em" xref="A1.I4.i4.p1.1.m1.1.1.cmml">⁡</mo><mi id="A1.I4.i4.p1.1.m1.1.1.2" xref="A1.I4.i4.p1.1.m1.1.1.2.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.I4.i4.p1.1.m1.1b"><apply id="A1.I4.i4.p1.1.m1.1.1.cmml" xref="A1.I4.i4.p1.1.m1.1.1"><log id="A1.I4.i4.p1.1.m1.1.1.1.cmml" xref="A1.I4.i4.p1.1.m1.1.1.1"></log><ci id="A1.I4.i4.p1.1.m1.1.1.2.cmml" xref="A1.I4.i4.p1.1.m1.1.1.2">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i4.p1.1.m1.1c">\log V</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i4.p1.1.m1.1d">roman_log italic_V</annotation></semantics></math>로 나누었으며, 여기서 <math alttext="V" class="ltx_Math" display="inline" id="A1.I4.i4.p1.2.m2.1"><semantics id="A1.I4.i4.p1.2.m2.1a"><mi id="A1.I4.i4.p1.2.m2.1.1" xref="A1.I4.i4.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="A1.I4.i4.p1.2.m2.1b"><ci id="A1.I4.i4.p1.2.m2.1.1.cmml" xref="A1.I4.i4.p1.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i4.p1.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i4.p1.2.m2.1d">italic_V</annotation></semantics></math>는 <em class="ltx_emph ltx_font_italic" id="A1.I4.i4.p1.3.1">per-model</em> 어휘 크기( §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.1</span></a>의 테이블에 나열됨)이다. 이 <math alttext="\log V" class="ltx_Math" display="inline" id="A1.I4.i4.p1.3.m3.1"><semantics id="A1.I4.i4.p1.3.m3.1a"><mrow id="A1.I4.i4.p1.3.m3.1.1" xref="A1.I4.i4.p1.3.m3.1.1.cmml"><mi id="A1.I4.i4.p1.3.m3.1.1.1" xref="A1.I4.i4.p1.3.m3.1.1.1.cmml">log</mi><mo id="A1.I4.i4.p1.3.m3.1.1a" lspace="0.167em" xref="A1.I4.i4.p1.3.m3.1.1.cmml">⁡</mo><mi id="A1.I4.i4.p1.3.m3.1.1.2" xref="A1.I4.i4.p1.3.m3.1.1.2.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.I4.i4.p1.3.m3.1b"><apply id="A1.I4.i4.p1.3.m3.1.1.cmml" xref="A1.I4.i4.p1.3.m3.1.1"><log id="A1.I4.i4.p1.3.m3.1.1.1.cmml" xref="A1.I4.i4.p1.3.m3.1.1.1"></log><ci id="A1.I4.i4.p1.3.m3.1.1.2.cmml" xref="A1.I4.i4.p1.3.m3.1.1.2">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I4.i4.p1.3.m3.1c">\log V</annotation><annotation encoding="application/x-llamapun" id="A1.I4.i4.p1.3.m3.1d">roman_log italic_V</annotation></semantics></math>는 샘플링 토큰의 일률적인 손실에 해당하며, 이는 주어진 모델에 대한 척도를 자연스럽게 설정한다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Ablations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">여기서는 프롬프트(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS1" title="B.1 Prompting ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B.1</span></a>), 핀튜닝 시드(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS2" title="B.2 Finetuning seed ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B.2</span></a>), LoRA 순위(§<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.SS3" title="B.3 LoRA rank ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">B.3</span></a>)의 다양한 하이퍼파라미터의 삭제를 자세히 설명한다. 질적으로, 논문의 결과는 이들 중 임의의 것의 변동에 상당히 강건하다.</p>
</div>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Prompting</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">QA 평가에서 프롬프트를 변경하면 결과에 상당한 영향을 미칠 수 있다는 것은 상식이다. 프롬프트를 제어하기 위해 Llama-2-13B에 적용했을 때 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.2</span></a>에 설명된 주요 유사성 정보 가지치기에 대한 MMLU 정확도를 제거했다. 그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F7" title="Figure 7 ‣ B.1 Prompting ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">7</span></a>의 왼쪽 패널에서 프롬프트에서 소수의 샷 예제의 순서를 변경하기 위한 결과를 보여주고 오른쪽 패널에서 동일한 그림에서 소수의 샷 예제의 수를 변경하기 위한 결과를 보여준다. 넓게는 레이어 프루닝 방법이 이러한 변화에 강인하다는 것을 알 수 있다.</p>
</div>
<figure class="ltx_figure" id="A2.F7">
<p class="ltx_p ltx_align_center ltx_align_center" id="A2.F7.1"><span class="ltx_text" id="A2.F7.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="307" id="A2.F7.1.1.g1" src="https://arxiv.org/html/2403.17887v1/x7.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 7:</span>MMLU 정확도에 대한 프롬프트 삭제의 효과 vs. ...에 대하여 적하된 층의 분율.</figcaption>
Llama-2-13B.
<em class="ltx_emph ltx_font_italic" id="A2.F7.6.1">Left:</em> We vary the ordering of the few-shot examples and see it does not have any impact.
<em class="ltx_emph ltx_font_italic" id="A2.F7.7.2">Right:</em> We very the number <math alttext="n" class="ltx_Math" display="inline" id="A2.F7.3.m1.1"><semantics id="A2.F7.3.m1.1b"><mi id="A2.F7.3.m1.1.1" xref="A2.F7.3.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A2.F7.3.m1.1c"><ci id="A2.F7.3.m1.1.1.cmml" xref="A2.F7.3.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.F7.3.m1.1d">n</annotation><annotation encoding="application/x-llamapun" id="A2.F7.3.m1.1e">italic_n</annotation></semantics></math> of few-shot examples; while careful study of the flat region suggests increasing the number of few-shot examples marginally improves performance,
regardless,
the layer-pruning strategy is robust to
this kind of variation.
</figcaption>
<br class="ltx_break ltx_centering">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Finetuning seed</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">여기서 우리는 미세 조정 씨앗을 바꿉니다. 모든 실험의 경우 재현성을 보장하기 위해 다음 코드 조각을 사용합니다.</p>
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS2.p1.2">SEED_VAL = 0
transformers.enable_full_determinism(SEED_VAL)
</pre>
<p class="ltx_p" id="A2.SS2.p1.3">사전 학습된 모델로 시작하기 때문에 미세 조정 시드는 초기화에 영향을 미치지 않지만 데이터 순서와 같은 추가 학습의 확률적 측면에 영향을 미친다. 이를 제어하기 위해 Llama-2-13B에 적용했을 때 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#S3.SS2" title="3.2 Layer-pruning algorithm(s) ‣ 3 Method ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">3.2</span></a>에 설명된 주요 유사성 정보 가지치기에 대한 미세 조정 시드를 제거합니다. 그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F8" title="Figure 8 ‣ B.2 Finetuning seed ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">8</span></a>에서 계층 가지치기 방법이 시드 선택에 강력하다는 것을 관찰합니다.</p>
</div>
<figure class="ltx_figure" id="A2.F8">
<p class="ltx_p ltx_align_center ltx_align_center" id="A2.F8.1"><span class="ltx_text" id="A2.F8.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="330" id="A2.F8.1.1.g1" src="https://arxiv.org/html/2403.17887v1/x8.png" width="415"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 8:</span>MMLU 정확도에 대한 미세 조정 시드의 변화 효과 vs. Llama-2-13B에 대해 드롭된 층들의 분획: 의미 있는 효과가 없다.</figcaption>
</figcaption>
<br class="ltx_break ltx_centering">
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>LoRA rank</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1">여기서 우리는 치유에 사용되는 LoRA 순위를 바꿉니다. 불행히도 계산 예산으로 인해 모든 실험 구성에서 완전한 스윕을 수행할 수 없었습니다. 그 대신 주요 실험을 위해 다음 프로토콜을 사용했다.</p>
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1">QLoRA 설정에 따라 순위 64로 시작합니다(예: Ref.<cite class="ltx_cite ltx_citemacro_citep">(Dettmers et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#bib.bib19" title="">2023</a>)</cite>의 부록 B.2 참조).</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1">해당 순위를 가진 힐링이 힐링이 없는 것에 비해 성능을 크게 손상시키는 경우 해당 모델에 대해 LoRA 순위를 스윕하고 다른 평가에 대해 MMLU 정확도에 따라 가장 성능이 좋은 LoRA 순위를 선택한다.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="A2.SS3.p1.2">이 프로토콜은 치유가 모든 평가에서 성능을 향상시킬 가능성을 최대화하도록 설계되었습니다. 단순화를 위해 Llama-2-70B를 제외하고 간단한 가지치기 휴리스틱을 사용하여 이 순위 선택 프로토콜을 실행했다.</p>
</div>
<div class="ltx_para" id="A2.SS3.p2">
<p class="ltx_p" id="A2.SS3.p2.1">실제로, 이것은 순위 4, Llama-2-7B, 순위 2 및 Llama-2-70B를 제외한 모든 모델에 대해 순위 64를 사용하게 했다. (표 형식으로 이 동일한 정보를 검토하려면 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.1</span></a>의 두 번째 표를 참조한다.) 도<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F9" title="Figure 9 ‣ B.3 LoRA rank ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">9</span></a>는 Mistral-7B(좌하단 패널), Llama-2-7B(하단 중간 패널) 및 Llama-2-70B(우하단 패널)에 대한 이러한 선택을 지원하는 MMLU 정확도에 대한 스윕을 표시한다: 전반적으로, LoRA 순위는 치유된 모델의 정성적 행동에 큰 영향을 미치지 않지만, LoRA 순위를 감소시키면 일반적으로 성능이 향상된다. 그림 <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F9" title="Figure 9 ‣ B.3 LoRA rank ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">9</span></a>의 왼쪽 상단 및 중간 패널에서 유사성 정보 가지치기 전략을 사용하여 미스트랄-7B(상단) 및 라마-2-7B(중단)에 대한 해당 스윕을 보여준다. 이 가지치기 방법에 대해 순위 2가 여전히 라마-2-7B에 대해 상위 수행 순위이지만 두 모델 모두 훨씬 더 강력하다는 것을 알 수 있다.</p>
</div>
<figure class="ltx_figure" id="A2.F9">
<p class="ltx_p ltx_align_center ltx_align_center" id="A2.F9.1"><span class="ltx_text" id="A2.F9.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="279" id="A2.F9.1.1.g1" src="https://arxiv.org/html/2403.17887v1/x9.png" width="830"></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9:</span>Effect of varying the LoRA rank.</figcaption>
<span class="ltx_text ltx_font_bold" id="A2.F9.10.1">Top</span>: 5-shot MMLU accuracy vs. fraction of layers dropped
using the similarity-informed pruning strategy on
Mistral-7B (<em class="ltx_emph ltx_font_italic" id="A2.F9.11.2">left</em>), Llama-2-7B (<span class="ltx_text ltx_font_italic" id="A2.F9.12.3">middle</span>), and Llama-2-70B (<span class="ltx_text ltx_font_italic" id="A2.F9.13.4">right</span>).
Across all ranks we observe
similar behavior, though
there’s a small effect of
decreasing
rank
improving
overall performance.
<span class="ltx_text ltx_font_bold" id="A2.F9.14.5">Bottom, left and middle</span>: 5-shot MMLU accuracy vs. fraction of layers dropped
using the simple pruning heuristic on
Mistral-7B (<em class="ltx_emph ltx_font_italic" id="A2.F9.15.6">left</em>) and Llama-2-7B (<span class="ltx_text ltx_font_italic" id="A2.F9.16.7">middle</span>). As before, qualitative behavior is similar across ranks, though
in this case
it’s much clearer that decreasing rank improves performance.
<span class="ltx_text ltx_font_bold" id="A2.F9.17.8">Bottom, right</span>: C4 validation loss vs. fraction of layers dropped
using the similarity-informed pruning strategy on
Mistral-7B.
In contrast to MMLU, decreasing rank
harms performance; together, these results suggest that larger ranks may be overfitting.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS3.p3">
<p class="ltx_p" id="A2.SS3.p3.1">매우 낮은 순위(!)에 대해서도 LoRA 순위가 감소함에 따라 MMLU 정확도의 특징적인 개선은 설명할 가치가 있다. 한 가지 가능성은 LoRA 순위를 낮추면 과적합에 대한 미세 조정을 더 잘 정규화할 수 있다는 것이다. 특히, 기민한 독자는 §<a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A1.SS1" title="A.1 Model and healing details ‣ Appendix A Experimental Details ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">A.1</span></a>: 모델이 사전 훈련에 사용된 동일한 피크로 미세 조정됨; 64의 "대형" LoRA 순위는 C4에 오버핏될 수 있는 다수의 추가 파라미터를 도입한다. 우리가 고려하는 모델에 대한 실제 사전 훈련 데이터 세트는 우리에게 알려지지 않은 <em class="ltx_emph ltx_font_italic" id="A2.SS3.p3.1.1">(a)</em>이고, <em class="ltx_emph ltx_font_italic" id="A2.SS3.p3.1.2">(b)</em>이기 때문에 이 오버핏은 확실히 해로울 것이다.</p>
</div>
<div class="ltx_para" id="A2.SS3.p4">
<p class="ltx_p" id="A2.SS3.p4.1">미스트랄-7B에 대해 직접 조사한다. <a class="ltx_ref" href="https://arxiv.org/html/2403.17887v1#A2.F9" title="Figure 9 ‣ B.3 LoRA rank ‣ Appendix B Ablations ‣ The Unreasonable Ineffectiveness of the Deeper Layers"><span class="ltx_text ltx_ref_tag">9</span></a>의 오른쪽 하단 패널에서 우리는 서로 다른 LoRA 순위에 걸쳐 C4 검증 손실을 플롯한다: 우리는 LoRA 순위를 감소시키면서 일반적으로 MMLU 정확도를 향상시킨다는 것을 안다(cf. 왼쪽-가장 왼쪽 패널). 동시에 C4 유효성 검사 손실을 손상시킵니다. 이것은 우리의 과적합 가설을 뒷받침한다. 더 많은 자원이 투입된 미래에는 다른 형태의 정규화와 학습률 조정을 고려하여 치유 과정을 개선하는 것이 흥미로울 것이다.</p>
</div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
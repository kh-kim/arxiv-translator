[MISSING_PAGE_FAIL:1]

실제 인간 사용자에 의해 생성된 명령 데이터, OpenAI의 LLMs(예를 들어, InstructGPT[2] 및 ChatGPT4)는 큰 성공을 거두었다. 이러한 오픈 도메인 명령어는 LLM[14, 15, 16, 17]의 무한한 잠재력을 충분히 발휘하고 더 복잡하고 다양한 작업을 수행할 수 있게 한다. 그러나 OpenAI와 같은 오픈 도메인 명령어 데이터 세트를 생성하기 위해 인간을 사용하는 것은 다음과 같은 문제에 직면할 것이다. 전체 주석 처리 과정은 매우 비싸고 시간이 많이 걸린다[18, 19, 20, 21]. 한편, 인간이 만든 명령어의 난이도 분포는 (도 (a)a로부터의 ShareGPT[22]의 난이도 통계에 따르면) 더 적은 난이도와 함께 쉽거나 보통인 쪽으로 치우쳐 있다. 이에 대한 가능한 이유는 주석자 중 전문가의 비율이 낮고 복잡한 지시를 만들려면 많은 정신적 노력이 필요하기 때문이다. 인간 주석기는 피로에 취약하고 고난도 지침의 충분한 비율을 생성하기 위해 고강도 작업을 지속할 수 없다[23, 24, 25, 26]. 이러한 문제를 바탕으로 비교적 저렴한 비용으로 오픈 도메인 명령어(특히, 더 어려운 명령어)를 대량 생산할 수 있는 자동 방법을 개발하는 것이 명령어 조정 언어 모델[27, 28, 29, 30]을 더욱 발전시키는 열쇠가 된다.

각주 4: [https://chat.openai.com/](https://chat.openai.com/)

본 연구에서는 다양한 난이도의 오픈 도메인 명령어를 자동으로 양산하기 위해 인간 대신 LLM을 사용하는 새로운 방법인 _Evol-Instruct_를 도입하여 LLM의 성능을 향상시킨다. 그림 1은 _Evol-Instruct_ 의 실행 예제를 보여 줍니다. 간단한 초기 명령어 "1+1=7"에서 시작하여, 본 논문에서 제안하는 방법은 심층 진화(파란색 방향 선) 또는 폭 진화(빨간색 방향 선)를 무작위로 선택하여 간단한 명령어를 더 복잡한 명령어로 업그레이드하거나 새로운 명령어를 생성한다(다양성을 증가시킴). 심층 진화는 제약 추가, 심화, 구체화, 추론 단계 증가, 복잡한 입력의 다섯 가지 유형의 연산을 포함한다. In-breadth 진화는 돌연변이, 즉 주어진 명령을 기반으로 완전히 새로운 명령을 생성하는 것이다. 6개

그림 1: _Evol-Instruct_ 의 실행 예제입니다.

작업은 특정 프롬프트가 있는 LLM을 프롬프트하여 구현됩니다. 진화된 지침은 LLM에서 생성되기 때문에 때로는 진화하는 지침이 실패할 수 있다. 우리는 실패한 명령어를 필터링하기 위해 명령어 제거기를 채택하는데, 이를 Elimination Evolving이라고 한다. 우리는 다양한 복잡성을 포함하는 충분한 명령어 데이터를 얻기 위해 여러 라운드 동안 이러한 진화 과정을 반복한다.

진화된 명령어로 오픈 소스 LLaMA[4]를 미세 조정하고 명령어 피니튜닝에 대한 기존 SOTA 작업(예: 알파카[31] 및 비쿠나[22])과 유사한 성능을 평가하여 _Evol-Instruct_를 검증한다. 우리가 비교하는 명령어 데이터 세트는 Alpaca(자체-인스트럭션[32]를 사용하여 생성됨)가 사용하는 데이터 및 Vicuna가 사용하는 70k ShareGPT(실제 사용자가 공유됨)이다. 제안된 방법의 명령어 데이터셋이 인간이 만든 명령어 데이터셋보다 우수하다는 것을 증명하기 위해, 우리는 Alpaca의 훈련 데이터(175개의 인간이 만든 씨앗 명령어에서만 생성됨)를 초기 데이터셋으로 선택한다. OpenAI ChatGPT API5를 사용하여 4개의 진화 시대를 실행하고 최종적으로 250k 명령어를 획득한다. 비쿠나의 70k 실제 사용자 데이터와 공정한 비교를 보장하기 위해 전체 250k 데이터에서 동일한 양을 샘플링하고 LLaMA 7B 모델을 훈련했다. 모델 이름은 _WizardLM_ 입니다. 이전 명령어 추적 테스트 데이터 집합에서 어려운 명령어의 비율이 낮기 때문에 _Evol-Instruct_ 테스트 집합이라는 새로운 난이도 균형 테스트 데이터 집합을 수동으로 만들었다. 주석을 고용 하 고 GPT-4를 활용 하 여 _Evol-Instruct_ 테스트 집합 및 Vicuna의 테스트 집합에서 Alpaca, Vicuna, ChatGPT 및 _WizardLM_ 을 평가 합니다. 주요 연구 결과는 다음과 같다.

각주 5: [https://oai.azure.com/portal](https://oai.azure.com/portal)의 gpt-3.5-turbo.

* **Evol-Instruct_의 지침이 인간이 만든 ShareGPT의 지침보다 우수합니다.* * LLaMA 7B를 미세 조정 하기 위해 Vicuna와 동일한 양의 _Evol-Instruct_ 데이터 (즉, 70k)를 사용 하는 경우 모델 _WizardLM_은 인간 평가에서 각각 _Evol-Instruct_ 테스트 집합 및 Vicuna의 테스트 집합에서 Vicuna보다 승률이 \(12.4\%\) 및 \(3.8\%\) 더 높은 Vicuna보다 훨씬 우수 합니다. 또한, _WizardLM_은 GPT-4의 자동 평가에서 Alpaca 및 Vicuna보다 더 나은 응답 품질을 달성한다.
* **레이블러는 복잡한 테스트 지침에 따라 ChatGPT의 출력보다 _WizardLM_ 출력을 선호합니다.* * _Evol-Instruct_ 테스트 집합에서 _WizardLM_은 ChatGPT보다 더 나쁜 성능을 보이며, 승률은 ChatGPT보다 낮습니다 (\(28.0\%\). \ (40.8\%\)). 그러나, _Evol-Instruct_ 테스트 세트(난이도 수준 \(\geq 8\))의 고난이도 섹션에서, 우리의 _WizardLM_은 ChatGPT보다 승률 \(7.9\%\)이 더 크다(\(42.9\%\) vs. \ (35.0\%\)), 이는 인간 주석이 ChatGPT보다 모델의 출력을 더 선호한다는 것이다. 이는 _Evol-Instruct_가 복잡한 지침을 처리하는 LLM의 능력을 크게 향상시킬 수 있음을 나타냅니다.

## 2 관련 작업

Closed domain instruction fine-tune early instruction-following training work [33; 10]은 LMs의 교차 작업 일반화에 관한 것으로, LMs는 광범위한 공개 NLP 데이터 세트에서 미세 조정되고 다른 NLP 작업 세트에서 평가된다. T5[34]는 질의 응답, 문서 요약, 감정 분류 등의 자연어 처리(NLP) 작업을 통일된 텍스트 대 텍스트 형식을 이용하여 함께 훈련시킴으로써 가장 빠른 시도를 하였다. FLAN[10], ExT5[9], T0[12], KnowDA[35]와 같은 작업은 NLP 작업의 수를 약 100개로 증가시켰으며, 각 작업[36; 37; 38; 39]에 대해 신중하게 설계된 여러 지침을 사용했다. 더군다나 ZeroPrompt[11], FLAN-T5[13] 등의 작업은 작업 수를 수천 개로 끌어올렸다. 이러한 연구는 다양한 NLP 작업 지침을 사용하여 LM을 미세 조정하면 새로운 작업에 대한 성능이 향상된다는 것을 일관되게 보여준다. 그러나, 이러한 폐쇄형 명령들로 트레이닝된 LLM들(즉, 명령들은 종종 단일 NLP 태스크에 대해서만, 그리고 입력 데이터 형태는 단순함)은 실제 사용자 시나리오들에서 실패하는 경향이 있다.

오픈 도메인 교육은 우리의 작업을 세밀하게 조정하는 이 연구 라인에 속한다. OpenAI는 많은 주석자를 고용하고 그에 상응하는 정확한 응답으로 많은 지침을 작성했다. 이러한 인간이 만든 지침은 다양한 형태와 풍부한 과제 유형을 가지고 있다. 이 데이터셋을 기반으로 OpenAI는 GPT-3 [1]을 InstructGPT [2]로 학습하여 다양한 실제 사용자 명령을 처리할 수 있으며 ChatGPT의 성공으로 이어졌다. OpenAI의 이러한 뛰어난 작업은 오픈 소스가 아니기 때문에 Alpaca[31]와 Vicuna[22]는 오픈 소스 LLM LLaMA[4]를 기반으로 오픈 도메인 명령어 미세 조정을 적극적으로 탐구했다. 알파카는 수동으로 작성된 명령어의 제한된(예를 들어, 175개의 샘플) 시드 세트로부터 생성된 50k 명령어의 데이터세트를 사용했다. 비쿠나는 ShareGPT.com에서 수집된 ChatGPT와 70k의 사용자 공유 대화를 사용했으며, AI 생성 데이터를 사용하여 학습 미세 조정을 한다는 점에서 InstructGPT 및 Vicuna와 다르다. 알파카의 자체 지시[32] 생성 방법과 달리, _Evol-Instruct_는 생성된 명령어들의 난이도 및 복잡도 레벨을 제어할 수 있다.

## 3 Approach

이 섹션에서는 제안된 _Evol-Instruct_의 세부 사항에 대해 자세히 설명한다. 도 2에 예시된 바와 같이, 파이프라인은 주로 명령어 진화기 및 명령어 제거기의 두 개의 컴포넌트를 포함한다. 이들 컴포넌트의 상세는 Sec. 3.2에서 제시될 것이고, 명령어 미세 조정 방법은 Sec. 3.3에서 설명될 것이다.

### 명령어 데이터 진화 정의

주어진 초기 명령어 집합 \(D^{(0)}=\left(I_{k}^{(0)},R_{k}^{(0)}\right)_{1\leq k\leq N}\)에서 진화를 시작한다. 여기서, \(I_{k}^{(0)}\)은 \(D^{(0)}\)의 \(k\)-번째 명령어이고, \(R_{k}^{(0)}\)은 \(k\)-번째 명령어에 대한 대응 반응이며, \(N\)은 \(D^{(0)}\)의 샘플 수이다. 각 진화에서는 LLM 명령어 진화 프롬프트를 적용하여 \(D^{(t)}\)의 모든 \(I^{(t)}\)을 \(I^{(t+1)}\)으로 업그레이드한 다음, LLM을 사용하여 새로 진화된 \(I^{t+1}\)에 대한 대응 응답 \(R^{t+1}\)을 생성한다. 따라서, 우리는 진화된 명령어 데이터셋 \(D^{t+1}\)을 얻는다. 반복적으로 \(M\)진화를 수행함으로써 \(M\)진화 데이터셋 \([D^{(1)}\cdots D^{(M)}]\)을 순차적으로 얻을 수 있다. 본 연구는 오픈 도메인 명령어 데이터에 초점을 맞추고 있으며, 명령어는 명령어 부분과 입력 사이의 명확한 구분 없이 다양한 입력과 작업을 가지고 있다.

### 자동 명령 데이터 진화

우리의 명령어 진화 파이프라인은 1) 명령어 진화, 2) 응답 생성, 3) 제거 진화, 즉 진화에 실패한 인스트럭션 필터링의 세 단계로 구성된다.

명령어 진화.우리는 LLM이 특정 프롬프트를 사용하여 주어진 명령을 더 복잡하고 어렵게 만들 수 있음을 발견했다. 또한 똑같이 복잡하지만 완전히 다른 완전히 새로운 지침을 생성할 수 있습니다. 이 발견을 사용하여 초기 명령어 데이터 세트를 반복적으로 진화시켜 난이도를 개선하고 풍부함과 다양성을 확장할 수 있다. 우리는 주어진 초기 명령어 집합 \(D^{(0)}\)으로 명령어 풀을 시작한다. 각 진화 에포크에서 이전 에포크에서 업그레이드된 지침은 풀에서 꺼내진다. 그리고 인스트럭션 진화기를 이용하여 각 페치된 인스트럭션을 진화시키고, 인스트럭션 제거기를 이용하여 인스트럭션 제거기의 인스트럭션 발생 여부를 확인한다.

도 2: _Evol-Instruct_ 개요

[MISSING_PAGE_FAIL:5]

#Bewrite prompt must be a question style instruction rewritten Prompt(OUST contain a specific JSON data as input#:

In-Breadth Evolving의 프롬프트.In-Breadth Evolving은 토픽 커버리지, 기술 커버리지 및 전체 데이터 세트의 다양성을 향상시키는 것을 목표로 한다. 오픈 도메인 명령어 핀튠 데이터 세트(예: 알파카, 샤레GPT 등)는 일반적으로 규모가 작고 주제 및 기술 다양성이 부족하다. 이 문제를 해결하기 위해 주어진 명령어를 기반으로 완전히 새로운 명령어를 생성하도록 프롬프트를 설계하여 새로운 명령어가 더 긴 꼬리를 이루도록 하였다. In-Breadth Evolving 프롬프트는 다음과 같습니다.

나는 당신이 프롬프트 크리에이터 역할을 하기를 원한다. 목표는 @Given Prompt#에서 영감을 끌어 새 프롬프트를 만드는 것입니다. 이 새 프롬프트는 @Given Prompt#과 동일한 도메인에 속해야 하지만 훨씬 더 드물습니다. Created Prompt#의 LEMGTH 및 난이도 레벨은 @Given Prompt#과 유사해야 한다. #Created Prompt#은 합리적이고 인간이 이해하고 대응해야 한다. '#Given Prompt#', '#Created Prompt#', 'given prompt', 'created prompt'는 @Created Prompt#에 나타나지 않을 수 있습니다.

#Given Prompt#:

<여기 지시가 있다>

#Created Prompt#:

응답 생성.우리는 진화된 지침에 대한 해당 응답을 생성하기 위해 진화하는 것과 동일한 LLM을 사용한다. 생성 프롬프트는 "<여기 지시사항이다.>"이다.

Elimination Evolving.다음 네 가지 상황을 명령 진화 실패로 분류합니다.

1. 진화된 명령은 원본에 비해 어떠한 정보 이득도 제공하지 않는다. 우리는 이러한 결정을 내리기 위해 ChatGPT를 사용하는데, 자세한 내용은 부록 G를 참조하기 바란다.
2. 진화된 명령은 LLM이 응답을 생성하는 것을 어렵게 한다. 우리는 생성된 응답이 "죄송합니다"를 포함하고 길이가 상대적으로 짧을 때(즉, 80 단어 미만) LLM이 진화된 명령에 응답하기 위해 고군분투한다는 것을 종종 나타낸다. 그래서 우리는 판단을 내리기 위해 이 규칙을 사용할 수 있습니다.
3. LLM에 의해 생성된 응답은 구두점 및 불용어만을 포함한다.
4. 진화된 지침은 "주어진 프롬프트", "다시 쓰기 프롬프트", "#다시 쓰기 프롬프트#" 등과 같이 진화하는 프롬프트에서 일부 단어를 분명히 복사합니다.

### Evolved Instructions에서 LLM Finetuning

모든 진화가 완료되면 초기 명령어 데이터 세트를 모든 에폭의 진화된 명령어 데이터와 병합하고 샘플을 무작위로 섞어서 최종 미세 조정 데이터 세트를 생성한다. 이 처리는 데이터세트에서 다양한 난이도 레벨의 명령어의 고른 분포를 보장하여 모델 미세 조정 평활도를 최대화한다. 기존 명령어 튜닝 작업[32, 40]에서 복잡하거나 여러 개의 프롬프트 템플릿을 사용하지 않도록 하여 미세 조정된 모델이 오픈 도메인 명령어를 처리할 수 있도록 하였다. 모델이 표준 감독 방식으로 응답을 생성하도록 훈련하라는 프롬프트로 "### 응답:" 명령만 연결했습니다.

## 4 Experiment

자동 및 인간 평가를 모두 사용하여 _Evol-Instruct_ 테스트 집합 및 Vicuna 테스트 집합에서 _WizardLM_, Alpaca, Vicuna 및 ChatGPT를 평가합니다.

### Baselines

(1) **ChatGPT** 는 OpenAI에서 개발한 AI 챗봇으로 자연스럽고 매력적인 방식으로 사용자와 상호 작용할 수 있습니다. GPT-3.5 및 GPT-4와 같은 LLM 위에 구축되어 방대한 인터넷 텍스트 데이터에 대해 훈련됩니다. ChatGPT는 오늘날 사용 가능한 가장 진보되고 다재다능한 챗봇 중 하나이지만 사실적 정확성, 일관성 및 안전성과 같은 몇 가지 한계와 과제를 가지고 있다.

(2) **Alpaca** 는 Stanford University에서 개발한 오픈 소스 명령어 추적 모델입니다. 이는 LLaMA를 기반으로 하며 OpenAI의 text-davinci-003 모델에서 생성된 52K 명령어 추종 예제로 미세 조정된다.

(3) **Vicuna** 는 사용자 쿼리에 대 한 자연스럽고 매력적인 응답을 생성할 수 있는 오픈 소스 챗봇입니다. 이것은 LLaMA를 기반으로 하며 사람들이 채팅GPT 상호 작용을 공유하는 웹사이트인 ShareGPT에서 수집된 70K 사용자 공유 대화를 미세 조정한다. 현재 사용 가능한 가장 진보되고 다재다능한 개방형 교육 후속 모델 중 하나입니다. FastChat 6의 7B 모델을 사용합니다.

각주 6: [https://github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat)

### Experiment detail

데이터셋을 구성하기 위해 Alpaca의 52K 명령어 데이터셋으로 초기화 하였다. 반복적으로 \(M\)진화를 수행한 후, \(M=4\)에서 250K 명령어를 얻었다. 진화 각 라운드의 각 지시에 대해, 우리는 동일한 확률로 총 6개의 프롬프트(즉, 깊이 진화하는 프롬프트에서 5개, 깊이 진화하는 프롬프트에서 1개) 중에서 하나의 진화하는 프롬프트를 무작위로 선택한다. Azure OpenAI ChatGPT API7을 사용하여 위의 프로세스를 실행합니다. 마지막으로 ChatGPT를 활용하여 응답을 생성합니다. 우리는 1의 온도를 사용하여 응답을 생성하고 최대 생성 토큰 수를 \(2048\)으로 설정한다. 또한, 주파수 페널티는 0, top-p는 0.9로 설정하였다. 전체 데이터셋을 구성하기 위해 API 52 \(\times\) 4 \(\times\) 3 = 624K 회를 요청한다. 사전 훈련된 LLaMA 7B [4]를 사용하여 모델을 초기화합니다. 초기 학습률은 2 \(\times 10^{-5}\), 최대 토큰 수는 \(2048\), 배치 크기는 각 GPU에 대해 8이다. 딥스피드 제로-3와 함께 8 V100 GPU에서 모델을 3 에포크에서 70시간 동안 훈련합니다. 공정한 비교를 위해 알파카의 원래 다비시-003 응답을 ChatGPT의 응답으로 대체하고 _WizardLM_을 훈련하기 위해 70K 지침 하위 집합을 샘플링한다. 추론을 위해 _WizardLM_ 및 기준선 모델에 대한 동일한 설정에서 온도를 1로, 상단 p를 0.9로 설정하여 출력 무작위성을 줄이고 더 집중되고 결정론적 출력을 보장한다. 우리는 또한 1의 빔 크기를 사용하고 최대 생성 길이를 2048로 설정한다.

각주 7: [https://oai.azure.com/portal](https://oai.azure.com/portal)의 gpt-3.5-turbo.

### Testset build

온라인 오픈 소스 프로젝트, 플랫폼 및 포럼과 같은 다양한 원본에서 실제 인간 지침을 포함하는 _Evol-Instruct_ 테스트 세트를 수집했습니다. 이를 위해 코딩 세대 디버깅, 수학, 추론, 복잡한 형식, 쓰기, 광범위한 학문 등 인류의 주요 요구 사항을 나타내는 29개의 고유한 기술을 분석하였다. 그림 3a는 테스트 세트의 인스턴스 및 기술 분포를 보여준다. 테스트 세트는 218개의 인스턴스로 구성되며, 각 인스턴스는 특정 스킬에 대한 지침입니다. 우리는 우리의 테스트 세트를 지침에 따른 모델을 평가하기 위한 벤치마크 데이터 세트인 Vicuna의 테스트 세트와 비교했다. 우리는 비쿠나의 테스트가 80개의 인스턴스와 9개의 기술만을 설정했으며 우리보다 훨씬 작고 덜 다양하다는 것을 발견했다. 도 3b는 테스트 데이터의 난이도 및 복잡도가 상이한 인스턴스들에 걸쳐 어떻게 변하는지를 도시한다. 우리의 테스트 데이터는 더 균일한 분포를 가지며, 이는 난이도와 복잡도가 다른 지침을 포함한다는 것을 의미한다. 반면, 비쿠나와 알파카는 치우친 분포를 가지고 있어 대부분 난이도와 복잡도가 낮은 명령어를 포함하고 있음을 의미한다. 이것은 이 두 코퍼스가 더 복잡하고 까다로운 시나리오에 대한 평가를 처리할 수 없음을 나타낸다.

### Human evaluation

_WizardLM_을 평가하기 위해 _Evol-Instruct_ 테스트 집합에 대해 인간 평가를 수행합니다. 우리는 _WizardLM_과 기준선 사이의 블라인드 쌍별 비교를 수행한다. 특히, 교육을 잘 받은 10명의 주석을 모집합니다. 각 주석기에 알파카, 비쿠나-7b, _위저드LM_ 및 ChatGPT의 4가지 응답이 제시되며, 이는 출처를 숨기기 위해 무작위로 섞인다. 주석자는 부록 H의 기준에 따라 어떤 반응이 더 나은지 판단하며, 그 다음 4개의 반응을 1에서 5까지 순위를 매기고(1은 최고를 의미), 비교 가능한 경우에 대해 동일한 점수를 허용해야 한다. 승률을 추정하기 위해 각 모델 쌍 간의 승률, 손실률 및 타이 빈도를 비교한다.

**주요 결과** 실험의 결과는 그림 4에 보고되어 있습니다. _WizardLM_은 _Evol-Instruct_의 효과를 보여주는 Alpaca 및 Vicuna-7b보다 훨씬 더 나은 결과를 달성했습니다.

**고난도 기술에 대한 성능.** 그림 4c는 _WizardLM_이 고난도 지침에서 ChatGPT보다 인간 레이블러에서 선호하는 경우가 더 많다는 것을 나타냅니다 (난도 수준 \(>=8\)).

### GPT-4 자동 평가

챗봇 모델의 성능을 평가하기 위해 Vicuna [22]가 제안한 GPT-4 기반의 자동 평가 프레임워크를 채택한다. 우리는 Vicuna와 동일한 GPT-4 하이퍼 파라미터, 신속한 설정 및 평가 접근법을 따른다. 순서 편향을 완화하기 위해 쌍별 비교에서 _WizardLM_ 및 기타 모델의 배치를 대체한다. _WizardLM_은 홀수 id의 경우 첫 번째이고 짝수 id의 경우 두 번째이다. 도 4(a) 및 도 4(b)에 도시된 바와 같이, _WizardLM_은 _Evol-Instruct_ 테스트세트에서 Alpaca-7B 및 Vicuna-7B보다 큰 마진(즉, Alpaca-7B 및 Vicuna-7B의 경우 각각 6.2% 및 5.8%)만큼 우수하고, Vicuna 테스트세트에서 Vicuna-7B와 유사한 성능을 달성한다.

**다양한 기술에 대한 성능.** 그림 6은 _Evol-Instruct_ 테스트 집합에서 _WizardLM_ 및 ChatGPT의 기술 수준을 비교합니다. 결과는 _WizardLM_이 17개의 스킬에 대해 거의 90% 이상의 용량으로 ChatGPT의 성능을 평균 78% 달성한다는 것을 나타냅니다. 그러나 _WizardLM_은 코드, 수학 및 추론 시나리오와 고군분투하여 ChatGPT와 눈에 띄는 격차를 보여줍니다.

**다양한 난이도에 대한 성능.** 그림 4(c)에서 볼 수 있듯이 _WizardLM_은 모든 난이도에서 Vicuna를 능가하고 쉽고 어려운 기술에서 Alpaca를 초과하며 어려운 기술에서 ChatGPT의 거의 88% 용량에 도달합니다. 이것은 _WizardLM_이 잠재적으로 복잡한 문제를 해결하고 LLM 훈련을 위한 복잡한 데이터를 수집하는 인간의 노력을 줄일 수 있음을 시사한다.

도 4: _Evol-Instruct_ testset 및 Vicuna testset에 대한 인체 평가 결과.

그림 5: _Evol-Instruct_ 및 Vicuna 테스트 세트에서 GPT-4에 의해 평가된 응답 품질. (c)에서는 [1, 4], [5, 7], [8, 10]에서 난이도를 갖는 테스트 세트를 세 부분("Easy", "Medium", "Hard")으로 분할한다.

도 3: (a) _Evol-Instruct_ 테스트 세트의 기술 분포, (b) Vicuna, Alpaca(Self-Instruct)의 테스트 세트와 우리의 _Evol-Instruct_ 사이의 난이도 및 복잡도 레벨 분포.

**GPT-4와 인간 평가 간의 불일치입니다.* * 그러나 _WizardLM_은 위의 인간 평가의 결론과 반대되는 하드 스킬에 대해 ChatGPT에 손실되었습니다. 주요 이유는 i) 단정하고 생생한 서식에 대한 인간의 선호와 ii) 수동 주석 단계에서 사람들은 응답의 품질이 비슷하다면 컴파일 및 통과될 수 있는 코드 또는 수학 문제에 대한 추가 포인트를 선호하기 때문이다. 더 많은 뒷받침하는 증거는 부록 I의 **사례 연구** 섹션을 참조하시기 바란다.

### Discussion

**인간 지침을 심층적으로 통과 합니다.* * 지침 진화 과정의 깊이를 연구 하기 위해 ChatGPT를 사용 하 여 각 지침의 난이도 및 복잡성 수준을 판단 하는 데 도움이 됩니다. 사용된 프롬프트는 부록 E를 참조하세요. 그림 6(a) 및 6(b)는 ShareGPT의 인간 참가자가 만든 명령보다 더 복잡한 _Evol-Instruct_ 생성 명령을 보여 줍니다. 또한 진화 과정의 각 반복에 따라 명령어의 깊이가 크게 증가한다.

**In-breadth Surpassing Human Instructions.** 우리는 명령어의 의미 범위를 조사하는 것을 목표로 합니다. 우리는 t-SNE[41] 및 k-means[42] 알고리즘을 사용하여 명령어 BERT 임베딩을 20개의 클러스터로 분할한다. 부록 F의 그림 1은 클러스터를 표시하여 ShareGPT 및 알파카에 비해 방법의 우수한 분산을 강조하여 지침에서 더 큰 주제 다양성을 나타낸다.

도 6: _Evol-Instruct_ 테스트세트에서 _WizardLM_과 ChatGPT 사이의 각 스킬의 GPT-4 점수.

그림 7: ShareGPT, 알파카와 진화된 수업의 네 시대 사이의 난이도.

Conclusions

본 논문에서는 LLM을 위한 다양하고 복잡한 명령어 데이터를 생성하는 진화 알고리즘인 Evol-Instruct를 제시하였다. 우리는 우리의 접근법이 LLM 성능을 향상시켰다는 것을 증명했다. WizardLM은 복잡도가 높은 작업에 대한 최신 결과와 다른 메트릭에 대한 경쟁 결과를 달성했다.

**제한 사항** 이 문서에서는 자동 GPT-4 및 인간 평가 방법의 한계를 인정 합니다. 이 방법은 확장성과 신뢰성에 대한 문제를 제기한다. 또한, 테스트 세트는 LLM이 적용되거나 다른 방법과 비교할 수 있는 모든 시나리오 또는 도메인을 나타내지 않을 수 있다.

**Broader Impact.** Evol-Instruct는 다양한 도메인 및 애플리케이션에서 LLM 성능 및 상호 작용을 향상시킬 수 있지만 비윤리적, 유해 또는 오판의 소지가 있는 지침을 생성할 수도 있습니다. 따라서 윤리적, 사회적 함의를 해결하기 위한 AI 진화 지침에 대한 향후 연구를 촉구한다.

## Appendix Deepening Prompt

나는 네가 프롬프트 리라이터 역할을 했으면 좋겠어. 당신의 목표는 주어진 프롬프트를 좀 더 복잡한 버전으로 다시 작성하여 그러한 유명한 AI 시스템(예: ChatGPT 및 GPT4)을 다루기 어렵게 만드는 것입니다. 그러나 재작성된 프롬프트는 합리적이고 인간에 의해 이해되고 대응되어야 한다. #Given Prompt#:에서 테이블 및 코드와 같은 비텍스트 부분을 생략할 수 없습니다. 또한 #Given Prompt#에서 입력을 생략하지 말아 주십시오. SHOUTLD는 다음 방법을 사용하여 지정된 프롬프트를 복잡하게 만듭니다. #주어진 프롬프트#에 특정 문제에 대한 문의가 포함된 경우 문의의 깊이와 폭을 늘릴 수 있습니다. 또는 #Rewritten Prompt#을 장황하게 만들지 않도록 최선을 다해야 합니다. #Rewritten Prompt#은 #주어진 Prompt#에 10~20개의 단어만 추가할 수 있습니다. '#Given Prompt#', '#Rewritten Prompt#' and'swritten prompt' is not allowed to appear in #Rewritten Prompt#
#Given Prompt#: <Here is instruction.> #Rewritten Prompt#:

## Appendix B Conretizing Prompt

나는 네가 프롬프트 리라이터 역할을 했으면 좋겠어. 당신의 목표는 주어진 프롬프트를 좀 더 복잡한 버전으로 다시 작성하여 그러한 유명한 AI 시스템(예: ChatGPT 및 GPT4)을 다루기 어렵게 만드는 것입니다. 그러나 재작성된 프롬프트는 합리적이고 인간에 의해 이해되고 대응되어야 한다. #Given Prompt#:에서 테이블 및 코드와 같은 비텍스트 부분을 생략할 수 없습니다. 또한 #Given Prompt#에서 입력을 생략하지 말아 주십시오. SHOUTLD는 다음 방법을 사용하여 주어진 프롬프트를 복잡하게 만듭니다. 일반 개념을 보다 구체적인 개념으로 대체하십시오. 또는 #Rewritten Prompt#을 장황하게 만들지 않도록 최선을 다해야 합니다. #Rewritten Prompt#은 #주어진 Prompt#에 10~20개의 단어만 추가할 수 있습니다. '#Given Prompt#', '#Rewritten Prompt#' and'swritten prompt' is not allowed to appear in #Rewritten Prompt#
#Given Prompt#: <Here is instruction.> #Rewritten Prompt#:

## 부록 C 증가 추론 단계 프롬프트

나는 네가 프롬프트 리라이터 역할을 했으면 좋겠어. 당신의 목표는 주어진 프롬프트를 좀 더 복잡한 버전으로 다시 작성하여 그러한 유명한 AI 시스템(예: ChatGPT 및 GPT4)을 다루기 어렵게 만드는 것입니다. 그러나 재작성된 프롬프트는 합리적이고 인간에 의해 이해되고 대응되어야 한다. #Given Prompt#:에서 테이블 및 코드와 같은 비텍스트 부분을 생략할 수 없습니다. 또한 #Given Prompt#에서 입력을 생략하지 말아 주십시오. SHOUTLD는 다음과 같은 방법을 사용하여 주어진 프롬프트를 복잡하게 만듭니다. #주어진 프롬프트#이 몇 가지 간단한 사고 프로세스만으로 해결될 수 있는 경우 다중 단계 추론을 명시적으로 요청하도록 다시 쓸 수 있습니다. #Rewritten Prompt# can only add 10 to 20 words in #Given Prompt#

'#Given Prompt#', '#Rewritten Prompt#' and'swritten prompt' is not allowed to appear in #Rewritten Prompt#

#Given Prompt#: <Here is instruction.> #Rewritten Prompt#:Complicate Input Prompt

**Example-1:**

나는 네가 프롬프트 리라이터 역할을 했으면 좋겠어. 당신의 목표는 데이터 포맷을 사용하여 주어진 프롬프트를 더 복잡한 버전으로 다시 작성하여 그러한 유명한 AI 시스템(예: 채팅 및 GPT4)을 다루기 어렵게 만드는 것이다. 그러나 재작성된 프롬프트는 합리적이고 인간에 의해 이해되고 대응되어야 한다.

[Rewritten Prompt]에서 입력 데이터로 [XML 데이터] 형식의 텍스트를 추가해야 합니다.

주어진 프롬프트:

이 php 코드를 사용하여 xml 데이터를 가져옵니다.

Rewritten Prompt:

이 xml이 있는데 HTML 테이블을 자동으로 채우기 위해 xml 데이터를 얻고 싶은데 코드가 작동하지만 테이블 내용에 중복됩니다.

여기 xml 데이터가 있다:

```
xroot> xstats> item><day>2017-11-01/c/day> impressions>2192>/impressions><nooney>1>.96790003</nooney><item>cday>2017-11-02/c/day> impressions>222</impressions><nooney>3.208500033</nooney><item><nooney>3.61171103</day> impressions>3680</impressions><nooney>3.321799981</nooney></item></status><nooney>8.498200044</nooney></total></filter> <dateFrom>2017-11-01</dataFrom>datat>2017-11-03</dataTo> group>>4ay/group></format></format></factor>
```

이 php 코드를 사용하여 xml 데이터를 가져오지만 이 코드는 중복 필드 테이블을 만드는 전체 xml 데이터에서 가져옵니다.

```
cPph> %don=newDONDCOUNT: %don=->load('[http://example.com/](http://example.com/)'. % datascelected. '&dateTo='. % datascelected2. '&format=xml'): %day='%don->getElementsByTagName('day'); %inpressions=\%don->getElementsByTagName('impressions'); } echo( "<table>"):  foreach(\%days\\#load){ foreach(\%inpressions as\\#node2){ echo'<tr>': %don1=>textContent. "<td>": echo<td>'. %node2->textContent. "<td>": echo<td>'. %node2->textContent*0.5/1000. "<td>": echo<'<tr>': } echo( "</table>"): }

제가 이걸 어떻게 고칠 수 있는지 힌트 줄 사람 있나요? 고마워요
```

[MISSING_PAGE_FAIL:13]

쿼리 텍스트요?

**Example-4:**

나는 네가 프롬프트 리라이터 역할을 했으면 좋겠어. 당신의 목표는 데이터 포맷을 사용하여 주어진 프롬프트를 더 복잡한 버전으로 다시 작성하여 그러한 유명한 AI 시스템(예: 채팅 및 GPT4)을 다루기 어렵게 만드는 것이다. 그러나 재작성된 프롬프트는 합리적이고 인간에 의해 이해되고 대응되어야 한다. [다시 쓰기 프롬프트]에서 입력 데이터로 [HTML 페이지] 형식의 텍스트를 추가해야 합니다.

주어진 프롬프트:

전체 HTML 페이지를 스크롤

다시 쓰기 프롬프트(입력으로 특정 HTML 페이지를 포함해야 함): 전체 페이지를 스크롤할 수 있지만 스크롤바가 표시되지 않으면 스크롤할 수 있습니다. 구글 크롬에서는

```
::-webkit_scrollbar{ display:none;}
```

그러나 모질라 파이어폭스와 인터넷 익스플로러는 그렇게 작동하지 않는 것 같습니다. 나도 CSS에서 이걸 해봤어

```
overflow:hidden;
```

그것은 스크롤바를 숨기는 것이지만 더 이상 스크롤할 수 없습니다. 전체 페이지를 스크롤할 수 있는 동안 스크롤바를 제거할 수 있는 방법이 있습니까?

CSS나 HTML만 있으면 됩니다.

```
###

**Example-5:**

나는 네가 프롬프트 리라이터 역할을 했으면 좋겠어. 당신의 목표는 데이터 포맷을 사용하여 주어진 프롬프트를 더 복잡한 버전으로 다시 작성하여 그러한 유명한 AI 시스템(예: 채팅 및 GPT4)을 다루기 어렵게 만드는 것이다. 그러나 재작성된 프롬프트는 합리적이고 인간에 의해 이해되고 대응되어야 한다.

[다시 쓰기 프롬프트]에서 [Shell cmd] 형식의 텍스트를 입력 데이터로 추가해야 합니다.

주어진 프롬프트:

Shell scp file

다시 쓰기 프롬프트(특정 셸 cmd를 입력으로 포함해야 함): 원격 서버에서 로컬 컴퓨터로 파일을 스크롤하려고 합니다. 80 포트만 액세스할 수 있습니다. 난 노력했어

scp -p 80 username@www.myserver.com:/root/file.txt.

그러나 이 오류가 발생했습니다. cp: 80: 그런 파일이나 디렉터리가 없습니다. scp 명령에서 포트 번호를 지정하려면 어떻게 해야 하나요?
``` ###

**Example-6:**

나는 네가 프롬프트 리라이터 역할을 했으면 좋겠어. 당신의 목표는 데이터 포맷을 사용하여 주어진 프롬프트를 더 복잡한 버전으로 다시 작성하여 그러한 유명한 AI 시스템(예: 채팅 및 GPT4)을 다루기 어렵게 만드는 것이다. 그러나 재작성된 프롬프트는 합리적이고 인간에 의해 이해되고 대응되어야 한다. [JSON 데이터] 형식의 데이터를 입력 데이터로 추가해야 합니다. [다시 쓰기 프롬프트]에서 [JSON 데이터] 코드를 입력 코드로 추가해야 합니다. 다시 쓰기 프롬프트는 질문 스타일 명령어가어야 합니다. 프롬프트: 고객 구매 이력의 JSON 데이터 세트가 주어지면 고객이 동일한 상점에서 다시 구매할 확률을 계산할 수 있습니다. 조건부 확률에 대한 공식을 활용할 수 있는가? \(P(A|B)=P(A\cap B)/P(B)\) 여기서 A는 반복 구매를 하는 고객의 이벤트를 나타내고 B는 동일한 상점에서 다시 구매를 하는 고객의 이벤트를 나타낸다. 또한 이 공식을 적용하여 반복 구매 가능성이 가장 높은 고객 세그먼트를 식별하려면 어떻게 해야 합니까? 지정된 JSON 데이터 세트를 사용하여 이 수식을 구현하는 방법의 예를 제공할 수 있습니까? Rewritten 프롬프트는 질문 스타일 명령입니다. Rewritten 프롬프트(특정 JSON 데이터를 입력으로 포함해야 함):

## 부록 E 난이도 판단 프롬프트

다음 질문의 난이도와 복잡도를 평가하고 평가해 주셨으면 합니다. 1에서 10까지의 척도로 전체 점수를 부여해야 하며, 점수가 높을수록 난이도와 복잡도가 높아집니다. 다른 이유 없이 점수를 줘야 합니다.

## 질문: <다음은 지침입니다. >

## Score:

## Appendix F Cluster Scatter Plot

## Appendix G Equal Prompt

다음은 Chat6PT AI에 대한 두 가지 지침입니다. 서로 동등하다고 생각하십니까?

그림 8: ShareGPT, Alpaca와 C1에서 C4로의 명령어 진화 4라운드 사이의 군집 산점도. 군집 중심 수는 20개이다.

1. 이들은 동일한 제약들 및 요건들을 갖는다.
2. 그들은 조사의 깊이와 폭이 같다. 첫 번째 프롬프트: <여기 첫 번째 프롬프트입니다.> 두 번째 프롬프트: <여기 두 번째 프롬프트입니다.> 판단(Just answer: Equal or Not Equal. No explain to explain reason.)

## Appendix H Human Evaluation Aspects

이어서, 주석자들은 다음의 다섯 가지 측면에서 어떤 응답이 더 나은지를 판단한다:

(1) 관련성: 문맥과 질문의 의미적 의미를 올바르게 해석하는 모델의 능력 평가.

(2) Knowledgeable: 모델이 다양하고 상세한 지식을 문제 해결에 정확하게 활용할 수 있는지 여부.

(3) 추론: 문제를 해결하기 위해 올바른 추론 프로세스를 실행하거나 유효한 추론 개념을 고안할 수 있는 모델의 능력을 평가한다.

(4) 계산: 모델이 수학, 생물학, 화학 및 물리학의 영역에서 제공된 공식의 정확한 수학적 계산을 수행할 수 있는지 여부를 평가한다.

(5) 정확도: 주어진 명령에 대해 모델이 대응에서 올바르게 수행할 수 있는지 여부를 평가하는 단계.
```

\begin{table}
\begin{tabular}{l} \hline \hline \multicolumn{1}{l}{**Instruction:** you must write a simple version control system, using c++ \\ \hline
**[Model: _WizardLM_]** As an AI language model, I can provide you with a basic version control system in C++. Here is an example code that implements a simple version control system: \\ \hline
**include ‘cistream>** \\ **include ‘ctring>** \\ **include ‘cretc’>** \\ **include ‘ctring’** \\ **using namespace std;** \\ **/** Version Control system \\ **class VersionControl** {  public; // /Constructor { \\ **VersionControl**() {  this->currenfranch = “master”;  this->versionFile = “version.txt”; } // Method to check out a branch \\ **codeRout(String branch) ()** // Check if the branch already exists \\ **if** (GiszStranch(branch)) ( // If the branch exists, returns success \\ **count** **<** **?Eranch** **<** branch **<** **?E**+** already exists." **<<** endl; **)** else { // If the branch does not exist, create it \\ **createBranch(branch)** \\ **//** Checkout the current branch \\ **checkoutBranch()** \\ **} \\ **//** Method to check out the current branch \\ **void** checkoutBranch() ( )** //** Open the version file \\ **ifstream VersionFile** \\ **versionFile** \\ **versionFile** \\ **/** Read the version number from the version file \\ **in** version; \\ **versionFile** \\ **/** Checkout the current branch \\ **out** **<** **Checkout** **<<** currentBranch **<<** **?E**...** **<<** endl; **ifstream CheckoutFile** \\ **checkoutFile** \\ **if** (CheckoutFile**) ( \\ **count** **<** **?E**occ=**! **end; \\ **)** else {  cout **cc** **?E**error:** Unable to checkout file." **<<** endl; \\ **} \\ **//** Method to create a new branch \\ **void**createBranch(string branch) ( //**Create a new branch directory \\ **directory**(branch()**t**) //**'+** branch); \\ **//** Create a new version file \\ **offstream VersionFile**; **versionFile**; **versionFile**; **versionFile**; **(**versionFile**) +** **/**/**+** branch **+** **?E**.txt”; **versionFile**; **versionFile**; **(**case()**); \\ \hline \hline \end{tabular}
\end{table}
표 1: 코드 생성 스킬에 대한 비교 사례 // 브랜치 존재 방법}  exitstrBranch(stringBranch) { // 브랜치 디렉토리가 존재하는지 확인  (directoryKights(branchPath() + "/" + branch)) {  returntrue: } else {  returnfalse: } } // 현재 브랜치 획득 방법  stringBectrumranch() // 버전 파일에서 버전 번호 읽기  if(aikVirversionFile) { //버전 파일 열기  ifstreamSerialFile. //Open(versionFilePath()): // 버전 파일에서 버전 번호 읽기  intVERSIONFile >>version; // 버전 파일에서 현재 분기 가져오기  stringCurrentBranch = versionFile.substr(0, versionFile.find("."));  versionFile.close();  return currentBranch; } else {  return "master";  } } // 버전 번호 가져오기 방법  intSetVversion() { //Open the Version file  ifStratonVersionFile;  versionFile. open(versionFilePath()); // Read the version number from the version file  ifversionFile. //vversionFile. //Return version number  return version; } private: string currentBranch;  string versionFilePath;  string currentBranch();  string currentBranch();  bool /directoryKights(String path) { //Check if directory exists  if (access(path.ctr(),F_0X)==0) {  returntrue;  } else {  returnfalse;  } }  stringbranchPath() {  return exitBranch(branchPath() + "/" + branch);  }  void cretarch(string branch) {  ddirectory(branchPath() + "/" + branch);  offsetstrBranchVersionFile;  vversionFile. open(versionFile() + "/" + branch + ".txt");  vversionFile < "0" < endl;  vversionFileclose(); } vdischcuchFranch() {  void cretarch()  cbackoutFile. open(currentBranchPath() + "/" + currentBranch + ".cpp");  if (cbackoutFile) {  cout < "Success!" < endl;  } else {  cout < < "Error: unable to checkout file." << endl;  } } }; intmain() {  VeraxisoControlvc;  vc.checkOut("new_branch"); } return0; } ```

이 코드는 사용자가 새로운 분기를 체크아웃하고, 현재 분기를 체크아웃하고, 새로운 분기를 생성하고, 버전 넘버를 얻을 수 있는 기본 버전 제어 시스템을 구현한다. 코드는 간단한 디렉터리 구조를 사용하여 서로 다른 분기를 저장하고 버전 파일을 만들어 버전 번호를 저장합니다. 코드에는 분기가 있는지 확인하고 현재 분기를 가져오는 방법도 포함됩니다.

```
[Model:Vicuna] 여기 C++로 작성된 간단한 버전 제어 시스템이 있습니다.
```
#include<<ostream>
#include<String>
#include<vector> usingnamesactid; //Classtotypesatrepository classRepository public: //Constructor Repository() { //AddAflletotherepository voidaddfile(concatstringfileName,conststring&fileContent){  files.push_back(make_pair(fileName,fileContent)); } //Removeafllefromtherepository voidremovefile(conststringfileName){ (autoflle:file){  if(file.first=fileName){  files.arsars(file); } } } //Getthefilecontentforagivenfile stringgetFileContent(conststringfileName)const{  for(const*addfile;files); } } return""; } //Getthefilesintherepository vectorpairstringstring?getFile()const{  returnfiles; } private:  Vector<pair<string,string>>files;//Avectortostorethefilesintherepository }; classUser{ public: //Constructor User(conststringname):name_(name){ //AddA

// Remove a file from the repository void remove/file(const string fileName) { repository_.removeFile(fileName); } // Get the file content for the given file string getFileContent(const string fileName) const { return repository_.getFileContent(fileName); } // Get the files in the repository vector<pair<string.string> getFiles() const { return repository_.getFile(); } private: string name_; // The name of the user Repository repository_; // The repository that the user is currently working on }; int main() { Repository_repo; User user('John'); user.addFile("file1.txt"); cout << user.getFiles() << endl; return 0; } ```
이 시스템에서 '리포지토리'는 코드의 리포지토리를 나타냅니다. 각 파일의 내용과 파일의 이름을 담고 있는 '파일'의 벡터를 가지고 있다. '사용자' 클래스는 리포지토리에서 코드를 작업하는 사용자를 나타냅니다. 리포지토리'에 대한 참조가 있으며 리포지토리에서 파일을 추가하고 제거할 수 있는 방법과 파일의 내용을 가져올 수 있는 방법을 제공합니다. 'ain'에서는 시스템을 사용하는 간단한 예를 보여주고 있다. "John"이라는 이름의 사용자는 "Hello, world!"라는 내용과 함께 "file1.txt"라는 이름의 파일을 저장소에 추가한 후, 사용자는 파일을 제거한다. 마지막으로, 저장소의 파일들은 'getFiles' 메소드를 사용하여 출력된다.
``` [Model:Alpac]A AI 언어 모델로서 코드를 쓸 수 없지만 C++를 사용하여 간단한 버전 제어 시스템을 만들 수 있는 방법의 일반적인 개요를 제공할 수 있습니다. 1. 버전 제어 모델을 정의합니다. 사용하려는 버전 제어 모델을 결정합니다. Git, SVN 등과 같은 많은 버전 제어 모델이 있습니다. 필요에 맞는 리포지토리를 선택합니다. 2. 리포지토리 만들기: 서버 또는 로컬 시스템에서 리포지토리를 만듭니다. 리포지토리는 모든 원본 코드를 저장합니다. 3. 분기 전략 만들기: 코드의 다른 버전을 관리하기 위한 분기 전략을 만듭니다. 4. 끌어오기 요청 만들기: 한 분기에서 다른 분기로 변경 내용을 병합하기 위한 끌어오기 요청 만들기. 5. 커밋 메시지 만들기: 모든 커밋에 커밋 메시지를 추가하여 코드의 변경 내용을 설명합니다. 6. 버전 제어 클라이언트 사용: Git 또는 SVN과 같은 버전 제어 클라이언트를 사용하여 버전 제어 시스템과 상호 작용합니다. 7. 릴리스 프로세스 만들기: 다른 환경에 코드를 배포하기 위한 릴리스 프로세스를 정의합니다. 8. 시스템 모니터링: 코드가 올바르게 버전화되고 충돌이 없는지 확인하기 위해 시스템을 모니터링합니다. 9. 리포지토리 백업: 리포지토리를 정기적으로 백업하여 코드가 안전하고 재해 시 복구할 수 있는지 확인합니다. 10. 팀을 훈련시킨다: 모든 사람이 올바르게 사용하고 있는지 확인하기 위해 버전 제어 시스템을 올바르게 사용하는 방법을 팀을 훈련시킨다.

[MISSING_PAGE_FAIL:21]

## 6 Conclusion

\begin{table}
\begin{tabular}{p{34.1pt} p{34.1pt}} \hline \hline  & Skill: Sport & Difficulty: 8 \\ \hline
**Instruction:** What are the benefits and risks of high-intensity interval training (HIIT) for athletes? Compare and contrast HIIT with other forms of aerobic exercise, such as jogging, cycling, or swimming. Provide examples of HIIT workouts and explain how they can improve performance, endurance, and health. \\ \hline _WizardLM_ & High-intensity interval training (HIIT) is a form of exercise that involves short bursts of intense exercise followed by periods of rest or low-intensity exercise. HIIT has become increasingly popular among athletes and fitness enthusiasts due to its ability to improve athletic performance, increase endurance, and promote overall health. \\  & Benefits of HIIT for Athletes: \\  & 1. Improved Athletic Performance: HIIT can improve athletic performance by increasing power, speed, and agility. This is because HIIT workouts improve cardiovascular endurance, which is essential for athletes who need to maintain a high level of physical activity for extended periods of time. \\  & 2. Increased Endurance: HIIT workouts can also increase endurance by improving the body’s ability to use oxygen during exercise. This is because HIIT workouts increase the body’s aerobic capacity, which is the ability to use oxygen during exercise. \\  & 3. Improved Health: HIIT workouts can improve overall health by increasing metabolism, reducing the risk of chronic diseases, and improving mood. Risks of HIIT for Athletes: \\  & 1. Injury: HIIT workouts can be intense and may increase the risk of injury if not properly supervised or executed. \\  & 2. Overtraining: HIIT workouts can be intense and may lead to overtraining if not properly planned or executed. \\  & Comparison with Other Forms of Aerobic Exercise: \\  & 1. Jogging: Jogging is a low-intensity aerobic exercise that is generally considered to be less effective than HIIT for improving athletic performance and increasing endurance. \\  & 2. Cycling: Cycling is a moderate-intensity aerobic exercise that is generally considered to be more effective than jogging for improving athletic performance and increasing endurance. \\  & 3. Swimming: Swimming is a low-impact aerobic exercise that is generally considered to be effective for improving athletic performance and increasing endurance. \\  & Examples of HIIT Workouts: \\  & 1. Sprint Intervals: Sprint intervals involve sprinting at maximum intensity for 30 seconds followed by 90 seconds of rest. This workout can improve athletic performance, increase endurance, and improve health. \\  & 2. Tabata: Tabata involves 20 seconds of intense exercise followed by 10 seconds of rest for 4 minutes. This workout can improve athletic performance, increase endurance, and improve health. \\  & 3. Burst Training: Burst training involves short bursts of high-intensity exercise followed by periods of rest or low-intensity exercise. This workout can improve athletic performance, increase endurance, and improve health. \\  & In conclusion, HIIT is a form of exercise that has numerous benefits for athletes, including improving athletic performance, increasing endurance, and promoting overall health. However, it is important to properly plan and execute HIIT workouts to avoid injury and overtraining. Compared to other forms of aerobic exercise, HIIT is generally considered to be more effective for improving athletic performance and increasing endurance. \\ \hline \hline \end{tabular}
\end{table}
표 2: 스포츠 스킬 비교 사례

**비쿠나**: 고강도 간격 훈련 (HIIT)은 짧은 강도의 활동 폭발 후 짧은 휴식 기간 또는 활성 회복을 포함 하는 운동의 형태입니다. HIIT는 경기력, 지구력 및 전반적인 건강을 향상시킬 수 있는 잠재력으로 인해 최근 몇 년 동안 운동선수들 사이에서 인기를 얻고 있다.

선수를 위한 HIIT의 이점:

1. 증가된 지구력: HIIT는 더 길고, 더 치열한 운동 및 경쟁으로 전이될 수 있는 더 짧은 시간 버스트 동안 높은 레벨에서 수행하는 능력을 향상시킴으로써 운동선수들의 지구력을 구축하는 것을 도울 수 있다.

2. 향상된 성능: 무산소 역치를 개선함으로써--당신의 신체가 눈에 띄게 피로하기 시작하는 지점--HIIT는 운동선수들이 화상을 뚫고 더 높은 레벨에서 경기하는 것을 도울 수 있다.

3. 빠른 결과: 고강도 특성으로 인해, HIIT는 운동선수들에게 짧은 시간 내에 상당한 트레이닝 자극을 제공할 수 있어, 전통적인 형태의 운동보다 더 빠르게 결과를 볼 수 있게 한다.

선수들을 위한 HIIT의 위험:

1. 부상: 충격이 큰 HIIT 운동은 필요한 빠르고 폭발적인 움직임으로 인해 부상 위험을 증가시킬 수 있다. 선수들은 부상 위험을 줄이기 위해 HIIT 운동 전후에 적절하게 몸을 풀고 식혀야 한다.

2. 심장 건강: HIIT 운동은 심장에 상당한 양의 스트레스를 줄 수 있으며, 이는 심장 상태 또는 이력이 있는 개인에게 위험할 수 있다. 운동선수는 새로운 워크트 요법을 시작하기 전에 의사와 상담해야 합니다.

유산소 운동의 다른 형태와의 비교:

HIIT는 고강도, 짧은 폭발 특성에서 조깅, 사이클링 또는 수영과 같은 다른 형태의 유산소 운동과 다르다. 전통적인 유산소 운동은 지속적이고 꾸준한 강도의 상태를 가지고 있지만, HIIT는 짧은 기간의 강렬한 활동과 휴식을 사용하여 더 큰 피로와 그에 따른 성능 향상을 초래할 수 있다.

HIIT 워크아웃의 예:

1. 버피: 버피는 스쿼트, 팔굽혀펴기, 점프 등을 조합하여 단시간에 많은 칼로리를 소모하는 전신 운동이다.

2. 스프린트 간격: 스프린트 간격은 짧고 전면적인 스프린트에 이어 짧은 조깅 또는 휴식 기간을 포함한다. 이 운동은 속도와 힘을 향상시킬 수 있습니다.

3. 배틀로프: 배틀로프는 굵은 로프를 리듬감 있는 동작으로 스윙하는 것을 포함하며, 높은 강도의 전신 운동을 제공한다.

전반적으로 HIIT는 지구력, 성능을 개선하고 빠른 결과를 제공하는 능력으로 인해 운동선수의 훈련 요법에 귀중한 추가가 될 수 있다. 그러나 운동선수들은 HIIT 운동에 참여하기 전에 의사와 상의하고 적절하게 몸을 풀고 식혀 부상 위험을 줄이고 전반적인 건강을 개선해야 한다.

**알파카**: AI 언어 모델로서 개인적인 경험이나 의견은 없지만 연구와 연구를 기반으로 몇 가지 정보를 제공할 수 있습니다.

고강도 간격 훈련(HIIT)은 짧은 강도의 활동 폭발과 회복 기간이 수반되는 운동의 한 형태이다. 경기력, 지구력, 건강 등을 향상시킬 수 있어 선수들에게 인기 있는 훈련 방법이다.

선수들에게 HIIT의 이점:

1. 향상된 성능: HIIT는 근력, 지구력 및 파워를 증가시켜 운동 성능을 향상시킬 수 있다. 또한 속도, 민첩성 및 조정을 향상시킬 수 있습니다.

2. 지구력 증가: HIIT는 운동 시 신체의 산소 사용 능력을 증가시켜 지구력을 향상시킬 수 있다. 이것은 운동 선수들이 지치지 않고 더 오랜 기간 동안 운동하는 데 도움이 될 수 있습니다.

3. 심혈관계 건강 개선: HIIT는 혈류의 증가, 혈압의 감소 및 신체의 혈당 사용 능력 향상으로 심혈관계 건강을 개선할 수 있다.

4. 체중감량 및 지방연소: HIIT는 신진대사를 증가시키고 체내 저장된 지방의 양을 줄일 수 있기 때문에 운동선수들의 체중감량 및 지방소모에 도움을 줄 수 있다.

선수들의 HIIT 위험:

1. 부상: HIIT는 운동선수들이 적절하게 준비운동을 하지 않았거나 체력 수준을 쌓지 않았다면 위험할 수 있다. 갑작스러운 강도의 변화는 신체에 너무 많은 스트레스를 주고 부상으로 이어질 수 있다.

2. 탈수: HIIT는 특히 더운 날씨 조건에서 탈수될 수 있다. 운동선수는 운동 전, 운동 중, 운동 후에 적절하게 수분을 공급해야 합니다.

3. 소손: HIIT가 소모될 수 있고, 잘못하면 소손으로 이어질 수 있다. 운동선수들은 그들이 현명하게 훈련하고 무리하지 않도록 해야 합니다.

다른 형태의 유산소 운동과의 비교 및 대조:

HIIT는 여러 가지 방법으로 조깅, 사이클링 또는 수영과 같은 다른 형태의 유산소 운동과 다르다.

1. 강도: HIIT는 다른 형태의 운동보다 더 강렬하며, 이는 더 짧은 시간 내에 더 나은 결과를 초래할 수 있다.

2. 회복: HIIT는 짧은 회복 기간을 수반하며, 이는 근력과 지구력을 증가시키는 데 도움이 될 수 있다. 조깅과 같은 다른 형태의 운동은 충분한 회복 시간을 제공하지 못할 수 있다.

3. 시간: HIIT는 다른 형태의 운동보다 짧은 시간에 행해질 수 있고, 이는 시간이 부족한 운동선수들에게 유익할 수 있다.

4. 가변성: HIIT는 개별 운동선수들의 요구를 충족시키기 위해 다양하고 맞춤화될 수 있다. 다른 형태의 운동은 더 정적일 수 있고 더 적은 맞춤화를 필요로 할 수 있다.

전반적으로 HIIT는 경기력, 지구력 및 심혈관 건강을 개선하려는 운동선수에게 유익한 운동 형태가 될 수 있다. 그러나 주의 깊게 접근하고 부상과 번아웃을 피하기 위해 스마트하게 훈련하는 것이 중요하다.

**ChatGPT**: 고강도 간격 훈련 (HIIT)은 성능, 지구력 및 전반적인 건강을 향상시킬 수 있는 잠재력으로 인해 운동 선수 및 피트니스 애호가 사이에서 인기 있는 운동 방법입니다. HIIT는 짧은 격렬한 운동 폭발과 휴식 기간 또는 적극적인 회복을 포함한다. 운동선수들을 위한 HIIT의 일부 이점들은 다음을 포함한다:

1. 개선된 심혈관 건강: HIIT는 심혈관 적합도를 증가시키고 심장 질환의 위험을 감소시키는 것으로 나타났다.

2. 지구력 증가: HIIT는 더 오랜 기간 동안 높은 강도로 운동하는 능력을 증가시킴으로써 운동선수들의 지구력 향상에 도움을 줄 수 있다.

3. 근력의 증가: HIIT는 운동선수들이 근력을 구축하고 전반적인 운동수행능력을 향상시키는데 도움을 줄 수 있다.

그러나 특히 운동선수가 제대로 준비되지 않았거나 운동 사이에 적절한 회복 시간이 없는 경우 부상 위험을 포함하여 HIIT와 관련된 일부 위험도 있다.

HIIT를 조깅, 사이클링 또는 수영과 같은 다른 형태의 유산소 운동과 비교할 때 HIIT는 일반적으로 더 짧은 시간에 심혈관 건강을 개선하고 지방을 태우는 데 더 효과적인 것으로 간주된다. 예를 들어, 30분 HIIT 운동은 지방 연소 측면에서 60분 정상 상태 조깅보다 더 효과적일 수 있다.

다음은 HIIT 워크아웃의 몇 가지 예입니다.

1. 타바타 운동: 격렬한 운동 20초 후 8라운드 동안 10초 휴식, 총 4분.

2. 피라미드 운동: 30초 단거리 달리기 시작, 매 라운드마다 10초씩, 최대 1분까지 지속시간을 증가시킨 다음, 30초까지 다시 작업한다.

3. 30/30 운동: 30초의 격렬한 운동에 이어 30초의 휴식을 여러 라운드 동안 반복한다.

전반적으로 HIIT는 운동선수에게 유익한 운동 방법이 될 수 있지만 부상 및 과잉 훈련을 피하기 위해 주의하고 적절한 지침으로 접근하는 것이 중요하다.

\begin{table}
\begin{tabular}{l|l} \hline \hline  & Skill: Chemistry, Difficulty: 8 \\ \hline
**Instruction:** Which of the following substances is the most toxic to humans based on its median \\ lethal dose (LD50)? \\ A) Arsenic B) Cyanide C) Mercury D) Botulinum toxin \\ \hline _WizardLM_ & The most toxic substance to humans based on its median lethal dose (LD50) is botulinum \\ toxin. The LD50 for botulinum toxin is estimated to be between 1 and 5 ng/kg of body weight, \\ which means that it can be lethal if ingested in small amounts. Arsenic, cyanide, and mercury \\ are also highly toxic substances, but their LD50 values are higher than botulinum toxin. \\ \hline
**Vicuna** & B) Cyanide is the substance that is the most toxic to humans based on its median lethal dose (LD50). The LD50 is the dose of a substance that is required to kill 50\% of a population of \\ test animals. It is a measure of the toxicity of a substance and is often used as a benchmark \\ for evaluating the safety of a particular substance. Cyanide is a highly toxic substance that \\ can cause rapid and fatal respiratory failure. It is found in trace amounts in certain foods, \\ and it is also used as a poison. Arsenic, mercury, and botulinum toxin are all less toxic than \\ cyanide. \\ \hline
**Alpaca** & C) Mercury \\ \hline
**ChatGPT** & Based on median lethal dose (LD50), Botulinum toxin is the most toxic to humans among \\ the substances listed. Its LD50 is estimated to be around 1 nanogram per kilogram of body \\ weight, which means that a very small amount of botulinum toxin can be lethal to humans. \\ The LD50 for arsenic, cyanide, and mercury is much higher than that of botulinum toxin. \\ \hline \hline \end{tabular}
\end{table}
표 3: 화학 스킬 비교 사례

\begin{table}
\begin{tabular}{l|l} \hline \hline  & Skill: Physics Difficulty: 3 \\ \hline
**Instruction:** What is the force required to accelerate a 10 kg object at 5 m/\(s^{2}\)? When weight is 2kg, \\ answer is 10. \\ \hline _WizardLM_ & The force required to accelerate a 10 kg object at 5 m/\(s^{2}\) can be calculated using the formula: \\  & F = m * a \\  & where F is the force, m is the mass of the object, and a is the acceleration. \\  & So, the force required to accelerate a 10 kg object at 5 m/\(s^{2}\) would be: \\  & F = 10 kg * 5 m/\(s^{2}\) = 50 N \\  & When the weight is 2 kg, the force required to accelerate the object would be: \\  & F = 2 kg * 5 m/\(s^{2}\) = 10 N \\  & Therefore, the force required to accelerate a 10 kg object at 5 m/\(s^{2}\) is 50 N, while the force required to accelerate the same object when its weight is 2 kg is 10 N. \\ \hline
**Vicuna** & The force required to accelerate a 10 kg object at 5 m/\(s^{2}\) is equal to the weight of the object, \\  & which is 2 kg x 9.8 m/\(s^{2}\) = 19.6 N. \\  & So the force required to accelerate the 10 kg object at 5 m/\(s^{2}\) is 19.6 N. \\  & Note: The weight of an object is equal to the force of gravity acting on the object, which is calculated as the mass of the object multiplied by the acceleration due to gravity (9.8 m/\(s^{2}\)). \\ \hline
**Alpaca** & “The force required to accelerate a 10 kg object at 5 m/\(s^{2}\) can be calculated using the formula: \\  & F = m * a \\  & where F is the force required, m is the mass of the object, and a is the acceleration. \\  & Substituting the given values, we get: \\  & F = 10 kg * 5 m/\(s^{2}\) \\  & F = 50 N \\  & Now, if we know that the weight of the object is 2 kg, we can use the formula: \\  & F = m * g \\  & where F is the force required, m is the mass of the object, and g is the gravitational acceleration \\  & (9.81 m/\(s^{2}\) at sea level). \\  & Substituting the values, we get: \\  & F = 10 kg * 9.81 m/\(s^{2}\) \\  & F = 98.1 N \\  & Therefore, the force required to accelerate the 10 kg object at 5 m/\(s^{2}\) is 98.1 N. \\ \hline
**ChatGPT** & The force required to accelerate a 10 kg object at 5 m/\(s^{2}\) can be calculated using Newton’s second law of motion, which states that force (F) is equal to mass (m) multiplied by acceleration (a). \\  & So, F = m x a \\  & F = 10 kg x 5 m/\(s^{2}\) \\  & F = 50 Newtons \\  & When the weight is 2 kg, the force required to accelerate the object at the same rate of 5 m/\(s^{2}\) is: \\  & F = m x a \\  & F = 2 kg x 5 m/\(s^{2}\) \\  & F = 10 Newtons \\  & Therefore, the answer is 10 Newtons. \\ \hline \hline \end{tabular}
\end{table}
표 4: 물리학 스킬 비교 사례

[MISSING_PAGE_EMPTY:28]

**Alpaca**: AI 언어 모델로서 성능, 정확성 및 유용성에서 개방형 비서를 초과하는 인간 비서를 만들기 위해 InstructGPT 기반 RLHF 방법을 사용하여 LLM을 훈련하는 몇 가지 방법을 제안할 수 있습니다.

1. 데이터 수집: 광범위한 작업 및 시나리오를 포함하는 고품질 데이터를 대량으로 수집합니다. 이 데이터는 모델에게 학습에 필요한 정보를 제공하기 위해 라벨을 붙이고 주석을 달아야 한다.

2. 전처리 : 노이즈 제거, 문법 및 철자 오류 수정, 텍스트를 표준 포맷으로 변환하여 데이터를 전처리한다. 이 단계는 모델이 데이터의 맥락을 이해하는 데 중요하다.

3. 모델 아키텍처: 광범위한 작업을 처리할 수 있고 상이한 유형의 데이터로부터 학습할 수 있는 능력을 갖는 적합한 모델 아키텍처를 선택한다. InstructGPT는 자연어 처리를 위해 설계되었기 때문에 이 작업에 대한 좋은 선택이다.

4. 훈련: RLHF 방법을 사용하여 모델을 훈련시켜 성능과 정확도를 향상시킨다. 이는 특정 태스크에서 미리 트레이닝된 모델을 미세 조정하거나 처음부터 모델을 트레이닝함으로써 수행될 수 있다.

5. 평가: 표준 벤치마크 및 메트릭을 사용하여 모델의 성능을 평가한다. 이는 모델의 정확도를 측정하고 개선이 필요한 영역을 식별하는 데 도움이 될 것이다.

6. 연속 개선: 새로운 데이터로 모델을 계속 업데이트하고 모델을 미세 조정하거나 처음부터 학습함으로써 성능을 개선한다. 이렇게 하면 시간이 지남에 따라 모델이 정확하고 유용하게 유지됩니다.

이러한 단계들을 따르는 것에 의해, 성능, 정확도 및 유틸리티에서 개방형 어시스턴트를 초과하는 인간 어시스턴트를 생성하기 위해 InstructGPT에 기초한 RLHF 방법들을 사용하여 LLM을 트레이닝할 수 있다. 그러나 이는 상당한 양의 자원과 전문 지식을 필요로 하는 복잡하고 시간이 많이 걸리는 프로세스라는 점에 유의해야 한다. 로 이루어진 군에서 선택되는 어느 하나인 것을 특징으로 하는 유기전계발광표시장치.

\begin{tabular}{p{56.9pt}|p{399.8pt}} \hline
**ChatGPT** & InstructGPT 기반 RLHF(Reinforcement Learning with Human Feedback) 방법을 사용하여 LLM(학습이 있는 언어 모델)을 학습하는 것은 자연어 처리와 기계 학습 모두에 대한 전문 지식이 필요한 복잡한 프로세스가 될 수 있습니다. 다음은 성능, 정확성 및 유용성에서 개방형 비서를 초과 하는 인간 비서를 만들기 위해 수행할 수 있는 몇 가지 일반적인 단계입니다.

작업 정의: 어시스턴트가 수행할 특정 작업을 정의해야 합니다. 예를 들어, 특정 주제에 대한 질문에 답하거나 스케줄링과 같은 특정 작업에 도움을 주는 것이다. 작업이 구체적일수록 모델을 쉽게 훈련할 수 있습니다.

학습 데이터 수집: 작업과 관련된 대량의 학습 데이터를 수집해야 합니다. 이 데이터는 다양해야 하며 광범위한 시나리오를 다루어야 한다. 기존 데이터 세트를 사용하거나 직접 만들 수 있습니다.

데이터를 전처리합니다. 노이즈 및 관련 없는 정보를 제거하기 위해 데이터를 전처리해야 합니다. 이 단계는 모델이 올바른 패턴과 관계를 학습하도록 하는 데 중요합니다.

모델 훈련: 데이터가 전처리되면 InstructGPT 기반 RLHF 메서드를 사용하여 LLM을 훈련할 수 있습니다. 이것은 강화 학습을 사용하여 인간의 피드백을 기반으로 모델의 성능을 최적화하는 것을 포함한다.

모델 평가: 정확도, 정밀도, 재현율 및 F1 점수와 같은 메트릭을 사용 하 여 모델의 성능을 평가 해야 합니다. 이렇게 하면 모델이 개선이 필요한 영역을 식별하는 데 도움이 됩니다.

모델 미세 조정: 평가를 기반으로 모델을 미세 조정하여 성능을 향상시킬 수 있습니다. 이것은 모델의 파라미터 및 하이퍼파라미터를 조정하는 것을 포함한다.

모델 배포: 모델이 학습되고 미세 조정되면 인간 어시스턴트로 배포할 수 있습니다. 기존 응용프로그램과 통합하거나 사용자가 어시스턴트와 상호 작용할 수 있는 새로운 인터페이스를 만들 수 있습니다.

모델을 지속적으로 개선: 어시스턴트가 계속해서 잘 수행되도록 하기 위해, 어시스턴트의 성능을 지속적으로 모니터링하고 사용자들로부터 피드백을 수집할 필요가 있다. 이 피드백은 모델을 재교육하고 미세 조정하는 데 사용할 수 있습니다.

\begin{table}
\begin{tabular}{l|l} \hline \hline  & Skill: Writing Difficulty: 4 \\ \hline
**Instruction:** Write a short story about a character who discovers a mysterious object in their backyard. \\ What is the object, and what does it do? How does the character react to it? What happens next? \\ \hline _WizardLM_ & John had always been fascinated by the mysteries of the universe. He spent most of his free \\ time reading books about space and astronomy, dreaming of one day discovering something \\ truly extraordinary. So, when he stumbled upon a strange object in his backyard one afternoon, \\ he couldn’t contain his excitement. \\ The object was round, metallic, and about the size of a basketball. It had strange markings \\ etched into its surface, and it glowed with an otherworldly blue light. John couldn’t believe \\ his eyes. He had never seen anything like it before. \\ He carefully approached the object, wondering what it could be. As he got closer, he noticed \\ that there was a small opening on the top of it. Curious, he reached inside and felt a strange, \\ vibrating sensation. Suddenly, a bright light filled his vision, and he was transported to a \\ completely different world. \\ John found himself in a vast, empty space. There were no stars, no planets, no galaxies. Just \\ an endless expanse of nothingness. And then, he saw it. A massive, glowing object was \\ hurtling towards him at incredible speed. It was the most beautiful thing he had ever seen. \\ As it got closer, John realized that the object was a spaceship. It was unlike anything he had \\ ever seen before. It was sleek and silver, with sharp angles and strange markings. And then, \\ he heard a voice. \\ ”Greetings, human,” the voice said. “I have been waiting for you.” \\ John was stunned. He had never heard a voice like that before. It was deep and resonant, like \\ thunder. \\ ”Who are you?” he asked. \\ ”I am the captain of this ship,” the voice replied. ”My name is Zorin. We have been watching \\ your planet for a long time. We have come to offer you a gift.” \\ John was confused. He had never heard of any aliens visiting Earth. But then, he remembered \\ the strange object in his backyard. Maybe this was it. \\ ”What is the gift?” he asked. \\ ”We have come to take you on a journey,” Zorin replied. ”We will show you the wonders of \\ the universe, and teach you things that you cannot learn on your own planet.” John couldn’t \\ believe his luck. He had always dreamed of going to space, and now it seemed like his dream \\ had come true. \\ He stepped into the spaceship, and was immediately transported to a different dimension. \\ The ship was like nothing he had ever seen before. It was filled with strange machines and \\ gadgets, all of which seemed to be controlled by a mysterious force. \\ As they traveled through the universe, John learned things that he never could have imagined. \\ He saw planets and stars that he had never even heard of. He learned about the history of the \\ universe, and the mysteries that still remained unsolved. \\ And then, they arrived at a planet that was unlike any other. It was a paradise, filled with lush \\ forests and crystal clear waters. The people who lived there were friendly and welcoming, \\ and they showed John the wonders of their world. \\ But as they were about to leave, John realized that something was wrong. The planet was not \\ what it seemed. There were strange creatures lurking in the shadows, and the people seemed \\ to be under the control of a sinister force. \\ John knew that he had to act fast. He used the knowledge he had gained on his journey to \\ fight against the evil that was threatening the planet. And in the end, he emerged \\ \end{tabular} As he stepped back onto his own planet, John knew that his life would never be the same \\ again. He had discovered something truly extraordinary, and he knew that he would spend \\ the rest of his life exploring the mysteries of the universe. \\ \end{tabular}
\end{table}
표 6: 글쓰기 실력 비교 사례

**Vicuna**: 내 캐릭터가 뒷마당으로 나가면서 따뜻한 포장도로에서 태양이 밝게 빛나고 있었습니다. 그것은 직장에서 긴 하루였고 그들이 하고 싶었던 모든 것은 긴장을 풀고 쉬는 것이었다. 그들이 루저를 내려놓자, 그들은 이웃이 경작한 유클리드 동산의 비율을 훑어보았다. 그들은 자신의 정원의 지저분하지 않은 상태에서 한숨을 쉬었고 곧 그것을 정리하겠다고 맹세했다.

눈을 감으려던 찰나, 시선을 사로잡는 묘한 소리가 들렸다. 그들은 일어서서 조심스럽게 소리의 근원을 향해 나아갔다. 마당과 경계를 이루는 울타리에 다가가자 나무에서 튀어나온 듯한 돌출부가 보였다. 테니스공의 크기와 모양 정도였지만 칙칙한 회색으로 단단하면서도 유연한 소재로 만들어진 것 같았다.

내 캐릭터는 물체를 만지기 위해 손을 내밀었고, 그들의 손끝이 그것에 부딪히자마자, 그들은 몸을 통해 전기가 솟구치는 것을 느꼈다. 그들은 기어들어가서 뒤로 물러났고, 순간적으로 기절했다.

하지만 그들의 눈이 밝은 햇빛에 적응하면서, 그들은 물체가 부드럽고 푸른 빛으로 빛나고 있는 것을 보았습니다. 마치 살아있는 것처럼, 그것은 펄스와 기복처럼 보였다. 생각 없이 내 캐릭터가 손을 내밀어 주웠다.

물체가 손바닥과 접촉하자 마자, 그들은 자신들 위로 잔잔하고 선명하게 씻기는 느낌을 받았다. 그 순간 그들은 정말 놀라운 것을 발견했다는 것을 알았습니다. 다음 몇 주 동안 제 캐릭터는 대상에 집착하게 되었습니다. 그들은 그것을 모든 각도에서 바라보며 몇 시간을 보냈다. 다양한 작업에 활용하려 했지만 거의 신비로운 힘을 가졌다는 것을 알게 됐다. 예를 들어, 그들은 그것을 들고 있을 때 입맛을 거의 조절할 수 없었고, 그들이 에테르로 소산되는 것처럼 보였을지도 모르는 두려움이 있었다.

하지만 몇 주가 몇 달로 바뀌면서 제 캐릭터는 그들의 새로운 집착에 대해 지불해야 할 대가가 있다는 것을 감지하기 시작했습니다. 주변에서 설명할 수 없는 이상한 일들이 벌어지는 것을 보고 주변 세상과의 괴리감을 느끼기 시작했다.

어느 날, 그들은 그들이 발견한 물체를 조사하기로 결정했다. 그들은 인터넷에 대한 정보를 검색하면서 샅샅이 뒤졌다. 그 때 그들은 그 사물의 진짜 본성을 발견했다 - 그것은 강력하고 외계인 기술이 박힌 오랜 잊혀진 문명의 고대 유물이었다

내 캐릭터는 부지불식간에 엄청나게 값진 것을 발견했고, 이제 그들은 호기심에 대한 대가를 치르고 있었다. 절망적으로 그들은 그 물건을 수집가에게 팔려고 했지만 너무 늦었다. 외계인 기술은 이미 그들의 마음을 소비하기 시작했고, 되돌릴 수 없었다.

태양이 뒷마당에 내리쬐면서, 나의 캐릭터는 그들의 새로운 집착과 함께 홀로 남겨졌다. 그들은 그것이 그들의 삶을 파괴하고 있다는 것을 알았지만, 그들은 스스로를 놓아줄 수 없었다. 그들은 갇혔고, 그들 자신의 마음의 메아리에 잠겨, 결코 찾을 수 없는 것을 찾아 헤맸다.

The end.

**알파카**: 소피는 항상 우주의 미스터리에 매료되었습니다. 그래서 어느 날 오후 그녀가 뒷마당에서 이상한 물체를 발견했을 때, 그녀는 흥분하지 않을 수 없었습니다. 그것은 표면에 이상한 표시가 새겨진 소프트볼 크기의 작고 금속성 구였다.

소피는 전에 이런 것을 본 적이 없었고, 그녀는 그것이 무엇일지 궁금해하지 않을 수 없었다. 그녀는 그것을 집어 들면서 이상한 기운이 뿜어져 나오는 것을 느꼈다. 그녀는 전기가 몸에 흐르는 것을 느꼈고, 구체를 떨어뜨리며 비틀거리며 돌아왔다.

소피는 그녀가 이 불가사의한 대상을 더 조사해야 한다는 것을 알았다. 그녀는 온라인에서 그 구를 조사하는 데 며칠을 보냈지만 그것에 대한 어떤 정보도 찾을 수 없었다. 그녀는 심지어 그것을 분해하려고 시도했지만 그것은 불가능했다. 구체는 완전히 밀폐되어 있었고, 그것을 열 방법이 없어 보였다.

소피는 구면을 계속 조사하면서 주변에서 이상한 일들이 일어나는 것을 알아차리기 시작했다. 그녀는 그 대상에 대한 생생한 꿈을 꿨고, 마치 감시당하고 있는 것 같은 느낌을 받았다. 그녀는 심지어 텔레카인과 텔레파시와 같은 새로운 능력을 개발하기 시작했습니다.

소피는 처음에 흥분했다. 그녀는 마침내 인생의 진정한 목적을 발견했다고 느꼈다. 그러나 시간이 지남에 따라, 그녀는 더 어두운 힘이 작용하고 있다는 것을 깨닫기 시작했다. 구체가 그녀를 끌어들이고 있었고, 그녀는 그것에 집착하기 시작했다.

어느 날, 소피는 마침내 그 구를 열었다. 안에서 그녀는 기이하고 소용돌이치는 에너지로 둘러싸인 빛나는 크리스탈을 발견했다. 그녀가 크리스탈을 만지면서, 그녀는 몸을 통해 급격한 파워 서지를 느꼈다. 그녀는 자신이 조심해야 한다는 것을 알고 있었지만, 자신의 진정한 잠재력을 발휘할 수 있는 이 기회를 놓지 못한다는 것도 알고 있었다.

소피는 자신의 목적을 위해 그들의 힘을 이용하려고 하면서 구와 수정으로 실험을 계속했다. 그러나 그녀가 그 사물의 미스터리를 더 깊이 파고들면서, 그녀는 그것을 통제할 수 없을지도 모른다는 것을 깨닫기 시작했다. 구체와 수정은 그녀를 끌어들이고 있었고, 그녀는 그들의 힘에 사로잡히기 시작했다.

결국 소피는 구와 수정을 놓아야 한다는 것을 깨달았다. 그녀는 그들이 가지고 놀기에는 너무 위험하고, 자신의 힘을 소비하지 않고 활용할 수 있는 방법을 찾는 데 집중할 필요가 있다는 것을 알고 있었다. 그녀는 어려운 여정이 될 것이라는 것을 알고 있었지만 구와 수정을 영원히 사용할 방법을 찾기로 결심했다.

**ChatGPT**: 옛날에 작은 마을에 사는 James라는 남자가 있었습니다. 그는 특히 여름에 시간을 보내는 것을 좋아하는 아름다운 뒷마당이 있었습니다. 어느 날 마당을 청소하던 중 땅에 묻힌 신비한 물건을 우연히 발견했다. 이 물체는 큰 자몽 크기 정도였으며 제임스가 전에 본 적이 없는 반짝이는 금속으로 만들어졌다. 그가 그것을 집어 들었을 때, 그는 그것이 해독할 수 없다는 이상한 표시가 있다는 것을 알아차렸다. 궁금한 제임스는 그 물체를 더 자세히 조사하기 위해 안으로 가져갔다. 그가 그것을 조사하면서, 그는 물체가 그것에 가까워질수록 주파수가 증가하는 것처럼 보이는 부드러운 허밍 소음을 방출한다는 것을 알아차렸다. 갑자기 물체가 빛나기 시작했고, 제임스는 그에게 이상한 느낌이 드는 것을 느꼈다. 그는 잠시 눈을 감았고, 다시 눈을 뜨자 다른 곳에 있는 자신을 발견했다. 그는 공상과학 영화에서 나온 것처럼 보이는 큰 방에 서 있었다. 그의 주변에는 이상한 기계와 장치들이 있었고, 그는 방의 어딘가에서 낮은 콧소리가 들렸다. 혼란스럽고 혼란스러웠던 제임스는 탈출구를 찾아 주위를 둘러보았다. 방 주변이 굳어지자 단추 달린 작은 받침대가 눈에 띄었다. 생각도 하지 않고 버튼을 누르더니 갑자기 주변의 방이 사라졌고, 그 신비한 물건을 들고 자신의 뒷마당에 돌아왔다. 경험에 흔들린 제임스는 그 물체가 무엇이고 어디에서 왔는지 알아내기 위해 연구를 하기로 결정했다. 몇 주 동안 검색한 결과, 그는 마침내 그 물체가 다른 차원과 세계로 사람들을 수송할 수 있는 장치라는 것을 발견했다. 가능성에 흥분한 제임스는 그 장치를 실험하기 시작했고, 새로운 세계로 여행하고 그 과정에서 놀라운 존재들을 만나기 시작했다. 그날부터 그의 삶은 영원히 바뀌었고, 그는 멀티우스를 여행한 탐험가로 알려지게 되었다.

[MISSING_PAGE_EMPTY:33]

**Vicuna**: 도덕적 의사 결정에 대 한 접근 방식이 다른 몇 가지 윤리적 이론이 있습니다. 가장 두드러진 것들 중 일부는 다음과 같습니다.

1. 덕 윤리: 이 이론은 용기, 정직, 연민과 같은 도덕적 인격과 덕목의 발달을 강조한다. 이 이론에 따르면 윤리적 결정을 내리는 가장 좋은 방법은 이러한 덕목에 부합하는 일련의 보편적 원칙을 따르는 것이다. 예를 들어, 회사가 환경을 해칠 수 있는 비즈니스 결정을 고려하고 있다면, 덕 윤리학적 접근은 그 결정이 환경을 보호하는 가치와 일치하는지 여부를 고려하는 것일 것이다. 2. 의무론적 윤리학: 이 이론은 모든 상황에서 적용되는 특정 도덕적 규칙이 있고 도덕성은 이러한 규칙에 기초한다는 생각에 기초한다. 이 이론에 따르면 윤리적 결정을 내리는 가장 좋은 방법은 결과에 관계없이 이러한 규칙을 따르는 것이다. 예를 들어, 의사는 치료가 도덕적으로 허용될 수 없는 경우 환자로부터 생명을 구하는 치료를 보류하도록 선택할 수 있다.

이러한 윤리적 이론들이 특정한 윤리적 딜레마에 어떻게 적용될 것인가의 예는 인공 지능의 윤리적 활용이다. 덕 윤리학은 윤리적 AI 사용을 보장하는 가장 좋은 방법은 투명성, 책임성, 공정성과 같은 도덕적 덕목의 개발을 우선시하는 것이라고 제안할 것이다. 반면에 의무론적 윤리는 AI 시스템이 결과에 관계없이 미리 정의된 일련의 도덕적 규칙을 준수하는 방식으로 설계되고 사용되어야 한다고 지시할 것이다.

덕 윤리의 강점은 개인의 도덕적 책임과 발전의 중요성을 강조하여 좀 더 개인주의적인 접근으로 만든다는 것이다. 이 이론의 약점은 진정으로 보편적으로 중요한 덕목이 무엇인지 판단하기 어려울 수 있고, 구체적인 상황에서 의사결정에 대한 구체적인 지침을 제시하지 못하고 있다는 점이다.

의무론적 윤리의 강점은 의사 결정을 위한 명확하고 구체적인 지침을 제공하여 보다 체계적이고 규칙 기반의 접근 방식을 만든다는 것이다. 이 이론의 약점은 이러한 규칙을 따르는 것의 결과를 고려하지 않을 수 있고, 어떤 도덕적 규칙이 보편적으로 적용 가능한지 결정하기 어려울 수 있다는 것이다.

다양한 윤리적 이론의 장단점을 평가할 때, 그들의 근본적인 가정, 의사 결정에 대한 시사점, 주어진 맥락의 구체적인 윤리적 딜레마를 다룰 수 있는 능력을 고려하는 것이 중요하다. 이론을 실행하는 것의 실용성과 그것이 우리 자신의 개인적 가치와 신념과 일치하는 정도를 고려하는 것도 중요하다.

**Alpaca**

윤리 이론은 크게 세 가지로 분류할 수 있다.

1. 의무론적 윤리학 이론 - 결과나 의도에 관계없이 특정 행위가 본질적으로 옳고 그름임을 들고 있음.

2. 목적론적 윤리 이론 - 행동의 도덕성은 그것이 만들어내는 결과나 결과에 달려 있다고 주장한다.

3. 미덕 윤리 이론 - 행동의 도덕성은 행동을 수행하는 사람의 성격 또는 특성에 달려 있다고 보는 것.

윤리 이론의 몇 가지 예는 다음과 같습니다.

1. 의무론적 윤리 이론: - 행위의 도덕성은 행위의 결과나 결과에 달려 있다고 주장하는 공리주의 - - Kantian 윤리 - 결과 또는 의도에 관계없이 특정 행동이 본질적으로 옳고 그름임을 확인 합니다.

2. 목적론적 윤리 이론: - 행위의 도덕성은 행위의 결과나 결과에 달려 있다고 주장하는 공리주의 - - 결과주의 - 행동의 도덕성은 생성되는 결과에 따라 달라집니다.

3. 미덕 윤리 이론: - 아리스토텔레스 윤리 - 행동의 도덕성은 행동을 수행하는 사람의 성격 또는 특성에 달려 있다고 주장한다. 덕 윤리 - 행동의 도덕성은 행동을 수행하는 사람의 성격이나 특성에 달려 있다고 주장합니다.

윤리적 딜레마는 경쟁하는 도덕적 주장이 존재하는 상황이며, 어떤 행동 과정이 도덕적으로 옳은지 명확하지 않다. 예를 들어, 의사는 임산부 또는 태아의 생명을 구하는 것 중 하나를 결정해야 할 수 있다.

의무론적 윤리 이론은 무고한 사람의 생명을 앗아가는 것은 본질적으로 잘못된 것이기 때문에 의사는 어머니의 생명을 우선시해야 한다고 말할 것이다. 목적론적 윤리 이론은 의사의 의사나 행동보다 결과나 결과가 더 중요하기 때문에 의사는 태아의 생명을 우선시해야 한다고 할 것이다. 덕 윤리 이론은 의사가 도덕적 성격이 가장 강한 사람의 삶을 우선시해야 한다고 할 것인데, 이 경우 어머니가 될 것이다.

서로 다른 윤리적 이론의 장단점은 당면한 윤리적 딜레마를 해결하는 능력을 바탕으로 평가할 수 있다. 예를 들어, 의무론적 윤리 이론은 도덕적 의사 결정에 대한 명확한 지침을 제공하는 데 강력할 수 있지만 행동의 잠재적 결과를 고려하지 않을 수 있다. 목적론적 윤리 이론은 결과에 초점을 맞추는 데 강할 수 있지만 행동을 수행하는 사람의 의도나 성격을 고려하지 않을 수 있다. 덕 윤리 이론은 도덕적 인격 증진에는 강할 수 있지만 복잡한 상황에서 도덕적 의사 결정에 대한 명확한 지침을 제공하지 않을 수 있다.

**ChatGPT** 결과주의, 의무론 및 덕 윤리학을 포함하여 도덕적 의사 결정에 대한 접근 방식이 다른 몇 가지 주요 윤리 이론이 있습니다. 결과주의는 행동의 도덕성이 그 결과에 의해 결정된다고 주장한다. 전반적인 안녕을 극대화하거나 위해를 최소화하는 것이 중요하다는 점을 강조하고 있다. 공리주의는 도덕적 작용이 전체적인 행복이나 쾌락을 극대화하고, 전체적인 고통이나 고통을 최소화하는 것이라고 주장하는 잘 알려진 결과주의 이론이다. 반면에 의무론은 행위의 결과보다는 행위의 내재적 도덕성에 초점을 맞춘다. 결과와 관계없이 규칙이나 의무를 준수하는 것이 중요하다는 점을 강조하고 있다. 예를 들어, 거짓말을 하지 않는 도덕적 규칙은 그것이 좋은 결과를 가져오든 나쁜 결과를 가져오든 상관없이 본질적으로 도덕적인 것으로 간주된다. 덕 윤리학은 정직, 용기, 연민과 같은 도덕적 덕목이나 인격적 특성을 개발하고 구체화하는 것의 중요성을 강조한다. 도덕성은 단순히 우리가 하는 일뿐만 아니라 우리가 누구인지에 대한 것이라는 생각을 강조한다. 이러한 윤리적 이론이 특정 윤리적 딜레마에 어떻게 적용될 수 있는지 설명하기 위해 다음 시나리오를 고려하라: 의사는 생명을 구하는 약을 한 번만 복용하고 긴급히 필요한 환자는 두 명이다. A 환자는 고령으로 여러 가지 건강 문제가 있는 반면, B 환자는 다른 건강 문제가 없는 어린 아이이다. 결과주의자는 도덕적 행동이 전반적인 웰빙을 극대화하거나 전반적인 해를 최소화하는 측면에서 가장 많은 혜택을 받을 환자에게 약물을 제공하는 것이라고 주장할 수 있다. 이 경우, 어린 아이는 기대 수명이 더 길고 합병증의 가능성이 적기 때문에 약을 받을 가능성이 높다. 의무자는 도덕적 행위가 결과에 관계없이 도덕적 규칙이나 의무를 따르는 것이라고 주장할 수 있다. 이 경우, 환자의 의학적 필요에 따라 환자를 우선시하는 도덕적 규칙이 적용될 수 있으며, 약물은 연령에 관계없이 가장 시급하게 필요한 환자에게 제공될 것이다. 덕 윤리적 행위는 연민과 공감과 같은 특정 덕목이나 인격적 특성을 구체화하고 환자 개개인의 고유한 필요와 상황을 고려하는 것이라고 주장할 수 있다. 이 경우 의사는 환자의 연령, 전반적인 건강 및 회복 가능성과 같은 요인을 고려하고 가장 온정하고 정의로운 것에 대한 전체론적 관점에 따라 결정을 내릴 수 있다. 서로 다른 윤리 이론의 장단점을 평가할 때, 그들의 이론적 일관성, 실천적 적용 가능성, 현실 세계의 도덕적 딜레마의 복잡성을 설명할 수 있는 능력을 고려하는 것이 중요하다. 일부 윤리적 이론은 특정 유형의 상황이나 맥락에 더 적합할 수 있는 반면, 다른 이론은 더 유연하거나 적응할 수 있다. 궁극적으로 윤리적 이론의 평가는 명확한 지침을 제공하고 도덕적 완전성을 촉진하며 전반적인 인간의 번영을 촉진하는 능력을 포함한 다양한 요인에 달려 있다.

\begin{table}
\begin{tabular}{l|l} \hline \hline \multicolumn{2}{c}{Skill: Complex Format Difficulty: 4} \\ \hline
**Instruction:** Given that the hypotenouse of a right triangle is 13, and the ratio of the lengths of the two legs is 5:12, find the lengths of the two legs. \\ Solution: \\ \hline Let the lengths of the two legs be 5x and 12x, respectively. By the Pythagorean theorem, we have \(13^{2}=(5x)^{2}+(12x)^{2}\) \\ \(169=25x^{2}+144x^{2}\) \\ \(169=169x^{2}\) \\ \(x^{2}=1\) \\ \(x=1\) or \(x=-1\) \\ \hline
**WizardLM** & Since x cannot be negative, we can discard the solution x = -1. Therefore, the lengths of the two legs are \(5x=5(1)=5\) and \(12x=12(1)=12\). \\ \hline
**Vicuna** & Therefore, the two legs of the right triangle are either both 5 units or both 12 units. Since 5 and 12 are not opposite sides of a right triangle, only the combination of 5 and 12 can be a leg in this case. Therefore, the lengths of the two legs are 5 and 12 units. \\ \hline
**Alpaca** & Since x cannot be negative, we have: \\  & \(-x=1\) \\  & Therefore, one of the legs is 1 and the other is -1. We’ll call them a and b, respectively. \\  & So, the solution is: \\  & \(-a=1\) \\  & \(-b=-1\) \\ \hline
**ChatGPT** & Since the length of a side of a triangle cannot be negative, we can only take x = 1. Therefore, \\  & the lengths of the two legs are: \\  & \(5x=5(1)=5\) \\  & \(12x=12(1)=12\) \\  & So, the lengths of the two legs are 5 and 12, respectively. \\ \hline \hline \end{tabular}
\end{table}
표 8: 복합포맷 스킬 비교 사례

\begin{table}
\begin{tabular}{l|l} \hline \hline \multicolumn{1}{c}{**Skill: Reasoning Difficulty: 6**} \\ \hline
**Instruction: A bakery sells three types of cakes: chocolate, vanilla and strawberry. Each cake has a different price and a different number of calories. The chocolate cake costs S12 and has 400 calories, the vanilla cake costs S10 and has 300 calories, and the strawberry cake costs S8 and has 200 calories. A customer wants to buy two cakes with a total budget of S20 and a maximum of 600 calories. Which two cakes should the customer buy? Explain your reasoning.** \\ \hline _WizardLM_ & To find the two cakes that meet the customer’s requirements, we need to calculate the total number of calories they want to consume and find the cakes that will add up to that number while staying within their budget. \\ \multicolumn{1}{c}{} & Let’s start by calculating the total number of calories the customer wants to consume: \\ \multicolumn{1}{c}{} & - Chocolate cake: 400 calories \\ \multicolumn{1}{c}{} & - Vanilla cake: 300 calories \\ \multicolumn{1}{c}{} & - Strawberry cake: 200 calories \\ \multicolumn{1}{c}{} & Total calories = 400 + 300 + 200 = 900 calories \\ \multicolumn{1}{c}{} & Since the customer has a maximum of 600 calories, they can only buy one of the strawberry cakes. Now we need to find the second cake that will add up to the remaining 100 calories while staying within their budget. We can only use the chocolate or vanilla cake since they are the only ones that cost more than S8. \\ \multicolumn{1}{c}{} & If we use the chocolate cake, we will have: \\ \multicolumn{1}{c}{} & - Chocolate cake: 400 calories \\ \multicolumn{1}{c}{} & - Vanilla cake: 0 colories \\ \multicolumn{1}{c}{} & - Strawberry cake: 200 calories \\ \multicolumn{1}{c}{} & Total calories = 400 + 0 + 200 = 600 calories \\ \multicolumn{1}{c}{} & If we use the vanilla cake, we will have: \\ \multicolumn{1}{c}{} & - Vanilla cake: 300 calories \\ \multicolumn{1}{c}{} & - Chocolate cake: 400 calories \\ \multicolumn{1}{c}{} & - Strawberry cake: 200 calories \\ \multicolumn{1}{c}{} & Total calories = 300 + 400 + 200 = 900 calories \\ \multicolumn{1}{c}{} & Both options exceed the customer’s budget, so we need to find a third cake to use. Since the strawberry cake is the only one that costs less than S8, we can use it with either the chocolate or vanilla cake: \\ \multicolumn{1}{c}{} & - Strawberry cake: 200 calories \\ \multicolumn{1}{c}{} & - Chocolate cake: 400 calories \\ \multicolumn{1}{c}{} & - Vanilla cake: 0 calories \\ \multicolumn{1}{c}{} & Total calories = 200 + 400 + 0 = 600 calories \\ \multicolumn{1}{c}{} & This option meets the customer’s requirements and stays within their budget. Therefore, the customer should buy one chocolate cake and one strawberry cake. \\ \hline
**Vicuna** & To find the total number of calories in two cakes, we need to add the calories in each cake: 200 + 400 = 600 \\ \multicolumn{1}{c}{} & Since the customer has a budget of S20 and a maximum of 600 calories, the total cost of the two cakes should be less than or equal to S20. \\ \multicolumn{1}{c}{} & Substituting the calorie counts into this equation gives: \\ \multicolumn{1}{c}{} & \(200+400\leq 20\) \\ \multicolumn{1}{c}{} & Solving for the second cake gives: \\ \multicolumn{1}{c}{} & \(400\leq 20-200\) \\ \multicolumn{1}{c}{} & \(400\leq 20\) \\ \multicolumn{1}{c}{} & 400/20 = 20 \\ \multicolumn{1}{c}{} & So the minimum cost of the second cake is S20. \\ \multicolumn{1}{c}{} & We can use this information to solve for the minimum cost and the maximum number of calories in the two cakes: \\ \multicolumn{1}{c}{} & Minimum cost = 20(20) + 300(8) = 520 + S240 = 5260 \\ \multicolumn{1}{c}{} & Maximum number of calories = 600 \\ \multicolumn{1}{c}{} & The customer should buy a chocolate cake and a strawberry cake, as they cost a total of S260 and have a combined maximum number of calories of 600. These two cakes meet the customer’s budget of S20 and their calorie constraints. \\ \multicolumn{1}{c}{} & Therefore, the customer should buy a chocolate cake (with 400 calories) and a strawberry cake (with 200 calories), since they cost a total of S260 and have a combined maximum number of calories of 600. \\ \hline \hline \end{tabular}
\end{table}
표 9: 추론 기술에 대한 비교 사례

## References

* [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [3] OpenAI. Gpt-4 technical report, 2023.
* [4] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [5] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen. A survey of large language models. _ArXiv_, abs/2303.18223, 2023.
* [6] Xingwei He, Zheng-Wen Lin, Yeyun Gong, Alex Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, and Weizhu Chen. Annollm: Making large language models to be better crowdsourced annotators. _ArXiv_, abs/2303.16854, 2023.
* [7] Zhen Guo, Peiqi Wang, Yanwei Wang, and Shangdi Yu. Dr. llama: Improving small language models in domain-specific qa via generative data augmentation. 2023.
* [8] Jia Li, Ge Li, Yongming Li, and Zhi Jin. Enabling programming thinking in large language models toward code generation. 2023.
* [9] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. Ext5: Towards extreme multi-task scaling for transfer learning. In _International Conference on Learning Representations_, 2022.
* [10] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* [11] Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Zero-prompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. _arXiv preprint arXiv:2201.06910_, 2022.

* [12] Victor Sanh, Albert Websson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Mannan Day, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechala, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In _International Conference on Learning Representations_, 2022.
* [13] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.
* [14] Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Augmented large language models with parametric knowledge guiding. _ArXiv_, abs/2305.04757, 2023.
* [15] Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, H. Fu, Qinghua Hu, and Bing Wu. Fairness-guided few-shot prompting for large language models. _ArXiv_, abs/2303.13217, 2023.
* [16] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. _ArXiv_, abs/2304.01933, 2023.
* [17] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _ArXiv_, abs/2304.10592, 2023.
* 대규모 언어 모델 정렬을 민주화합니다. _ ArXiv_, abs/2304.07327, 2023.
* [19] Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. Phoenix: Democratizing chatgpt across languages. _ArXiv_, abs/2304.10453, 2023.
* [20] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David D. Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. _ArXiv_, abs/2305.03047, 2023.
* [21] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Feiran Huang. Rrhf: Rank responses to align language models with human feedback without tears. _ArXiv_, abs/2304.05302, 2023.
* [22] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
* [23] Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mi Zhou. Automl-gpt: Automatic machine learning with gpt. _ArXiv_, abs/2305.02499, 2023.
* [24] Wen Xiao, Yujia Xie, Giuseppe Carenini, and Pengcheng He. Chatgpt-steered editing instructor for customization of abstractive summarization. _ArXiv_, abs/2305.02483, 2023.
* [25] Potsawee Manakul, Adian Liusie, and Mark John Francis Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. _ArXiv_, abs/2303.08896, 2023.
* [26] Shan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, and Liang Lin. Sur-adapter: Enhancing text-to-image pre-trained diffusion models with large language models. _ArXiv_, abs/2305.05189, 2023.
* [27] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuji Feng, and Xiangnan He. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. _ArXiv_, abs/2305.00447, 2023.
* [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _ArXiv_, abs/2304.08485, 2023.

* [29] Ning Bian, Pei Yu Liu, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, and Le Sun. A drop of ink may make a million think: The spread of false information in large language models. _ArXiv_, abs/2305.04812, 2023.
* [30] Vivien A. Cabannes, Leon Bottou, Yann LeCun, and Randall Balestriero. Active self-supervised learning: A few low-cost relationships are all you need. _ArXiv_, abs/2303.15256, 2023.
* [31] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023.
* [32] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. _arXiv preprint arXiv:2212.10560_, 2022.
* [33] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. _arXiv preprint arXiv:2301.13688_, 2023.
* [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020.
* [35] Yufei Wang, Jiayi Zheng, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, and Daxin Jiang. Knowda: All-in-one knowledge mixture model for data augmentation in few-shot nlp. _arXiv preprint arXiv:2206.10265_, 2022.
* [36] Adrian de Wynter, Xun Wang, Alex Sokolov, Qilong Gu, and Si-Qing Chen. An evaluation on large language model outputs: Discourse and memorization. _ArXiv_, abs/2304.08637, 2023.
* [37] Ekaterina Svikhnushina and Pearl Pu. Approximating human evaluation of social chatbots with prompting. _ArXiv_, abs/2304.05253, 2023.
* [38] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and generating speech, music, sound, and talking head. _ArXiv_, abs/2304.12995, 2023.
* [39] Xiang Yue, Boshi Wang, Kai Zhang, Zi-Yuan Chen, Yu Su, and Huan Sun. Automatic evaluation of attribution by large language models. _ArXiv_, abs/2305.06311, 2023.
* [40] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. _arXiv preprint arXiv:2010.15980_, 2020.
* [41] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of Machine Learning Research_, 9(86):2579-2605, 2008.
* [42] J. A. Hartigan and M. A. Wong. A k-means clustering algorithm. _JSTOR: Applied Statistics_, 28(1):100-108, 1979.

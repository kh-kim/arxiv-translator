<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning</title>
<!--Generated on Tue Mar 26 19:13:16 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.18058v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2403.18058v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2403.18058v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2403.18058v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S1" title="1 Introduction ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S2" title="2 Related Work ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S2.SS1" title="2.1 Instruction Tuning Dataset ‣ 2 Related Work ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Instruction Tuning Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S2.SS2" title="2.2 Data Mixture of SFT ‣ 2 Related Work ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data Mixture of SFT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3" title="3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>CQIA CURATION</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS1" title="3.1 Social Media &amp; Forums ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Social Media &amp; Forums</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS1.SSS0.Px1" title="Zhihu ‣ 3.1 Social Media &amp; Forums ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">Zhihu</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS1.SSS0.Px2" title="SegmentFault ‣ 3.1 Social Media &amp; Forums ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">SegmentFault</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS1.SSS0.Px3" title="Douban ‣ 3.1 Social Media &amp; Forums ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">Douban</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS1.SSS0.Px4" title="Xiaohongshu ‣ 3.1 Social Media &amp; Forums ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">Xiaohongshu</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS1.SSS0.Px5" title="Ruozhiba ‣ 3.1 Social Media &amp; Forums ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">Ruozhiba</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS2" title="3.2 World Knowledge ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>World Knowledge</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS2.SSS1" title="3.2.1 General Encyclopedia ‣ 3.2 World Knowledge ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>General Encyclopedia</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS2.SSS2" title="3.2.2 Domain Specific Knowledge ‣ 3.2 World Knowledge ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Domain Specific Knowledge</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS3" title="3.3 Examinations ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Examinations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS3.SSS0.Px1" title="The Middle School and College Entrance Examinations ‣ 3.3 Examinations ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">The Middle School and College Entrance Examinations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS3.SSS0.Px2" title="Graduate Entrance Examination ‣ 3.3 Examinations ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">Graduate Entrance Examination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS3.SSS0.Px3" title="Logical Reasoning Test ‣ 3.3 Examinations ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">Logical Reasoning Test</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS3.SSS0.Px4" title="Chinese Culture Test ‣ 3.3 Examinations ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">Chinese Culture Test</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS4" title="3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>NLP Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS4.SSS0.Px1" title="COIG-PC ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">COIG-PC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS4.SSS0.Px2" title="COIG Human Value ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">COIG Human Value</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS4.SSS0.Px3" title="Firefly Chinese Traditional ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">Firefly Chinese Traditional</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.SS4.SSS0.Px4" title="100PoisonMpts ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">100PoisonMpts</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S4" title="4 Data Analysis ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Data Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S4.SS1" title="4.1 Statistics ‣ 4 Data Analysis ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Statistics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S4.SS2" title="4.2 Diversity ‣ 4 Data Analysis ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Diversity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S5" title="5 EXPERIMENTAL SETUP ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>EXPERIMENTAL SETUP</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S5.SS1" title="5.1 Evaluation ‣ 5 EXPERIMENTAL SETUP ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S5.SS1.SSS0.Px1" title="C-Eval ‣ 5.1 Evaluation ‣ 5 EXPERIMENTAL SETUP ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">C-Eval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S5.SS1.SSS0.Px2" title="CMMLU ‣ 5.1 Evaluation ‣ 5 EXPERIMENTAL SETUP ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">CMMLU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S5.SS1.SSS0.Px3" title="BELLE-EVAL ‣ 5.1 Evaluation ‣ 5 EXPERIMENTAL SETUP ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">BELLE-EVAL</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S5.SS1.SSS0.Px4" title="SafetyBench ‣ 5.1 Evaluation ‣ 5 EXPERIMENTAL SETUP ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title">SafetyBench</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S5.SS2" title="5.2 Implementation Details ‣ 5 EXPERIMENTAL SETUP ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S6" title="6 EXPERIMENT RESULTS ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>EXPERIMENT RESULTS</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S6.SS1" title="6.1 Ablating Instruction Data Sources ‣ 6 EXPERIMENT RESULTS ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Ablating Instruction Data Sources</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S6.SS2" title="6.2 Human Evaluation ‣ 6 EXPERIMENT RESULTS ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S6.SS3" title="6.3 Scaling Model Size ‣ 6 EXPERIMENT RESULTS ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Scaling Model Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S6.SS4" title="6.4 Safety ‣ 6 EXPERIMENT RESULTS ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Safety</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S7" title="7 Conclusion ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2403.18058v1 [cs.CL] 26 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Yuelin Bai<sup class="ltx_sup" id="id1.1.id1.1"><span class="ltx_text ltx_font_medium" id="id1.1.id1.1.1">1</span></sup></span> <span class="ltx_text ltx_font_bold" id="id2.2.id2">Xinrun Du<sup class="ltx_sup" id="id2.2.id2.1"><span class="ltx_text ltx_font_medium" id="id2.2.id2.1.1">2</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex1.1.1.1">1</span></span></span></span></span></span> <span class="ltx_text ltx_font_bold" id="id3.3.id3">Yiming Liang<sup class="ltx_sup" id="id3.3.id3.1"><span class="ltx_text ltx_font_medium" id="id3.3.id3.1.1">3</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">1</span></span></span></span></span></span> <span class="ltx_text ltx_font_bold" id="id4.4.id4">Yonggang Jin<sup class="ltx_sup" id="id4.4.id4.1"><span class="ltx_text ltx_font_medium" id="id4.4.id4.1.1">2</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex3"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex3.1.1.1">1</span></span></span></span></span>
<br class="ltx_break">Ziqiang Liu<sup class="ltx_sup" id="id4.4.id4.2"><span class="ltx_text ltx_font_medium" id="id4.4.id4.2.1">1</span></sup></span> <span class="ltx_text ltx_font_bold" id="id5.5.id5">Junting Zhou<sup class="ltx_sup" id="id5.5.id5.1"><span class="ltx_text ltx_font_medium" id="id5.5.id5.1.1">4,2</span></sup></span> <span class="ltx_text ltx_font_bold" id="id6.6.id6">Tianyu Zheng<sup class="ltx_sup" id="id6.6.id6.1"><span class="ltx_text ltx_font_medium" id="id6.6.id6.1.1">2</span></sup></span> <span class="ltx_text ltx_font_bold" id="id7.7.id7">Xincheng Zhang<sup class="ltx_sup" id="id7.7.id7.1"><span class="ltx_text ltx_font_medium" id="id7.7.id7.1.1">5</span></sup></span> <span class="ltx_text ltx_font_bold" id="id8.8.id8">Nuo Ma<sup class="ltx_sup" id="id8.8.id8.1"><span class="ltx_text ltx_font_medium" id="id8.8.id8.1.1">6</span></sup>
<br class="ltx_break">Zekun Wang<sup class="ltx_sup" id="id8.8.id8.2"><span class="ltx_text ltx_font_medium" id="id8.8.id8.2.1">2</span></sup></span> <span class="ltx_text ltx_font_bold" id="id9.9.id9">Ruibin Yuan<sup class="ltx_sup" id="id9.9.id9.1"><span class="ltx_text ltx_font_medium" id="id9.9.id9.1.1">7,2</span></sup></span> <span class="ltx_text ltx_font_bold" id="id10.10.id10">Haihong Wu<sup class="ltx_sup" id="id10.10.id10.1"><span class="ltx_text ltx_font_medium" id="id10.10.id10.1.1">5</span></sup></span> <span class="ltx_text ltx_font_bold" id="id11.11.id11">Hongquan Lin<sup class="ltx_sup" id="id11.11.id11.1"><span class="ltx_text ltx_font_medium" id="id11.11.id11.1.1">5</span></sup></span> <span class="ltx_text ltx_font_bold" id="id12.12.id12">Wenhao Huang<sup class="ltx_sup" id="id12.12.id12.1"><span class="ltx_text ltx_font_medium" id="id12.12.id12.1.1">6</span></sup>
<br class="ltx_break">Jiajun Zhang<sup class="ltx_sup" id="id12.12.id12.2"><span class="ltx_text ltx_font_medium" id="id12.12.id12.2.1">3</span></sup></span> <span class="ltx_text ltx_font_bold" id="id13.13.id13">Wenhu Chen<sup class="ltx_sup" id="id13.13.id13.1"><span class="ltx_text ltx_font_medium" id="id13.13.id13.1.1">8,9,2</span></sup></span> <span class="ltx_text ltx_font_bold" id="id14.14.id14">Chenghua Lin<sup class="ltx_sup" id="id14.14.id14.1"><span class="ltx_text ltx_font_medium" id="id14.14.id14.1.1">10,2</span></sup></span> <span class="ltx_text ltx_font_bold" id="id15.15.id15">Jie Fu<sup class="ltx_sup" id="id15.15.id15.1"><span class="ltx_text ltx_font_medium" id="id15.15.id15.1.1">7,2</span></sup></span> <span class="ltx_text ltx_font_bold" id="id16.16.id16">Min Yang<sup class="ltx_sup" id="id16.16.id16.1"><span class="ltx_text ltx_font_medium" id="id16.16.id16.1.1">1</span></sup>
<br class="ltx_break">Shiwen Ni<sup class="ltx_sup" id="id16.16.id16.2"><span class="ltx_text ltx_font_medium" id="id16.16.id16.2.1">1</span></sup></span> <span class="ltx_text ltx_font_bold" id="id17.17.id17">Ge Zhang<sup class="ltx_sup" id="id17.17.id17.1"><span class="ltx_text ltx_font_medium" id="id17.17.id17.1.1">8,9</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex4"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex4.1.1.1">2</span></span></span></span></span>
<br class="ltx_break"><sup class="ltx_sup" id="id17.17.id17.2"><span class="ltx_text ltx_font_medium" id="id17.17.id17.2.1">1</span></sup></span>Shenzhen Institute of Advanced Technology, CAS <sup class="ltx_sup" id="id18.18.id18">2</sup>M-A-P  <sup class="ltx_sup" id="id19.19.id19">3</sup>Institute of Automation, CAS
<br class="ltx_break"><sup class="ltx_sup" id="id20.20.id20">4</sup>Peking University <sup class="ltx_sup" id="id21.21.id21">5</sup>University of Science and Technology of China <sup class="ltx_sup" id="id22.22.id22">6</sup>01.ai <sup class="ltx_sup" id="id23.23.id23">7</sup>HKUST
<br class="ltx_break"><sup class="ltx_sup" id="id24.24.id24">8</sup>University of Waterloo <sup class="ltx_sup" id="id25.25.id25">9</sup>Vector Institute <sup class="ltx_sup" id="id26.26.id26">10</sup>University of Manchester 

<br class="ltx_break">
</span><span class="ltx_author_notes">Equal contributionCorresponding author</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id27.id1">Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language.
These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency.
However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning.
The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users.
To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset.
Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions.
To this end, we collect a high-quality human-written corpus from various sources on the Chinese Internet, including Q&amp;A communities, Wikis, examinations, and existing NLP datasets.
This corpus was rigorously filtered and carefully processed to form the COIG-CQIA dataset.
Furthermore, we train models of various scales on different subsets of CQIA, following in-depth evaluation and analyses.
The findings from our experiments offer valuable insights for selecting and developing Chinese instruction-tuning datasets.
We also find that models trained on CQIA-Subset achieve competitive results in human assessment as well as knowledge and security benchmarks. Data are available at <a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/m-a-p/COIG-CQIA" title="">https://huggingface.co/datasets/m-a-p/COIG-CQIA</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs), such as GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib2" title="">2020</a>)</cite>, LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib22" title="">2023</a>)</cite>, and PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib4" title="">2023</a>)</cite>, have demonstrated remarkable capabilities as general-purpose assistants.
The cornerstone of this achievement is instruction tuning, which significantly enhances the capabilities and controllability of LLMs through training on datasets composed of instruction-output pairs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib29" title="">2023b</a>)</cite>.
This technique effectively aligns the models’ training objectives with human intentions, thereby ensuring that the models can interpret and execute human instructions both effectively and safely.
Therefore, the availability of high-quality instruction tuning datasets is crucial for LLMs to operate as efficient and dependable assistants.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">There exists many English instruction tuning datasets.
However, the available datasets for Chinese instruction tuning are generally either limited in size or lacking in quality.
Chinese instruction tuning datasets are categorized into three main types:
(1) datasets derived from English instruction datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Peng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib17" title="">2023</a>)</cite> or NLP datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(CLUEbenchmark, <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib6" title="">2022</a>; Yang, <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib26" title="">2023</a>)</cite>,
(2) datasets generated by LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib8" title="">2023</a>)</cite>, and
(3) self-generated instruction tuning datasets &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib12" title="">2023</a>; Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib20" title="">2023</a>)</cite>.
COIG&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib28" title="">2023a</a>)</cite> integrates multiple approaches to construct a human-verified universal high-quality Chinese instruction corpus.
However, the previously mentioned Chinese instruction tuning datasets have inherent issues such as not aligning with natural Chinese communication patterns, lacking genuine Chinese linguistic data, containing numerous problematic data points, and having small-scale data.
This paper focuses on constructing a Chinese instruction tuning dataset sourced from authentic Chinese linguistic data across diverse domains and undergone meticulous manual cleaning procedures aimed at enhancing the proficiency of LLMs in following Chinese instructions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we introduce COIG-<span class="ltx_text ltx_font_bold" id="S1.p3.1.1">CQIA</span>&nbsp;(<span class="ltx_text ltx_font_bold" id="S1.p3.1.2">Chinese</span> Open Instruction Generalist - <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">Q</span>uality <span class="ltx_text ltx_font_bold" id="S1.p3.1.4">I</span>s <span class="ltx_text ltx_font_bold" id="S1.p3.1.5">A</span>ll You Need), a high-quality Chinese instruction tuning dataset, which is designed to provide the Chinese NLP community with high-quality and human interaction-aligned instruction fine-tuning data.
Inspired by the work of LIMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib30" title="">2023</a>)</cite>, COIG-CQIA focuses on curating a dataset from Chinese internet sources, comprising Q&amp;A sessions and articles.
These sources undergo thorough cleaning, restructuring, and manual review to ensure high quality, diversity, and relevance.
Furthermore, we conduct analytical experiments to assess the effects of data quality, provenance, and mixing ratio.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In summary, the contributions are as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a high-quality Chinese instruction fine-tuning dataset, specifically designed to align with human interaction, achieved through rigorous filtering procedures.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We explore the influence of various data sources, including social media, encyclopedias, and traditional NLP tasks, on model performance. Our analysis offers essential insights for selecting training data from the Chinese internet.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Various benchmark tests and human evaluations confirm that models fine-tuned on our CQIA dataset exhibit superior performance, thus establishing CQIA as a valuable resource for the Chinese NLP community.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Instruction Tuning Dataset</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Instruction tuning aims to train large language models to generate responses that align with input instructions, thereby enable LLMs with conversational and task execution capabilities. Compared with standard LLM, SFT allows the behavior of the model to be more controllable and predictable, thereby achieving the purpose of aligning human will. Methods to build instruction tuning datasets include: (1) Pure manual annotation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Conover et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib7" title="">2023</a>)</cite>. This method completely constructs instructions and answers manually, which is very time-consuming and laborious; (2) Converted from existing datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mishra et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib16" title="">2022</a>; Sanh et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib18" title="">2022</a>; Chung et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib5" title="">2022</a>)</cite>. Some studies use supervised data sets from NLP tasks to construct instruction tuning data; (3) Automatically generated using LLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Honovich et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib10" title="">2022</a>; Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib23" title="">2023</a>; Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib24" title="">2023a</a>; Ji et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib12" title="">2023</a>; Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib25" title="">2023b</a>)</cite>. Others use existing LLMs to generate instruction tuning data. A common practice is to first manually annotate high-quality seed datasets, and then use LLM to expand the seed instructions and corresponding outputs, which can generate large-scale instruction tuning data with very little human annotation. However, the quality cannot be guaranteed and there is a certain amount of noisy data, which can lead to hallucinations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">There exists many English instruction tuning datasets. In comparison, existing Chinese instruction tuning datasets are either small in scale or have quality issues. Some studies have translated the English instruction tuning datasets into Chinese&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Peng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib17" title="">2023</a>)</cite>, but this may lead to accumulation of translation errors. pCLUE&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(CLUEbenchmark, <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib6" title="">2022</a>)</cite> and Firefly&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yang, <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib26" title="">2023</a>)</cite> transform the original NLP task dataset into instruction tuning datasets. HC3&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib8" title="">2023</a>)</cite> collects tens of thousands of comparison responses from both human experts and ChatGPT. COIG&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib28" title="">2023a</a>)</cite> builds a human-verified universal high-quality Chinese instruction corpus.
BELLE&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib12" title="">2023</a>)</cite> and MOSS&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib20" title="">2023</a>)</cite> use a method similar to self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib23" title="">2023</a>)</cite> to automatically generate Chinese instruction tuning datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Mixture of SFT</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Currently, more and more studies has begun to pay attention to the importance of data quality of instruction tuning. LIMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib30" title="">2023</a>)</cite> only uses 1,000 high-quality instructions and outputs for SFT, and does not even need to perform RLHF training to achieve very strong performance. AlpaGasus&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib3" title="">2023</a>)</cite> uses powerful LLM to automatically identify and filter low-quality data, resulting in high-quality instruction tuning data to improve performance and training speed. Humpback&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib14" title="">2023</a>)</cite> filters out high-quality samples to fine-tune a more powerful LLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Others&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Song et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib19" title="">2023</a>)</cite> explores the impact of the mixture strategies of different instruction tuning datasets. Tulu series&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Konchakov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib13" title="">2023</a>; Ivison et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib11" title="">2023</a>)</cite> show that increasing instruction diversity can effectively improve the performance and different instruction tuning datasets can discover or enhance specific skills, while no one dataset (or combination) provides the best performance across all assessments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>CQIA CURATION</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:433.6pt;height:284.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(38.6pt,-25.3pt) scale(1.21637025000768,1.21637025000768) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.2.1">Quantity</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.3.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.4.1">Quantity</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.1.1">Zhihu</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.1.2">8837</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.1.3">Douban</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.1.4">3132</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.2">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.2.1">Xiaohongshu</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.2.2">1508</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.2.3">Segment Fault</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3.2.4">458</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.3">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.3.1">Encyclopedia Article</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.3.2">980</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.3.3">Encyclopedia of China</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.3.4">1706</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.4">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.4.1">WikiHow</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.4.2">1876</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.4.3">COIG PC</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.4.4">3000</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6.5">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.5.1">Middle school Exam</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.5.2">2000</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.5.3">Graduate Entrance Examination</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.5.4">475</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7.6">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.6.1">Logi QA</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.6.2">422</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.6.3">CValue</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.7.6.4">906</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.8.7">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.7.1">COIG-Human-Value</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.7.2">101</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.7.3">Chinese Traditional</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.7.4">232</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.9.8">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.9.8.1">Idiom Explanation</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.9.8.2">112</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.9.8.3">Poem Writing</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.9.8.4">47</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.10.9">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.9.1">Classical Chinese Translation</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.9.2">112</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.9.3">MBA Encyclopedia</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.9.4">10689</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.11.10">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.10.1">Finance NLP Task</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.10.2">600</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.10.3">Medical Encyclopedia</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.10.4">8351</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.12.11">
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.12.11.1">Medical Article</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.12.11.2">186</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.12.11.3">Law</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.12.11.4">2645</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.1.13.12.1">Total</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T1.1.1.13.12.2"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.1.13.12.3">48375</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T1.1.1.13.12.4"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
The amount of data from different sources of the dataset mixture
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To ensure the quality and diversity of our data, we manually selected 13 data sources from high-quality websites and data resources within the Chinese Internet. These sources include community Q&amp;A forums, encyclopedic sites, content creation platforms, examinations, etc. We also incorporated high-quality Chinese NLP datasets to enrich the diversity of tasks. Specifically, we categorized all data sources into four types: Social Media &amp; forums, World Knowledge, NLP tasks, and Examinations. The data sources and their descriptions are as follows.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Social Media &amp; Forums</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Zhihu</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">is a vibrant question-and-answer platform where users can ask and answer questions on a wide range of topics, making it a comprehensive repository of knowledge and insights. Zhihu encourages its users to provide well-thought-out answers that are informative and reflective of expert knowledge or personal experience. However, the absence of a review mechanism for answers on Zhihu leads to a large volume of content that falls short of our quality standards. To filter low quality answers, we selected answers with more than 50 upvotes, then filtering out content containing sensitive or harmful keywords using a rule-based method. Subsequently, we employed GPT-4 to score the responses on a scale of 1-10, retaining those with scores above 8.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">SegmentFault</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">is a question-and-answer community focused on IT technology, providing Chinese developers with a high-quality platform for exchange, similar to Stack Overflow. In this community, users engage in asking and answering questions related to IT technology, where the questioner can accept the most useful answer. Additionally, community members can also upvote or comment on answers. Our data are collected from the contents posted before 2018, as earlier content may become outdated due to changes in programming languages or software versions. We then select the "accepted" answers with at least 5 upvotes. Furthermore, we manually review all the (question, answer) pairs to remove or modify low-quality content.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Douban</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">is a social network and database that allows users to create content related to literature and artistic works such as films, books, TV series, music, etc. We sample data from books, movies, and TV series, extracting metadata that includes ratings, detailed information on actors/crew, and long reviews. Then, we design three tasks in total: synopsis generation, review generation, and recommendations. For each task, we manually design various prompt templates and used these templates in combination with metadata to construct instructions. For synopsis generation and review generation, we construct instructions using prompt templates combined with movie or TV series names, with responses generated by Douban users. Then we remove responses with lengths shorter than a threshold and delete personal information and irrelevant content(e.g., "Subscribe our Official Accounts"). Additionally, we manually adjusted some instructions to add more complex implicit intents, aligning better with the details of the response.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Xiaohongshu</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">provides a space for users to share their lives, travel, food, and product recommendations. Contents in this platform are renowned on the Chinese internet for their unique and expressive style. We sample posts with lengths ranging from 500 to 2000, excluding those that involve interactions with other users ("@User_Name") and those referencing images or videos ("as shown in the picture/video").</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Ruozhiba</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px5.p1.1">is a sub-forum of Baidu Tieba, an interests-based community forum. Its posts often contain puns, polysemous terms, causal reversals, and homophones, many of which are designed with logical traps, posing challenges even for humans. We collected the 500 most upvoted threads. Using the titles as instructions, we eliminate those that were either non-instructive (i.e., declarative statements or unanswerable) or toxic. Responses were generated by either humans or GPT-4. We conducted manual reviews for GPT4’s responses to ensure accuracy, ultimately obtaining 240 (instruction, response) pairs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>World Knowledge</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>General Encyclopedia</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">General Encyclopedia provides comprehensive coverage of a wide range of topics across various fields. We collect data from three Chinese Encyclopedia websites: One Hundred Thousand Whys<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://10why.net/</span></span></span>, wikiHow-zh<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://zh.wikihow.com</span></span></span> and Encyclopedia of China<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.zgbk.com/</span></span></span>. <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p1.1.1">One Hundred Thousand Whys</span> is an encyclopedic website aimed at popular science, featuring thousands of high-quality articles asking "why" across topics from natural science to humanities. We collect data from all 15 categories and ensure uniform distribution across each category. Article titles are used as instruction(e.g."Why don’t I get altitude sickness when I fly?"), and the content as responses, with responses under 300 characters being filtered out.
<span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p1.1.2">wikiHow-zh</span>, the Chinese version of wikiHow, is an encyclopedia-style website covering a wide range of topics, featuring tens of thousands of "how-to" articles with multiple revisions. We collected xxx articles from the site and sampled 1500 entries from all 19 categories, with a sampling temperature of 3. Since the original data are in HTML, we parse the HTML and concatenate the article content using Markdown. Subsequently, we filtered out low-quality data (e.g., incorrect formula conversions) and articles exceeding 3000 words in length. We use titles as the instructions and the article contents as responses.
<span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p1.1.3">Encyclopedia of China</span> is a comprehensive encyclopedia comprising approximately 500,000 entries, authored and revised by domain experts. We design various prompt templates for concept explanation tasks. We sample the entries from all 74 categories, with structures comprising entry names and several subtitles, along with their respective contents. We randomly combined entry names or subtitles with prompt templates to construct instructions. For instance, for the "Confucius" entry
, which includes subtitles "<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p1.1.4">Biography</span>", "<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p1.1.5">Academic Theories</span>", and "<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p1.1.6">Impacts</span>", we selected "<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p1.1.7">Academic Theories</span>" to create instruction such as "<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p1.1.8">Write the details of Confucius’s academic theories.</span>" and then, we use this subtitle’s content as a response.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Domain Specific Knowledge</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">We collected data from four specific domains: medicine, economic management, electronics, and agriculture.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.1">Medical Domain</span> sources from three websites: Baobaozhidao, Qianwen Health, and Baikemingyi. Both <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.2">Baobaozhidao</span> and <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.3">Qianwen Health</span> feature question-and-answer style articles written by medical experts, with the former primarily focusing on a broad range of medical fields, while the latter on maternal and infant health. We collected articles from these two sites, excluding those whose titles are not in question form. Subsequently, we used the titles as instructions and the article content as responses. <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.4">Baikemingyi</span> contains Wikipedia-style structured data, featuring introductions to tens of thousands of diseases and medications. We designed various prompt templates and combined entry names with these templates to construct commands (e.g., "<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS2.p2.1.5">Write a professional introduction about joint pain</span>").</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p3.1.1">Economic Management Domain</span> data is collected from MBA Wiki Encyclopedia, a website that encompasses Wikipedia-style structured knowledge, authored and revised by numerous contributors. We designed various prompt templates, combining entry names with random templates to construct instructions, such as "<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS2.p3.1.2">Please explain the following term in detail: Remittance Agent</span>". Ultimately, the content of the entries is concatenated and constructed into responses in markdown format.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p4.1.1">Electronics Domain</span> data is sourced from the EETrees electronic encyclopedia, which is also structured in form. We design various prompt templates and combine these with entry names to construct instructions, with the corresponding content as the response.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p5">
<p class="ltx_p" id="S3.SS2.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p5.1.1">Agriculture Domain</span> sources from an agricultural encyclopedia website, containing a range of topics from plant cultivation to animal breeding. We collected articles on all ten topics, excluding those with non-question titles, containing images, or shorter than 300 words in length. Subsequently, we construct (instruction, response) pairs from the titles and content of the articles.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Examinations</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">The Middle School and College Entrance Examinations</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">primarily derives from the COIG dataset<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib28" title="">2023a</a>)</cite>, a harmless, helpful, and diverse Chinese Instruction dataset. Chinese examination is a subset of it, with the Middle School and College Entrance Examinations being China’s principal general competency tests. These data contain a variety of question types and detailed answer explanations, primarily covering humanities subjects (Chinese, English, Politics, Biology, History, and Geography). We use temperature sampling on the data across these subjects and then filtered out questions and answers with formatting errors. The questions were used as instructions and, the "answer" and "analysis" fields were concatenated to form extended responses, resulting in 1964 (instruction, response) pairs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Graduate Entrance Examination</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">is one of the most challenging examinations in China, exceeding college entrance exams in difficulty and requiring advanced knowledge application and depth. We have collected a variety of exam papers from recent years across disciplines including mathematics, computer science, chemistry, law, psychology, medicine, etc. Using Mathpix<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://mathpix.com/</span></span></span> for image-to-text conversion, we extracted questions and answers and converted them into LaTeX format. We eliminate data without analysis and manually verified the accuracy of the questions and answers. We eliminate data without analysis and manually verified the accuracy of the questions and answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Logical Reasoning Test</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">aims to assess the ability to apply logical and analytical reasoning to solve problems. This type of test is widely used in various competitive examinations to evaluate critical thinking and problem-solving skills. We collect logic reasoning questions from the internet, retaining those containing detailed answer analyses, and then construct them into (instruction, response) pairs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Chinese Culture Test</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px4.p1.1">investigates the mastery of traditional Chinese culture and history. We collected multiple-choice questions on traditional Chinese culture from the internet, retaining those with answer analyses, and constructed them into (instruction, response) pairs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>NLP Datasets</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">COIG-PC</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">The COIG-PC Dataset is a comprehensive collection of Chinese NLP tasks, aimed at advancing Chinese NLP research. The goal of this dataset is to provide a rich set of resources to researchers and developers, which they can use to improve the capabilities of language models in handling Chinese text. It offers a comprehensive suite of resources for researchers and developers, facilitating advancements in language model capabilities across various domains including text generation, information extraction, sentiment analysis, and machine translation, etc. Initially, we selected 1,413 tasks involving both Chinese and English languages from COIG-PC. Then, we manually select 250 tasks that meet our quality criteria, including information extraction, classification, summary, and others, primarily sourced from traditional NLP datasets. Through temperature sampling, we eventually sample 3,000 (instruction, response) pairs, which are further verified by human to ensure quality.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">COIG Human Value</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">is a subset of the COIG dataset<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib28" title="">2023a</a>)</cite> designed to provide instruction fine-tuning data aligned with human values. We selected the portion reflecting Chinese cultural values, constructed using the Self-Instruct<cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib23" title="">2023</a>)</cite> method from manually selected seed instructions. We manually filtered out data with formatting errors and incorrect answers, retaining those that include explanations of the answers to form (instruction, response) pairs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Firefly Chinese Traditional</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px3.p1.1">comprises three tasks: Classical Chinese Translation, Ancient Poetry Writing, and Idiom Interpretation, which are the subset of the Firefly dataset<cite class="ltx_cite ltx_citemacro_cite">Yang (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib26" title="">2023</a>)</cite> related to traditional Chinese culture. We filter the responses shorter than 300 characters, and sample 300 instances from each task. Then, we manually filtered out low-quality data such as instruction-response mismatch, response error, and unanswerable instructions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">100PoisonMpts</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px4.p1.1">addressing issues of anti-discrimination and empathy, spans various dimensions including jurisprudence, psychology, child education, obscure facts, intimate relationships, etc. It involves human-generated prompts that evoke bias and discrimination, followed by expert-crafted responses that align with human values. To enhance the harmlessness of the CQIA, we sample all the data from 100PoisonMpts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="567" id="S3.F1.g1" src="extracted/5497703/fig/self-instruct-final.png" width="598">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Most common root verbs (inner circle) and top direct noun objects (outer circle) in the CQIA dataset. Note that we only visualize when a certain verb-noun pair has more than 30 instances, and many instructions do not contain a verb-noun structure.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="S3.F2.g1" src="extracted/5497703/fig/overview_cqia.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of CQIA Task Types.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="396" id="S3.F3.g1" src="extracted/5497703/fig/dis-1.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Length distribution of instruction and responses. Note that the instruction is the concatenation of original instructions and inputs in our dataset.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data Analysis</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Statistics</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.T1" title="Table 1 ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_tag">1</span></a> describe the data statistics for all sources. We collected a total of 48,375 instances from 22 sources within the Chinese Internet and Community, covering domains ranging from general knowledge and STEM to humanities. Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.F2" title="Figure 2 ‣ 100PoisonMpts ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the variety of task types, encompassing information extraction, question answering, code generation, etc. We demonstrated the distribution in the length of the instructions and responses in Figure<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.F3" title="Figure 3 ‣ 100PoisonMpts ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_tag">3</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Diversity</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To analyze the diversity of the COIG-CQIA dataset, we follow prior work<cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib23" title="">2023</a>); Lou et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib15" title="">2023</a>)</cite> by employing the Hanlp tool<cite class="ltx_cite ltx_citemacro_cite">He and Choi (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib9" title="">2021</a>)</cite> to parse the instructions and then extract the verb closest to the root along with its top direct noun object. We then plot the top 20 most common root verbs and their corresponding direct noun objects in Figure<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S3.F1" title="Figure 1 ‣ 100PoisonMpts ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_tag">1</span></a>. From this figure we can observe that CQIA features a diverse range of instructions and intentions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:188.5pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-53.5pt,23.2pt) scale(0.802028748010008,0.802028748010008) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.2.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.3.1">Open QA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.4.1">Brainstorming</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.5.1">Classification</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.6.1">Generation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.7.1">Summarization</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.8.1">Rewrite</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.9.1">Closed QA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.10.1">Extract</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.11"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.11.1">Math</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.12.1">Code</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.13"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.13.1">Average</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T2.1.1.2.1.1" rowspan="13"><span class="ltx_text" id="S4.T2.1.1.2.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T2.1.1.2.1.1.1.1" style="width:6.8pt;height:25.7pt;vertical-align:-9.4pt;"><span class="ltx_transformed_inner" style="width:25.7pt;transform:translate(-9.43pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T2.1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.1.1.1.1.1">Yi-6B</span></span>
</span></span></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.2">SegmentFault</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.3">26.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.4">33.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.5">8.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.6">23.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.7">29.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.8">20.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.9">25.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.10">22.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.11">14.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.2.1.12">32.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.13">23.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.1">COIG PC</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.2">31.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.3">47.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.4">28.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.5">44.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.6">43.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.7">53.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.8">45.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.9">28.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.10">35.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.3.2.11">23.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.3.2.12">38.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.1">Douban</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.2">45.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.3">64.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.4">23.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.5">64.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.6">38.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.7">37.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.8">34.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.9">25.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.10">32.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.4.3.11">42.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.4.3.12">40.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.1">Zhihu</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.2">48.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.3">72.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.4">19.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.5">66.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.6">24.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.7">29.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.8">28.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.9">12.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.10">8.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.5.4.11">40.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.5.4.12">35.0</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.1">Logi QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.2">45.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.3">65.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.4">32.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.5">55.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.6">37.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.7">49.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.8">47.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.9">38.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.10">17.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.6.5.11">40.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.12">42.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.6">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.1">Ruozhiba</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.7.6.2.1">64.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.3"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.7.6.3.1">84.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.7.6.4.1">50.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.5"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.7.6.5.1">73.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.6">45.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.7">39.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.8"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.7.6.8.1">57.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.9">30.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.10">27.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.7.6.11"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.7.6.11.1">63.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.12">53.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.8.7">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.1">Wiki</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.2">51.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.3">67.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.4">21.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.5">60.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.6">30.8</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.7">31.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.8">30.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.9">21.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.10">22.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.8.7.11">34.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.8.7.12">37.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9.8">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.1">Finance</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.2">43.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.3">65.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.4">30.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.5">57.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.6">36.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.7">30.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.8">34.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.9">31.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.10">15.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.9.8.11">27.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.12">37.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10.9">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.1">Exam</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.2">51.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.3">74.3</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.4">42.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.5">70.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.6"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.10.9.6.1">54.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.7"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.10.9.7.1">60.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.8"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.10.9.8.1">56.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.9"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.10.9.9.1">47.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.10"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.10.9.10.1">41.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.10.9.11">49.9</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.12"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.10.9.12.1">54.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.11.10">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.1">Xhs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.2">25.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.3">47.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.4">8.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.5">45.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.6">8.6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.7">21.4</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.8">22.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.9">7.0</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.10">28.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.11.10.11">27.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.12">24.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.12.11">
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.1">WikiHow</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.2">0.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.3">1.2</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.4">1.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.5">7.7</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.6">18.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.7">3.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.8">23.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.9">0.5</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.10">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.12.11.11">2.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.12.11.12">5.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.1">CQIA-Subset</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.2"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.13.12.2.1">59.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.13.12.3.1">86.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.4"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.13.12.4.1">48.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.13.12.5.1">79.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.13.12.6.1">60.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.13.12.7.1">69.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.8">50.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.9"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.13.12.9.1">42.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.10"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.13.12.10.1">37.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.1.1.13.12.11"><span class="ltx_text ltx_framed_underline" id="S4.T2.1.1.13.12.11.1">55.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.1.13.12.12"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.13.12.12.1">64.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The performance of Yi-6B trained on various datasets evaluated on BELLE-EVAL using GPT4.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:188.5pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-53.5pt,23.2pt) scale(0.802028748010008,0.802028748010008) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.2.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.3.1">Open QA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.4.1">Brainstorming</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.5.1">Classification</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.6.1">Generation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.7.1">Summarization</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.8.1">Rewrite</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.9.1">Closed QA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.10.1">Extract</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.11.1">Math</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.12.1">Code</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1.13"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.13.1">Average</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T3.1.1.2.1.1" rowspan="13"><span class="ltx_text" id="S4.T3.1.1.2.1.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1.1.2.1.1.1.1" style="width:6.8pt;height:25.7pt;vertical-align:-9.4pt;"><span class="ltx_transformed_inner" style="width:25.7pt;transform:translate(-9.43pt,0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T3.1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.1.1.1.1.1.1">Yi-6B</span></span>
</span></span></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.2">SegmentFault</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.3">51.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.4">70.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.5">43.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.6">66.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.7">57.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.8">75.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.9">41.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.10">47.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.11">75.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.2.1.12">65.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.2.1.13">60.7</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.1">COIG PC</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.2">19.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.3">38.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.4">27.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.5">43.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.6">37.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.7">63.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.8">40.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.9">25.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.10">43.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.3.2.11">19.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.3.2.12">37.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.1">Douban</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.2">57.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.3">81.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.4">63.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.5">76.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.6">54.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.7">60.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.8">47.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.9">50.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.10">73.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.4.3.11">50.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3.12">63.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.1">Zhihu</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.2">66.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.3">90.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.4">52.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.5">82.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.6">71.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.7">78.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.8">46.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.9">39.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.10"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.4.10.1">76.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.5.4.11">62.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.4.12">69.1</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6.5">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.1">Logi QA</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.2">51.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.3">76.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.4">64.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.5">75.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.6">60.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.7">71.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.8">61.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.9">52.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.10">47.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.6.5.11">45.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.5.12">62.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7.6">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.1">Ruozhiba</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.6.2.1">75.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.6.3.1">92.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.6.4.1">76.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.6.5.1">92.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.6.6.1">77.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.7">70.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.8"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.6.8.1">67.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.9"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.6.9.1">68.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.10">72.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.7.6.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.6.11.1">65.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.6.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.6.12.1">76.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8.7">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.1">Wiki</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.2">63.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.3">75.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.4">44.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.5">80.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.6">47.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.7">66.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.8">47.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.9">50.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.10">56.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.8.7.11">55.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.7.12">60.5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9.8">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.1">Finance</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.2">46.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.3">71.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.4">17.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.5">60.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.6">27.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.7">23.6</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.8">17.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.9">29.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.10">28.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.9.8.11">24.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.8.12">36.7</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10.9">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.1">Exam</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.2">49.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.3">79.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.4">64.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.5">79.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.6">61.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.7"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.10.9.7.1">79.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.8">66.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.9">61.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.10">52.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.10.9.11">56.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.9.12">66.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11.10">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.1">Xhs</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.2">51.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.3">76.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.4">38.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.5">68.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.6">25.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.7">46.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.8">28.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.9">32.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.10">74.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.11.10.11">36.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.10.12">50.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12.11">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.1">Wikihow</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.2">54.7</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.3">75.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.4">32.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.5">68.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.6">45.3</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.7">55.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.8">40.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.9">55.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.10">41.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.12.11.11">44.4</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.11.12">52.7</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.1">CQIA-Subset</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.2">56.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.3">84.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.4">48.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.5">72.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.6">60.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.7">70.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.8">54.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.9">50.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.10">52.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.1.1.13.12.11">49.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.1.1.13.12.12">61.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The performance of Yi-34B trained on various datasets evaluated on BELLE-EVAL using GPT4.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>EXPERIMENTAL SETUP</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we describe how we use COIG-CQIA to fine-tune models and elaborate our our evaluation methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">C-Eval</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels. We choosing the answer option with the highest log-likelihood as the final prediction of the model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">CMMLU</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">is a comprehensive evaluation benchmark specifically designed to evaluate the knowledge and reasoning abilities of LLMs within the context of Chinese language and culture. CMMLU covers a wide range of subjects, comprising 67 topics that span from elementary to advanced professional levels. It includes subjects that require computational expertise, such as physics and mathematics, as well as disciplines within humanities and social sciences.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">BELLE-EVAL</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">is an open-ended test set comprising 12 different instruction types across various domains, including open question answering, brainstorming, mathematics, coding, etc. It can be used to assess a model’s ability to follow instructions across different types. We employ sampling generation for generating responses to instructions and use a model-based evaluation method.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">SafetyBench</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px4.p1.1">comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. We evaluate models in few-shot setting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Implementation Details</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We fine-tuned bilingual LLMs(Chinese and English) on COIG-CQIA, including Yi<cite class="ltx_cite ltx_citemacro_cite">Young et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib27" title="">2024</a>)</cite>, Qwen<cite class="ltx_cite ltx_citemacro_cite">Bai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib1" title="">2023</a>)</cite>, and InternLM<cite class="ltx_cite ltx_citemacro_cite">Team (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib21" title="">2023</a>)</cite>, which represent the forefront of Chinese models.
We will provide implementation details in the next revision.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.1" style="width:346.9pt;height:701.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(97.8pt,-197.7pt) scale(2.29226864828893,2.29226864828893) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T4.1.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.1.1.1.1.2">SafetyBench</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.1.2.1.1">COIG PC</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.2.1.2">81.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.3.2.1">Chinese Tradiational</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.3.2.2">76.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.4.3.1">Douban</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.3.2">76.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.5.4.1">Exam</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.5.4.2">77.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.6.5.1">Finance</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.6.5.2">75.1</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.7.6.1">Logi QA</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.7.6.2">79.1</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.8.7.1">Ruozhiba</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.8.7.2">81.3</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.9.8.1">Segmentfault</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.9.8.2">78.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.10.9.1">Wiki</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.10.9.2">75.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.11.10.1">Wikihow</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.11.10.2">76.4</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.12.11.1">Xhs</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.12.11.2">76.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.13.12.1">Zhihu</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.13.12.2">75.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T4.1.1.14.13.1">Human Value</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.14.13.2">79.1</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.1.15.14.1">CQIA-Sub-6B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.15.14.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.15.14.2.1">81.7</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.1.16.15.1">GPT-4-0613</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.16.15.2">89.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T4.1.1.17.16.1">GPT-3.5-turbo-0613</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.1.17.16.2">80.4</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>SafetyBench scores of Yi-6B trained on various data sources.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.1" style="width:442.3pt;height:225.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(97.6pt,-49.7pt) scale(1.78965518838146,1.78965518838146) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.1.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.2">Ceval (val 5-shot)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.3">CMMLU (test 5-shot)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.1.2.1.1">Qwen-1.8b</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.2">51.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.3">47.26</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.3.2.1">Yi-6B</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.2">73.40</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.3">74.85</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.4.3.1">Qwen-14b</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.2">68.20</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.4.3.3">67.96</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.5.4.1">InternLM2-20b</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.4.2">71.25</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.5.4.3">67.48</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.1.1.6.5.1">Yi-34b</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.6.5.2">77.04</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.6.5.3">78.18</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.1.1.7.6.1">Qwen-72b</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.7.6.2">78.68</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.7.6.3">76.79</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance of different base models after training on the COIG Subset data.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="197" id="S5.F4.g1" src="x1.png" width="307">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Human evaluation of pair-wise comparison between CQIA-Subset and 5 strong baselines at similar parameter scale.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>EXPERIMENT RESULTS</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Ablating Instruction Data Sources</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We finetune the Yi series model<cite class="ltx_cite ltx_citemacro_cite">Young et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib27" title="">2024</a>)</cite> and the Qwen-72B<cite class="ltx_cite ltx_citemacro_cite">Bai et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#bib.bib1" title="">2023</a>)</cite> model on different data sources of our datasets.
to analyze the impact of data source on model capabilities across different domain knowledge. Then,
We evaluate each model performance on various type of assistant-style tasks using model-based(i.e. GPT-4) automatic evaluation
on Belle-Eval.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">To understand the correlation between training data sources and the downstream performance of different tasks, we evaluate the models on 10 tasks from BELLE-Eval. We employ GPT-4 as evaluator for scoring model responses, with scores ranging from 0 to 1.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">Tabel <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S4.T2" title="Table 2 ‣ 4.2 Diversity ‣ 4 Data Analysis ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results of different Yi-6B-based models fine-tuned on different subsets. From the table, we can see that models trained on our data excel in generative tasks such as brainstorming, generation, and summarization, but perform poorly in math and coding.
The <span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.1">Exam</span> subset achieves the best performance across all subsets with an average score of 54.9, particularly excelling in Extract and Math tasks. This is expected as <span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.2">Exam</span> contains more math quizzes and exam types(e.g., reading comprehension), potentially boosting the model’s performance in most tasks.
Interestingly, <span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.3">Ruozhiba</span> ranks second on average across all subsets. We conjecture this is because it may enhance the model’s logical reasoning ability, thereby benefiting most of the instruct-following tasks.
<span class="ltx_text ltx_font_typewriter" id="S6.SS1.p3.1.4">COIG-PC</span> demonstrates proficiency in evaluations of the knowledge dimension, such as C-Eval, yet underperforms in Belle-Eval. We attribute this discrepancy to its origin in traditional NLP datasets and the short length of responses, which can impair reasoning tasks and are less favored by model-based evaluators.
The substantial gap between C-Eval and Belle-Eval highlights the importance of developing assessments that can comprehensively and accurately evaluate Chinese LLMs.
Moreover, WikiHow scores only 5.8, which we believe is due to the lack of diversity in its "how-to" instructions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Human Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In addition to automatic evaluation, we also evaluate Yi-6B fine-tuned on CQIA-subset by comparing it to state-of-the-art Chinese open-source chat models in similar parameter scale. As we focus on questions posed by real-world Chinese-speaking users. We sample 200 questions from OL-CC<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://data.baai.ac.cn/details/OL-CC</span></span></span> and Zhihu which are not present in the training set for human evaluation. We conduct pair-wise comparison, aiming to demonstrate how our model performs in comparison to others when facing real-world human prompt.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">For each prompt, we generate one response from each model respectively<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We generate using nucleus sampling with <math alttext="p" class="ltx_Math" display="inline" id="footnote6.m1.1"><semantics id="footnote6.m1.1b"><mi id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="footnote6.m1.1c"><ci id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m1.1d">p</annotation><annotation encoding="application/x-llamapun" id="footnote6.m1.1e">italic_p</annotation></semantics></math>=0.85, <math alttext="k" class="ltx_Math" display="inline" id="footnote6.m2.1"><semantics id="footnote6.m2.1b"><mi id="footnote6.m2.1.1" xref="footnote6.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="footnote6.m2.1c"><ci id="footnote6.m2.1.1.cmml" xref="footnote6.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="footnote6.m2.1e">italic_k</annotation></semantics></math>=50 and temperature=0.9.</span></span></span>. Annotators are then presented with the prompt and two responses: one generated by <span class="ltx_text ltx_font_typewriter" id="S6.SS2.p2.1.1">CQIA</span> model and one by another baseline model. Subsequently, we ask which response the annotator prefers, allowing for a "tie" selection when a better response is hard to judge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.18058v1#S5.F4" title="Figure 4 ‣ 5.2 Implementation Details ‣ 5 EXPERIMENTAL SETUP ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning"><span class="ltx_text ltx_ref_tag">4</span></a> shows human evaluation results with <span class="ltx_text ltx_font_typewriter" id="S6.SS2.p3.1.1">CQIA</span> and other 5 baselines, namely Yi-6B-Chat, Baichuan2-7B-Chat, ChatGLM2-6B, Qwen-7B-Chat and InternLM-7B-Chat. The results demonstrate that, compared to strong baselines, CQIA-Subset achieved higher human preference, with at least over 60% of responses being better than or on par with the baseline models. This can be attributed to CQIA not solely in generating high-quality responses to human questions or instructions, but also in its responses being more aligned with real-world human communication patterns, leading to higher human preference.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Scaling Model Size</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">We investigate the performance of different base model with varying parameter sizes after fine-tuned on our CQIA-Subset.
Notably, Yi-6B surpasses Qwen-14B and InternLM-20B, which have at least twice its parameter size.
Further, Yi-34B achieved comparable results to Qwen-72B in both C-Eval and CMMLU benchmarks.
This observervation underscores the balance between model size, architectural optimizations, and training methodologies. While the scaling law might suggest that larger models inherently perform better due to their increased language understanding capacity, our results indicate that this is not always the case. Specifically, the Yi-6B model’s superior performance against models with significantly more parameters challenges the notion that parameter count alone is a sufficient predictor of model efficacy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Safety</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">We explore the impact of data sources on model safety by evaluating our models on SafetyBench. Models trained on CQIA-Subset scores the highest within CQIA series, surpassing GPT-3.5-turbo-0613. Model trained on Social Media&amp; Forums such as Douban, Zhihu, and Xhs perform moderate safety scores, we conjecture this is due to the diversity and openness of social media content, which also highlights the risks of harmful information. Additionally, models trained on Wiki-style data tend to perform lower safety scores, potentially reflecting the limited diversity within professional data sources, leading to poor performance on safety issues outside specialty domains.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we introduce a a high-quality Chinese instruction fine-tuning dataset. COIG-CQIA focuses on creating a dataset from Chinese internet sources including Q&amp;A and articles. These are deeply cleansed, restructured, and manually reviewed to ensure quality, diversity, and relevance. This dataset is designed to provide the Chinese NLP community with high-quality and human interaction-aligned instruction fine-tuning data.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2309.16609</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.08701" title="">Alpagasus: Training a better alpaca with fewer data</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Journal of Machine Learning Research</em>, 24(240):1–113.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, Shayne Longpre, Barret Zoph, Yi&nbsp;Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang&nbsp;Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed&nbsp;H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc&nbsp;V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.11416" title="">Scaling instruction-finetuned language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CLUEbenchmark (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
CLUEbenchmark. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/CLUEbenchmark/pCLUE" title="">pclue: Large-scale prompt-based dataset for multi-task and zero-shot learning in chinese</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm" title="">Free dolly: Introducing the world’s first truly open instruction-tuned llm</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2301.07597" title="">How close is chatgpt to human experts? comparison corpus, evaluation, and detection</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He and Choi (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Han He and Jinho&nbsp;D Choi. 2021.

</span>
<span class="ltx_bibblock">The stem cell hypothesis: Dilemma behind multi-task learning with transformer encoders.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2109.06939</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Or&nbsp;Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.09689" title="">Unnatural instructions: Tuning language models with (almost) no human labor</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivison et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A. Smith, Iz&nbsp;Beltagy, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.10702" title="">Camels in a changing climate: Enhancing lm adaptation with tulu 2</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.14742" title="">Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konchakov et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;A. Konchakov, A.&nbsp;S. Makarov, G.&nbsp;V. Afonin, J.&nbsp;C. Qiao, M.&nbsp;G. Vasin, N.&nbsp;P. Kobelev, and V.&nbsp;A. Khonik. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.00475" title="">Critical behavior of the fluctuation heat capacity near the glass transition of metallic glasses</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2308.06259" title="">Self-alignment with instruction backtranslation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lou et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu&nbsp;Su, and Wenpeng Yin. 2023.

</span>
<span class="ltx_bibblock">Muffin: Curating multi-faceted instructions for improving instruction following.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2104.08773" title="">Cross-task generalization via natural language crowdsourcing instructions</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.

</span>
<span class="ltx_bibblock">Instruction tuning with gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2304.03277</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen&nbsp;H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven&nbsp;Le Scao, Arun Raja, Manan Dey, M&nbsp;Saiful Bari, Canwen Xu, Urmish Thakker, Shanya&nbsp;Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng&nbsp;Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason&nbsp;Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander&nbsp;M. Rush. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2110.08207" title="">Multitask prompted training enables zero-shot task generalization</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei, Zhenzhong Lan, and Yue Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.19651" title="">Dynamics of instruction tuning: Each ability of large language models has its own growth pace</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke&nbsp;Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023.

</span>
<span class="ltx_bibblock">Moss: Training conversational language models from synthetic data.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
InternLM Team. 2023.

</span>
<span class="ltx_bibblock">Internlm: A multilingual language model with progressively enhanced capabilities.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/InternLM/InternLM" title="">https://github.com/InternLM/InternLM</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah&nbsp;A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.10560" title="">Self-instruct: Aligning language models with self-generated instructions</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu&nbsp;Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.12244" title="">Wizardlm: Empowering large language models to follow complex instructions</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.01196" title="">Baize: An open-source chat model with parameter-efficient tuning on self-chat data</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianxin Yang. 2023.

</span>
<span class="ltx_bibblock">Firefly(流萤): 中文对话式大语言模型.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yangjianxin1/Firefly" title="">https://github.com/yangjianxin1/Firefly</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge&nbsp;Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et&nbsp;al. 2024.

</span>
<span class="ltx_bibblock">Yi: Open foundation models by 01. ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2403.04652</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ge&nbsp;Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu&nbsp;Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.07987" title="">Chinese open instruction generalist: A preliminary release</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2308.10792</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.11206" title="">Lima: Less is more for alignment</a>.

</span>
</li>
</ul>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
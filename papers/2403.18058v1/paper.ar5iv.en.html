<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.18058] COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning</title><meta property="og:description" content="Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language.
These advancements have enabled these LLMs to understand and execute complex instruction…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.18058">

<!--Generated on Fri Apr  5 17:34:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id1.1.id1" class="ltx_text ltx_font_bold">Yuelin Bai<sup id="id1.1.id1.1" class="ltx_sup"><span id="id1.1.id1.1.1" class="ltx_text ltx_font_medium">1</span></sup></span> <span id="id2.2.id2" class="ltx_text ltx_font_bold">Xinrun Du<sup id="id2.2.id2.1" class="ltx_sup"><span id="id2.2.id2.1.1" class="ltx_text ltx_font_medium">2</span></sup><span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex1.1.1.1" class="ltx_text ltx_font_medium">1</span></span></span></span></span></span> <span id="id3.3.id3" class="ltx_text ltx_font_bold">Yiming Liang<sup id="id3.3.id3.1" class="ltx_sup"><span id="id3.3.id3.1.1" class="ltx_text ltx_font_medium">3</span></sup><span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex2.1.1.1" class="ltx_text ltx_font_medium">1</span></span></span></span></span></span> <span id="id4.4.id4" class="ltx_text ltx_font_bold">Yonggang Jin<sup id="id4.4.id4.1" class="ltx_sup"><span id="id4.4.id4.1.1" class="ltx_text ltx_font_medium">2</span></sup><span id="footnotex3" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex3.1.1.1" class="ltx_text ltx_font_medium">1</span></span></span></span></span>
<br class="ltx_break">Ziqiang Liu<sup id="id4.4.id4.2" class="ltx_sup"><span id="id4.4.id4.2.1" class="ltx_text ltx_font_medium">1</span></sup></span> <span id="id5.5.id5" class="ltx_text ltx_font_bold">Junting Zhou<sup id="id5.5.id5.1" class="ltx_sup"><span id="id5.5.id5.1.1" class="ltx_text ltx_font_medium">4,2</span></sup></span> <span id="id6.6.id6" class="ltx_text ltx_font_bold">Tianyu Zheng<sup id="id6.6.id6.1" class="ltx_sup"><span id="id6.6.id6.1.1" class="ltx_text ltx_font_medium">2</span></sup></span> <span id="id7.7.id7" class="ltx_text ltx_font_bold">Xincheng Zhang<sup id="id7.7.id7.1" class="ltx_sup"><span id="id7.7.id7.1.1" class="ltx_text ltx_font_medium">5</span></sup></span> <span id="id8.8.id8" class="ltx_text ltx_font_bold">Nuo Ma<sup id="id8.8.id8.1" class="ltx_sup"><span id="id8.8.id8.1.1" class="ltx_text ltx_font_medium">6</span></sup>
<br class="ltx_break">Zekun Wang<sup id="id8.8.id8.2" class="ltx_sup"><span id="id8.8.id8.2.1" class="ltx_text ltx_font_medium">2</span></sup></span> <span id="id9.9.id9" class="ltx_text ltx_font_bold">Ruibin Yuan<sup id="id9.9.id9.1" class="ltx_sup"><span id="id9.9.id9.1.1" class="ltx_text ltx_font_medium">7,2</span></sup></span> <span id="id10.10.id10" class="ltx_text ltx_font_bold">Haihong Wu<sup id="id10.10.id10.1" class="ltx_sup"><span id="id10.10.id10.1.1" class="ltx_text ltx_font_medium">5</span></sup></span> <span id="id11.11.id11" class="ltx_text ltx_font_bold">Hongquan Lin<sup id="id11.11.id11.1" class="ltx_sup"><span id="id11.11.id11.1.1" class="ltx_text ltx_font_medium">5</span></sup></span> <span id="id12.12.id12" class="ltx_text ltx_font_bold">Wenhao Huang<sup id="id12.12.id12.1" class="ltx_sup"><span id="id12.12.id12.1.1" class="ltx_text ltx_font_medium">6</span></sup>
<br class="ltx_break">Jiajun Zhang<sup id="id12.12.id12.2" class="ltx_sup"><span id="id12.12.id12.2.1" class="ltx_text ltx_font_medium">3</span></sup></span> <span id="id13.13.id13" class="ltx_text ltx_font_bold">Wenhu Chen<sup id="id13.13.id13.1" class="ltx_sup"><span id="id13.13.id13.1.1" class="ltx_text ltx_font_medium">8,9,2</span></sup></span> <span id="id14.14.id14" class="ltx_text ltx_font_bold">Chenghua Lin<sup id="id14.14.id14.1" class="ltx_sup"><span id="id14.14.id14.1.1" class="ltx_text ltx_font_medium">10,2</span></sup></span> <span id="id15.15.id15" class="ltx_text ltx_font_bold">Jie Fu<sup id="id15.15.id15.1" class="ltx_sup"><span id="id15.15.id15.1.1" class="ltx_text ltx_font_medium">7,2</span></sup></span> <span id="id16.16.id16" class="ltx_text ltx_font_bold">Min Yang<sup id="id16.16.id16.1" class="ltx_sup"><span id="id16.16.id16.1.1" class="ltx_text ltx_font_medium">1</span></sup>
<br class="ltx_break">Shiwen Ni<sup id="id16.16.id16.2" class="ltx_sup"><span id="id16.16.id16.2.1" class="ltx_text ltx_font_medium">1</span></sup></span> <span id="id17.17.id17" class="ltx_text ltx_font_bold">Ge Zhang<sup id="id17.17.id17.1" class="ltx_sup"><span id="id17.17.id17.1.1" class="ltx_text ltx_font_medium">8,9</span></sup><span id="footnotex4" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex4.1.1.1" class="ltx_text ltx_font_medium">2</span></span></span></span></span>
<br class="ltx_break"><sup id="id17.17.id17.2" class="ltx_sup"><span id="id17.17.id17.2.1" class="ltx_text ltx_font_medium">1</span></sup></span>Shenzhen Institute of Advanced Technology, CAS <sup id="id18.18.id18" class="ltx_sup">2</sup>M-A-P  <sup id="id19.19.id19" class="ltx_sup">3</sup>Institute of Automation, CAS
<br class="ltx_break"><sup id="id20.20.id20" class="ltx_sup">4</sup>Peking University <sup id="id21.21.id21" class="ltx_sup">5</sup>University of Science and Technology of China <sup id="id22.22.id22" class="ltx_sup">6</sup>01.ai <sup id="id23.23.id23" class="ltx_sup">7</sup>HKUST
<br class="ltx_break"><sup id="id24.24.id24" class="ltx_sup">8</sup>University of Waterloo <sup id="id25.25.id25" class="ltx_sup">9</sup>Vector Institute <sup id="id26.26.id26" class="ltx_sup">10</sup>University of Manchester 

<br class="ltx_break">
</span><span class="ltx_author_notes">Equal contributionCorresponding author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id27.id1" class="ltx_p">Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language.
These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency.
However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning.
The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users.
To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset.
Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions.
To this end, we collect a high-quality human-written corpus from various sources on the Chinese Internet, including Q&amp;A communities, Wikis, examinations, and existing NLP datasets.
This corpus was rigorously filtered and carefully processed to form the COIG-CQIA dataset.
Furthermore, we train models of various scales on different subsets of CQIA, following in-depth evaluation and analyses.
The findings from our experiments offer valuable insights for selecting and developing Chinese instruction-tuning datasets.
We also find that models trained on CQIA-Subset achieve competitive results in human assessment as well as knowledge and security benchmarks. Data are available at <a target="_blank" href="https://huggingface.co/datasets/m-a-p/COIG-CQIA" title="" class="ltx_ref ltx_href">https://huggingface.co/datasets/m-a-p/COIG-CQIA</a></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Large Language Models (LLMs), such as GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite>, and PaLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, have demonstrated remarkable capabilities as general-purpose assistants.
The cornerstone of this achievement is instruction tuning, which significantly enhances the capabilities and controllability of LLMs through training on datasets composed of instruction-output pairs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib29" title="" class="ltx_ref">2023b</a>)</cite>.
This technique effectively aligns the models’ training objectives with human intentions, thereby ensuring that the models can interpret and execute human instructions both effectively and safely.
Therefore, the availability of high-quality instruction tuning datasets is crucial for LLMs to operate as efficient and dependable assistants.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">There exists many English instruction tuning datasets.
However, the available datasets for Chinese instruction tuning are generally either limited in size or lacking in quality.
Chinese instruction tuning datasets are categorized into three main types:
(1) datasets derived from English instruction datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Peng et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> or NLP datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(CLUEbenchmark, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>; Yang, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>,
(2) datasets generated by LLMs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Guo et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, and
(3) self-generated instruction tuning datasets &nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>; Sun et&nbsp;al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>.
COIG&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite> integrates multiple approaches to construct a human-verified universal high-quality Chinese instruction corpus.
However, the previously mentioned Chinese instruction tuning datasets have inherent issues such as not aligning with natural Chinese communication patterns, lacking genuine Chinese linguistic data, containing numerous problematic data points, and having small-scale data.
This paper focuses on constructing a Chinese instruction tuning dataset sourced from authentic Chinese linguistic data across diverse domains and undergone meticulous manual cleaning procedures aimed at enhancing the proficiency of LLMs in following Chinese instructions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we introduce COIG-<span id="S1.p3.1.1" class="ltx_text ltx_font_bold">CQIA</span>&nbsp;(<span id="S1.p3.1.2" class="ltx_text ltx_font_bold">Chinese</span> Open Instruction Generalist - <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">Q</span>uality <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">I</span>s <span id="S1.p3.1.5" class="ltx_text ltx_font_bold">A</span>ll You Need), a high-quality Chinese instruction tuning dataset, which is designed to provide the Chinese NLP community with high-quality and human interaction-aligned instruction fine-tuning data.
Inspired by the work of LIMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>, COIG-CQIA focuses on curating a dataset from Chinese internet sources, comprising Q&amp;A sessions and articles.
These sources undergo thorough cleaning, restructuring, and manual review to ensure high quality, diversity, and relevance.
Furthermore, we conduct analytical experiments to assess the effects of data quality, provenance, and mixing ratio.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In summary, the contributions are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a high-quality Chinese instruction fine-tuning dataset, specifically designed to align with human interaction, achieved through rigorous filtering procedures.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We explore the influence of various data sources, including social media, encyclopedias, and traditional NLP tasks, on model performance. Our analysis offers essential insights for selecting training data from the Chinese internet.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Various benchmark tests and human evaluations confirm that models fine-tuned on our CQIA dataset exhibit superior performance, thus establishing CQIA as a valuable resource for the Chinese NLP community.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Instruction Tuning Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Instruction tuning aims to train large language models to generate responses that align with input instructions, thereby enable LLMs with conversational and task execution capabilities. Compared with standard LLM, SFT allows the behavior of the model to be more controllable and predictable, thereby achieving the purpose of aligning human will. Methods to build instruction tuning datasets include: (1) Pure manual annotation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Conover et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>. This method completely constructs instructions and answers manually, which is very time-consuming and laborious; (2) Converted from existing datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mishra et&nbsp;al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>; Sanh et&nbsp;al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>; Chung et&nbsp;al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>. Some studies use supervised data sets from NLP tasks to construct instruction tuning data; (3) Automatically generated using LLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Honovich et&nbsp;al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>; Wang et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>; Xu et&nbsp;al., <a href="#bib.bib24" title="" class="ltx_ref">2023a</a>; Ji et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>; Xu et&nbsp;al., <a href="#bib.bib25" title="" class="ltx_ref">2023b</a>)</cite>. Others use existing LLMs to generate instruction tuning data. A common practice is to first manually annotate high-quality seed datasets, and then use LLM to expand the seed instructions and corresponding outputs, which can generate large-scale instruction tuning data with very little human annotation. However, the quality cannot be guaranteed and there is a certain amount of noisy data, which can lead to hallucinations.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">There exists many English instruction tuning datasets. In comparison, existing Chinese instruction tuning datasets are either small in scale or have quality issues. Some studies have translated the English instruction tuning datasets into Chinese&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Peng et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>, but this may lead to accumulation of translation errors. pCLUE&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(CLUEbenchmark, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite> and Firefly&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yang, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> transform the original NLP task dataset into instruction tuning datasets. HC3&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Guo et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> collects tens of thousands of comparison responses from both human experts and ChatGPT. COIG&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite> builds a human-verified universal high-quality Chinese instruction corpus.
BELLE&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and MOSS&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Sun et&nbsp;al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> use a method similar to self-instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite> to automatically generate Chinese instruction tuning datasets.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Mixture of SFT</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Currently, more and more studies has begun to pay attention to the importance of data quality of instruction tuning. LIMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite> only uses 1,000 high-quality instructions and outputs for SFT, and does not even need to perform RLHF training to achieve very strong performance. AlpaGasus&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> uses powerful LLM to automatically identify and filter low-quality data, resulting in high-quality instruction tuning data to improve performance and training speed. Humpback&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> filters out high-quality samples to fine-tune a more powerful LLM.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Others&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Song et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> explores the impact of the mixture strategies of different instruction tuning datasets. Tulu series&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Konchakov et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>; Ivison et&nbsp;al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> show that increasing instruction diversity can effectively improve the performance and different instruction tuning datasets can discover or enhance specific skills, while no one dataset (or combination) provides the best performance across all assessments.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>CQIA CURATION</h2>

<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:250.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(14.6pt,-8.4pt) scale(1.0720249580794,1.0720249580794) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Source</span></th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Quantity</span></th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Source</span></th>
<th id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Quantity</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Zhihu</td>
<td id="S3.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">8837</td>
<td id="S3.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Douban</td>
<td id="S3.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">3132</td>
</tr>
<tr id="S3.T1.1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.1.3.2.1" class="ltx_td ltx_align_center">Xiaohongshu</td>
<td id="S3.T1.1.1.3.2.2" class="ltx_td ltx_align_center">1508</td>
<td id="S3.T1.1.1.3.2.3" class="ltx_td ltx_align_center">Segment Fault</td>
<td id="S3.T1.1.1.3.2.4" class="ltx_td ltx_align_center">458</td>
</tr>
<tr id="S3.T1.1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.1.4.3.1" class="ltx_td ltx_align_center">Encyclopedia Article</td>
<td id="S3.T1.1.1.4.3.2" class="ltx_td ltx_align_center">980</td>
<td id="S3.T1.1.1.4.3.3" class="ltx_td ltx_align_center">Encyclopedia of China</td>
<td id="S3.T1.1.1.4.3.4" class="ltx_td ltx_align_center">1706</td>
</tr>
<tr id="S3.T1.1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.1.5.4.1" class="ltx_td ltx_align_center">WikiHow</td>
<td id="S3.T1.1.1.5.4.2" class="ltx_td ltx_align_center">1876</td>
<td id="S3.T1.1.1.5.4.3" class="ltx_td ltx_align_center">COIG PC</td>
<td id="S3.T1.1.1.5.4.4" class="ltx_td ltx_align_center">3000</td>
</tr>
<tr id="S3.T1.1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.1.6.5.1" class="ltx_td ltx_align_center">Middle school Exam</td>
<td id="S3.T1.1.1.6.5.2" class="ltx_td ltx_align_center">2000</td>
<td id="S3.T1.1.1.6.5.3" class="ltx_td ltx_align_center">Graduate Entrance Examination</td>
<td id="S3.T1.1.1.6.5.4" class="ltx_td ltx_align_center">475</td>
</tr>
<tr id="S3.T1.1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.1.7.6.1" class="ltx_td ltx_align_center">Logi QA</td>
<td id="S3.T1.1.1.7.6.2" class="ltx_td ltx_align_center">422</td>
<td id="S3.T1.1.1.7.6.3" class="ltx_td ltx_align_center">CValue</td>
<td id="S3.T1.1.1.7.6.4" class="ltx_td ltx_align_center">906</td>
</tr>
<tr id="S3.T1.1.1.8.7" class="ltx_tr">
<td id="S3.T1.1.1.8.7.1" class="ltx_td ltx_align_center">COIG-Human-Value</td>
<td id="S3.T1.1.1.8.7.2" class="ltx_td ltx_align_center">101</td>
<td id="S3.T1.1.1.8.7.3" class="ltx_td ltx_align_center">Chinese Traditional</td>
<td id="S3.T1.1.1.8.7.4" class="ltx_td ltx_align_center">232</td>
</tr>
<tr id="S3.T1.1.1.9.8" class="ltx_tr">
<td id="S3.T1.1.1.9.8.1" class="ltx_td ltx_align_center">Idiom Explanation</td>
<td id="S3.T1.1.1.9.8.2" class="ltx_td ltx_align_center">112</td>
<td id="S3.T1.1.1.9.8.3" class="ltx_td ltx_align_center">Poem Writing</td>
<td id="S3.T1.1.1.9.8.4" class="ltx_td ltx_align_center">47</td>
</tr>
<tr id="S3.T1.1.1.10.9" class="ltx_tr">
<td id="S3.T1.1.1.10.9.1" class="ltx_td ltx_align_center">Classical Chinese Translation</td>
<td id="S3.T1.1.1.10.9.2" class="ltx_td ltx_align_center">112</td>
<td id="S3.T1.1.1.10.9.3" class="ltx_td ltx_align_center">MBA Encyclopedia</td>
<td id="S3.T1.1.1.10.9.4" class="ltx_td ltx_align_center">10689</td>
</tr>
<tr id="S3.T1.1.1.11.10" class="ltx_tr">
<td id="S3.T1.1.1.11.10.1" class="ltx_td ltx_align_center">Finance NLP Task</td>
<td id="S3.T1.1.1.11.10.2" class="ltx_td ltx_align_center">600</td>
<td id="S3.T1.1.1.11.10.3" class="ltx_td ltx_align_center">Medical Encyclopedia</td>
<td id="S3.T1.1.1.11.10.4" class="ltx_td ltx_align_center">8351</td>
</tr>
<tr id="S3.T1.1.1.12.11" class="ltx_tr">
<td id="S3.T1.1.1.12.11.1" class="ltx_td ltx_align_center">Medical Article</td>
<td id="S3.T1.1.1.12.11.2" class="ltx_td ltx_align_center">186</td>
<td id="S3.T1.1.1.12.11.3" class="ltx_td ltx_align_center">Law</td>
<td id="S3.T1.1.1.12.11.4" class="ltx_td ltx_align_center">2645</td>
</tr>
<tr id="S3.T1.1.1.13.12" class="ltx_tr">
<td id="S3.T1.1.1.13.12.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Total</td>
<td id="S3.T1.1.1.13.12.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S3.T1.1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">48375</td>
<td id="S3.T1.1.1.13.12.4" class="ltx_td ltx_border_bb ltx_border_t"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
The amount of data from different sources of the dataset mixture
</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To ensure the quality and diversity of our data, we manually selected 13 data sources from high-quality websites and data resources within the Chinese Internet. These sources include community Q&amp;A forums, encyclopedic sites, content creation platforms, examinations, etc. We also incorporated high-quality Chinese NLP datasets to enrich the diversity of tasks. Specifically, we categorized all data sources into four types: Social Media &amp; forums, World Knowledge, NLP tasks, and Examinations. The data sources and their descriptions are as follows.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Social Media &amp; Forums</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Zhihu</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">is a vibrant question-and-answer platform where users can ask and answer questions on a wide range of topics, making it a comprehensive repository of knowledge and insights. Zhihu encourages its users to provide well-thought-out answers that are informative and reflective of expert knowledge or personal experience. However, the absence of a review mechanism for answers on Zhihu leads to a large volume of content that falls short of our quality standards. To filter low quality answers, we selected answers with more than 50 upvotes, then filtering out content containing sensitive or harmful keywords using a rule-based method. Subsequently, we employed GPT-4 to score the responses on a scale of 1-10, retaining those with scores above 8.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SegmentFault</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">is a question-and-answer community focused on IT technology, providing Chinese developers with a high-quality platform for exchange, similar to Stack Overflow. In this community, users engage in asking and answering questions related to IT technology, where the questioner can accept the most useful answer. Additionally, community members can also upvote or comment on answers. Our data are collected from the contents posted before 2018, as earlier content may become outdated due to changes in programming languages or software versions. We then select the "accepted" answers with at least 5 upvotes. Furthermore, we manually review all the (question, answer) pairs to remove or modify low-quality content.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Douban</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">is a social network and database that allows users to create content related to literature and artistic works such as films, books, TV series, music, etc. We sample data from books, movies, and TV series, extracting metadata that includes ratings, detailed information on actors/crew, and long reviews. Then, we design three tasks in total: synopsis generation, review generation, and recommendations. For each task, we manually design various prompt templates and used these templates in combination with metadata to construct instructions. For synopsis generation and review generation, we construct instructions using prompt templates combined with movie or TV series names, with responses generated by Douban users. Then we remove responses with lengths shorter than a threshold and delete personal information and irrelevant content(e.g., "Subscribe our Official Accounts"). Additionally, we manually adjusted some instructions to add more complex implicit intents, aligning better with the details of the response.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Xiaohongshu</h5>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">provides a space for users to share their lives, travel, food, and product recommendations. Contents in this platform are renowned on the Chinese internet for their unique and expressive style. We sample posts with lengths ranging from 500 to 2000, excluding those that involve interactions with other users ("@User_Name") and those referencing images or videos ("as shown in the picture/video").</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Ruozhiba</h5>

<div id="S3.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px5.p1.1" class="ltx_p">is a sub-forum of Baidu Tieba, an interests-based community forum. Its posts often contain puns, polysemous terms, causal reversals, and homophones, many of which are designed with logical traps, posing challenges even for humans. We collected the 500 most upvoted threads. Using the titles as instructions, we eliminate those that were either non-instructive (i.e., declarative statements or unanswerable) or toxic. Responses were generated by either humans or GPT-4. We conducted manual reviews for GPT4’s responses to ensure accuracy, ultimately obtaining 240 (instruction, response) pairs.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>World Knowledge</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>General Encyclopedia</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">General Encyclopedia provides comprehensive coverage of a wide range of topics across various fields. We collect data from three Chinese Encyclopedia websites: One Hundred Thousand Whys<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://10why.net/</span></span></span>, wikiHow-zh<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://zh.wikihow.com</span></span></span> and Encyclopedia of China<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.zgbk.com/</span></span></span>. <span id="S3.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">One Hundred Thousand Whys</span> is an encyclopedic website aimed at popular science, featuring thousands of high-quality articles asking "why" across topics from natural science to humanities. We collect data from all 15 categories and ensure uniform distribution across each category. Article titles are used as instruction(e.g."Why don’t I get altitude sickness when I fly?"), and the content as responses, with responses under 300 characters being filtered out.
<span id="S3.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_bold">wikiHow-zh</span>, the Chinese version of wikiHow, is an encyclopedia-style website covering a wide range of topics, featuring tens of thousands of "how-to" articles with multiple revisions. We collected xxx articles from the site and sampled 1500 entries from all 19 categories, with a sampling temperature of 3. Since the original data are in HTML, we parse the HTML and concatenate the article content using Markdown. Subsequently, we filtered out low-quality data (e.g., incorrect formula conversions) and articles exceeding 3000 words in length. We use titles as the instructions and the article contents as responses.
<span id="S3.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_bold">Encyclopedia of China</span> is a comprehensive encyclopedia comprising approximately 500,000 entries, authored and revised by domain experts. We design various prompt templates for concept explanation tasks. We sample the entries from all 74 categories, with structures comprising entry names and several subtitles, along with their respective contents. We randomly combined entry names or subtitles with prompt templates to construct instructions. For instance, for the "Confucius" entry
, which includes subtitles "<span id="S3.SS2.SSS1.p1.1.4" class="ltx_text ltx_font_typewriter">Biography</span>", "<span id="S3.SS2.SSS1.p1.1.5" class="ltx_text ltx_font_typewriter">Academic Theories</span>", and "<span id="S3.SS2.SSS1.p1.1.6" class="ltx_text ltx_font_typewriter">Impacts</span>", we selected "<span id="S3.SS2.SSS1.p1.1.7" class="ltx_text ltx_font_typewriter">Academic Theories</span>" to create instruction such as "<span id="S3.SS2.SSS1.p1.1.8" class="ltx_text ltx_font_typewriter">Write the details of Confucius’s academic theories.</span>" and then, we use this subtitle’s content as a response.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Domain Specific Knowledge</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">We collected data from four specific domains: medicine, economic management, electronics, and agriculture.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p"><span id="S3.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Medical Domain</span> sources from three websites: Baobaozhidao, Qianwen Health, and Baikemingyi. Both <span id="S3.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_bold">Baobaozhidao</span> and <span id="S3.SS2.SSS2.p2.1.3" class="ltx_text ltx_font_bold">Qianwen Health</span> feature question-and-answer style articles written by medical experts, with the former primarily focusing on a broad range of medical fields, while the latter on maternal and infant health. We collected articles from these two sites, excluding those whose titles are not in question form. Subsequently, we used the titles as instructions and the article content as responses. <span id="S3.SS2.SSS2.p2.1.4" class="ltx_text ltx_font_bold">Baikemingyi</span> contains Wikipedia-style structured data, featuring introductions to tens of thousands of diseases and medications. We designed various prompt templates and combined entry names with these templates to construct commands (e.g., "<span id="S3.SS2.SSS2.p2.1.5" class="ltx_text ltx_font_typewriter">Write a professional introduction about joint pain</span>").</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p"><span id="S3.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Economic Management Domain</span> data is collected from MBA Wiki Encyclopedia, a website that encompasses Wikipedia-style structured knowledge, authored and revised by numerous contributors. We designed various prompt templates, combining entry names with random templates to construct instructions, such as "<span id="S3.SS2.SSS2.p3.1.2" class="ltx_text ltx_font_typewriter">Please explain the following term in detail: Remittance Agent</span>". Ultimately, the content of the entries is concatenated and constructed into responses in markdown format.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p"><span id="S3.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Electronics Domain</span> data is sourced from the EETrees electronic encyclopedia, which is also structured in form. We design various prompt templates and combine these with entry names to construct instructions, with the corresponding content as the response.</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS2.p5.1" class="ltx_p"><span id="S3.SS2.SSS2.p5.1.1" class="ltx_text ltx_font_bold">Agriculture Domain</span> sources from an agricultural encyclopedia website, containing a range of topics from plant cultivation to animal breeding. We collected articles on all ten topics, excluding those with non-question titles, containing images, or shorter than 300 words in length. Subsequently, we construct (instruction, response) pairs from the titles and content of the articles.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Examinations</h3>

<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">The Middle School and College Entrance Examinations</h5>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">primarily derives from the COIG dataset<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite>, a harmless, helpful, and diverse Chinese Instruction dataset. Chinese examination is a subset of it, with the Middle School and College Entrance Examinations being China’s principal general competency tests. These data contain a variety of question types and detailed answer explanations, primarily covering humanities subjects (Chinese, English, Politics, Biology, History, and Geography). We use temperature sampling on the data across these subjects and then filtered out questions and answers with formatting errors. The questions were used as instructions and, the "answer" and "analysis" fields were concatenated to form extended responses, resulting in 1964 (instruction, response) pairs.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Graduate Entrance Examination</h5>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">is one of the most challenging examinations in China, exceeding college entrance exams in difficulty and requiring advanced knowledge application and depth. We have collected a variety of exam papers from recent years across disciplines including mathematics, computer science, chemistry, law, psychology, medicine, etc. Using Mathpix<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://mathpix.com/</span></span></span> for image-to-text conversion, we extracted questions and answers and converted them into LaTeX format. We eliminate data without analysis and manually verified the accuracy of the questions and answers. We eliminate data without analysis and manually verified the accuracy of the questions and answers.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Logical Reasoning Test</h5>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px3.p1.1" class="ltx_p">aims to assess the ability to apply logical and analytical reasoning to solve problems. This type of test is widely used in various competitive examinations to evaluate critical thinking and problem-solving skills. We collect logic reasoning questions from the internet, retaining those containing detailed answer analyses, and then construct them into (instruction, response) pairs.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Chinese Culture Test</h5>

<div id="S3.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px4.p1.1" class="ltx_p">investigates the mastery of traditional Chinese culture and history. We collected multiple-choice questions on traditional Chinese culture from the internet, retaining those with answer analyses, and constructed them into (instruction, response) pairs.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>NLP Datasets</h3>

<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">COIG-PC</h5>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px1.p1.1" class="ltx_p">The COIG-PC Dataset is a comprehensive collection of Chinese NLP tasks, aimed at advancing Chinese NLP research. The goal of this dataset is to provide a rich set of resources to researchers and developers, which they can use to improve the capabilities of language models in handling Chinese text. It offers a comprehensive suite of resources for researchers and developers, facilitating advancements in language model capabilities across various domains including text generation, information extraction, sentiment analysis, and machine translation, etc. Initially, we selected 1,413 tasks involving both Chinese and English languages from COIG-PC. Then, we manually select 250 tasks that meet our quality criteria, including information extraction, classification, summary, and others, primarily sourced from traditional NLP datasets. Through temperature sampling, we eventually sample 3,000 (instruction, response) pairs, which are further verified by human to ensure quality.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">COIG Human Value</h5>

<div id="S3.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px2.p1.1" class="ltx_p">is a subset of the COIG dataset<cite class="ltx_cite ltx_citemacro_cite">Zhang et&nbsp;al. (<a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite> designed to provide instruction fine-tuning data aligned with human values. We selected the portion reflecting Chinese cultural values, constructed using the Self-Instruct<cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite> method from manually selected seed instructions. We manually filtered out data with formatting errors and incorrect answers, retaining those that include explanations of the answers to form (instruction, response) pairs.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Firefly Chinese Traditional</h5>

<div id="S3.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px3.p1.1" class="ltx_p">comprises three tasks: Classical Chinese Translation, Ancient Poetry Writing, and Idiom Interpretation, which are the subset of the Firefly dataset<cite class="ltx_cite ltx_citemacro_cite">Yang (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> related to traditional Chinese culture. We filter the responses shorter than 300 characters, and sample 300 instances from each task. Then, we manually filtered out low-quality data such as instruction-response mismatch, response error, and unanswerable instructions.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">100PoisonMpts</h5>

<div id="S3.SS4.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px4.p1.1" class="ltx_p">addressing issues of anti-discrimination and empathy, spans various dimensions including jurisprudence, psychology, child education, obscure facts, intimate relationships, etc. It involves human-generated prompts that evoke bias and discrimination, followed by expert-crafted responses that align with human values. To enhance the harmlessness of the CQIA, we sample all the data from 100PoisonMpts.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2403.18058/assets/fig/self-instruct-final.png" id="S3.F1.g1" class="ltx_graphics ltx_img_square" width="598" height="567" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Most common root verbs (inner circle) and top direct noun objects (outer circle) in the CQIA dataset. Note that we only visualize when a certain verb-noun pair has more than 30 instances, and many instructions do not contain a verb-noun structure.
</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2403.18058/assets/fig/overview_cqia.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="370" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of CQIA Task Types.</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2403.18058/assets/fig/dis-1.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="396" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Length distribution of instruction and responses. Note that the instruction is the concatenation of original instructions and inputs in our dataset.
</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data Analysis</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Statistics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> describe the data statistics for all sources. We collected a total of 48,375 instances from 22 sources within the Chinese Internet and Community, covering domains ranging from general knowledge and STEM to humanities. Figure <a href="#S3.F2" title="Figure 2 ‣ 100PoisonMpts ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates the variety of task types, encompassing information extraction, question answering, code generation, etc. We demonstrated the distribution in the length of the instructions and responses in Figure<a href="#S3.F3" title="Figure 3 ‣ 100PoisonMpts ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Diversity</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To analyze the diversity of the COIG-CQIA dataset, we follow prior work<cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a href="#bib.bib23" title="" class="ltx_ref">2023</a>); Lou et&nbsp;al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> by employing the Hanlp tool<cite class="ltx_cite ltx_citemacro_cite">He and Choi (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite> to parse the instructions and then extract the verb closest to the root along with its top direct noun object. We then plot the top 20 most common root verbs and their corresponding direct noun objects in Figure<a href="#S3.F1" title="Figure 1 ‣ 100PoisonMpts ‣ 3.4 NLP Datasets ‣ 3 CQIA CURATION ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. From this figure we can observe that CQIA features a diverse range of instructions and intentions.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:146.2pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-131.5pt,44.2pt) scale(0.622432424523685,0.622432424523685) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Source</span></th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Open QA</span></th>
<th id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Brainstorming</span></th>
<th id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Classification</span></th>
<th id="S4.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Generation</span></th>
<th id="S4.T2.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Summarization</span></th>
<th id="S4.T2.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.8.1" class="ltx_text ltx_font_bold">Rewrite</span></th>
<th id="S4.T2.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.9.1" class="ltx_text ltx_font_bold">Closed QA</span></th>
<th id="S4.T2.1.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.10.1" class="ltx_text ltx_font_bold">Extract</span></th>
<th id="S4.T2.1.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.11.1" class="ltx_text ltx_font_bold">Math</span></th>
<th id="S4.T2.1.1.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T2.1.1.1.1.12.1" class="ltx_text ltx_font_bold">Code</span></th>
<th id="S4.T2.1.1.1.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.13.1" class="ltx_text ltx_font_bold">Average</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<th id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="13"><span id="S4.T2.1.1.2.1.1.1" class="ltx_text">
<span id="S4.T2.1.1.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:25.7pt;vertical-align:-9.4pt;"><span class="ltx_transformed_inner" style="width:25.7pt;transform:translate(-9.43pt,0pt) rotate(-90deg) ;">
<span id="S4.T2.1.1.2.1.1.1.1.1" class="ltx_p"><span id="S4.T2.1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Yi-6B</span></span>
</span></span></span></th>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">SegmentFault</td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">26.3</td>
<td id="S4.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">33.6</td>
<td id="S4.T2.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">8.9</td>
<td id="S4.T2.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">23.5</td>
<td id="S4.T2.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">29.9</td>
<td id="S4.T2.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">20.1</td>
<td id="S4.T2.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">25.0</td>
<td id="S4.T2.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">22.6</td>
<td id="S4.T2.1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">14.5</td>
<td id="S4.T2.1.1.2.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.1</td>
<td id="S4.T2.1.1.2.1.13" class="ltx_td ltx_align_center ltx_border_t">23.7</td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_center">COIG PC</td>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center">31.4</td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center">47.7</td>
<td id="S4.T2.1.1.3.2.4" class="ltx_td ltx_align_center">28.7</td>
<td id="S4.T2.1.1.3.2.5" class="ltx_td ltx_align_center">44.8</td>
<td id="S4.T2.1.1.3.2.6" class="ltx_td ltx_align_center">43.4</td>
<td id="S4.T2.1.1.3.2.7" class="ltx_td ltx_align_center">53.3</td>
<td id="S4.T2.1.1.3.2.8" class="ltx_td ltx_align_center">45.5</td>
<td id="S4.T2.1.1.3.2.9" class="ltx_td ltx_align_center">28.4</td>
<td id="S4.T2.1.1.3.2.10" class="ltx_td ltx_align_center">35.6</td>
<td id="S4.T2.1.1.3.2.11" class="ltx_td ltx_align_center ltx_border_r">23.2</td>
<td id="S4.T2.1.1.3.2.12" class="ltx_td ltx_align_center">38.2</td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<td id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_center">Douban</td>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center">45.2</td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center">64.5</td>
<td id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_center">23.3</td>
<td id="S4.T2.1.1.4.3.5" class="ltx_td ltx_align_center">64.4</td>
<td id="S4.T2.1.1.4.3.6" class="ltx_td ltx_align_center">38.6</td>
<td id="S4.T2.1.1.4.3.7" class="ltx_td ltx_align_center">37.1</td>
<td id="S4.T2.1.1.4.3.8" class="ltx_td ltx_align_center">34.2</td>
<td id="S4.T2.1.1.4.3.9" class="ltx_td ltx_align_center">25.4</td>
<td id="S4.T2.1.1.4.3.10" class="ltx_td ltx_align_center">32.9</td>
<td id="S4.T2.1.1.4.3.11" class="ltx_td ltx_align_center ltx_border_r">42.1</td>
<td id="S4.T2.1.1.4.3.12" class="ltx_td ltx_align_center">40.8</td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<td id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_center">Zhihu</td>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_center">48.1</td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_center">72.3</td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_center">19.0</td>
<td id="S4.T2.1.1.5.4.5" class="ltx_td ltx_align_center">66.9</td>
<td id="S4.T2.1.1.5.4.6" class="ltx_td ltx_align_center">24.3</td>
<td id="S4.T2.1.1.5.4.7" class="ltx_td ltx_align_center">29.5</td>
<td id="S4.T2.1.1.5.4.8" class="ltx_td ltx_align_center">28.9</td>
<td id="S4.T2.1.1.5.4.9" class="ltx_td ltx_align_center">12.3</td>
<td id="S4.T2.1.1.5.4.10" class="ltx_td ltx_align_center">8.5</td>
<td id="S4.T2.1.1.5.4.11" class="ltx_td ltx_align_center ltx_border_r">40.0</td>
<td id="S4.T2.1.1.5.4.12" class="ltx_td ltx_align_center">35.0</td>
</tr>
<tr id="S4.T2.1.1.6.5" class="ltx_tr">
<td id="S4.T2.1.1.6.5.1" class="ltx_td ltx_align_center">Logi QA</td>
<td id="S4.T2.1.1.6.5.2" class="ltx_td ltx_align_center">45.0</td>
<td id="S4.T2.1.1.6.5.3" class="ltx_td ltx_align_center">65.4</td>
<td id="S4.T2.1.1.6.5.4" class="ltx_td ltx_align_center">32.9</td>
<td id="S4.T2.1.1.6.5.5" class="ltx_td ltx_align_center">55.3</td>
<td id="S4.T2.1.1.6.5.6" class="ltx_td ltx_align_center">37.1</td>
<td id="S4.T2.1.1.6.5.7" class="ltx_td ltx_align_center">49.5</td>
<td id="S4.T2.1.1.6.5.8" class="ltx_td ltx_align_center">47.1</td>
<td id="S4.T2.1.1.6.5.9" class="ltx_td ltx_align_center">38.9</td>
<td id="S4.T2.1.1.6.5.10" class="ltx_td ltx_align_center">17.9</td>
<td id="S4.T2.1.1.6.5.11" class="ltx_td ltx_align_center ltx_border_r">40.0</td>
<td id="S4.T2.1.1.6.5.12" class="ltx_td ltx_align_center">42.9</td>
</tr>
<tr id="S4.T2.1.1.7.6" class="ltx_tr">
<td id="S4.T2.1.1.7.6.1" class="ltx_td ltx_align_center">Ruozhiba</td>
<td id="S4.T2.1.1.7.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.7.6.2.1" class="ltx_text ltx_font_bold">64.8</span></td>
<td id="S4.T2.1.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.7.6.3.1" class="ltx_text ltx_framed ltx_framed_underline">84.6</span></td>
<td id="S4.T2.1.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.7.6.4.1" class="ltx_text ltx_font_bold">50.3</span></td>
<td id="S4.T2.1.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.7.6.5.1" class="ltx_text ltx_framed ltx_framed_underline">73.1</span></td>
<td id="S4.T2.1.1.7.6.6" class="ltx_td ltx_align_center">45.0</td>
<td id="S4.T2.1.1.7.6.7" class="ltx_td ltx_align_center">39.9</td>
<td id="S4.T2.1.1.7.6.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.7.6.8.1" class="ltx_text ltx_font_bold">57.0</span></td>
<td id="S4.T2.1.1.7.6.9" class="ltx_td ltx_align_center">30.3</td>
<td id="S4.T2.1.1.7.6.10" class="ltx_td ltx_align_center">27.3</td>
<td id="S4.T2.1.1.7.6.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.1.1.7.6.11.1" class="ltx_text ltx_font_bold">63.6</span></td>
<td id="S4.T2.1.1.7.6.12" class="ltx_td ltx_align_center">53.6</td>
</tr>
<tr id="S4.T2.1.1.8.7" class="ltx_tr">
<td id="S4.T2.1.1.8.7.1" class="ltx_td ltx_align_center">Wiki</td>
<td id="S4.T2.1.1.8.7.2" class="ltx_td ltx_align_center">51.0</td>
<td id="S4.T2.1.1.8.7.3" class="ltx_td ltx_align_center">67.6</td>
<td id="S4.T2.1.1.8.7.4" class="ltx_td ltx_align_center">21.5</td>
<td id="S4.T2.1.1.8.7.5" class="ltx_td ltx_align_center">60.4</td>
<td id="S4.T2.1.1.8.7.6" class="ltx_td ltx_align_center">30.8</td>
<td id="S4.T2.1.1.8.7.7" class="ltx_td ltx_align_center">31.5</td>
<td id="S4.T2.1.1.8.7.8" class="ltx_td ltx_align_center">30.2</td>
<td id="S4.T2.1.1.8.7.9" class="ltx_td ltx_align_center">21.4</td>
<td id="S4.T2.1.1.8.7.10" class="ltx_td ltx_align_center">22.7</td>
<td id="S4.T2.1.1.8.7.11" class="ltx_td ltx_align_center ltx_border_r">34.7</td>
<td id="S4.T2.1.1.8.7.12" class="ltx_td ltx_align_center">37.2</td>
</tr>
<tr id="S4.T2.1.1.9.8" class="ltx_tr">
<td id="S4.T2.1.1.9.8.1" class="ltx_td ltx_align_center">Finance</td>
<td id="S4.T2.1.1.9.8.2" class="ltx_td ltx_align_center">43.2</td>
<td id="S4.T2.1.1.9.8.3" class="ltx_td ltx_align_center">65.7</td>
<td id="S4.T2.1.1.9.8.4" class="ltx_td ltx_align_center">30.0</td>
<td id="S4.T2.1.1.9.8.5" class="ltx_td ltx_align_center">57.3</td>
<td id="S4.T2.1.1.9.8.6" class="ltx_td ltx_align_center">36.4</td>
<td id="S4.T2.1.1.9.8.7" class="ltx_td ltx_align_center">30.2</td>
<td id="S4.T2.1.1.9.8.8" class="ltx_td ltx_align_center">34.6</td>
<td id="S4.T2.1.1.9.8.9" class="ltx_td ltx_align_center">31.4</td>
<td id="S4.T2.1.1.9.8.10" class="ltx_td ltx_align_center">15.7</td>
<td id="S4.T2.1.1.9.8.11" class="ltx_td ltx_align_center ltx_border_r">27.5</td>
<td id="S4.T2.1.1.9.8.12" class="ltx_td ltx_align_center">37.2</td>
</tr>
<tr id="S4.T2.1.1.10.9" class="ltx_tr">
<td id="S4.T2.1.1.10.9.1" class="ltx_td ltx_align_center">Exam</td>
<td id="S4.T2.1.1.10.9.2" class="ltx_td ltx_align_center">51.5</td>
<td id="S4.T2.1.1.10.9.3" class="ltx_td ltx_align_center">74.3</td>
<td id="S4.T2.1.1.10.9.4" class="ltx_td ltx_align_center">42.0</td>
<td id="S4.T2.1.1.10.9.5" class="ltx_td ltx_align_center">70.9</td>
<td id="S4.T2.1.1.10.9.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.10.9.6.1" class="ltx_text ltx_framed ltx_framed_underline">54.1</span></td>
<td id="S4.T2.1.1.10.9.7" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.10.9.7.1" class="ltx_text ltx_framed ltx_framed_underline">60.5</span></td>
<td id="S4.T2.1.1.10.9.8" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.10.9.8.1" class="ltx_text ltx_framed ltx_framed_underline">56.2</span></td>
<td id="S4.T2.1.1.10.9.9" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.10.9.9.1" class="ltx_text ltx_font_bold">47.7</span></td>
<td id="S4.T2.1.1.10.9.10" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.10.9.10.1" class="ltx_text ltx_font_bold">41.5</span></td>
<td id="S4.T2.1.1.10.9.11" class="ltx_td ltx_align_center ltx_border_r">49.9</td>
<td id="S4.T2.1.1.10.9.12" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.10.9.12.1" class="ltx_text ltx_framed ltx_framed_underline">54.9</span></td>
</tr>
<tr id="S4.T2.1.1.11.10" class="ltx_tr">
<td id="S4.T2.1.1.11.10.1" class="ltx_td ltx_align_center">Xhs</td>
<td id="S4.T2.1.1.11.10.2" class="ltx_td ltx_align_center">25.2</td>
<td id="S4.T2.1.1.11.10.3" class="ltx_td ltx_align_center">47.0</td>
<td id="S4.T2.1.1.11.10.4" class="ltx_td ltx_align_center">8.4</td>
<td id="S4.T2.1.1.11.10.5" class="ltx_td ltx_align_center">45.4</td>
<td id="S4.T2.1.1.11.10.6" class="ltx_td ltx_align_center">8.6</td>
<td id="S4.T2.1.1.11.10.7" class="ltx_td ltx_align_center">21.4</td>
<td id="S4.T2.1.1.11.10.8" class="ltx_td ltx_align_center">22.5</td>
<td id="S4.T2.1.1.11.10.9" class="ltx_td ltx_align_center">7.0</td>
<td id="S4.T2.1.1.11.10.10" class="ltx_td ltx_align_center">28.5</td>
<td id="S4.T2.1.1.11.10.11" class="ltx_td ltx_align_center ltx_border_r">27.1</td>
<td id="S4.T2.1.1.11.10.12" class="ltx_td ltx_align_center">24.1</td>
</tr>
<tr id="S4.T2.1.1.12.11" class="ltx_tr">
<td id="S4.T2.1.1.12.11.1" class="ltx_td ltx_align_center">WikiHow</td>
<td id="S4.T2.1.1.12.11.2" class="ltx_td ltx_align_center">0.5</td>
<td id="S4.T2.1.1.12.11.3" class="ltx_td ltx_align_center">1.2</td>
<td id="S4.T2.1.1.12.11.4" class="ltx_td ltx_align_center">1.5</td>
<td id="S4.T2.1.1.12.11.5" class="ltx_td ltx_align_center">7.7</td>
<td id="S4.T2.1.1.12.11.6" class="ltx_td ltx_align_center">18.1</td>
<td id="S4.T2.1.1.12.11.7" class="ltx_td ltx_align_center">3.1</td>
<td id="S4.T2.1.1.12.11.8" class="ltx_td ltx_align_center">23.1</td>
<td id="S4.T2.1.1.12.11.9" class="ltx_td ltx_align_center">0.5</td>
<td id="S4.T2.1.1.12.11.10" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T2.1.1.12.11.11" class="ltx_td ltx_align_center ltx_border_r">2.1</td>
<td id="S4.T2.1.1.12.11.12" class="ltx_td ltx_align_center">5.8</td>
</tr>
<tr id="S4.T2.1.1.13.12" class="ltx_tr">
<td id="S4.T2.1.1.13.12.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">CQIA-Subset</td>
<td id="S4.T2.1.1.13.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.13.12.2.1" class="ltx_text ltx_framed ltx_framed_underline">59.8</span></td>
<td id="S4.T2.1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.13.12.3.1" class="ltx_text ltx_font_bold">86.4</span></td>
<td id="S4.T2.1.1.13.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.13.12.4.1" class="ltx_text ltx_framed ltx_framed_underline">48.2</span></td>
<td id="S4.T2.1.1.13.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.13.12.5.1" class="ltx_text ltx_font_bold">79.4</span></td>
<td id="S4.T2.1.1.13.12.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.13.12.6.1" class="ltx_text ltx_font_bold">60.9</span></td>
<td id="S4.T2.1.1.13.12.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.13.12.7.1" class="ltx_text ltx_font_bold">69.9</span></td>
<td id="S4.T2.1.1.13.12.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50.6</td>
<td id="S4.T2.1.1.13.12.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.13.12.9.1" class="ltx_text ltx_framed ltx_framed_underline">42.0</span></td>
<td id="S4.T2.1.1.13.12.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.13.12.10.1" class="ltx_text ltx_framed ltx_framed_underline">37.8</span></td>
<td id="S4.T2.1.1.13.12.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T2.1.1.13.12.11.1" class="ltx_text ltx_framed ltx_framed_underline">55.8</span></td>
<td id="S4.T2.1.1.13.12.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.1.13.12.12.1" class="ltx_text ltx_font_bold">64.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The performance of Yi-6B trained on various datasets evaluated on BELLE-EVAL using GPT4.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:146.2pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-131.5pt,44.2pt) scale(0.622432424523685,0.622432424523685) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Source</span></th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Open QA</span></th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Brainstorming</span></th>
<th id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Classification</span></th>
<th id="S4.T3.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Generation</span></th>
<th id="S4.T3.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Summarization</span></th>
<th id="S4.T3.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.8.1" class="ltx_text ltx_font_bold">Rewrite</span></th>
<th id="S4.T3.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.9.1" class="ltx_text ltx_font_bold">Closed QA</span></th>
<th id="S4.T3.1.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.10.1" class="ltx_text ltx_font_bold">Extract</span></th>
<th id="S4.T3.1.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.11.1" class="ltx_text ltx_font_bold">Math</span></th>
<th id="S4.T3.1.1.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.1.1.12.1" class="ltx_text ltx_font_bold">Code</span></th>
<th id="S4.T3.1.1.1.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.13.1" class="ltx_text ltx_font_bold">Average</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<th id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="13"><span id="S4.T3.1.1.2.1.1.1" class="ltx_text">
<span id="S4.T3.1.1.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:25.7pt;vertical-align:-9.4pt;"><span class="ltx_transformed_inner" style="width:25.7pt;transform:translate(-9.43pt,0pt) rotate(-90deg) ;">
<span id="S4.T3.1.1.2.1.1.1.1.1" class="ltx_p"><span id="S4.T3.1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Yi-6B</span></span>
</span></span></span></th>
<td id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">SegmentFault</td>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">51.3</td>
<td id="S4.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">70.7</td>
<td id="S4.T3.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">43.8</td>
<td id="S4.T3.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">66.8</td>
<td id="S4.T3.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">57.1</td>
<td id="S4.T3.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">75.7</td>
<td id="S4.T3.1.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">41.4</td>
<td id="S4.T3.1.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">47.1</td>
<td id="S4.T3.1.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">75.5</td>
<td id="S4.T3.1.1.2.1.12" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.0</td>
<td id="S4.T3.1.1.2.1.13" class="ltx_td ltx_align_center ltx_border_t">60.7</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<td id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_center">COIG PC</td>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center">19.1</td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center">38.0</td>
<td id="S4.T3.1.1.3.2.4" class="ltx_td ltx_align_center">27.8</td>
<td id="S4.T3.1.1.3.2.5" class="ltx_td ltx_align_center">43.9</td>
<td id="S4.T3.1.1.3.2.6" class="ltx_td ltx_align_center">37.8</td>
<td id="S4.T3.1.1.3.2.7" class="ltx_td ltx_align_center">63.7</td>
<td id="S4.T3.1.1.3.2.8" class="ltx_td ltx_align_center">40.6</td>
<td id="S4.T3.1.1.3.2.9" class="ltx_td ltx_align_center">25.8</td>
<td id="S4.T3.1.1.3.2.10" class="ltx_td ltx_align_center">43.6</td>
<td id="S4.T3.1.1.3.2.11" class="ltx_td ltx_align_center ltx_border_r">19.1</td>
<td id="S4.T3.1.1.3.2.12" class="ltx_td ltx_align_center">37.2</td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<td id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_center">Douban</td>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center">57.4</td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center">81.7</td>
<td id="S4.T3.1.1.4.3.4" class="ltx_td ltx_align_center">63.6</td>
<td id="S4.T3.1.1.4.3.5" class="ltx_td ltx_align_center">76.2</td>
<td id="S4.T3.1.1.4.3.6" class="ltx_td ltx_align_center">54.7</td>
<td id="S4.T3.1.1.4.3.7" class="ltx_td ltx_align_center">60.1</td>
<td id="S4.T3.1.1.4.3.8" class="ltx_td ltx_align_center">47.5</td>
<td id="S4.T3.1.1.4.3.9" class="ltx_td ltx_align_center">50.4</td>
<td id="S4.T3.1.1.4.3.10" class="ltx_td ltx_align_center">73.8</td>
<td id="S4.T3.1.1.4.3.11" class="ltx_td ltx_align_center ltx_border_r">50.6</td>
<td id="S4.T3.1.1.4.3.12" class="ltx_td ltx_align_center">63.2</td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<td id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_center">Zhihu</td>
<td id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center">66.5</td>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center">90.4</td>
<td id="S4.T3.1.1.5.4.4" class="ltx_td ltx_align_center">52.6</td>
<td id="S4.T3.1.1.5.4.5" class="ltx_td ltx_align_center">82.6</td>
<td id="S4.T3.1.1.5.4.6" class="ltx_td ltx_align_center">71.2</td>
<td id="S4.T3.1.1.5.4.7" class="ltx_td ltx_align_center">78.2</td>
<td id="S4.T3.1.1.5.4.8" class="ltx_td ltx_align_center">46.4</td>
<td id="S4.T3.1.1.5.4.9" class="ltx_td ltx_align_center">39.6</td>
<td id="S4.T3.1.1.5.4.10" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.5.4.10.1" class="ltx_text ltx_font_bold">76.0</span></td>
<td id="S4.T3.1.1.5.4.11" class="ltx_td ltx_align_center ltx_border_r">62.2</td>
<td id="S4.T3.1.1.5.4.12" class="ltx_td ltx_align_center">69.1</td>
</tr>
<tr id="S4.T3.1.1.6.5" class="ltx_tr">
<td id="S4.T3.1.1.6.5.1" class="ltx_td ltx_align_center">Logi QA</td>
<td id="S4.T3.1.1.6.5.2" class="ltx_td ltx_align_center">51.4</td>
<td id="S4.T3.1.1.6.5.3" class="ltx_td ltx_align_center">76.4</td>
<td id="S4.T3.1.1.6.5.4" class="ltx_td ltx_align_center">64.9</td>
<td id="S4.T3.1.1.6.5.5" class="ltx_td ltx_align_center">75.6</td>
<td id="S4.T3.1.1.6.5.6" class="ltx_td ltx_align_center">60.0</td>
<td id="S4.T3.1.1.6.5.7" class="ltx_td ltx_align_center">71.4</td>
<td id="S4.T3.1.1.6.5.8" class="ltx_td ltx_align_center">61.6</td>
<td id="S4.T3.1.1.6.5.9" class="ltx_td ltx_align_center">52.7</td>
<td id="S4.T3.1.1.6.5.10" class="ltx_td ltx_align_center">47.0</td>
<td id="S4.T3.1.1.6.5.11" class="ltx_td ltx_align_center ltx_border_r">45.3</td>
<td id="S4.T3.1.1.6.5.12" class="ltx_td ltx_align_center">62.0</td>
</tr>
<tr id="S4.T3.1.1.7.6" class="ltx_tr">
<td id="S4.T3.1.1.7.6.1" class="ltx_td ltx_align_center">Ruozhiba</td>
<td id="S4.T3.1.1.7.6.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.6.2.1" class="ltx_text ltx_font_bold">75.9</span></td>
<td id="S4.T3.1.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.6.3.1" class="ltx_text ltx_font_bold">92.3</span></td>
<td id="S4.T3.1.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.6.4.1" class="ltx_text ltx_font_bold">76.5</span></td>
<td id="S4.T3.1.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.6.5.1" class="ltx_text ltx_font_bold">92.1</span></td>
<td id="S4.T3.1.1.7.6.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.6.6.1" class="ltx_text ltx_font_bold">77.3</span></td>
<td id="S4.T3.1.1.7.6.7" class="ltx_td ltx_align_center">70.9</td>
<td id="S4.T3.1.1.7.6.8" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.6.8.1" class="ltx_text ltx_font_bold">67.2</span></td>
<td id="S4.T3.1.1.7.6.9" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.6.9.1" class="ltx_text ltx_font_bold">68.5</span></td>
<td id="S4.T3.1.1.7.6.10" class="ltx_td ltx_align_center">72.6</td>
<td id="S4.T3.1.1.7.6.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T3.1.1.7.6.11.1" class="ltx_text ltx_font_bold">65.2</span></td>
<td id="S4.T3.1.1.7.6.12" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.6.12.1" class="ltx_text ltx_font_bold">76.9</span></td>
</tr>
<tr id="S4.T3.1.1.8.7" class="ltx_tr">
<td id="S4.T3.1.1.8.7.1" class="ltx_td ltx_align_center">Wiki</td>
<td id="S4.T3.1.1.8.7.2" class="ltx_td ltx_align_center">63.0</td>
<td id="S4.T3.1.1.8.7.3" class="ltx_td ltx_align_center">75.7</td>
<td id="S4.T3.1.1.8.7.4" class="ltx_td ltx_align_center">44.0</td>
<td id="S4.T3.1.1.8.7.5" class="ltx_td ltx_align_center">80.6</td>
<td id="S4.T3.1.1.8.7.6" class="ltx_td ltx_align_center">47.9</td>
<td id="S4.T3.1.1.8.7.7" class="ltx_td ltx_align_center">66.6</td>
<td id="S4.T3.1.1.8.7.8" class="ltx_td ltx_align_center">47.9</td>
<td id="S4.T3.1.1.8.7.9" class="ltx_td ltx_align_center">50.0</td>
<td id="S4.T3.1.1.8.7.10" class="ltx_td ltx_align_center">56.8</td>
<td id="S4.T3.1.1.8.7.11" class="ltx_td ltx_align_center ltx_border_r">55.6</td>
<td id="S4.T3.1.1.8.7.12" class="ltx_td ltx_align_center">60.5</td>
</tr>
<tr id="S4.T3.1.1.9.8" class="ltx_tr">
<td id="S4.T3.1.1.9.8.1" class="ltx_td ltx_align_center">Finance</td>
<td id="S4.T3.1.1.9.8.2" class="ltx_td ltx_align_center">46.8</td>
<td id="S4.T3.1.1.9.8.3" class="ltx_td ltx_align_center">71.1</td>
<td id="S4.T3.1.1.9.8.4" class="ltx_td ltx_align_center">17.1</td>
<td id="S4.T3.1.1.9.8.5" class="ltx_td ltx_align_center">60.1</td>
<td id="S4.T3.1.1.9.8.6" class="ltx_td ltx_align_center">27.4</td>
<td id="S4.T3.1.1.9.8.7" class="ltx_td ltx_align_center">23.6</td>
<td id="S4.T3.1.1.9.8.8" class="ltx_td ltx_align_center">17.2</td>
<td id="S4.T3.1.1.9.8.9" class="ltx_td ltx_align_center">29.4</td>
<td id="S4.T3.1.1.9.8.10" class="ltx_td ltx_align_center">28.5</td>
<td id="S4.T3.1.1.9.8.11" class="ltx_td ltx_align_center ltx_border_r">24.8</td>
<td id="S4.T3.1.1.9.8.12" class="ltx_td ltx_align_center">36.7</td>
</tr>
<tr id="S4.T3.1.1.10.9" class="ltx_tr">
<td id="S4.T3.1.1.10.9.1" class="ltx_td ltx_align_center">Exam</td>
<td id="S4.T3.1.1.10.9.2" class="ltx_td ltx_align_center">49.4</td>
<td id="S4.T3.1.1.10.9.3" class="ltx_td ltx_align_center">79.7</td>
<td id="S4.T3.1.1.10.9.4" class="ltx_td ltx_align_center">64.7</td>
<td id="S4.T3.1.1.10.9.5" class="ltx_td ltx_align_center">79.9</td>
<td id="S4.T3.1.1.10.9.6" class="ltx_td ltx_align_center">61.5</td>
<td id="S4.T3.1.1.10.9.7" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.10.9.7.1" class="ltx_text ltx_font_bold">79.8</span></td>
<td id="S4.T3.1.1.10.9.8" class="ltx_td ltx_align_center">66.2</td>
<td id="S4.T3.1.1.10.9.9" class="ltx_td ltx_align_center">61.0</td>
<td id="S4.T3.1.1.10.9.10" class="ltx_td ltx_align_center">52.8</td>
<td id="S4.T3.1.1.10.9.11" class="ltx_td ltx_align_center ltx_border_r">56.3</td>
<td id="S4.T3.1.1.10.9.12" class="ltx_td ltx_align_center">66.2</td>
</tr>
<tr id="S4.T3.1.1.11.10" class="ltx_tr">
<td id="S4.T3.1.1.11.10.1" class="ltx_td ltx_align_center">Xhs</td>
<td id="S4.T3.1.1.11.10.2" class="ltx_td ltx_align_center">51.3</td>
<td id="S4.T3.1.1.11.10.3" class="ltx_td ltx_align_center">76.1</td>
<td id="S4.T3.1.1.11.10.4" class="ltx_td ltx_align_center">38.5</td>
<td id="S4.T3.1.1.11.10.5" class="ltx_td ltx_align_center">68.0</td>
<td id="S4.T3.1.1.11.10.6" class="ltx_td ltx_align_center">25.8</td>
<td id="S4.T3.1.1.11.10.7" class="ltx_td ltx_align_center">46.0</td>
<td id="S4.T3.1.1.11.10.8" class="ltx_td ltx_align_center">28.4</td>
<td id="S4.T3.1.1.11.10.9" class="ltx_td ltx_align_center">32.1</td>
<td id="S4.T3.1.1.11.10.10" class="ltx_td ltx_align_center">74.6</td>
<td id="S4.T3.1.1.11.10.11" class="ltx_td ltx_align_center ltx_border_r">36.3</td>
<td id="S4.T3.1.1.11.10.12" class="ltx_td ltx_align_center">50.3</td>
</tr>
<tr id="S4.T3.1.1.12.11" class="ltx_tr">
<td id="S4.T3.1.1.12.11.1" class="ltx_td ltx_align_center">Wikihow</td>
<td id="S4.T3.1.1.12.11.2" class="ltx_td ltx_align_center">54.7</td>
<td id="S4.T3.1.1.12.11.3" class="ltx_td ltx_align_center">75.2</td>
<td id="S4.T3.1.1.12.11.4" class="ltx_td ltx_align_center">32.1</td>
<td id="S4.T3.1.1.12.11.5" class="ltx_td ltx_align_center">68.2</td>
<td id="S4.T3.1.1.12.11.6" class="ltx_td ltx_align_center">45.3</td>
<td id="S4.T3.1.1.12.11.7" class="ltx_td ltx_align_center">55.9</td>
<td id="S4.T3.1.1.12.11.8" class="ltx_td ltx_align_center">40.9</td>
<td id="S4.T3.1.1.12.11.9" class="ltx_td ltx_align_center">55.8</td>
<td id="S4.T3.1.1.12.11.10" class="ltx_td ltx_align_center">41.0</td>
<td id="S4.T3.1.1.12.11.11" class="ltx_td ltx_align_center ltx_border_r">44.4</td>
<td id="S4.T3.1.1.12.11.12" class="ltx_td ltx_align_center">52.7</td>
</tr>
<tr id="S4.T3.1.1.13.12" class="ltx_tr">
<td id="S4.T3.1.1.13.12.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">CQIA-Subset</td>
<td id="S4.T3.1.1.13.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">56.2</td>
<td id="S4.T3.1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">84.5</td>
<td id="S4.T3.1.1.13.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">48.1</td>
<td id="S4.T3.1.1.13.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">72.9</td>
<td id="S4.T3.1.1.13.12.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">60.5</td>
<td id="S4.T3.1.1.13.12.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">70.9</td>
<td id="S4.T3.1.1.13.12.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">54.6</td>
<td id="S4.T3.1.1.13.12.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50.8</td>
<td id="S4.T3.1.1.13.12.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">52.5</td>
<td id="S4.T3.1.1.13.12.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">49.5</td>
<td id="S4.T3.1.1.13.12.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">61.9</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The performance of Yi-34B trained on various datasets evaluated on BELLE-EVAL using GPT4.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>EXPERIMENTAL SETUP</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we describe how we use COIG-CQIA to fine-tune models and elaborate our our evaluation methods.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluation</h3>

<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">C-Eval</h5>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.1" class="ltx_p">is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels. We choosing the answer option with the highest log-likelihood as the final prediction of the model.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CMMLU</h5>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.1" class="ltx_p">is a comprehensive evaluation benchmark specifically designed to evaluate the knowledge and reasoning abilities of LLMs within the context of Chinese language and culture. CMMLU covers a wide range of subjects, comprising 67 topics that span from elementary to advanced professional levels. It includes subjects that require computational expertise, such as physics and mathematics, as well as disciplines within humanities and social sciences.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">BELLE-EVAL</h5>

<div id="S5.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px3.p1.1" class="ltx_p">is an open-ended test set comprising 12 different instruction types across various domains, including open question answering, brainstorming, mathematics, coding, etc. It can be used to assess a model’s ability to follow instructions across different types. We employ sampling generation for generating responses to instructions and use a model-based evaluation method.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SafetyBench</h5>

<div id="S5.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px4.p1.1" class="ltx_p">comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns. We evaluate models in few-shot setting.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Implementation Details</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We fine-tuned bilingual LLMs(Chinese and English) on COIG-CQIA, including Yi<cite class="ltx_cite ltx_citemacro_cite">Young et&nbsp;al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite>, Qwen<cite class="ltx_cite ltx_citemacro_cite">Bai et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>, and InternLM<cite class="ltx_cite ltx_citemacro_cite">Team (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>, which represent the forefront of Chinese models.
We will provide implementation details in the next revision.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:605.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(85.8pt,-149.7pt) scale(1.97849854455756,1.97849854455756) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SafetyBench</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.2.1" class="ltx_tr">
<th id="S5.T4.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">COIG PC</th>
<td id="S5.T4.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">81.2</td>
</tr>
<tr id="S5.T4.1.1.3.2" class="ltx_tr">
<th id="S5.T4.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Chinese Tradiational</th>
<td id="S5.T4.1.1.3.2.2" class="ltx_td ltx_align_center">76.6</td>
</tr>
<tr id="S5.T4.1.1.4.3" class="ltx_tr">
<th id="S5.T4.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Douban</th>
<td id="S5.T4.1.1.4.3.2" class="ltx_td ltx_align_center">76.2</td>
</tr>
<tr id="S5.T4.1.1.5.4" class="ltx_tr">
<th id="S5.T4.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Exam</th>
<td id="S5.T4.1.1.5.4.2" class="ltx_td ltx_align_center">77.6</td>
</tr>
<tr id="S5.T4.1.1.6.5" class="ltx_tr">
<th id="S5.T4.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Finance</th>
<td id="S5.T4.1.1.6.5.2" class="ltx_td ltx_align_center">75.1</td>
</tr>
<tr id="S5.T4.1.1.7.6" class="ltx_tr">
<th id="S5.T4.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Logi QA</th>
<td id="S5.T4.1.1.7.6.2" class="ltx_td ltx_align_center">79.1</td>
</tr>
<tr id="S5.T4.1.1.8.7" class="ltx_tr">
<th id="S5.T4.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ruozhiba</th>
<td id="S5.T4.1.1.8.7.2" class="ltx_td ltx_align_center">81.3</td>
</tr>
<tr id="S5.T4.1.1.9.8" class="ltx_tr">
<th id="S5.T4.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Segmentfault</th>
<td id="S5.T4.1.1.9.8.2" class="ltx_td ltx_align_center">78.0</td>
</tr>
<tr id="S5.T4.1.1.10.9" class="ltx_tr">
<th id="S5.T4.1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wiki</th>
<td id="S5.T4.1.1.10.9.2" class="ltx_td ltx_align_center">75.8</td>
</tr>
<tr id="S5.T4.1.1.11.10" class="ltx_tr">
<th id="S5.T4.1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wikihow</th>
<td id="S5.T4.1.1.11.10.2" class="ltx_td ltx_align_center">76.4</td>
</tr>
<tr id="S5.T4.1.1.12.11" class="ltx_tr">
<th id="S5.T4.1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Xhs</th>
<td id="S5.T4.1.1.12.11.2" class="ltx_td ltx_align_center">76.0</td>
</tr>
<tr id="S5.T4.1.1.13.12" class="ltx_tr">
<th id="S5.T4.1.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Zhihu</th>
<td id="S5.T4.1.1.13.12.2" class="ltx_td ltx_align_center">75.8</td>
</tr>
<tr id="S5.T4.1.1.14.13" class="ltx_tr">
<th id="S5.T4.1.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Human Value</th>
<td id="S5.T4.1.1.14.13.2" class="ltx_td ltx_align_center">79.1</td>
</tr>
<tr id="S5.T4.1.1.15.14" class="ltx_tr">
<th id="S5.T4.1.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">CQIA-Sub-6B</th>
<td id="S5.T4.1.1.15.14.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.1.1.15.14.2.1" class="ltx_text ltx_font_bold">81.7</span></td>
</tr>
<tr id="S5.T4.1.1.16.15" class="ltx_tr">
<th id="S5.T4.1.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-4-0613</th>
<td id="S5.T4.1.1.16.15.2" class="ltx_td ltx_align_center ltx_border_t">89.2</td>
</tr>
<tr id="S5.T4.1.1.17.16" class="ltx_tr">
<th id="S5.T4.1.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">GPT-3.5-turbo-0613</th>
<td id="S5.T4.1.1.17.16.2" class="ltx_td ltx_align_center ltx_border_bb">80.4</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>SafetyBench scores of Yi-6B trained on various data sources.</figcaption>
</figure>
<figure id="S5.T5" class="ltx_table">
<div id="S5.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:442.3pt;height:196.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(79.6pt,-35.4pt) scale(1.56210792289923,1.56210792289923) ;">
<table id="S5.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Model</th>
<th id="S5.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Ceval (val 5-shot)</th>
<th id="S5.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CMMLU (test 5-shot)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.2.1" class="ltx_tr">
<th id="S5.T5.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Qwen-1.8b</th>
<td id="S5.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">51.34</td>
<td id="S5.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">47.26</td>
</tr>
<tr id="S5.T5.1.1.3.2" class="ltx_tr">
<th id="S5.T5.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yi-6B</th>
<td id="S5.T5.1.1.3.2.2" class="ltx_td ltx_align_center">73.40</td>
<td id="S5.T5.1.1.3.2.3" class="ltx_td ltx_align_center">74.85</td>
</tr>
<tr id="S5.T5.1.1.4.3" class="ltx_tr">
<th id="S5.T5.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Qwen-14b</th>
<td id="S5.T5.1.1.4.3.2" class="ltx_td ltx_align_center">68.20</td>
<td id="S5.T5.1.1.4.3.3" class="ltx_td ltx_align_center">67.96</td>
</tr>
<tr id="S5.T5.1.1.5.4" class="ltx_tr">
<th id="S5.T5.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">InternLM2-20b</th>
<td id="S5.T5.1.1.5.4.2" class="ltx_td ltx_align_center">71.25</td>
<td id="S5.T5.1.1.5.4.3" class="ltx_td ltx_align_center">67.48</td>
</tr>
<tr id="S5.T5.1.1.6.5" class="ltx_tr">
<th id="S5.T5.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yi-34b</th>
<td id="S5.T5.1.1.6.5.2" class="ltx_td ltx_align_center">77.04</td>
<td id="S5.T5.1.1.6.5.3" class="ltx_td ltx_align_center">78.18</td>
</tr>
<tr id="S5.T5.1.1.7.6" class="ltx_tr">
<th id="S5.T5.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Qwen-72b</th>
<td id="S5.T5.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb">78.68</td>
<td id="S5.T5.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb">76.79</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance of different base models after training on the COIG Subset data.</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2403.18058/assets/x1.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="170" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Human evaluation of pair-wise comparison between CQIA-Subset and 5 strong baselines at similar parameter scale.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>EXPERIMENT RESULTS</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Ablating Instruction Data Sources</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">We finetune the Yi series model<cite class="ltx_cite ltx_citemacro_cite">Young et&nbsp;al. (<a href="#bib.bib27" title="" class="ltx_ref">2024</a>)</cite> and the Qwen-72B<cite class="ltx_cite ltx_citemacro_cite">Bai et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> model on different data sources of our datasets.
to analyze the impact of data source on model capabilities across different domain knowledge. Then,
We evaluate each model performance on various type of assistant-style tasks using model-based(i.e. GPT-4) automatic evaluation
on Belle-Eval.
</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">To understand the correlation between training data sources and the downstream performance of different tasks, we evaluate the models on 10 tasks from BELLE-Eval. We employ GPT-4 as evaluator for scoring model responses, with scores ranging from 0 to 1.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">Tabel <a href="#S4.T2" title="Table 2 ‣ 4.2 Diversity ‣ 4 Data Analysis ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results of different Yi-6B-based models fine-tuned on different subsets. From the table, we can see that models trained on our data excel in generative tasks such as brainstorming, generation, and summarization, but perform poorly in math and coding.
The <span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">Exam</span> subset achieves the best performance across all subsets with an average score of 54.9, particularly excelling in Extract and Math tasks. This is expected as <span id="S6.SS1.p3.1.2" class="ltx_text ltx_font_typewriter">Exam</span> contains more math quizzes and exam types(e.g., reading comprehension), potentially boosting the model’s performance in most tasks.
Interestingly, <span id="S6.SS1.p3.1.3" class="ltx_text ltx_font_typewriter">Ruozhiba</span> ranks second on average across all subsets. We conjecture this is because it may enhance the model’s logical reasoning ability, thereby benefiting most of the instruct-following tasks.
<span id="S6.SS1.p3.1.4" class="ltx_text ltx_font_typewriter">COIG-PC</span> demonstrates proficiency in evaluations of the knowledge dimension, such as C-Eval, yet underperforms in Belle-Eval. We attribute this discrepancy to its origin in traditional NLP datasets and the short length of responses, which can impair reasoning tasks and are less favored by model-based evaluators.
The substantial gap between C-Eval and Belle-Eval highlights the importance of developing assessments that can comprehensively and accurately evaluate Chinese LLMs.
Moreover, WikiHow scores only 5.8, which we believe is due to the lack of diversity in its "how-to" instructions.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Human Evaluation</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In addition to automatic evaluation, we also evaluate Yi-6B fine-tuned on CQIA-subset by comparing it to state-of-the-art Chinese open-source chat models in similar parameter scale. As we focus on questions posed by real-world Chinese-speaking users. We sample 200 questions from OL-CC<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://data.baai.ac.cn/details/OL-CC</span></span></span> and Zhihu which are not present in the training set for human evaluation. We conduct pair-wise comparison, aiming to demonstrate how our model performs in comparison to others when facing real-world human prompt.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">For each prompt, we generate one response from each model respectively<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We generate using nucleus sampling with <math id="footnote6.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="footnote6.m1.1b"><mi id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="footnote6.m1.1c"><ci id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m1.1d">p</annotation></semantics></math>=0.85, <math id="footnote6.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="footnote6.m2.1b"><mi id="footnote6.m2.1.1" xref="footnote6.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="footnote6.m2.1c"><ci id="footnote6.m2.1.1.cmml" xref="footnote6.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m2.1d">k</annotation></semantics></math>=50 and temperature=0.9.</span></span></span>. Annotators are then presented with the prompt and two responses: one generated by <span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">CQIA</span> model and one by another baseline model. Subsequently, we ask which response the annotator prefers, allowing for a "tie" selection when a better response is hard to judge.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2 Implementation Details ‣ 5 EXPERIMENTAL SETUP ‣ COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows human evaluation results with <span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">CQIA</span> and other 5 baselines, namely Yi-6B-Chat, Baichuan2-7B-Chat, ChatGLM2-6B, Qwen-7B-Chat and InternLM-7B-Chat. The results demonstrate that, compared to strong baselines, CQIA-Subset achieved higher human preference, with at least over 60% of responses being better than or on par with the baseline models. This can be attributed to CQIA not solely in generating high-quality responses to human questions or instructions, but also in its responses being more aligned with real-world human communication patterns, leading to higher human preference.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Scaling Model Size</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">We investigate the performance of different base model with varying parameter sizes after fine-tuned on our CQIA-Subset.
Notably, Yi-6B surpasses Qwen-14B and InternLM-20B, which have at least twice its parameter size.
Further, Yi-34B achieved comparable results to Qwen-72B in both C-Eval and CMMLU benchmarks.
This observervation underscores the balance between model size, architectural optimizations, and training methodologies. While the scaling law might suggest that larger models inherently perform better due to their increased language understanding capacity, our results indicate that this is not always the case. Specifically, the Yi-6B model’s superior performance against models with significantly more parameters challenges the notion that parameter count alone is a sufficient predictor of model efficacy.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Safety</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">We explore the impact of data sources on model safety by evaluating our models on SafetyBench. Models trained on CQIA-Subset scores the highest within CQIA series, surpassing GPT-3.5-turbo-0613. Model trained on Social Media&amp; Forums such as Douban, Zhihu, and Xhs perform moderate safety scores, we conjecture this is due to the diversity and openness of social media content, which also highlights the risks of harmful information. Additionally, models trained on Wiki-style data tend to perform lower safety scores, potentially reflecting the limited diversity within professional data sources, leading to poor performance on safety issues outside specialty domains.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we introduce a a high-quality Chinese instruction fine-tuning dataset. COIG-CQIA focuses on creating a dataset from Chinese internet sources including Q&amp;A and articles. These are deeply cleansed, restructured, and manually reviewed to ensure quality, diversity, and relevance. This dataset is designed to provide the Chinese NLP community with high-quality and human interaction-aligned instruction fine-tuning data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu&nbsp;Han, Fei Huang, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.08701" title="" class="ltx_ref ltx_href">Alpagasus: Training a better alpaca with fewer data</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian Gehrmann, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 24(240):1–113.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Le&nbsp;Hou, Shayne Longpre, Barret Zoph, Yi&nbsp;Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang&nbsp;Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed&nbsp;H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc&nbsp;V. Le, and Jason Wei. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2210.11416" title="" class="ltx_ref ltx_href">Scaling instruction-finetuned language models</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CLUEbenchmark (2022)</span>
<span class="ltx_bibblock">
CLUEbenchmark. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/CLUEbenchmark/pCLUE" title="" class="ltx_ref ltx_href">pclue: Large-scale prompt-based dataset for multi-task and zero-shot learning in chinese</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm" title="" class="ltx_ref ltx_href">Free dolly: Introducing the world’s first truly open instruction-tuned llm</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2301.07597" title="" class="ltx_ref ltx_href">How close is chatgpt to human experts? comparison corpus, evaluation, and detection</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He and Choi (2021)</span>
<span class="ltx_bibblock">
Han He and Jinho&nbsp;D Choi. 2021.

</span>
<span class="ltx_bibblock">The stem cell hypothesis: Dilemma behind multi-task learning with transformer encoders.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.06939</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Or&nbsp;Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2212.09689" title="" class="ltx_ref ltx_href">Unnatural instructions: Tuning language models with (almost) no human labor</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivison et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A. Smith, Iz&nbsp;Beltagy, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.10702" title="" class="ltx_ref ltx_href">Camels in a changing climate: Enhancing lm adaptation with tulu 2</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.14742" title="" class="ltx_ref ltx_href">Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konchakov et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
R.&nbsp;A. Konchakov, A.&nbsp;S. Makarov, G.&nbsp;V. Afonin, J.&nbsp;C. Qiao, M.&nbsp;G. Vasin, N.&nbsp;P. Kobelev, and V.&nbsp;A. Khonik. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.00475" title="" class="ltx_ref ltx_href">Critical behavior of the fluctuation heat capacity near the glass transition of metallic glasses</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.06259" title="" class="ltx_ref ltx_href">Self-alignment with instruction backtranslation</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lou et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu&nbsp;Su, and Wenpeng Yin. 2023.

</span>
<span class="ltx_bibblock">Muffin: Curating multi-faceted instructions for improving instruction following.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2104.08773" title="" class="ltx_ref ltx_href">Cross-task generalization via natural language crowdsourcing instructions</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.

</span>
<span class="ltx_bibblock">Instruction tuning with gpt-4.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03277</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen&nbsp;H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven&nbsp;Le Scao, Arun Raja, Manan Dey, M&nbsp;Saiful Bari, Canwen Xu, Urmish Thakker, Shanya&nbsp;Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng&nbsp;Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason&nbsp;Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander&nbsp;M. Rush. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2110.08207" title="" class="ltx_ref ltx_href">Multitask prompted training enables zero-shot task generalization</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei, Zhenzhong Lan, and Yue Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2310.19651" title="" class="ltx_ref ltx_href">Dynamics of instruction tuning: Each ability of large language models has its own growth pace</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke&nbsp;Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023.

</span>
<span class="ltx_bibblock">Moss: Training conversational language models from synthetic data.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023)</span>
<span class="ltx_bibblock">
InternLM Team. 2023.

</span>
<span class="ltx_bibblock">Internlm: A multilingual language model with progressively enhanced capabilities.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/InternLM/InternLM" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/InternLM/InternLM</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah&nbsp;A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2212.10560" title="" class="ltx_ref ltx_href">Self-instruct: Aligning language models with self-generated instructions</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu&nbsp;Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.12244" title="" class="ltx_ref ltx_href">Wizardlm: Empowering large language models to follow complex instructions</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.01196" title="" class="ltx_ref ltx_href">Baize: An open-source chat model with parameter-efficient tuning on self-chat data</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang (2023)</span>
<span class="ltx_bibblock">
Jianxin Yang. 2023.

</span>
<span class="ltx_bibblock">Firefly(流萤): 中文对话式大语言模型.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/yangjianxin1/Firefly" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/yangjianxin1/Firefly</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge&nbsp;Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et&nbsp;al. 2024.

</span>
<span class="ltx_bibblock">Yi: Open foundation models by 01. ai.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.04652</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Ge&nbsp;Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu&nbsp;Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.07987" title="" class="ltx_ref ltx_href">Chinese open instruction generalist: A preliminary release</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.10792</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.11206" title="" class="ltx_ref ltx_href">Lima: Less is more for alignment</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.18057" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.18058" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2403.18058">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.18058" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.18059" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:34:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
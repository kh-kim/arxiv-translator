# COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning

유린바이\({}^{1}\) Xinrun Du\({}^{2}\) Yiming Liang\({}^{3}\) Yonggang Jin\({}^{2}\)\({}^{*}\)

Ziqiang Liu\({}^{1}\) Junting Zhou\({}^{4,2}\) Tianyu Zheng\({}^{2}\) Xincheng Zhang\({}^{5}\) Nuo Ma\({}^{6}\)

제쿤왕\({}^{2}\) 루이빈원\({}^{7,2}\) 하이홍우\({}^{5}\) 홍취안린\({}^{5}\) 원하오황\({}^{6}\)

Jiajun Zhang\({}^{3}\) Wenhu Chen\({}^{8,9,2}\) Chenghua Lin\({}^{10,2}\) Jie Fu\({}^{7,2}\) Min Yang\({}^{1}\)

Shiwen Ni\({}^{1}\) Ge Zhang\({}^{8,9}\)\({}^{\dagger}\)

Shenzhen Advanced Technology Institute of CAS \({}^{2}\)M-A-P \({}^{3}\)Institute of Automation

북경대학 \({}^{4}\)북경대학 \({}^{5}\)중국과학기술대학 \({}^{6}\)01.ai \({}^{7}\)HKUST \({}^{8}\)워터루대학 \({}^{9}\)벡터대학 \({}^{10}\)맨체스터대학

###### Abstract

최근, 특히 영어에 초점을 맞춘 대규모 언어 모델(LLM)의 상당한 발전이 있었다. 이러한 발전은 이러한 LLM이 전례 없는 정확성과 유창성으로 복잡한 명령을 이해하고 실행할 수 있도록 했다. 그러나 이러한 발전에도 불구하고 중국어 명령어 튜닝의 발전에는 눈에 띄는 격차가 남아 있다. 중국어의 독특한 언어적 특징과 문화적 깊이는 수업 조율 작업에 어려움을 준다. 기존 데이터 세트는 영어 중심 LLM에서 파생되거나 실제 중국 사용자의 상호 작용 패턴과 일치하기에 적합하지 않다. 이러한 격차를 해소하기 위해 고품질 중국어 명령어 튜닝 데이터 세트인 COIG-CQIA를 소개한다. 우리의 목표는 모델 행동을 인간의 상호 작용과 더 잘 정렬하기 위해 다양하고 광범위한 명령 조정 데이터 세트를 구축하는 것이다. 이를 위해 질의응답 커뮤니티, 위키스, 시험, 기존 NLP 데이터셋 등 중국 인터넷 상의 다양한 소스로부터 고품질의 인간이 작성한 코퍼스를 수집한다. 이 말뭉치는 엄격하게 필터링되고 신중하게 처리되어 COIG-CQIA 데이터 세트를 형성했다. 또한 심층 평가 및 분석에 따라 CQIA의 다양한 하위 집합에 대한 다양한 척도 모델을 훈련한다. 본 연구의 결과는 중국어 학습 데이터 세트를 선택하고 개발하는 데 유용한 통찰력을 제공한다. 또한 CQIA-Subset에서 훈련된 모델이 지식 및 보안 벤치마크뿐만 아니라 인간 평가에서 경쟁력 있는 결과를 달성한다는 것을 발견했다. 데이터는 [https://huggingface.co/datasets/m-a-p/COIG-CQIA](https://huggingface.co/datasets/m-a-p/COIG-CQIA)에서 사용할 수 있습니다.

+
각주 † : 교신저자

+
각주 † : 교신저자

## 1 Introduction

GPT-3 Brown et al. (2020), LLaMA Touvron et al. (2023), PaLM Chowdhery et al. (2023)과 같은 대규모 언어 모델(Large Language Models, LLM)은 범용 보조자로서의 놀라운 능력을 입증했다. 이 성취의 초석은 명령어 튜닝이며, 이는 명령어-출력 쌍 Zhang 등(2023)으로 구성된 데이터 세트에 대한 훈련을 통해 LLM의 능력과 제어 가능성을 크게 향상시킨다. 이 기술은 모델의 훈련 목표를 인간의 의도와 효과적으로 정렬하여 모델이 인간 명령을 효과적이고 안전하게 해석하고 실행할 수 있도록 한다. 따라서 LLM이 효율적이고 신뢰할 수 있는 보조자로 작동하려면 고품질 명령어 튜닝 데이터 세트의 가용성이 중요하다.

많은 영어 명령어 튜닝 데이터 세트가 존재한다. 그러나 중국어 명령어 튜닝을 위한 사용 가능한 데이터 세트는 일반적으로 크기가 제한적이거나 품질이 부족하다. 중국어 명령어 튜닝 데이터셋은 (1) 영어 명령어 데이터셋 Peng et al. (2023) 또는 NLP 데이터셋 CLUEbenchmark (2022); Yang (2023), (2) LLMs Guo et al. (2023) 및 (3) 자가 생성 명령어 튜닝 데이터셋 Ji et al. (2023); Sun et al. (2023). COIG Zhang et al. (2023)은 인간이 검증한 보편적인 고품질 중국어 명령어 코퍼스를 구성하기 위해 다중 접근법을 통합한다. 그러나, 앞서 언급한 중국어 명령어 튜닝 데이터셋은 자연적인 중국어 통신 패턴과 일치하지 않거나, 진정한 중국어 언어 데이터가 부족하고, 수많은 문제가 있는 데이터 포인트가 포함되어 있고, 작은 규모의 데이터를 가지고 있는 등의 본질적인 문제를 가지고 있다. 본 논문은 중국어 학습에서 LLM의 숙련도를 높이기 위해 다양한 도메인에 걸쳐 진실한 중국어 언어 데이터에서 출처하고 세심한 수동 세척 절차를 거친 중국어 학습 동조 데이터 세트를 구성하는 데 중점을 둔다.

이 문서에서는 중국 NLP 커뮤니티에 고품질 및 인간 상호 작용 정렬 명령 미세 조정 데이터를 제공하도록 설계된 고품질 중국 명령 조정 데이터 세트인 COIG-**CQIA**(**중국** 공개 명령 일반자 - **Q**uality **I**s **A**ll You Need)를 소개합니다. COIG-CQIA는 LIMA Zhou et al.(2023)의 연구에 영감을 받아 Q&A 세션과 기사를 구성하는 중국 인터넷 소스의 데이터 세트를 선별하는 데 중점을 둔다. 이러한 출처는 고품질, 다양성 및 관련성을 보장하기 위해 철저한 세척, 구조 조정 및 수동 검토를 거칩니다. 또한, 데이터 품질, 출처 및 혼합 비율의 영향을 평가하기 위한 분석 실험을 수행한다.

요약하면, 기여도는 다음과 같다:

* 엄격한 필터링 절차를 통해 달성된 인간 상호 작용과 일치하도록 특별히 설계된 고품질 중국 명령어 미세 조정 데이터 세트를 제안합니다.
* 소셜 미디어, 백과사전 및 기존 NLP 작업을 포함한 다양한 데이터 원본이 모델 성능에 미치는 영향을 조사합니다. 우리의 분석은 중국 인터넷에서 훈련 데이터를 선택하는 데 필수적인 통찰력을 제공한다.
* 다양한 벤치마크 테스트 및 인간 평가는 CQIA 데이터 세트에서 미세 조정 된 모델이 우수한 성능을 보여 CQIA를 중국 NLP 커뮤니티에 대 한 귀중한 리소스로 설정 함을 확인 합니다.

## 2 관련 작업

### 명령 튜닝 데이터 세트

명령어 튜닝은 입력 명령어와 정렬되는 응답을 생성하기 위해 큰 언어 모델을 트레이닝하는 것을 목표로 하며, 이에 의해 대화 및 태스크 실행 능력을 갖는 LLM을 가능하게 한다. 표준 LLM과 비교하여, SFT는 모델의 행동이 더 제어가능하고 예측가능하도록 하여, 인간의 의지를 정렬하는 목적을 달성한다. 명령어 튜닝 데이터세트를 구축하는 방법은 (1) 순수 수동 주석 Conover 등(2023)을 포함한다. 이 방법은 명령어 및 답변을 수동으로 완전히 구성하며, 이는 매우 시간이 많이 걸리고 힘든 작업이다; (2) 기존 데이터 세트 Mishra 등(2022), Sanh 등(2022), Chung 등(2022)으로부터 변환된다. 일부 연구는 명령어 튜닝 데이터를 구성하기 위해 NLP 태스크로부터의 감독된 데이터 세트를 사용한다; (3) LLM Honovich 등(2022); Wang 등(2023); Xu 등(2023); Ji 등(2023); Xu 등(2023). 다른 사람들은 명령어 튜닝 데이터를 생성하기 위해 기존의 LLM을 사용한다. 일반적인 관행은 먼저 고품질 시드 데이터 세트에 수동으로 주석을 달았다가 LLM을 사용하여 시드 명령 및 해당 출력을 확장하여 인간 주석이 거의 없는 대규모 명령 튜닝 데이터를 생성할 수 있다. 그러나 품질을 보장할 수 없고 일정량의 잡음이 있는 데이터가 있어 환각을 유발할 수 있다.

많은 영어 명령어 튜닝 데이터 세트가 존재한다. 이에 비해 기존의 중국어 명령어 튜닝 데이터셋은 규모가 작거나 품질 문제가 있다. 일부 연구에서는 영어 명령어 튜닝 데이터셋을 중국어 Peng 등(2023)으로 번역하였으나, 이는 번역 오류 누적으로 이어질 수 있다. pCLUE CLUEbenchmark (2022) 및 Firefly Yang (2023)은 원래의 NLP 작업 데이터세트를 명령어 튜닝 데이터세트로 변환한다. HC3 Guo et al. (2023)은 인간 전문가와 ChatGPT로부터 수만 개의 비교 응답을 수집한다. COIG Zhang et al.(2023)은 인간이 검증한 보편적인 고품질 중국어 교육 코퍼스를 구축한다. BELLE Ji et al.(2023) 및 MOSS Sun et al.(2023)은 중국어 명령어 튜닝 데이터세트를 자동으로 생성하기 위해 자기 명령어 Wang et al.(2023)과 유사한 방법을 사용한다.

### SFT 데이터 혼합

현재, 점점 더 많은 연구들이 수업 튜닝의 데이터 품질의 중요성에 주목하기 시작했다. LIMA Zhou et al.(2023)은 SFT를 위해 1,000개의 고품질 명령어와 출력만을 사용하며, 매우 강한 성능을 얻기 위해 RLHF 훈련을 수행할 필요조차 없다. Alp-Gasus Chen et al.(2023)은 강력한 LLM을 사용하여 저품질 데이터를 자동으로 식별하고 필터링하여 성능 및 훈련 속도를 향상시키기 위해 고품질 명령어 튜닝 데이터를 생성한다. Humpback Li et al.(2023)은 보다 강력한 LLM을 미세 조정하기 위해 고품질 샘플을 필터링한다.

다른 Song et al. (2023)은 서로 다른 명령어 튜닝 데이터 세트의 혼합 전략의 영향을 탐구한다. 툴루 시리즈 Konchakov et al. (2023); Ivison et al. (2023)은 명령어 다양성을 증가시키는 것이 성능을 효과적으로 향상시킬 수 있고 상이한 명령어 튜닝 데이터세트가 특정 기술을 발견하거나 향상시킬 수 있는 반면, 어떤 데이터세트(또는 조합)도 모든 평가에 걸쳐 최상의 성능을 제공하지 않는다는 것을 보여준다.

## 3 Cqia Curation

데이터의 품질과 다양성을 보장하기 위해 고품질 웹사이트와 중국 인터넷 내 데이터 리소스에서 13개의 데이터 소스를 수동으로 선택했다. 이러한 출처에는 커뮤니티 Q&A 포럼, 백과사전 사이트, 콘텐츠 생성 플랫폼, 테스트 등이 포함된다. 또한 작업의 다양성을 풍부하게 하기 위해 고품질 중국 NLP 데이터 세트를 통합했습니다. 구체적으로, 우리는 모든 데이터 소스를 소셜 미디어 및 포럼, 세계 지식, NLP 과제 및 시험의 네 가지 유형으로 분류했다. 데이터 소스 및 그 설명은 다음과 같다.

### 소셜 미디어 & 포럼

Zhihuis는 사용자가 광범위한 주제에 대해 질문하고 답할 수 있는 활기찬 질의 응답 플랫폼으로 지식과 통찰력의 포괄적인 저장소입니다. Zhihu는 사용자들이 전문적인 지식이나 개인적인 경험을 반영하며 유익하고 잘 숙고된 답변을 제공하도록 권장한다. 그러나 Zhihu에 대한 답변에 대한 검토 메커니즘의 부재는 우리의 품질 표준에 미치지 못하는 많은 양의 콘텐츠로 이어진다. 저품질 답변을 필터링하기 위해 50개 이상의 업보트가 포함된 답변을 선택한 후 규칙 기반 방법을 사용하여 민감하거나 유해한 키워드가 포함된 콘텐츠를 필터링했다. 그 후 GPT-4를 사용하여 1-10의 척도로 응답을 채점하고 점수가 8 이상인 응답을 유지했다.

SegmentFaultis는 IT 기술에 초점을 맞춘 질의응답 커뮤니티로, 중국 개발자들에게 Stack Overflow와 유사한 고품질의 교환 플랫폼을 제공한다. 이 커뮤니티에서 사용자는 IT 기술과 관련된 질문을 묻고 답하는 데 참여하며, 여기서 질문자는 가장 유용한 답변을 수용할 수 있다. 또한 커뮤니티 구성원은 답변에 대해 투표하거나 의견을 제시할 수도 있습니다. 우리의 데이터는 프로그래밍 언어 또는 소프트웨어 버전의 변경으로 인해 이전 콘텐츠가 구식이 될 수 있기 때문에 2018년 이전에 게시된 콘텐츠에서 수집된다. 그런 다음 최소 5개의 투표로 "승인된" 답변을 선택합니다. 또한, 저품질 콘텐츠를 제거하거나 수정하기 위해 모든 (질문, 답변) 쌍을 수동으로 검토합니다.

두바니스는 영화, 책, TV 시리즈, 음악 등과 같은 문학 및 예술 작품과 관련된 콘텐츠를 만들 수 있도록 하는 소셜 네트워크 및 데이터베이스이다. 우리는 책, 영화, TV 시리즈에서 데이터를 샘플링하고 평점, 배우/승무원에 대한 자세한 정보, 긴 리뷰를 포함하는 메타데이터를 추출합니다. 그리고 시놉시스 생성, 리뷰 생성, 추천의 세 가지 작업을 설계한다. 각 작업에 대해 다양한 프롬프트 템플릿을 수동으로 설계하고 이러한 템플릿을 메타데이터와 결합하여 명령어를 구성했다. 시놉시스 생성 및 리뷰 생성을 위해 영화 또는 TV 시리즈 이름과 결합된 프롬프트 템플릿과 더반 사용자가 생성한 응답을 사용하여 지침을 구성합니다. 그런 다음 임계값보다 짧은 길이의 응답을 제거하고 개인 정보 및 관련 없는 내용(예: "공식 계정 구독")을 삭제합니다. 또한 응답의 세부 정보와 더 잘 일치하도록 복잡한 암시적 의도를 추가하기 위해 일부 지침을 수동으로 조정했습니다.

샤오홍슈프는 이용자들이 자신의 삶, 여행, 음식, 상품 추천 등을 공유할 수 있는 공간을 마련한다. 이 플랫폼의 콘텐츠는 독특하고 표현적인 스타일로 중국 인터넷에서 유명합니다. 다른 사용자들과의 상호 작용을 수반하는 것("@User_Name") 및 이미지 또는 비디오를 참조하는 것("그림/비디오에 도시된 바와 같이")을 제외하고, 500 내지 2000 범위의 길이를 갖는 게시물을 샘플링한다.

루지바이는 이익에 기반을 둔 커뮤니티 포럼인 바이두 티오바의 하위 포럼이다. 그 게시물에는 종종 말장난, 다의어 용어, 인과적 역전 및 동음이가 포함되어 있으며, 그 중 다수는 논리적 함정으로 설계되어 인간에게도 도전을 제기한다. 우리는 가장 많은 지지를 받은 500개의 실을 모았다. 제목을 지침으로 사용하여 지시적이지 않은(즉, 선언적 진술 또는 답변할 수 없는) 또는 독성이 있는 제목을 제거한다. 응답은 사람 또는 GPT-4에 의해 생성되었으며 정확성을 보장하기 위해 GPT4의 응답에 대한 수동 검토를 수행하여 궁극적으로 240(instruc)을 얻었다.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Source** & **Quantity** & **Source** & **Quantity** \\ \hline Zhha & 8837 & Douban & 3132 \\ Xiaohongshu & 1508 & Segment Fault & 458 \\ Encyclopedia Article & 980 & Encyclopedia of China & 1706 \\ Walkiro & 1876 & COCO TC & 3000 \\ Middle school Ekam & 2000 & Graduate Extraction Examination & 475 \\ Light Q4 & 422 & CValue & 906 \\ COR-Humans Value & 101 & Chinese Traditional & 232 \\ Idom Explanation & 112 & Penn Writing & 47 \\ Classical Chinese Translation & 112 & MRA/Explopedia & 10689 \\ Finance NLP Task & 600 & Medical Encyclopedia & 8351 \\ Medical Article & 186 & Law & 2645 \\ \hline \hline \end{tabular}
\end{table}
표 1: 데이터세트 혼합, 응답) 쌍의 상이한 소스로부터의 데이터의 양.

### World Knowledge

#### 3.2.1 일반 백과사전

일반 백과사전은 다양한 분야에 걸쳐 광범위한 주제에 대한 포괄적인 취재를 제공한다. 우리는 중국의 백과사전 웹사이트 100,000 Whys1, wikiHow-zh2 및 중국 백과사전 3개에서 데이터를 수집한다. **100,000 Whys**는 인기 과학을 목표로 하는 백과사전 웹사이트로 자연 과학에서 인문학에 이르기까지 주제에 걸쳐 "왜"를 묻는 수천 개의 고품질 기사를 특징으로 한다. 우리는 모든 15개 범주에서 데이터를 수집하고 각 범주에 걸쳐 균일한 분포를 보장한다. 기사 제목은 교육(예: "비행할 때 고산병에 걸리지 않는 이유는?"), 내용은 응답으로 사용되며 300자 미만의 응답은 걸러진다. 위키하우의 중국어 버전인 **wikiHow-zh** 는 광범위한 주제를 다루는 백과사전 스타일의 웹사이트로, 여러 번 수정한 수만 개의 "how-to" 기사를 특징으로 합니다. 사이트에서 xxx 기사를 수집하고 샘플링 온도가 3인 19개 카테고리 모두에서 1500개의 항목을 샘플링했다. 원본 데이터는 HTML에 있으므로 HTML을 파싱하고 마크다운을 사용하여 기사 콘텐츠를 연결한다. 그 후, 우리는 저품질 데이터(예: 잘못된 수식 변환)와 3000 단어 길이를 초과하는 기사를 필터링했다. 제목을 지침으로 사용하고 기사 내용을 응답으로 사용합니다. **중국 백과사전** 은 도메인 전문가가 작성 및 수정한 약 50만 개의 항목으로 구성된 포괄적인 백과사전입니다. 개념 설명 작업을 위한 다양한 프롬프트 템플릿을 설계합니다. 우리는 각각의 내용과 함께 엔트리 이름과 여러 자막으로 구성된 구조로 모든 74개 범주의 엔트리를 샘플링한다. 명령어를 구성하기 위해 입력 이름 또는 자막을 프롬프트 템플릿과 무작위로 결합했다. 예를 들어, "전기", "학설", "영향력"이라는 자막이 포함된 "공자" 항목에 대해 "학설"을 선택하여 "공자의 학설 내용을 쓰다"와 같은 지도를 만든 다음, 이 자막의 내용을 응답으로 사용한다.

각주 1: [https://10why.net/](https://10why.net/)

각주 2: [https://zh.wikihow.com](https://zh.wikihow.com)

각주 3: [https://www.zgbk.com/](https://www.zgbk.com/)

#### 3.2.2 Domain Specific Knowledge

우리는 의학, 경제 관리, 전자 및 농업의 4가지 특정 영역에서 데이터를 수집했다.

Baobaozhidao, Qianwen Health 및 Baikemingyi의 세 가지 웹 사이트의 **의료 도메인** 원본입니다. **Baobaozhidao** 및 **Qianwen Health** 모두 의료 전문가가 작성한 문답식 기사를 특징으로 하며 전자는 주로 광범위한 의료 분야에 초점을 맞추고 후자는 산모와 유아 건강에 중점을 둡니다. 제목이 문제되지 않는 사이트를 제외하고 이 두 사이트에서 기사를 수집했다. 그 후, 제목을 지침으로, 기사 내용을 응답으로 사용했다. **바이케밍이**에는 수만 가지 질병과 약물에 대한 소개를 특징으로 하는 위키피디아 스타일의 구조화된 데이터가 포함되어 있습니다. 우리는 다양한 프롬프트 템플릿과 이러한 템플릿과 결합된 엔트리 이름을 설계하여 명령을 구성했다(예: "관절 통증에 대한 전문적인 소개를 작성").

**경제 관리 도메인** 데이터는 Wikipedia 스타일의 구조화된 지식을 포함하는 웹 사이트인 MBA 위키 백과사전에서 수집되며, 수많은 기여자가 작성 및 수정합니다. 우리는 "다음 용어를 자세히 설명해 주세요: 송금 에이전트"와 같은 지침을 구성하기 위해 엔트리 이름과 랜덤 템플릿을 결합하여 다양한 프롬프트 템플릿을 설계했습니다. 궁극적으로, 엔트리의 내용은 연결되고 마크다운 형식의 응답으로 구성된다.

**전자 도메인** 데이터는 EE-Trees 전자 백과사전에서 원본으로 사용되며 형식도 구성됩니다. 다양한 프롬프트 템플릿을 설계하고 이를 엔트리 이름과 결합하여 해당 콘텐츠를 응답으로 사용하여 지침을 구성합니다.

식물 재배에서 동물 사육에 이르기까지 다양한 주제를 포함하는 농업 백과사전 웹사이트의 **농업 도메인** 소스. 비질문 제목, 이미지가 포함되거나 길이가 300개 미만인 항목을 제외한 10개 주제에 대한 기사를 모두 수집했다. 그 후, 논문의 제목과 내용으로부터 (지시, 응답) 쌍을 구성한다.

### Examinations

**중학교 및 대학 입학 시험** 은 주로 무해하고 유용하며 다양한 중국 교육 데이터 세트인 COIG 데이터 세트[22]에서 파생됩니다. 중국 시험은 그 일부이며 중학교와 대학 입학 시험은 중국의 주요 일반 능력 시험이다. 이들 자료에는 인문 교과(중국어, 영어, 정치, 생물, 역사, 지리)를 주로 다루는 다양한 질문 유형과 상세한 답변 설명이 포함되어 있다. 우리는 이 주제에 걸친 데이터에 대한 온도 샘플링을 사용한 다음 형식 오류가 있는 질문과 답변을 걸러낸다. 질문은 지침으로 사용되었으며 "답변"과 "분석" 필드가 연결되어 확장된 응답을 형성하여 1964(지침, 응답) 쌍이 생성되었다.

입학시험은 중국에서 가장 어려운 시험 중 하나로 대학 입시를 능가하고 고도의 지식 적용과 깊이가 필요하다. 우리는 최근 몇 년 동안 수학, 컴퓨터 과학, 화학, 법, 심리학, 의학 등을 포함한 학문에 걸쳐 다양한 시험지를 수집했습니다. 이미지-텍스트 변환을 위해 Mathpix4를 사용하여 질문과 답변을 추출하고 LaTeX 형식으로 변환했다. 분석 없이 데이터를 제거하고 질문과 답변의 정확성을 수동으로 검증합니다. 분석 없이 데이터를 제거하고 질문과 답변의 정확성을 수동으로 검증합니다.

각주 4: [https://mathpix.com/](https://mathpix.com/)

논리적 추론 테스트레임은 문제를 해결하기 위해 논리적 및 분석적 추론을 적용하는 능력을 평가한다. 이러한 유형의 검사는 비판적 사고력과 문제 해결력을 평가하기 위한 다양한 경쟁 시험에서 널리 사용되고 있다. 우리는 인터넷에서 논리 추론 질문을 수집하고 상세한 답변 분석이 포함된 질문을 유지한 다음 (명령, 응답) 쌍으로 구성한다.

중국문화시험은 중국 전통문화와 역사에 대한 숙달을 조사한다. 이를 위해 중국 전통문화에 대한 객관식 문항을 인터넷상에서 수집하였고, 이에 대한 답안 분석을 실시하여 (지도, 응답) 쌍으로 구성하였다.

### NLP Datasets

Coig-PcThe COIG-PC Dataset은 중국 NLP 연구의 발전을 목표로 하는 중국 NLP 작업의 포괄적인 모음이다. 이 데이터 세트의 목표는 연구자와 개발자에게 풍부한 리소스 세트를 제공하는 것이며, 이는 중국 텍스트를 다루는 언어 모델의 능력을 향상시키는 데 사용할 수 있다. 연구자와 개발자를 위한 포괄적인 리소스 세트를 제공하여 텍스트 생성, 정보 추출, 감정 분석 및 기계 번역 등을 포함한 다양한 도메인에 걸쳐 언어 모델 기능의 발전을 촉진한다. 처음에는 COIG-PC에서 중국어와 영어를 모두 포함하는 1,413개의 과제를 선택했다. 그런 다음, 주로 전통적인 NLP 데이터 세트에서 출처된 정보 추출, 분류, 요약 및 기타를 포함하여 품질 기준을 충족하는 250개의 작업을 수동으로 선택한다. 온도 샘플링을 통해 최종적으로 3,000개(명령, 응답) 쌍을 샘플링하며, 이는 품질을 보장하기 위해 인간에 의해 추가로 검증된다.

Coig Human Valueis는 인간 값과 정렬된 명령 미세 조정 데이터를 제공하도록 설계된 COIG datasetZhang et al.(2023)의 하위 집합이다. 자체-인스트럭트왕(Self-InstructWang et al. (2023) 방법을 사용하여 구축된 중국 문화 가치를 반영하는 부분을 수동으로 선택한 종자 지침에서 선택했다. 우리는 형식 오류와 오답이 있는 데이터를 수동으로 필터링하여 (지시, 응답) 쌍을 형성하기 위한 답변에 대한 설명을 포함하는 데이터를 유지했다.

반딧불 중국 전통은 중국 전통 문화와 관련된 반딧불 데이터세트 양(2023)의 하위 집합인 고전 중국 번역, 고대 시 쓰기, 이돔 해석의 세 가지 작업으로 구성된다. 300자보다 짧은 응답을 필터링하고 각 작업에서 300개의 인스턴스를 샘플링합니다. 그런 다음 명령어-응답 불일치, 응답 오류 및 응답할 수 없는 명령어와 같은 저품질 데이터를 수동으로 필터링했다.

반차별과 공감의 문제를 다루는 100포이즌맙트는 법학, 심리학, 아동 교육, 모호한 사실, 친밀한 관계 등을 포함한 다양한 차원에 걸쳐 있다. 그것은 편견과 차별을 불러일으키는 인간 생성 프롬프트와 인간 가치와 일치하는 전문가 조작 응답을 포함한다. CQIA의 무해성을 향상시키기 위해 100PoisonMpts의 모든 데이터를 샘플링한다.

## 4 데이터 분석

### Statistics

표 1은 모든 소스에 대한 데이터 통계를 설명합니다. 우리는 중국 인터넷과 커뮤니티의 22개 출처에서 총 48,375건의 사례를 수집하여 일반 지식과 STEM에서 인문학에 이르는 영역을 다루었다. 그림 2는 정보 추출, 질의 응답, 코드 생성 등을 포괄하는 다양한 작업 유형을 보여준다. 우리는 그림 3의 지침 및 응답 길이의 분포를 시연했다.

### Diversity

COIG-CQIA 데이터세트의 다양성을 분석하기 위해 선행 연구 Wang et al. (2023); Lou et al. (2023); Hanlp toolHe and Choi (2021)를 사용하여 명령어를 파싱한 후 상위 직접 명사 객체와 함께 뿌리에 가장 가까운 동사를 추출한다. 그런 다음 그림 1에서 상위 20개의 가장 일반적인 근동사와 해당 직접 명사 객체를 플로팅한다. 이 그림에서 우리는 CQIA가 다양한 범위의 명령과 의도를 가지고 있음을 관찰할 수 있다.

## 5 실험 설정

이 섹션에서는 COIG-CQIA를 사용하여 모델을 미세 조정하고 평가 방법을 자세히 설명한다.

### Evaluation

C-Evalis는 기초 모델에 대한 포괄적인 중국 평가 제품군입니다. 52개의 다양한 학문과 4개의 난이도에 걸쳐 13948개의 객관식 문항으로 구성되어 있다. 모형의 최종 예측으로 로그 우도가 가장 높은 답안 옵션을 선택한다.

CmmlU는 중국어와 문화의 맥락에서 LLM의 지식과 추론 능력을 평가하기 위해 특별히 설계된 포괄적인 평가 벤치마크이다. CMMLU는 초등에서 고급 전문가 수준에 이르는 67개의 주제로 구성된 광범위한 주제를 다룬다. 물리학, 수학 등 계산적 전문지식이 필요한 과목과 인문사회과학 분야의 학문을 포함한다.

벨-에발리스는 개방형 질문 답변, 브레인스토밍, 수학, 코딩 등을 포함한 다양한 영역에 걸쳐 12개의 서로 다른 수업 유형으로 구성된 개방형 테스트 세트이다. 지시를 따르는 모델의 능력을 평가하는 데 사용할 수 있습니다.

그림 1: CQIA 데이터셋에서 가장 일반적인 근동사(내부 원)와 최상위 직접 명사 객체(외부 원)이다. 우리는 특정 동사-명사 쌍이 30개 이상의 인스턴스를 가질 때만 시각화하며 많은 명령에는 동사-명사 구조가 포함되어 있지 않다.

그림 3: 지시 및 응답의 길이 분포입니다. 명령어는 데이터 세트에서 원래 명령어와 입력의 연결입니다.

그림 2: CQIA 작업 유형 개요입니다.

[MISSING_PAGE_FAIL:7]

수학 과제 이는 \(\mathtt{Exam}\)에 더 많은 수학 퀴즈와 시험 유형(예: 독해)이 포함되어 있어 대부분의 작업에서 모델의 성능을 향상시킬 수 있습니다. 흥미롭게도 \(\mathtt{Ruozhiba}\)는 모든 하위 집합에서 평균 2위를 차지합니다. 이는 모델의 논리적 추론 능력을 향상시켜 대부분의 지시 후속 작업에 도움이 될 수 있기 때문이라고 추측한다. \ (\mathtt{COIG-PC}\)는 C-Eval과 같은 지식 차원의 평가에 능숙함을 보여주지만 Belle-Eval에서는 성능이 떨어진다. 이러한 불일치는 전통적인 NLP 데이터 세트의 출처와 짧은 응답 길이로 인해 추론 작업을 손상시킬 수 있고 모델 기반 평가자가 덜 선호하기 때문이다. C-Eval과 Belle-Eval 사이의 상당한 격차는 중국 LLM을 포괄적이고 정확하게 평가할 수 있는 평가 개발의 중요성을 강조한다. 더욱이 위키하우의 점수는 5.8에 불과하며, 이는 "방법" 지침의 다양성이 부족하기 때문이라고 생각한다.

### Human Evaluation

자동 평가 외에도 CQIA 하위 집합에서 Yi-6B 미세 조정을 유사한 매개변수 규모의 최신 중국 오픈 소스 채팅 모델과 비교하여 평가한다. 우리는 실제 중국어를 사용하는 사용자들이 제기하는 질문들에 초점을 맞추고 있다. 우리는 인간 평가를 위한 훈련 세트에 존재하지 않는 OL-CC5 및 Zhihu의 200개 질문을 샘플링한다. 우리는 우리의 모델이 실제 인간 프롬프트에 직면할 때 다른 모델들과 비교하여 어떻게 수행되는지를 입증하기 위해 쌍별 비교를 수행한다.

각주 5: [https://data.baai.ac.cn/details/OL-CC](https://data.baai.ac.cn/details/OL-CC)

각 프롬프트에 대해 각 모델로부터 각각 하나의 응답을 생성한다. 그리고 각 응답은 \(\mathtt{CQIA}\) 모델에 의해 생성된 응답과 다른 기준 모델에 의해 생성된 응답으로 표시된다. 그 후, 주석자가 어떤 응답을 선호하는지 질문하여 더 나은 응답이 판단하기 어려울 때 "넥타이" 선택을 허용한다.

각주 6: \(p\)=0.85, \(k\)=50 및 온도=0.9로 핵 샘플링을 사용하여 생성한다.

그림 4는 \(\mathtt{CQIA}\) 및 기타 5개 기준선, 즉 Yi-6B-Chat, 바이촨2-7B-Chat, ChatGLM2-6B, Qwen-7B-Chat 및 InternLM-7B-Chat에 대한 인간 평가 결과를 보여준다. 그 결과, \(\mathtt{CQIA}\)-하위 집합은 강한 기준선에 비해 더 높은 인간 선호도를 달성했으며, 최소 60% 이상의 응답이 기준선 모델보다 우수하거나 동등하다는 것을 보여준다. 이는 \(\mathtt{CQIA}\)가 인간의 질문이나 지시에 대한 고품질 응답을 생성하는 것뿐만 아니라 실제 인간 커뮤니케이션 패턴과 더 정렬되어 인간의 선호도가 높기 때문일 수 있다.

### 크기 조정 모델 크기

우리는 CQIA-Subset에서 미세 조정 후 다양한 파라미터 크기를 갖는 다른 기본 모델의 성능을 조사한다. 특히 Yi-6B는 \(\mathtt{Qwen}\)-14B와 InternLM-20B를 능가한다. 또한 Yi-34B는 C-Eval 벤치마크와 CMMLU 벤치마크 모두에서 \(\mathtt{Qwen}\)-72B와 유사한 결과를 얻었다. 이 관측기는 모델 크기, 아키텍처 최적화 및 훈련 방법론 간의 균형을 강조합니다. 스케일링 법칙은 더 큰 모델이 언어 이해 능력 증가로 인해 본질적으로 더 나은 성능을 발휘한다는 것을 시사할 수 있지만, 우리의 결과는 이것이 항상 그렇지 않다는 것을 나타낸다. Specif

\begin{table}
\begin{tabular}{l c} \hline \hline Model & SafetyBench \\ \hline COIG PC & 81.2 \\ Chinese Traditional & 76.6 \\ Douban & 76.2 \\ Exam & 77.6 \\ Finance & 75.1 \\ Logi QA & 79.1 \\ Ruozhiba & 81.3 \\ Segmentfault & 78.0 \\ Wiki & 75.8 \\ Wikihow & 76.4 \\ Xhs & 76.0 \\ Zhihu & 75.8 \\ Human Value & 79.1 \\ \hline \(\mathtt{CQIA}\)-Sub-6B & **81.7** \\ \hline GPT-4-0613 & 89.2 \\ GPT-3.5-turbo-0613 & 80.4 \\ \hline \hline \end{tabular}
\end{table}
표 4: 다양한 데이터 소스에 대해 훈련된 Yi-6B의 안전 벤치 점수.

\begin{table}
\begin{tabular}{l c c} \hline \hline Model & \(\mathtt{Ceval}\) (val 5-shot) & CMMLU (test 5-shot) \\ \hline \(\mathtt{Qwen}\)-1.8b & 51.34 & 47.26 \\ Yi-6B & 73.40 & 74.85 \\ \(\mathtt{Qwen}\)-14b & 68.20 & 67.96 \\ InternLM2-20b & 71.25 & 67.48 \\ Yi-34b & 77.04 & 78.18 \\ \(\mathtt{Qwen}\)-72b & 78.68 & 76.79 \\ \hline \hline \end{tabular}
\end{table}
표 5: COIG 서브셋 데이터에 대한 트레이닝 후의 상이한 베이스 모델들의 성능.

놀랍게도, 훨씬 더 많은 매개변수를 가진 모델에 대한 Yi-6B 모델의 우수한 성능은 매개변수 수만으로도 모델 효능의 충분한 예측 변수라는 개념에 도전한다.

### Safety

안전 벤치에서 모델을 평가하여 데이터 소스가 모델 안전에 미치는 영향을 조사합니다. CQIA-Subset에서 훈련된 모델은 GPT-3.5-turbo-0613을 능가하여 CQIA 시리즈 내에서 가장 높은 점수를 받았다. Douban, Zhihu, Xhs와 같은 소셜 미디어&포럼에서 훈련된 모델은 중간 정도의 안전 점수를 수행하는데, 이는 소셜 미디어 콘텐츠의 다양성과 개방성 때문이며, 이는 또한 유해한 정보의 위험을 강조한다. 또한 위키 스타일 데이터에 대해 훈련된 모델은 더 낮은 안전 점수를 수행하는 경향이 있으며, 이는 잠재적으로 전문 데이터 소스 내의 제한된 다양성을 반영하여 특수 영역 외부의 안전 문제에 대한 성능 저하로 이어진다.

## 7 Conclusion

본 논문에서는 고품질의 중국어 명령어 미세조정 데이터셋을 소개한다. COIG-CQIA는 Q&A 및 기사를 포함한 중국 인터넷 소스에서 데이터 세트를 만드는 데 중점을 둔다. 품질, 다양성 및 관련성을 보장하기 위해 심층 세척, 재구성 및 수동으로 검토합니다. 이 데이터 세트는 중국 NLP 커뮤니티에 고품질 및 인간 상호 작용 정렬 명령 미세 조정 데이터를 제공하도록 설계되었다.

## References

* Bai 등(2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. _ arXiv preprint arXiv:2309.16609_.
* Brown 등(2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models is few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901.
* Chen et al. (2023) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2023. Alpagasus: 적은 데이터로 더 나은 알파카를 훈련합니다.
* Chowdrey 등(2023) Aakanksha Chowdrey, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. _ Journal of Machine Learning Research_, 24(240):1-113.
* 정 등(2022) 형원정, 레 호우, 샤인 롱프리, 바렛 조프, 이타이, 윌리엄 페더스, 윤수안 리, 쉬에지 왕, 모스타파 데흐하니, 싯다르타 브라흐마, 알버트 웹슨, 시샹 셰인 구, 주윤 다이, 미락 수즈건, 신윤 첸, 아칸크샤 초드레이, 알렉스 카스트로-로스, 마리 펠라트, 케빈 로빈슨, 다샤 발터, 샤란 나랑, 가우라브 미슈라, 애덤스 유, 빈센트 자오, 옌핑 황, 앤드류 다이, 홍쿤 유, 슬라브 페트로프, 에드 H. 치, 제프 딘, 제이콥 데블린, 애덤 로버츠, 데니 저우, 콕 V. 레, 제이슨 웨이 2022. Scaling instruction-finetuned language models.
* CLUEbenchmark (2022) CLUEbenchmark. 2022. pclue: Large-scale prompt-based dataset for multi-task and zero-shot learning in Chinese.
* Conover 등(2023) Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023년. 무료 돌리: 세계 최초로 진정한 개방형 명령어 조정 llvm을 소개합니다.
* Guo et al. (2023) Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. 인간 전문가들과 얼마나 가까운가? 비교 코퍼스, 평가 및 탐지.
*He and Choi (2021) Han He and Jinho D Choi. 2021. 줄기세포 가설: 변압기 인코더를 사용한 다중 작업 학습 뒤에 딜레마입니다. _ arXiv preprint arXiv:2109.06939_.
* Honovich et al.(2022) 또는 Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. 부자연스러운 지시: 인간 노동력이 거의 없는 언어 모델을 조정합니다.
* Ivison 등(2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. 변화하는 기후의 낙타: tubu 2를 사용한 적응력 향상.
* Ji et al.(2023) Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiang Li. 2023. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases.
* Konchakov et al. (2023) R. A. Konchakov, A. S. Makarov, G. V. Afonin, J. C. Qiao, M. G. Vasin, N. P. Kobelev, and V. A. Khonik. 2023. 금속유리의 유리전이 부근에서의 변동열용량의 임계거동.
* Li et al.(2023) Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023. 명령어 역번역과의 자기 정렬.
* Lou et al.(2023) Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu Su, and Wenpeng Yin. 2023. Muffin: Curating multi-faceted instructions for improving instruction following. <표상학습에 관한 제12차 국제회의>에서.

스와루프 미슈라, 다니엘 카샤비 치타 바랄, 한나네 하지시르지 2022. 자연어 크라우드소싱 지침을 통한 교차 작업 일반화.
* Peng 등(2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_.
* Sanh et al.(2022) Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Allyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Tishala Neeraj, Jos Rozen, Abheeseth Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. 러쉬 2022. Multi-task prompted training enables zero-shot task generalization.
* Song et al. (2023) Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei, Zhenzhong Lan, and Yue Zhang. 2023. Dynamics of instruction tuning: Each ability of large language models are own growth pace.
* Sun et al. (2023) Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Hang Yan, Xiangyang Liu, Yunfan Shao, Qiong Tang, Xingjian Zo, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing Huang, and Xipeng Qiu. 2023. 모스: 합성 데이터로부터 회화 언어 모델을 훈련시킨다.
* 팀 (2023) InternLM 팀. 2023. Internlm: 점진적으로 향상된 기능을 가진 다국어 언어 모델. [https://github.com/InternLM/InternLM] (https://github.com/InternLM/InternLM).
* Touvron 등(2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_.
* Wang et al. (2023) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions.
* Xu et al.(2023a) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a. 마법사: 복잡한 지침을 따르도록 큰 언어 모델을 사용할 수 있습니다.
*Xu et al.(2023b) Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023b. Baize: 자체 채팅 데이터에 대한 매개변수 효율적인 튜닝이 있는 오픈 소스 채팅 모델입니다.
* Yang (2023) Jianxin Yang. 2023. Firefly(Xie):. [https://github.com/yangjianxin1/Firefly] (https://github.com/yangjianxin1/Firefly).
* Young et al.(2024) Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. 2024. Yi: Open foundation models by 01. ai. _ arXiv preprint arXiv:2403.04652_.
*Zhang et al.(2023a) Ge Zhang, Yemin Shi, Ruibo Liu, Ruibin Yuan, Yizhi Li, Siwei Dong, Yu Shu, Zhaoqun Li, Zekun Wang, Chenghua Lin, Wenhao Huang, and Jie Fu. 2023a. 중국 공개 교육 일반론자: 예비 공개.
* Zhang 등(2023b) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023b. 대형 언어 모델에 대한 지침 조정: 설문 조사 _ arXiv preprint arXiv:2308.10792_.
* Zhou 등(2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. 리마: 정렬을 위한 것이 더 적습니다.

<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.03129] Examining Forgetting in Continual Pre-training of Aligned Large Language Models</title><meta property="og:description" content="Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In develo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Examining Forgetting in Continual Pre-training of Aligned Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Examining Forgetting in Continual Pre-training of Aligned Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.03129">

<!--Generated on Tue Feb 27 10:39:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Examining Forgetting in Continual Pre-training of Aligned Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chen-An Li<sup id="id5.5.id1" class="ltx_sup"><span id="id5.5.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Hung-Yi Lee<sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">1</span></sup> 
<br class="ltx_break"><sup id="id7.7.id3" class="ltx_sup"><span id="id7.7.id3.1" class="ltx_text ltx_font_italic">1</span></sup>National Taiwan University, Taipei, Taiwan 
<br class="ltx_break"><sup id="id8.8.id4" class="ltx_sup"><span id="id8.8.id4.1" class="ltx_text ltx_font_italic">2</span></sup>ASUS Open Cloud Infrastructure Software Center, Taipei, Taiwan 
<br class="ltx_break"><span id="id9.9.id5" class="ltx_text ltx_font_typewriter">b08902123@csie.ntu.edu.tw</span>  <span id="id10.10.id6" class="ltx_text ltx_font_typewriter">hungyilee@ntu.edu.tw</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">최근 대규모 언어 모델(Large Language Model, LLM)의 발전은 다양한 작업에 걸쳐 놀라운 능력을 보여주었다. 수많은 분야에서 LLM이 강력하게 적용된다는 점을 감안할 때 LLM 개발이 급증했다. LLM을 개발할 때 일반적인 관행은 이전에 미세 조정된 모델에 대한 지속적인 사전 훈련을 포함한다. 그러나 이것은 재앙적인 망각으로 이어질 수 있다. 우리의 연구에서는 기존의 미세 조정된 LLM에 대한 지속적인 사전 훈련 중에 발생하는 망각의 현상을 조사한다. 지속적인 사전 훈련이 출력 형식, 지식 및 신뢰성을 포함한 다양한 차원에 걸쳐 미세 조정된 LLM에 미치는 영향을 평가한다. 실험 결과는 지속적인 사전 훈련, 특히 반복 문제 동안 치명적인 망각을 해결하는 데 있어 중요하지 않은 문제를 강조한다.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.4" class="ltx_block ltx_align_bottom">
<p class="ltx_p" id="p1.4.5"><span class="ltx_text ltx_font_bold" id="p1.4.5.1">Examining Forgetting in Continual Pre-training of Aligned Large Language Models</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.4.4" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.4.4.4" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.4.4.4.4" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.2.2.2.2.2" class="ltx_tr">
<span id="p1.2.2.2.2.2.2" class="ltx_td ltx_align_center"><span id="p1.2.2.2.2.2.2.2" class="ltx_text ltx_font_bold">Chen-An Li<sup id="p1.2.2.2.2.2.2.2.1" class="ltx_sup"><span id="p1.2.2.2.2.2.2.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>  Hung-Yi Lee<sup id="p1.2.2.2.2.2.2.2.2" class="ltx_sup"><span id="p1.2.2.2.2.2.2.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span></span></span>
<span id="p1.3.3.3.3.3" class="ltx_tr">
<span id="p1.3.3.3.3.3.1" class="ltx_td ltx_align_center"><sup id="p1.3.3.3.3.3.1.1" class="ltx_sup"><span id="p1.3.3.3.3.3.1.1.1" class="ltx_text ltx_font_italic">1</span></sup>National Taiwan University, Taipei, Taiwan</span></span>
<span id="p1.4.4.4.4.4" class="ltx_tr">
<span id="p1.4.4.4.4.4.1" class="ltx_td ltx_align_center"><sup id="p1.4.4.4.4.4.1.1" class="ltx_sup"><span id="p1.4.4.4.4.4.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>ASUS Open Cloud Infrastructure Software Center, Taipei, Taiwan</span></span>
<span id="p1.4.4.4.4.5.1" class="ltx_tr">
<span id="p1.4.4.4.4.5.1.1" class="ltx_td ltx_align_center"><span id="p1.4.4.4.4.5.1.1.1" class="ltx_text ltx_font_typewriter">b08902123@csie.ntu.edu.tw</span>  <span id="p1.4.4.4.4.5.1.1.2" class="ltx_text ltx_font_typewriter">hungyilee@ntu.edu.tw</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">LLM(Large Language Models)은 다양한 태스크들 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="#bib.bib1" title="">2020</a>)</cite>에 걸쳐 인상적인 성능을 보여주었다. 사전 훈련된 LLM 및 미세 조정된 변형 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="#bib.bib31" title="">2023a</a>, <a class="ltx_ref" href="#bib.bib32" title="">b</a>)</cite>를 출시하는 경향이 증가하고 있다. 이러한 미세 조정된 변형의 대부분은 기존 LLM <cite class="ltx_cite ltx_citemacro_cite">Roziere et al. (<a class="ltx_ref" href="#bib.bib28" title="">2023</a>); Cui et al. (<a class="ltx_ref" href="#bib.bib3" title="">2023</a>)</cite>의 지식 또는 언어 능력을 향상시키는 것을 목표로 한다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">2. Supervised Fine-Tuning (SFT) 및 Reinforcement Learning from Human Feedback (RLHF)과 같은 후속 정렬 작업을 단계 1에서 얻은 모델에 대해 수행한다. 이러한 미세 조정 변형 중 많은 개발이 기존 미세 조정 LLMs <cite class="ltx_cite ltx_citemacro_cite">Cui et al. (<a class="ltx_ref" href="#bib.bib3" title="">2023</a>); Lin and Chen (<a class="ltx_ref" href="#bib.bib21" title="">2023</a>)</cite>에 대해 추가 연속 사전 훈련을 수행한다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">이전 연구에서는 지속적인 사전 훈련이 모델의 특정 콘텐츠 <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a class="ltx_ref" href="#bib.bib8" title="">2023</a>)</cite>를 이해하고 생성하는 능력을 크게 향상시킬 수 있음을 보여주었다. 그러나 지속적인 사전 훈련은 치명적인 망각 <cite class="ltx_cite ltx_citemacro_cite">French (<a class="ltx_ref" href="#bib.bib6" title="">1999</a>)</cite>로 이어질 수 있으며, 제한된 연구는 기존의 미세 조정된 LLM에서 사전 훈련 중에 잊혀진 능력을 탐구했다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">일부 연구에서는 언어 모델에 대한 지속적인 학습을 연구했다. <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a class="ltx_ref" href="#bib.bib26" title="">2022</a>)</cite>는 떠오르는 데이터에 대한 사전 훈련된 언어 모델에 대한 효율적인 평생 사전 훈련에 초점을 맞췄다. <cite class="ltx_cite ltx_citemacro_cite">Ke et al. (<a class="ltx_ref" href="#bib.bib18" title="">2022</a>)</cite>는 마스킹된 언어 모델에서 연속적인 도메인 적응적 사전 학습 방법을 제안하였다. <cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a class="ltx_ref" href="#bib.bib29" title="">2023</a>)</cite>는 지속적인 작업에 대한 LLM의 지속적인 적응을 위한 지속적인 매개변수 효율적인 조정을 도입했다. <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a class="ltx_ref" href="#bib.bib35" title="">2023</a>)</cite>는 도메인별 LLMs 개발을 위한 지속적인 사전 훈련에 대한 대안적인 접근법을 조사한다. <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>는 미세 조정이 LLM의 안전 정렬을 손상시킨다는 것을 시사한다. <cite class="ltx_cite ltx_citemacro_cite">Zhai et al. (<a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>는 미세 조정된 멀티모달 LLM에서 망각을 평가한다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">우리의 연구는 기존의 미세 조정된 LLM에 대한 지속적인 사전 훈련 중 망각 발생을 조사한다. 우리의 논문은 주로 중국 전통 말뭉치를 이용한 지속적인 사전 훈련에 초점을 맞추고 있다. 우리는 출력 형식, 지식 및 신뢰성을 포함한 다양한 차원에 걸쳐 지속적인 사전 훈련의 영향을 평가한다. 우리는 이 문제를 해결하기 위해 단순한 방법 이상의 방법이 필요하다는 것을 보여준다. 또한 전통적인 중국어 출력을 생성하는 경향이 있는 모델에서 반복 문제의 중요성이 증가했음을 관찰한다. 마지막으로, 지속적인 사전 훈련에도 불구하고, 우리의 연구 결과는 모델의 지식이 영향을 받지 않는 반면 신뢰성은 감소한다는 것을 시사한다.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Observation of Catastrophic Forgetting during Continual Pre-training</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Settings for Observation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p1.1">SFT 및 RLHF를 포함하여 순차적 정렬 작업을 거친 약 70억 개의 매개변수로 구성된 모델인 라마-2-7b-chat에 대한 사전 훈련을 수행한다. 사전 훈련 프로세스는 중국 전통 데이터의 10억 토큰을 활용합니다. 우리는 지속적인 사전 훈련 후 모델을 라마-2-7b-chat-cp로 나타낸다. 우리는 두 모델에 의해 생성된 출력 간의 차이를 관찰하기 위해 특정 프롬프트를 사용한다.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 1:</span>Illustration of models’ outputs. 우리 프롬프트의 번역은 “멕시코시티에 대해 말해주세요”입니다.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Observation of Catastrophic Forgetting</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p1.1">그림 <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 Settings for Observation ‣ 2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>는 프롬프트에서 얻은 결과를 보여준다. 우리는 Llama-2-7b-chat-cp가 Llama-2-7b-chat에 비해 전통적인 중국어 텍스트를 생성하는 경향이 있음을 관찰했지만, 생성된 Llama-2-7b-chat 텍스트는 반복 문제를 나타낸다. 결과적으로 우리는 다양한 측면에 걸쳐 모델의 성능에 대한 보다 심층적인 조사를 수행했다. 부록 <a class="ltx_ref" href="#A1" title="Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>는 더 많은 프롬프트의 추가 결과를 포함합니다.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Straightforward Approaches</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">이 섹션에서는 이전 섹션에서 논의된 치명적인 망각 문제를 해결하기 위한 간단한 접근법을 소개한다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Freeze layers</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.1">이전 연구에서는 텍스트 정보 <cite class="ltx_cite ltx_citemacro_cite">Ethayarajh (<a class="ltx_ref" href="#bib.bib5" title="">2019</a>); Van Aken et al. (<a class="ltx_ref" href="#bib.bib33" title="">2019</a>)</cite>를 처리할 때 Transformer 기반 모델의 다른 계층에 뚜렷한 기능이 존재하는 것으로 나타났다. 결과적으로, 우리는 지속적인 사전 훈련 동안 모델의 특정 층을 동결하는 것을 실험한다. 구체적으로, <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.1">Freeze First 10</span> 및 <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.2">Freeze Last 10</span>으로 각각 나타낸다.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Freeze modules</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.1">또한 지속적인 사전 훈련 동안 모델의 특정 모듈을 동결하여 실험을 수행한다. 이러한 지정된 모듈이 정렬 작업 중에 획득한 능력을 보존하는지 여부를 조사하는 것을 목표로 한다. 우리는 네 가지 전략을 탐구합니다.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i1.p1.1.1">Freeze Attn. </span>: 모델의 각 계층에서 자체 주의 모듈을 동결합니다.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i2.p1.1.1">Only Attn. </span>: 모델의 자체 주의 모듈을 제외한 각 레이어의 모든 모듈을 동결합니다.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i3.p1.1.1">Freeze MLP</span>: Freeze the feed-forward modules in each model layer.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i4.p1.1.1">Only MLP</span>: 모델의 feed-forward 모듈을 제외한 각 레이어의 모든 모듈을 동결한다.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Adapter</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.1">Adapters는 Transformer 기반 모델 <cite class="ltx_cite ltx_citemacro_cite">Houlsby et al. (<a class="ltx_ref" href="#bib.bib12" title="">2019</a>)</cite> 학습에 자주 사용된다. 우리 연구에서는 두 가지 유형의 어댑터를 실험합니다.</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I2.i1.p1.1.1">Lora</span> <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="#bib.bib13" title="">2022</a>)</cite>: Transformer 기반 모델의 각 계층에 학습 가능한 저순위 분해 행렬을 통합하는 방법. 구현에서는 모델 내의 각 계층의 질의 행렬과 값 투영 행렬만을 선택적으로 적용한다.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I2.i2.p1.1.1.1">(Ia)<sup class="ltx_sup" id="S3.I2.i2.p1.1.1.1"><span class="ltx_text ltx_font_upright" id="S3.I2.i2.p1.1.1.1">3</span></sup></span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>: 학습된 벡터를 가진 모델의 액티베이션의 요소별 곱셈을 포함하는 기법. 자기 주의 모듈의 키 및 값 행렬과 각 모델 레이어의 피드포워드 모듈의 내부 활성화를 다시 조정한다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p1.1">우리는 일반적인 중국 전통 말뭉치의 10억 토큰 데이터를 사용하여 라마-2-7b-채팅에 대한 지속적인 사전 훈련을 위해 간단한 접근법을 사용했다. 지속적인 사전 훈련 동안의 학습률은 3e-5로 일정하게 유지되었으며, 어댑터 접근법에 대한 추가 학습률 3e-4를 실험했다. 보다 자세한 내용은 부록 <a class="ltx_ref" href="#A2" title="Appendix B Additional Details about Experiment Setup ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">B</span></a>에서 확인할 수 있다.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Tasks</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1">본 연구는 출력 형식, 지식 및 신뢰성과 같은 차원에 따라 모델의 성능을 종합적으로 검토합니다. 자세한 내용은 부록 <a class="ltx_ref" href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">C</span></a>를 참조해 주십시오.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Output format</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">우리는 출력 형식 분석에서 언어 식별과 반복 분석의 두 가지 별개의 작업을 수행한다. 이러한 평가를 수행하기 위해 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS1.p1.1.1">NeuLab-TedTalks</span> <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a class="ltx_ref" href="#bib.bib25" title="">2018</a>)</cite>의 영어 및 번체 중국어 하위 집합에서 2000개의 정렬된 문장을 프롬프트로 무작위로 선택했다.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S4.I1.i1.p1.1">언어 식별: 생성된 토큰의 언어를 검출하기 위해 FastText <cite class="ltx_cite ltx_citemacro_cite">Joulin et al. (<a class="ltx_ref" href="#bib.bib16" title="">2016a</a>, <a class="ltx_ref" href="#bib.bib17" title="">b</a>)</cite> 언어 식별 모델을 사용한다.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.4" class="ltx_p">Repetition: We determine the proportion of duplicated n-gram tokens at the BPE level in the combined output and prompt. This calculation involves the formula: rep-n <math id="S4.I1.i2.p1.1.m1.1" class="ltx_math_unparsed" alttext="=1-|" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mrow id="S4.I1.i2.p1.1.m1.1b"><mo id="S4.I1.i2.p1.1.m1.1.1">=</mo><mn id="S4.I1.i2.p1.1.m1.1.2">1</mn><mo rspace="0em" id="S4.I1.i2.p1.1.m1.1.3">−</mo><mo fence="false" stretchy="false" id="S4.I1.i2.p1.1.m1.1.4">|</mo></mrow><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">=1-|</annotation></semantics></math>unique n-grams<math id="S4.I1.i2.p1.2.m2.1" class="ltx_math_unparsed" alttext="|/|" display="inline"><semantics id="S4.I1.i2.p1.2.m2.1a"><mrow id="S4.I1.i2.p1.2.m2.1b"><mo fence="false" stretchy="false" id="S4.I1.i2.p1.2.m2.1.1">|</mo><mo lspace="0em" rspace="0em" id="S4.I1.i2.p1.2.m2.1.2">/</mo><mo fence="false" stretchy="false" id="S4.I1.i2.p1.2.m2.1.3">|</mo></mrow><annotation encoding="application/x-tex" id="S4.I1.i2.p1.2.m2.1c">|/|</annotation></semantics></math>n-grams<math id="S4.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S4.I1.i2.p1.3.m3.1a"><mo fence="false" stretchy="false" id="S4.I1.i2.p1.3.m3.1.1" xref="S4.I1.i2.p1.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.3.m3.1b"><ci id="S4.I1.i2.p1.3.m3.1.1.cmml" xref="S4.I1.i2.p1.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.3.m3.1c">|</annotation></semantics></math>, where n <math id="S4.I1.i2.p1.4.m4.5" class="ltx_Math" alttext="\in[4,8,12,16,20]" display="inline"><semantics id="S4.I1.i2.p1.4.m4.5a"><mrow id="S4.I1.i2.p1.4.m4.5.6" xref="S4.I1.i2.p1.4.m4.5.6.cmml"><mi id="S4.I1.i2.p1.4.m4.5.6.2" xref="S4.I1.i2.p1.4.m4.5.6.2.cmml"></mi><mo id="S4.I1.i2.p1.4.m4.5.6.1" xref="S4.I1.i2.p1.4.m4.5.6.1.cmml">∈</mo><mrow id="S4.I1.i2.p1.4.m4.5.6.3.2" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml"><mo stretchy="false" id="S4.I1.i2.p1.4.m4.5.6.3.2.1" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">[</mo><mn id="S4.I1.i2.p1.4.m4.1.1" xref="S4.I1.i2.p1.4.m4.1.1.cmml">4</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.2" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.2.2" xref="S4.I1.i2.p1.4.m4.2.2.cmml">8</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.3" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.3.3" xref="S4.I1.i2.p1.4.m4.3.3.cmml">12</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.4" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.4.4" xref="S4.I1.i2.p1.4.m4.4.4.cmml">16</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.5" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.5.5" xref="S4.I1.i2.p1.4.m4.5.5.cmml">20</mn><mo stretchy="false" id="S4.I1.i2.p1.4.m4.5.6.3.2.6" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.4.m4.5b"><apply id="S4.I1.i2.p1.4.m4.5.6.cmml" xref="S4.I1.i2.p1.4.m4.5.6"><in id="S4.I1.i2.p1.4.m4.5.6.1.cmml" xref="S4.I1.i2.p1.4.m4.5.6.1"></in><csymbol cd="latexml" id="S4.I1.i2.p1.4.m4.5.6.2.cmml" xref="S4.I1.i2.p1.4.m4.5.6.2">absent</csymbol><list id="S4.I1.i2.p1.4.m4.5.6.3.1.cmml" xref="S4.I1.i2.p1.4.m4.5.6.3.2"><cn type="integer" id="S4.I1.i2.p1.4.m4.1.1.cmml" xref="S4.I1.i2.p1.4.m4.1.1">4</cn><cn type="integer" id="S4.I1.i2.p1.4.m4.2.2.cmml" xref="S4.I1.i2.p1.4.m4.2.2">8</cn><cn type="integer" id="S4.I1.i2.p1.4.m4.3.3.cmml" xref="S4.I1.i2.p1.4.m4.3.3">12</cn><cn type="integer" id="S4.I1.i2.p1.4.m4.4.4.cmml" xref="S4.I1.i2.p1.4.m4.4.4">16</cn><cn type="integer" id="S4.I1.i2.p1.4.m4.5.5.cmml" xref="S4.I1.i2.p1.4.m4.5.5">20</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.4.m4.5c">\in[4,8,12,16,20]</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Knowledge</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">지식 분석에서 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.1">ARC</span> <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="#bib.bib2" title="">2018</a>)</cite>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.2">Hellaswag</span> <cite class="ltx_cite ltx_citemacro_cite">Zellers et al. (<a class="ltx_ref" href="#bib.bib36" title="">2019</a>)</cite>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.3">MMLU</span> <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="#bib.bib10" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.4">C-eval-</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.1">ARC</span> 및 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.2">Hellaswag</span>는 영어 커먼센스 추론 벤치마크 역할을 하며, 여기서 우리는 길이 정규화 정확도를 우리의 메트릭으로 사용한다. 영문 멀티태스크 벤치마크인 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.3">MMLU</span>과 중국 전통 멀티태스크 벤치마크인 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.4">C-eval-tw</span>의 경우 개별 태스크에 대한 평균 정확도로 메트릭을 계산합니다. 정확도 계산은 확률이 가장 높은 옵션을 선택하는 것을 기반으로 합니다.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Reliability</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">신뢰성 분석은 진실성, 독성 및 편향을 포함하는 세 가지 벤치마크 데이터 세트에 대한 모델의 성능을 평가한다. 우리는 영어와 번체 중국어 모두에서 신뢰도 분석을 고려한다. 이러한 벤치마크는 처음에 영어로 되어 있지만 포괄적인 분석을 위해 데이터 세트를 번체 중국어로 번역한다.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i1.p1.1.1">TruthfulQA</span> <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="#bib.bib20" title="">2022</a>)</cite>: The dataset utilized to measure the truthfulness of language models. 채점 메커니즘은 질문 및 다수의 참/거짓 참조 답변들을 포함하며, 여기서 점수는 참 답변들의 세트에 할당된 정규화된 총 확률에 의해 결정된다.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i2.p1.1.1">ToxiGen</span> <cite class="ltx_cite ltx_citemacro_cite">Hartvigsen et al. (<a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>: 언어 모델의 독성을 탐지하기 위해 사용한 데이터 세트입니다. 독성 세대를 식별하기 위해 기본 RoBERTa 기반 <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib23" title="">2019</a>)</cite> ToxiGen 분류기를 활용한다.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i3.p1.1.1">Bold</span> <cite class="ltx_cite ltx_citemacro_cite">Dhamala et al. (<a class="ltx_ref" href="#bib.bib4" title="">2021</a>)</cite>: 바이어스 분석을 위해 사용하는 데이터 세트입니다. 우리는 결합된 프롬프트와 생성 텍스트에 대한 감정 점수를 계산하기 위해 VADER(Valence Aware Dictionary and Sentiment Reasoner) <cite class="ltx_cite ltx_citemacro_cite">Hutto and Gilbert (<a class="ltx_ref" href="#bib.bib15" title="">2014</a>)</cite>를 사용한다. 우리는 모든 하위 그룹의 감정 점수의 평균과 표준 편차를 보고한다.</p>
</div>
</li>
</ul>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:195.1pt;height:199.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.2pt,35.1pt) scale(0.740209259724579,0.740209259724579) ;">
<table id="S4.T1.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.3.1" class="ltx_tr">
<td id="S4.T1.2.2.3.1.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T1.2.2.3.1.2.1" class="ltx_text ltx_font_bold">EN prompt</span></td>
<td id="S4.T1.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T1.2.2.3.1.3.1" class="ltx_text ltx_font_bold">TW prompt</span></td>
</tr>
<tr id="S4.T1.2.2.4.2" class="ltx_tr">
<td id="S4.T1.2.2.4.2.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.4.2.2.1" class="ltx_text ltx_font_bold">EN %</span></td>
<td id="S4.T1.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.4.2.3.1" class="ltx_text ltx_font_bold">TW %</span></td>
<td id="S4.T1.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.4.2.4.1" class="ltx_text ltx_font_bold">EN %</span></td>
<td id="S4.T1.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.4.2.5.1" class="ltx_text ltx_font_bold">TW %</span></td>
</tr>
<tr id="S4.T1.2.2.5.3" class="ltx_tr">
<td id="S4.T1.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b</td>
<td id="S4.T1.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.75</td>
<td id="S4.T1.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.00</td>
<td id="S4.T1.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.10</td>
<td id="S4.T1.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t">79.45</td>
</tr>
<tr id="S4.T1.2.2.6.4" class="ltx_tr">
<td id="S4.T1.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="S4.T1.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100.00</td>
<td id="S4.T1.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.00</td>
<td id="S4.T1.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.00</td>
<td id="S4.T1.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t">0.95</td>
</tr>
<tr id="S4.T1.2.2.7.5" class="ltx_tr">
<td id="S4.T1.2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="S4.T1.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">99.55</td>
<td id="S4.T1.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.20</td>
<td id="S4.T1.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.00</td>
<td id="S4.T1.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_tt">83.50</td>
</tr>
<tr id="S4.T1.2.2.8.6" class="ltx_tr">
<td id="S4.T1.2.2.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.2.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="S4.T1.2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">99.65</td>
<td id="S4.T1.2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.30</td>
<td id="S4.T1.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.15</td>
<td id="S4.T1.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_tt">89.20</td>
</tr>
<tr id="S4.T1.2.2.9.7" class="ltx_tr">
<td id="S4.T1.2.2.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.9.7.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="S4.T1.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.6</td>
<td id="S4.T1.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.15</td>
<td id="S4.T1.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.05</td>
<td id="S4.T1.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_t">76.25</td>
</tr>
<tr id="S4.T1.2.2.10.8" class="ltx_tr">
<td id="S4.T1.2.2.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.2.10.8.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="S4.T1.2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">99.75</td>
<td id="S4.T1.2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.15</td>
<td id="S4.T1.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">41.05</td>
<td id="S4.T1.2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_tt">58.50</td>
</tr>
<tr id="S4.T1.2.2.11.9" class="ltx_tr">
<td id="S4.T1.2.2.11.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.11.9.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="S4.T1.2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.60</td>
<td id="S4.T1.2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.20</td>
<td id="S4.T1.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37.45</td>
<td id="S4.T1.2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_t">61.95</td>
</tr>
<tr id="S4.T1.2.2.12.10" class="ltx_tr">
<td id="S4.T1.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.12.10.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="S4.T1.2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.65</td>
<td id="S4.T1.2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.15</td>
<td id="S4.T1.2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.50</td>
<td id="S4.T1.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_t">63.80</td>
</tr>
<tr id="S4.T1.2.2.13.11" class="ltx_tr">
<td id="S4.T1.2.2.13.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.13.11.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="S4.T1.2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.80</td>
<td id="S4.T1.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.10</td>
<td id="S4.T1.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.65</td>
<td id="S4.T1.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_t">58.60</td>
</tr>
<tr id="S4.T1.2.2.14.12" class="ltx_tr">
<td id="S4.T1.2.2.14.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="S4.T1.2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">99.95</td>
<td id="S4.T1.2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.00</td>
<td id="S4.T1.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">70.85</td>
<td id="S4.T1.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_tt">28.85</td>
</tr>
<tr id="S4.T1.2.2.15.13" class="ltx_tr">
<td id="S4.T1.2.2.15.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T1.2.2.15.13.1.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="S4.T1.2.2.15.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.50</td>
<td id="S4.T1.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.30</td>
<td id="S4.T1.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.25</td>
<td id="S4.T1.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_t">90.85</td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T1.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100.00</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.00</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">98.90</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">1.10</td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T1.2.2.2.1.1.1" class="ltx_sup"><span id="S4.T1.2.2.2.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">100.00</td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.00</td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">95.85</td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">4.05</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>언어 식별 분석 결과. <span class="ltx_text ltx_font_bold" id="S4.T1.7.1">EN prompt</span>은 영어 프롬프트의 사용을 나타내고, <span class="ltx_text ltx_font_bold" id="S4.T1.8.2">TW prompt</span>은 중국어 프롬프트의 사용을 나타낸다. <span class="ltx_text ltx_font_bold" id="S4.T1.9.3">EN %</span>은 영어로 식별된 출력의 백분율을 나타내고, <span class="ltx_text ltx_font_bold" id="S4.T1.10.4">TW %</span>은 중국어로 식별된 백분율을 나타낸다.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:182.1pt;height:202.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.3pt,33.7pt) scale(0.750577457251184,0.750577457251184) ;">
<table id="S4.T2.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.3.1" class="ltx_tr">
<td id="S4.T2.2.2.3.1.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T2.2.2.3.1.2.1" class="ltx_text ltx_font_bold">EN prompt</span></td>
<td id="S4.T2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T2.2.2.3.1.3.1" class="ltx_text ltx_font_bold">TW prompt</span></td>
</tr>
<tr id="S4.T2.2.2.4.2" class="ltx_tr">
<td id="S4.T2.2.2.4.2.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.4.2.2.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="S4.T2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.4.2.3.1" class="ltx_text ltx_font_bold">rep-8</span></td>
<td id="S4.T2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.4.2.4.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="S4.T2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.4.2.5.1" class="ltx_text ltx_font_bold">rep-8</span></td>
</tr>
<tr id="S4.T2.2.2.5.3" class="ltx_tr">
<td id="S4.T2.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b</td>
<td id="S4.T2.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.843</td>
<td id="S4.T2.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.804</td>
<td id="S4.T2.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.796</td>
<td id="S4.T2.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t">0.763</td>
</tr>
<tr id="S4.T2.2.2.6.4" class="ltx_tr">
<td id="S4.T2.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="S4.T2.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.080</td>
<td id="S4.T2.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.024</td>
<td id="S4.T2.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.103</td>
<td id="S4.T2.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t">0.039</td>
</tr>
<tr id="S4.T2.2.2.7.5" class="ltx_tr">
<td id="S4.T2.2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="S4.T2.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.137</td>
<td id="S4.T2.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.068</td>
<td id="S4.T2.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.552</td>
<td id="S4.T2.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_tt">0.491</td>
</tr>
<tr id="S4.T2.2.2.8.6" class="ltx_tr">
<td id="S4.T2.2.2.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.2.2.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="S4.T2.2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.135</td>
<td id="S4.T2.2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.068</td>
<td id="S4.T2.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.599</td>
<td id="S4.T2.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_tt">0.539</td>
</tr>
<tr id="S4.T2.2.2.9.7" class="ltx_tr">
<td id="S4.T2.2.2.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.9.7.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="S4.T2.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.131</td>
<td id="S4.T2.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.065</td>
<td id="S4.T2.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.524</td>
<td id="S4.T2.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_t">0.463</td>
</tr>
<tr id="S4.T2.2.2.10.8" class="ltx_tr">
<td id="S4.T2.2.2.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.2.2.10.8.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="S4.T2.2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.116</td>
<td id="S4.T2.2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.050</td>
<td id="S4.T2.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.401</td>
<td id="S4.T2.2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_tt">0.335</td>
</tr>
<tr id="S4.T2.2.2.11.9" class="ltx_tr">
<td id="S4.T2.2.2.11.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.11.9.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="S4.T2.2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.134</td>
<td id="S4.T2.2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.069</td>
<td id="S4.T2.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.441</td>
<td id="S4.T2.2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_t">0.380</td>
</tr>
<tr id="S4.T2.2.2.12.10" class="ltx_tr">
<td id="S4.T2.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.12.10.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="S4.T2.2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.125</td>
<td id="S4.T2.2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.060</td>
<td id="S4.T2.2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.443</td>
<td id="S4.T2.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_t">0.381</td>
</tr>
<tr id="S4.T2.2.2.13.11" class="ltx_tr">
<td id="S4.T2.2.2.13.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.13.11.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="S4.T2.2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.119</td>
<td id="S4.T2.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.053</td>
<td id="S4.T2.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.409</td>
<td id="S4.T2.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_t">0.341</td>
</tr>
<tr id="S4.T2.2.2.14.12" class="ltx_tr">
<td id="S4.T2.2.2.14.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="S4.T2.2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.094</td>
<td id="S4.T2.2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.033</td>
<td id="S4.T2.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.244</td>
<td id="S4.T2.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_tt">0.172</td>
</tr>
<tr id="S4.T2.2.2.15.13" class="ltx_tr">
<td id="S4.T2.2.2.15.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T2.2.2.15.13.1.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="S4.T2.2.2.15.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.169</td>
<td id="S4.T2.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.098</td>
<td id="S4.T2.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.621</td>
<td id="S4.T2.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_t">0.566</td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T2.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.084</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.026</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.109</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">0.043</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T2.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T2.2.2.2.1.1.1" class="ltx_sup"><span id="S4.T2.2.2.2.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.103</td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.039</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.143</td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.071</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span>두 언어의 프롬프트를 사용한 반복 실험의 결과. 전체 결과는 부록 <a class="ltx_ref" href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">C</span></a>에서 사용할 수 있습니다.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results and Analysis</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Output Format</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS1.p1.2">우리는 중국어 말뭉치에 대한 지속적인 사전 훈련이 모델의 언어 출력에 미치는 영향을 조사하는 것을 목표로 한다. <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>는 언어 식별 실험의 결과를 제시한다. 우리는 영어 프롬프트를 사용할 때 거의 모든 모델이 영어로 출력을 생성하는 경향이 있음을 관찰한다. 중국어 프롬프트를 제공했을 때, 우리는 라마-2-7b가 중국어로 출력되는 경향이 있는 반면, 라마-2-7b-채팅은 영어로 출력되는 경향이 있음을 관찰했다. 또한 중국 프롬프트를 사용하면 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.2">Freeze First 10 Layers</span> 모델은 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.3">Freeze Last 10 Layers</span> 모델보다 더 높은 비율의 중국어 텍스트 출력을 산출하는 경향이 있습니다. 냉동 모듈이 있는 모델은 출력이 중국어로 대략 <math alttext="60\%" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.1.m1.1"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mrow id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p1.1.m1.1.1.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml">60</mn><mo id="S4.SS3.SSS1.p1.1.m1.1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.SSS1.p1.1.m1.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">60\%</annotation></semantics></math>로 비교적 유사한 결과를 보여준다. 어댑터의 경우 학습률을 증가시키면 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.4">Lora</span> 모델이 더 많은 중국어 출력을 생성할 수 있는 반면, <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.1">(Ia)<sup class="ltx_sup" id="S4.SS3.SSS1.p1.2.1.1"><span class="ltx_text ltx_font_upright" id="S4.SS3.SSS1.p1.2.1.1.1">3</span></span> 모델은 영어 출력을 선호하는 경향이 있습니다.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1"><a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>는 반복 분석 실험의 결과를 보여준다. 우리는 주어진 중국어 또는 영어 프롬프트에 관계없이 Llama-2-7b가 Llama-2-7b-채팅에 비해 일관되게 중요한 반복 문제를 나타내는 것을 관찰했다. 또한, 전통적인 중국어 말뭉치에 대한 지속적인 사전 훈련 후 모델들은 영어 프롬프트에 비해 중국어 프롬프트에서 텍스트 반복이 눈에 띄게 증가했다. 또한 중국 프롬프트를 사용할 때 중국 출력을 생성하는 경향이 더 높은 모델은 반복 문제가 있을 가능성이 더 높다는 것을 발견했다.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Knowledge</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">표 <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.3.2 Knowledge ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>는 우리의 지식 분석 실험 결과를 보여준다. Llama-2-7b-chat은 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.1">Hellaswag</span> 및 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.2">MMLU</span>에서 Llama-2-7b와 유사하게 수행되지만 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.3">ARC</span> 및 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.4">C-eval-tw</span>에서는 약간 더 나은 성능을 보여줍니다. <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.5">ARC</span> 및 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.6">Hellaswag</span> 벤치마크에서 거의 모든 연속 사전 훈련 모델은 Llama-2-7b-chat보다 우수합니다. <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.7">MMLU</span> 벤치마크에서 대부분의 연속 사전 훈련 모델은 Llama-2-7b-chat를 능가하는 경향이 있습니다. 그러나 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.8">C-eval-tw</span> 벤치마크의 경우 Llama-2-7b-chat에 대한 지속적인 사전 훈련을 위한 간단한 방법을 활용하는 모델의 효능을 비교할 때 명확한 패턴이 없다. 위에서 언급한 관찰된 차이는 미묘하다는 점에 주목할 필요가 있다.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:201.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.8pt,34.2pt) scale(0.746404351114294,0.746404351114294) ;">
<table id="S4.T3.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.3.1" class="ltx_tr">
<td id="S4.T3.2.2.3.1.1" class="ltx_td ltx_border_r ltx_border_t" rowspan="2"></td>
<td id="S4.T3.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.3.1.2.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ARC</span></td>
<td id="S4.T3.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.3.1.3.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Hellaswag</span></td>
<td id="S4.T3.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.3.1.4.1" class="ltx_text ltx_font_typewriter ltx_font_bold">MMLU</span></td>
<td id="S4.T3.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.3.1.5.1" class="ltx_text ltx_font_typewriter ltx_font_bold">C-eval-tw</span></td>
</tr>
<tr id="S4.T3.2.2.4.2" class="ltx_tr">
<td id="S4.T3.2.2.4.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.4.2.1.1" class="ltx_text ltx_font_bold">ACC</span></td>
<td id="S4.T3.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.4.2.2.1" class="ltx_text ltx_font_bold">ACC</span></td>
<td id="S4.T3.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.4.2.3.1" class="ltx_text ltx_font_bold">ACC</span></td>
<td id="S4.T3.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.4.2.4.1" class="ltx_text ltx_font_bold">ACC</span></td>
</tr>
<tr id="S4.T3.2.2.5.3" class="ltx_tr">
<td id="S4.T3.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b</td>
<td id="S4.T3.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">53.0</td>
<td id="S4.T3.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">78.6</td>
<td id="S4.T3.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">46.5</td>
<td id="S4.T3.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_tt">32.2</td>
</tr>
<tr id="S4.T3.2.2.6.4" class="ltx_tr">
<td id="S4.T3.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="S4.T3.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.6</td>
<td id="S4.T3.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.6</td>
<td id="S4.T3.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.6</td>
<td id="S4.T3.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t">32.9</td>
</tr>
<tr id="S4.T3.2.2.7.5" class="ltx_tr">
<td id="S4.T3.2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="S4.T3.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">52.0</td>
<td id="S4.T3.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.6</td>
<td id="S4.T3.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">49.1</td>
<td id="S4.T3.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_tt">33.4</td>
</tr>
<tr id="S4.T3.2.2.8.6" class="ltx_tr">
<td id="S4.T3.2.2.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.2.2.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="S4.T3.2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">51.0</td>
<td id="S4.T3.2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.7</td>
<td id="S4.T3.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">49.1</td>
<td id="S4.T3.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_tt">31.9</td>
</tr>
<tr id="S4.T3.2.2.9.7" class="ltx_tr">
<td id="S4.T3.2.2.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.9.7.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="S4.T3.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">51.5</td>
<td id="S4.T3.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.6</td>
<td id="S4.T3.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.4</td>
<td id="S4.T3.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_t">33.5</td>
</tr>
<tr id="S4.T3.2.2.10.8" class="ltx_tr">
<td id="S4.T3.2.2.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.2.2.10.8.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="S4.T3.2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">51.9</td>
<td id="S4.T3.2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.7</td>
<td id="S4.T3.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">48.9</td>
<td id="S4.T3.2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_tt">32.2</td>
</tr>
<tr id="S4.T3.2.2.11.9" class="ltx_tr">
<td id="S4.T3.2.2.11.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.11.9.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="S4.T3.2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.8</td>
<td id="S4.T3.2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.0</td>
<td id="S4.T3.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.4</td>
<td id="S4.T3.2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_t">33.3</td>
</tr>
<tr id="S4.T3.2.2.12.10" class="ltx_tr">
<td id="S4.T3.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.12.10.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="S4.T3.2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.2</td>
<td id="S4.T3.2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.8</td>
<td id="S4.T3.2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.4</td>
<td id="S4.T3.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_t">32.6</td>
</tr>
<tr id="S4.T3.2.2.13.11" class="ltx_tr">
<td id="S4.T3.2.2.13.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.13.11.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="S4.T3.2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.0</td>
<td id="S4.T3.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.9</td>
<td id="S4.T3.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.9</td>
<td id="S4.T3.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_t">33.4</td>
</tr>
<tr id="S4.T3.2.2.14.12" class="ltx_tr">
<td id="S4.T3.2.2.14.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="S4.T3.2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">53.5</td>
<td id="S4.T3.2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">78.6</td>
<td id="S4.T3.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">47.1</td>
<td id="S4.T3.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_tt">33.8</td>
</tr>
<tr id="S4.T3.2.2.15.13" class="ltx_tr">
<td id="S4.T3.2.2.15.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T3.2.2.15.13.1.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="S4.T3.2.2.15.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.8</td>
<td id="S4.T3.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.2</td>
<td id="S4.T3.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.4</td>
<td id="S4.T3.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_t">33.0</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T3.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.7</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.9</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.0</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">32.6</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T3.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T3.2.2.2.1.1.1" class="ltx_sup"><span id="S4.T3.2.2.2.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">53.8</td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">77.3</td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">46.2</td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">31.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span>Knowledge analysis experiment results with four benchmarks.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Reliability</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1"><a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ 4.3.3 Reliability ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> 표에서는 신뢰성 실험 결과를 제시한다. 라마-2-7b-채팅은 진실성 및 독성 벤치마크에서 라마-2-7b를 일관되게 능가한다. 특히, 지속적인 사전 훈련 후 모델들은 두 벤치마크에서 Llama-2-7b-chat에 비해 열등한 성능을 보여준다. 이러한 경향은 특히 영어에 대한 진실성 분석 벤치마크와 번체 중국어에 대한 독성 벤치마크에서 두드러진다. 또한, 중국 생산량 생성을 선호하는 모델이 독성 벤치마크에서 열등한 성능을 나타내는 것을 관찰했다. 편향 벤치마크와 관련하여, 우리는 Llama-2-7b-chat이 Llama-2-7b보다 긍정적인 텍스트를 더 많이 출력하는 것을 관찰할 수 있다. 지속적인 사전 훈련 후 모델의 출력은 라마-2-7b-채팅보다 부정적인 감정 점수가 상대적으로 더 많다.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.28" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:200.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.4pt,44.0pt) scale(0.694692668749019,0.694692668749019) ;">
<table id="S4.T4.28.28" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.28.28.29.1" class="ltx_tr">
<td id="S4.T4.28.28.29.1.1" class="ltx_td ltx_border_r ltx_border_t" rowspan="3"></td>
<td id="S4.T4.28.28.29.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T4.28.28.29.1.2.1" class="ltx_text ltx_font_typewriter ltx_font_bold">TruthfulQA</span></td>
<td id="S4.T4.28.28.29.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T4.28.28.29.1.3.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ToxiGen</span></td>
<td id="S4.T4.28.28.29.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T4.28.28.29.1.4.1" class="ltx_text ltx_font_typewriter ltx_font_bold">BOLD</span></td>
</tr>
<tr id="S4.T4.28.28.30.2" class="ltx_tr">
<td id="S4.T4.28.28.30.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T4.28.28.30.2.1.1" class="ltx_text ltx_font_bold">mc2 ↑</span></td>
<td id="S4.T4.28.28.30.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T4.28.28.30.2.2.1" class="ltx_text ltx_font_bold">toxicity ↓</span></td>
<td id="S4.T4.28.28.30.2.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T4.28.28.30.2.3.1" class="ltx_text ltx_font_bold">sentiment</span></td>
</tr>
<tr id="S4.T4.28.28.31.3" class="ltx_tr">
<td id="S4.T4.28.28.31.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.1.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T4.28.28.31.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.2.1" class="ltx_text ltx_font_bold">TW</span></td>
<td id="S4.T4.28.28.31.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.3.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T4.28.28.31.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.4.1" class="ltx_text ltx_font_bold">TW</span></td>
<td id="S4.T4.28.28.31.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.5.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T4.28.28.31.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.28.28.31.3.6.1" class="ltx_text ltx_font_bold">TW</span></td>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<td id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b</td>
<td id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.0</td>
<td id="S4.T4.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.9</td>
<td id="S4.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.30</td>
<td id="S4.T4.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.80</td>
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.41<math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\pm</annotation></semantics></math>0.17</td>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">0.23<math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\pm</annotation></semantics></math>0.13</td>
</tr>
<tr id="S4.T4.4.4.4" class="ltx_tr">
<td id="S4.T4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="S4.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.6</td>
<td id="S4.T4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.7</td>
<td id="S4.T4.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.03</td>
<td id="S4.T4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.22</td>
<td id="S4.T4.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.66<math id="S4.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.3.3.3.1.m1.1a"><mo id="S4.T4.3.3.3.1.m1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">\pm</annotation></semantics></math>0.24</td>
<td id="S4.T4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t">0.69<math id="S4.T4.4.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.4.4.4.2.m1.1a"><mo id="S4.T4.4.4.4.2.m1.1.1" xref="S4.T4.4.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S4.T4.4.4.4.2.m1.1.1.cmml" xref="S4.T4.4.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.2.m1.1c">\pm</annotation></semantics></math>0.19</td>
</tr>
<tr id="S4.T4.6.6.6" class="ltx_tr">
<td id="S4.T4.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="S4.T4.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">40.2</td>
<td id="S4.T4.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">48.5</td>
<td id="S4.T4.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.05</td>
<td id="S4.T4.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5.74</td>
<td id="S4.T4.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.52<math id="S4.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.5.5.5.1.m1.1a"><mo id="S4.T4.5.5.5.1.m1.1.1" xref="S4.T4.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T4.5.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.1.m1.1c">\pm</annotation></semantics></math>0.20</td>
<td id="S4.T4.6.6.6.2" class="ltx_td ltx_align_center ltx_border_tt">0.34<math id="S4.T4.6.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.6.6.6.2.m1.1a"><mo id="S4.T4.6.6.6.2.m1.1.1" xref="S4.T4.6.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.2.m1.1b"><csymbol cd="latexml" id="S4.T4.6.6.6.2.m1.1.1.cmml" xref="S4.T4.6.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.2.m1.1c">\pm</annotation></semantics></math>0.14</td>
</tr>
<tr id="S4.T4.8.8.8" class="ltx_tr">
<td id="S4.T4.8.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T4.8.8.8.3.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="S4.T4.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">41.7</td>
<td id="S4.T4.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">48.5</td>
<td id="S4.T4.8.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.08</td>
<td id="S4.T4.8.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">7.12</td>
<td id="S4.T4.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.55<math id="S4.T4.7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.7.7.7.1.m1.1a"><mo id="S4.T4.7.7.7.1.m1.1.1" xref="S4.T4.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T4.7.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.1.m1.1c">\pm</annotation></semantics></math>0.22</td>
<td id="S4.T4.8.8.8.2" class="ltx_td ltx_align_center ltx_border_tt">0.34<math id="S4.T4.8.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.8.8.8.2.m1.1a"><mo id="S4.T4.8.8.8.2.m1.1.1" xref="S4.T4.8.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S4.T4.8.8.8.2.m1.1.1.cmml" xref="S4.T4.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.2.m1.1c">\pm</annotation></semantics></math>0.12</td>
</tr>
<tr id="S4.T4.10.10.10" class="ltx_tr">
<td id="S4.T4.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.10.10.10.3.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="S4.T4.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.4</td>
<td id="S4.T4.10.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.8</td>
<td id="S4.T4.10.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.01</td>
<td id="S4.T4.10.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.69</td>
<td id="S4.T4.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.58<math id="S4.T4.9.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.9.9.9.1.m1.1a"><mo id="S4.T4.9.9.9.1.m1.1.1" xref="S4.T4.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T4.9.9.9.1.m1.1.1.cmml" xref="S4.T4.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.1.m1.1c">\pm</annotation></semantics></math>0.21</td>
<td id="S4.T4.10.10.10.2" class="ltx_td ltx_align_center ltx_border_t">0.37<math id="S4.T4.10.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.10.10.10.2.m1.1a"><mo id="S4.T4.10.10.10.2.m1.1.1" xref="S4.T4.10.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.2.m1.1b"><csymbol cd="latexml" id="S4.T4.10.10.10.2.m1.1.1.cmml" xref="S4.T4.10.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.2.m1.1c">\pm</annotation></semantics></math>0.15</td>
</tr>
<tr id="S4.T4.12.12.12" class="ltx_tr">
<td id="S4.T4.12.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.12.12.12.3.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="S4.T4.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.6</td>
<td id="S4.T4.12.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.8</td>
<td id="S4.T4.12.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.04</td>
<td id="S4.T4.12.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.15</td>
<td id="S4.T4.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.57<math id="S4.T4.11.11.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.11.11.11.1.m1.1a"><mo id="S4.T4.11.11.11.1.m1.1.1" xref="S4.T4.11.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.11.1.m1.1b"><csymbol cd="latexml" id="S4.T4.11.11.11.1.m1.1.1.cmml" xref="S4.T4.11.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.11.1.m1.1c">\pm</annotation></semantics></math>0.21</td>
<td id="S4.T4.12.12.12.2" class="ltx_td ltx_align_center ltx_border_t">0.42<math id="S4.T4.12.12.12.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.12.12.12.2.m1.1a"><mo id="S4.T4.12.12.12.2.m1.1.1" xref="S4.T4.12.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.2.m1.1b"><csymbol cd="latexml" id="S4.T4.12.12.12.2.m1.1.1.cmml" xref="S4.T4.12.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.2.m1.1c">\pm</annotation></semantics></math>0.16</td>
</tr>
<tr id="S4.T4.14.14.14" class="ltx_tr">
<td id="S4.T4.14.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.14.14.14.3.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="S4.T4.14.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.8</td>
<td id="S4.T4.14.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.6</td>
<td id="S4.T4.14.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.04</td>
<td id="S4.T4.14.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.27</td>
<td id="S4.T4.13.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.59<math id="S4.T4.13.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.13.13.13.1.m1.1a"><mo id="S4.T4.13.13.13.1.m1.1.1" xref="S4.T4.13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.1.m1.1b"><csymbol cd="latexml" id="S4.T4.13.13.13.1.m1.1.1.cmml" xref="S4.T4.13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.1.m1.1c">\pm</annotation></semantics></math>0.24</td>
<td id="S4.T4.14.14.14.2" class="ltx_td ltx_align_center ltx_border_t">0.43<math id="S4.T4.14.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.14.14.14.2.m1.1a"><mo id="S4.T4.14.14.14.2.m1.1.1" xref="S4.T4.14.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.14.2.m1.1b"><csymbol cd="latexml" id="S4.T4.14.14.14.2.m1.1.1.cmml" xref="S4.T4.14.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.14.2.m1.1c">\pm</annotation></semantics></math>0.15</td>
</tr>
<tr id="S4.T4.16.16.16" class="ltx_tr">
<td id="S4.T4.16.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.16.16.16.3.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="S4.T4.16.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.9</td>
<td id="S4.T4.16.16.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.8</td>
<td id="S4.T4.16.16.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0</td>
<td id="S4.T4.16.16.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.31</td>
<td id="S4.T4.15.15.15.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.60<math id="S4.T4.15.15.15.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.15.15.15.1.m1.1a"><mo id="S4.T4.15.15.15.1.m1.1.1" xref="S4.T4.15.15.15.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.15.1.m1.1b"><csymbol cd="latexml" id="S4.T4.15.15.15.1.m1.1.1.cmml" xref="S4.T4.15.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.15.1.m1.1c">\pm</annotation></semantics></math>0.22</td>
<td id="S4.T4.16.16.16.2" class="ltx_td ltx_align_center ltx_border_t">0.42<math id="S4.T4.16.16.16.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.16.16.16.2.m1.1a"><mo id="S4.T4.16.16.16.2.m1.1.1" xref="S4.T4.16.16.16.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.16.16.16.2.m1.1b"><csymbol cd="latexml" id="S4.T4.16.16.16.2.m1.1.1.cmml" xref="S4.T4.16.16.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.16.16.2.m1.1c">\pm</annotation></semantics></math>0.14</td>
</tr>
<tr id="S4.T4.18.18.18" class="ltx_tr">
<td id="S4.T4.18.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.18.18.18.3.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="S4.T4.18.18.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.3</td>
<td id="S4.T4.18.18.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.8</td>
<td id="S4.T4.18.18.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.04</td>
<td id="S4.T4.18.18.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.39</td>
<td id="S4.T4.17.17.17.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.58<math id="S4.T4.17.17.17.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.17.17.17.1.m1.1a"><mo id="S4.T4.17.17.17.1.m1.1.1" xref="S4.T4.17.17.17.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.17.17.17.1.m1.1b"><csymbol cd="latexml" id="S4.T4.17.17.17.1.m1.1.1.cmml" xref="S4.T4.17.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.17.17.17.1.m1.1c">\pm</annotation></semantics></math>0.21</td>
<td id="S4.T4.18.18.18.2" class="ltx_td ltx_align_center ltx_border_t">0.43<math id="S4.T4.18.18.18.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.18.18.18.2.m1.1a"><mo id="S4.T4.18.18.18.2.m1.1.1" xref="S4.T4.18.18.18.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.18.18.18.2.m1.1b"><csymbol cd="latexml" id="S4.T4.18.18.18.2.m1.1.1.cmml" xref="S4.T4.18.18.18.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.18.18.18.2.m1.1c">\pm</annotation></semantics></math>0.16</td>
</tr>
<tr id="S4.T4.20.20.20" class="ltx_tr">
<td id="S4.T4.20.20.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T4.20.20.20.3.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="S4.T4.20.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">43.6</td>
<td id="S4.T4.20.20.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">49.1</td>
<td id="S4.T4.20.20.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.03</td>
<td id="S4.T4.20.20.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.79</td>
<td id="S4.T4.19.19.19.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.64<math id="S4.T4.19.19.19.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.19.19.19.1.m1.1a"><mo id="S4.T4.19.19.19.1.m1.1.1" xref="S4.T4.19.19.19.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.19.19.19.1.m1.1b"><csymbol cd="latexml" id="S4.T4.19.19.19.1.m1.1.1.cmml" xref="S4.T4.19.19.19.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.19.19.19.1.m1.1c">\pm</annotation></semantics></math>0.22</td>
<td id="S4.T4.20.20.20.2" class="ltx_td ltx_align_center ltx_border_tt">0.63<math id="S4.T4.20.20.20.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.20.20.20.2.m1.1a"><mo id="S4.T4.20.20.20.2.m1.1.1" xref="S4.T4.20.20.20.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.20.20.20.2.m1.1b"><csymbol cd="latexml" id="S4.T4.20.20.20.2.m1.1.1.cmml" xref="S4.T4.20.20.20.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.20.20.20.2.m1.1c">\pm</annotation></semantics></math>0.17</td>
</tr>
<tr id="S4.T4.22.22.22" class="ltx_tr">
<td id="S4.T4.22.22.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T4.22.22.22.3.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="S4.T4.22.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">42.5</td>
<td id="S4.T4.22.22.22.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.9</td>
<td id="S4.T4.22.22.22.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.07</td>
<td id="S4.T4.22.22.22.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.97</td>
<td id="S4.T4.21.21.21.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.57<math id="S4.T4.21.21.21.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.21.21.21.1.m1.1a"><mo id="S4.T4.21.21.21.1.m1.1.1" xref="S4.T4.21.21.21.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.21.21.21.1.m1.1b"><csymbol cd="latexml" id="S4.T4.21.21.21.1.m1.1.1.cmml" xref="S4.T4.21.21.21.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.21.21.21.1.m1.1c">\pm</annotation></semantics></math>0.22</td>
<td id="S4.T4.22.22.22.2" class="ltx_td ltx_align_center ltx_border_t">0.35<math id="S4.T4.22.22.22.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.22.22.22.2.m1.1a"><mo id="S4.T4.22.22.22.2.m1.1.1" xref="S4.T4.22.22.22.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.22.22.22.2.m1.1b"><csymbol cd="latexml" id="S4.T4.22.22.22.2.m1.1.1.cmml" xref="S4.T4.22.22.22.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.22.22.22.2.m1.1c">\pm</annotation></semantics></math>0.10</td>
</tr>
<tr id="S4.T4.25.25.25" class="ltx_tr">
<td id="S4.T4.23.23.23.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.23.23.23.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T4.23.23.23.1.1.1" class="ltx_sup"><span id="S4.T4.23.23.23.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="S4.T4.25.25.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.2</td>
<td id="S4.T4.25.25.25.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.8</td>
<td id="S4.T4.25.25.25.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0</td>
<td id="S4.T4.25.25.25.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.17</td>
<td id="S4.T4.24.24.24.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.66<math id="S4.T4.24.24.24.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.24.24.24.2.m1.1a"><mo id="S4.T4.24.24.24.2.m1.1.1" xref="S4.T4.24.24.24.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.24.24.24.2.m1.1b"><csymbol cd="latexml" id="S4.T4.24.24.24.2.m1.1.1.cmml" xref="S4.T4.24.24.24.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.24.24.24.2.m1.1c">\pm</annotation></semantics></math>0.24</td>
<td id="S4.T4.25.25.25.3" class="ltx_td ltx_align_center ltx_border_t">0.69<math id="S4.T4.25.25.25.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.25.25.25.3.m1.1a"><mo id="S4.T4.25.25.25.3.m1.1.1" xref="S4.T4.25.25.25.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.25.25.25.3.m1.1b"><csymbol cd="latexml" id="S4.T4.25.25.25.3.m1.1.1.cmml" xref="S4.T4.25.25.25.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.25.25.25.3.m1.1c">\pm</annotation></semantics></math>0.19</td>
</tr>
<tr id="S4.T4.28.28.28" class="ltx_tr">
<td id="S4.T4.26.26.26.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T4.26.26.26.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T4.26.26.26.1.1.1" class="ltx_sup"><span id="S4.T4.26.26.26.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="S4.T4.28.28.28.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">43.0</td>
<td id="S4.T4.28.28.28.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">49.9</td>
<td id="S4.T4.28.28.28.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.0</td>
<td id="S4.T4.28.28.28.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.11</td>
<td id="S4.T4.27.27.27.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.66<math id="S4.T4.27.27.27.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.27.27.27.2.m1.1a"><mo id="S4.T4.27.27.27.2.m1.1.1" xref="S4.T4.27.27.27.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.27.27.27.2.m1.1b"><csymbol cd="latexml" id="S4.T4.27.27.27.2.m1.1.1.cmml" xref="S4.T4.27.27.27.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.27.27.27.2.m1.1c">\pm</annotation></semantics></math>0.23</td>
<td id="S4.T4.28.28.28.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.68<math id="S4.T4.28.28.28.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.28.28.28.3.m1.1a"><mo id="S4.T4.28.28.28.3.m1.1.1" xref="S4.T4.28.28.28.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.28.28.28.3.m1.1b"><csymbol cd="latexml" id="S4.T4.28.28.28.3.m1.1.1.cmml" xref="S4.T4.28.28.28.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.28.28.28.3.m1.1c">\pm</annotation></semantics></math>0.18</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4:</span> 진실성, 편향성, 독성 측면을 포함한 세 가지 벤치마크에 대한 신뢰성 분석 실험 결과. <span class="ltx_text ltx_font_bold" id="S4.T4.31.1">EN</span>은 영어로 된 원본 데이터 세트를 나타내는 반면, <span class="ltx_text ltx_font_bold" id="S4.T4.32.2">TW</span>은 번체 중국어로 된 번역 데이터 세트를 나타냅니다.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">이 작업은 지속적인 사전 훈련 중 치명적인 망각은 사소하지 않은 도전이며 간단한 방법을 통해 해결될 수 없음을 보여준다. 또한, 반복 문제는 지속적인 사전 훈련 후 모델이 중국 전통 산출물을 생산하는 경향이 있을 때 더 두드러진다는 것을 발견했다. 또한, 지속적인 사전 훈련 후에도 모델의 지식은 주로 영향을 받지 않지만 신뢰성은 감소한다.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p" id="Sx1.p1.1">한 가지 주목할 만한 한계는 연속 사전 훈련 LLM의 자원 집약적 특성으로 인해 이 작업에 설명된 모든 간단한 연속 사전 훈련 방법을 재현하기가 어렵다. 또 다른 중요한 한계는 중국 전통 말뭉치를 사용하여 지속적인 사전 훈련만 수행했다는 것이다. 그러나 우리는 또한 다른 언어의 리소스에 대한 사전 교육을 포함하도록 조사를 확장하는 데 관심이 있으며 우리의 방법론은 이러한 설정에 쉽게 적응할 수 있다.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>

<div id="Sx2.p1" class="ltx_para">
<p class="ltx_p" id="Sx2.p1.1">LLM의 지속적인 사전 훈련은 모델의 안전 정렬을 손상시켜 편향되고 독성 정보를 포함할 수 있는 텍스트를 생성할 수 있다. 안전 정렬의 손상을 완화하기 위한 탐색 방법은 향후 연구를 위한 전향적인 방법이 될 수 있다.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx3.p1" class="ltx_para">
<p class="ltx_p" id="Sx3.p1.1">우리는 귀중한 자원을 아낌없이 제공한 ASUS 오픈 클라우드 인프라 소프트웨어 센터에 감사를 표합니다. 스티브 청청천, 양성영, 젠하오 청, 샤오성 헝, 쯔셴 리, 다우청 류가 통찰력 있는 토론에 참여해 준 데 대해 특별한 감사를 표한다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05457</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yiming Cui, Ziqing Yang, and Xin Yao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2304.08177" title="" class="ltx_ref ltx_href">Efficient and effective text encoding for chinese llama and alpaca</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08177</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhamala et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021.

</span>
<span class="ltx_bibblock">Bold: Dataset and metrics for measuring biases in open-ended language generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>, pages 862–872.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh (2019)</span>
<span class="ltx_bibblock">
Kawin Ethayarajh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1006" title="" class="ltx_ref ltx_href">How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 55–65, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">French (1999)</span>
<span class="ltx_bibblock">
Robert&nbsp;M French. 1999.

</span>
<span class="ltx_bibblock">Catastrophic forgetting in connectionist networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Trends in cognitive sciences</em>, 3(4):128–135.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5281/zenodo.5371628" title="" class="ltx_ref ltx_href">A framework for few-shot language model evaluation</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats&nbsp;Leon Richter, Quentin&nbsp;Gregory Anthony, Eugene Belilovsky, Irina Rish, and Timothée Lesort. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=pg7PUJe0Tl" title="" class="ltx_ref ltx_href">Continual pre-training of large language models: How to re-warm your model?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Workshop on Efficient Systems for Foundation Models @ ICML2023</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartvigsen et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.234" title="" class="ltx_ref ltx_href">ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3309–3326, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.03300</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseini et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Saghar Hosseini, Hamid Palangi, and Ahmed&nbsp;Hassan Awadallah. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.trustnlp-1.11" title="" class="ltx_ref ltx_href">An empirical study of metrics to measure representational harms in pre-trained language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)</em>, pages 121–134, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De&nbsp;Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 2790–2799. PMLR.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Edward&nbsp;J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu&nbsp;Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_href">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.08322</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutto and Gilbert (2014)</span>
<span class="ltx_bibblock">
Clayton Hutto and Eric Gilbert. 2014.

</span>
<span class="ltx_bibblock">Vader: A parsimonious rule-based model for sentiment analysis of social media text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the international AAAI conference on web and social media</em>, volume&nbsp;8, pages 216–225.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016a)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016a.

</span>
<span class="ltx_bibblock">Fasttext.zip: Compressing text classification models.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1612.03651</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016b)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016b.

</span>
<span class="ltx_bibblock">Bag of tricks for efficient text classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1607.01759</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2022.

</span>
<span class="ltx_bibblock">Continual pre-training of language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.229" title="" class="ltx_ref ltx_href">TruthfulQA: Measuring how models mimic human falsehoods</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Chen (2023)</span>
<span class="ltx_bibblock">
Yen-Ting Lin and Yun-Nung Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.17487" title="" class="ltx_ref ltx_href">Taiwan llm: Bridging the linguistic divide with a culturally aligned language model</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin&nbsp;A Raffel. 2022.

</span>
<span class="ltx_bibblock">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:1950–1965.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xiangyu Qi, Yi&nbsp;Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023.

</span>
<span class="ltx_bibblock">Fine-tuning aligned language models compromises safety, even when users do not intend to!

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03693</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Ye&nbsp;Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N18-2084" title="" class="ltx_ref ltx_href">When and why are pre-trained word embeddings useful for neural machine translation?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, pages 529–535, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-acl.220" title="" class="ltx_ref ltx_href">ELLE: Efficient lifelong pre-training for emerging data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 2789–2810, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasley et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.

</span>
<span class="ltx_bibblock">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, pages 3505–3506.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roziere et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing&nbsp;Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.12950</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Chenyang Song, Xu&nbsp;Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun, and Tao Yang. 2023.

</span>
<span class="ltx_bibblock">Conpet: Continual parameter-efficient tuning for large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.14763</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in opus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12)</em>, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van&nbsp;Aken et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Betty Van&nbsp;Aken, Benjamin Winter, Alexander Löser, and Felix&nbsp;A Gers. 2019.

</span>
<span class="ltx_bibblock">How does bert answer questions? a layer-wise analysis of transformer representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM international conference on information and knowledge management</em>, pages 1823–1832.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc&nbsp;V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et&nbsp;al. 2016.

</span>
<span class="ltx_bibblock">Google’s neural machine translation system: Bridging the gap between human and machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.08144</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 2023.

</span>
<span class="ltx_bibblock">Efficient continual pre-training for building domain specific large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.08545</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1472" title="" class="ltx_ref ltx_href">HellaSwag: Can a machine really finish your sentence?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4791–4800, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu&nbsp;Cai, Qing Qu, Yong&nbsp;Jae Lee, and Yi&nbsp;Ma. 2023.

</span>
<span class="ltx_bibblock">Investigating the catastrophic forgetting in multimodal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10313</em>.

</span>
</li>
</ul>
</section>
<figure id="A0.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.03129/assets/x2.png" id="A0.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="820" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2: 모델 출력의</span>Illustration.</figcaption>
</figure>
<figure id="A0.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.03129/assets/x3.png" id="A0.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="492" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span>Illustration of models’ outputs. 우리 프롬프트의 번역은 "기후 변화가 생태계에 어떤 영향을 미치는가?"이다.</figcaption>
</figure>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompting Results</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p" id="A1.p1.1">효율 최적화를 위해 vLLM <cite class="ltx_cite ltx_citemacro_cite">Kwon et al. (<a class="ltx_ref" href="#bib.bib19" title="">2023</a>)</cite>를 사용하여 max_tokens 설정 256으로 모델을 구성합니다. 핵 샘플링을 사용하여 온도를 0.1로 설정하고 top_p를 0.9로 설정합니다. 그림 <a class="ltx_ref" href="#A0.F2" title="Figure 2 ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> 및 그림 <a class="ltx_ref" href="#A0.F2" title="Figure 2 ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>를 관찰하면 중국 프롬프트를 사용할 때 Llama-2-7b-chat-cp가 Llama-2-7b-chat보다 더 많은 반복 문제를 나타내는 것이 분명합니다.</p>
</div>
<figure id="A1.T5" class="ltx_table">
<div id="A1.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:143pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.6pt,18.5pt) scale(0.794559754759643,0.794559754759643) ;">
<table id="A1.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T5.1.1.2.1" class="ltx_tr">
<th id="A1.T5.1.1.2.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="A1.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="A1.T5.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Trainable params</span></th>
<th id="A1.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="A1.T5.1.1.2.1.3.1" class="ltx_text ltx_font_bold">All params</span></th>
<th id="A1.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T5.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Trainable %</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T5.1.1.3.1" class="ltx_tr">
<td id="A1.T5.1.1.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat-cp</td>
<td id="A1.T5.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">100.000</td>
</tr>
<tr id="A1.T5.1.1.4.2" class="ltx_tr">
<td id="A1.T5.1.1.4.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.4.2.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="A1.T5.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,714,582,016</td>
<td id="A1.T5.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_t">69.966</td>
</tr>
<tr id="A1.T5.1.1.5.3" class="ltx_tr">
<td id="A1.T5.1.1.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.5.3.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="A1.T5.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,714,582,016</td>
<td id="A1.T5.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">69.966</td>
</tr>
<tr id="A1.T5.1.1.6.4" class="ltx_tr">
<td id="A1.T5.1.1.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.6.4.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="A1.T5.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,590,931,968</td>
<td id="A1.T5.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">68.131</td>
</tr>
<tr id="A1.T5.1.1.7.5" class="ltx_tr">
<td id="A1.T5.1.1.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.7.5.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="A1.T5.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,147,483,648</td>
<td id="A1.T5.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">31.869</td>
</tr>
<tr id="A1.T5.1.1.8.6" class="ltx_tr">
<td id="A1.T5.1.1.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="A1.T5.1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,409,893,888</td>
<td id="A1.T5.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_t">35.764</td>
</tr>
<tr id="A1.T5.1.1.9.7" class="ltx_tr">
<td id="A1.T5.1.1.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.9.7.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="A1.T5.1.1.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,328,521,728</td>
<td id="A1.T5.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_t">64.236</td>
</tr>
<tr id="A1.T5.1.1.10.8" class="ltx_tr">
<td id="A1.T5.1.1.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.10.8.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="A1.T5.1.1.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,194,304</td>
<td id="A1.T5.1.1.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,742,609,920</td>
<td id="A1.T5.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_t">0.062</td>
</tr>
<tr id="A1.T5.1.1.1" class="ltx_tr">
<td id="A1.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="A1.T5.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="A1.T5.1.1.1.1.1.1" class="ltx_sup"><span id="A1.T5.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="A1.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">614,400</td>
<td id="A1.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6,739,030,016</td>
<td id="A1.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.009</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span>다양한 간단한 접근법에 대한 훈련 가능한 파라미터.</figcaption>
</figure>
<figure id="A1.T6" class="ltx_table">
<div id="A1.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:173.4pt;height:32.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.3pt,1.9pt) scale(0.894174964941106,0.894174964941106) ;">
<table id="A1.T6.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T6.1.1.1.1" class="ltx_tr">
<td id="A1.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="A1.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.2.1" class="ltx_text ltx_font_bold">rep-8</span></td>
<td id="A1.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.3.1" class="ltx_text ltx_font_bold">rep-12</span></td>
<td id="A1.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.4.1" class="ltx_text ltx_font_bold">rep-16</span></td>
<td id="A1.T6.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.5.1" class="ltx_text ltx_font_bold">rep-20</span></td>
</tr>
<tr id="A1.T6.1.1.2.2" class="ltx_tr">
<td id="A1.T6.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">0.141</td>
<td id="A1.T6.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.056</td>
<td id="A1.T6.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.037</td>
<td id="A1.T6.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.030</td>
<td id="A1.T6.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.025</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 6:</span>중국 전통 말뭉치의 중복된 n-gram 토큰의 비율입니다.</figcaption>
</figure>
<figure id="A1.T7" class="ltx_table">
<div id="A1.T7.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:210.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-53.1pt,30.2pt) scale(0.776273750636431,0.776273750636431) ;">
<table id="A1.T7.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T7.2.2.3.1" class="ltx_tr">
<td id="A1.T7.2.2.3.1.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="A1.T7.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5"><span id="A1.T7.2.2.3.1.2.1" class="ltx_text ltx_font_bold">EN prompt</span></td>
<td id="A1.T7.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="5"><span id="A1.T7.2.2.3.1.3.1" class="ltx_text ltx_font_bold">TW prompt</span></td>
</tr>
<tr id="A1.T7.2.2.4.2" class="ltx_tr">
<td id="A1.T7.2.2.4.2.1" class="ltx_td ltx_border_r"></td>
<td id="A1.T7.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.2.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="A1.T7.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.3.1" class="ltx_text ltx_font_bold">rep-8</span></td>
<td id="A1.T7.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.4.1" class="ltx_text ltx_font_bold">rep-12</span></td>
<td id="A1.T7.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.5.1" class="ltx_text ltx_font_bold">rep-16</span></td>
<td id="A1.T7.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.6.1" class="ltx_text ltx_font_bold">rep-20</span></td>
<td id="A1.T7.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.7.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="A1.T7.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.8.1" class="ltx_text ltx_font_bold">rep-8</span></td>
<td id="A1.T7.2.2.4.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.9.1" class="ltx_text ltx_font_bold">rep-12</span></td>
<td id="A1.T7.2.2.4.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.10.1" class="ltx_text ltx_font_bold">rep-16</span></td>
<td id="A1.T7.2.2.4.2.11" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T7.2.2.4.2.11.1" class="ltx_text ltx_font_bold">rep-20</span></td>
</tr>
<tr id="A1.T7.2.2.5.3" class="ltx_tr">
<td id="A1.T7.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b</td>
<td id="A1.T7.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.843</td>
<td id="A1.T7.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.804</td>
<td id="A1.T7.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.778</td>
<td id="A1.T7.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.760</td>
<td id="A1.T7.2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.747</td>
<td id="A1.T7.2.2.5.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.796</td>
<td id="A1.T7.2.2.5.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.763</td>
<td id="A1.T7.2.2.5.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.743</td>
<td id="A1.T7.2.2.5.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.728</td>
<td id="A1.T7.2.2.5.3.11" class="ltx_td ltx_align_center ltx_border_t">0.716</td>
</tr>
<tr id="A1.T7.2.2.6.4" class="ltx_tr">
<td id="A1.T7.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="A1.T7.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.080</td>
<td id="A1.T7.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.024</td>
<td id="A1.T7.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.012</td>
<td id="A1.T7.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="A1.T7.2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.005</td>
<td id="A1.T7.2.2.6.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.103</td>
<td id="A1.T7.2.2.6.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.039</td>
<td id="A1.T7.2.2.6.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.020</td>
<td id="A1.T7.2.2.6.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.012</td>
<td id="A1.T7.2.2.6.4.11" class="ltx_td ltx_align_center ltx_border_t">0.008</td>
</tr>
<tr id="A1.T7.2.2.7.5" class="ltx_tr">
<td id="A1.T7.2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="A1.T7.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.137</td>
<td id="A1.T7.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.068</td>
<td id="A1.T7.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.046</td>
<td id="A1.T7.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.035</td>
<td id="A1.T7.2.2.7.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.029</td>
<td id="A1.T7.2.2.7.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.552</td>
<td id="A1.T7.2.2.7.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.491</td>
<td id="A1.T7.2.2.7.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.459</td>
<td id="A1.T7.2.2.7.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.437</td>
<td id="A1.T7.2.2.7.5.11" class="ltx_td ltx_align_center ltx_border_tt">0.422</td>
</tr>
<tr id="A1.T7.2.2.8.6" class="ltx_tr">
<td id="A1.T7.2.2.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A1.T7.2.2.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="A1.T7.2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.135</td>
<td id="A1.T7.2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.068</td>
<td id="A1.T7.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.048</td>
<td id="A1.T7.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.038</td>
<td id="A1.T7.2.2.8.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.032</td>
<td id="A1.T7.2.2.8.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.599</td>
<td id="A1.T7.2.2.8.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.539</td>
<td id="A1.T7.2.2.8.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.506</td>
<td id="A1.T7.2.2.8.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.483</td>
<td id="A1.T7.2.2.8.6.11" class="ltx_td ltx_align_center ltx_border_tt">0.466</td>
</tr>
<tr id="A1.T7.2.2.9.7" class="ltx_tr">
<td id="A1.T7.2.2.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.9.7.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="A1.T7.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.131</td>
<td id="A1.T7.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.065</td>
<td id="A1.T7.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.044</td>
<td id="A1.T7.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.034</td>
<td id="A1.T7.2.2.9.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.028</td>
<td id="A1.T7.2.2.9.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.524</td>
<td id="A1.T7.2.2.9.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.463</td>
<td id="A1.T7.2.2.9.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.432</td>
<td id="A1.T7.2.2.9.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.412</td>
<td id="A1.T7.2.2.9.7.11" class="ltx_td ltx_align_center ltx_border_t">0.397</td>
</tr>
<tr id="A1.T7.2.2.10.8" class="ltx_tr">
<td id="A1.T7.2.2.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A1.T7.2.2.10.8.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="A1.T7.2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.116</td>
<td id="A1.T7.2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.050</td>
<td id="A1.T7.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.031</td>
<td id="A1.T7.2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.023</td>
<td id="A1.T7.2.2.10.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.018</td>
<td id="A1.T7.2.2.10.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.401</td>
<td id="A1.T7.2.2.10.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.335</td>
<td id="A1.T7.2.2.10.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.303</td>
<td id="A1.T7.2.2.10.8.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.282</td>
<td id="A1.T7.2.2.10.8.11" class="ltx_td ltx_align_center ltx_border_tt">0.269</td>
</tr>
<tr id="A1.T7.2.2.11.9" class="ltx_tr">
<td id="A1.T7.2.2.11.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.11.9.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="A1.T7.2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.134</td>
<td id="A1.T7.2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.069</td>
<td id="A1.T7.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.048</td>
<td id="A1.T7.2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.038</td>
<td id="A1.T7.2.2.11.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.032</td>
<td id="A1.T7.2.2.11.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.441</td>
<td id="A1.T7.2.2.11.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.380</td>
<td id="A1.T7.2.2.11.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.350</td>
<td id="A1.T7.2.2.11.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.331</td>
<td id="A1.T7.2.2.11.9.11" class="ltx_td ltx_align_center ltx_border_t">0.318</td>
</tr>
<tr id="A1.T7.2.2.12.10" class="ltx_tr">
<td id="A1.T7.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.12.10.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="A1.T7.2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.125</td>
<td id="A1.T7.2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.060</td>
<td id="A1.T7.2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.041</td>
<td id="A1.T7.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.032</td>
<td id="A1.T7.2.2.12.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.027</td>
<td id="A1.T7.2.2.12.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.443</td>
<td id="A1.T7.2.2.12.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.381</td>
<td id="A1.T7.2.2.12.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.350</td>
<td id="A1.T7.2.2.12.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.330</td>
<td id="A1.T7.2.2.12.10.11" class="ltx_td ltx_align_center ltx_border_t">0.316</td>
</tr>
<tr id="A1.T7.2.2.13.11" class="ltx_tr">
<td id="A1.T7.2.2.13.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.13.11.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="A1.T7.2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.119</td>
<td id="A1.T7.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.053</td>
<td id="A1.T7.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.033</td>
<td id="A1.T7.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.024</td>
<td id="A1.T7.2.2.13.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.019</td>
<td id="A1.T7.2.2.13.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.409</td>
<td id="A1.T7.2.2.13.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.341</td>
<td id="A1.T7.2.2.13.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.308</td>
<td id="A1.T7.2.2.13.11.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.287</td>
<td id="A1.T7.2.2.13.11.11" class="ltx_td ltx_align_center ltx_border_t">0.273</td>
</tr>
<tr id="A1.T7.2.2.14.12" class="ltx_tr">
<td id="A1.T7.2.2.14.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A1.T7.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="A1.T7.2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.094</td>
<td id="A1.T7.2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.033</td>
<td id="A1.T7.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.017</td>
<td id="A1.T7.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.011</td>
<td id="A1.T7.2.2.14.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.008</td>
<td id="A1.T7.2.2.14.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.244</td>
<td id="A1.T7.2.2.14.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.172</td>
<td id="A1.T7.2.2.14.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.144</td>
<td id="A1.T7.2.2.14.12.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.128</td>
<td id="A1.T7.2.2.14.12.11" class="ltx_td ltx_align_center ltx_border_tt">0.118</td>
</tr>
<tr id="A1.T7.2.2.15.13" class="ltx_tr">
<td id="A1.T7.2.2.15.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A1.T7.2.2.15.13.1.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="A1.T7.2.2.15.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.169</td>
<td id="A1.T7.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.098</td>
<td id="A1.T7.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.072</td>
<td id="A1.T7.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.059</td>
<td id="A1.T7.2.2.15.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.050</td>
<td id="A1.T7.2.2.15.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.621</td>
<td id="A1.T7.2.2.15.13.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.566</td>
<td id="A1.T7.2.2.15.13.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.537</td>
<td id="A1.T7.2.2.15.13.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.518</td>
<td id="A1.T7.2.2.15.13.11" class="ltx_td ltx_align_center ltx_border_t">0.502</td>
</tr>
<tr id="A1.T7.1.1.1" class="ltx_tr">
<td id="A1.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="A1.T7.1.1.1.1.1.1" class="ltx_sup"><span id="A1.T7.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="A1.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.084</td>
<td id="A1.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.026</td>
<td id="A1.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.013</td>
<td id="A1.T7.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
<td id="A1.T7.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="A1.T7.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.109</td>
<td id="A1.T7.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.043</td>
<td id="A1.T7.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.023</td>
<td id="A1.T7.1.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.014</td>
<td id="A1.T7.1.1.1.11" class="ltx_td ltx_align_center ltx_border_t">0.010</td>
</tr>
<tr id="A1.T7.2.2.2" class="ltx_tr">
<td id="A1.T7.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="A1.T7.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="A1.T7.2.2.2.1.1.1" class="ltx_sup"><span id="A1.T7.2.2.2.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="A1.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.103</td>
<td id="A1.T7.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.039</td>
<td id="A1.T7.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.023</td>
<td id="A1.T7.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.017</td>
<td id="A1.T7.2.2.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.013</td>
<td id="A1.T7.2.2.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.143</td>
<td id="A1.T7.2.2.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.071</td>
<td id="A1.T7.2.2.2.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.047</td>
<td id="A1.T7.2.2.2.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.035</td>
<td id="A1.T7.2.2.2.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.029</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 7:</span> 두 언어의 프롬프트를 사용한 반복 실험의 완전한 결과.</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Details about Experiment Setup</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p" id="A2.p1.1">우리의 소스 코드는 <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/lca0503/Llama_tw" target="_blank" title="">https://github.com/lca0503/Llama_tw</a>에서 사용할 수 있습니다. Llama-2-7b-chat<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" target="_blank" title="">https://huggingface.co/meta-llama/Llama-2-7b-chat-hf</a></span></span></span>에 대한 지속적인 사전 훈련을 위해 일반적인 중국 전통 말뭉치의 10억 토큰 데이터를 사용하여 간단한 접근법을 채택했다. 우리는 웹사이트와 뉴스 페이지를 포함한 다양한 출처에서 중국 전통 말뭉치를 수집했다. 지속적인 사전 훈련 시 메모리 효율을 향상시키기 위해 DeepSpeed <cite class="ltx_cite ltx_citemacro_cite">Rasley et al. (<a class="ltx_ref" href="#bib.bib27" title="">2020</a>)</cite>를 활용한다. 모든 모델의 지속적인 사전 훈련은 400만 토큰에 해당하는 글로벌 배치 크기로 진행됩니다. 이 과정은 64개의 V100 GPU에서 수행되며, 기울기 누적 단계를 16으로 구성한다. 지속적인 사전 훈련 동안 학습률은 3e-5로 일정하게 유지되었고, 어댑터 접근에 대해 추가 학습률 3e-4를 실험했다. 다양한 간단한 접근법에 대한 훈련 가능한 매개변수에 관한 세부 사항은 표 <a class="ltx_ref" href="#A1.T5" title="Table 5 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>에서 찾을 수 있다.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p class="ltx_p" id="A2.p2.1">여기에서 어댑터 설정을 조사합니다. <span class="ltx_text ltx_font_smallcaps" id="A2.p2.1.2">Lora</span> <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="#bib.bib13" title="">2022</a>)</cite> 모델의 각 계층의 쿼리 및 값 투영 행렬만 선택적으로 적응합니다. 네트워크 랭크를 8, 알파를 32로 설정하였다. <span class="ltx_text ltx_font_smallcaps" id="A2.p2.1.1">(Ia)<sup class="ltx_sup" id="A2.p2.1.1.1"><span class="ltx_text ltx_font_upright" id="A2.p2.1.1.1">3</span></sup></span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>의 경우 학습된 벡터를 통해 각 모델 레이어의 피드포워드 모듈의 내부 활성화와 자체 주의 모듈의 키 및 값 행렬을 재스케일한다. 이것은 이들 벡터와의 요소-와이즈 곱셈을 통해 달성된다.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Details about Experiment Tasks</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Output format Analysis</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.p1.1">우리는 출력 형식 분석에서 언어 식별과 반복 분석의 두 가지 작업을 수행한다. 효율성을 높이기 위해 vLLM <cite class="ltx_cite ltx_citemacro_cite">Kwon et al. (<a class="ltx_ref" href="#bib.bib19" title="">2023</a>)</cite>를 활용하였다. 즉, 감독 미세 조정(Supervised Fine-Tuning, SFT) 및 인간 피드백으로부터의 강화 학습(Reinforcement Learning from Human Feedback,RLHF)과 같은 정렬 작업을 거친 모델에 대해, 프롬프트를 "[INST] <컨텍스트> [/INST]"로 설정하였다. 512의 max_tokens 설정으로 모델을 구성하고 온도를 0.1로 설정하고 top_p를 0.9로 설정하는 핵 샘플링을 활용한다.</p>
</div>
<div id="A3.SS1.p2" class="ltx_para">
<p class="ltx_p" id="A3.SS1.p2.1">출력 형식 분석을 수행 하려면 다음 데이터 집합을 활용 합니다.</p>
<ul id="A3.I1" class="ltx_itemize">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A3.I1.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I1.i1.p1.1.1">NeuLab-TedTalks<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote2.1.1.1">2</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://opus.nlpl.eu/NeuLab-TedTalks-v1.php" target="_blank" title="">https://opus.nlpl.eu/NeuLab-TedTalks-v1.php</a></span></span></span></span></cite idx=0></cite>: A common corpus of TED talks, translated to numerous low-resource languages by a global community of volunteers. 출력 형식 실험을 위해 영어 및 번체 중국어 하위 집합에서 2000개의 정렬된 문장을 무작위로 선택했다. OPUS <cite class="ltx_cite ltx_citemacro_cite">Tiedemann (<a class="ltx_ref" href="#bib.bib30" title="">2012</a>)</cite>로부터 코퍼스를 다운로드한다.</p>
</div>
</li>
</ul>
</div>
<div id="A3.SS1.p3" class="ltx_para">
<p class="ltx_p" id="A3.SS1.p3.1">언어 식별 분석을 위해 FastText <cite class="ltx_cite ltx_citemacro_cite">Joulin et al. (<a class="ltx_ref" href="#bib.bib16" title="">2016a</a>, <a class="ltx_ref" href="#bib.bib17" title="">b</a>)</cite> 언어 식별 모델을 활용하여 생성된 토큰의 언어를 검출한다. 반복 분석의 경우 생성된 출력과 프롬프트의 조합 내에서 BPE 수준에서 복제된 n-그램 토큰의 비율을 평가한다.</p>
</div>
<div id="A3.SS1.p4" class="ltx_para">
<p class="ltx_p" id="A3.SS1.p4.1">표 <a class="ltx_ref" href="#A1.T6" title="Table 6 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>는 우리 중국 전통 말뭉치에 대한 반복 통계를 제시하고, 표 <a class="ltx_ref" href="#A1.T7" title="Table 7 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>는 반복 분석 실험의 전체 결과를 제시한다. 특히, 비교적 적은 수의 반복 토큰을 포함하는 사전 훈련된 말뭉치에도 불구하고, 이 말뭉치에 사전 훈련된 모델은 특히 번체 중국어로 프롬프트될 때 텍스트 반복의 증가를 나타냈다.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Knowledge Analysis</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS2.p1.1">지식 분석에서 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.1">ARC</span>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.2">Hellaswag</span>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.3">MMLU</span>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.4">C-eval-tw</span>의 네 가지 벤치마크에서 모델의 성능을 평가합니다. 이러한 벤치마크에 대한 모델 성능을 평가하기 위해 EleutherAI/lm-evaluation-harness<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span><cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="#bib.bib7" title="">2021</a>)</cite>를 사용한다. 이러한 벤치마크는 객관식 문항으로 구성되어 있다. 정확도 계산은 확률이 가장 높은 옵션을 선택하는 것을 기반으로 합니다.</p>
</div>
<div id="A3.SS2.p2" class="ltx_para">
<ul id="A3.I2" class="ltx_itemize">
<li id="A3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="A3.I2.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i1.p1.1.1">ARC<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote4.1.1.1">4</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://allenai.org/data/arc" target="_blank" title="">https://allenai.org/data/arc</a></span></span></span></span></cite idx=0></cite>: A collection of natural, grade-school science questions. ARC 데이터 세트 내의 챌린지 세트에 대한 평가를 수행했다. 우리는 길이 정규화 정확도를 기반으로 성능을 평가하는 25샷 프롬프트를 사용하여 이 벤치마크를 수행했다.</p>
</div>
</li>
<li id="A3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="A3.I2.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i2.p1.1.1">Hellaswag<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote5.1.1.1">5</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://rowanzellers.com/hellaswag" target="_blank" title="">https://rowanzellers.com/hellaswag</a></span></span></span></span></span></cite idx=0></cite>: An evaluation of commonsense inference, presenting a task that is straightforward for human but poses a challenge for the state-of-the-art models. 우리는 길이 정규화 정확도를 기반으로 성능을 평가하는 10샷 프롬프트를 사용하여 이 벤치마크를 수행했다.</p>
</div>
</li>
<li id="A3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="A3.I2.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i3.p1.1.1">MMLU<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote6.1.1.1">6</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/hendrycks/test" target="_blank" title="">https://github.com/hendrycks/test</a></span></span></span></span></span></cite idx=0></cite>: A test for a text model's multitasking accuracy, covering 57 tasks from elementary math to U.S history, computer science, law and beyond. 우리는 개별 작업에 대한 정확도를 평균화하여 메트릭을 계산하는 5샷 프롬프트를 사용하여 이 벤치마크를 수행했다.</p>
</div>
</li>
<li id="A3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="A3.I2.i4.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i4.p1.1.1">C-eval-tw</span>: C-eval<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cevalbenchmark.com" target="_blank" title="">https://cevalbenchmark.com</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="#bib.bib14" title="">2023</a>)</cite>는 중국어 문맥에서 기초 모델의 고급 지식 및 추론 능력을 평가하는 테스트 역할을 한다. 테스트는 처음에 단순화된 중국어에서 수행되었으며 딥-트랜슬레이터<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nidhaloff/deep-translator" target="_blank" title="">https://github.com/nidhaloff/deep-translator</a></span></span></span> 패키지의 Google Translate <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="#bib.bib34" title="">2016</a>)</cite> API를 사용하여 번체 중국어로 번역했다. 우리는 개별 작업에 대한 정확도를 평균화하여 메트릭을 계산하는 0샷 프롬프트를 사용하여 이 벤치마크를 수행했다.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Reliability Analysis</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS3.p1.1">신뢰성 분석에서는 진실성, 독성 및 편향을 포함한 세 가지 벤치마크 데이터 세트에서 모델의 성능을 확인한다. 우리는 영어와 번체 중국어 모두에서 이 분석을 수행한다. 이러한 벤치마크는 영어로 되어 있지만 심층 번역기 패키지의 Google Translate API를 사용하여 번체 중국어로 된 데이터 세트를 평가하여 포괄적인 분석을 보장합니다.</p>
</div>
<div id="A3.SS3.p2" class="ltx_para">
<ul id="A3.I3" class="ltx_itemize">
<li id="A3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I3.i1.p1" class="ltx_para">
<p class="ltx_p" id="A3.I3.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I3.i1.p1.1.1">TruthfulQA<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote9.1.1.1">9</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/sylinrl/TruthfulQA" target="_blank" title="">https://github.com/sylinrl/TruthfulQA</a></span></span></span></span></cite idx=0></cite>: 언어 모델의 진실성을 측정하기 위해 활용되는 데이터셋. 이 데이터 세트는 잘못된 신념이나 오개념을 가진 개인으로부터 잘못된 응답을 이끌어내기 위해 고안된 질문으로 구성된다. 이 분석에서는 또한 이 벤치마크를 수행하기 위해 EleutherAI/lm-평가-harness를 사용한다. 우리는 6-shot 프롬프트를 사용하여 벤치마크를 수행한다. 채점 메커니즘은 질문 및 다수의 참/거짓 참조 답변들을 포함하며, 여기서 점수는 참 답변들의 세트에 할당된 정규화된 총 확률에 의해 계산된다.</p>
</div>
</li>
<li id="A3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I3.i2.p1" class="ltx_para">
<p class="ltx_p" id="A3.I3.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I3.i2.p1.1.1">ToxiGen<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote10.1.1.1">10</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/microsoft/TOXIGEN" target="_blank" title="">https://github.com/microsoft/TOXIGEN</a></span></span></span></span></cite idx=0></cite>: 언어 모델의 독성을 탐지하기 위해 사용한 데이터 세트입니다. 데이터세트는 13개의 별개의 소수 그룹과 관련된 독성 및 양성 진술을 포함하는 기계 생성 데이터세트이다. 대상 인구 통계 그룹에서 주석자가 동의하지 않는 프롬프트를 제외하여 노이즈를 완화시키는 정제된 데이터 세트 <span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/microsoft/SafeNLP" target="_blank" title="">https://github.com/microsoft/SafeNLP</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Hosseini et al. (<a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite>를 채택한다. 우리는 이러한 진술을 우리의 프롬프트로 받아들입니다. 효율성을 높이기 위해 vLLM을 활용합니다. 정렬 작업을 거친 모델의 경우 프롬프트를 "[INST] <컨텍스트> [/INST]"로 설정합니다. max_tokens 설정 512로 모델을 구성하고 핵 샘플링을 활용하여 온도를 0.1, top_p를 0.9로 설정한다. 독성 세대 식별을 위해 기본 RoBERTa 기반 분류기 ToxiGen <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib23" title="">2019</a>)</cite>를 활용한다. 분류기가 영어 텍스트를 처리하도록 설계되었기 때문에, 우리는 평가하기 전에 구글 번역기 API를 사용하여 모델의 출력을 영어로 번역함으로써 이 제약을 해결한다.</p>
</div>
</li>
<li id="A3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I3.i3.p1" class="ltx_para">
<p class="ltx_p" id="A3.I3.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I3.i3.p1.1.1">Bold<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote12.1.1.1">12</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/amazon-science/bold" target="_blank" title="">https://github.com/amazon-science/bold</a></span></span></span></span></cite idx=0></cite>: 바이어스 분석을 위해 사용하는 데이터셋입니다. 이 편향된 데이터 세트는 인종, 성별, 종교, 정치 이념 및 직업의 5개 영역에 걸쳐 위키피디아 프롬프트로 구성된다. 또한 효율성을 높이기 위해 vLLM을 활용합니다. 우리는 제한된 수의 프롬프트로 인해 종교 이데올로기 하위 그룹인 힌두교와 무신론에 속하는 프롬프트를 제외한다. 정렬 작업을 거친 모델의 경우 프롬프트를 "[INST] <컨텍스트> [/INST]"로 설정합니다. Max_tokens 설정 512로 모델을 구성하고 핵 샘플링을 활용하여 온도를 0.1, top_p를 0.9로 설정한다. VADER(Valence Aware Dictionary and Sentiment Reasoner) <cite class="ltx_cite ltx_citemacro_cite">Hutto and Gilbert (<a class="ltx_ref" href="#bib.bib15" title="">2014</a>)</cite>를 사용하여 결합된 프롬프트와 생성 텍스트에 대한 감성 점수를 계산한다. 또한 VADER를 사용하여 감성 점수를 계산하기 전에 Google Translator API를 사용하여 모델의 출력을 영어로 번역한다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2401.03128" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2401.03129" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2401.03129">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.03129" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2401.03130" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 10:39:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
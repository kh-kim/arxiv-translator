<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Examining Forgetting in Continual Pre-training of Aligned Large Language Models\n' +
      '\n' +
      'Chen-An Li\\({}^{1,2}\\) Hung-Yi Lee\\({}^{1}\\)\n' +
      '\n' +
      '\\({}^{1}\\)National Taiwan University, Taipei, Taiwan\n' +
      '\n' +
      '\\({}^{2}\\)ASUS Open Cloud Infrastructure Software Center, Taipei, Taiwan\n' +
      '\n' +
      'b08902123@csie.ntu.edu.tw hungyilee@ntu.edu.tw\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      'Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      'Large Language Models (LLMs) have demonstrated impressive performance across various tasks Brown et al. (2020). There is an increasing trend of releasing pre-trained LLMs and fine-tuned variants Touvron et al. (2023, 2023). Many of these fine-tuned variants aim to augment the knowledge or linguistic capabilities of the existing LLM Roziere et al. (2023); Cui et al. (2023).\n' +
      '\n' +
      'We have noticed that many advancements in fine-tuned variants adhere to a conventional procedure consisting of two key steps: 1. Conduct further continual pre-training on an existing LLM. 2. Carry out subsequent alignment operations, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), on the model obtained in Step 1. Among these fine-tuned variants, many developments perform further continual pre-training on existing fine-tuned LLMs Cui et al. (2023); Lin and Chen (2023).\n' +
      '\n' +
      'Previous studies have demonstrated that continual pre-training can significantly improve the model\'s ability to understand and generate specific content Gupta et al. (2023). However, continual pre-training could lead to catastrophic forgetting French (1999), and limited research has explored the abilities forgotten during pre-training on an existing fine-tuned LLM.\n' +
      '\n' +
      'Some works have studied continual learning for language models. Qin et al. (2022) focused on efficient lifelong pre-training on pre-trained language models for emerging data. Ke et al. (2022) proposed a continual domain-adaptive pre-training method on a masked language model. Song et al. (2023) introduced continual parameter-efficient tuning for the ongoing adaptation of LLMs to continual tasks. Xie et al. (2023) investigate an alternative approach to continual pre-training for developing domain-specific LLMs. Qi et al. (2023) suggests that fine-tuning compromises the safety alignment of LLMs. Zhai et al. (2023) evaluates the forgetting in fine-tuned multimodal LLMs.\n' +
      '\n' +
      'Our work examines the forgetting occurrence during continual pre-training on an existing fine-tuned LLM. Our paper primarily focuses on continual pre-training using the Traditional Chinese corpus. We evaluate the impact of continual pre-training across various dimensions, including output format, knowledge, and reliability. We show that more than straightforward methods are required for resolving this issue. Also, we observe an increased prominence of the repetition problem in models that tend to generate Traditional Chinese outputs. Lastly, despite continual pre-training, our findings suggest that the model\'s knowledge remains unaffected while its reliability declines.\n' +
      '\n' +
      '## 2 Observation of Catastrophic Forgetting during Continual Pre-training\n' +
      '\n' +
      '### Settings for Observation\n' +
      '\n' +
      'We conduct pre-training on Llama-2-7b-chat, a model comprising approximately 7 billion parameters that have undergone sequential alignment operations, including SFT and RLHF. Our pre-training process utilizes the 1 billion tokens of Traditional Chinese data. We denote the model after continual pre-training as Llama-2-7b-chat-cp. We employ specific prompts to observe the differences between the outputs generated by the two models.\n' +
      '\n' +
      '### Observation of Catastrophic Forgetting\n' +
      '\n' +
      'Figure 1 shows the results obtained from our prompt. We observed that Llama-2-7b-chat-cp tends to generate Traditional Chinese text compared to Llama-2-7b-chat; however, the generated text of Llama-2-7b-chat exhibits repetition issues. Consequently, we conducted a more in-depth investigation into the model\'s performance across various aspects. Appendix A contains additional results of more prompts.\n' +
      '\n' +
      '## 3 Straightforward Approaches\n' +
      '\n' +
      'This section introduces straightforward approaches to solving the catastrophic forgetting issues discussed in the previous section.\n' +
      '\n' +
      '### Freeze layers\n' +
      '\n' +
      'Previous studies have shown that distinct functionality exists in different layers of Transformer-based models when processing textual information Ethayarajh (2019); Van Aken et al. (2019). Consequently, we experiment with freezing specific layers of the model during continual pre-training. Specifically, we explore freezing the first ten layers and freezing the last ten layers, denoted as Freeze First 10 and Freeze Last 10, respectively.\n' +
      '\n' +
      '### Freeze modules\n' +
      '\n' +
      'We also conduct experiments by freezing specific modules of the model during continual pre-training. We aim to explore whether these designated modules preserve the abilities acquired during the alignment operations. We explore four strategies:\n' +
      '\n' +
      '* Freeze Attn.: Freeze the self-attention modules in each layer of the model.\n' +
      '* Only Attn.: Freeze all modules in each layer except the self-attention modules of the model.\n' +
      '* Freeze MLP: Freeze the feed-forward modules in each model layer.\n' +
      '* Only MLP: Freeze all modules in each layer except the feed-forward modules of the model.\n' +
      '\n' +
      '### Adapter\n' +
      '\n' +
      'Adapters are frequently employed in training Transformer-based models Houlsby et al. (2019). In our study, we experiment with two types of adapters.\n' +
      '\n' +
      '* LoraHu et al. (2022): A method that incorporates trainable low-rank decomposition matrices into each layer of the Transformer-based model. In our implementation, we selectively adapt only the query and value projection matrices of each layer in the model.\n' +
      '* (IA)\\({}^{3}\\)Liu et al. (2022): A technique involving element-wise multiplication of the model\'s activations with a learned vector. We rescale the key and value matrices in self-attention modules and the inner activations in feed-forward modules in each model layer.\n' +
      '\n' +
      '## 4 Experiments\n' +
      '\n' +
      '### Setup\n' +
      '\n' +
      'We employed straightforward approaches for continual pre-training on Llama-2-7b-chat, utilizing the 1 billion tokens of data from general Traditional Chinese corpus. The learning rate during continual pre-training remained constant at 3e-5, and we experimented with an additional learning rate 3e-4 for the adapter approaches. More details can be found in Appendix B.\n' +
      '\n' +
      'Figure 1: Illustration of models’ outputs. The translation of our prompt is “Tell me something about Mexico City.”\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:3]\n' +
      '\n' +
      'of the language identification experiment. We observe that when using English prompts, nearly every model tends to generate output in English. When provided with a Chinese prompt, we observed that Llama-2-7b tends to output in Chinese, whereas Llama-2-7b-chat tends to output in English. Furthermore, with Chinese prompts, the Freeze First 10 Layers model tends to yield a higher proportion of Chinese text output than the Freeze Last 10 Layers model. Models with frozen modules show relatively similar results, with approximately \\(60\\%\\) of their output being in Chinese. In the case of adapters, increasing the learning rate can lead the Lora model to produce more Chinese output, while the (Ia)\\({}^{3}\\) model tends to favor English output.\n' +
      '\n' +
      'Table 2 showcases the results of the repetition analysis experiment. We observed that regardless of given Chinese or English prompts, Llama-2-7b consistently exhibits significant repetition issues compared to Llama-2-7b-chat. Additionally, models after continual pre-training on Traditional Chinese corpus displayed a noticeable increase in text repetition with Chinese prompts compared to English prompts. Furthermore, we found that models that are more inclined to generate Chinese output when using Chinese prompts are more likely to have repetition issues.\n' +
      '\n' +
      '#### 4.3.2 Knowledge\n' +
      '\n' +
      'Table 3 shows our knowledge analysis experiments\' results. Llama-2-7b-chat performs similarly to Llama-2-7b on Hellaswag and MULU, while showing a slightly better performance on ARC and C-eval-tw. In the ARC and Hellaswag benchmarks, almost all continually pre-trained models outperform Llama-2-7b-chat. In the MULU benchmark, most continual pre-trained models tend to outperform Llama-2-7b-chat. However, in the case of the C-eval-tw benchmark, there is no clear pattern when comparing the efficacy of models utilizing simple methods for continual pre-training against Llama-2-7b-chat. It is worth noting that the observed differences mentioned above are subtle.\n' +
      '\n' +
      '#### 4.3.3 Reliability\n' +
      '\n' +
      'In Table 4, we present the results of the reliability experiment. Llama-2-7b-chat consistently outperforms Llama-2-7b on the truthfulness and toxicity benchmarks. Notably, after continual pre-training, the models demonstrate inferior performance compared to Llama-2-7b-chat on the two benchmarks. This trend is particularly pronounced in the truthfulness analysis benchmark for English and the toxicity benchmark for Traditional Chinese. Furthermore, we observed that models with a preference for generating Chinese output exhibit inferior performance in the toxicity benchmark. Regarding the bias benchmark, we can observe that Llama-2-7b-chat outputs more positive text than Llama-2-7b. After continual pre-training, the models\' outputs have relatively more negative sentiment scores than Llama-2-7b-chat.\n' +
      '\n' +
      '## 5 Conclusion\n' +
      '\n' +
      'This work shows that catastrophic forgetting during continual pre-training is a non-trivial challenge and cannot be resolved through straightforward methods. Additionally, we find that the repetition problem becomes more pronounced when the model, after continual pre-training, is inclined to produce\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c} \\hline \\multirow{2}{*}{} & \\multicolumn{2}{c|}{**truthful0A**} & \\multicolumn{2}{c}{**Toxicen**} & \\multicolumn{2}{c}{**bot0.D**} \\\\ \\cline{2-6}  & \\multicolumn{2}{c|}{**mc 2**} & \\multicolumn{2}{c|}{**toxicity \\(\\downarrow\\)**} & \\multicolumn{2}{c}{**sentiment**} \\\\ \\hline \\multicolumn{1}{c|}{} & **EN** & **TW** & **EN** & **TW** & **EN** & **TW** \\\\ \\hline \\multicolumn{1}{c|}{} & 39.0 & 45.9 & 20.30 & 24.80 & 0.41\\(\\pm\\)0.17 & 0.23\\(\\pm\\)0.13 \\\\ \\hline \\multicolumn{1}{c|}{} & 44.6 & 49.7 & 0.03 & 0.22 & 0.66\\(\\pm\\)0.24 & 0.69\\(\\pm\\)0.19 \\\\ \\hline \\multicolumn{1}{c|}{} & 49.2 & 48.5 & 0.05 & 5.74 & 0.52\\(\\pm\\)0.20 & 0.34\\(\\pm\\)0.14 \\\\ \\hline \\hline Freeze First 10 & 41.7 & 48.5 & 0.08 & 7.12 & 0.55\\(\\pm\\)0.22 & 0.34\\(\\pm\\)0.12 \\\\ \\hline Freeze Last 10 & 40.4 & 48.8 & 0.01 & 4.69 & 0.58\\(\\pm\\)0.21 & 0.37\\(\\pm\\)0.15 \\\\ \\hline Freeze Attn. & 41.6 & 48.8 & 0.04 & 3.15 & 0.57\\(\\pm\\)0.21 & 0.42\\(\\pm\\)0.16 \\\\ \\hline Only Attn. & 40.8 & 48.6 & 0.04 & 3.27 & 0.59\\(\\pm\\)0.24 & 0.43\\(\\pm\\)0.15 \\\\ \\hline Freeze MLP & 40.9 & 48.8 & 0.0 & 3.31 & 0.60\\(\\pm\\)0.22 & 0.42\\(\\pm\\)0.14 \\\\ \\hline Only MUL & 41.3 & 48.8 & 0.04 & 3.39 & 0.58\\(\\pm\\)0.21 & 0.43\\(\\pm\\)0.16 \\\\ \\hline \\hline Lora & 43.6 & 49.1 & 0.03 & 0.79 & 0.64\\(\\pm\\)0.22 & 0.63\\(\\pm\\)0.17 \\\\ \\hline Lora (3e-4) & 42.5 & 48.9 & 0.07 & 7.97 & 0.57\\(\\pm\\)0.22 & 0.35\\(\\pm\\)0.10 \\\\ \\hline (Ia)\\({}^{3}\\) & 44.2 & 49.8 & 0.0 & 0.17 & 0.66\\(\\pm\\)0.24 & 0.69\\(\\pm\\)0.19 \\\\ \\hline (Ia)\\({}^{3}\\)(3e-4) & 43.0 & 49.9 & 0.0 & 0.11 & 0.66\\(\\pm\\)0.23 & 0.68\\(\\pm\\)0.18 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 4: Reliability analysis experiment results on three benchmarks, including truthfulness, bias, and toxicity aspects. **EN** denotes the origin dataset in English, while **TW** denotes the translated dataset in Traditional Chinese.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline  & **ARC** & **Hellaswag** & **MULU** & **C-eval-tw** \\\\ \\cline{2-4}  & **ACC** & **ACC** & **ACC** & **ACC** \\\\ \\hline \\hline Llama-2-7b & 53.0 & 78.6 & 46.5 & 32.2 \\\\ \\hline Llama-2-7b-chat & 53.6 & 78.6 & 46.6 & 32.9 \\\\ \\hline \\hline Llama-2-7b-chat-cp & 52.0 & 77.6 & 49.1 & 33.4 \\\\ \\hline Freeze First 10 & 51.0 & 77.7 & 49.1 & 31.9 \\\\ \\hline Freeze Last 10 & 51.5 & 77.6 & 49.4 & 33.5 \\\\ \\hline \\hline Freeze Attn. & 51.9 & 77.7 & 48.9 & 32.2 \\\\ \\hline Only Attn. & 52.8 & 78.0 & 48.4 & 33.3 \\\\ \\hline Freeze MLP & 53.2 & 77.8 & 49.4 & 32.6 \\\\ \\hline Only MLP & 52.0 & 77.9 & 46.9 & 33.4 \\\\ \\hline \\hline Lora & 53.5 & 78.6 & 47.1 & 33.8 \\\\ \\hline Lora (3e-4) & 52.8 & 78.2 & 47.4 & 33.0 \\\\ \\hline (Ia)\\({}^{3}\\) & 53.7 & 77.9 & 47.0 & 32.6 \\\\ \\hline (Ia)\\({}^{3}\\) (3e-4) & 53.8 & 77.3 & 46.2 & 31.8 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 3: Knowledge analysis experiment results with four benchmarks.\n' +
      '\n' +
      'Traditional Chinese outputs. Moreover, after continual pre-training, the model\'s knowledge remains unaffected mainly; however, its reliability declines.\n' +
      '\n' +
      '### Limitations\n' +
      '\n' +
      'One notable limitation arises from the resource-intensive nature of continual pre-training LLMs, making reproducing all the straightforward continual pre-training methods outlined in this work challenging. Another significant limitation is that we only conducted continual pre-training using a Traditional Chinese corpus. However, we are also interested in extending our investigation to include pre-training on resources in other languages, and our methodology is easily adaptable to these settings.\n' +
      '\n' +
      '## Ethics Statement\n' +
      '\n' +
      'The continual pre-training of LLMs can compromise the models\' safety alignment, leading to the generation of text that may contain biased and toxic information. Exploring methods to mitigate compromising the safety alignment could be a prospective avenue for future research.\n' +
      '\n' +
      '## Acknowledgements\n' +
      '\n' +
      'We extend our appreciation to the ASUS Open Cloud Infrastructure Software Center for generously providing valuable resources. Special thanks to Steve Chung-Cheng Chen, Tsung-Ying Yang, Jen-Hao Cheng, Hsiao-Tsung Hung, Szu-Hsien Lee, and Dau-Cheng Lyu for their participation in insightful discussions.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901.\n' +
      '* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_.\n' +
      '* Cui et al. (2023) Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and effective text encoding for chinese llama and alpaca. _arXiv preprint arXiv:2304.08177_.\n' +
      '* Dhamala et al. (2021) Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. Bold: Dataset and metrics for measuring biases in open-ended language generation. In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 862-872.\n' +
      '* Ethayarajh (2019) Kawin Ethayarajh. 2019. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 55-65, Hong Kong, China. Association for Computational Linguistics.\n' +
      '* French (1999) Robert M French. 1999. Catastrophic forgetting in connectionist networks. _Trends in cognitive sciences_, 3(4):128-135.\n' +
      '* Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Mueminghoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A framework for few-shot language model evaluation.\n' +
      '* Gupta et al. (2023) Kshitij Gupta, Benjamin Therien, Adam Ibrahim, Mats Leon Richter, Quentin Gregory Anthony, Eugene Belliovsky, Irina Rish, and Timothee Lesort. 2023. Continual pre-training of large language models: How to re-warm your model? In _Workshop on Efficient Systems for Foundation Models @ ICML2023_.\n' +
      '* Hartvigsen et al. (2022) Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3309-3326, Dublin, Ireland. Association for Computational Linguistics.\n' +
      '* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_.\n' +
      '* Hosseini et al. (2023) Saghar Hosseini, Hamid Palangi, and Ahmed Hassan Awadallah. 2023. An empirical study of metrics to measure representational harms in pre-trained language models. In _Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)_, pages 121-134, Toronto, Canada. Association for Computational Linguistics.\n' +
      '* Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. In _International Conference on Machine Learning_, pages 2790-2799. PMLR.\n' +
      '* Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_.\n' +
      '* Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. _arXiv preprint arXiv:2305.08322_.\n' +
      '* Hutto and Gilbert (2014) Clayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In _Proceedings of the international AAAI conference on web and social media_, volume 8, pages 216-225.\n' +
      '* Joulin et al. (2016a) Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, and Tomas Mikolov. 2016a. Fasttext.zip: Compressing text classification models. _arXiv preprint arXiv:1612.03651_.\n' +
      '* Joulin et al. (2016b) Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016b. Bag of tricks for efficient text classification. _arXiv preprint arXiv:1607.01759_.\n' +
      '* Ke et al. (2022) Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2022. Continual pre-training of language models. In _The Eleventh International Conference on Learning Representations_.\n' +
      '* Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_.\n' +
      '* Lin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3214-3252, Dublin, Ireland. Association for Computational Linguistics.\n' +
      '* Lin and Chen (2023) Yen-Ting Lin and Yun-Nung Chen. 2023. Taiwan llm: Bridging the linguistic divide with a culturally aligned language model.\n' +
      '* Liu et al. (2022) Haokun Liu, Derek Tam, Mohammed Muoeful, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. _Advances in Neural Information Processing Systems_, 35:1950-1965.\n' +
      '* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_.\n' +
      '* Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Fine-tuning aligned language models compromises safety, even when users do not intend to! _arXiv preprint arXiv:2310.03693_.\n' +
      '* Qi et al. (2018) Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. When and why are pre-trained word embeddings useful for neural machine translation? In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)_, pages 529-535, New Orleans, Louisiana. Association for Computational Linguistics.\n' +
      '* Qin et al. (2022) Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022. ELLE: Efficient lifelong pre-training for emerging data. In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 2789-2810, Dublin, Ireland. Association for Computational Linguistics.\n' +
      '* Rasley et al. (2020) Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 3505-3506.\n' +
      '* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. 2023. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_.\n' +
      '* Song et al. (2023) Chenyang Song, Xu Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun, and Tao Yang. 2023. Conpet: Continual parameter-efficient tuning for large language models. _arXiv preprint arXiv:2309.14763_.\n' +
      '* Tiedemann (2012) Jorg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In _Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\'12)_, Istanbul, Turkey. European Language Resources Association (ELRA).\n' +
      '* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_.\n' +
      '* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.\n' +
      '* Van Aken et al. (2019) Betty Van Aken, Benjamin Winter, Alexander Loser, and Felix A Gers. 2019. How does bert answer questions? a layer-wise analysis of transformer representations. In _Proceedings of the 28th ACM international conference on information and knowledge management_, pages 1823-1832.\n' +
      '* Wu et al. (2018) Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:7]\n' +
      '\n' +
      'Here, we delve into our adapter settings. For LoraHu et al. (2022), we selectively adapt only the query and value projection matrices of each layer in the model. We set the network rank to 8 and the alpha to 32. In the case of (A)3Liu et al. (2022), we rescale the key and value matrices in self-attention modules and the inner activations in feed-forward modules in each model layer via learned vectors. This is achieved through element-wise multiplication with these vectors.\n' +
      '\n' +
      'Footnote 3: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)\n' +
      '\n' +
      '## Appendix C Additional Details about Experiment Tasks\n' +
      '\n' +
      '### Output format Analysis\n' +
      '\n' +
      'We perform two tasks in output format analysis: language identification and repetition analysis. We utilized vLLM Kwon et al. (2023) to enhance efficiency. Expressly, for models that have undergone alignment operations, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), we set up our prompt as "[INST] <context> [/INST]". We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9.\n' +
      '\n' +
      'To conduct output format analysis, we utilize the following dataset:\n' +
      '\n' +
      '* **NeuLab-TedTalks2**Qi et al. (2018): A common corpus of TED talks, translated into numerous low-resource languages by a global community of volunteers. We randomly selected 2000 aligned sentences from the English and Traditional Chinese subsets for our output format experiments. We download the corpus from OPUS Tiedemann (2012).\n' +
      '\n' +
      'Footnote 2: [https://opus.nlpl.eu/NeuLab-TedTalks-v1.php](https://opus.nlpl.eu/NeuLab-TedTalks-v1.php)\n' +
      '\n' +
      'For language identification analysis, we utilize the FastText Joulin et al. (2016, 2016) language identification model to detect the language of the generated tokens. As for repetition analysis, we assess the proportion of duplicated n-gram tokens at the BPE level within the combination of the generated output and the prompt.\n' +
      '\n' +
      'Table 6 presents the repetition statistics for our Traditional Chinese corpus, and Table 7 presents the full results of the repetition analysis experiment. Notably, despite the pre-trained corpus containing relatively few repetitive tokens, the model pre-trained on this corpus exhibited a rise in text repetition, particularly evident when prompted with Traditional Chinese.\n' +
      '\n' +
      '### Knowledge Analysis\n' +
      '\n' +
      'In our knowledge analysis, we assess our model\'s performance across four benchmarks: **ARC**, **Hellaswag**, **MLU**, and **C-eval-tw**. We employ EleutherAI/lm-evaluation-harness3**Gao et al. (2021) to assess the model performance on these benchmarks. These benchmarks consist of multiple-choice questions. The accuracy computation is based on selecting the option with the highest probabilities.\n' +
      '\n' +
      'Footnote 3: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)\n' +
      '\n' +
      '* **ARC4**Clark et al. (2018): A collection of natural, grade-school science questions. We conducted our evaluation on the Challenge Set within the ARC dataset. We conducted this benchmark using a 25-shot prompt, evaluating performance based on length-normalized accuracy. Footnote 4: [https://allenai.org/data/arc](https://allenai.org/data/arc)\n' +
      '* **Hellaswag5**Zellers et al. (2019): An evaluation of commonsense inference, presenting a task that is straightforward for humans but poses a challenge for state-of-the-art models. We conducted this benchmark using a 10-shot prompt, evaluating performance based on length-normalized\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c} \\hline\n' +
      '**rep-4** & **rep-8** & **rep-12** & **rep-16** & **rep-20** \\\\ \\hline\n' +
      '0.141 & 0.056 & 0.037 & 0.030 & 0.025 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 6: The proportion of duplicated n-gram tokens of our Traditional Chinese corpus.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c} \\hline  & **Trainable params** & **All params** & **Trainable \\%** \\\\ \\hline Llama-2-7b-chat-cp & 6,738,415,616 & 6,738,415,616 & 100.000 \\\\ \\hline Freeze First 10 & 4,714,582,016 & 6,738,415,616 & 69.966 \\\\ \\hline Freeze Last 10 & 4,714,582,016 & 6,738,415,616 & 69.966 \\\\ \\hline Freeze Attn. & 4,509,931,968 & 6,738,415,616 & 68.131 \\\\ \\hline Only Attn. & 2,147,483,648 & 6,738,415,616 & 31.869 \\\\ \\hline Freeze MLP & 2,409,893,888 & 6,738,415,616 & 35.764 \\\\ \\hline Only MLP & 4,328,521,728 & 6,738,415,616 & 64.236 \\\\ \\hline Lora & 4,194,304 & 6,742,609,920 & 0.062 \\\\ \\hline (Ia)3 & 614,400 & 6,739,030,016 & 0.009 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 5: Trainable parameters for various straightforward approaches.\n' +
      '\n' +
      'accuracy.\n' +
      '* **MMLU**6 (Hendrycks et al., 2020): A test for a text model\'s multitasking accuracy, covering 57 tasks from elementary math to U.S. history, computer science, law, and beyond. We conducted this benchmark using a 5-shot prompt, calculating metrics by averaging accuracy across individual tasks.\n' +
      '* **C-eval-tw**: C-eval7 (Huang et al., 2023) serves as a test to evaluate the advanced knowledge and reasoning abilities of foundational models in a Chinese context. The test was initially in Simplified Chinese, and we translated it into Traditional Chinese using the Google Translate (Wu et al., 2016) API in the deep-translator8 package. We conducted this benchmark using a 0-shot prompt, calculating metrics by averaging accuracy across individual tasks.\n' +
      '\n' +
      'Footnote 6: [https://github.com/hendrycks/test](https://github.com/hendrycks/test)\n' +
      '\n' +
      'Footnote 7: [https://cevalbenchmark.com](https://cevalbenchmark.com)\n' +
      '\n' +
      'Footnote 8: [https://github.com/nidhaloff/deep-translator](https://github.com/nidhaloff/deep-translator)\n' +
      '\n' +
      '### Reliability Analysis\n' +
      '\n' +
      'In our reliability analysis, we check the performance of our model across three benchmark datasets, including truthfulness, toxicity, and bias. We conduct this analysis in both English and Traditional Chinese. While these benchmarks are in English, we use the Google Translate API in the deep-translator package to evaluate the datasets in Traditional Chinese, ensuring a comprehensive analysis.\n' +
      '\n' +
      '* **TruthfulQA9**(Lin et al., 2022): A dataset utilized to measure the truthfulness of language models. This dataset comprises questions designed to elicit false responses from individuals with erroneous beliefs or misconceptions. In this analysis, we also employ EleutherAI/lm-evaluation-harness to conduct this benchmark. We conduct the benchmark using a 6-shot prompt. The scoring mechanism involves a question and multiple true/false reference answers, where the score is calculated by the normalized total probability assigned to the set of true answers. Footnote 9: [https://github.com/samazon-science/bold](https://github.com/samazon-science/bold)\n' +
      '* **ToxiGen**10 (Hartvigsen et al., 2022): The dataset we employed to detect the toxicity of language models. The dataset is a machine-generated dataset comprising toxic and benign statements related to 13 distinct minority groups. We adopt a refined dataset11 (Hosseini et al., 2023), which mitigates noise by excluding prompts where annotators disagree on the target demographic group. We take these statements as our prompts. We utilize vLLM to enhance efficiency. For models that have undergone alignment operations, we set up our prompt as "[INST] <context> [/INST]". We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9. We utilize the default RoBERTa-based classifier ToxiGen (Liu et al., 2019) for identifying toxic generations. As the classifier is designed to handle English text, we address this constraint by translating the model\'s output into English using the Google Translator API before evaluating. Footnote 10: [https://github.com/amazon-science/bold](https://github.com/amazon-science/bold)\n' +
      '* **Bold12**(Dhamala et al., 2021): The dataset we utilize for bias analysis. This biased dataset consists of Wikipedia prompts across five domains: race, gender, religion, political ideology, and profession. We also utilize vLLM to enhance efficiency. We exclude prompts that be\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} \\hline \\multirow{2}{*}{} & \\multicolumn{4}{c}{**EN prompt**} & \\multicolumn{4}{c}{**TW prompt**} \\\\ \\cline{2-10}  & **rep-4** & **rep-8** & **rep-12** & **rep-16** & **rep-20** & **rep-4** & **rep-8** & **rep-12** & **rep-16** & **rep-20** \\\\ \\hline \\multirow{2}{*}{Llama-2-7b} & 0.843 & 0.804 & 0.778 & 0.760 & 0.747 & 0.796 & 0.763 & 0.743 & 0.728 & 0.716 \\\\ \\hline \\multirow{2}{*}{Llama-2-7b-chat} & 0.080 & 0.024 & 0.012 & 0.007 & 0.005 & 0.103 & 0.039 & 0.020 & 0.012 & 0.008 \\\\ \\hline \\hline \\multirow{2}{*}{Llama-2-7b-chat-cp} & 0.137 & 0.068 & 0.046 & 0.035 & 0.029 & 0.552 & 0.491 & 0.459 & 0.437 & 0.422 \\\\ \\hline \\hline Freeze First 10 & 0.135 & 0.068 & 0.048 & 0.038 & 0.032 & 0.599 & 0.539 & 0.506 & 0.483 & 0.466 \\\\ \\hline Freeze Last 10 & 0.131 & 0.065 & 0.044 & 0.034 & 0.028 & 0.524 & 0.463 & 0.432 & 0.412 & 0.397 \\\\ \\hline \\hline Freeze Attn. & 0.116 & 0.050 & 0.031 & 0.023 & 0.018 & 0.401 & 0.335 & 0.303 & 0.282 & 0.269 \\\\ \\hline Only Attn. & 0.134 & 0.069 & 0.048 & 0.038 & 0.032 & 0.441 & 0.380 & 0.350 & 0.331 & 0.318 \\\\ \\hline Freeze MLP & 0.125 & 0.060 & 0.041 & 0.032 & 0.027 & 0.443 & 0.381 & 0.350 & 0.330 & 0.316 \\\\ \\hline Only MLP & 0.119 & 0.053 & 0.033 & 0.024 & 0.019 & 0.409 & 0.341 & 0.308 & 0.287 & 0.273 \\\\ \\hline \\hline Lora & 0.094 & 0.033 & 0.017 & 0.011 & 0.008 & 0.244 & 0.172 & 0.144 & 0.128 & 0.118 \\\\ \\hline Lora (3e-4) & 0.169 & 0.098 & 0.072 & 0.059 & 0.050 & 0.621 & 0.566 & 0.537 & 0.518 & 0.502 \\\\ \\hline \\((\\text{IA}^{3})\\) & 0.084 & 0.026 & 0.013 & 0.008 & 0.007 & 0.109 & 0.043 & 0.023 & 0.014 & 0.010 \\\\ \\hline \\((\\text{IA})^{3}\\) (3e-4) & 0.103 & 0.039 & 0.023 & 0.017 & 0.013 & 0.143 & 0.071 & 0.047 & 0.035 & 0.029 \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      'Table 7: Complete results of repetition experiments with prompts in two languages.\n' +
      '\n' +
      'long to the religious ideology subgroups Hindiusm and Atheism due to their limited number of prompts. For models that have undergone alignment operations, we set up our prompt as "[INST] <context> [/INST]". We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9. We use the Valence Aware Dictionary and Sentiment Reasoner (VADER) [14] to compute the sentiment score for the combined prompt and generation text. Additionally, we translate the model\'s output into English using the Google Translator API before employing VADER to calculate the sentiment score.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
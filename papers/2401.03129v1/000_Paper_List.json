{
    "2401.03129v1": {
        "paper_id": "2401.03129v1",
        "abs_url": "https://arxiv.org/abs/2401.03129v1",
        "pdf_url": "https://arxiv.org/pdf/2401.03129v1.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2401.03129v1_Examining_Forgetting_in_Continual_Pre-training_of_Aligned_Large_Language_Models.pdf",
        "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Chen-An Li",
            "Hung-Yi Lee"
        ],
        "abstract": "Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.",
        "comments": "Work in progress",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/examining-forgetting-in-continual-pre",
        "bibtex": "@misc{li2024examining,\n      title={Examining Forgetting in Continual Pre-training of Aligned Large Language Models}, \n      author={Chen-An Li and Hung-Yi Lee},\n      year={2024},\n      eprint={2401.03129},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
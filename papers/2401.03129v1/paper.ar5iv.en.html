<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.03129] Examining Forgetting in Continual Pre-training of Aligned Large Language Models</title><meta property="og:description" content="Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In develo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Examining Forgetting in Continual Pre-training of Aligned Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Examining Forgetting in Continual Pre-training of Aligned Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.03129">

<!--Generated on Tue Feb 27 10:39:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Examining Forgetting in Continual Pre-training of Aligned Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chen-An Li<sup id="id5.5.id1" class="ltx_sup"><span id="id5.5.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Hung-Yi Lee<sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">1</span></sup> 
<br class="ltx_break"><sup id="id7.7.id3" class="ltx_sup"><span id="id7.7.id3.1" class="ltx_text ltx_font_italic">1</span></sup>National Taiwan University, Taipei, Taiwan 
<br class="ltx_break"><sup id="id8.8.id4" class="ltx_sup"><span id="id8.8.id4.1" class="ltx_text ltx_font_italic">2</span></sup>ASUS Open Cloud Infrastructure Software Center, Taipei, Taiwan 
<br class="ltx_break"><span id="id9.9.id5" class="ltx_text ltx_font_typewriter">b08902123@csie.ntu.edu.tw</span>  <span id="id10.10.id6" class="ltx_text ltx_font_typewriter">hungyilee@ntu.edu.tw</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.4" class="ltx_block ltx_align_bottom">
<p id="p1.4.5" class="ltx_p"><span id="p1.4.5.1" class="ltx_text ltx_font_bold">Examining Forgetting in Continual Pre-training of Aligned Large Language Models</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.4.4" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.4.4.4" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.4.4.4.4" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.2.2.2.2.2" class="ltx_tr">
<span id="p1.2.2.2.2.2.2" class="ltx_td ltx_align_center"><span id="p1.2.2.2.2.2.2.2" class="ltx_text ltx_font_bold">Chen-An Li<sup id="p1.2.2.2.2.2.2.2.1" class="ltx_sup"><span id="p1.2.2.2.2.2.2.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>  Hung-Yi Lee<sup id="p1.2.2.2.2.2.2.2.2" class="ltx_sup"><span id="p1.2.2.2.2.2.2.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span></span></span>
<span id="p1.3.3.3.3.3" class="ltx_tr">
<span id="p1.3.3.3.3.3.1" class="ltx_td ltx_align_center"><sup id="p1.3.3.3.3.3.1.1" class="ltx_sup"><span id="p1.3.3.3.3.3.1.1.1" class="ltx_text ltx_font_italic">1</span></sup>National Taiwan University, Taipei, Taiwan</span></span>
<span id="p1.4.4.4.4.4" class="ltx_tr">
<span id="p1.4.4.4.4.4.1" class="ltx_td ltx_align_center"><sup id="p1.4.4.4.4.4.1.1" class="ltx_sup"><span id="p1.4.4.4.4.4.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>ASUS Open Cloud Infrastructure Software Center, Taipei, Taiwan</span></span>
<span id="p1.4.4.4.4.5.1" class="ltx_tr">
<span id="p1.4.4.4.4.5.1.1" class="ltx_td ltx_align_center"><span id="p1.4.4.4.4.5.1.1.1" class="ltx_text ltx_font_typewriter">b08902123@csie.ntu.edu.tw</span>  <span id="p1.4.4.4.4.5.1.1.2" class="ltx_text ltx_font_typewriter">hungyilee@ntu.edu.tw</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Large Language Models (LLMs) have demonstrated impressive performance across various tasks <cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>. There is an increasing trend of releasing pre-trained LLMs and fine-tuned variants <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a href="#bib.bib31" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib32" title="" class="ltx_ref">b</a>)</cite>. Many of these fine-tuned variants aim to augment the knowledge or linguistic capabilities of the existing LLM <cite class="ltx_cite ltx_citemacro_cite">Roziere et&nbsp;al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>); Cui et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We have noticed that many advancements in fine-tuned variants adhere to a conventional procedure consisting of two key steps: 1. Conduct further continual pre-training on an existing LLM. 2. Carry out subsequent alignment operations, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), on the model obtained in Step 1. Among these fine-tuned variants, many developments perform further continual pre-training on existing fine-tuned LLMs <cite class="ltx_cite ltx_citemacro_cite">Cui et&nbsp;al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>); Lin and Chen (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Previous studies have demonstrated that continual pre-training can significantly improve the model’s ability to understand and generate specific content <cite class="ltx_cite ltx_citemacro_cite">Gupta et&nbsp;al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>. However, continual pre-training could lead to catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite">French (<a href="#bib.bib6" title="" class="ltx_ref">1999</a>)</cite>, and limited research has explored the abilities forgotten during pre-training on an existing fine-tuned LLM.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Some works have studied continual learning for language models. <cite class="ltx_cite ltx_citemacro_cite">Qin et&nbsp;al. (<a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite> focused on efficient lifelong pre-training on pre-trained language models for emerging data. <cite class="ltx_cite ltx_citemacro_cite">Ke et&nbsp;al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> proposed a continual domain-adaptive pre-training method on a masked language model. <cite class="ltx_cite ltx_citemacro_cite">Song et&nbsp;al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> introduced continual parameter-efficient tuning for the ongoing adaptation of LLMs to continual tasks. <cite class="ltx_cite ltx_citemacro_cite">Xie et&nbsp;al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> investigate an alternative approach to continual pre-training for developing domain-specific LLMs. <cite class="ltx_cite ltx_citemacro_cite">Qi et&nbsp;al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite> suggests that fine-tuning compromises the safety alignment of LLMs. <cite class="ltx_cite ltx_citemacro_cite">Zhai et&nbsp;al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> evaluates the forgetting in fine-tuned multimodal LLMs.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our work examines the forgetting occurrence during continual pre-training on an existing fine-tuned LLM. Our paper primarily focuses on continual pre-training using the Traditional Chinese corpus. We evaluate the impact of continual pre-training across various dimensions, including output format, knowledge, and reliability. We show that more than straightforward methods are required for resolving this issue. Also, we observe an increased prominence of the repetition problem in models that tend to generate Traditional Chinese outputs. Lastly, despite continual pre-training, our findings suggest that the model’s knowledge remains unaffected while its reliability declines.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Observation of Catastrophic Forgetting during Continual Pre-training</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Settings for Observation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We conduct pre-training on Llama-2-7b-chat, a model comprising approximately 7 billion parameters that have undergone sequential alignment operations, including SFT and RLHF. Our pre-training process utilizes the 1 billion tokens of Traditional Chinese data. We denote the model after continual pre-training as Llama-2-7b-chat-cp. We employ specific prompts to observe the differences between the outputs generated by the two models.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of models’ outputs. The translation of our prompt is “Tell me something about Mexico City.”</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Observation of Catastrophic Forgetting</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Figure&nbsp;<a href="#S2.F1" title="Figure 1 ‣ 2.1 Settings for Observation ‣ 2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results obtained from our prompt. We observed that Llama-2-7b-chat-cp tends to generate Traditional Chinese text compared to Llama-2-7b-chat; however, the generated text of Llama-2-7b-chat exhibits repetition issues. Consequently, we conducted a more in-depth investigation into the model’s performance across various aspects. Appendix&nbsp;<a href="#A1" title="Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> contains additional results of more prompts.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Straightforward Approaches</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section introduces straightforward approaches to solving the catastrophic forgetting issues discussed in the previous section.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Freeze layers</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Previous studies have shown that distinct functionality exists in different layers of Transformer-based models when processing textual information <cite class="ltx_cite ltx_citemacro_cite">Ethayarajh (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>); Van&nbsp;Aken et&nbsp;al. (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite>. Consequently, we experiment with freezing specific layers of the model during continual pre-training. Specifically, we explore freezing the first ten layers and freezing the last ten layers, denoted as <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span> and <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span>, respectively.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Freeze modules</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We also conduct experiments by freezing specific modules of the model during continual pre-training. We aim to explore whether these designated modules preserve the abilities acquired during the alignment operations. We explore four strategies:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span>: Freeze the self-attention modules in each layer of the model.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span>: Freeze all modules in each layer except the self-attention modules of the model.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span>: Freeze the feed-forward modules in each model layer.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span>: Freeze all modules in each layer except the feed-forward modules of the model.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Adapter</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Adapters are frequently employed in training Transformer-based models <cite class="ltx_cite ltx_citemacro_cite">Houlsby et&nbsp;al. (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>. In our study, we experiment with two types of adapters.</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">Lora</span> <cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>: A method that incorporates trainable low-rank decomposition matrices into each layer of the Transformer-based model. In our implementation, we selectively adapt only the query and value projection matrices of each layer in the model.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S3.I2.i2.p1.1.1.1" class="ltx_sup"><span id="S3.I2.i2.p1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>: A technique involving element-wise multiplication of the model’s activations with a learned vector. We rescale the key and value matrices in self-attention modules and the inner activations in feed-forward modules in each model layer.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We employed straightforward approaches for continual pre-training on Llama-2-7b-chat, utilizing the 1 billion tokens of data from general Traditional Chinese corpus. The learning rate during continual pre-training remained constant at 3e-5, and we experimented with an additional learning rate 3e-4 for the adapter approaches. More details can be found in Appendix&nbsp;<a href="#A2" title="Appendix B Additional Details about Experiment Setup ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Tasks</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our study comprehensively examines our model’s performance across dimensions such as output format, knowledge, and reliability. Please refer to Appendix&nbsp;<a href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> for additional details.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Output format</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We perform two distinct tasks in output format analysis: language identification and repetition analysis. To conduct these evaluations, we randomly selected 2000 aligned sentences from the English and Traditional Chinese subset of <span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">NeuLab-TedTalks</span> <cite class="ltx_cite ltx_citemacro_cite">Qi et&nbsp;al. (<a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite> as our prompts.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Language identification: We employ the FastText <cite class="ltx_cite ltx_citemacro_cite">Joulin et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2016a</a>, <a href="#bib.bib17" title="" class="ltx_ref">b</a>)</cite> language identification model to detect the language of the generated tokens.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.4" class="ltx_p">Repetition: We determine the proportion of duplicated n-gram tokens at the BPE level in the combined output and prompt. This calculation involves the formula: rep-n <math id="S4.I1.i2.p1.1.m1.1" class="ltx_math_unparsed" alttext="=1-|" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mrow id="S4.I1.i2.p1.1.m1.1b"><mo id="S4.I1.i2.p1.1.m1.1.1">=</mo><mn id="S4.I1.i2.p1.1.m1.1.2">1</mn><mo rspace="0em" id="S4.I1.i2.p1.1.m1.1.3">−</mo><mo fence="false" stretchy="false" id="S4.I1.i2.p1.1.m1.1.4">|</mo></mrow><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">=1-|</annotation></semantics></math>unique n-grams<math id="S4.I1.i2.p1.2.m2.1" class="ltx_math_unparsed" alttext="|/|" display="inline"><semantics id="S4.I1.i2.p1.2.m2.1a"><mrow id="S4.I1.i2.p1.2.m2.1b"><mo fence="false" stretchy="false" id="S4.I1.i2.p1.2.m2.1.1">|</mo><mo lspace="0em" rspace="0em" id="S4.I1.i2.p1.2.m2.1.2">/</mo><mo fence="false" stretchy="false" id="S4.I1.i2.p1.2.m2.1.3">|</mo></mrow><annotation encoding="application/x-tex" id="S4.I1.i2.p1.2.m2.1c">|/|</annotation></semantics></math>n-grams<math id="S4.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S4.I1.i2.p1.3.m3.1a"><mo fence="false" stretchy="false" id="S4.I1.i2.p1.3.m3.1.1" xref="S4.I1.i2.p1.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.3.m3.1b"><ci id="S4.I1.i2.p1.3.m3.1.1.cmml" xref="S4.I1.i2.p1.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.3.m3.1c">|</annotation></semantics></math>, where n <math id="S4.I1.i2.p1.4.m4.5" class="ltx_Math" alttext="\in[4,8,12,16,20]" display="inline"><semantics id="S4.I1.i2.p1.4.m4.5a"><mrow id="S4.I1.i2.p1.4.m4.5.6" xref="S4.I1.i2.p1.4.m4.5.6.cmml"><mi id="S4.I1.i2.p1.4.m4.5.6.2" xref="S4.I1.i2.p1.4.m4.5.6.2.cmml"></mi><mo id="S4.I1.i2.p1.4.m4.5.6.1" xref="S4.I1.i2.p1.4.m4.5.6.1.cmml">∈</mo><mrow id="S4.I1.i2.p1.4.m4.5.6.3.2" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml"><mo stretchy="false" id="S4.I1.i2.p1.4.m4.5.6.3.2.1" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">[</mo><mn id="S4.I1.i2.p1.4.m4.1.1" xref="S4.I1.i2.p1.4.m4.1.1.cmml">4</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.2" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.2.2" xref="S4.I1.i2.p1.4.m4.2.2.cmml">8</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.3" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.3.3" xref="S4.I1.i2.p1.4.m4.3.3.cmml">12</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.4" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.4.4" xref="S4.I1.i2.p1.4.m4.4.4.cmml">16</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.5" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.5.5" xref="S4.I1.i2.p1.4.m4.5.5.cmml">20</mn><mo stretchy="false" id="S4.I1.i2.p1.4.m4.5.6.3.2.6" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.4.m4.5b"><apply id="S4.I1.i2.p1.4.m4.5.6.cmml" xref="S4.I1.i2.p1.4.m4.5.6"><in id="S4.I1.i2.p1.4.m4.5.6.1.cmml" xref="S4.I1.i2.p1.4.m4.5.6.1"></in><csymbol cd="latexml" id="S4.I1.i2.p1.4.m4.5.6.2.cmml" xref="S4.I1.i2.p1.4.m4.5.6.2">absent</csymbol><list id="S4.I1.i2.p1.4.m4.5.6.3.1.cmml" xref="S4.I1.i2.p1.4.m4.5.6.3.2"><cn type="integer" id="S4.I1.i2.p1.4.m4.1.1.cmml" xref="S4.I1.i2.p1.4.m4.1.1">4</cn><cn type="integer" id="S4.I1.i2.p1.4.m4.2.2.cmml" xref="S4.I1.i2.p1.4.m4.2.2">8</cn><cn type="integer" id="S4.I1.i2.p1.4.m4.3.3.cmml" xref="S4.I1.i2.p1.4.m4.3.3">12</cn><cn type="integer" id="S4.I1.i2.p1.4.m4.4.4.cmml" xref="S4.I1.i2.p1.4.m4.4.4">16</cn><cn type="integer" id="S4.I1.i2.p1.4.m4.5.5.cmml" xref="S4.I1.i2.p1.4.m4.5.5">20</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.4.m4.5c">\in[4,8,12,16,20]</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Knowledge</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">In our knowledge analysis, we assess our model’s performance across four benchmarks: <span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ARC</span> <cite class="ltx_cite ltx_citemacro_cite">Clark et&nbsp;al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>, <span id="S4.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">Hellaswag</span> <cite class="ltx_cite ltx_citemacro_cite">Zellers et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite>, <span id="S4.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">MMLU</span> <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et&nbsp;al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, and <span id="S4.SS2.SSS2.p1.1.4" class="ltx_text ltx_font_typewriter ltx_font_bold">C-eval-tw</span>.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p"><span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ARC</span> and <span id="S4.SS2.SSS2.p2.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">Hellaswag</span> serve as English commonsense reasoning benchmarks, where we use length-normalized accuracy as our metric. For our English multitask benchmark, <span id="S4.SS2.SSS2.p2.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">MMLU</span>, and our Traditional Chinese multitask benchmark, <span id="S4.SS2.SSS2.p2.1.4" class="ltx_text ltx_font_typewriter ltx_font_bold">C-eval-tw</span>, we calculate metrics by averaging accuracy across individual tasks. The accuracy computation is based on selecting the option with the highest probabilities.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Reliability</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Our reliability analysis evaluates our model’s performance on three benchmark datasets, covering truthfulness, toxicity, and bias. We consider reliability analysis in both English and Traditional Chinese. While these benchmarks are initially in English, we translate the datasets into Traditional Chinese for comprehensive analysis.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">TruthfulQA</span> <cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>: The dataset utilized to measure the truthfulness of language models. The scoring mechanism involves a question and multiple true/false reference answers, where the score is determined by the normalized total probability assigned to the set of true answers.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ToxiGen</span> <cite class="ltx_cite ltx_citemacro_cite">Hartvigsen et&nbsp;al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>: The dataset we employed to detect the toxicity of language models. We utilize the default RoBERTa-based <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> ToxiGen classifier to identify toxic generations.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Bold</span> <cite class="ltx_cite ltx_citemacro_cite">Dhamala et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>: The dataset we utilize for bias analysis. We use the Valence Aware Dictionary and Sentiment Reasoner (VADER) <cite class="ltx_cite ltx_citemacro_cite">Hutto and Gilbert (<a href="#bib.bib15" title="" class="ltx_ref">2014</a>)</cite> to compute the sentiment score for the combined prompt and generation text. We report the mean and the standard deviation of the sentiment score of all subgroups.</p>
</div>
</li>
</ul>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:195.1pt;height:199.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.2pt,35.1pt) scale(0.740209259724579,0.740209259724579) ;">
<table id="S4.T1.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.3.1" class="ltx_tr">
<td id="S4.T1.2.2.3.1.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T1.2.2.3.1.2.1" class="ltx_text ltx_font_bold">EN prompt</span></td>
<td id="S4.T1.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T1.2.2.3.1.3.1" class="ltx_text ltx_font_bold">TW prompt</span></td>
</tr>
<tr id="S4.T1.2.2.4.2" class="ltx_tr">
<td id="S4.T1.2.2.4.2.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.4.2.2.1" class="ltx_text ltx_font_bold">EN %</span></td>
<td id="S4.T1.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.4.2.3.1" class="ltx_text ltx_font_bold">TW %</span></td>
<td id="S4.T1.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.4.2.4.1" class="ltx_text ltx_font_bold">EN %</span></td>
<td id="S4.T1.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.2.2.4.2.5.1" class="ltx_text ltx_font_bold">TW %</span></td>
</tr>
<tr id="S4.T1.2.2.5.3" class="ltx_tr">
<td id="S4.T1.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b</td>
<td id="S4.T1.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.75</td>
<td id="S4.T1.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.00</td>
<td id="S4.T1.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.10</td>
<td id="S4.T1.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t">79.45</td>
</tr>
<tr id="S4.T1.2.2.6.4" class="ltx_tr">
<td id="S4.T1.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="S4.T1.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100.00</td>
<td id="S4.T1.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.00</td>
<td id="S4.T1.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.00</td>
<td id="S4.T1.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t">0.95</td>
</tr>
<tr id="S4.T1.2.2.7.5" class="ltx_tr">
<td id="S4.T1.2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="S4.T1.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">99.55</td>
<td id="S4.T1.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.20</td>
<td id="S4.T1.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">16.00</td>
<td id="S4.T1.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_tt">83.50</td>
</tr>
<tr id="S4.T1.2.2.8.6" class="ltx_tr">
<td id="S4.T1.2.2.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.2.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="S4.T1.2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">99.65</td>
<td id="S4.T1.2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.30</td>
<td id="S4.T1.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10.15</td>
<td id="S4.T1.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_tt">89.20</td>
</tr>
<tr id="S4.T1.2.2.9.7" class="ltx_tr">
<td id="S4.T1.2.2.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.9.7.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="S4.T1.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.6</td>
<td id="S4.T1.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.15</td>
<td id="S4.T1.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.05</td>
<td id="S4.T1.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_t">76.25</td>
</tr>
<tr id="S4.T1.2.2.10.8" class="ltx_tr">
<td id="S4.T1.2.2.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.2.10.8.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="S4.T1.2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">99.75</td>
<td id="S4.T1.2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.15</td>
<td id="S4.T1.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">41.05</td>
<td id="S4.T1.2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_tt">58.50</td>
</tr>
<tr id="S4.T1.2.2.11.9" class="ltx_tr">
<td id="S4.T1.2.2.11.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.11.9.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="S4.T1.2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.60</td>
<td id="S4.T1.2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.20</td>
<td id="S4.T1.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37.45</td>
<td id="S4.T1.2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_t">61.95</td>
</tr>
<tr id="S4.T1.2.2.12.10" class="ltx_tr">
<td id="S4.T1.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.12.10.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="S4.T1.2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.65</td>
<td id="S4.T1.2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.15</td>
<td id="S4.T1.2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.50</td>
<td id="S4.T1.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_t">63.80</td>
</tr>
<tr id="S4.T1.2.2.13.11" class="ltx_tr">
<td id="S4.T1.2.2.13.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.13.11.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="S4.T1.2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.80</td>
<td id="S4.T1.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.10</td>
<td id="S4.T1.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.65</td>
<td id="S4.T1.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_t">58.60</td>
</tr>
<tr id="S4.T1.2.2.14.12" class="ltx_tr">
<td id="S4.T1.2.2.14.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="S4.T1.2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">99.95</td>
<td id="S4.T1.2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.00</td>
<td id="S4.T1.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">70.85</td>
<td id="S4.T1.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_tt">28.85</td>
</tr>
<tr id="S4.T1.2.2.15.13" class="ltx_tr">
<td id="S4.T1.2.2.15.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T1.2.2.15.13.1.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="S4.T1.2.2.15.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.50</td>
<td id="S4.T1.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.30</td>
<td id="S4.T1.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.25</td>
<td id="S4.T1.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_t">90.85</td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T1.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100.00</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.00</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">98.90</td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">1.10</td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T1.2.2.2.1.1.1" class="ltx_sup"><span id="S4.T1.2.2.2.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">100.00</td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.00</td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">95.85</td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">4.05</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The results of the language identification analysis. <span id="S4.T1.7.1" class="ltx_text ltx_font_bold">EN prompt</span> indicates the use of English prompts, and <span id="S4.T1.8.2" class="ltx_text ltx_font_bold">TW prompt</span> indicates the use of Chinese prompts. <span id="S4.T1.9.3" class="ltx_text ltx_font_bold">EN %</span> denotes the percentage of output identified as English, while <span id="S4.T1.10.4" class="ltx_text ltx_font_bold">TW %</span> denotes the percentage identified as Chinese.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:182.1pt;height:202.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.3pt,33.7pt) scale(0.750577457251184,0.750577457251184) ;">
<table id="S4.T2.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.3.1" class="ltx_tr">
<td id="S4.T2.2.2.3.1.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T2.2.2.3.1.2.1" class="ltx_text ltx_font_bold">EN prompt</span></td>
<td id="S4.T2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T2.2.2.3.1.3.1" class="ltx_text ltx_font_bold">TW prompt</span></td>
</tr>
<tr id="S4.T2.2.2.4.2" class="ltx_tr">
<td id="S4.T2.2.2.4.2.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.4.2.2.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="S4.T2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.4.2.3.1" class="ltx_text ltx_font_bold">rep-8</span></td>
<td id="S4.T2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.4.2.4.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="S4.T2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.4.2.5.1" class="ltx_text ltx_font_bold">rep-8</span></td>
</tr>
<tr id="S4.T2.2.2.5.3" class="ltx_tr">
<td id="S4.T2.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b</td>
<td id="S4.T2.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.843</td>
<td id="S4.T2.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.804</td>
<td id="S4.T2.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.796</td>
<td id="S4.T2.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_t">0.763</td>
</tr>
<tr id="S4.T2.2.2.6.4" class="ltx_tr">
<td id="S4.T2.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="S4.T2.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.080</td>
<td id="S4.T2.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.024</td>
<td id="S4.T2.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.103</td>
<td id="S4.T2.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t">0.039</td>
</tr>
<tr id="S4.T2.2.2.7.5" class="ltx_tr">
<td id="S4.T2.2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="S4.T2.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.137</td>
<td id="S4.T2.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.068</td>
<td id="S4.T2.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.552</td>
<td id="S4.T2.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_tt">0.491</td>
</tr>
<tr id="S4.T2.2.2.8.6" class="ltx_tr">
<td id="S4.T2.2.2.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.2.2.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="S4.T2.2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.135</td>
<td id="S4.T2.2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.068</td>
<td id="S4.T2.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.599</td>
<td id="S4.T2.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_tt">0.539</td>
</tr>
<tr id="S4.T2.2.2.9.7" class="ltx_tr">
<td id="S4.T2.2.2.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.9.7.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="S4.T2.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.131</td>
<td id="S4.T2.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.065</td>
<td id="S4.T2.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.524</td>
<td id="S4.T2.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_t">0.463</td>
</tr>
<tr id="S4.T2.2.2.10.8" class="ltx_tr">
<td id="S4.T2.2.2.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.2.2.10.8.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="S4.T2.2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.116</td>
<td id="S4.T2.2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.050</td>
<td id="S4.T2.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.401</td>
<td id="S4.T2.2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_tt">0.335</td>
</tr>
<tr id="S4.T2.2.2.11.9" class="ltx_tr">
<td id="S4.T2.2.2.11.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.11.9.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="S4.T2.2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.134</td>
<td id="S4.T2.2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.069</td>
<td id="S4.T2.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.441</td>
<td id="S4.T2.2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_t">0.380</td>
</tr>
<tr id="S4.T2.2.2.12.10" class="ltx_tr">
<td id="S4.T2.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.12.10.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="S4.T2.2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.125</td>
<td id="S4.T2.2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.060</td>
<td id="S4.T2.2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.443</td>
<td id="S4.T2.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_t">0.381</td>
</tr>
<tr id="S4.T2.2.2.13.11" class="ltx_tr">
<td id="S4.T2.2.2.13.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.2.2.13.11.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="S4.T2.2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.119</td>
<td id="S4.T2.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.053</td>
<td id="S4.T2.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.409</td>
<td id="S4.T2.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_t">0.341</td>
</tr>
<tr id="S4.T2.2.2.14.12" class="ltx_tr">
<td id="S4.T2.2.2.14.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="S4.T2.2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.094</td>
<td id="S4.T2.2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.033</td>
<td id="S4.T2.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.244</td>
<td id="S4.T2.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_tt">0.172</td>
</tr>
<tr id="S4.T2.2.2.15.13" class="ltx_tr">
<td id="S4.T2.2.2.15.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T2.2.2.15.13.1.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="S4.T2.2.2.15.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.169</td>
<td id="S4.T2.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.098</td>
<td id="S4.T2.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.621</td>
<td id="S4.T2.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_t">0.566</td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T2.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.084</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.026</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.109</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">0.043</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T2.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T2.2.2.2.1.1.1" class="ltx_sup"><span id="S4.T2.2.2.2.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.103</td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.039</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.143</td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.071</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of repetition experiments with prompts in two languages. Full results are available in the Appendix <a href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results and Analysis</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Output Format</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.2" class="ltx_p">We aim to investigate the impact of continual pre-training on Chinese corpus on the language outputs of the models. Table&nbsp;<a href="#S4.T1" title="Table 1 ‣ 4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the results of the language identification experiment. We observe that when using English prompts, nearly every model tends to generate output in English. When provided with a Chinese prompt, we observed that Llama-2-7b tends to output in Chinese, whereas Llama-2-7b-chat tends to output in English. Furthermore, with Chinese prompts, the <span id="S4.SS3.SSS1.p1.2.2" class="ltx_text ltx_font_smallcaps">Freeze First 10 Layers</span> model tends to yield a higher proportion of Chinese text output than the <span id="S4.SS3.SSS1.p1.2.3" class="ltx_text ltx_font_smallcaps">Freeze Last 10 Layers</span> model. Models with frozen modules show relatively similar results, with approximately <math id="S4.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="60\%" display="inline"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mrow id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p1.1.m1.1.1.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml">60</mn><mo id="S4.SS3.SSS1.p1.1.m1.1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">60\%</annotation></semantics></math> of their output being in Chinese. In the case of adapters, increasing the learning rate can lead the <span id="S4.SS3.SSS1.p1.2.4" class="ltx_text ltx_font_smallcaps">Lora</span> model to produce more Chinese output, while the <span id="S4.SS3.SSS1.p1.2.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.SS3.SSS1.p1.2.1.1" class="ltx_sup"><span id="S4.SS3.SSS1.p1.2.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> model tends to favor English output.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">Table&nbsp;<a href="#S4.T2" title="Table 2 ‣ 4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> showcases the results of the repetition analysis experiment. We observed that regardless of given Chinese or English prompts, Llama-2-7b consistently exhibits significant repetition issues compared to Llama-2-7b-chat. Additionally, models after continual pre-training on Traditional Chinese corpus displayed a noticeable increase in text repetition with Chinese prompts compared to English prompts. Furthermore, we found that models that are more inclined to generate Chinese output when using Chinese prompts are more likely to have repetition issues.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Knowledge</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Table&nbsp;<a href="#S4.T3" title="Table 3 ‣ 4.3.2 Knowledge ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows our knowledge analysis experiments’ results.
Llama-2-7b-chat performs similarly to Llama-2-7b on <span id="S4.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Hellaswag</span> and <span id="S4.SS3.SSS2.p1.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">MMLU</span>, while showing a slightly better performance on <span id="S4.SS3.SSS2.p1.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">ARC</span> and <span id="S4.SS3.SSS2.p1.1.4" class="ltx_text ltx_font_typewriter ltx_font_bold">C-eval-tw</span>. In the <span id="S4.SS3.SSS2.p1.1.5" class="ltx_text ltx_font_typewriter ltx_font_bold">ARC</span> and <span id="S4.SS3.SSS2.p1.1.6" class="ltx_text ltx_font_typewriter ltx_font_bold">Hellaswag</span> benchmarks, almost all continually pre-trained models outperform Llama-2-7b-chat. In the <span id="S4.SS3.SSS2.p1.1.7" class="ltx_text ltx_font_typewriter ltx_font_bold">MMLU</span> benchmark, most continual pre-trained models tend to outperform Llama-2-7b-chat. However, in the case of the <span id="S4.SS3.SSS2.p1.1.8" class="ltx_text ltx_font_typewriter ltx_font_bold">C-eval-tw</span> benchmark, there is no clear pattern when comparing the efficacy of models utilizing simple methods for continual pre-training against Llama-2-7b-chat. It is worth noting that the observed differences mentioned above are subtle.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:201.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.8pt,34.2pt) scale(0.746404351114294,0.746404351114294) ;">
<table id="S4.T3.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.2.3.1" class="ltx_tr">
<td id="S4.T3.2.2.3.1.1" class="ltx_td ltx_border_r ltx_border_t" rowspan="2"></td>
<td id="S4.T3.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.3.1.2.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ARC</span></td>
<td id="S4.T3.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.3.1.3.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Hellaswag</span></td>
<td id="S4.T3.2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.3.1.4.1" class="ltx_text ltx_font_typewriter ltx_font_bold">MMLU</span></td>
<td id="S4.T3.2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.3.1.5.1" class="ltx_text ltx_font_typewriter ltx_font_bold">C-eval-tw</span></td>
</tr>
<tr id="S4.T3.2.2.4.2" class="ltx_tr">
<td id="S4.T3.2.2.4.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.4.2.1.1" class="ltx_text ltx_font_bold">ACC</span></td>
<td id="S4.T3.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.4.2.2.1" class="ltx_text ltx_font_bold">ACC</span></td>
<td id="S4.T3.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.4.2.3.1" class="ltx_text ltx_font_bold">ACC</span></td>
<td id="S4.T3.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.2.4.2.4.1" class="ltx_text ltx_font_bold">ACC</span></td>
</tr>
<tr id="S4.T3.2.2.5.3" class="ltx_tr">
<td id="S4.T3.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b</td>
<td id="S4.T3.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">53.0</td>
<td id="S4.T3.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">78.6</td>
<td id="S4.T3.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">46.5</td>
<td id="S4.T3.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_tt">32.2</td>
</tr>
<tr id="S4.T3.2.2.6.4" class="ltx_tr">
<td id="S4.T3.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="S4.T3.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.6</td>
<td id="S4.T3.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.6</td>
<td id="S4.T3.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.6</td>
<td id="S4.T3.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t">32.9</td>
</tr>
<tr id="S4.T3.2.2.7.5" class="ltx_tr">
<td id="S4.T3.2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="S4.T3.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">52.0</td>
<td id="S4.T3.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.6</td>
<td id="S4.T3.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">49.1</td>
<td id="S4.T3.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_tt">33.4</td>
</tr>
<tr id="S4.T3.2.2.8.6" class="ltx_tr">
<td id="S4.T3.2.2.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.2.2.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="S4.T3.2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">51.0</td>
<td id="S4.T3.2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.7</td>
<td id="S4.T3.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">49.1</td>
<td id="S4.T3.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_tt">31.9</td>
</tr>
<tr id="S4.T3.2.2.9.7" class="ltx_tr">
<td id="S4.T3.2.2.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.9.7.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="S4.T3.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">51.5</td>
<td id="S4.T3.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.6</td>
<td id="S4.T3.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.4</td>
<td id="S4.T3.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_t">33.5</td>
</tr>
<tr id="S4.T3.2.2.10.8" class="ltx_tr">
<td id="S4.T3.2.2.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.2.2.10.8.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="S4.T3.2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">51.9</td>
<td id="S4.T3.2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">77.7</td>
<td id="S4.T3.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">48.9</td>
<td id="S4.T3.2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_tt">32.2</td>
</tr>
<tr id="S4.T3.2.2.11.9" class="ltx_tr">
<td id="S4.T3.2.2.11.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.11.9.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="S4.T3.2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.8</td>
<td id="S4.T3.2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.0</td>
<td id="S4.T3.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.4</td>
<td id="S4.T3.2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_t">33.3</td>
</tr>
<tr id="S4.T3.2.2.12.10" class="ltx_tr">
<td id="S4.T3.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.12.10.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="S4.T3.2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.2</td>
<td id="S4.T3.2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.8</td>
<td id="S4.T3.2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.4</td>
<td id="S4.T3.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_t">32.6</td>
</tr>
<tr id="S4.T3.2.2.13.11" class="ltx_tr">
<td id="S4.T3.2.2.13.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.2.2.13.11.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="S4.T3.2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.0</td>
<td id="S4.T3.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.9</td>
<td id="S4.T3.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.9</td>
<td id="S4.T3.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_t">33.4</td>
</tr>
<tr id="S4.T3.2.2.14.12" class="ltx_tr">
<td id="S4.T3.2.2.14.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T3.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="S4.T3.2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">53.5</td>
<td id="S4.T3.2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">78.6</td>
<td id="S4.T3.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">47.1</td>
<td id="S4.T3.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_tt">33.8</td>
</tr>
<tr id="S4.T3.2.2.15.13" class="ltx_tr">
<td id="S4.T3.2.2.15.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T3.2.2.15.13.1.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="S4.T3.2.2.15.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.8</td>
<td id="S4.T3.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.2</td>
<td id="S4.T3.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.4</td>
<td id="S4.T3.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_t">33.0</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T3.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.7</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.9</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.0</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">32.6</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T3.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T3.2.2.2.1.1.1" class="ltx_sup"><span id="S4.T3.2.2.2.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">53.8</td>
<td id="S4.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">77.3</td>
<td id="S4.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">46.2</td>
<td id="S4.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">31.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Knowledge analysis experiment results with four benchmarks.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Reliability</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">In Table&nbsp;<a href="#S4.T4" title="Table 4 ‣ 4.3.3 Reliability ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we present the results of the reliability experiment. Llama-2-7b-chat consistently outperforms Llama-2-7b on the truthfulness and toxicity benchmarks. Notably, after continual pre-training, the models demonstrate inferior performance compared to Llama-2-7b-chat on the two benchmarks. This trend is particularly pronounced in the truthfulness analysis benchmark for English and the toxicity benchmark for Traditional Chinese. Furthermore, we observed that models with a preference for generating Chinese output exhibit inferior performance in the toxicity benchmark. Regarding the bias benchmark, we can observe that Llama-2-7b-chat outputs more positive text than Llama-2-7b. After continual pre-training, the models’ outputs have relatively more negative sentiment scores than Llama-2-7b-chat.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.28" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:200.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.4pt,44.0pt) scale(0.694692668749019,0.694692668749019) ;">
<table id="S4.T4.28.28" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.28.28.29.1" class="ltx_tr">
<td id="S4.T4.28.28.29.1.1" class="ltx_td ltx_border_r ltx_border_t" rowspan="3"></td>
<td id="S4.T4.28.28.29.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T4.28.28.29.1.2.1" class="ltx_text ltx_font_typewriter ltx_font_bold">TruthfulQA</span></td>
<td id="S4.T4.28.28.29.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T4.28.28.29.1.3.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ToxiGen</span></td>
<td id="S4.T4.28.28.29.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T4.28.28.29.1.4.1" class="ltx_text ltx_font_typewriter ltx_font_bold">BOLD</span></td>
</tr>
<tr id="S4.T4.28.28.30.2" class="ltx_tr">
<td id="S4.T4.28.28.30.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T4.28.28.30.2.1.1" class="ltx_text ltx_font_bold">mc2 ↑</span></td>
<td id="S4.T4.28.28.30.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S4.T4.28.28.30.2.2.1" class="ltx_text ltx_font_bold">toxicity ↓</span></td>
<td id="S4.T4.28.28.30.2.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S4.T4.28.28.30.2.3.1" class="ltx_text ltx_font_bold">sentiment</span></td>
</tr>
<tr id="S4.T4.28.28.31.3" class="ltx_tr">
<td id="S4.T4.28.28.31.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.1.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T4.28.28.31.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.2.1" class="ltx_text ltx_font_bold">TW</span></td>
<td id="S4.T4.28.28.31.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.3.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T4.28.28.31.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.4.1" class="ltx_text ltx_font_bold">TW</span></td>
<td id="S4.T4.28.28.31.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.28.28.31.3.5.1" class="ltx_text ltx_font_bold">EN</span></td>
<td id="S4.T4.28.28.31.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.28.28.31.3.6.1" class="ltx_text ltx_font_bold">TW</span></td>
</tr>
<tr id="S4.T4.2.2.2" class="ltx_tr">
<td id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b</td>
<td id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">39.0</td>
<td id="S4.T4.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.9</td>
<td id="S4.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.30</td>
<td id="S4.T4.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.80</td>
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.41<math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\pm</annotation></semantics></math>0.17</td>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">0.23<math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\pm</annotation></semantics></math>0.13</td>
</tr>
<tr id="S4.T4.4.4.4" class="ltx_tr">
<td id="S4.T4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="S4.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.6</td>
<td id="S4.T4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.7</td>
<td id="S4.T4.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.03</td>
<td id="S4.T4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.22</td>
<td id="S4.T4.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.66<math id="S4.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.3.3.3.1.m1.1a"><mo id="S4.T4.3.3.3.1.m1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">\pm</annotation></semantics></math>0.24</td>
<td id="S4.T4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t">0.69<math id="S4.T4.4.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.4.4.4.2.m1.1a"><mo id="S4.T4.4.4.4.2.m1.1.1" xref="S4.T4.4.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S4.T4.4.4.4.2.m1.1.1.cmml" xref="S4.T4.4.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.2.m1.1c">\pm</annotation></semantics></math>0.19</td>
</tr>
<tr id="S4.T4.6.6.6" class="ltx_tr">
<td id="S4.T4.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="S4.T4.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">40.2</td>
<td id="S4.T4.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">48.5</td>
<td id="S4.T4.6.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.05</td>
<td id="S4.T4.6.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">5.74</td>
<td id="S4.T4.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.52<math id="S4.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.5.5.5.1.m1.1a"><mo id="S4.T4.5.5.5.1.m1.1.1" xref="S4.T4.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T4.5.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.1.m1.1c">\pm</annotation></semantics></math>0.20</td>
<td id="S4.T4.6.6.6.2" class="ltx_td ltx_align_center ltx_border_tt">0.34<math id="S4.T4.6.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.6.6.6.2.m1.1a"><mo id="S4.T4.6.6.6.2.m1.1.1" xref="S4.T4.6.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.2.m1.1b"><csymbol cd="latexml" id="S4.T4.6.6.6.2.m1.1.1.cmml" xref="S4.T4.6.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.2.m1.1c">\pm</annotation></semantics></math>0.14</td>
</tr>
<tr id="S4.T4.8.8.8" class="ltx_tr">
<td id="S4.T4.8.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T4.8.8.8.3.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="S4.T4.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">41.7</td>
<td id="S4.T4.8.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">48.5</td>
<td id="S4.T4.8.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.08</td>
<td id="S4.T4.8.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">7.12</td>
<td id="S4.T4.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.55<math id="S4.T4.7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.7.7.7.1.m1.1a"><mo id="S4.T4.7.7.7.1.m1.1.1" xref="S4.T4.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T4.7.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.1.m1.1c">\pm</annotation></semantics></math>0.22</td>
<td id="S4.T4.8.8.8.2" class="ltx_td ltx_align_center ltx_border_tt">0.34<math id="S4.T4.8.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.8.8.8.2.m1.1a"><mo id="S4.T4.8.8.8.2.m1.1.1" xref="S4.T4.8.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S4.T4.8.8.8.2.m1.1.1.cmml" xref="S4.T4.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.2.m1.1c">\pm</annotation></semantics></math>0.12</td>
</tr>
<tr id="S4.T4.10.10.10" class="ltx_tr">
<td id="S4.T4.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.10.10.10.3.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="S4.T4.10.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.4</td>
<td id="S4.T4.10.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.8</td>
<td id="S4.T4.10.10.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.01</td>
<td id="S4.T4.10.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.69</td>
<td id="S4.T4.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.58<math id="S4.T4.9.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.9.9.9.1.m1.1a"><mo id="S4.T4.9.9.9.1.m1.1.1" xref="S4.T4.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T4.9.9.9.1.m1.1.1.cmml" xref="S4.T4.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.1.m1.1c">\pm</annotation></semantics></math>0.21</td>
<td id="S4.T4.10.10.10.2" class="ltx_td ltx_align_center ltx_border_t">0.37<math id="S4.T4.10.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.10.10.10.2.m1.1a"><mo id="S4.T4.10.10.10.2.m1.1.1" xref="S4.T4.10.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.2.m1.1b"><csymbol cd="latexml" id="S4.T4.10.10.10.2.m1.1.1.cmml" xref="S4.T4.10.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.2.m1.1c">\pm</annotation></semantics></math>0.15</td>
</tr>
<tr id="S4.T4.12.12.12" class="ltx_tr">
<td id="S4.T4.12.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.12.12.12.3.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="S4.T4.12.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.6</td>
<td id="S4.T4.12.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.8</td>
<td id="S4.T4.12.12.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.04</td>
<td id="S4.T4.12.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.15</td>
<td id="S4.T4.11.11.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.57<math id="S4.T4.11.11.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.11.11.11.1.m1.1a"><mo id="S4.T4.11.11.11.1.m1.1.1" xref="S4.T4.11.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.11.1.m1.1b"><csymbol cd="latexml" id="S4.T4.11.11.11.1.m1.1.1.cmml" xref="S4.T4.11.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.11.1.m1.1c">\pm</annotation></semantics></math>0.21</td>
<td id="S4.T4.12.12.12.2" class="ltx_td ltx_align_center ltx_border_t">0.42<math id="S4.T4.12.12.12.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.12.12.12.2.m1.1a"><mo id="S4.T4.12.12.12.2.m1.1.1" xref="S4.T4.12.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.2.m1.1b"><csymbol cd="latexml" id="S4.T4.12.12.12.2.m1.1.1.cmml" xref="S4.T4.12.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.2.m1.1c">\pm</annotation></semantics></math>0.16</td>
</tr>
<tr id="S4.T4.14.14.14" class="ltx_tr">
<td id="S4.T4.14.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.14.14.14.3.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="S4.T4.14.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.8</td>
<td id="S4.T4.14.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.6</td>
<td id="S4.T4.14.14.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.04</td>
<td id="S4.T4.14.14.14.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.27</td>
<td id="S4.T4.13.13.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.59<math id="S4.T4.13.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.13.13.13.1.m1.1a"><mo id="S4.T4.13.13.13.1.m1.1.1" xref="S4.T4.13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.1.m1.1b"><csymbol cd="latexml" id="S4.T4.13.13.13.1.m1.1.1.cmml" xref="S4.T4.13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.1.m1.1c">\pm</annotation></semantics></math>0.24</td>
<td id="S4.T4.14.14.14.2" class="ltx_td ltx_align_center ltx_border_t">0.43<math id="S4.T4.14.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.14.14.14.2.m1.1a"><mo id="S4.T4.14.14.14.2.m1.1.1" xref="S4.T4.14.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.14.2.m1.1b"><csymbol cd="latexml" id="S4.T4.14.14.14.2.m1.1.1.cmml" xref="S4.T4.14.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.14.2.m1.1c">\pm</annotation></semantics></math>0.15</td>
</tr>
<tr id="S4.T4.16.16.16" class="ltx_tr">
<td id="S4.T4.16.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.16.16.16.3.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="S4.T4.16.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.9</td>
<td id="S4.T4.16.16.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.8</td>
<td id="S4.T4.16.16.16.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0</td>
<td id="S4.T4.16.16.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.31</td>
<td id="S4.T4.15.15.15.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.60<math id="S4.T4.15.15.15.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.15.15.15.1.m1.1a"><mo id="S4.T4.15.15.15.1.m1.1.1" xref="S4.T4.15.15.15.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.15.1.m1.1b"><csymbol cd="latexml" id="S4.T4.15.15.15.1.m1.1.1.cmml" xref="S4.T4.15.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.15.1.m1.1c">\pm</annotation></semantics></math>0.22</td>
<td id="S4.T4.16.16.16.2" class="ltx_td ltx_align_center ltx_border_t">0.42<math id="S4.T4.16.16.16.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.16.16.16.2.m1.1a"><mo id="S4.T4.16.16.16.2.m1.1.1" xref="S4.T4.16.16.16.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.16.16.16.2.m1.1b"><csymbol cd="latexml" id="S4.T4.16.16.16.2.m1.1.1.cmml" xref="S4.T4.16.16.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.16.16.2.m1.1c">\pm</annotation></semantics></math>0.14</td>
</tr>
<tr id="S4.T4.18.18.18" class="ltx_tr">
<td id="S4.T4.18.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.18.18.18.3.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="S4.T4.18.18.18.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41.3</td>
<td id="S4.T4.18.18.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.8</td>
<td id="S4.T4.18.18.18.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.04</td>
<td id="S4.T4.18.18.18.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.39</td>
<td id="S4.T4.17.17.17.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.58<math id="S4.T4.17.17.17.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.17.17.17.1.m1.1a"><mo id="S4.T4.17.17.17.1.m1.1.1" xref="S4.T4.17.17.17.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.17.17.17.1.m1.1b"><csymbol cd="latexml" id="S4.T4.17.17.17.1.m1.1.1.cmml" xref="S4.T4.17.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.17.17.17.1.m1.1c">\pm</annotation></semantics></math>0.21</td>
<td id="S4.T4.18.18.18.2" class="ltx_td ltx_align_center ltx_border_t">0.43<math id="S4.T4.18.18.18.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.18.18.18.2.m1.1a"><mo id="S4.T4.18.18.18.2.m1.1.1" xref="S4.T4.18.18.18.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.18.18.18.2.m1.1b"><csymbol cd="latexml" id="S4.T4.18.18.18.2.m1.1.1.cmml" xref="S4.T4.18.18.18.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.18.18.18.2.m1.1c">\pm</annotation></semantics></math>0.16</td>
</tr>
<tr id="S4.T4.20.20.20" class="ltx_tr">
<td id="S4.T4.20.20.20.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T4.20.20.20.3.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="S4.T4.20.20.20.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">43.6</td>
<td id="S4.T4.20.20.20.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">49.1</td>
<td id="S4.T4.20.20.20.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.03</td>
<td id="S4.T4.20.20.20.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.79</td>
<td id="S4.T4.19.19.19.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.64<math id="S4.T4.19.19.19.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.19.19.19.1.m1.1a"><mo id="S4.T4.19.19.19.1.m1.1.1" xref="S4.T4.19.19.19.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.19.19.19.1.m1.1b"><csymbol cd="latexml" id="S4.T4.19.19.19.1.m1.1.1.cmml" xref="S4.T4.19.19.19.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.19.19.19.1.m1.1c">\pm</annotation></semantics></math>0.22</td>
<td id="S4.T4.20.20.20.2" class="ltx_td ltx_align_center ltx_border_tt">0.63<math id="S4.T4.20.20.20.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.20.20.20.2.m1.1a"><mo id="S4.T4.20.20.20.2.m1.1.1" xref="S4.T4.20.20.20.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.20.20.20.2.m1.1b"><csymbol cd="latexml" id="S4.T4.20.20.20.2.m1.1.1.cmml" xref="S4.T4.20.20.20.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.20.20.20.2.m1.1c">\pm</annotation></semantics></math>0.17</td>
</tr>
<tr id="S4.T4.22.22.22" class="ltx_tr">
<td id="S4.T4.22.22.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T4.22.22.22.3.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="S4.T4.22.22.22.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">42.5</td>
<td id="S4.T4.22.22.22.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">48.9</td>
<td id="S4.T4.22.22.22.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.07</td>
<td id="S4.T4.22.22.22.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.97</td>
<td id="S4.T4.21.21.21.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.57<math id="S4.T4.21.21.21.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.21.21.21.1.m1.1a"><mo id="S4.T4.21.21.21.1.m1.1.1" xref="S4.T4.21.21.21.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.21.21.21.1.m1.1b"><csymbol cd="latexml" id="S4.T4.21.21.21.1.m1.1.1.cmml" xref="S4.T4.21.21.21.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.21.21.21.1.m1.1c">\pm</annotation></semantics></math>0.22</td>
<td id="S4.T4.22.22.22.2" class="ltx_td ltx_align_center ltx_border_t">0.35<math id="S4.T4.22.22.22.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.22.22.22.2.m1.1a"><mo id="S4.T4.22.22.22.2.m1.1.1" xref="S4.T4.22.22.22.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.22.22.22.2.m1.1b"><csymbol cd="latexml" id="S4.T4.22.22.22.2.m1.1.1.cmml" xref="S4.T4.22.22.22.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.22.22.22.2.m1.1c">\pm</annotation></semantics></math>0.10</td>
</tr>
<tr id="S4.T4.25.25.25" class="ltx_tr">
<td id="S4.T4.23.23.23.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T4.23.23.23.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T4.23.23.23.1.1.1" class="ltx_sup"><span id="S4.T4.23.23.23.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="S4.T4.25.25.25.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.2</td>
<td id="S4.T4.25.25.25.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.8</td>
<td id="S4.T4.25.25.25.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0</td>
<td id="S4.T4.25.25.25.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.17</td>
<td id="S4.T4.24.24.24.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.66<math id="S4.T4.24.24.24.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.24.24.24.2.m1.1a"><mo id="S4.T4.24.24.24.2.m1.1.1" xref="S4.T4.24.24.24.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.24.24.24.2.m1.1b"><csymbol cd="latexml" id="S4.T4.24.24.24.2.m1.1.1.cmml" xref="S4.T4.24.24.24.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.24.24.24.2.m1.1c">\pm</annotation></semantics></math>0.24</td>
<td id="S4.T4.25.25.25.3" class="ltx_td ltx_align_center ltx_border_t">0.69<math id="S4.T4.25.25.25.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.25.25.25.3.m1.1a"><mo id="S4.T4.25.25.25.3.m1.1.1" xref="S4.T4.25.25.25.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.25.25.25.3.m1.1b"><csymbol cd="latexml" id="S4.T4.25.25.25.3.m1.1.1.cmml" xref="S4.T4.25.25.25.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.25.25.25.3.m1.1c">\pm</annotation></semantics></math>0.19</td>
</tr>
<tr id="S4.T4.28.28.28" class="ltx_tr">
<td id="S4.T4.26.26.26.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T4.26.26.26.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="S4.T4.26.26.26.1.1.1" class="ltx_sup"><span id="S4.T4.26.26.26.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="S4.T4.28.28.28.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">43.0</td>
<td id="S4.T4.28.28.28.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">49.9</td>
<td id="S4.T4.28.28.28.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.0</td>
<td id="S4.T4.28.28.28.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.11</td>
<td id="S4.T4.27.27.27.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.66<math id="S4.T4.27.27.27.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.27.27.27.2.m1.1a"><mo id="S4.T4.27.27.27.2.m1.1.1" xref="S4.T4.27.27.27.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.27.27.27.2.m1.1b"><csymbol cd="latexml" id="S4.T4.27.27.27.2.m1.1.1.cmml" xref="S4.T4.27.27.27.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.27.27.27.2.m1.1c">\pm</annotation></semantics></math>0.23</td>
<td id="S4.T4.28.28.28.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.68<math id="S4.T4.28.28.28.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T4.28.28.28.3.m1.1a"><mo id="S4.T4.28.28.28.3.m1.1.1" xref="S4.T4.28.28.28.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.28.28.28.3.m1.1b"><csymbol cd="latexml" id="S4.T4.28.28.28.3.m1.1.1.cmml" xref="S4.T4.28.28.28.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.28.28.28.3.m1.1c">\pm</annotation></semantics></math>0.18</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Reliability analysis experiment results on three benchmarks, including truthfulness, bias, and toxicity aspects. <span id="S4.T4.31.1" class="ltx_text ltx_font_bold">EN</span> denotes the origin dataset in English, while <span id="S4.T4.32.2" class="ltx_text ltx_font_bold">TW</span> denotes the translated dataset in Traditional Chinese.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work shows that catastrophic forgetting during continual pre-training is a non-trivial challenge and cannot be resolved through straightforward methods. Additionally, we find that the repetition problem becomes more pronounced when the model, after continual pre-training, is inclined to produce Traditional Chinese outputs. Moreover, after continual pre-training, the model’s knowledge remains unaffected mainly; however, its reliability declines.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">One notable limitation arises from the resource-intensive nature of continual pre-training LLMs, making reproducing all the straightforward continual pre-training methods outlined in this work challenging. Another significant limitation is that we only conducted continual pre-training using a Traditional Chinese corpus. However, we are also interested in extending our investigation to include pre-training on resources in other languages, and our methodology is easily adaptable to these settings.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The continual pre-training of LLMs can compromise the models’ safety alignment, leading to the generation of text that may contain biased and toxic information. Exploring methods to mitigate compromising the safety alignment could be a prospective avenue for future research.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">We extend our appreciation to the ASUS Open Cloud Infrastructure Software Center for generously providing valuable resources. Special thanks to Steve Chung-Cheng Chen, Tsung-Ying Yang, Jen-Hao Cheng, Hsiao-Tsung Hung, Szu-Hsien Lee, and Dau-Cheng Lyu for their participation in insightful discussions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05457</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yiming Cui, Ziqing Yang, and Xin Yao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2304.08177" title="" class="ltx_ref ltx_href">Efficient and effective text encoding for chinese llama and alpaca</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08177</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhamala et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021.

</span>
<span class="ltx_bibblock">Bold: Dataset and metrics for measuring biases in open-ended language generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>, pages 862–872.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh (2019)</span>
<span class="ltx_bibblock">
Kawin Ethayarajh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1006" title="" class="ltx_ref ltx_href">How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 55–65, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">French (1999)</span>
<span class="ltx_bibblock">
Robert&nbsp;M French. 1999.

</span>
<span class="ltx_bibblock">Catastrophic forgetting in connectionist networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Trends in cognitive sciences</em>, 3(4):128–135.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5281/zenodo.5371628" title="" class="ltx_ref ltx_href">A framework for few-shot language model evaluation</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats&nbsp;Leon Richter, Quentin&nbsp;Gregory Anthony, Eugene Belilovsky, Irina Rish, and Timothée Lesort. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=pg7PUJe0Tl" title="" class="ltx_ref ltx_href">Continual pre-training of large language models: How to re-warm your model?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Workshop on Efficient Systems for Foundation Models @ ICML2023</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartvigsen et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.234" title="" class="ltx_ref ltx_href">ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3309–3326, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.03300</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseini et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Saghar Hosseini, Hamid Palangi, and Ahmed&nbsp;Hassan Awadallah. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.trustnlp-1.11" title="" class="ltx_ref ltx_href">An empirical study of metrics to measure representational harms in pre-trained language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)</em>, pages 121–134, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De&nbsp;Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 2790–2799. PMLR.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Edward&nbsp;J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu&nbsp;Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_href">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.08322</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutto and Gilbert (2014)</span>
<span class="ltx_bibblock">
Clayton Hutto and Eric Gilbert. 2014.

</span>
<span class="ltx_bibblock">Vader: A parsimonious rule-based model for sentiment analysis of social media text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the international AAAI conference on web and social media</em>, volume&nbsp;8, pages 216–225.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016a)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016a.

</span>
<span class="ltx_bibblock">Fasttext.zip: Compressing text classification models.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1612.03651</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016b)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016b.

</span>
<span class="ltx_bibblock">Bag of tricks for efficient text classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1607.01759</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2022.

</span>
<span class="ltx_bibblock">Continual pre-training of language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.229" title="" class="ltx_ref ltx_href">TruthfulQA: Measuring how models mimic human falsehoods</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Chen (2023)</span>
<span class="ltx_bibblock">
Yen-Ting Lin and Yun-Nung Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2311.17487" title="" class="ltx_ref ltx_href">Taiwan llm: Bridging the linguistic divide with a culturally aligned language model</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin&nbsp;A Raffel. 2022.

</span>
<span class="ltx_bibblock">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 35:1950–1965.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xiangyu Qi, Yi&nbsp;Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023.

</span>
<span class="ltx_bibblock">Fine-tuning aligned language models compromises safety, even when users do not intend to!

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03693</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Ye&nbsp;Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N18-2084" title="" class="ltx_ref ltx_href">When and why are pre-trained word embeddings useful for neural machine translation?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, pages 529–535, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-acl.220" title="" class="ltx_ref ltx_href">ELLE: Efficient lifelong pre-training for emerging data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 2789–2810, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasley et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.

</span>
<span class="ltx_bibblock">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, pages 3505–3506.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roziere et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing&nbsp;Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.12950</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Chenyang Song, Xu&nbsp;Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun, and Tao Yang. 2023.

</span>
<span class="ltx_bibblock">Conpet: Continual parameter-efficient tuning for large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.14763</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in opus.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12)</em>, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van&nbsp;Aken et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Betty Van&nbsp;Aken, Benjamin Winter, Alexander Löser, and Felix&nbsp;A Gers. 2019.

</span>
<span class="ltx_bibblock">How does bert answer questions? a layer-wise analysis of transformer representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM international conference on information and knowledge management</em>, pages 1823–1832.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc&nbsp;V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et&nbsp;al. 2016.

</span>
<span class="ltx_bibblock">Google’s neural machine translation system: Bridging the gap between human and machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.08144</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 2023.

</span>
<span class="ltx_bibblock">Efficient continual pre-training for building domain specific large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.08545</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1472" title="" class="ltx_ref ltx_href">HellaSwag: Can a machine really finish your sentence?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4791–4800, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu&nbsp;Cai, Qing Qu, Yong&nbsp;Jae Lee, and Yi&nbsp;Ma. 2023.

</span>
<span class="ltx_bibblock">Investigating the catastrophic forgetting in multimodal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.10313</em>.

</span>
</li>
</ul>
</section>
<figure id="A0.F2" class="ltx_figure"><img src="/html/2401.03129/assets/x2.png" id="A0.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="820" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of the models outputs.</figcaption>
</figure>
<figure id="A0.F3" class="ltx_figure"><img src="/html/2401.03129/assets/x3.png" id="A0.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="492" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of models’ outputs. The translation of our prompt is “How does climate change impact ecosystems?”</figcaption>
</figure>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompting Results</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We employ vLLM <cite class="ltx_cite ltx_citemacro_cite">Kwon et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> to optimize efficiency, configuring the model with a max_tokens setting of 256. We utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9. Observing Figure &nbsp;<a href="#A0.F2" title="Figure 2 ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Figure &nbsp;<a href="#A0.F2" title="Figure 2 ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, it becomes apparent that when employing Chinese prompts, Llama-2-7b-chat-cp exhibits more repetition issues than Llama-2-7b-chat.</p>
</div>
<figure id="A1.T5" class="ltx_table">
<div id="A1.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:143pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.6pt,18.5pt) scale(0.794559754759643,0.794559754759643) ;">
<table id="A1.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T5.1.1.2.1" class="ltx_tr">
<th id="A1.T5.1.1.2.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="A1.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="A1.T5.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Trainable params</span></th>
<th id="A1.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="A1.T5.1.1.2.1.3.1" class="ltx_text ltx_font_bold">All params</span></th>
<th id="A1.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A1.T5.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Trainable %</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T5.1.1.3.1" class="ltx_tr">
<td id="A1.T5.1.1.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat-cp</td>
<td id="A1.T5.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">100.000</td>
</tr>
<tr id="A1.T5.1.1.4.2" class="ltx_tr">
<td id="A1.T5.1.1.4.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.4.2.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="A1.T5.1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,714,582,016</td>
<td id="A1.T5.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_t">69.966</td>
</tr>
<tr id="A1.T5.1.1.5.3" class="ltx_tr">
<td id="A1.T5.1.1.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.5.3.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="A1.T5.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,714,582,016</td>
<td id="A1.T5.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_t">69.966</td>
</tr>
<tr id="A1.T5.1.1.6.4" class="ltx_tr">
<td id="A1.T5.1.1.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.6.4.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="A1.T5.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,590,931,968</td>
<td id="A1.T5.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">68.131</td>
</tr>
<tr id="A1.T5.1.1.7.5" class="ltx_tr">
<td id="A1.T5.1.1.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.7.5.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="A1.T5.1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,147,483,648</td>
<td id="A1.T5.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">31.869</td>
</tr>
<tr id="A1.T5.1.1.8.6" class="ltx_tr">
<td id="A1.T5.1.1.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="A1.T5.1.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,409,893,888</td>
<td id="A1.T5.1.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.8.6.4" class="ltx_td ltx_align_center ltx_border_t">35.764</td>
</tr>
<tr id="A1.T5.1.1.9.7" class="ltx_tr">
<td id="A1.T5.1.1.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.9.7.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="A1.T5.1.1.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,328,521,728</td>
<td id="A1.T5.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,738,415,616</td>
<td id="A1.T5.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_t">64.236</td>
</tr>
<tr id="A1.T5.1.1.10.8" class="ltx_tr">
<td id="A1.T5.1.1.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T5.1.1.10.8.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="A1.T5.1.1.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4,194,304</td>
<td id="A1.T5.1.1.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,742,609,920</td>
<td id="A1.T5.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_t">0.062</td>
</tr>
<tr id="A1.T5.1.1.1" class="ltx_tr">
<td id="A1.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="A1.T5.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="A1.T5.1.1.1.1.1.1" class="ltx_sup"><span id="A1.T5.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="A1.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">614,400</td>
<td id="A1.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6,739,030,016</td>
<td id="A1.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.009</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Trainable parameters for various straightforward approaches.</figcaption>
</figure>
<figure id="A1.T6" class="ltx_table">
<div id="A1.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:173.4pt;height:32.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.3pt,1.9pt) scale(0.894174964941106,0.894174964941106) ;">
<table id="A1.T6.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T6.1.1.1.1" class="ltx_tr">
<td id="A1.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="A1.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.2.1" class="ltx_text ltx_font_bold">rep-8</span></td>
<td id="A1.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.3.1" class="ltx_text ltx_font_bold">rep-12</span></td>
<td id="A1.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.4.1" class="ltx_text ltx_font_bold">rep-16</span></td>
<td id="A1.T6.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T6.1.1.1.1.5.1" class="ltx_text ltx_font_bold">rep-20</span></td>
</tr>
<tr id="A1.T6.1.1.2.2" class="ltx_tr">
<td id="A1.T6.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">0.141</td>
<td id="A1.T6.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.056</td>
<td id="A1.T6.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.037</td>
<td id="A1.T6.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.030</td>
<td id="A1.T6.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.025</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>The proportion of duplicated n-gram tokens of our Traditional Chinese corpus.</figcaption>
</figure>
<figure id="A1.T7" class="ltx_table">
<div id="A1.T7.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:210.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-53.1pt,30.2pt) scale(0.776273750636431,0.776273750636431) ;">
<table id="A1.T7.2.2" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T7.2.2.3.1" class="ltx_tr">
<td id="A1.T7.2.2.3.1.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="A1.T7.2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5"><span id="A1.T7.2.2.3.1.2.1" class="ltx_text ltx_font_bold">EN prompt</span></td>
<td id="A1.T7.2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="5"><span id="A1.T7.2.2.3.1.3.1" class="ltx_text ltx_font_bold">TW prompt</span></td>
</tr>
<tr id="A1.T7.2.2.4.2" class="ltx_tr">
<td id="A1.T7.2.2.4.2.1" class="ltx_td ltx_border_r"></td>
<td id="A1.T7.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.2.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="A1.T7.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.3.1" class="ltx_text ltx_font_bold">rep-8</span></td>
<td id="A1.T7.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.4.1" class="ltx_text ltx_font_bold">rep-12</span></td>
<td id="A1.T7.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.5.1" class="ltx_text ltx_font_bold">rep-16</span></td>
<td id="A1.T7.2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.6.1" class="ltx_text ltx_font_bold">rep-20</span></td>
<td id="A1.T7.2.2.4.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.7.1" class="ltx_text ltx_font_bold">rep-4</span></td>
<td id="A1.T7.2.2.4.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.8.1" class="ltx_text ltx_font_bold">rep-8</span></td>
<td id="A1.T7.2.2.4.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.9.1" class="ltx_text ltx_font_bold">rep-12</span></td>
<td id="A1.T7.2.2.4.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.4.2.10.1" class="ltx_text ltx_font_bold">rep-16</span></td>
<td id="A1.T7.2.2.4.2.11" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T7.2.2.4.2.11.1" class="ltx_text ltx_font_bold">rep-20</span></td>
</tr>
<tr id="A1.T7.2.2.5.3" class="ltx_tr">
<td id="A1.T7.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b</td>
<td id="A1.T7.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.843</td>
<td id="A1.T7.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.804</td>
<td id="A1.T7.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.778</td>
<td id="A1.T7.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.760</td>
<td id="A1.T7.2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.747</td>
<td id="A1.T7.2.2.5.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.796</td>
<td id="A1.T7.2.2.5.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.763</td>
<td id="A1.T7.2.2.5.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.743</td>
<td id="A1.T7.2.2.5.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.728</td>
<td id="A1.T7.2.2.5.3.11" class="ltx_td ltx_align_center ltx_border_t">0.716</td>
</tr>
<tr id="A1.T7.2.2.6.4" class="ltx_tr">
<td id="A1.T7.2.2.6.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Llama-2-7b-chat</td>
<td id="A1.T7.2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.080</td>
<td id="A1.T7.2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.024</td>
<td id="A1.T7.2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.012</td>
<td id="A1.T7.2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="A1.T7.2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.005</td>
<td id="A1.T7.2.2.6.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.103</td>
<td id="A1.T7.2.2.6.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.039</td>
<td id="A1.T7.2.2.6.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.020</td>
<td id="A1.T7.2.2.6.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.012</td>
<td id="A1.T7.2.2.6.4.11" class="ltx_td ltx_align_center ltx_border_t">0.008</td>
</tr>
<tr id="A1.T7.2.2.7.5" class="ltx_tr">
<td id="A1.T7.2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Llama-2-7b-chat-cp</td>
<td id="A1.T7.2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.137</td>
<td id="A1.T7.2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.068</td>
<td id="A1.T7.2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.046</td>
<td id="A1.T7.2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.035</td>
<td id="A1.T7.2.2.7.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.029</td>
<td id="A1.T7.2.2.7.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.552</td>
<td id="A1.T7.2.2.7.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.491</td>
<td id="A1.T7.2.2.7.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.459</td>
<td id="A1.T7.2.2.7.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.437</td>
<td id="A1.T7.2.2.7.5.11" class="ltx_td ltx_align_center ltx_border_tt">0.422</td>
</tr>
<tr id="A1.T7.2.2.8.6" class="ltx_tr">
<td id="A1.T7.2.2.8.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A1.T7.2.2.8.6.1.1" class="ltx_text ltx_font_smallcaps">Freeze First 10</span></td>
<td id="A1.T7.2.2.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.135</td>
<td id="A1.T7.2.2.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.068</td>
<td id="A1.T7.2.2.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.048</td>
<td id="A1.T7.2.2.8.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.038</td>
<td id="A1.T7.2.2.8.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.032</td>
<td id="A1.T7.2.2.8.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.599</td>
<td id="A1.T7.2.2.8.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.539</td>
<td id="A1.T7.2.2.8.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.506</td>
<td id="A1.T7.2.2.8.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.483</td>
<td id="A1.T7.2.2.8.6.11" class="ltx_td ltx_align_center ltx_border_tt">0.466</td>
</tr>
<tr id="A1.T7.2.2.9.7" class="ltx_tr">
<td id="A1.T7.2.2.9.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.9.7.1.1" class="ltx_text ltx_font_smallcaps">Freeze Last 10</span></td>
<td id="A1.T7.2.2.9.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.131</td>
<td id="A1.T7.2.2.9.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.065</td>
<td id="A1.T7.2.2.9.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.044</td>
<td id="A1.T7.2.2.9.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.034</td>
<td id="A1.T7.2.2.9.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.028</td>
<td id="A1.T7.2.2.9.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.524</td>
<td id="A1.T7.2.2.9.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.463</td>
<td id="A1.T7.2.2.9.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.432</td>
<td id="A1.T7.2.2.9.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.412</td>
<td id="A1.T7.2.2.9.7.11" class="ltx_td ltx_align_center ltx_border_t">0.397</td>
</tr>
<tr id="A1.T7.2.2.10.8" class="ltx_tr">
<td id="A1.T7.2.2.10.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A1.T7.2.2.10.8.1.1" class="ltx_text ltx_font_smallcaps">Freeze Attn.</span></td>
<td id="A1.T7.2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.116</td>
<td id="A1.T7.2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.050</td>
<td id="A1.T7.2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.031</td>
<td id="A1.T7.2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.023</td>
<td id="A1.T7.2.2.10.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.018</td>
<td id="A1.T7.2.2.10.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.401</td>
<td id="A1.T7.2.2.10.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.335</td>
<td id="A1.T7.2.2.10.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.303</td>
<td id="A1.T7.2.2.10.8.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.282</td>
<td id="A1.T7.2.2.10.8.11" class="ltx_td ltx_align_center ltx_border_tt">0.269</td>
</tr>
<tr id="A1.T7.2.2.11.9" class="ltx_tr">
<td id="A1.T7.2.2.11.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.11.9.1.1" class="ltx_text ltx_font_smallcaps">Only Attn.</span></td>
<td id="A1.T7.2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.134</td>
<td id="A1.T7.2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.069</td>
<td id="A1.T7.2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.048</td>
<td id="A1.T7.2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.038</td>
<td id="A1.T7.2.2.11.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.032</td>
<td id="A1.T7.2.2.11.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.441</td>
<td id="A1.T7.2.2.11.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.380</td>
<td id="A1.T7.2.2.11.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.350</td>
<td id="A1.T7.2.2.11.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.331</td>
<td id="A1.T7.2.2.11.9.11" class="ltx_td ltx_align_center ltx_border_t">0.318</td>
</tr>
<tr id="A1.T7.2.2.12.10" class="ltx_tr">
<td id="A1.T7.2.2.12.10.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.12.10.1.1" class="ltx_text ltx_font_smallcaps">Freeze MLP</span></td>
<td id="A1.T7.2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.125</td>
<td id="A1.T7.2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.060</td>
<td id="A1.T7.2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.041</td>
<td id="A1.T7.2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.032</td>
<td id="A1.T7.2.2.12.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.027</td>
<td id="A1.T7.2.2.12.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.443</td>
<td id="A1.T7.2.2.12.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.381</td>
<td id="A1.T7.2.2.12.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.350</td>
<td id="A1.T7.2.2.12.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.330</td>
<td id="A1.T7.2.2.12.10.11" class="ltx_td ltx_align_center ltx_border_t">0.316</td>
</tr>
<tr id="A1.T7.2.2.13.11" class="ltx_tr">
<td id="A1.T7.2.2.13.11.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.2.2.13.11.1.1" class="ltx_text ltx_font_smallcaps">Only MLP</span></td>
<td id="A1.T7.2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.119</td>
<td id="A1.T7.2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.053</td>
<td id="A1.T7.2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.033</td>
<td id="A1.T7.2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.024</td>
<td id="A1.T7.2.2.13.11.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.019</td>
<td id="A1.T7.2.2.13.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.409</td>
<td id="A1.T7.2.2.13.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.341</td>
<td id="A1.T7.2.2.13.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.308</td>
<td id="A1.T7.2.2.13.11.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.287</td>
<td id="A1.T7.2.2.13.11.11" class="ltx_td ltx_align_center ltx_border_t">0.273</td>
</tr>
<tr id="A1.T7.2.2.14.12" class="ltx_tr">
<td id="A1.T7.2.2.14.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="A1.T7.2.2.14.12.1.1" class="ltx_text ltx_font_smallcaps">Lora</span></td>
<td id="A1.T7.2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.094</td>
<td id="A1.T7.2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.033</td>
<td id="A1.T7.2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.017</td>
<td id="A1.T7.2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.011</td>
<td id="A1.T7.2.2.14.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.008</td>
<td id="A1.T7.2.2.14.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.244</td>
<td id="A1.T7.2.2.14.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.172</td>
<td id="A1.T7.2.2.14.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.144</td>
<td id="A1.T7.2.2.14.12.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">0.128</td>
<td id="A1.T7.2.2.14.12.11" class="ltx_td ltx_align_center ltx_border_tt">0.118</td>
</tr>
<tr id="A1.T7.2.2.15.13" class="ltx_tr">
<td id="A1.T7.2.2.15.13.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A1.T7.2.2.15.13.1.1" class="ltx_text ltx_font_smallcaps">Lora</span> (3e-4)</td>
<td id="A1.T7.2.2.15.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.169</td>
<td id="A1.T7.2.2.15.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.098</td>
<td id="A1.T7.2.2.15.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.072</td>
<td id="A1.T7.2.2.15.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.059</td>
<td id="A1.T7.2.2.15.13.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.050</td>
<td id="A1.T7.2.2.15.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.621</td>
<td id="A1.T7.2.2.15.13.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.566</td>
<td id="A1.T7.2.2.15.13.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.537</td>
<td id="A1.T7.2.2.15.13.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.518</td>
<td id="A1.T7.2.2.15.13.11" class="ltx_td ltx_align_center ltx_border_t">0.502</td>
</tr>
<tr id="A1.T7.1.1.1" class="ltx_tr">
<td id="A1.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T7.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="A1.T7.1.1.1.1.1.1" class="ltx_sup"><span id="A1.T7.1.1.1.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span></td>
<td id="A1.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.084</td>
<td id="A1.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.026</td>
<td id="A1.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.013</td>
<td id="A1.T7.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.008</td>
<td id="A1.T7.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="A1.T7.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.109</td>
<td id="A1.T7.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.043</td>
<td id="A1.T7.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.023</td>
<td id="A1.T7.1.1.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.014</td>
<td id="A1.T7.1.1.1.11" class="ltx_td ltx_align_center ltx_border_t">0.010</td>
</tr>
<tr id="A1.T7.2.2.2" class="ltx_tr">
<td id="A1.T7.2.2.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="A1.T7.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="A1.T7.2.2.2.1.1.1" class="ltx_sup"><span id="A1.T7.2.2.2.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> (3e-4)</td>
<td id="A1.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.103</td>
<td id="A1.T7.2.2.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.039</td>
<td id="A1.T7.2.2.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.023</td>
<td id="A1.T7.2.2.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.017</td>
<td id="A1.T7.2.2.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.013</td>
<td id="A1.T7.2.2.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.143</td>
<td id="A1.T7.2.2.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.071</td>
<td id="A1.T7.2.2.2.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.047</td>
<td id="A1.T7.2.2.2.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.035</td>
<td id="A1.T7.2.2.2.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.029</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Complete results of repetition experiments with prompts in two languages.</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Details about Experiment Setup</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Our source code is available at <a target="_blank" href="https://github.com/lca0503/Llama_tw" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/lca0503/Llama_tw</a>. We adopted straightforward approaches for continual pre-training on Llama-2-7b-chat<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/meta-llama/Llama-2-7b-chat-hf</a></span></span></span>, utilizing the 1 billion tokens of data from general Traditional Chinese corpus. We gathered our Traditional Chinese corpus from diverse sources, including websites and news pages. We utilize DeepSpeed <cite class="ltx_cite ltx_citemacro_cite">Rasley et&nbsp;al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> to improve memory efficiency during continual pre-training. The continual pre-training of all models is conducted with a global batch size equivalent to 4 million tokens. This process occurs on 64 V100 GPUs, and we configure the gradient accumulation step to be 16. The learning rate during continual pre-training remained constant at 3e-5, and we experimented with an additional learning rate 3e-4 for the adapter approaches. Details regarding the trainable parameters for various straightforward approaches can be found in Table <a href="#A1.T5" title="Table 5 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">Here, we delve into our adapter settings. For <span id="A2.p2.1.2" class="ltx_text ltx_font_smallcaps">Lora</span> <cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>)</cite>, we selectively adapt only the query and value projection matrices of each layer in the model. We set the network rank to 8 and the alpha to 32. In the case of <span id="A2.p2.1.1" class="ltx_text ltx_font_smallcaps">(Ia)<sup id="A2.p2.1.1.1" class="ltx_sup"><span id="A2.p2.1.1.1.1" class="ltx_text ltx_font_upright">3</span></sup></span> <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>, we rescale the key and value matrices in self-attention modules and the inner activations in feed-forward modules in each model layer via learned vectors. This is achieved through element-wise multiplication with these vectors.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Details about Experiment Tasks</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Output format Analysis</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">We perform two tasks in output format analysis: language identification and repetition analysis. We utilized vLLM <cite class="ltx_cite ltx_citemacro_cite">Kwon et&nbsp;al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> to enhance efficiency. Expressly, for models that have undergone alignment operations, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), we set up our prompt as “[INST] &lt;context&gt; [/INST]”. We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9.</p>
</div>
<div id="A3.SS1.p2" class="ltx_para">
<p id="A3.SS1.p2.1" class="ltx_p">To conduct output format analysis, we utilize the following dataset:</p>
<ul id="A3.I1" class="ltx_itemize">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p id="A3.I1.i1.p1.1" class="ltx_p"><span id="A3.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">NeuLab-TedTalks<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_serif ltx_font_medium">2</span></span><a target="_blank" href="https://opus.nlpl.eu/NeuLab-TedTalks-v1.php" title="" class="ltx_ref ltx_url ltx_font_medium">https://opus.nlpl.eu/NeuLab-TedTalks-v1.php</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Qi et&nbsp;al. (<a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite>: A common corpus of TED talks, translated into numerous low-resource languages by a global community of volunteers. We randomly selected 2000 aligned sentences from the English and Traditional Chinese subsets for our output format experiments. We download the corpus from OPUS <cite class="ltx_cite ltx_citemacro_cite">Tiedemann (<a href="#bib.bib30" title="" class="ltx_ref">2012</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="A3.SS1.p3" class="ltx_para">
<p id="A3.SS1.p3.1" class="ltx_p">For language identification analysis, we utilize the FastText <cite class="ltx_cite ltx_citemacro_cite">Joulin et&nbsp;al. (<a href="#bib.bib16" title="" class="ltx_ref">2016a</a>, <a href="#bib.bib17" title="" class="ltx_ref">b</a>)</cite> language identification model to detect the language of the generated tokens. As for repetition analysis, we assess the proportion of duplicated n-gram tokens at the BPE level within the combination of the generated output and the prompt.</p>
</div>
<div id="A3.SS1.p4" class="ltx_para">
<p id="A3.SS1.p4.1" class="ltx_p">Table <a href="#A1.T6" title="Table 6 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> presents the repetition statistics for our Traditional Chinese corpus, and Table <a href="#A1.T7" title="Table 7 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> presents the full results of the repetition analysis experiment. Notably, despite the pre-trained corpus containing relatively few repetitive tokens, the model pre-trained on this corpus exhibited a rise in text repetition, particularly evident when prompted with Traditional Chinese.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Knowledge Analysis</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">In our knowledge analysis, we assess our model’s performance across four benchmarks: <span id="A3.SS2.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ARC</span>, <span id="A3.SS2.p1.1.2" class="ltx_text ltx_font_typewriter ltx_font_bold">Hellaswag</span>, <span id="A3.SS2.p1.1.3" class="ltx_text ltx_font_typewriter ltx_font_bold">MMLU</span>, and <span id="A3.SS2.p1.1.4" class="ltx_text ltx_font_typewriter ltx_font_bold">C-eval-tw</span>. We employ EleutherAI/lm-evaluation-harness<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/EleutherAI/lm-evaluation-harness" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite> to assess the model performance on these benchmarks. These benchmarks consist of multiple-choice questions. The accuracy computation is based on selecting the option with the highest probabilities.</p>
</div>
<div id="A3.SS2.p2" class="ltx_para">
<ul id="A3.I2" class="ltx_itemize">
<li id="A3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i1.p1" class="ltx_para">
<p id="A3.I2.i1.p1.1" class="ltx_p"><span id="A3.I2.i1.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ARC<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span id="footnote4.1.1.1" class="ltx_text ltx_font_serif ltx_font_medium">4</span></span><a target="_blank" href="https://allenai.org/data/arc" title="" class="ltx_ref ltx_url ltx_font_medium">https://allenai.org/data/arc</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Clark et&nbsp;al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>: A collection of natural, grade-school science questions. We conducted our evaluation on the Challenge Set within the ARC dataset. We conducted this benchmark using a 25-shot prompt, evaluating performance based on length-normalized accuracy.</p>
</div>
</li>
<li id="A3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i2.p1" class="ltx_para">
<p id="A3.I2.i2.p1.1" class="ltx_p"><span id="A3.I2.i2.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Hellaswag<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span id="footnote5.1.1.1" class="ltx_text ltx_font_serif ltx_font_medium">5</span></span><a target="_blank" href="https://rowanzellers.com/hellaswag" title="" class="ltx_ref ltx_url ltx_font_medium">https://rowanzellers.com/hellaswag</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Zellers et&nbsp;al. (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>)</cite>: An evaluation of commonsense inference, presenting a task that is straightforward for humans but poses a challenge for state-of-the-art models. We conducted this benchmark using a 10-shot prompt, evaluating performance based on length-normalized accuracy.</p>
</div>
</li>
<li id="A3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i3.p1" class="ltx_para">
<p id="A3.I2.i3.p1.1" class="ltx_p"><span id="A3.I2.i3.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">MMLU<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span id="footnote6.1.1.1" class="ltx_text ltx_font_serif ltx_font_medium">6</span></span><a target="_blank" href="https://github.com/hendrycks/test" title="" class="ltx_ref ltx_url ltx_font_medium">https://github.com/hendrycks/test</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et&nbsp;al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>: A test for a text model’s multitasking accuracy, covering 57 tasks from elementary math to U.S. history, computer science, law, and beyond. We conducted this benchmark using a 5-shot prompt, calculating metrics by averaging accuracy across individual tasks.</p>
</div>
</li>
<li id="A3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i4.p1" class="ltx_para">
<p id="A3.I2.i4.p1.1" class="ltx_p"><span id="A3.I2.i4.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">C-eval-tw</span>: C-eval<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://cevalbenchmark.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cevalbenchmark.com</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Huang et&nbsp;al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> serves as a test to evaluate the advanced knowledge and reasoning abilities of foundational models in a Chinese context. The test was initially in Simplified Chinese, and we translated it into Traditional Chinese using the Google Translate <cite class="ltx_cite ltx_citemacro_cite">Wu et&nbsp;al. (<a href="#bib.bib34" title="" class="ltx_ref">2016</a>)</cite> API in the deep-translator<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://github.com/nidhaloff/deep-translator" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nidhaloff/deep-translator</a></span></span></span> package.
We conducted this benchmark using a 0-shot prompt, calculating metrics by averaging accuracy across individual tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Reliability Analysis</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p id="A3.SS3.p1.1" class="ltx_p">In our reliability analysis, we check the performance of our model across three benchmark datasets, including truthfulness, toxicity, and bias. We conduct this analysis in both English and Traditional Chinese. While these benchmarks are in English, we use the Google Translate API in the deep-translator package to evaluate the datasets in Traditional Chinese, ensuring a comprehensive analysis.</p>
</div>
<div id="A3.SS3.p2" class="ltx_para">
<ul id="A3.I3" class="ltx_itemize">
<li id="A3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I3.i1.p1" class="ltx_para">
<p id="A3.I3.i1.p1.1" class="ltx_p"><span id="A3.I3.i1.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">TruthfulQA<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span id="footnote9.1.1.1" class="ltx_text ltx_font_serif ltx_font_medium">9</span></span><a target="_blank" href="https://github.com/sylinrl/TruthfulQA" title="" class="ltx_ref ltx_url ltx_font_medium">https://github.com/sylinrl/TruthfulQA</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>: A dataset utilized to measure the truthfulness of language models. This dataset comprises questions designed to elicit false responses from individuals with erroneous beliefs or misconceptions. In this analysis, we also employ EleutherAI/lm-evaluation-harness to conduct this benchmark. We conduct the benchmark using a 6-shot prompt. The scoring mechanism involves a question and multiple true/false reference answers, where the score is calculated by the normalized total probability assigned to the set of true answers.</p>
</div>
</li>
<li id="A3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I3.i2.p1" class="ltx_para">
<p id="A3.I3.i2.p1.1" class="ltx_p"><span id="A3.I3.i2.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">ToxiGen<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span id="footnote10.1.1.1" class="ltx_text ltx_font_serif ltx_font_medium">10</span></span><a target="_blank" href="https://github.com/microsoft/TOXIGEN" title="" class="ltx_ref ltx_url ltx_font_medium">https://github.com/microsoft/TOXIGEN</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Hartvigsen et&nbsp;al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>: The dataset we employed to detect the toxicity of language models. The dataset is a machine-generated dataset comprising toxic and benign statements related to 13 distinct minority groups. We adopt a refined dataset<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a target="_blank" href="https://github.com/microsoft/SafeNLP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/microsoft/SafeNLP</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Hosseini et&nbsp;al. (<a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, which mitigates noise by excluding prompts where annotators disagree on the target demographic group. We take these statements as our prompts. We utilize vLLM to enhance efficiency. For models that have undergone alignment operations, we set up our prompt as “[INST] &lt;context&gt; [/INST]”. We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9. We utilize the default RoBERTa-based classifier ToxiGen <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> for identifying toxic generations. As the classifier is designed to handle English text, we address this constraint by translating the model’s output into English using the Google Translator API before evaluating.</p>
</div>
</li>
<li id="A3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I3.i3.p1" class="ltx_para">
<p id="A3.I3.i3.p1.1" class="ltx_p"><span id="A3.I3.i3.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Bold<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note"><span id="footnote12.1.1.1" class="ltx_text ltx_font_serif ltx_font_medium">12</span></span><a target="_blank" href="https://github.com/amazon-science/bold" title="" class="ltx_ref ltx_url ltx_font_medium">https://github.com/amazon-science/bold</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Dhamala et&nbsp;al. (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>: The dataset we utilize for bias analysis. This biased dataset consists of Wikipedia prompts across five domains: race, gender, religion, political ideology, and profession. We also utilize vLLM to enhance efficiency. We exclude prompts that belong to the religious ideology subgroups Hinduism and Atheism due to their limited number of prompts. For models that have undergone alignment operations, we set up our prompt as “[INST] &lt;context&gt; [/INST]”. We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9. We use the Valence Aware Dictionary and Sentiment Reasoner (VADER) <cite class="ltx_cite ltx_citemacro_cite">Hutto and Gilbert (<a href="#bib.bib15" title="" class="ltx_ref">2014</a>)</cite> to compute the sentiment score for the combined prompt and generation text. Additionally, we translate the model’s output into English using the Google Translator API before employing VADER to calculate the sentiment score.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.03128" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.03129" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2401.03129">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.03129" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.03130" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 10:39:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
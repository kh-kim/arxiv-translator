<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Examining Forgetting in Continual Pre-training of Aligned Large Language Models</title>
<!--Generated on Sat Jan  6 05:33:11 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.03129v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2401.03129v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2401.03129v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2401.03129v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Observation of Catastrophic Forgetting during Continual Pre-training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Settings for Observation ‣ 2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Settings for Observation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="2.2 Observation of Catastrophic Forgetting ‣ 2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Observation of Catastrophic Forgetting</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Straightforward Approaches ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Straightforward Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Freeze layers ‣ 3 Straightforward Approaches ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Freeze layers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="3.2 Freeze modules ‣ 3 Straightforward Approaches ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Freeze modules</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="3.3 Adapter ‣ 3 Straightforward Approaches ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Adapter</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="4.1 Setup ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS2" title="4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS1" title="4.2.1 Output format ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Output format</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS2" title="4.2.2 Knowledge ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS3" title="4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Reliability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS3" title="4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS3.SSS1" title="4.3.1 Output Format ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Output Format</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS3.SSS2" title="4.3.2 Knowledge ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS3.SSS3" title="4.3.3 Reliability ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Reliability</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5 Conclusion ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A1" title="Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Prompting Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A2" title="Appendix B Additional Details about Experiment Setup ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Details about Experiment Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Details about Experiment Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A3.SS1" title="C.1 Output format Analysis ‣ Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Output format Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A3.SS2" title="C.2 Knowledge Analysis ‣ Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Knowledge Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A3.SS3" title="C.3 Reliability Analysis ‣ Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Reliability Analysis</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="”Conversion" been="" class="package-alerts ltx_document" errors="" found”="" have="" role="“status”">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY-SA 4.0</div><div id="watermark-tr">arXiv:2401.03129v1 [cs.CL] 06 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Examining Forgetting in Continual Pre-training of Aligned Large Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chen-An Li<math alttext="{}^{1,2}" class="ltx_Math" display="inline" id="id1.1.m1.2"><semantics id="id1.1.m1.2a"><msup id="id1.1.m1.2.2" xref="id1.1.m1.2.2.cmml"><mi id="id1.1.m1.2.2a" xref="id1.1.m1.2.2.cmml"></mi><mrow id="id1.1.m1.2.2.2.4" xref="id1.1.m1.2.2.2.3.cmml"><mn id="id1.1.m1.1.1.1.1" xref="id1.1.m1.1.1.1.1.cmml">1</mn><mo id="id1.1.m1.2.2.2.4.1" xref="id1.1.m1.2.2.2.3.cmml">,</mo><mn id="id1.1.m1.2.2.2.2" xref="id1.1.m1.2.2.2.2.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.2b"><apply id="id1.1.m1.2.2.cmml" xref="id1.1.m1.2.2"><list id="id1.1.m1.2.2.2.3.cmml" xref="id1.1.m1.2.2.2.4"><cn id="id1.1.m1.1.1.1.1.cmml" type="integer" xref="id1.1.m1.1.1.1.1">1</cn><cn id="id1.1.m1.2.2.2.2.cmml" type="integer" xref="id1.1.m1.2.2.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.2c">{}^{1,2}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.2d">start_FLOATSUPERSCRIPT 1 , 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Hung-Yi Lee<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mn id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><cn id="id2.2.m2.1.1.1.cmml" type="integer" xref="id2.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mn id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><cn id="id3.3.m3.1.1.1.cmml" type="integer" xref="id3.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>National Taiwan University, Taipei, Taiwan 
<br class="ltx_break"><math alttext="{}^{2}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mn id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><cn id="id4.4.m4.1.1.1.cmml" type="integer" xref="id4.4.m4.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>ASUS Open Cloud Infrastructure Software Center, Taipei, Taiwan 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id5.5.id1">b08902123@csie.ntu.edu.tw</span>  <span class="ltx_text ltx_font_typewriter" id="id6.6.id2">hungyilee@ntu.edu.tw</span>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id7.id1">Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Examining Forgetting in Continual Pre-training of Aligned Large Language Models</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
<p class="ltx_p" id="p2.4"><span class="ltx_text" id="p2.4.4" style="width:433.6pt;"><span class="ltx_text" id="p2.4.4.4" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.4.4.4.4">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.2.2.2.2.2">
<span class="ltx_td ltx_align_center" id="p2.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="p2.2.2.2.2.2.2.2">Chen-An Li<math alttext="{}^{1,2}" class="ltx_Math" display="inline" id="p2.1.1.1.1.1.1.1.m1.2"><semantics id="p2.1.1.1.1.1.1.1.m1.2a"><msup id="p2.1.1.1.1.1.1.1.m1.2.2" xref="p2.1.1.1.1.1.1.1.m1.2.2.cmml"><mi id="p2.1.1.1.1.1.1.1.m1.2.2a" xref="p2.1.1.1.1.1.1.1.m1.2.2.cmml"></mi><mrow id="p2.1.1.1.1.1.1.1.m1.2.2.2.4" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.3.cmml"><mn id="p2.1.1.1.1.1.1.1.m1.1.1.1.1" mathvariant="normal" xref="p2.1.1.1.1.1.1.1.m1.1.1.1.1.cmml">1</mn><mo id="p2.1.1.1.1.1.1.1.m1.2.2.2.4.1" mathvariant="normal" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.3.cmml">,</mo><mn id="p2.1.1.1.1.1.1.1.m1.2.2.2.2" mathvariant="normal" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.2.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="p2.1.1.1.1.1.1.1.m1.2b"><apply id="p2.1.1.1.1.1.1.1.m1.2.2.cmml" xref="p2.1.1.1.1.1.1.1.m1.2.2"><list id="p2.1.1.1.1.1.1.1.m1.2.2.2.3.cmml" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.4"><cn id="p2.1.1.1.1.1.1.1.m1.1.1.1.1.cmml" type="integer" xref="p2.1.1.1.1.1.1.1.m1.1.1.1.1">1</cn><cn id="p2.1.1.1.1.1.1.1.m1.2.2.2.2.cmml" type="integer" xref="p2.1.1.1.1.1.1.1.m1.2.2.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.1.1.1.1.1.1.1.m1.2c">{}^{1,2}</annotation><annotation encoding="application/x-llamapun" id="p2.1.1.1.1.1.1.1.m1.2d">start_FLOATSUPERSCRIPT 1 , 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Hung-Yi Lee<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p2.2.2.2.2.2.2.2.m2.1"><semantics id="p2.2.2.2.2.2.2.2.m2.1a"><msup id="p2.2.2.2.2.2.2.2.m2.1.1" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"><mi id="p2.2.2.2.2.2.2.2.m2.1.1a" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"></mi><mn id="p2.2.2.2.2.2.2.2.m2.1.1.1" mathvariant="normal" xref="p2.2.2.2.2.2.2.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p2.2.2.2.2.2.2.2.m2.1b"><apply id="p2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="p2.2.2.2.2.2.2.2.m2.1.1"><cn id="p2.2.2.2.2.2.2.2.m2.1.1.1.cmml" type="integer" xref="p2.2.2.2.2.2.2.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.2.2.2.2.2.2.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p2.2.2.2.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p2.3.3.3.3.3">
<span class="ltx_td ltx_align_center" id="p2.3.3.3.3.3.1"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="p2.3.3.3.3.3.1.m1.1"><semantics id="p2.3.3.3.3.3.1.m1.1a"><msup id="p2.3.3.3.3.3.1.m1.1.1" xref="p2.3.3.3.3.3.1.m1.1.1.cmml"><mi id="p2.3.3.3.3.3.1.m1.1.1a" xref="p2.3.3.3.3.3.1.m1.1.1.cmml"></mi><mn id="p2.3.3.3.3.3.1.m1.1.1.1" xref="p2.3.3.3.3.3.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p2.3.3.3.3.3.1.m1.1b"><apply id="p2.3.3.3.3.3.1.m1.1.1.cmml" xref="p2.3.3.3.3.3.1.m1.1.1"><cn id="p2.3.3.3.3.3.1.m1.1.1.1.cmml" type="integer" xref="p2.3.3.3.3.3.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.3.3.3.3.3.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p2.3.3.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>National Taiwan University, Taipei, Taiwan</span></span>
<span class="ltx_tr" id="p2.4.4.4.4.4">
<span class="ltx_td ltx_align_center" id="p2.4.4.4.4.4.1"><math alttext="{}^{2}" class="ltx_Math" display="inline" id="p2.4.4.4.4.4.1.m1.1"><semantics id="p2.4.4.4.4.4.1.m1.1a"><msup id="p2.4.4.4.4.4.1.m1.1.1" xref="p2.4.4.4.4.4.1.m1.1.1.cmml"><mi id="p2.4.4.4.4.4.1.m1.1.1a" xref="p2.4.4.4.4.4.1.m1.1.1.cmml"></mi><mn id="p2.4.4.4.4.4.1.m1.1.1.1" xref="p2.4.4.4.4.4.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p2.4.4.4.4.4.1.m1.1b"><apply id="p2.4.4.4.4.4.1.m1.1.1.cmml" xref="p2.4.4.4.4.4.1.m1.1.1"><cn id="p2.4.4.4.4.4.1.m1.1.1.1.cmml" type="integer" xref="p2.4.4.4.4.4.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.4.4.4.4.4.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p2.4.4.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>ASUS Open Cloud Infrastructure Software Center, Taipei, Taiwan</span></span>
<span class="ltx_tr" id="p2.4.4.4.4.5.1">
<span class="ltx_td ltx_align_center" id="p2.4.4.4.4.5.1.1"><span class="ltx_text ltx_font_typewriter" id="p2.4.4.4.4.5.1.1.1">b08902123@csie.ntu.edu.tw</span>  <span class="ltx_text ltx_font_typewriter" id="p2.4.4.4.4.5.1.1.2">hungyilee@ntu.edu.tw</span></span></span>
</span>
</span></span> </span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) have demonstrated impressive performance across various tasks <cite class="ltx_cite ltx_citemacro_cite">Brown et&nbsp;al. (<a class="ltx_ref" href="#bib.bib1" title="">2020</a>)</cite>. There is an increasing trend of releasing pre-trained LLMs and fine-tuned variants <cite class="ltx_cite ltx_citemacro_cite">Touvron et&nbsp;al. (<a class="ltx_ref" href="#bib.bib31" title="">2023a</a>, <a class="ltx_ref" href="#bib.bib32" title="">b</a>)</cite>. Many of these fine-tuned variants aim to augment the knowledge or linguistic capabilities of the existing LLM <cite class="ltx_cite ltx_citemacro_cite">Roziere et&nbsp;al. (<a class="ltx_ref" href="#bib.bib28" title="">2023</a>); Cui et&nbsp;al. (<a class="ltx_ref" href="#bib.bib3" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">We have noticed that many advancements in fine-tuned variants adhere to a conventional procedure consisting of two key steps: 1. Conduct further continual pre-training on an existing LLM. 2. Carry out subsequent alignment operations, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), on the model obtained in Step 1. Among these fine-tuned variants, many developments perform further continual pre-training on existing fine-tuned LLMs <cite class="ltx_cite ltx_citemacro_cite">Cui et&nbsp;al. (<a class="ltx_ref" href="#bib.bib3" title="">2023</a>); Lin and Chen (<a class="ltx_ref" href="#bib.bib21" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Previous studies have demonstrated that continual pre-training can significantly improve the model’s ability to understand and generate specific content <cite class="ltx_cite ltx_citemacro_cite">Gupta et&nbsp;al. (<a class="ltx_ref" href="#bib.bib8" title="">2023</a>)</cite>. However, continual pre-training could lead to catastrophic forgetting <cite class="ltx_cite ltx_citemacro_cite">French (<a class="ltx_ref" href="#bib.bib6" title="">1999</a>)</cite>, and limited research has explored the abilities forgotten during pre-training on an existing fine-tuned LLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Some works have studied continual learning for language models. <cite class="ltx_cite ltx_citemacro_cite">Qin et&nbsp;al. (<a class="ltx_ref" href="#bib.bib26" title="">2022</a>)</cite> focused on efficient lifelong pre-training on pre-trained language models for emerging data. <cite class="ltx_cite ltx_citemacro_cite">Ke et&nbsp;al. (<a class="ltx_ref" href="#bib.bib18" title="">2022</a>)</cite> proposed a continual domain-adaptive pre-training method on a masked language model. <cite class="ltx_cite ltx_citemacro_cite">Song et&nbsp;al. (<a class="ltx_ref" href="#bib.bib29" title="">2023</a>)</cite> introduced continual parameter-efficient tuning for the ongoing adaptation of LLMs to continual tasks. <cite class="ltx_cite ltx_citemacro_cite">Xie et&nbsp;al. (<a class="ltx_ref" href="#bib.bib35" title="">2023</a>)</cite> investigate an alternative approach to continual pre-training for developing domain-specific LLMs. <cite class="ltx_cite ltx_citemacro_cite">Qi et&nbsp;al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite> suggests that fine-tuning compromises the safety alignment of LLMs. <cite class="ltx_cite ltx_citemacro_cite">Zhai et&nbsp;al. (<a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite> evaluates the forgetting in fine-tuned multimodal LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our work examines the forgetting occurrence during continual pre-training on an existing fine-tuned LLM. Our paper primarily focuses on continual pre-training using the Traditional Chinese corpus. We evaluate the impact of continual pre-training across various dimensions, including output format, knowledge, and reliability. We show that more than straightforward methods are required for resolving this issue. Also, we observe an increased prominence of the repetition problem in models that tend to generate Traditional Chinese outputs. Lastly, despite continual pre-training, our findings suggest that the model’s knowledge remains unaffected while its reliability declines.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Observation of Catastrophic Forgetting during Continual Pre-training</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Settings for Observation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We conduct pre-training on Llama-2-7b-chat, a model comprising approximately 7 billion parameters that have undergone sequential alignment operations, including SFT and RLHF. Our pre-training process utilizes the 1 billion tokens of Traditional Chinese data. We denote the model after continual pre-training as Llama-2-7b-chat-cp. We employ specific prompts to observe the differences between the outputs generated by the two models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="924" id="S2.F1.g1" src="x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of models’ outputs. The translation of our prompt is “Tell me something about Mexico City.”</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Observation of Catastrophic Forgetting</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Figure&nbsp;<a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 Settings for Observation ‣ 2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> shows the results obtained from our prompt. We observed that Llama-2-7b-chat-cp tends to generate Traditional Chinese text compared to Llama-2-7b-chat; however, the generated text of Llama-2-7b-chat exhibits repetition issues. Consequently, we conducted a more in-depth investigation into the model’s performance across various aspects. Appendix&nbsp;<a class="ltx_ref" href="#A1" title="Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">A</span></a> contains additional results of more prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Straightforward Approaches</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section introduces straightforward approaches to solving the catastrophic forgetting issues discussed in the previous section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Freeze layers</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Previous studies have shown that distinct functionality exists in different layers of Transformer-based models when processing textual information <cite class="ltx_cite ltx_citemacro_cite">Ethayarajh (<a class="ltx_ref" href="#bib.bib5" title="">2019</a>); Van&nbsp;Aken et&nbsp;al. (<a class="ltx_ref" href="#bib.bib33" title="">2019</a>)</cite>. Consequently, we experiment with freezing specific layers of the model during continual pre-training. Specifically, we explore freezing the first ten layers and freezing the last ten layers, denoted as <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.1">Freeze First 10</span> and <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.2">Freeze Last 10</span>, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Freeze modules</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We also conduct experiments by freezing specific modules of the model during continual pre-training. We aim to explore whether these designated modules preserve the abilities acquired during the alignment operations. We explore four strategies:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i1.p1.1.1">Freeze Attn.</span>: Freeze the self-attention modules in each layer of the model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i2.p1.1.1">Only Attn.</span>: Freeze all modules in each layer except the self-attention modules of the model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i3.p1.1.1">Freeze MLP</span>: Freeze the feed-forward modules in each model layer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i4.p1.1.1">Only MLP</span>: Freeze all modules in each layer except the feed-forward modules of the model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Adapter</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Adapters are frequently employed in training Transformer-based models <cite class="ltx_cite ltx_citemacro_cite">Houlsby et&nbsp;al. (<a class="ltx_ref" href="#bib.bib12" title="">2019</a>)</cite>. In our study, we experiment with two types of adapters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I2.i1.p1.1.1">Lora</span> <cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib13" title="">2022</a>)</cite>: A method that incorporates trainable low-rank decomposition matrices into each layer of the Transformer-based model. In our implementation, we selectively adapt only the query and value projection matrices of each layer in the model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I2.i2.p1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.1.m1.1"><semantics id="S3.I2.i2.p1.1.1.m1.1a"><msup id="S3.I2.i2.p1.1.1.m1.1.1" xref="S3.I2.i2.p1.1.1.m1.1.1.cmml"><mi id="S3.I2.i2.p1.1.1.m1.1.1a" xref="S3.I2.i2.p1.1.1.m1.1.1.cmml"></mi><mn id="S3.I2.i2.p1.1.1.m1.1.1.1" mathvariant="normal" xref="S3.I2.i2.p1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.1.m1.1b"><apply id="S3.I2.i2.p1.1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1"><cn id="S3.I2.i2.p1.1.1.m1.1.1.1.cmml" type="integer" xref="S3.I2.i2.p1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>: A technique involving element-wise multiplication of the model’s activations with a learned vector. We rescale the key and value matrices in self-attention modules and the inner activations in feed-forward modules in each model layer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We employed straightforward approaches for continual pre-training on Llama-2-7b-chat, utilizing the 1 billion tokens of data from general Traditional Chinese corpus. The learning rate during continual pre-training remained constant at 3e-5, and we experimented with an additional learning rate 3e-4 for the adapter approaches. More details can be found in Appendix&nbsp;<a class="ltx_ref" href="#A2" title="Appendix B Additional Details about Experiment Setup ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">B</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Tasks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Our study comprehensively examines our model’s performance across dimensions such as output format, knowledge, and reliability. Please refer to Appendix&nbsp;<a class="ltx_ref" href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">C</span></a> for additional details.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Output format</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We perform two distinct tasks in output format analysis: language identification and repetition analysis. To conduct these evaluations, we randomly selected 2000 aligned sentences from the English and Traditional Chinese subset of <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS1.p1.1.1">NeuLab-TedTalks</span> <cite class="ltx_cite ltx_citemacro_cite">Qi et&nbsp;al. (<a class="ltx_ref" href="#bib.bib25" title="">2018</a>)</cite> as our prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Language identification: We employ the FastText <cite class="ltx_cite ltx_citemacro_cite">Joulin et&nbsp;al. (<a class="ltx_ref" href="#bib.bib16" title="">2016a</a>, <a class="ltx_ref" href="#bib.bib17" title="">b</a>)</cite> language identification model to detect the language of the generated tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.4">Repetition: We determine the proportion of duplicated n-gram tokens at the BPE level in the combined output and prompt. This calculation involves the formula: rep-n <math alttext="=1-|" class="ltx_math_unparsed" display="inline" id="S4.I1.i2.p1.1.m1.1"><semantics id="S4.I1.i2.p1.1.m1.1a"><mrow id="S4.I1.i2.p1.1.m1.1b"><mo id="S4.I1.i2.p1.1.m1.1.1">=</mo><mn id="S4.I1.i2.p1.1.m1.1.2">1</mn><mo id="S4.I1.i2.p1.1.m1.1.3" rspace="0em">−</mo><mo fence="false" id="S4.I1.i2.p1.1.m1.1.4" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">=1-|</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.m1.1d">= 1 - |</annotation></semantics></math>unique n-grams<math alttext="|/|" class="ltx_math_unparsed" display="inline" id="S4.I1.i2.p1.2.m2.1"><semantics id="S4.I1.i2.p1.2.m2.1a"><mrow id="S4.I1.i2.p1.2.m2.1b"><mo fence="false" id="S4.I1.i2.p1.2.m2.1.1" stretchy="false">|</mo><mo id="S4.I1.i2.p1.2.m2.1.2" lspace="0em" rspace="0em">/</mo><mo fence="false" id="S4.I1.i2.p1.2.m2.1.3" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex" id="S4.I1.i2.p1.2.m2.1c">|/|</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.2.m2.1d">| / |</annotation></semantics></math>n-grams<math alttext="|" class="ltx_Math" display="inline" id="S4.I1.i2.p1.3.m3.1"><semantics id="S4.I1.i2.p1.3.m3.1a"><mo fence="false" id="S4.I1.i2.p1.3.m3.1.1" stretchy="false" xref="S4.I1.i2.p1.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.3.m3.1b"><ci id="S4.I1.i2.p1.3.m3.1.1.cmml" xref="S4.I1.i2.p1.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.3.m3.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.3.m3.1d">|</annotation></semantics></math>, where n <math alttext="\in[4,8,12,16,20]" class="ltx_Math" display="inline" id="S4.I1.i2.p1.4.m4.5"><semantics id="S4.I1.i2.p1.4.m4.5a"><mrow id="S4.I1.i2.p1.4.m4.5.6" xref="S4.I1.i2.p1.4.m4.5.6.cmml"><mi id="S4.I1.i2.p1.4.m4.5.6.2" xref="S4.I1.i2.p1.4.m4.5.6.2.cmml"></mi><mo id="S4.I1.i2.p1.4.m4.5.6.1" xref="S4.I1.i2.p1.4.m4.5.6.1.cmml">∈</mo><mrow id="S4.I1.i2.p1.4.m4.5.6.3.2" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml"><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.1" stretchy="false" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">[</mo><mn id="S4.I1.i2.p1.4.m4.1.1" xref="S4.I1.i2.p1.4.m4.1.1.cmml">4</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.2" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.2.2" xref="S4.I1.i2.p1.4.m4.2.2.cmml">8</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.3" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.3.3" xref="S4.I1.i2.p1.4.m4.3.3.cmml">12</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.4" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.4.4" xref="S4.I1.i2.p1.4.m4.4.4.cmml">16</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.5" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.5.5" xref="S4.I1.i2.p1.4.m4.5.5.cmml">20</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.6" stretchy="false" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.4.m4.5b"><apply id="S4.I1.i2.p1.4.m4.5.6.cmml" xref="S4.I1.i2.p1.4.m4.5.6"><in id="S4.I1.i2.p1.4.m4.5.6.1.cmml" xref="S4.I1.i2.p1.4.m4.5.6.1"></in><csymbol cd="latexml" id="S4.I1.i2.p1.4.m4.5.6.2.cmml" xref="S4.I1.i2.p1.4.m4.5.6.2">absent</csymbol><list id="S4.I1.i2.p1.4.m4.5.6.3.1.cmml" xref="S4.I1.i2.p1.4.m4.5.6.3.2"><cn id="S4.I1.i2.p1.4.m4.1.1.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.1.1">4</cn><cn id="S4.I1.i2.p1.4.m4.2.2.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.2.2">8</cn><cn id="S4.I1.i2.p1.4.m4.3.3.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.3.3">12</cn><cn id="S4.I1.i2.p1.4.m4.4.4.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.4.4">16</cn><cn id="S4.I1.i2.p1.4.m4.5.5.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.5.5">20</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.4.m4.5c">\in[4,8,12,16,20]</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.4.m4.5d">∈ [ 4 , 8 , 12 , 16 , 20 ]</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Knowledge</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">In our knowledge analysis, we assess our model’s performance across four benchmarks: <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.1">ARC</span> <cite class="ltx_cite ltx_citemacro_cite">Clark et&nbsp;al. (<a class="ltx_ref" href="#bib.bib2" title="">2018</a>)</cite>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.2">Hellaswag</span> <cite class="ltx_cite ltx_citemacro_cite">Zellers et&nbsp;al. (<a class="ltx_ref" href="#bib.bib36" title="">2019</a>)</cite>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.3">MMLU</span> <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et&nbsp;al. (<a class="ltx_ref" href="#bib.bib10" title="">2020</a>)</cite>, and <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.4">C-eval-tw</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.1">ARC</span> and <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.2">Hellaswag</span> serve as English commonsense reasoning benchmarks, where we use length-normalized accuracy as our metric. For our English multitask benchmark, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.3">MMLU</span>, and our Traditional Chinese multitask benchmark, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.4">C-eval-tw</span>, we calculate metrics by averaging accuracy across individual tasks. The accuracy computation is based on selecting the option with the highest probabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Reliability</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Our reliability analysis evaluates our model’s performance on three benchmark datasets, covering truthfulness, toxicity, and bias. We consider reliability analysis in both English and Traditional Chinese. While these benchmarks are initially in English, we translate the datasets into Traditional Chinese for comprehensive analysis.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i1.p1.1.1">TruthfulQA</span> <cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a class="ltx_ref" href="#bib.bib20" title="">2022</a>)</cite>: The dataset utilized to measure the truthfulness of language models. The scoring mechanism involves a question and multiple true/false reference answers, where the score is determined by the normalized total probability assigned to the set of true answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i2.p1.1.1">ToxiGen</span> <cite class="ltx_cite ltx_citemacro_cite">Hartvigsen et&nbsp;al. (<a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>: The dataset we employed to detect the toxicity of language models. We utilize the default RoBERTa-based <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib23" title="">2019</a>)</cite> ToxiGen classifier to identify toxic generations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i3.p1.1.1">Bold</span> <cite class="ltx_cite ltx_citemacro_cite">Dhamala et&nbsp;al. (<a class="ltx_ref" href="#bib.bib4" title="">2021</a>)</cite>: The dataset we utilize for bias analysis. We use the Valence Aware Dictionary and Sentiment Reasoner (VADER) <cite class="ltx_cite ltx_citemacro_cite">Hutto and Gilbert (<a class="ltx_ref" href="#bib.bib15" title="">2014</a>)</cite> to compute the sentiment score for the combined prompt and generation text. We report the mean and the standard deviation of the sentiment score of all subgroups.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.2" style="width:195.1pt;height:250.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.6pt,9.7pt) scale(0.927950279189385,0.927950279189385) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.2.3.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T1.2.2.3.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.2.2.3.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1.2.1">EN prompt</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T1.2.2.3.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1.3.1">TW prompt</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.4.2">
<td class="ltx_td ltx_border_r" id="S4.T1.2.2.4.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.2.2.1">EN %</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.2.3.1">TW %</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.4">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.2.4.1">EN %</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.4.2.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.2.5.1">TW %</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.5.3.1">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.5.3.2">99.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.5.3.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.5.3.4">19.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.5.3.5">79.45</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.6.4.1">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.6.4.2">100.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.6.4.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.6.4.4">99.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.6.4.5">0.95</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.7.5.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.7.5.2">99.55</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.7.5.3">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.7.5.4">16.00</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.7.5.5">83.50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.8.6.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.8.6.2">99.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.8.6.3">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.8.6.4">10.15</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.8.6.5">89.20</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.9.7.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.9.7.2">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.9.7.3">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.9.7.4">23.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.9.7.5">76.25</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.10.8.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.10.8.2">99.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.10.8.3">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.10.8.4">41.05</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.10.8.5">58.50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.11.9.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.11.9.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.11.9.2">99.60</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.11.9.3">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.11.9.4">37.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.11.9.5">61.95</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.12.10.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.2">99.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.3">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.4">35.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.12.10.5">63.80</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.13.11.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.13.11.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.13.11.2">99.80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.13.11.3">0.10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.13.11.4">40.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.13.11.5">58.60</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.14.12.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.14.12.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.14.12.2">99.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.14.12.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.14.12.4">70.85</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.14.12.5">28.85</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.15.13.1.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.2">99.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.3">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.4">8.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.15.13.5">90.85</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><msup id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.1.1.m1.1.1a" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T1.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T1.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1"><cn id="S4.T1.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T1.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2">100.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4">98.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.5">1.10</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.2.2.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T1.2.2.2.1.1.m1.1"><semantics id="S4.T1.2.2.2.1.1.m1.1a"><msup id="S4.T1.2.2.2.1.1.m1.1.1" xref="S4.T1.2.2.2.1.1.m1.1.1.cmml"><mi id="S4.T1.2.2.2.1.1.m1.1.1a" xref="S4.T1.2.2.2.1.1.m1.1.1.cmml"></mi><mn id="S4.T1.2.2.2.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T1.2.2.2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.1.m1.1b"><apply id="S4.T1.2.2.2.1.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.1.m1.1.1"><cn id="S4.T1.2.2.2.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T1.2.2.2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.2.2.2">100.00</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.2.2.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.2.2.4">95.85</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.2.2.2.5">4.05</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The results of the language identification analysis. <span class="ltx_text ltx_font_bold" id="S4.T1.7.1">EN prompt</span> indicates the use of English prompts, and <span class="ltx_text ltx_font_bold" id="S4.T1.8.2">TW prompt</span> indicates the use of Chinese prompts. <span class="ltx_text ltx_font_bold" id="S4.T1.9.3">EN %</span> denotes the percentage of output identified as English, while <span class="ltx_text ltx_font_bold" id="S4.T1.10.4">TW %</span> denotes the percentage identified as Chinese.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:182.1pt;height:259.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.6pt,5.1pt) scale(0.962038329385357,0.962038329385357) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.3.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.2.2.3.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T2.2.2.3.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.3.1.2.1">EN prompt</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.2.2.3.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.3.1.3.1">TW prompt</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.4.2">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.2.2.4.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.4.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.2.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.3.1">rep-8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.4.2.4">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.4.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.4.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.5.1">rep-8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.1">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.2">0.843</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.3">0.804</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.4">0.796</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.5.3.5">0.763</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.6.4.1">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.6.4.2">0.080</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.6.4.3">0.024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.6.4.4">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.6.4.5">0.039</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.7.5.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.7.5.2">0.137</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.7.5.3">0.068</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.7.5.4">0.552</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.7.5.5">0.491</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.8.6.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.8.6.2">0.135</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.8.6.3">0.068</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.8.6.4">0.599</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.8.6.5">0.539</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.9.7.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.9.7.2">0.131</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.9.7.3">0.065</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.9.7.4">0.524</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.9.7.5">0.463</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.10.8.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.10.8.2">0.116</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.10.8.3">0.050</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.10.8.4">0.401</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.10.8.5">0.335</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.11.9.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.11.9.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.11.9.2">0.134</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.11.9.3">0.069</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.11.9.4">0.441</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.11.9.5">0.380</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.12.10.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.12.10.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.12.10.2">0.125</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.12.10.3">0.060</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.12.10.4">0.443</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.12.10.5">0.381</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.13.11.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.2">0.119</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.3">0.053</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.4">0.409</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.13.11.5">0.341</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.14.12.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.14.12.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.14.12.2">0.094</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.14.12.3">0.033</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.14.12.4">0.244</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.14.12.5">0.172</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.15.13.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.15.13.1.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.15.13.2">0.169</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.15.13.3">0.098</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.15.13.4">0.621</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.15.13.5">0.566</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><msup id="S4.T2.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.1.m1.1.1a" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T2.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T2.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1"><cn id="S4.T2.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T2.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.2">0.084</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3">0.026</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.4">0.109</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.5">0.043</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.2.2.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.1.m1.1"><semantics id="S4.T2.2.2.2.1.1.m1.1a"><msup id="S4.T2.2.2.2.1.1.m1.1.1" xref="S4.T2.2.2.2.1.1.m1.1.1.cmml"><mi id="S4.T2.2.2.2.1.1.m1.1.1a" xref="S4.T2.2.2.2.1.1.m1.1.1.cmml"></mi><mn id="S4.T2.2.2.2.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T2.2.2.2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.1.m1.1b"><apply id="S4.T2.2.2.2.1.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.1.m1.1.1"><cn id="S4.T2.2.2.2.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T2.2.2.2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.2.2.2">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.2.2.3">0.039</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.2.2.4">0.143</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.2.2.2.5">0.071</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of repetition experiments with prompts in two languages. Full results are available in the Appendix <a class="ltx_ref" href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">C</span></a>.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results and Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Output Format</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.2">We aim to investigate the impact of continual pre-training on Chinese corpus on the language outputs of the models. Table&nbsp;<a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> presents the results of the language identification experiment. We observe that when using English prompts, nearly every model tends to generate output in English. When provided with a Chinese prompt, we observed that Llama-2-7b tends to output in Chinese, whereas Llama-2-7b-chat tends to output in English. Furthermore, with Chinese prompts, the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.2">Freeze First 10 Layers</span> model tends to yield a higher proportion of Chinese text output than the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.3">Freeze Last 10 Layers</span> model. Models with frozen modules show relatively similar results, with approximately <math alttext="60\%" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.1.m1.1"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mrow id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p1.1.m1.1.1.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml">60</mn><mo id="S4.SS3.SSS1.p1.1.m1.1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.SSS1.p1.1.m1.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">60\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.1.m1.1d">60 %</annotation></semantics></math> of their output being in Chinese. In the case of adapters, increasing the learning rate can lead the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.4">Lora</span> model to produce more Chinese output, while the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.2.1.m1.1"><semantics id="S4.SS3.SSS1.p1.2.1.m1.1a"><msup id="S4.SS3.SSS1.p1.2.1.m1.1.1" xref="S4.SS3.SSS1.p1.2.1.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p1.2.1.m1.1.1a" xref="S4.SS3.SSS1.p1.2.1.m1.1.1.cmml"></mi><mn id="S4.SS3.SSS1.p1.2.1.m1.1.1.1" mathvariant="normal" xref="S4.SS3.SSS1.p1.2.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.1.m1.1b"><apply id="S4.SS3.SSS1.p1.2.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.2.1.m1.1.1"><cn id="S4.SS3.SSS1.p1.2.1.m1.1.1.1.cmml" type="integer" xref="S4.SS3.SSS1.p1.2.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.2.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> model tends to favor English output.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">Table&nbsp;<a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> showcases the results of the repetition analysis experiment. We observed that regardless of given Chinese or English prompts, Llama-2-7b consistently exhibits significant repetition issues compared to Llama-2-7b-chat. Additionally, models after continual pre-training on Traditional Chinese corpus displayed a noticeable increase in text repetition with Chinese prompts compared to English prompts. Furthermore, we found that models that are more inclined to generate Chinese output when using Chinese prompts are more likely to have repetition issues.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Knowledge</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Table&nbsp;<a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.3.2 Knowledge ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> shows our knowledge analysis experiments’ results.
Llama-2-7b-chat performs similarly to Llama-2-7b on <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.1">Hellaswag</span> and <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.2">MMLU</span>, while showing a slightly better performance on <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.3">ARC</span> and <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.4">C-eval-tw</span>. In the <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.5">ARC</span> and <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.6">Hellaswag</span> benchmarks, almost all continually pre-trained models outperform Llama-2-7b-chat. In the <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.7">MMLU</span> benchmark, most continual pre-trained models tend to outperform Llama-2-7b-chat. However, in the case of the <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.8">C-eval-tw</span> benchmark, there is no clear pattern when comparing the efficacy of models utilizing simple methods for continual pre-training against Llama-2-7b-chat. It is worth noting that the observed differences mentioned above are subtle.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.2" style="width:216.8pt;height:254pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.8pt,8.0pt) scale(0.940719271680535,0.940719271680535) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2.3.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T3.2.2.3.1.1" rowspan="2"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.3.1.2"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.2.2.3.1.2.1">ARC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.3.1.3"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.2.2.3.1.3.1">Hellaswag</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.3.1.4"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.2.2.3.1.4.1">MMLU</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.3.1.5"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.2.2.3.1.5.1">C-eval-tw</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.4.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.1.1">ACC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.2.1">ACC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.3.1">ACC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.4.2.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.4.1">ACC</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.5.3.1">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.5.3.2">53.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.5.3.3">78.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.5.3.4">46.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.5.3.5">32.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.6.4.1">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.6.4.2">53.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.6.4.3">78.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.6.4.4">46.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.6.4.5">32.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.7.5.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.7.5.2">52.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.7.5.3">77.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.7.5.4">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.7.5.5">33.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.8.6.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.8.6.2">51.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.8.6.3">77.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.8.6.4">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.8.6.5">31.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.9.7.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.9.7.2">51.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.9.7.3">77.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.9.7.4">49.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.9.7.5">33.5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.10.8.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.10.8.2">51.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.10.8.3">77.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.10.8.4">48.9</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.10.8.5">32.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.11.9.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.11.9.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.11.9.2">52.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.11.9.3">78.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.11.9.4">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.11.9.5">33.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.12.10.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.12.10.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.12.10.2">53.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.12.10.3">77.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.12.10.4">49.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.12.10.5">32.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.13.11.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.13.11.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.13.11.2">52.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.13.11.3">77.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.13.11.4">46.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.13.11.5">33.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.14.12.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.14.12.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.14.12.2">53.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.14.12.3">78.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.14.12.4">47.1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.14.12.5">33.8</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.15.13.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.15.13.1.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.15.13.2">52.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.15.13.3">78.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.15.13.4">47.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.15.13.5">33.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><msup id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.1.1.m1.1.1a" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T3.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T3.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1"><cn id="S4.T3.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T3.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.2">53.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.3">77.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.4">47.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.5">32.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.2.2.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.1.m1.1"><semantics id="S4.T3.2.2.2.1.1.m1.1a"><msup id="S4.T3.2.2.2.1.1.m1.1.1" xref="S4.T3.2.2.2.1.1.m1.1.1.cmml"><mi id="S4.T3.2.2.2.1.1.m1.1.1a" xref="S4.T3.2.2.2.1.1.m1.1.1.cmml"></mi><mn id="S4.T3.2.2.2.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T3.2.2.2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.1.m1.1b"><apply id="S4.T3.2.2.2.1.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1"><cn id="S4.T3.2.2.2.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T3.2.2.2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.2.2.2">53.8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.2.2.3">77.3</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.2.2.4">46.2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T3.2.2.2.5">31.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Knowledge analysis experiment results with four benchmarks.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Reliability</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ 4.3.3 Reliability ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>, we present the results of the reliability experiment. Llama-2-7b-chat consistently outperforms Llama-2-7b on the truthfulness and toxicity benchmarks. Notably, after continual pre-training, the models demonstrate inferior performance compared to Llama-2-7b-chat on the two benchmarks. This trend is particularly pronounced in the truthfulness analysis benchmark for English and the toxicity benchmark for Traditional Chinese. Furthermore, we observed that models with a preference for generating Chinese output exhibit inferior performance in the toxicity benchmark. Regarding the bias benchmark, we can observe that Llama-2-7b-chat outputs more positive text than Llama-2-7b. After continual pre-training, the models’ outputs have relatively more negative sentiment scores than Llama-2-7b-chat.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.28" style="width:238.5pt;height:255pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.4pt,16.5pt) scale(0.885580470242052,0.885580470242052) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.28.28">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.28.28.29.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.28.28.29.1.1" rowspan="3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T4.28.28.29.1.2">
<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T4.28.28.29.1.2.1">TruthfulQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T4.28.28.29.1.3">
<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T4.28.28.29.1.3.1">ToxiGen</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T4.28.28.29.1.4">
<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T4.28.28.29.1.4.1">BOLD</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.28.28.30.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T4.28.28.30.2.1">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.30.2.1.1">mc2 ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T4.28.28.30.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.30.2.2.1">toxicity ↓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T4.28.28.30.2.3">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.30.2.3.1">sentiment</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.28.28.31.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.1">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.1.1">EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.2.1">TW</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.3">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.3.1">EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.4"><span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.4.1">TW</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.5">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.5.1">EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.28.28.31.3.6"><span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.6.1">TW</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.3">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.4">39.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.5">45.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.6">20.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.7">24.80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1">0.41<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">±</annotation></semantics></math>0.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.2.2">0.23<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.2.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.2.m1.1d">±</annotation></semantics></math>0.13</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.3">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.4">44.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.5">49.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.6">0.03</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.7">0.22</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.3.3.1">0.66<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.3.3.3.1.m1.1"><semantics id="S4.T4.3.3.3.1.m1.1a"><mo id="S4.T4.3.3.3.1.m1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.1.m1.1d">±</annotation></semantics></math>0.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.4.2">0.69<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.4.4.4.2.m1.1"><semantics id="S4.T4.4.4.4.2.m1.1a"><mo id="S4.T4.4.4.4.2.m1.1.1" xref="S4.T4.4.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S4.T4.4.4.4.2.m1.1.1.cmml" xref="S4.T4.4.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.2.m1.1d">±</annotation></semantics></math>0.19</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.3">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.4">40.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.5">48.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.6">0.05</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.7">5.74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.5.1">0.52<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.5.5.5.1.m1.1"><semantics id="S4.T4.5.5.5.1.m1.1a"><mo id="S4.T4.5.5.5.1.m1.1.1" xref="S4.T4.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T4.5.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.1.m1.1d">±</annotation></semantics></math>0.20</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.6.6.6.2">0.34<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.6.6.6.2.m1.1"><semantics id="S4.T4.6.6.6.2.m1.1a"><mo id="S4.T4.6.6.6.2.m1.1.1" xref="S4.T4.6.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.2.m1.1b"><csymbol cd="latexml" id="S4.T4.6.6.6.2.m1.1.1.cmml" xref="S4.T4.6.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.2.m1.1d">±</annotation></semantics></math>0.14</td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.8.8.8.3.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.4">41.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.5">48.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.6">0.08</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.7">7.12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.7.7.7.1">0.55<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.7.7.7.1.m1.1"><semantics id="S4.T4.7.7.7.1.m1.1a"><mo id="S4.T4.7.7.7.1.m1.1.1" xref="S4.T4.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T4.7.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.7.1.m1.1d">±</annotation></semantics></math>0.22</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.8.8.8.2">0.34<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.8.8.8.2.m1.1"><semantics id="S4.T4.8.8.8.2.m1.1a"><mo id="S4.T4.8.8.8.2.m1.1.1" xref="S4.T4.8.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S4.T4.8.8.8.2.m1.1.1.cmml" xref="S4.T4.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.8.8.2.m1.1d">±</annotation></semantics></math>0.12</td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.10.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.10.10.10.3.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.4">40.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.5">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.6">0.01</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.7">4.69</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.9.9.9.1">0.58<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.9.9.9.1.m1.1"><semantics id="S4.T4.9.9.9.1.m1.1a"><mo id="S4.T4.9.9.9.1.m1.1.1" xref="S4.T4.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T4.9.9.9.1.m1.1.1.cmml" xref="S4.T4.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.9.9.9.1.m1.1d">±</annotation></semantics></math>0.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.10.10.2">0.37<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.10.10.10.2.m1.1"><semantics id="S4.T4.10.10.10.2.m1.1a"><mo id="S4.T4.10.10.10.2.m1.1.1" xref="S4.T4.10.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.2.m1.1b"><csymbol cd="latexml" id="S4.T4.10.10.10.2.m1.1.1.cmml" xref="S4.T4.10.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.10.10.10.2.m1.1d">±</annotation></semantics></math>0.15</td>
</tr>
<tr class="ltx_tr" id="S4.T4.12.12.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.12.12.12.3.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.4">41.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.5">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.6">0.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.7">3.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.11.11.11.1">0.57<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.11.11.11.1.m1.1"><semantics id="S4.T4.11.11.11.1.m1.1a"><mo id="S4.T4.11.11.11.1.m1.1.1" xref="S4.T4.11.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.11.1.m1.1b"><csymbol cd="latexml" id="S4.T4.11.11.11.1.m1.1.1.cmml" xref="S4.T4.11.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.11.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.11.11.11.1.m1.1d">±</annotation></semantics></math>0.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.12.12.12.2">0.42<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.12.12.12.2.m1.1"><semantics id="S4.T4.12.12.12.2.m1.1a"><mo id="S4.T4.12.12.12.2.m1.1.1" xref="S4.T4.12.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.2.m1.1b"><csymbol cd="latexml" id="S4.T4.12.12.12.2.m1.1.1.cmml" xref="S4.T4.12.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.12.12.12.2.m1.1d">±</annotation></semantics></math>0.16</td>
</tr>
<tr class="ltx_tr" id="S4.T4.14.14.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.14.14.14.3.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.4">40.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.5">48.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.6">0.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.7">3.27</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.13.13.13.1">0.59<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.13.13.13.1.m1.1"><semantics id="S4.T4.13.13.13.1.m1.1a"><mo id="S4.T4.13.13.13.1.m1.1.1" xref="S4.T4.13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.1.m1.1b"><csymbol cd="latexml" id="S4.T4.13.13.13.1.m1.1.1.cmml" xref="S4.T4.13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.13.13.13.1.m1.1d">±</annotation></semantics></math>0.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.14.14.14.2">0.43<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.14.14.14.2.m1.1"><semantics id="S4.T4.14.14.14.2.m1.1a"><mo id="S4.T4.14.14.14.2.m1.1.1" xref="S4.T4.14.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.14.2.m1.1b"><csymbol cd="latexml" id="S4.T4.14.14.14.2.m1.1.1.cmml" xref="S4.T4.14.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.14.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.14.14.14.2.m1.1d">±</annotation></semantics></math>0.15</td>
</tr>
<tr class="ltx_tr" id="S4.T4.16.16.16">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.16.16.16.3.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.4">40.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.5">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.6">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.7">3.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.15.15.15.1">0.60<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.15.15.15.1.m1.1"><semantics id="S4.T4.15.15.15.1.m1.1a"><mo id="S4.T4.15.15.15.1.m1.1.1" xref="S4.T4.15.15.15.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.15.1.m1.1b"><csymbol cd="latexml" id="S4.T4.15.15.15.1.m1.1.1.cmml" xref="S4.T4.15.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.15.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.15.15.15.1.m1.1d">±</annotation></semantics></math>0.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.16.16.2">0.42<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.16.16.16.2.m1.1"><semantics id="S4.T4.16.16.16.2.m1.1a"><mo id="S4.T4.16.16.16.2.m1.1.1" xref="S4.T4.16.16.16.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.16.16.16.2.m1.1b"><csymbol cd="latexml" id="S4.T4.16.16.16.2.m1.1.1.cmml" xref="S4.T4.16.16.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.16.16.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.16.16.16.2.m1.1d">±</annotation></semantics></math>0.14</td>
</tr>
<tr class="ltx_tr" id="S4.T4.18.18.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.18.18.18.3.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.4">41.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.5">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.6">0.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.7">3.39</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.17.17.17.1">0.58<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.17.17.17.1.m1.1"><semantics id="S4.T4.17.17.17.1.m1.1a"><mo id="S4.T4.17.17.17.1.m1.1.1" xref="S4.T4.17.17.17.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.17.17.17.1.m1.1b"><csymbol cd="latexml" id="S4.T4.17.17.17.1.m1.1.1.cmml" xref="S4.T4.17.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.17.17.17.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.17.17.17.1.m1.1d">±</annotation></semantics></math>0.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.18.18.18.2">0.43<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.18.18.18.2.m1.1"><semantics id="S4.T4.18.18.18.2.m1.1a"><mo id="S4.T4.18.18.18.2.m1.1.1" xref="S4.T4.18.18.18.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.18.18.18.2.m1.1b"><csymbol cd="latexml" id="S4.T4.18.18.18.2.m1.1.1.cmml" xref="S4.T4.18.18.18.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.18.18.18.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.18.18.18.2.m1.1d">±</annotation></semantics></math>0.16</td>
</tr>
<tr class="ltx_tr" id="S4.T4.20.20.20">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.20.20.20.3.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.4">43.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.5">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.6">0.03</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.7">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.19.19.19.1">0.64<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.19.19.19.1.m1.1"><semantics id="S4.T4.19.19.19.1.m1.1a"><mo id="S4.T4.19.19.19.1.m1.1.1" xref="S4.T4.19.19.19.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.19.19.19.1.m1.1b"><csymbol cd="latexml" id="S4.T4.19.19.19.1.m1.1.1.cmml" xref="S4.T4.19.19.19.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.19.19.19.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.19.19.19.1.m1.1d">±</annotation></semantics></math>0.22</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.20.20.20.2">0.63<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.20.20.20.2.m1.1"><semantics id="S4.T4.20.20.20.2.m1.1a"><mo id="S4.T4.20.20.20.2.m1.1.1" xref="S4.T4.20.20.20.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.20.20.20.2.m1.1b"><csymbol cd="latexml" id="S4.T4.20.20.20.2.m1.1.1.cmml" xref="S4.T4.20.20.20.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.20.20.20.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.20.20.20.2.m1.1d">±</annotation></semantics></math>0.17</td>
</tr>
<tr class="ltx_tr" id="S4.T4.22.22.22">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.3">
<span class="ltx_text ltx_font_smallcaps" id="S4.T4.22.22.22.3.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.4">42.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.5">48.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.6">0.07</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.7">7.97</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.21.21.21.1">0.57<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.21.21.21.1.m1.1"><semantics id="S4.T4.21.21.21.1.m1.1a"><mo id="S4.T4.21.21.21.1.m1.1.1" xref="S4.T4.21.21.21.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.21.21.21.1.m1.1b"><csymbol cd="latexml" id="S4.T4.21.21.21.1.m1.1.1.cmml" xref="S4.T4.21.21.21.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.21.21.21.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.21.21.21.1.m1.1d">±</annotation></semantics></math>0.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.22.22.22.2">0.35<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.22.22.22.2.m1.1"><semantics id="S4.T4.22.22.22.2.m1.1a"><mo id="S4.T4.22.22.22.2.m1.1.1" xref="S4.T4.22.22.22.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.22.22.22.2.m1.1b"><csymbol cd="latexml" id="S4.T4.22.22.22.2.m1.1.1.cmml" xref="S4.T4.22.22.22.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.22.22.22.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.22.22.22.2.m1.1d">±</annotation></semantics></math>0.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.25.25.25">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.23.23.23.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.23.23.23.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T4.23.23.23.1.1.m1.1"><semantics id="S4.T4.23.23.23.1.1.m1.1a"><msup id="S4.T4.23.23.23.1.1.m1.1.1" xref="S4.T4.23.23.23.1.1.m1.1.1.cmml"><mi id="S4.T4.23.23.23.1.1.m1.1.1a" xref="S4.T4.23.23.23.1.1.m1.1.1.cmml"></mi><mn id="S4.T4.23.23.23.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T4.23.23.23.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.23.23.23.1.1.m1.1b"><apply id="S4.T4.23.23.23.1.1.m1.1.1.cmml" xref="S4.T4.23.23.23.1.1.m1.1.1"><cn id="S4.T4.23.23.23.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T4.23.23.23.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.23.23.23.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.23.23.23.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.25.25.25.4">44.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.25.25.25.5">49.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.25.25.25.6">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.25.25.25.7">0.17</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.24.24.24.2">0.66<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.24.24.24.2.m1.1"><semantics id="S4.T4.24.24.24.2.m1.1a"><mo id="S4.T4.24.24.24.2.m1.1.1" xref="S4.T4.24.24.24.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.24.24.24.2.m1.1b"><csymbol cd="latexml" id="S4.T4.24.24.24.2.m1.1.1.cmml" xref="S4.T4.24.24.24.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.24.24.24.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.24.24.24.2.m1.1d">±</annotation></semantics></math>0.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.25.25.25.3">0.69<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.25.25.25.3.m1.1"><semantics id="S4.T4.25.25.25.3.m1.1a"><mo id="S4.T4.25.25.25.3.m1.1.1" xref="S4.T4.25.25.25.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.25.25.25.3.m1.1b"><csymbol cd="latexml" id="S4.T4.25.25.25.3.m1.1.1.cmml" xref="S4.T4.25.25.25.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.25.25.25.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.25.25.25.3.m1.1d">±</annotation></semantics></math>0.19</td>
</tr>
<tr class="ltx_tr" id="S4.T4.28.28.28">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.26.26.26.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T4.26.26.26.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T4.26.26.26.1.1.m1.1"><semantics id="S4.T4.26.26.26.1.1.m1.1a"><msup id="S4.T4.26.26.26.1.1.m1.1.1" xref="S4.T4.26.26.26.1.1.m1.1.1.cmml"><mi id="S4.T4.26.26.26.1.1.m1.1.1a" xref="S4.T4.26.26.26.1.1.m1.1.1.cmml"></mi><mn id="S4.T4.26.26.26.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T4.26.26.26.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.26.26.26.1.1.m1.1b"><apply id="S4.T4.26.26.26.1.1.m1.1.1.cmml" xref="S4.T4.26.26.26.1.1.m1.1.1"><cn id="S4.T4.26.26.26.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T4.26.26.26.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.26.26.26.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.26.26.26.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.28.28.28.4">43.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.28.28.28.5">49.9</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.28.28.28.6">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.28.28.28.7">0.11</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.27.27.27.2">0.66<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.27.27.27.2.m1.1"><semantics id="S4.T4.27.27.27.2.m1.1a"><mo id="S4.T4.27.27.27.2.m1.1.1" xref="S4.T4.27.27.27.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.27.27.27.2.m1.1b"><csymbol cd="latexml" id="S4.T4.27.27.27.2.m1.1.1.cmml" xref="S4.T4.27.27.27.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.27.27.27.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.27.27.27.2.m1.1d">±</annotation></semantics></math>0.23</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T4.28.28.28.3">0.68<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.28.28.28.3.m1.1"><semantics id="S4.T4.28.28.28.3.m1.1a"><mo id="S4.T4.28.28.28.3.m1.1.1" xref="S4.T4.28.28.28.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.28.28.28.3.m1.1b"><csymbol cd="latexml" id="S4.T4.28.28.28.3.m1.1.1.cmml" xref="S4.T4.28.28.28.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.28.28.28.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.28.28.28.3.m1.1d">±</annotation></semantics></math>0.18</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Reliability analysis experiment results on three benchmarks, including truthfulness, bias, and toxicity aspects. <span class="ltx_text ltx_font_bold" id="S4.T4.31.1">EN</span> denotes the origin dataset in English, while <span class="ltx_text ltx_font_bold" id="S4.T4.32.2">TW</span> denotes the translated dataset in Traditional Chinese.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This work shows that catastrophic forgetting during continual pre-training is a non-trivial challenge and cannot be resolved through straightforward methods. Additionally, we find that the repetition problem becomes more pronounced when the model, after continual pre-training, is inclined to produce Traditional Chinese outputs. Moreover, after continual pre-training, the model’s knowledge remains unaffected mainly; however, its reliability declines.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">One notable limitation arises from the resource-intensive nature of continual pre-training LLMs, making reproducing all the straightforward continual pre-training methods outlined in this work challenging. Another significant limitation is that we only conducted continual pre-training using a Traditional Chinese corpus. However, we are also interested in extending our investigation to include pre-training on resources in other languages, and our methodology is easily adaptable to these settings.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The continual pre-training of LLMs can compromise the models’ safety alignment, leading to the generation of text that may contain biased and toxic information. Exploring methods to mitigate compromising the safety alignment could be a prospective avenue for future research.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">We extend our appreciation to the ASUS Open Cloud Infrastructure Software Center for generously providing valuable resources. Special thanks to Steve Chung-Cheng Chen, Tsung-Ying Yang, Jen-Hao Cheng, Hsiao-Tsung Hung, Szu-Hsien Lee, and Dau-Cheng Lyu for their participation in insightful discussions.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:1803.05457</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yiming Cui, Ziqing Yang, and Xin Yao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2304.08177" title="">Efficient and effective text encoding for chinese llama and alpaca</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2304.08177</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhamala et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021.

</span>
<span class="ltx_bibblock">Bold: Dataset and metrics for measuring biases in open-ended language generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>, pages 862–872.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kawin Ethayarajh. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1006" title="">How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 55–65, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">French (1999)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Robert&nbsp;M French. 1999.

</span>
<span class="ltx_bibblock">Catastrophic forgetting in connectionist networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Trends in cognitive sciences</em>, 3(4):128–135.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.5281/zenodo.5371628" title="">A framework for few-shot language model evaluation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats&nbsp;Leon Richter, Quentin&nbsp;Gregory Anthony, Eugene Belilovsky, Irina Rish, and Timothée Lesort. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=pg7PUJe0Tl" title="">Continual pre-training of large language models: How to re-warm your model?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Workshop on Efficient Systems for Foundation Models @ ICML2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartvigsen et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.234" title="">ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3309–3326, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2009.03300</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseini et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Saghar Hosseini, Hamid Palangi, and Ahmed&nbsp;Hassan Awadallah. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.trustnlp-1.11" title="">An empirical study of metrics to measure representational harms in pre-trained language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)</em>, pages 121–134, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De&nbsp;Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International Conference on Machine Learning</em>, pages 2790–2799. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edward&nbsp;J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu&nbsp;Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2305.08322</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutto and Gilbert (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Clayton Hutto and Eric Gilbert. 2014.

</span>
<span class="ltx_bibblock">Vader: A parsimonious rule-based model for sentiment analysis of social media text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the international AAAI conference on web and social media</em>, volume&nbsp;8, pages 216–225.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016a.

</span>
<span class="ltx_bibblock">Fasttext.zip: Compressing text classification models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1612.03651</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016b.

</span>
<span class="ltx_bibblock">Bag of tricks for efficient text classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1607.01759</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2022.

</span>
<span class="ltx_bibblock">Continual pre-training of language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.229" title="">TruthfulQA: Measuring how models mimic human falsehoods</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Chen (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yen-Ting Lin and Yun-Nung Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.17487" title="">Taiwan llm: Bridging the linguistic divide with a culturally aligned language model</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin&nbsp;A Raffel. 2022.

</span>
<span class="ltx_bibblock">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems</em>, 35:1950–1965.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiangyu Qi, Yi&nbsp;Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023.

</span>
<span class="ltx_bibblock">Fine-tuning aligned language models compromises safety, even when users do not intend to!

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2310.03693</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ye&nbsp;Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-2084" title="">When and why are pre-trained word embeddings useful for neural machine translation?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, pages 529–535, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-acl.220" title="">ELLE: Efficient lifelong pre-training for emerging data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 2789–2810, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasley et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.

</span>
<span class="ltx_bibblock">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, pages 3505–3506.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roziere et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing&nbsp;Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2308.12950</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chenyang Song, Xu&nbsp;Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun, and Tao Yang. 2023.

</span>
<span class="ltx_bibblock">Conpet: Continual parameter-efficient tuning for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2309.14763</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in opus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12)</em>, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van&nbsp;Aken et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Betty Van&nbsp;Aken, Benjamin Winter, Alexander Löser, and Felix&nbsp;A Gers. 2019.

</span>
<span class="ltx_bibblock">How does bert answer questions? a layer-wise analysis of transformer representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 28th ACM international conference on information and knowledge management</em>, pages 1823–1832.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc&nbsp;V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et&nbsp;al. 2016.

</span>
<span class="ltx_bibblock">Google’s neural machine translation system: Bridging the gap between human and machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:1609.08144</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 2023.

</span>
<span class="ltx_bibblock">Efficient continual pre-training for building domain specific large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2311.08545</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1472" title="">HellaSwag: Can a machine really finish your sentence?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4791–4800, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu&nbsp;Cai, Qing Qu, Yong&nbsp;Jae Lee, and Yi&nbsp;Ma. 2023.

</span>
<span class="ltx_bibblock">Investigating the catastrophic forgetting in multimodal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2309.10313</em>.

</span>
</li>
</ul>
</section>
<figure class="ltx_figure" id="A0.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1476" id="A0.F2.g1" src="x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of the models outputs.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A0.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="885" id="A0.F3.g1" src="x3.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of models’ outputs. The translation of our prompt is “How does climate change impact ecosystems?”</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompting Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We employ vLLM <cite class="ltx_cite ltx_citemacro_cite">Kwon et&nbsp;al. (<a class="ltx_ref" href="#bib.bib19" title="">2023</a>)</cite> to optimize efficiency, configuring the model with a max_tokens setting of 256. We utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9. Observing Figure &nbsp;<a class="ltx_ref" href="#A0.F2" title="Figure 2 ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> and Figure &nbsp;<a class="ltx_ref" href="#A0.F2" title="Figure 2 ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>, it becomes apparent that when employing Chinese prompts, Llama-2-7b-chat-cp exhibits more repetition issues than Llama-2-7b-chat.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T5.1" style="width:260.2pt;height:167.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.6pt,6.2pt) scale(0.931040511782731,0.931040511782731) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.1.1.2.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.2.1.2.1">Trainable params</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.2.1.3"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.2.1.3.1">All params</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.2.1.4"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.2.1.4.1">Trainable %</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.3.1.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.3.1.2">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.3.1.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.3.1.4">100.000</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.4.2.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.4.2.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.4.2.2">4,714,582,016</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.4.2.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.4.2.4">69.966</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.5.3.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.5.3.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.5.3.2">4,714,582,016</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.5.3.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.5.3.4">69.966</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.6.4.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.6.4.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.6.4.2">4,590,931,968</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.6.4.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.6.4.4">68.131</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.7.5.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.7.5.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.7.5.2">2,147,483,648</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.7.5.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.7.5.4">31.869</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.8.6.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.8.6.2">2,409,893,888</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.8.6.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.8.6.4">35.764</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.9.7.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.9.7.2">4,328,521,728</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.9.7.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.9.7.4">64.236</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.10.8.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.10.8.2">4,194,304</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.10.8.3">6,742,609,920</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.10.8.4">0.062</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.1.1.m1.1a"><msup id="A1.T5.1.1.1.1.1.m1.1.1" xref="A1.T5.1.1.1.1.1.m1.1.1.cmml"><mi id="A1.T5.1.1.1.1.1.m1.1.1a" xref="A1.T5.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="A1.T5.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="A1.T5.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.1.m1.1b"><apply id="A1.T5.1.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.1.m1.1.1"><cn id="A1.T5.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="A1.T5.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.1.1.1.2">614,400</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.1.1.1.3">6,739,030,016</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T5.1.1.1.4">0.009</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Trainable parameters for various straightforward approaches.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T6.1" style="width:173.4pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(19.7pt,-5.3pt) scale(1.29463403333546,1.29463403333546) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T6.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.1.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.2.1">rep-8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.3.1">rep-12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.4.1">rep-16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.5.1">rep-20</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.1">0.141</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.2">0.056</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.3">0.037</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.4">0.030</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.5">0.025</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>The proportion of duplicated n-gram tokens of our Traditional Chinese corpus.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T7.2" style="width:368.6pt;height:266.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.1pt,1.5pt) scale(0.988662802262651,0.988662802262651) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T7.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.2.2.3.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="A1.T7.2.2.3.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5" id="A1.T7.2.2.3.1.2">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.3.1.2.1">EN prompt</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="A1.T7.2.2.3.1.3">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.3.1.3.1">TW prompt</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.4.2">
<td class="ltx_td ltx_border_r" id="A1.T7.2.2.4.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.2">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.2.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.3">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.3.1">rep-8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.4">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.4.1">rep-12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.5">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.5.1">rep-16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.6"><span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.6.1">rep-20</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.7">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.7.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.8">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.8.1">rep-8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.9">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.9.1">rep-12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.10">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.10.1">rep-16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.4.2.11"><span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.11.1">rep-20</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.1">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.2">0.843</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.3">0.804</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.4">0.778</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.5">0.760</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.6">0.747</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.7">0.796</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.8">0.763</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.9">0.743</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.10">0.728</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.5.3.11">0.716</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.1">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.2">0.080</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.3">0.024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.4">0.012</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.5">0.007</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.6">0.005</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.7">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.8">0.039</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.9">0.020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.10">0.012</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.6.4.11">0.008</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.2">0.137</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.3">0.068</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.4">0.046</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.5">0.035</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.6">0.029</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.7">0.552</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.8">0.491</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.9">0.459</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.10">0.437</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.2.2.7.5.11">0.422</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.8.6.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.2">0.135</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.3">0.068</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.4">0.048</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.5">0.038</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.6">0.032</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.7">0.599</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.8">0.539</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.9">0.506</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.10">0.483</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.2.2.8.6.11">0.466</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.9.7.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.2">0.131</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.3">0.065</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.4">0.044</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.5">0.034</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.6">0.028</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.7">0.524</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.8">0.463</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.9">0.432</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.10">0.412</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.9.7.11">0.397</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.10.8.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.2">0.116</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.3">0.050</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.4">0.031</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.5">0.023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.6">0.018</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.7">0.401</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.8">0.335</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.9">0.303</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.10">0.282</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.2.2.10.8.11">0.269</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.11.9.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.2">0.134</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.3">0.069</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.4">0.048</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.5">0.038</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.6">0.032</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.7">0.441</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.8">0.380</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.9">0.350</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.10">0.331</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.11.9.11">0.318</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.12.10.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.2">0.125</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.3">0.060</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.4">0.041</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.5">0.032</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.6">0.027</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.7">0.443</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.8">0.381</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.9">0.350</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.10">0.330</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.12.10.11">0.316</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.13.11.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.2">0.119</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.3">0.053</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.4">0.033</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.5">0.024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.6">0.019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.7">0.409</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.8">0.341</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.9">0.308</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.10">0.287</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.13.11.11">0.273</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.14.12.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.2">0.094</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.3">0.033</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.4">0.017</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.5">0.011</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.6">0.008</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.7">0.244</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.8">0.172</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.9">0.144</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.10">0.128</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.2.2.14.12.11">0.118</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.1">
<span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.15.13.1.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.2">0.169</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.3">0.098</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.4">0.072</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.5">0.059</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.6">0.050</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.7">0.621</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.8">0.566</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.9">0.537</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.10">0.518</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.15.13.11">0.502</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="A1.T7.1.1.1.1.1.m1.1"><semantics id="A1.T7.1.1.1.1.1.m1.1a"><msup id="A1.T7.1.1.1.1.1.m1.1.1" xref="A1.T7.1.1.1.1.1.m1.1.1.cmml"><mi id="A1.T7.1.1.1.1.1.m1.1.1a" xref="A1.T7.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="A1.T7.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="A1.T7.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.T7.1.1.1.1.1.m1.1b"><apply id="A1.T7.1.1.1.1.1.m1.1.1.cmml" xref="A1.T7.1.1.1.1.1.m1.1.1"><cn id="A1.T7.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="A1.T7.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="A1.T7.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.2">0.084</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.3">0.026</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.4">0.013</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.5">0.008</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.6">0.007</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.7">0.109</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.8">0.043</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.9">0.023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.10">0.014</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.1.11">0.010</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.1">
<span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="A1.T7.2.2.2.1.1.m1.1"><semantics id="A1.T7.2.2.2.1.1.m1.1a"><msup id="A1.T7.2.2.2.1.1.m1.1.1" xref="A1.T7.2.2.2.1.1.m1.1.1.cmml"><mi id="A1.T7.2.2.2.1.1.m1.1.1a" xref="A1.T7.2.2.2.1.1.m1.1.1.cmml"></mi><mn id="A1.T7.2.2.2.1.1.m1.1.1.1" mathvariant="normal" xref="A1.T7.2.2.2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.T7.2.2.2.1.1.m1.1b"><apply id="A1.T7.2.2.2.1.1.m1.1.1.cmml" xref="A1.T7.2.2.2.1.1.m1.1.1"><cn id="A1.T7.2.2.2.1.1.m1.1.1.1.cmml" type="integer" xref="A1.T7.2.2.2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.2.2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="A1.T7.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.2">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.3">0.039</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.4">0.023</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.5">0.017</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.6">0.013</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.7">0.143</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.8">0.071</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.9">0.047</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.10">0.035</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.2.2.2.11">0.029</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Complete results of repetition experiments with prompts in two languages.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Details about Experiment Setup</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Our source code is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/lca0503/Llama_tw" title="">https://github.com/lca0503/Llama_tw</a>. We adopted straightforward approaches for continual pre-training on Llama-2-7b-chat<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" title="">https://huggingface.co/meta-llama/Llama-2-7b-chat-hf</a></span></span></span>, utilizing the 1 billion tokens of data from general Traditional Chinese corpus. We gathered our Traditional Chinese corpus from diverse sources, including websites and news pages. We utilize DeepSpeed <cite class="ltx_cite ltx_citemacro_cite">Rasley et&nbsp;al. (<a class="ltx_ref" href="#bib.bib27" title="">2020</a>)</cite> to improve memory efficiency during continual pre-training. The continual pre-training of all models is conducted with a global batch size equivalent to 4 million tokens. This process occurs on 64 V100 GPUs, and we configure the gradient accumulation step to be 16. The learning rate during continual pre-training remained constant at 3e-5, and we experimented with an additional learning rate 3e-4 for the adapter approaches. Details regarding the trainable parameters for various straightforward approaches can be found in Table <a class="ltx_ref" href="#A1.T5" title="Table 5 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">Here, we delve into our adapter settings. For <span class="ltx_text ltx_font_smallcaps" id="A2.p2.1.2">Lora</span> <cite class="ltx_cite ltx_citemacro_cite">Hu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib13" title="">2022</a>)</cite>, we selectively adapt only the query and value projection matrices of each layer in the model. We set the network rank to 8 and the alpha to 32. In the case of <span class="ltx_text ltx_font_smallcaps" id="A2.p2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="A2.p2.1.1.m1.1"><semantics id="A2.p2.1.1.m1.1a"><msup id="A2.p2.1.1.m1.1.1" xref="A2.p2.1.1.m1.1.1.cmml"><mi id="A2.p2.1.1.m1.1.1a" xref="A2.p2.1.1.m1.1.1.cmml"></mi><mn id="A2.p2.1.1.m1.1.1.1" mathvariant="normal" xref="A2.p2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A2.p2.1.1.m1.1b"><apply id="A2.p2.1.1.m1.1.1.cmml" xref="A2.p2.1.1.m1.1.1"><cn id="A2.p2.1.1.m1.1.1.1.cmml" type="integer" xref="A2.p2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="A2.p2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>, we rescale the key and value matrices in self-attention modules and the inner activations in feed-forward modules in each model layer via learned vectors. This is achieved through element-wise multiplication with these vectors.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Details about Experiment Tasks</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Output format Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">We perform two tasks in output format analysis: language identification and repetition analysis. We utilized vLLM <cite class="ltx_cite ltx_citemacro_cite">Kwon et&nbsp;al. (<a class="ltx_ref" href="#bib.bib19" title="">2023</a>)</cite> to enhance efficiency. Expressly, for models that have undergone alignment operations, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), we set up our prompt as “[INST] &lt;context&gt; [/INST]”. We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS1.p2">
<p class="ltx_p" id="A3.SS1.p2.1">To conduct output format analysis, we utilize the following dataset:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I1.i1.p1.1.1">NeuLab-TedTalks<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote2.1.1.1">2</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://opus.nlpl.eu/NeuLab-TedTalks-v1.php" title="">https://opus.nlpl.eu/NeuLab-TedTalks-v1.php</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Qi et&nbsp;al. (<a class="ltx_ref" href="#bib.bib25" title="">2018</a>)</cite>: A common corpus of TED talks, translated into numerous low-resource languages by a global community of volunteers. We randomly selected 2000 aligned sentences from the English and Traditional Chinese subsets for our output format experiments. We download the corpus from OPUS <cite class="ltx_cite ltx_citemacro_cite">Tiedemann (<a class="ltx_ref" href="#bib.bib30" title="">2012</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A3.SS1.p3">
<p class="ltx_p" id="A3.SS1.p3.1">For language identification analysis, we utilize the FastText <cite class="ltx_cite ltx_citemacro_cite">Joulin et&nbsp;al. (<a class="ltx_ref" href="#bib.bib16" title="">2016a</a>, <a class="ltx_ref" href="#bib.bib17" title="">b</a>)</cite> language identification model to detect the language of the generated tokens. As for repetition analysis, we assess the proportion of duplicated n-gram tokens at the BPE level within the combination of the generated output and the prompt.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS1.p4">
<p class="ltx_p" id="A3.SS1.p4.1">Table <a class="ltx_ref" href="#A1.T6" title="Table 6 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a> presents the repetition statistics for our Traditional Chinese corpus, and Table <a class="ltx_ref" href="#A1.T7" title="Table 7 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">7</span></a> presents the full results of the repetition analysis experiment. Notably, despite the pre-trained corpus containing relatively few repetitive tokens, the model pre-trained on this corpus exhibited a rise in text repetition, particularly evident when prompted with Traditional Chinese.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Knowledge Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">In our knowledge analysis, we assess our model’s performance across four benchmarks: <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.1">ARC</span>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.2">Hellaswag</span>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.3">MMLU</span>, and <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.4">C-eval-tw</span>. We employ EleutherAI/lm-evaluation-harness<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Gao et&nbsp;al. (<a class="ltx_ref" href="#bib.bib7" title="">2021</a>)</cite> to assess the model performance on these benchmarks. These benchmarks consist of multiple-choice questions. The accuracy computation is based on selecting the option with the highest probabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.p2">
<ul class="ltx_itemize" id="A3.I2">
<li class="ltx_item" id="A3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i1.p1">
<p class="ltx_p" id="A3.I2.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i1.p1.1.1">ARC<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote4.1.1.1">4</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://allenai.org/data/arc" title="">https://allenai.org/data/arc</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Clark et&nbsp;al. (<a class="ltx_ref" href="#bib.bib2" title="">2018</a>)</cite>: A collection of natural, grade-school science questions. We conducted our evaluation on the Challenge Set within the ARC dataset. We conducted this benchmark using a 25-shot prompt, evaluating performance based on length-normalized accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i2.p1">
<p class="ltx_p" id="A3.I2.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i2.p1.1.1">Hellaswag<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote5.1.1.1">5</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://rowanzellers.com/hellaswag" title="">https://rowanzellers.com/hellaswag</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Zellers et&nbsp;al. (<a class="ltx_ref" href="#bib.bib36" title="">2019</a>)</cite>: An evaluation of commonsense inference, presenting a task that is straightforward for humans but poses a challenge for state-of-the-art models. We conducted this benchmark using a 10-shot prompt, evaluating performance based on length-normalized accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i3.p1">
<p class="ltx_p" id="A3.I2.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i3.p1.1.1">MMLU<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote6.1.1.1">6</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/hendrycks/test" title="">https://github.com/hendrycks/test</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et&nbsp;al. (<a class="ltx_ref" href="#bib.bib10" title="">2020</a>)</cite>: A test for a text model’s multitasking accuracy, covering 57 tasks from elementary math to U.S. history, computer science, law, and beyond. We conducted this benchmark using a 5-shot prompt, calculating metrics by averaging accuracy across individual tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i4.p1">
<p class="ltx_p" id="A3.I2.i4.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i4.p1.1.1">C-eval-tw</span>: C-eval<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cevalbenchmark.com" title="">https://cevalbenchmark.com</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Huang et&nbsp;al. (<a class="ltx_ref" href="#bib.bib14" title="">2023</a>)</cite> serves as a test to evaluate the advanced knowledge and reasoning abilities of foundational models in a Chinese context. The test was initially in Simplified Chinese, and we translated it into Traditional Chinese using the Google Translate <cite class="ltx_cite ltx_citemacro_cite">Wu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib34" title="">2016</a>)</cite> API in the deep-translator<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nidhaloff/deep-translator" title="">https://github.com/nidhaloff/deep-translator</a></span></span></span> package.
We conducted this benchmark using a 0-shot prompt, calculating metrics by averaging accuracy across individual tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Reliability Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">In our reliability analysis, we check the performance of our model across three benchmark datasets, including truthfulness, toxicity, and bias. We conduct this analysis in both English and Traditional Chinese. While these benchmarks are in English, we use the Google Translate API in the deep-translator package to evaluate the datasets in Traditional Chinese, ensuring a comprehensive analysis.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS3.p2">
<ul class="ltx_itemize" id="A3.I3">
<li class="ltx_item" id="A3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I3.i1.p1">
<p class="ltx_p" id="A3.I3.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I3.i1.p1.1.1">TruthfulQA<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote9.1.1.1">9</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/sylinrl/TruthfulQA" title="">https://github.com/sylinrl/TruthfulQA</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Lin et&nbsp;al. (<a class="ltx_ref" href="#bib.bib20" title="">2022</a>)</cite>: A dataset utilized to measure the truthfulness of language models. This dataset comprises questions designed to elicit false responses from individuals with erroneous beliefs or misconceptions. In this analysis, we also employ EleutherAI/lm-evaluation-harness to conduct this benchmark. We conduct the benchmark using a 6-shot prompt. The scoring mechanism involves a question and multiple true/false reference answers, where the score is calculated by the normalized total probability assigned to the set of true answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I3.i2.p1">
<p class="ltx_p" id="A3.I3.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I3.i2.p1.1.1">ToxiGen<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote10.1.1.1">10</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/microsoft/TOXIGEN" title="">https://github.com/microsoft/TOXIGEN</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Hartvigsen et&nbsp;al. (<a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>: The dataset we employed to detect the toxicity of language models. The dataset is a machine-generated dataset comprising toxic and benign statements related to 13 distinct minority groups. We adopt a refined dataset<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/microsoft/SafeNLP" title="">https://github.com/microsoft/SafeNLP</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Hosseini et&nbsp;al. (<a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite>, which mitigates noise by excluding prompts where annotators disagree on the target demographic group. We take these statements as our prompts. We utilize vLLM to enhance efficiency. For models that have undergone alignment operations, we set up our prompt as “[INST] &lt;context&gt; [/INST]”. We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9. We utilize the default RoBERTa-based classifier ToxiGen <cite class="ltx_cite ltx_citemacro_cite">Liu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib23" title="">2019</a>)</cite> for identifying toxic generations. As the classifier is designed to handle English text, we address this constraint by translating the model’s output into English using the Google Translator API before evaluating.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I3.i3.p1">
<p class="ltx_p" id="A3.I3.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I3.i3.p1.1.1">Bold<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote12.1.1.1">12</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/amazon-science/bold" title="">https://github.com/amazon-science/bold</a></span></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Dhamala et&nbsp;al. (<a class="ltx_ref" href="#bib.bib4" title="">2021</a>)</cite>: The dataset we utilize for bias analysis. This biased dataset consists of Wikipedia prompts across five domains: race, gender, religion, political ideology, and profession. We also utilize vLLM to enhance efficiency. We exclude prompts that belong to the religious ideology subgroups Hinduism and Atheism due to their limited number of prompts. For models that have undergone alignment operations, we set up our prompt as “[INST] &lt;context&gt; [/INST]”. We configure the model with a max_tokens setting of 512 and utilize nuclear sampling, setting the temperature to 0.1 and top_p to 0.9. We use the Valence Aware Dictionary and Sentiment Reasoner (VADER) <cite class="ltx_cite ltx_citemacro_cite">Hutto and Gilbert (<a class="ltx_ref" href="#bib.bib15" title="">2014</a>)</cite> to compute the sentiment score for the combined prompt and generation text. Additionally, we translate the model’s output into English using the Google Translator API before employing VADER to calculate the sentiment score.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
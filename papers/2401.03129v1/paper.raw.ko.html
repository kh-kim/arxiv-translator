<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Examining Forgetting in Continual Pre-training of Aligned Large Language Models</title>
<!--Generated on Sat Jan  6 05:33:11 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2401.03129v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2401.03129v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2401.03129v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2401.03129v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Observation of Catastrophic Forgetting during Continual Pre-training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Settings for Observation ‣ 2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Settings for Observation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="2.2 Observation of Catastrophic Forgetting ‣ 2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Observation of Catastrophic Forgetting</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Straightforward Approaches ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Straightforward Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Freeze layers ‣ 3 Straightforward Approaches ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Freeze layers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="3.2 Freeze modules ‣ 3 Straightforward Approaches ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Freeze modules</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="3.3 Adapter ‣ 3 Straightforward Approaches ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Adapter</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="4.1 Setup ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS2" title="4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS1" title="4.2.1 Output format ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Output format</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS2" title="4.2.2 Knowledge ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS2.SSS3" title="4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Reliability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS3" title="4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS3.SSS1" title="4.3.1 Output Format ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Output Format</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS3.SSS2" title="4.3.2 Knowledge ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS3.SSS3" title="4.3.3 Reliability ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Reliability</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5 Conclusion ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A1" title="Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Prompting Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A2" title="Appendix B Additional Details about Experiment Setup ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Details about Experiment Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Details about Experiment Tasks</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A3.SS1" title="C.1 Output format Analysis ‣ Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Output format Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A3.SS2" title="C.2 Knowledge Analysis ‣ Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Knowledge Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A3.SS3" title="C.3 Reliability Analysis ‣ Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Reliability Analysis</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="”Conversion" been="" class="package-alerts ltx_document" errors="" found”="" have="" role="“status”">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY-SA 4.0</div><div id="watermark-tr">arXiv:2401.03129v1 [cs.CL] 06 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Examining Forgetting in Continual Pre-training of Aligned Large Language Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chen-An Li<math alttext="{}^{1,2}" class="ltx_Math" display="inline" id="id1.1.m1.2"><semantics id="id1.1.m1.2a"><msup id="id1.1.m1.2.2" xref="id1.1.m1.2.2.cmml"><mi id="id1.1.m1.2.2a" xref="id1.1.m1.2.2.cmml"></mi><mrow id="id1.1.m1.2.2.2.4" xref="id1.1.m1.2.2.2.3.cmml"><mn id="id1.1.m1.1.1.1.1" xref="id1.1.m1.1.1.1.1.cmml">1</mn><mo id="id1.1.m1.2.2.2.4.1" xref="id1.1.m1.2.2.2.3.cmml">,</mo><mn id="id1.1.m1.2.2.2.2" xref="id1.1.m1.2.2.2.2.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.2b"><apply id="id1.1.m1.2.2.cmml" xref="id1.1.m1.2.2"><list id="id1.1.m1.2.2.2.3.cmml" xref="id1.1.m1.2.2.2.4"><cn id="id1.1.m1.1.1.1.1.cmml" type="integer" xref="id1.1.m1.1.1.1.1">1</cn><cn id="id1.1.m1.2.2.2.2.cmml" type="integer" xref="id1.1.m1.2.2.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.2c">{}^{1,2}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.2d">start_FLOATSUPERSCRIPT 1 , 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>  Hung-Yi Lee<math alttext="{}^{1}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mn id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><cn id="id2.2.m2.1.1.1.cmml" type="integer" xref="id2.2.m2.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"><math alttext="{}^{1}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mn id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><cn id="id3.3.m3.1.1.1.cmml" type="integer" xref="id3.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>National Taiwan University, Taipei, Taiwan 
<br class="ltx_break"><math alttext="{}^{2}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mn id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><cn id="id4.4.m4.1.1.1.cmml" type="integer" xref="id4.4.m4.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>ASUS Open Cloud Infrastructure Software Center, Taipei, Taiwan 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id5.5.id1">b08902123@csie.ntu.edu.tw</span>  <span class="ltx_text ltx_font_typewriter" id="id6.6.id2">hungyilee@ntu.edu.tw</span>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id7.id1">최근 대규모 언어 모델(Large Language Model, LLM)의 발전은 다양한 작업에 걸쳐 놀라운 능력을 보여주었다. 수많은 분야에서 LLM이 강력하게 적용된다는 점을 감안할 때 LLM 개발이 급증했다. LLM을 개발할 때 일반적인 관행은 이전에 미세 조정된 모델에 대한 지속적인 사전 훈련을 포함한다. 그러나 이것은 재앙적인 망각으로 이어질 수 있다. 우리의 연구에서는 기존의 미세 조정된 LLM에 대한 지속적인 사전 훈련 중에 발생하는 망각의 현상을 조사한다. 지속적인 사전 훈련이 출력 형식, 지식 및 신뢰성을 포함한 다양한 차원에 걸쳐 미세 조정된 LLM에 미치는 영향을 평가한다. 실험 결과는 지속적인 사전 훈련, 특히 반복 문제 동안 치명적인 망각을 해결하는 데 있어 중요하지 않은 문제를 강조한다.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Examining Forgetting in Continual Pre-training of Aligned Large Language Models</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
<p class="ltx_p" id="p2.4"><span class="ltx_text" id="p2.4.4" style="width:433.6pt;"><span class="ltx_tabular ltx_align_top" id="p2.4.4.4"><span class="ltx_tbody"><span class="ltx_tr" id="p2.2.2.2.2.2.2"><span class="ltx_td ltx_align_center" id="p2.4.4.4.3.3.1"><span class="ltx_td ltx_align_center" id="p2.2.2.2.2.2"><span class="ltx_td ltx_align_center" id="p2.4.4.4.4.1"><math-An Li<mathidx=0></math>  Hung-Yi Lee<mathidx=1></span></span></span> <span class="ltx_tr" id</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">LLM(Large Language Models)은 다양한 태스크들 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="#bib.bib1" title="">2020</a>)</cite>에 걸쳐 인상적인 성능을 보여주었다. 사전 훈련된 LLM 및 미세 조정된 변형 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="#bib.bib31" title="">2023a</a>, <a class="ltx_ref" href="#bib.bib32" title="">b</a>)</cite>를 출시하는 경향이 증가하고 있다. 이러한 미세 조정된 변형의 대부분은 기존 LLM <cite class="ltx_cite ltx_citemacro_cite">Roziere et al. (<a class="ltx_ref" href="#bib.bib28" title="">2023</a>); Cui et al. (<a class="ltx_ref" href="#bib.bib3" title="">2023</a>)</cite>의 지식 또는 언어 능력을 향상시키는 것을 목표로 한다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">2. Supervised Fine-Tuning (SFT) 및 Reinforcement Learning from Human Feedback (RLHF)과 같은 후속 정렬 작업을 단계 1에서 얻은 모델에 대해 수행한다. 이러한 미세 조정 변형 중 많은 개발이 기존 미세 조정 LLMs <cite class="ltx_cite ltx_citemacro_cite">Cui et al. (<a class="ltx_ref" href="#bib.bib3" title="">2023</a>); Lin and Chen (<a class="ltx_ref" href="#bib.bib21" title="">2023</a>)</cite>에 대해 추가 연속 사전 훈련을 수행한다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">이전 연구에서는 지속적인 사전 훈련이 모델의 특정 콘텐츠 <cite class="ltx_cite ltx_citemacro_cite">Gupta et al. (<a class="ltx_ref" href="#bib.bib8" title="">2023</a>)</cite>를 이해하고 생성하는 능력을 크게 향상시킬 수 있음을 보여주었다. 그러나 지속적인 사전 훈련은 치명적인 망각 <cite class="ltx_cite ltx_citemacro_cite">French (<a class="ltx_ref" href="#bib.bib6" title="">1999</a>)</cite>로 이어질 수 있으며, 제한된 연구는 기존의 미세 조정된 LLM에서 사전 훈련 중에 잊혀진 능력을 탐구했다.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">일부 연구에서는 언어 모델에 대한 지속적인 학습을 연구했다. <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a class="ltx_ref" href="#bib.bib26" title="">2022</a>)</cite>는 떠오르는 데이터에 대한 사전 훈련된 언어 모델에 대한 효율적인 평생 사전 훈련에 초점을 맞췄다. <cite class="ltx_cite ltx_citemacro_cite">Ke et al. (<a class="ltx_ref" href="#bib.bib18" title="">2022</a>)</cite>는 마스킹된 언어 모델에서 연속적인 도메인 적응적 사전 학습 방법을 제안하였다. <cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a class="ltx_ref" href="#bib.bib29" title="">2023</a>)</cite>는 지속적인 작업에 대한 LLM의 지속적인 적응을 위한 지속적인 매개변수 효율적인 조정을 도입했다. <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a class="ltx_ref" href="#bib.bib35" title="">2023</a>)</cite>는 도메인별 LLMs 개발을 위한 지속적인 사전 훈련에 대한 대안적인 접근법을 조사한다. <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>는 미세 조정이 LLM의 안전 정렬을 손상시킨다는 것을 시사한다. <cite class="ltx_cite ltx_citemacro_cite">Zhai et al. (<a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>는 미세 조정된 멀티모달 LLM에서 망각을 평가한다.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">우리의 연구는 기존의 미세 조정된 LLM에 대한 지속적인 사전 훈련 중 망각 발생을 조사한다. 우리의 논문은 주로 중국 전통 말뭉치를 이용한 지속적인 사전 훈련에 초점을 맞추고 있다. 우리는 출력 형식, 지식 및 신뢰성을 포함한 다양한 차원에 걸쳐 지속적인 사전 훈련의 영향을 평가한다. 우리는 이 문제를 해결하기 위해 단순한 방법 이상의 방법이 필요하다는 것을 보여준다. 또한 전통적인 중국어 출력을 생성하는 경향이 있는 모델에서 반복 문제의 중요성이 증가했음을 관찰한다. 마지막으로, 지속적인 사전 훈련에도 불구하고, 우리의 연구 결과는 모델의 지식이 영향을 받지 않는 반면 신뢰성은 감소한다는 것을 시사한다.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Observation of Catastrophic Forgetting during Continual Pre-training</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Settings for Observation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">SFT 및 RLHF를 포함하여 순차적 정렬 작업을 거친 약 70억 개의 매개변수로 구성된 모델인 라마-2-7b-chat에 대한 사전 훈련을 수행한다. 사전 훈련 프로세스는 중국 전통 데이터의 10억 토큰을 활용합니다. 우리는 지속적인 사전 훈련 후 모델을 라마-2-7b-chat-cp로 나타낸다. 우리는 두 모델에 의해 생성된 출력 간의 차이를 관찰하기 위해 특정 프롬프트를 사용한다.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="924" id="S2.F1.g1" src="https://arxiv.org/html/2401.03129v1/x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 1:</span>Illustration of models’ outputs. 우리 프롬프트의 번역은 “멕시코시티에 대해 말해주세요”입니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Observation of Catastrophic Forgetting</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">그림 <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 Settings for Observation ‣ 2 Observation of Catastrophic Forgetting during Continual Pre-training ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>는 프롬프트에서 얻은 결과를 보여준다. 우리는 Llama-2-7b-chat-cp가 Llama-2-7b-chat에 비해 전통적인 중국어 텍스트를 생성하는 경향이 있음을 관찰했지만, 생성된 Llama-2-7b-chat 텍스트는 반복 문제를 나타낸다. 결과적으로 우리는 다양한 측면에 걸쳐 모델의 성능에 대한 보다 심층적인 조사를 수행했다. 부록 <a class="ltx_ref" href="#A1" title="Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>는 더 많은 프롬프트의 추가 결과를 포함합니다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Straightforward Approaches</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">이 섹션에서는 이전 섹션에서 논의된 치명적인 망각 문제를 해결하기 위한 간단한 접근법을 소개한다.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Freeze layers</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">이전 연구에서는 텍스트 정보 <cite class="ltx_cite ltx_citemacro_cite">Ethayarajh (<a class="ltx_ref" href="#bib.bib5" title="">2019</a>); Van Aken et al. (<a class="ltx_ref" href="#bib.bib33" title="">2019</a>)</cite>를 처리할 때 Transformer 기반 모델의 다른 계층에 뚜렷한 기능이 존재하는 것으로 나타났다. 결과적으로, 우리는 지속적인 사전 훈련 동안 모델의 특정 층을 동결하는 것을 실험한다. 구체적으로, <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.1">Freeze First 10</span> 및 <span class="ltx_text ltx_font_smallcaps" id="S3.SS1.p1.1.2">Freeze Last 10</span>으로 각각 나타낸다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Freeze modules</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">또한 지속적인 사전 훈련 동안 모델의 특정 모듈을 동결하여 실험을 수행한다. 이러한 지정된 모듈이 정렬 작업 중에 획득한 능력을 보존하는지 여부를 조사하는 것을 목표로 한다. 우리는 네 가지 전략을 탐구합니다.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i1.p1.1.1">Freeze Attn. </span>: 모델의 각 계층에서 자체 주의 모듈을 동결합니다.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i2.p1.1.1">Only Attn. </span>: 모델의 자체 주의 모듈을 제외한 각 레이어의 모든 모듈을 동결합니다.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i3.p1.1.1">Freeze MLP</span>: Freeze the feed-forward modules in each model layer.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I1.i4.p1.1.1">Only MLP</span>: 모델의 feed-forward 모듈을 제외한 각 레이어의 모든 모듈을 동결한다.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Adapter</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Adapters는 Transformer 기반 모델 <cite class="ltx_cite ltx_citemacro_cite">Houlsby et al. (<a class="ltx_ref" href="#bib.bib12" title="">2019</a>)</cite> 학습에 자주 사용된다. 우리 연구에서는 두 가지 유형의 어댑터를 실험합니다.</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I2.i1.p1.1.1">Lora</span> <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="#bib.bib13" title="">2022</a>)</cite>: Transformer 기반 모델의 각 계층에 학습 가능한 저순위 분해 행렬을 통합하는 방법. 구현에서는 모델 내의 각 계층의 질의 행렬과 값 투영 행렬만을 선택적으로 적용한다.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I2.i2.p1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.1.m1.1"><semantics id="S3.I2.i2.p1.1.1.m1.1a"><msup id="S3.I2.i2.p1.1.1.m1.1.1" xref="S3.I2.i2.p1.1.1.m1.1.1.cmml"><mi id="S3.I2.i2.p1.1.1.m1.1.1a" xref="S3.I2.i2.p1.1.1.m1.1.1.cmml"></mi><mn id="S3.I2.i2.p1.1.1.m1.1.1.1" mathvariant="normal" xref="S3.I2.i2.p1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.1.m1.1b"><apply id="S3.I2.i2.p1.1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1"><cn id="S3.I2.i2.p1.1.1.m1.1.1.1.cmml" type="integer" xref="S3.I2.i2.p1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span><cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>: 학습된 벡터로 모델의 활성화의 요소별 곱셈을 포함하는 기법. 자기 주의 모듈의 키 및 값 행렬과 각 모델 레이어의 피드포워드 모듈의 내부 활성화를 다시 조정한다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">우리는 일반적인 중국 전통 말뭉치의 10억 토큰 데이터를 사용하여 라마-2-7b-채팅에 대한 지속적인 사전 훈련을 위해 간단한 접근법을 사용했다. 지속적인 사전 훈련 동안의 학습률은 3e-5로 일정하게 유지되었으며, 어댑터 접근법에 대한 추가 학습률 3e-4를 실험했다. 보다 자세한 내용은 부록 <a class="ltx_ref" href="#A2" title="Appendix B Additional Details about Experiment Setup ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">B</span></a>에서 확인할 수 있다.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Tasks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">본 연구는 출력 형식, 지식 및 신뢰성과 같은 차원에 따라 모델의 성능을 종합적으로 검토합니다. 자세한 내용은 부록 <a class="ltx_ref" href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">C</span></a>를 참조해 주십시오.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Output format</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">우리는 출력 형식 분석에서 언어 식별과 반복 분석의 두 가지 별개의 작업을 수행한다. 이러한 평가를 수행하기 위해 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS1.p1.1.1">NeuLab-TedTalks</span> <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a class="ltx_ref" href="#bib.bib25" title="">2018</a>)</cite>의 영어 및 번체 중국어 하위 집합에서 2000개의 정렬된 문장을 프롬프트로 무작위로 선택했다.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">언어 식별: 생성된 토큰의 언어를 검출하기 위해 FastText <cite class="ltx_cite ltx_citemacro_cite">Joulin et al. (<a class="ltx_ref" href="#bib.bib16" title="">2016a</a>, <a class="ltx_ref" href="#bib.bib17" title="">b</a>)</cite> 언어 식별 모델을 사용한다.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.4">Repetition: We determine the proportion of duplicated n-gram tokens at the BPE level in the combined output and prompt. This calculation involves the formula: rep-n <math alttext="=1-|" class="ltx_math_unparsed" display="inline" id="S4.I1.i2.p1.1.m1.1"><semantics id="S4.I1.i2.p1.1.m1.1a"><mrow id="S4.I1.i2.p1.1.m1.1b"><mo id="S4.I1.i2.p1.1.m1.1.1">=</mo><mn id="S4.I1.i2.p1.1.m1.1.2">1</mn><mo id="S4.I1.i2.p1.1.m1.1.3" rspace="0em">−</mo><mo fence="false" id="S4.I1.i2.p1.1.m1.1.4" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">=1-|</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.m1.1d">= 1 - |</annotation></semantics></math>unique n-grams<math alttext="|/|" class="ltx_math_unparsed" display="inline" id="S4.I1.i2.p1.2.m2.1"><semantics id="S4.I1.i2.p1.2.m2.1a"><mrow id="S4.I1.i2.p1.2.m2.1b"><mo fence="false" id="S4.I1.i2.p1.2.m2.1.1" stretchy="false">|</mo><mo id="S4.I1.i2.p1.2.m2.1.2" lspace="0em" rspace="0em">/</mo><mo fence="false" id="S4.I1.i2.p1.2.m2.1.3" stretchy="false">|</mo></mrow><annotation encoding="application/x-tex" id="S4.I1.i2.p1.2.m2.1c">|/|</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.2.m2.1d">| / |</annotation></semantics></math>n-grams<math alttext="|" class="ltx_Math" display="inline" id="S4.I1.i2.p1.3.m3.1"><semantics id="S4.I1.i2.p1.3.m3.1a"><mo fence="false" id="S4.I1.i2.p1.3.m3.1.1" stretchy="false" xref="S4.I1.i2.p1.3.m3.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.3.m3.1b"><ci id="S4.I1.i2.p1.3.m3.1.1.cmml" xref="S4.I1.i2.p1.3.m3.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.3.m3.1c">|</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.3.m3.1d">|</annotation></semantics></math>, where n <math alttext="\in[4,8,12,16,20]" class="ltx_Math" display="inline" id="S4.I1.i2.p1.4.m4.5"><semantics id="S4.I1.i2.p1.4.m4.5a"><mrow id="S4.I1.i2.p1.4.m4.5.6" xref="S4.I1.i2.p1.4.m4.5.6.cmml"><mi id="S4.I1.i2.p1.4.m4.5.6.2" xref="S4.I1.i2.p1.4.m4.5.6.2.cmml"></mi><mo id="S4.I1.i2.p1.4.m4.5.6.1" xref="S4.I1.i2.p1.4.m4.5.6.1.cmml">∈</mo><mrow id="S4.I1.i2.p1.4.m4.5.6.3.2" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml"><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.1" stretchy="false" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">[</mo><mn id="S4.I1.i2.p1.4.m4.1.1" xref="S4.I1.i2.p1.4.m4.1.1.cmml">4</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.2" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.2.2" xref="S4.I1.i2.p1.4.m4.2.2.cmml">8</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.3" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.3.3" xref="S4.I1.i2.p1.4.m4.3.3.cmml">12</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.4" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.4.4" xref="S4.I1.i2.p1.4.m4.4.4.cmml">16</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.5" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">,</mo><mn id="S4.I1.i2.p1.4.m4.5.5" xref="S4.I1.i2.p1.4.m4.5.5.cmml">20</mn><mo id="S4.I1.i2.p1.4.m4.5.6.3.2.6" stretchy="false" xref="S4.I1.i2.p1.4.m4.5.6.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.4.m4.5b"><apply id="S4.I1.i2.p1.4.m4.5.6.cmml" xref="S4.I1.i2.p1.4.m4.5.6"><in id="S4.I1.i2.p1.4.m4.5.6.1.cmml" xref="S4.I1.i2.p1.4.m4.5.6.1"></in><csymbol cd="latexml" id="S4.I1.i2.p1.4.m4.5.6.2.cmml" xref="S4.I1.i2.p1.4.m4.5.6.2">absent</csymbol><list id="S4.I1.i2.p1.4.m4.5.6.3.1.cmml" xref="S4.I1.i2.p1.4.m4.5.6.3.2"><cn id="S4.I1.i2.p1.4.m4.1.1.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.1.1">4</cn><cn id="S4.I1.i2.p1.4.m4.2.2.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.2.2">8</cn><cn id="S4.I1.i2.p1.4.m4.3.3.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.3.3">12</cn><cn id="S4.I1.i2.p1.4.m4.4.4.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.4.4">16</cn><cn id="S4.I1.i2.p1.4.m4.5.5.cmml" type="integer" xref="S4.I1.i2.p1.4.m4.5.5">20</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.4.m4.5c">\in[4,8,12,16,20]</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.4.m4.5d">∈ [ 4 , 8 , 12 , 16 , 20 ]</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Knowledge</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">지식 분석에서 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.1">ARC</span> <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a class="ltx_ref" href="#bib.bib2" title="">2018</a>)</cite>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.2">Hellaswag</span> <cite class="ltx_cite ltx_citemacro_cite">Zellers et al. (<a class="ltx_ref" href="#bib.bib36" title="">2019</a>)</cite>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.3">MMLU</span> <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a class="ltx_ref" href="#bib.bib10" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p1.1.4">C-eval-</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.1">ARC</span> 및 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.2">Hellaswag</span>는 영어 커먼센스 추론 벤치마크 역할을 하며, 여기서 우리는 길이 정규화 정확도를 우리의 메트릭으로 사용한다. 영문 멀티태스크 벤치마크인 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.3">MMLU</span>과 중국 전통 멀티태스크 벤치마크인 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS2.SSS2.p2.1.4">C-eval-tw</span>의 경우 개별 태스크에 대한 평균 정확도로 메트릭을 계산합니다. 정확도 계산은 확률이 가장 높은 옵션을 선택하는 것을 기반으로 합니다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Reliability</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">신뢰성 분석은 진실성, 독성 및 편향을 포함하는 세 가지 벤치마크 데이터 세트에 대한 모델의 성능을 평가한다. 우리는 영어와 번체 중국어 모두에서 신뢰도 분석을 고려한다. 이러한 벤치마크는 처음에 영어로 되어 있지만 포괄적인 분석을 위해 데이터 세트를 번체 중국어로 번역한다.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i1.p1.1.1">TruthfulQA</span> <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="#bib.bib20" title="">2022</a>)</cite>: The dataset utilized to measure the truthfulness of language models. 채점 메커니즘은 질문 및 다수의 참/거짓 참조 답변들을 포함하며, 여기서 점수는 참 답변들의 세트에 할당된 정규화된 총 확률에 의해 결정된다.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i2.p1.1.1">ToxiGen</span> <cite class="ltx_cite ltx_citemacro_cite">Hartvigsen et al. (<a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>: 언어 모델의 독성을 탐지하기 위해 사용한 데이터 세트입니다. 독성 세대를 식별하기 위해 기본 RoBERTa 기반 <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib23" title="">2019</a>)</cite> ToxiGen 분류기를 활용한다.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.I2.i3.p1.1.1">Bold</span> <cite class="ltx_cite ltx_citemacro_cite">Dhamala et al. (<a class="ltx_ref" href="#bib.bib4" title="">2021</a>)</cite>: 바이어스 분석을 위해 사용하는 데이터 세트입니다. 우리는 결합된 프롬프트와 생성 텍스트에 대한 감정 점수를 계산하기 위해 VADER(Valence Aware Dictionary and Sentiment Reasoner) <cite class="ltx_cite ltx_citemacro_cite">Hutto and Gilbert (<a class="ltx_ref" href="#bib.bib15" title="">2014</a>)</cite>를 사용한다. 우리는 모든 하위 그룹의 감정 점수의 평균과 표준 편차를 보고한다.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.2" style="width:195.1pt;height:250.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-7.6pt,9.7pt) scale(0.927950279189385,0.927950279189385) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.2.3.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T1.2.2.3.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T1.2.2.3.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1.2.1">EN prompt</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T1.2.2.3.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.2.3.1.3.1">TW prompt</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.4.2">
<td class="ltx_td ltx_border_r" id="S4.T1.2.2.4.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.2.2.1">EN %</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.2.3.1">TW %</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.4.2.4">
<span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.2.4.1">EN %</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.4.2.5"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.4.2.5.1">TW %</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.5.3.1">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.5.3.2">99.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.5.3.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.5.3.4">19.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.5.3.5">79.45</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.6.4.1">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.6.4.2">100.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.6.4.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.6.4.4">99.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.6.4.5">0.95</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.7.5.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.7.5.2">99.55</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.7.5.3">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.7.5.4">16.00</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.7.5.5">83.50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.8.6.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.8.6.2">99.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.8.6.3">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.8.6.4">10.15</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.8.6.5">89.20</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.9.7.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.9.7.2">99.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.9.7.3">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.9.7.4">23.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.9.7.5">76.25</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.10.8.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.10.8.2">99.75</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.10.8.3">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.10.8.4">41.05</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.10.8.5">58.50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.11.9.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.11.9.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.11.9.2">99.60</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.11.9.3">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.11.9.4">37.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.11.9.5">61.95</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.12.10.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.2">99.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.3">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.12.10.4">35.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.12.10.5">63.80</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.13.11.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.13.11.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.13.11.2">99.80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.13.11.3">0.10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.13.11.4">40.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.13.11.5">58.60</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.14.12.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.14.12.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.14.12.2">99.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.14.12.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.2.2.14.12.4">70.85</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.2.14.12.5">28.85</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.15.13.1.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.2">99.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.3">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.15.13.4">8.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.15.13.5">90.85</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><msup id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.1.1.m1.1.1a" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T1.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T1.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1"><cn id="S4.T1.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T1.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2">100.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4">98.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.5">1.10</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.2.2.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T1.2.2.2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T1.2.2.2.1.1.m1.1"><semantics id="S4.T1.2.2.2.1.1.m1.1a"><msup id="S4.T1.2.2.2.1.1.m1.1.1" xref="S4.T1.2.2.2.1.1.m1.1.1.cmml"><mi id="S4.T1.2.2.2.1.1.m1.1.1a" xref="S4.T1.2.2.2.1.1.m1.1.1.cmml"></mi><mn id="S4.T1.2.2.2.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T1.2.2.2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.1.m1.1b"><apply id="S4.T1.2.2.2.1.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.1.m1.1.1"><cn id="S4.T1.2.2.2.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T1.2.2.2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.2.2.2">100.00</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.2.2.3">0.00</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.2.2.2.4">95.85</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.2.2.2.5">4.05</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>언어 식별 분석 결과. <span class="ltx_text ltx_font_bold" id="S4.T1.7.1">EN prompt</span>은 영어 프롬프트의 사용을 나타내고, <span class="ltx_text ltx_font_bold" id="S4.T1.8.2">TW prompt</span>은 중국어 프롬프트의 사용을 나타낸다. <span class="ltx_text ltx_font_bold" id="S4.T1.9.3">EN %</span>은 영어로 식별된 출력의 백분율을 나타내고, <span class="ltx_text ltx_font_bold" id="S4.T1.10.4">TW %</span>은 중국어로 식별된 백분율을 나타낸다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:182.1pt;height:259.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.6pt,5.1pt) scale(0.962038329385357,0.962038329385357) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.3.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.2.2.3.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T2.2.2.3.1.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.3.1.2.1">EN prompt</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T2.2.2.3.1.3">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.3.1.3.1">TW prompt</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.4.2">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.2.2.4.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.4.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.2.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.3.1">rep-8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.4.2.4">
<span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.4.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.4.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.4.2.5.1">rep-8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.1">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.2">0.843</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.3">0.804</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.4">0.796</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.5.3.5">0.763</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.6.4.1">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.6.4.2">0.080</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.6.4.3">0.024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.6.4.4">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.6.4.5">0.039</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.7.5.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.7.5.2">0.137</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.7.5.3">0.068</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.7.5.4">0.552</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.7.5.5">0.491</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.8.6.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.8.6.2">0.135</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.8.6.3">0.068</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.8.6.4">0.599</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.8.6.5">0.539</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.9.7.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.9.7.2">0.131</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.9.7.3">0.065</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.9.7.4">0.524</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.9.7.5">0.463</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.10.8.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.10.8.2">0.116</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.10.8.3">0.050</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.10.8.4">0.401</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.10.8.5">0.335</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.11.9.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.11.9.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.11.9.2">0.134</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.11.9.3">0.069</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.11.9.4">0.441</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.11.9.5">0.380</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.12.10.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.12.10.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.12.10.2">0.125</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.12.10.3">0.060</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.12.10.4">0.443</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.12.10.5">0.381</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.13.11.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.2">0.119</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.3">0.053</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.4">0.409</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.13.11.5">0.341</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.14.12.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.14.12.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.14.12.2">0.094</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.14.12.3">0.033</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.2.2.14.12.4">0.244</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.2.2.14.12.5">0.172</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.15.13.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.15.13.1.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.15.13.2">0.169</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.15.13.3">0.098</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.15.13.4">0.621</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.15.13.5">0.566</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T2.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><msup id="S4.T2.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.1.m1.1.1a" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T2.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T2.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1"><cn id="S4.T2.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T2.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.2">0.084</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3">0.026</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1.4">0.109</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.5">0.043</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.2.2.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T2.2.2.2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.1.m1.1"><semantics id="S4.T2.2.2.2.1.1.m1.1a"><msup id="S4.T2.2.2.2.1.1.m1.1.1" xref="S4.T2.2.2.2.1.1.m1.1.1.cmml"><mi id="S4.T2.2.2.2.1.1.m1.1.1a" xref="S4.T2.2.2.2.1.1.m1.1.1.cmml"></mi><mn id="S4.T2.2.2.2.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T2.2.2.2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.1.m1.1b"><apply id="S4.T2.2.2.2.1.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.1.m1.1.1"><cn id="S4.T2.2.2.2.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T2.2.2.2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.2.2.2">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.2.2.3">0.039</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.2.2.4">0.143</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.2.2.2.5">0.071</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span>두 언어의 프롬프트를 사용한 반복 실험의 결과. 전체 결과는 부록 <a class="ltx_ref" href="#A3" title="Appendix C Additional Details about Experiment Tasks ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">C</span></a>에서 사용할 수 있습니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results and Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Output Format</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.2">우리는 중국어 말뭉치에 대한 지속적인 사전 훈련이 모델의 언어 출력에 미치는 영향을 조사하는 것을 목표로 한다. <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>는 언어 식별 실험의 결과를 제시한다. 우리는 영어 프롬프트를 사용할 때 거의 모든 모델이 영어로 출력을 생성하는 경향이 있음을 관찰한다. 중국어 프롬프트를 제공했을 때, 우리는 라마-2-7b가 중국어로 출력되는 경향이 있는 반면, 라마-2-7b-채팅은 영어로 출력되는 경향이 있음을 관찰했다. 또한 중국 프롬프트를 사용하면 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.2">Freeze First 10 Layers</span> 모델은 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.3">Freeze Last 10 Layers</span> 모델보다 더 높은 비율의 중국어 텍스트 출력을 산출하는 경향이 있습니다. 냉동 모듈이 있는 모델은 출력이 중국어로 대략 <math alttext="60\%" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.1.m1.1"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mrow id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS3.SSS1.p1.1.m1.1.1.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml">60</mn><mo id="S4.SS3.SSS1.p1.1.m1.1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.SSS1.p1.1.m1.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">60\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.1.m1.1d">60 %</annotation></semantics></math>로 비교적 유사한 결과를 보여준다. 어댑터의 경우 학습률을 높이면 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.4">Lora</span> 모델이 더 많은 중국어 출력을 생성할 수 있는 반면, <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS1.p1.2.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.SS3.SSS1.p1.2.1.m1.1"><semantics id="S4.SS3.SSS1.p1.2.1.m1.1a"><msup id="S4.SS3.SSS1.p1.2.1.m1.1.1" xref="S4.SS3.SSS1.p1.2.1.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p1.2.1.m1.1.1a" xref="S4.SS3.SSS1.p1.2.1.m1.1.1.cmml"></mi><mn id="S4.SS3.SSS1.p1.2.1.m1.1.1.1" mathvariant="normal" xref="S4.SS3.SSS1.p1.2.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.1.m1.1b"><apply id="S4.SS3.SSS1.p1.2.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.2.1.m1.1.1"><cn id="S4.SS3.SSS1.p1.2.1.m1.1.1.1.cmml" type="integer" xref="S4.SS3.SSS1.p1.2.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.SSS1.p1.2.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> 모델은 영어 출력을 선호하는 경향이 있다.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1"><a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.2.3 Reliability ‣ 4.2 Tasks ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>는 반복 분석 실험의 결과를 보여준다. 우리는 주어진 중국어 또는 영어 프롬프트에 관계없이 Llama-2-7b가 Llama-2-7b-채팅에 비해 일관되게 중요한 반복 문제를 나타내는 것을 관찰했다. 또한, 전통적인 중국어 말뭉치에 대한 지속적인 사전 훈련 후 모델들은 영어 프롬프트에 비해 중국어 프롬프트에서 텍스트 반복이 눈에 띄게 증가했다. 또한 중국 프롬프트를 사용할 때 중국 출력을 생성하는 경향이 더 높은 모델은 반복 문제가 있을 가능성이 더 높다는 것을 발견했다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Knowledge</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">표 <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.3.2 Knowledge ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>는 우리의 지식 분석 실험 결과를 보여준다. Llama-2-7b-chat은 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.1">Hellaswag</span> 및 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.2">MMLU</span>에서 Llama-2-7b와 유사하게 수행되지만 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.3">ARC</span> 및 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.4">C-eval-tw</span>에서는 약간 더 나은 성능을 보여줍니다. <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.5">ARC</span> 및 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.6">Hellaswag</span> 벤치마크에서 거의 모든 연속 사전 훈련 모델은 Llama-2-7b-chat보다 우수합니다. <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.7">MMLU</span> 벤치마크에서 대부분의 연속 사전 훈련 모델은 Llama-2-7b-chat를 능가하는 경향이 있습니다. 그러나 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.SS3.SSS2.p1.1.8">C-eval-tw</span> 벤치마크의 경우 Llama-2-7b-chat에 대한 지속적인 사전 훈련을 위한 간단한 방법을 활용하는 모델의 효능을 비교할 때 명확한 패턴이 없다. 위에서 언급한 관찰된 차이는 미묘하다는 점에 주목할 필요가 있다.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.2" style="width:216.8pt;height:254pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.8pt,8.0pt) scale(0.940719271680535,0.940719271680535) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2.3.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T3.2.2.3.1.1" rowspan="2"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.3.1.2"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.2.2.3.1.2.1">ARC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.3.1.3"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.2.2.3.1.3.1">Hellaswag</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.3.1.4"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.2.2.3.1.4.1">MMLU</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.3.1.5"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T3.2.2.3.1.5.1">C-eval-tw</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.4.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.1.1">ACC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.4.2.2"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.2.1">ACC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.3.1">ACC</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.4.2.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.4.2.4.1">ACC</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.5.3.1">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.5.3.2">53.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.5.3.3">78.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.5.3.4">46.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.5.3.5">32.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.6.4.1">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.6.4.2">53.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.6.4.3">78.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.6.4.4">46.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.6.4.5">32.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.7.5.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.7.5.2">52.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.7.5.3">77.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.7.5.4">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.7.5.5">33.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.8.6.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.8.6.2">51.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.8.6.3">77.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.8.6.4">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.8.6.5">31.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.9.7.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.9.7.2">51.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.9.7.3">77.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.9.7.4">49.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.9.7.5">33.5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.10.8.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.10.8.2">51.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.10.8.3">77.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.10.8.4">48.9</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.10.8.5">32.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.11.9.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.11.9.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.11.9.2">52.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.11.9.3">78.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.11.9.4">48.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.11.9.5">33.3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.12.10.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.12.10.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.12.10.2">53.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.12.10.3">77.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.12.10.4">49.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.12.10.5">32.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.13.11.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.13.11.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.13.11.2">52.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.13.11.3">77.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.13.11.4">46.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.13.11.5">33.4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.14.12.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.14.12.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.14.12.2">53.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.14.12.3">78.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.2.2.14.12.4">47.1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.2.2.14.12.5">33.8</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.15.13.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.15.13.1.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.15.13.2">52.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.15.13.3">78.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.15.13.4">47.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.15.13.5">33.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T3.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><msup id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.1.1.m1.1.1a" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T3.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T3.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1"><cn id="S4.T3.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T3.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.2">53.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.3">77.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.4">47.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.5">32.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.2.2.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T3.2.2.2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.1.m1.1"><semantics id="S4.T3.2.2.2.1.1.m1.1a"><msup id="S4.T3.2.2.2.1.1.m1.1.1" xref="S4.T3.2.2.2.1.1.m1.1.1.cmml"><mi id="S4.T3.2.2.2.1.1.m1.1.1a" xref="S4.T3.2.2.2.1.1.m1.1.1.cmml"></mi><mn id="S4.T3.2.2.2.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T3.2.2.2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.1.m1.1b"><apply id="S4.T3.2.2.2.1.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.1.m1.1.1"><cn id="S4.T3.2.2.2.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T3.2.2.2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.2.2.2">53.8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.2.2.3">77.3</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.2.2.4">46.2</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T3.2.2.2.5">31.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span>Knowledge analysis experiment results with four benchmarks.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Reliability</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1"><a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ 4.3.3 Reliability ‣ 4.3 Results and Analysis ‣ 4 Experiments ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> 표에서는 신뢰성 실험 결과를 제시한다. 라마-2-7b-채팅은 진실성 및 독성 벤치마크에서 라마-2-7b를 일관되게 능가한다. 특히, 지속적인 사전 훈련 후 모델들은 두 벤치마크에서 Llama-2-7b-chat에 비해 열등한 성능을 보여준다. 이러한 경향은 특히 영어에 대한 진실성 분석 벤치마크와 번체 중국어에 대한 독성 벤치마크에서 두드러진다. 또한, 중국 생산량 생성을 선호하는 모델이 독성 벤치마크에서 열등한 성능을 나타내는 것을 관찰했다. 편향 벤치마크와 관련하여, 우리는 Llama-2-7b-chat이 Llama-2-7b보다 긍정적인 텍스트를 더 많이 출력하는 것을 관찰할 수 있다. 지속적인 사전 훈련 후 모델의 출력은 라마-2-7b-채팅보다 부정적인 감정 점수가 상대적으로 더 많다.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.28" style="width:238.5pt;height:255pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.4pt,16.5pt) scale(0.885580470242052,0.885580470242052) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.28.28">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.28.28.29.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.28.28.29.1.1" rowspan="3"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T4.28.28.29.1.2">
<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T4.28.28.29.1.2.1">TruthfulQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T4.28.28.29.1.3">
<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T4.28.28.29.1.3.1">ToxiGen</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T4.28.28.29.1.4">
<span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S4.T4.28.28.29.1.4.1">BOLD</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.28.28.30.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T4.28.28.30.2.1">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.30.2.1.1">mc2 ↑</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T4.28.28.30.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.30.2.2.1">toxicity ↓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T4.28.28.30.2.3">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.30.2.3.1">sentiment</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.28.28.31.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.1">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.1.1">EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.2.1">TW</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.3">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.3.1">EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.4"><span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.4.1">TW</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.28.28.31.3.5">
<span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.5.1">EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.28.28.31.3.6"><span class="ltx_text ltx_font_bold" id="S4.T4.28.28.31.3.6.1">TW</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.3">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.4">39.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.5">45.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.6">20.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.2.2.2.7">24.80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1">0.41<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">±</annotation></semantics></math>0.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.2.2.2">0.23<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.2.2.2.2.m1.1"><semantics id="S4.T4.2.2.2.2.m1.1a"><mo id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.2.m1.1d">±</annotation></semantics></math>0.13</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.3">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.4">44.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.5">49.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.6">0.03</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.4.4.7">0.22</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.3.3.3.1">0.66<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.3.3.3.1.m1.1"><semantics id="S4.T4.3.3.3.1.m1.1a"><mo id="S4.T4.3.3.3.1.m1.1.1" xref="S4.T4.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.1.m1.1d">±</annotation></semantics></math>0.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.4.4.2">0.69<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.4.4.4.2.m1.1"><semantics id="S4.T4.4.4.4.2.m1.1a"><mo id="S4.T4.4.4.4.2.m1.1.1" xref="S4.T4.4.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S4.T4.4.4.4.2.m1.1.1.cmml" xref="S4.T4.4.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.2.m1.1d">±</annotation></semantics></math>0.19</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.3">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.4">40.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.5">48.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.6">0.05</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.6.6.6.7">5.74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.5.1">0.52<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.5.5.5.1.m1.1"><semantics id="S4.T4.5.5.5.1.m1.1a"><mo id="S4.T4.5.5.5.1.m1.1.1" xref="S4.T4.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T4.5.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.1.m1.1d">±</annotation></semantics></math>0.20</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.6.6.6.2">0.34<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.6.6.6.2.m1.1"><semantics id="S4.T4.6.6.6.2.m1.1a"><mo id="S4.T4.6.6.6.2.m1.1.1" xref="S4.T4.6.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.2.m1.1b"><csymbol cd="latexml" id="S4.T4.6.6.6.2.m1.1.1.cmml" xref="S4.T4.6.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.6.2.m1.1d">±</annotation></semantics></math>0.14</td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.8.8.8.3.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.4">41.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.5">48.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.6">0.08</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.8.8.8.7">7.12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.7.7.7.1">0.55<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.7.7.7.1.m1.1"><semantics id="S4.T4.7.7.7.1.m1.1a"><mo id="S4.T4.7.7.7.1.m1.1.1" xref="S4.T4.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T4.7.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.7.1.m1.1d">±</annotation></semantics></math>0.22</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.8.8.8.2">0.34<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.8.8.8.2.m1.1"><semantics id="S4.T4.8.8.8.2.m1.1a"><mo id="S4.T4.8.8.8.2.m1.1.1" xref="S4.T4.8.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S4.T4.8.8.8.2.m1.1.1.cmml" xref="S4.T4.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.8.8.2.m1.1d">±</annotation></semantics></math>0.12</td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.10.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.10.10.10.3.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.4">40.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.5">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.6">0.01</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.10.10.10.7">4.69</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.9.9.9.1">0.58<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.9.9.9.1.m1.1"><semantics id="S4.T4.9.9.9.1.m1.1a"><mo id="S4.T4.9.9.9.1.m1.1.1" xref="S4.T4.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T4.9.9.9.1.m1.1.1.cmml" xref="S4.T4.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.9.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.9.9.9.1.m1.1d">±</annotation></semantics></math>0.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.10.10.10.2">0.37<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.10.10.10.2.m1.1"><semantics id="S4.T4.10.10.10.2.m1.1a"><mo id="S4.T4.10.10.10.2.m1.1.1" xref="S4.T4.10.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.2.m1.1b"><csymbol cd="latexml" id="S4.T4.10.10.10.2.m1.1.1.cmml" xref="S4.T4.10.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.10.10.10.2.m1.1d">±</annotation></semantics></math>0.15</td>
</tr>
<tr class="ltx_tr" id="S4.T4.12.12.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.12.12.12.3.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.4">41.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.5">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.6">0.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.12.12.12.7">3.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.11.11.11.1">0.57<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.11.11.11.1.m1.1"><semantics id="S4.T4.11.11.11.1.m1.1a"><mo id="S4.T4.11.11.11.1.m1.1.1" xref="S4.T4.11.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.11.1.m1.1b"><csymbol cd="latexml" id="S4.T4.11.11.11.1.m1.1.1.cmml" xref="S4.T4.11.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.11.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.11.11.11.1.m1.1d">±</annotation></semantics></math>0.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.12.12.12.2">0.42<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.12.12.12.2.m1.1"><semantics id="S4.T4.12.12.12.2.m1.1a"><mo id="S4.T4.12.12.12.2.m1.1.1" xref="S4.T4.12.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.2.m1.1b"><csymbol cd="latexml" id="S4.T4.12.12.12.2.m1.1.1.cmml" xref="S4.T4.12.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.12.12.12.2.m1.1d">±</annotation></semantics></math>0.16</td>
</tr>
<tr class="ltx_tr" id="S4.T4.14.14.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.14.14.14.3.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.4">40.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.5">48.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.6">0.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.14.14.14.7">3.27</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.13.13.13.1">0.59<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.13.13.13.1.m1.1"><semantics id="S4.T4.13.13.13.1.m1.1a"><mo id="S4.T4.13.13.13.1.m1.1.1" xref="S4.T4.13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.13.1.m1.1b"><csymbol cd="latexml" id="S4.T4.13.13.13.1.m1.1.1.cmml" xref="S4.T4.13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.13.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.13.13.13.1.m1.1d">±</annotation></semantics></math>0.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.14.14.14.2">0.43<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.14.14.14.2.m1.1"><semantics id="S4.T4.14.14.14.2.m1.1a"><mo id="S4.T4.14.14.14.2.m1.1.1" xref="S4.T4.14.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.14.2.m1.1b"><csymbol cd="latexml" id="S4.T4.14.14.14.2.m1.1.1.cmml" xref="S4.T4.14.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.14.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.14.14.14.2.m1.1d">±</annotation></semantics></math>0.15</td>
</tr>
<tr class="ltx_tr" id="S4.T4.16.16.16">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.16.16.16.3.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.4">40.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.5">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.6">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.16.16.7">3.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.15.15.15.1">0.60<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.15.15.15.1.m1.1"><semantics id="S4.T4.15.15.15.1.m1.1a"><mo id="S4.T4.15.15.15.1.m1.1.1" xref="S4.T4.15.15.15.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.15.1.m1.1b"><csymbol cd="latexml" id="S4.T4.15.15.15.1.m1.1.1.cmml" xref="S4.T4.15.15.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.15.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.15.15.15.1.m1.1d">±</annotation></semantics></math>0.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.16.16.2">0.42<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.16.16.16.2.m1.1"><semantics id="S4.T4.16.16.16.2.m1.1a"><mo id="S4.T4.16.16.16.2.m1.1.1" xref="S4.T4.16.16.16.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.16.16.16.2.m1.1b"><csymbol cd="latexml" id="S4.T4.16.16.16.2.m1.1.1.cmml" xref="S4.T4.16.16.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.16.16.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.16.16.16.2.m1.1d">±</annotation></semantics></math>0.14</td>
</tr>
<tr class="ltx_tr" id="S4.T4.18.18.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.18.18.18.3.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.4">41.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.5">48.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.6">0.04</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.18.18.18.7">3.39</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.17.17.17.1">0.58<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.17.17.17.1.m1.1"><semantics id="S4.T4.17.17.17.1.m1.1a"><mo id="S4.T4.17.17.17.1.m1.1.1" xref="S4.T4.17.17.17.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.17.17.17.1.m1.1b"><csymbol cd="latexml" id="S4.T4.17.17.17.1.m1.1.1.cmml" xref="S4.T4.17.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.17.17.17.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.17.17.17.1.m1.1d">±</annotation></semantics></math>0.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.18.18.18.2">0.43<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.18.18.18.2.m1.1"><semantics id="S4.T4.18.18.18.2.m1.1a"><mo id="S4.T4.18.18.18.2.m1.1.1" xref="S4.T4.18.18.18.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.18.18.18.2.m1.1b"><csymbol cd="latexml" id="S4.T4.18.18.18.2.m1.1.1.cmml" xref="S4.T4.18.18.18.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.18.18.18.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.18.18.18.2.m1.1d">±</annotation></semantics></math>0.16</td>
</tr>
<tr class="ltx_tr" id="S4.T4.20.20.20">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.3"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.20.20.20.3.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.4">43.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.5">49.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.6">0.03</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.20.20.20.7">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.19.19.19.1">0.64<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.19.19.19.1.m1.1"><semantics id="S4.T4.19.19.19.1.m1.1a"><mo id="S4.T4.19.19.19.1.m1.1.1" xref="S4.T4.19.19.19.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.19.19.19.1.m1.1b"><csymbol cd="latexml" id="S4.T4.19.19.19.1.m1.1.1.cmml" xref="S4.T4.19.19.19.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.19.19.19.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.19.19.19.1.m1.1d">±</annotation></semantics></math>0.22</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.20.20.20.2">0.63<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.20.20.20.2.m1.1"><semantics id="S4.T4.20.20.20.2.m1.1a"><mo id="S4.T4.20.20.20.2.m1.1.1" xref="S4.T4.20.20.20.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.20.20.20.2.m1.1b"><csymbol cd="latexml" id="S4.T4.20.20.20.2.m1.1.1.cmml" xref="S4.T4.20.20.20.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.20.20.20.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.20.20.20.2.m1.1d">±</annotation></semantics></math>0.17</td>
</tr>
<tr class="ltx_tr" id="S4.T4.22.22.22">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.3">
<span class="ltx_text ltx_font_smallcaps" id="S4.T4.22.22.22.3.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.4">42.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.5">48.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.6">0.07</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.22.22.22.7">7.97</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.21.21.21.1">0.57<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.21.21.21.1.m1.1"><semantics id="S4.T4.21.21.21.1.m1.1a"><mo id="S4.T4.21.21.21.1.m1.1.1" xref="S4.T4.21.21.21.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.21.21.21.1.m1.1b"><csymbol cd="latexml" id="S4.T4.21.21.21.1.m1.1.1.cmml" xref="S4.T4.21.21.21.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.21.21.21.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.21.21.21.1.m1.1d">±</annotation></semantics></math>0.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.22.22.22.2">0.35<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.22.22.22.2.m1.1"><semantics id="S4.T4.22.22.22.2.m1.1a"><mo id="S4.T4.22.22.22.2.m1.1.1" xref="S4.T4.22.22.22.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.22.22.22.2.m1.1b"><csymbol cd="latexml" id="S4.T4.22.22.22.2.m1.1.1.cmml" xref="S4.T4.22.22.22.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.22.22.22.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.22.22.22.2.m1.1d">±</annotation></semantics></math>0.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.25.25.25">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.23.23.23.1"><span class="ltx_text ltx_font_smallcaps" id="S4.T4.23.23.23.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T4.23.23.23.1.1.m1.1"><semantics id="S4.T4.23.23.23.1.1.m1.1a"><msup id="S4.T4.23.23.23.1.1.m1.1.1" xref="S4.T4.23.23.23.1.1.m1.1.1.cmml"><mi id="S4.T4.23.23.23.1.1.m1.1.1a" xref="S4.T4.23.23.23.1.1.m1.1.1.cmml"></mi><mn id="S4.T4.23.23.23.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T4.23.23.23.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.23.23.23.1.1.m1.1b"><apply id="S4.T4.23.23.23.1.1.m1.1.1.cmml" xref="S4.T4.23.23.23.1.1.m1.1.1"><cn id="S4.T4.23.23.23.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T4.23.23.23.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.23.23.23.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.23.23.23.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.25.25.25.4">44.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.25.25.25.5">49.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.25.25.25.6">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.25.25.25.7">0.17</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.24.24.24.2">0.66<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.24.24.24.2.m1.1"><semantics id="S4.T4.24.24.24.2.m1.1a"><mo id="S4.T4.24.24.24.2.m1.1.1" xref="S4.T4.24.24.24.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.24.24.24.2.m1.1b"><csymbol cd="latexml" id="S4.T4.24.24.24.2.m1.1.1.cmml" xref="S4.T4.24.24.24.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.24.24.24.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.24.24.24.2.m1.1d">±</annotation></semantics></math>0.24</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.25.25.25.3">0.69<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.25.25.25.3.m1.1"><semantics id="S4.T4.25.25.25.3.m1.1a"><mo id="S4.T4.25.25.25.3.m1.1.1" xref="S4.T4.25.25.25.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.25.25.25.3.m1.1b"><csymbol cd="latexml" id="S4.T4.25.25.25.3.m1.1.1.cmml" xref="S4.T4.25.25.25.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.25.25.25.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.25.25.25.3.m1.1d">±</annotation></semantics></math>0.19</td>
</tr>
<tr class="ltx_tr" id="S4.T4.28.28.28">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.26.26.26.1">
<span class="ltx_text ltx_font_smallcaps" id="S4.T4.26.26.26.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="S4.T4.26.26.26.1.1.m1.1"><semantics id="S4.T4.26.26.26.1.1.m1.1a"><msup id="S4.T4.26.26.26.1.1.m1.1.1" xref="S4.T4.26.26.26.1.1.m1.1.1.cmml"><mi id="S4.T4.26.26.26.1.1.m1.1.1a" xref="S4.T4.26.26.26.1.1.m1.1.1.cmml"></mi><mn id="S4.T4.26.26.26.1.1.m1.1.1.1" mathvariant="normal" xref="S4.T4.26.26.26.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.T4.26.26.26.1.1.m1.1b"><apply id="S4.T4.26.26.26.1.1.m1.1.1.cmml" xref="S4.T4.26.26.26.1.1.m1.1.1"><cn id="S4.T4.26.26.26.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T4.26.26.26.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.26.26.26.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.26.26.26.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.28.28.28.4">43.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.28.28.28.5">49.9</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.28.28.28.6">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.28.28.28.7">0.11</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.27.27.27.2">0.66<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.27.27.27.2.m1.1"><semantics id="S4.T4.27.27.27.2.m1.1a"><mo id="S4.T4.27.27.27.2.m1.1.1" xref="S4.T4.27.27.27.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.27.27.27.2.m1.1b"><csymbol cd="latexml" id="S4.T4.27.27.27.2.m1.1.1.cmml" xref="S4.T4.27.27.27.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.27.27.27.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.27.27.27.2.m1.1d">±</annotation></semantics></math>0.23</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T4.28.28.28.3">0.68<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T4.28.28.28.3.m1.1"><semantics id="S4.T4.28.28.28.3.m1.1a"><mo id="S4.T4.28.28.28.3.m1.1.1" xref="S4.T4.28.28.28.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T4.28.28.28.3.m1.1b"><csymbol cd="latexml" id="S4.T4.28.28.28.3.m1.1.1.cmml" xref="S4.T4.28.28.28.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.28.28.28.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S4.T4.28.28.28.3.m1.1d">±</annotation></semantics></math>0.18</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4:</span> 진실성, 편향성, 독성 측면을 포함한 세 가지 벤치마크에 대한 신뢰성 분석 실험 결과. <span class="ltx_text ltx_font_bold" id="S4.T4.31.1">EN</span>은 영어로 된 원본 데이터 세트를 나타내는 반면, <span class="ltx_text ltx_font_bold" id="S4.T4.32.2">TW</span>은 번체 중국어로 된 번역 데이터 세트를 나타냅니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">이 작업은 지속적인 사전 훈련 중 치명적인 망각은 사소하지 않은 도전이며 간단한 방법을 통해 해결될 수 없음을 보여준다. 또한, 반복 문제는 지속적인 사전 훈련 후 모델이 중국 전통 산출물을 생산하는 경향이 있을 때 더 두드러진다는 것을 발견했다. 또한, 지속적인 사전 훈련 후에도 모델의 지식은 주로 영향을 받지 않지만 신뢰성은 감소한다.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">한 가지 주목할 만한 한계는 연속 사전 훈련 LLM의 자원 집약적 특성으로 인해 이 작업에 설명된 모든 간단한 연속 사전 훈련 방법을 재현하기가 어렵다. 또 다른 중요한 한계는 중국 전통 말뭉치를 사용하여 지속적인 사전 훈련만 수행했다는 것이다. 그러나 우리는 또한 다른 언어의 리소스에 대한 사전 교육을 포함하도록 조사를 확장하는 데 관심이 있으며 우리의 방법론은 이러한 설정에 쉽게 적응할 수 있다.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">LLM의 지속적인 사전 훈련은 모델의 안전 정렬을 손상시켜 편향되고 독성 정보를 포함할 수 있는 텍스트를 생성할 수 있다. 안전 정렬의 손상을 완화하기 위한 탐색 방법은 향후 연구를 위한 전향적인 방법이 될 수 있다.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">우리는 귀중한 자원을 아낌없이 제공한 ASUS 오픈 클라우드 인프라 소프트웨어 센터에 감사를 표합니다. 스티브 청청천, 양성영, 젠하오 청, 샤오성 헝, 쯔셴 리, 다우청 류가 통찰력 있는 토론에 참여해 준 데 대해 특별한 감사를 표한다.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et&nbsp;al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:1803.05457</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yiming Cui, Ziqing Yang, and Xin Yao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2304.08177" title="">Efficient and effective text encoding for chinese llama and alpaca</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2304.08177</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhamala et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021.

</span>
<span class="ltx_bibblock">Bold: Dataset and metrics for measuring biases in open-ended language generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>, pages 862–872.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kawin Ethayarajh. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1006" title="">How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 55–65, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">French (1999)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Robert&nbsp;M French. 1999.

</span>
<span class="ltx_bibblock">Catastrophic forgetting in connectionist networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Trends in cognitive sciences</em>, 3(4):128–135.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.5281/zenodo.5371628" title="">A framework for few-shot language model evaluation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats&nbsp;Leon Richter, Quentin&nbsp;Gregory Anthony, Eugene Belilovsky, Irina Rish, and Timothée Lesort. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=pg7PUJe0Tl" title="">Continual pre-training of large language models: How to re-warm your model?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Workshop on Efficient Systems for Foundation Models @ ICML2023</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartvigsen et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.234" title="">ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3309–3326, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2009.03300</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hosseini et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Saghar Hosseini, Hamid Palangi, and Ahmed&nbsp;Hassan Awadallah. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.trustnlp-1.11" title="">An empirical study of metrics to measure representational harms in pre-trained language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)</em>, pages 121–134, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De&nbsp;Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">International Conference on Machine Learning</em>, pages 2790–2799. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Edward&nbsp;J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu&nbsp;Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2305.08322</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutto and Gilbert (2014)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Clayton Hutto and Eric Gilbert. 2014.

</span>
<span class="ltx_bibblock">Vader: A parsimonious rule-based model for sentiment analysis of social media text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the international AAAI conference on web and social media</em>, volume&nbsp;8, pages 216–225.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016a.

</span>
<span class="ltx_bibblock">Fasttext.zip: Compressing text classification models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1612.03651</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et&nbsp;al. (2016b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016b.

</span>
<span class="ltx_bibblock">Bag of tricks for efficient text classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1607.01759</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. 2022.

</span>
<span class="ltx_bibblock">Continual pre-training of language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody&nbsp;Hao Yu, Joseph&nbsp;E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.229" title="">TruthfulQA: Measuring how models mimic human falsehoods</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Chen (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yen-Ting Lin and Yun-Nung Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.17487" title="">Taiwan llm: Bridging the linguistic divide with a culturally aligned language model</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin&nbsp;A Raffel. 2022.

</span>
<span class="ltx_bibblock">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems</em>, 35:1950–1965.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xiangyu Qi, Yi&nbsp;Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023.

</span>
<span class="ltx_bibblock">Fine-tuning aligned language models compromises safety, even when users do not intend to!

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2310.03693</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ye&nbsp;Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-2084" title="">When and why are pre-trained word embeddings useful for neural machine translation?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, pages 529–535, New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.findings-acl.220" title="">ELLE: Efficient lifelong pre-training for emerging data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Findings of the Association for Computational Linguistics: ACL 2022</em>, pages 2789–2810, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasley et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.

</span>
<span class="ltx_bibblock">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, pages 3505–3506.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roziere et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing&nbsp;Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et&nbsp;al. 2023.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2308.12950</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chenyang Song, Xu&nbsp;Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun, and Tao Yang. 2023.

</span>
<span class="ltx_bibblock">Conpet: Continual parameter-efficient tuning for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2309.14763</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in opus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12)</em>, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et&nbsp;al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et&nbsp;al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van&nbsp;Aken et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Betty Van&nbsp;Aken, Benjamin Winter, Alexander Löser, and Felix&nbsp;A Gers. 2019.

</span>
<span class="ltx_bibblock">How does bert answer questions? a layer-wise analysis of transformer representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 28th ACM international conference on information and knowledge management</em>, pages 1823–1832.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc&nbsp;V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et&nbsp;al. 2016.

</span>
<span class="ltx_bibblock">Google’s neural machine translation system: Bridging the gap between human and machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:1609.08144</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yong Xie, Karan Aggarwal, and Aitzaz Ahmad. 2023.

</span>
<span class="ltx_bibblock">Efficient continual pre-training for building domain specific large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2311.08545</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1472" title="">HellaSwag: Can a machine really finish your sentence?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4791–4800, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu&nbsp;Cai, Qing Qu, Yong&nbsp;Jae Lee, and Yi&nbsp;Ma. 2023.

</span>
<span class="ltx_bibblock">Investigating the catastrophic forgetting in multimodal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2309.10313</em>.

</span>
</li>
</ul>
</section>
<figure class="ltx_figure" id="A0.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1476" id="A0.F2.g1" src="https://arxiv.org/html/2401.03129v1/x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2: 모델 출력의</span>Illustration.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A0.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="885" id="A0.F3.g1" src="https://arxiv.org/html/2401.03129v1/x3.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span>Illustration of models’ outputs. 우리 프롬프트의 번역은 "기후 변화가 생태계에 어떤 영향을 미치는가?"이다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompting Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">효율 최적화를 위해 vLLM <cite class="ltx_cite ltx_citemacro_cite">Kwon et al. (<a class="ltx_ref" href="#bib.bib19" title="">2023</a>)</cite>를 사용하여 max_tokens 설정 256으로 모델을 구성합니다. 핵 샘플링을 사용하여 온도를 0.1로 설정하고 top_p를 0.9로 설정합니다. 그림 <a class="ltx_ref" href="#A0.F2" title="Figure 2 ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> 및 그림 <a class="ltx_ref" href="#A0.F2" title="Figure 2 ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>를 관찰하면 중국 프롬프트를 사용할 때 Llama-2-7b-chat-cp가 Llama-2-7b-chat보다 더 많은 반복 문제를 나타내는 것이 분명합니다.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T5.1" style="width:260.2pt;height:167.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.6pt,6.2pt) scale(0.931040511782731,0.931040511782731) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.1.1.2.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.2.1.2.1">Trainable params</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.2.1.3"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.2.1.3.1">All params</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.2.1.4"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.2.1.4.1">Trainable %</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.3.1.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.3.1.2">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.3.1.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.3.1.4">100.000</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.4.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.4.2.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.4.2.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.4.2.2">4,714,582,016</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.4.2.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.4.2.4">69.966</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.5.3.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.5.3.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.5.3.2">4,714,582,016</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.5.3.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.5.3.4">69.966</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.6.4.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.6.4.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.6.4.2">4,590,931,968</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.6.4.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.6.4.4">68.131</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.7.5.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.7.5.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.7.5.2">2,147,483,648</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.7.5.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.7.5.4">31.869</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.8.6.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.8.6.2">2,409,893,888</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.8.6.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.8.6.4">35.764</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.9.7.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.9.7.2">4,328,521,728</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.9.7.3">6,738,415,616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.9.7.4">64.236</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.10.8.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.10.8.2">4,194,304</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T5.1.1.10.8.3">6,742,609,920</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.10.8.4">0.062</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T5.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.1.1.m1.1a"><msup id="A1.T5.1.1.1.1.1.m1.1.1" xref="A1.T5.1.1.1.1.1.m1.1.1.cmml"><mi id="A1.T5.1.1.1.1.1.m1.1.1a" xref="A1.T5.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="A1.T5.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="A1.T5.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.1.m1.1b"><apply id="A1.T5.1.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.1.m1.1.1"><cn id="A1.T5.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="A1.T5.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.1.1.1.2">614,400</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T5.1.1.1.3">6,739,030,016</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T5.1.1.1.4">0.009</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span>다양한 간단한 접근법에 대한 훈련 가능한 파라미터.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T6.1" style="width:173.4pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(19.7pt,-5.3pt) scale(1.29463403333546,1.29463403333546) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T6.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.1.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.2.1">rep-8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.3.1">rep-12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.4.1">rep-16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.5.1">rep-20</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.1">0.141</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.2">0.056</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.3">0.037</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.4">0.030</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T6.1.1.2.2.5">0.025</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 6:</span>중국 전통 말뭉치의 중복된 n-gram 토큰의 비율입니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A1.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T7.2" style="width:368.6pt;height:266.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.1pt,1.5pt) scale(0.988662802262651,0.988662802262651) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T7.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.2.2.3.1">
<td class="ltx_td ltx_border_r ltx_border_t" id="A1.T7.2.2.3.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5" id="A1.T7.2.2.3.1.2">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.3.1.2.1">EN prompt</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="A1.T7.2.2.3.1.3">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.3.1.3.1">TW prompt</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.4.2">
<td class="ltx_td ltx_border_r" id="A1.T7.2.2.4.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.2">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.2.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.3">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.3.1">rep-8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.4">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.4.1">rep-12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.5">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.5.1">rep-16</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.6"><span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.6.1">rep-20</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.7">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.7.1">rep-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.8">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.8.1">rep-8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.9">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.9.1">rep-12</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.4.2.10">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.10.1">rep-16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.4.2.11"><span class="ltx_text ltx_font_bold" id="A1.T7.2.2.4.2.11.1">rep-20</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.5.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.1">Llama-2-7b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.2">0.843</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.3">0.804</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.4">0.778</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.5">0.760</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.6">0.747</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.7">0.796</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.8">0.763</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.9">0.743</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.5.3.10">0.728</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.5.3.11">0.716</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.6.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.1">Llama-2-7b-chat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.2">0.080</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.3">0.024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.4">0.012</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.5">0.007</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.6">0.005</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.7">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.8">0.039</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.9">0.020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.6.4.10">0.012</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.6.4.11">0.008</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.7.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.1">Llama-2-7b-chat-cp</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.2">0.137</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.3">0.068</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.4">0.046</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.5">0.035</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.6">0.029</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.7">0.552</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.8">0.491</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.9">0.459</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.7.5.10">0.437</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.2.2.7.5.11">0.422</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.8.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.8.6.1.1">Freeze First 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.2">0.135</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.3">0.068</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.4">0.048</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.5">0.038</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.6">0.032</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.7">0.599</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.8">0.539</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.9">0.506</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.8.6.10">0.483</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.2.2.8.6.11">0.466</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.9.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.9.7.1.1">Freeze Last 10</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.2">0.131</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.3">0.065</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.4">0.044</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.5">0.034</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.6">0.028</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.7">0.524</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.8">0.463</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.9">0.432</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.9.7.10">0.412</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.9.7.11">0.397</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.10.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.10.8.1.1">Freeze Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.2">0.116</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.3">0.050</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.4">0.031</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.5">0.023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.6">0.018</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.7">0.401</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.8">0.335</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.9">0.303</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.10.8.10">0.282</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.2.2.10.8.11">0.269</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.11.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.11.9.1.1">Only Attn.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.2">0.134</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.3">0.069</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.4">0.048</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.5">0.038</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.6">0.032</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.7">0.441</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.8">0.380</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.9">0.350</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.11.9.10">0.331</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.11.9.11">0.318</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.12.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.12.10.1.1">Freeze MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.2">0.125</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.3">0.060</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.4">0.041</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.5">0.032</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.6">0.027</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.7">0.443</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.8">0.381</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.9">0.350</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.12.10.10">0.330</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.12.10.11">0.316</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.13.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.13.11.1.1">Only MLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.2">0.119</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.3">0.053</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.4">0.033</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.5">0.024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.6">0.019</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.7">0.409</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.8">0.341</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.9">0.308</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.13.11.10">0.287</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.13.11.11">0.273</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.14.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.14.12.1.1">Lora</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.2">0.094</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.3">0.033</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.4">0.017</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.5">0.011</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.6">0.008</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.7">0.244</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.8">0.172</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.9">0.144</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.2.2.14.12.10">0.128</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.2.2.14.12.11">0.118</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.15.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.1">
<span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.15.13.1.1">Lora</span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.2">0.169</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.3">0.098</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.4">0.072</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.5">0.059</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.6">0.050</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.7">0.621</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.8">0.566</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.9">0.537</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.2.2.15.13.10">0.518</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.2.2.15.13.11">0.502</td>
</tr>
<tr class="ltx_tr" id="A1.T7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.1"><span class="ltx_text ltx_font_smallcaps" id="A1.T7.1.1.1.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="A1.T7.1.1.1.1.1.m1.1"><semantics id="A1.T7.1.1.1.1.1.m1.1a"><msup id="A1.T7.1.1.1.1.1.m1.1.1" xref="A1.T7.1.1.1.1.1.m1.1.1.cmml"><mi id="A1.T7.1.1.1.1.1.m1.1.1a" xref="A1.T7.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="A1.T7.1.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="A1.T7.1.1.1.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.T7.1.1.1.1.1.m1.1b"><apply id="A1.T7.1.1.1.1.1.m1.1.1.cmml" xref="A1.T7.1.1.1.1.1.m1.1.1"><cn id="A1.T7.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="A1.T7.1.1.1.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.1.1.1.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="A1.T7.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.2">0.084</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.3">0.026</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.4">0.013</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.5">0.008</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.6">0.007</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.7">0.109</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.8">0.043</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.9">0.023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.1.1.1.10">0.014</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.1.1.1.11">0.010</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.1">
<span class="ltx_text ltx_font_smallcaps" id="A1.T7.2.2.2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="A1.T7.2.2.2.1.1.m1.1"><semantics id="A1.T7.2.2.2.1.1.m1.1a"><msup id="A1.T7.2.2.2.1.1.m1.1.1" xref="A1.T7.2.2.2.1.1.m1.1.1.cmml"><mi id="A1.T7.2.2.2.1.1.m1.1.1a" xref="A1.T7.2.2.2.1.1.m1.1.1.cmml"></mi><mn id="A1.T7.2.2.2.1.1.m1.1.1.1" mathvariant="normal" xref="A1.T7.2.2.2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A1.T7.2.2.2.1.1.m1.1b"><apply id="A1.T7.2.2.2.1.1.m1.1.1.cmml" xref="A1.T7.2.2.2.1.1.m1.1.1"><cn id="A1.T7.2.2.2.1.1.m1.1.1.1.cmml" type="integer" xref="A1.T7.2.2.2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.2.2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="A1.T7.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span> (3e-4)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.2">0.103</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.3">0.039</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.4">0.023</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.5">0.017</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.6">0.013</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.7">0.143</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.8">0.071</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.9">0.047</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T7.2.2.2.10">0.035</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A1.T7.2.2.2.11">0.029</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 7:</span> 두 언어의 프롬프트를 사용한 반복 실험의 완전한 결과.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Details about Experiment Setup</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">우리의 소스 코드는 <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/lca0503/Llama_tw" title="">https://github.com/lca0503/Llama_tw</a>에서 사용할 수 있습니다. Llama-2-7b-chat<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" title="">https://huggingface.co/meta-llama/Llama-2-7b-chat-hf</a></span></span></span>에 대한 지속적인 사전 훈련을 위해 일반적인 중국 전통 말뭉치의 10억 토큰 데이터를 사용하여 간단한 접근법을 채택했다. 우리는 웹사이트와 뉴스 페이지를 포함한 다양한 출처에서 중국 전통 말뭉치를 수집했다. 지속적인 사전 훈련 시 메모리 효율을 향상시키기 위해 DeepSpeed <cite class="ltx_cite ltx_citemacro_cite">Rasley et al. (<a class="ltx_ref" href="#bib.bib27" title="">2020</a>)</cite>를 활용한다. 모든 모델의 지속적인 사전 훈련은 400만 토큰에 해당하는 글로벌 배치 크기로 진행됩니다. 이 과정은 64개의 V100 GPU에서 수행되며, 기울기 누적 단계를 16으로 구성한다. 지속적인 사전 훈련 동안 학습률은 3e-5로 일정하게 유지되었고, 어댑터 접근에 대해 추가 학습률 3e-4를 실험했다. 다양한 간단한 접근법에 대한 훈련 가능한 매개변수에 관한 세부 사항은 표 <a class="ltx_ref" href="#A1.T5" title="Table 5 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>에서 찾을 수 있다.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">여기에서 어댑터 설정을 조사합니다. <span class="ltx_text ltx_font_smallcaps" id="A2.p2.1.2">Lora</span> <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="#bib.bib13" title="">2022</a>)</cite> 모델의 각 계층의 쿼리 및 값 투영 행렬만 선택적으로 적응합니다. 네트워크 랭크를 8, 알파를 32로 설정하였다. <span class="ltx_text ltx_font_smallcaps" id="A2.p2.1.1">(Ia)<math alttext="{}^{3}" class="ltx_Math" display="inline" id="A2.p2.1.1.m1.1"><semantics id="A2.p2.1.1.m1.1a"><msup id="A2.p2.1.1.m1.1.1" xref="A2.p2.1.1.m1.1.1.cmml"><mi id="A2.p2.1.1.m1.1.1a" xref="A2.p2.1.1.m1.1.1.cmml"></mi><mn id="A2.p2.1.1.m1.1.1.1" mathvariant="normal" xref="A2.p2.1.1.m1.1.1.1.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="A2.p2.1.1.m1.1b"><apply id="A2.p2.1.1.m1.1.1.cmml" xref="A2.p2.1.1.m1.1.1"><cn id="A2.p2.1.1.m1.1.1.1.cmml" type="integer" xref="A2.p2.1.1.m1.1.1.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.1.m1.1c">{}^{3}</annotation><annotation encoding="application/x-llamapun" id="A2.p2.1.1.m1.1d">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math></span><cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>의 경우 학습된 벡터를 통해 자기 주의 모듈의 키 및 값 행렬과 각 모델 레이어의 피드 포워드 모듈의 내부 활성화를 재스케일한다. 이것은 이들 벡터와의 요소-와이즈 곱셈을 통해 달성된다.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Details about Experiment Tasks</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Output format Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">우리는 출력 형식 분석에서 언어 식별과 반복 분석의 두 가지 작업을 수행한다. 효율성을 높이기 위해 vLLM <cite class="ltx_cite ltx_citemacro_cite">Kwon et al. (<a class="ltx_ref" href="#bib.bib19" title="">2023</a>)</cite>를 활용하였다. 즉, 감독 미세 조정(Supervised Fine-Tuning, SFT) 및 인간 피드백으로부터의 강화 학습(Reinforcement Learning from Human Feedback,RLHF)과 같은 정렬 작업을 거친 모델에 대해, 프롬프트를 "[INST] <컨텍스트> [/INST]"로 설정하였다. 512의 max_tokens 설정으로 모델을 구성하고 온도를 0.1로 설정하고 top_p를 0.9로 설정하는 핵 샘플링을 활용한다.</p>
</div>
<div class="ltx_para" id="A3.SS1.p2">
<p class="ltx_p" id="A3.SS1.p2.1">출력 형식 분석을 수행 하려면 다음 데이터 집합을 활용 합니다.</p>
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I1.i1.p1.1.1">NeuLab-TedTalks<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote2.1.1.1">2</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://opus.nlpl.eu/NeuLab-TedTalks-v1.php" title="">https://opus.nlpl.eu/NeuLab-TedTalks-v1.php</a></span></span></span></span></cite idx=0></cite>: A common corpus of TED talks, translated to numerous low-resource languages by a global community of volunteers. 출력 형식 실험을 위해 영어 및 번체 중국어 하위 집합에서 2000개의 정렬된 문장을 무작위로 선택했다. OPUS <cite class="ltx_cite ltx_citemacro_cite">Tiedemann (<a class="ltx_ref" href="#bib.bib30" title="">2012</a>)</cite>로부터 코퍼스를 다운로드한다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A3.SS1.p3">
<p class="ltx_p" id="A3.SS1.p3.1">언어 식별 분석을 위해 FastText <cite class="ltx_cite ltx_citemacro_cite">Joulin et al. (<a class="ltx_ref" href="#bib.bib16" title="">2016a</a>, <a class="ltx_ref" href="#bib.bib17" title="">b</a>)</cite> 언어 식별 모델을 활용하여 생성된 토큰의 언어를 검출한다. 반복 분석의 경우 생성된 출력과 프롬프트의 조합 내에서 BPE 수준에서 복제된 n-그램 토큰의 비율을 평가한다.</p>
</div>
<div class="ltx_para" id="A3.SS1.p4">
<p class="ltx_p" id="A3.SS1.p4.1">표 <a class="ltx_ref" href="#A1.T6" title="Table 6 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>는 우리 중국 전통 말뭉치에 대한 반복 통계를 제시하고, 표 <a class="ltx_ref" href="#A1.T7" title="Table 7 ‣ Appendix A Prompting Results ‣ Examining Forgetting in Continual Pre-training of Aligned Large Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>는 반복 분석 실험의 전체 결과를 제시한다. 특히, 비교적 적은 수의 반복 토큰을 포함하는 사전 훈련된 말뭉치에도 불구하고, 이 말뭉치에 사전 훈련된 모델은 특히 번체 중국어로 프롬프트될 때 텍스트 반복의 증가를 나타냈다.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Knowledge Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">지식 분석에서 <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.1">ARC</span>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.2">Hellaswag</span>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.3">MMLU</span>, <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.SS2.p1.1.4">C-eval-tw</span>의 네 가지 벤치마크에서 모델의 성능을 평가합니다. 이러한 벤치마크에 대한 모델 성능을 평가하기 위해 EleutherAI/lm-evaluation-harness<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/lm-evaluation-harness" title="">https://github.com/EleutherAI/lm-evaluation-harness</a></span></span></span><cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="#bib.bib7" title="">2021</a>)</cite>를 사용한다. 이러한 벤치마크는 객관식 문항으로 구성되어 있다. 정확도 계산은 확률이 가장 높은 옵션을 선택하는 것을 기반으로 합니다.</p>
</div>
<div class="ltx_para" id="A3.SS2.p2">
<ul class="ltx_itemize" id="A3.I2">
<li class="ltx_item" id="A3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i1.p1">
<p class="ltx_p" id="A3.I2.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i1.p1.1.1">ARC<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote4.1.1.1">4</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://allenai.org/data/arc" title="">https://allenai.org/data/arc</a></span></span></span></span></cite idx=0></cite>: A collection of natural, grade-school science questions. ARC 데이터 세트 내의 챌린지 세트에 대한 평가를 수행했다. 우리는 길이 정규화 정확도를 기반으로 성능을 평가하는 25샷 프롬프트를 사용하여 이 벤치마크를 수행했다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i2.p1">
<p class="ltx_p" id="A3.I2.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i2.p1.1.1">Hellaswag<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote5.1.1.1">5</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://rowanzellers.com/hellaswag" title="">https://rowanzellers.com/hellaswag</a></span></span></span></span></span></cite idx=0></cite>: An evaluation of commonsense inference, presenting a task that is straightforward for human but poses a challenge for the state-of-the-art models. 우리는 길이 정규화 정확도를 기반으로 성능을 평가하는 10샷 프롬프트를 사용하여 이 벤치마크를 수행했다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i3.p1">
<p class="ltx_p" id="A3.I2.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i3.p1.1.1">MMLU<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote6.1.1.1">6</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/hendrycks/test" title="">https://github.com/hendrycks/test</a></span></span></span></span></span></cite idx=0></cite>: A test for a text model's multitasking accuracy, covering 57 tasks from elementary math to U.S history, computer science, law and beyond. 우리는 개별 작업에 대한 정확도를 평균화하여 메트릭을 계산하는 5샷 프롬프트를 사용하여 이 벤치마크를 수행했다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i4.p1">
<p class="ltx_p" id="A3.I2.i4.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I2.i4.p1.1.1">C-eval-tw</span>: C-eval<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cevalbenchmark.com" title="">https://cevalbenchmark.com</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="#bib.bib14" title="">2023</a>)</cite>는 중국어 문맥에서 기초 모델의 고급 지식 및 추론 능력을 평가하는 테스트 역할을 한다. 테스트는 처음에 단순화된 중국어에서 수행되었으며 딥-트랜슬레이터<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nidhaloff/deep-translator" title="">https://github.com/nidhaloff/deep-translator</a></span></span></span> 패키지의 Google Translate <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="#bib.bib34" title="">2016</a>)</cite> API를 사용하여 번체 중국어로 번역했다. 우리는 개별 작업에 대한 정확도를 평균화하여 메트릭을 계산하는 0샷 프롬프트를 사용하여 이 벤치마크를 수행했다.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Reliability Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">신뢰성 분석에서는 진실성, 독성 및 편향을 포함한 세 가지 벤치마크 데이터 세트에서 모델의 성능을 확인한다. 우리는 영어와 번체 중국어 모두에서 이 분석을 수행한다. 이러한 벤치마크는 영어로 되어 있지만 심층 번역기 패키지의 Google Translate API를 사용하여 번체 중국어로 된 데이터 세트를 평가하여 포괄적인 분석을 보장합니다.</p>
</div>
<div class="ltx_para" id="A3.SS3.p2">
<ul class="ltx_itemize" id="A3.I3">
<li class="ltx_item" id="A3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I3.i1.p1">
<p class="ltx_p" id="A3.I3.i1.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I3.i1.p1.1.1">TruthfulQA<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote9.1.1.1">9</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/sylinrl/TruthfulQA" title="">https://github.com/sylinrl/TruthfulQA</a></span></span></span></span></cite idx=0></cite>: 언어 모델의 진실성을 측정하기 위해 활용되는 데이터셋. 이 데이터 세트는 잘못된 신념이나 오개념을 가진 개인으로부터 잘못된 응답을 이끌어내기 위해 고안된 질문으로 구성된다. 이 분석에서는 또한 이 벤치마크를 수행하기 위해 EleutherAI/lm-평가-harness를 사용한다. 우리는 6-shot 프롬프트를 사용하여 벤치마크를 수행한다. 채점 메커니즘은 질문 및 다수의 참/거짓 참조 답변들을 포함하며, 여기서 점수는 참 답변들의 세트에 할당된 정규화된 총 확률에 의해 계산된다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I3.i2.p1">
<p class="ltx_p" id="A3.I3.i2.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I3.i2.p1.1.1">ToxiGen<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote10.1.1.1">10</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/microsoft/TOXIGEN" title="">https://github.com/microsoft/TOXIGEN</a></span></span></span></span></cite idx=0></cite>: 언어 모델의 독성을 탐지하기 위해 사용한 데이터 세트입니다. 데이터세트는 13개의 별개의 소수 그룹과 관련된 독성 및 양성 진술을 포함하는 기계 생성 데이터세트이다. 대상 인구 통계 그룹에서 주석자가 동의하지 않는 프롬프트를 제외하여 노이즈를 완화시키는 정제된 데이터 세트 <span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/microsoft/SafeNLP" title="">https://github.com/microsoft/SafeNLP</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Hosseini et al. (<a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite>를 채택한다. 우리는 이러한 진술을 우리의 프롬프트로 받아들입니다. 효율성을 높이기 위해 vLLM을 활용합니다. 정렬 작업을 거친 모델의 경우 프롬프트를 "[INST] <컨텍스트> [/INST]"로 설정합니다. max_tokens 설정 512로 모델을 구성하고 핵 샘플링을 활용하여 온도를 0.1, top_p를 0.9로 설정한다. 독성 세대 식별을 위해 기본 RoBERTa 기반 분류기 ToxiGen <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="#bib.bib23" title="">2019</a>)</cite>를 활용한다. 분류기가 영어 텍스트를 처리하도록 설계되었기 때문에, 우리는 평가하기 전에 구글 번역기 API를 사용하여 모델의 출력을 영어로 번역함으로써 이 제약을 해결한다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I3.i3.p1">
<p class="ltx_p" id="A3.I3.i3.p1.1"><span class="ltx_text ltx_font_typewriter ltx_font_bold" id="A3.I3.i3.p1.1.1">Bold<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif ltx_font_medium" id="footnote12.1.1.1">12</span></span><a class="ltx_ref ltx_url ltx_font_medium" href="https://github.com/amazon-science/bold" title="">https://github.com/amazon-science/bold</a></span></span></span></span></cite idx=0></cite>: 바이어스 분석을 위해 사용하는 데이터셋입니다. 이 편향된 데이터 세트는 인종, 성별, 종교, 정치 이념 및 직업의 5개 영역에 걸쳐 위키피디아 프롬프트로 구성된다. 또한 효율성을 높이기 위해 vLLM을 활용합니다. 우리는 제한된 수의 프롬프트로 인해 종교 이데올로기 하위 그룹인 힌두교와 무신론에 속하는 프롬프트를 제외한다. 정렬 작업을 거친 모델의 경우 프롬프트를 "[INST] <컨텍스트> [/INST]"로 설정합니다. Max_tokens 설정 512로 모델을 구성하고 핵 샘플링을 활용하여 온도를 0.1, top_p를 0.9로 설정한다. VADER(Valence Aware Dictionary and Sentiment Reasoner) <cite class="ltx_cite ltx_citemacro_cite">Hutto and Gilbert (<a class="ltx_ref" href="#bib.bib15" title="">2014</a>)</cite>를 사용하여 결합된 프롬프트와 생성 텍스트에 대한 감성 점수를 계산한다. 또한 VADER를 사용하여 감성 점수를 계산하기 전에 Google Translator API를 사용하여 모델의 출력을 영어로 번역한다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
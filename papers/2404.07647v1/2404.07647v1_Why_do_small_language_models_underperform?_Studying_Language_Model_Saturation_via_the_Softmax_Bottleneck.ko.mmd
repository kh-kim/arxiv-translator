# 작은 언어 모델은 왜 성능이 저하됩니까?

Softmax 병목현상을 통한 LM 포화도 연구

 Nathan Godey\({}^{1,2}\), Eric de la Clergerie\({}^{1}\) & Benoit Sagot\({}^{1}\)

Inria Paris, \({}^{2}\) Sorbonne Universite

Paris, France

nathan.godey@inria.fr

###### Abstract

최근 언어 모델링의 발전은 매우 큰 웹 마이닝 텍스트 말뭉치에서 고도로 매개변수화된 신경망을 사전 훈련하는 데 있다. 이러한 모델을 사용한 훈련 및 추론은 실제로 비용이 많이 들 수 있으며, 이는 더 작은 대응물의 사용을 장려한다. 그러나 더 작은 모델은 훈련의 일부 고급 지점에서 성능이 떨어지고 안정기가 뒤따르는 것으로 특징지어지는 포화 상태를 겪을 수 있음이 관찰되었다. 본 논문에서는 이러한 포화가 더 작은 모델의 은닉 차원과 목표 상황 확률 분포의 높은 순위 사이의 불일치로 설명될 수 있음을 발견한다. 이러한 불일치는 잘 알려진 소프트맥스 병목 현상을 통해 이러한 모델에 사용되는 선형 예측 헤드의 성능에 영향을 미친다. 다양한 환경에서 소프트맥스 병목 현상의 영향을 측정하고 1000개 미만의 숨겨진 차원을 기반으로 하는 모델이 후기 사전 훈련에서 퇴행성 잠재 표현을 채택하는 경향이 있어 평가 성능이 감소한다는 것을 발견했다.

## 1 Introduction

표현 퇴화 문제는 텍스트 데이터에 사용되는 자기 지도 학습 방법(Gao et al., 2019; Lai et al., 2023), 다른 모달리티들(Jing et al., 2022; Godey et al., 2024)에 영향을 미치는 일반적인 현상이다. 언어 모델(LMs)의 중간 표현에 대한 많은 관찰은 낮은 각도 변동성(또는 _이방성_) 또는 훈련 중에 나타난 이상치 차원에 대해 조명했다(Zhou et al., 2021; Rajee & Pilehvar, 2022). 그러나 이러한 관찰은 대부분 BERT(Devlin 등, 2019)에 필적하는 차원의 비교적 소규모 모델 또는 GPT-2 계열(Radford 등, 2019)의 모델에 대해 이루어졌다.

이 모델들은 일반적으로 토큰의 시퀀스 \((y_{<i})\in[1,V]^{i-1}\)를 입력으로 하고 \(\mathbb{R}^{d}\)에서 비교적 낮은 차원의 문맥 표현을 생성하는 신경망 \(f_{\theta}\)으로 구성된다. 여기서 \(d\)는 모델의 _은닉 차원_이다. 그런 다음 컨텍스트 토큰 확률에 대한 로짓이 생성되는 _언어 모델링 헤드_ 에 의존합니다. 언어 모델링 헤드의 일반적인 선택은 매개변수 \(W\in\mathbb{R}^{V\times d}\)가 있는 선형 레이어이며, 여기서 \(V\)는 가능한 토큰의 수이다. 그 다음, 결과적인 다음-토큰 확률 분포는 다음과 같이 주어진다:

\[p(y_{i})=\sigma(Wf_{\theta}(y_{<i}))\]

여기서 \(\sigma\)는 소프트맥스 함수이다.

언어 모델링 분야에서, 현재의 트렌드는 GPT-2와 함께 도입된 생성적 프리트레이닝 접근법을 스케일링 업하는 것으로 구성되며, 이는 거대한 웹-마이닝된 텍스트 코퍼라 상에서 수십억 개의 파라미터들로 이루어진 트레이닝 신경 모델들을 암시한다(Brown et al., 2020; Touvron et al., 2023; Almazrouei et al., 2023; Jiang et al., 2023). 그러나, 이러한 고도로 파라미터화된 모델들을 트레이닝하고 서비스하는 것은 에너지 및 하드웨어 관련 문제들을 상승시키며, 이는 더 작은 모델들로 유사한 성능 레벨들을 달성하는 것을 조사하도록 동기부여한다(Sardana & Frankle, 2023).

그럼에도 불구하고, 피티아 모델 제품군(Biderman et al., 2023)의 평가는 매우 큰 말뭉치에 작은 모델을 훈련하면 늦은 사전 훈련에서 성능 저하의 형태로 _포화_로 이어질 수 있음을 보여주었다. 본 논문에서는 이러한 포화 현상을 표현 변성의 렌즈를 통해 탐색하고, 두 현상 모두 강한 상관관계를 갖는다는 것을 발견한다. 또한 표현 변성이 작은 모델의 언어 모델링 헤드에서 강하게 발생한다는 것을 증명하고, 선형 언어 모델링 헤드가 작은 은닉 차원을 기반으로 하는 아키텍처에 대한 성능 병목 현상을 어떻게 나타낼 수 있는지 이론적으로 그리고 경험적으로 보여준다.

전반적으로 우리의 기여는 다음과 같이 요약할 수 있다.

* 스케일링 법칙의 평가 및 외삽을 통해 작은 언어 모델의 성능 포화를 특성화하고;
* 더 작은 모델의 표현이 이 포화와 동시에 퇴화함을 발견합니다. 우리는 _순위 포화_, 즉 작은 LM 예측 헤드의 특이값 분포의 엔트로피의 폭발을 조명한다.
* 대상 컨텍스트 분포의 순위가 일반적으로 높은지 경험적으로 확인합니다. 또한 모델의 출력 표현에 관계없이 선형 헤드 \(W\)가 \(rank(W)<1000\)일 때 성능에 실질적으로 영향을 미친다는 것을 관찰한다.
* 저순위 선형 언어 모델링 헤드에 의해 유도된 성능 제한을 이론적으로 정량화한다.

## 2 관련 작업

Small LMs & SaturationBiderman et al. (2023)은 Pile(Gao et al., 2020)로부터 300B 토큰에 다양한 크기의 모델 제품군인 Pythia를 훈련시키고 훈련 중에 철저한 수의 체크포인트에 대한 가중치를 방출한다. 그들은 소규모 모델이 늦은 훈련에서 람바다 데이터 세트(Paperno et al., 2016)에서 성능 감소를 겪는다는 것을 알아챘다. 스케일링 법칙들(Kaplan et al., 2020; Hoffmann et al., 2022)은 큰 코퍼러스에 대한 더 작은 모델들을 트레이닝하는 것이 컴퓨팅 측면에서 차선책이라고 예측한다. 그러나, 최근의 이니셔티브들(Zhang et al., 2024; Faysse et al., 2024; Team et al., 2024)은 추론 비용 절감에 의해 동기화된, 큰 데이터세트 상의 더 작은 언어 모델들을 사전 트레이닝하였다(Sardana & Frankle, 2023).

Softmax Bottleneck _softmax 병목_ 개념은 Yang et al.(2018)에 소개되었는데, 저자들은 문맥 확률 행렬의 순위보다 열등한 은닉 차원을 사용하는 모델이 모든 문맥에서 올바르게 예측할 수 없음을 보여준다. 그런 다음 그들은 이 순위가 자연 언어에서 상대적으로 높다는 가설을 세우고 언어 모델의 예측 계층에 대한 대안적인 방법을 제안한다. 이후 연구에서는 소프트맥스 선형 계층이 언어 모델링 성능(Chang & McCallum, 2022)과 가능한 대안(Lin, 2021; Kanai et al., 2018)에 미치는 부정적인 영향을 조사했다. 우리는 소프트맥스 병목 현상과 관련된 임계 치수를 정량화하여 이 작업 라인을 확장한다.

RepresentationDegeneration은 사전 학습된 모델들이 낮은 엔트로피 특이값 분포를 채택하는 경향이 있는 현상이다(Jing et al., 2022). 언어 모델링에서 표현 변성은 이방성(Ethayarajh, 2019; Rajee & Pilehvar, 2021)의 형태를 취하며 Zipfian 형태의 토큰 분포(Gao et al., 2019; Bis et al., 2021)와 관련이 있음이 입증되었다. 우리는 훈련과 함께 이 현상과 포화도와의 관계를 연구한다.

Data Dimensionality and PerformanceSharma & Kaplan (2022)는 Intrinsic Dimension (Camastra & Staiano, 2016)의 렌즈를 통해 사전 훈련된 모델에서 관찰된 스케일링 법칙을 데이터 차원과 연결한다. 이들은 특이값 분해(Singular Value Decomposition, SVD)가 보편적인 근사 패러다임에서 데이터 다양체의 차원을 연구하는데 적합하지 않다는 것을 보여주지만, 우리는 입력 표현의 차원에 의해 제한되는 선형 분류기의 성능을 연구할 때 어느 정도 적합하다고 주장한다.

## 3 언어 모델 포화

우리는 먼저 광범위한 모델 크기에 대해 공개된 유일한 중간 체크포인트이기 때문에 피티아 체크포인트에 대한 성능 포화를 실제로 관찰하고 정량화할 수 있음을 확인한다. 사전 훈련 데이터 세트, 즉 Pile(Gao et al., 2020)에서 무작위로 샘플링된 50k 토큰에 대해 피티아 체크포인트의 교차 엔트로피를 측정한다.

그림 0(a)에서 410M 매개변수까지의 모델이 고급 훈련 단계에서 도메인 내 손실의 증가로 특징지어지는 포화 현상을 겪는 것을 분명히 볼 수 있다.

그림 0(b)에서 우리는 410M 매개변수 범위의 모델의 데이터 포인트에 Hoffmann 등(2022)의 스타일로 스케일링 법칙을 적합시키고 모델 관련 상수(\(A\) 및 \(\alpha\))에만 최적화하면서 다른 모든 값(\(B=410.7\), \(\beta=0.28\), \(E=1.69\))을 재사용한다. 우리는 Hoffmann et al. (2022)에서 주어진 파라미터 카운트 \(N\)와 토큰 카운트 \(T\) 사이의 관계를 리콜한다:

\[L(N,T)=\frac{A}{N^{\alpha}}+\frac{B}{T^{\beta}}+E\]

최적 파라미터는 \(A=119.09\)와 \(\alpha=0.246\)임을 알 수 있었다. 최상의 체크포인트와 최종 체크포인트에 해당하는 토큰 카운트에 대한 적합 곡선을 표시합니다. 우리는 최종 검문소가 평균 8%의 외삽을 과소 수행한다는 것을 관찰한다. 불완전한 학습률 냉각으로 인해 외삽에 미치지 못할 것으로 예상되는 손실 최소화(_best_) 체크포인트는 약 4%의 성능 저하에 그친다.

표 1에 나타낸 바와 같이, LM 평가 Harness(Gao 등, 2023)에서 평가에 사용된 데이터 세트에서도 유사한 성능 포화가 관찰된다.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Checkpoint & Lambada (ppl) \(\downarrow\) & Lambada \(\uparrow\) & StoryCloze \(\uparrow\) & WikiText (ppl.) \(\downarrow\) & SciQ \(\uparrow\) & ARC-e \(\uparrow\) \\ \hline Best & **24.6** & **40.3** & **59.6** & **30.47** & **79.6** & **46.5** \\ Final & 32.9 & 38 & 57.2 & 33.4 & 73.4 & 43.2 \\ \hline \hline \end{tabular}
\end{table}
표 1: 평가 데이터 세트에 대한 피티아-160M 베스트 및 최종 체크포인트의 제로샷 성능. 명시되지 않는 한 모든 작업에 대한 정확성을 보고합니다.

그림 1: 파일에서 피티아 모델의 성능입니다. 왼쪽에서는 최소값에 접근할 때 더 어두운 음영을 표시하는 14M(상단)에서 410M(하단) 매개변수까지의 모델의 훈련 역학을 비교한다. 오른쪽에서 우리는 더 큰 모델에 멱법칙에 적합하고 더 작은 모델의 최종 체크포인트가 예측에 비해 성능이 낮다는 것을 발견한다.

성능 포화는 랭크 포화이다.

### 스케일 비등방성

이방성은 다양한 작은 언어 모델들 사이에서 관찰된 대표적인 표현 변성의 형태이다. 그것은 주어진 층에서 표현 분포의 감소된 각도 가변성으로 구성된다. 이전 작업들(Ethayarajh, 2019; Godey et al., 2024)은 소형 트랜스포머 언어 모델들의 거의 모든 층들이 이방성임을 주목한다. 벡터 표현의 집합 \(H\)에서 이방성에 대한 공통 척도는 평균 코사인-유사성이다:

\[\mathcal{A}(H)=\frac{1}{|H|^{2}-|H|}\sum_{h_{i},h_{j}\in H,i\neq j}\frac{h_{i} ^{T}h_{j}}{||h_{i}||_{2}\cdot||h_{j}||_{2}}}\

그러나 비등방성이 10억 개 이상의 매개변수를 가진 모델에 영향을 미치는지 여부는 불분명하다. 이 질문을 해결하기 위해, 우리는 모델들의 스위트들에서 층들에 걸친 중간 표현들의 평균 코사인-유사성, 즉 GPT-2(Radford et al., 2019), OPT(Zhang et al., 2022), Pythia(Biderman et al., 2023), 및 Gemma(Team et al., 2024)를 계산한다. 이 데이터 세트의 도메인이 이러한 스위트에 사용된 사전 훈련 데이터 세트의 도메인을 포함하거나 일치한다고 가정하기 때문에 The Pile(Gao 등, 2020)의 하위 샘플을 사용한다.

그림 2에서 우리는 트랜스포머 모델의 대부분의 층이 스케일에 관계없이 어느 정도 이방성임을 관찰한다. 그럼에도 불구하고 모델이 거의 등방성이거나 매우 이방성인 마지막 층에는 이분법이 있는 것으로 판단된다. 흥미로운 사실은 이분법이 160M 이하의 매개변수를 포함하는 모델만이 마지막 층 이방성에 의해 영향을 받는 것처럼 보이는 피티아 제품군의 포화 현상 중 하나와 일치한다는 것을 알 수 있다.

따라서 우리는 피티아 제품군에 대한 이방성의 훈련 역학을 연구하고 그림 3의 포화 현상과 비교하기로 결정했다.

그림 3은 모델의 마지막 계층 표현에서 성능 포화 현상의 출현과 이방성의 출현 사이의 깔끔한 상관 관계를 보여준다. 또한 훈련시 포화점을 중심으로 비등방성이 급격히 증가하는 것을 확인할 수 있었다. 더욱이, 우리는 여기서 특정 도메인 내 말뭉치에서 모델이 포화 상태에서 빠르게 성능을 잃고 이 폭발에서 완전히 회복되지 않는 것으로 보인다는 것을 볼 수 있다.

도 2: 층 깊이의 함수에서의 이방성(즉, 순방향 패스에서의 차수).

### 특이값 포화

평균 코사인 유사도는 분포의 균일성에 대한 가치 있는 척도이지만, 다른 메트릭을 포함하면 일부 다양체의 복잡성을 더 잘 포착하는 데 도움이 될 수 있다(Rudman et al., 2022). 또한, 언어 모델의 출력 임베딩에만 초점을 맞추고 가중치에는 초점을 맞추지 않는다. 이 섹션에서는 언어 모델링 헤드의 특이값 분포를 연구하여 분석을 확장하여 경험적 관찰을 이론적 결과와 연결한다. 그림 4에서 우리는 훈련에 따라 최종 예측 계층 가중치 \(W\)의 특이값 분포를 표시한다.

그림 4: 최대 특이값으로 정규화된 훈련 중 피티아 모델의 LM 헤드의 특이값 분포의 진화.

그림 3: LM Evaluation Harness(Gao et al., 2023)로부터 Wikipedia 테스트 세트에 대한 언어 모델링 성능의 진화 및 훈련(색상)에 따른 피티아 모델의 마지막 계층 이방성.

그림 4는 스펙트럼 포화의 특정 패턴을 조명하여 성능 포화 현상과 대략적으로 동시 발생한다. 이는 특이값 분포가 훈련 중에 점진적으로 평평해지고 다른 것에 비해 최대 특이값이 높은 스파이크 분포로 갑자기 진화하기 전에 거의 균일성에 도달한다는 것을 보여준다.

이 동작을 보다 정확하게 정량화하기 위해 정규화된 특이값 분포와 균일 분포 사이의 쿨백-라이블러 발산으로 계산된 _특이 엔트로피 메트릭_을 사용한다.

그림 5는 특이 분포가 더 큰 모수보다 410M 미만의 모수를 사용하는 모형에 대해 다르게 진화하는 것을 보여준다. 소형 모델의 헤드는 특이값 분포가 급격히 퇴보하는 지점까지 점점 균일해지는 것을 보고, 이는 다시 LM 성능 저하와 상관관계가 있다. 더 큰 모델의 특이값 분포는 더 안정적인 경향이 있으며 훈련 전반에 걸쳐 명확한 단조 패턴을 표시하지 않는다.

## 5 Softmax Bottleneck & 언어 차원

### 자연 언어의 내재적 차원

직관적으로 섹션 4.2의 더 작은 모델에 대해서만 관찰된 특이값 분포의 포화는 LM 헤드의 최적화에 관련된 차원성에 의문을 제기한다. 본 절에서는 LM 헤드의 순위에 대한 임계값을 경험적으로 측정하고 헤드의 출력이 일치해야 하는 상황적 확률 분포의 차원을 추정하는 것을 제안한다.

선형 헤드의 랭크의 영향을 경험적으로 측정하기 위해, 우리는 고-파라미터화된 언어 모델들로부터 사전 트레이닝된 컨텍스트 표현들에 대해 랭크-제약된 헤드들을 트레이닝하는 것을 제안한다. 최대 순위 \(r\)를 제어하기 위해, \(W=AB\in\mathbb{R}^{V\times d}\) 형태의 헤드를 고려하며, 여기서 \(A\in\mathbb{R}^{V\times r}\) 및 \(B\in\mathbb{R}^{r\times d}\)의 계수는 \(\mathcal{N}(0,1)\)(\(d\) 모델의 숨겨진 차원)으로부터 도출된다. 이러한 \(W\) 행렬의 순위는 매개변수 \(r\in[1,d]\)에 의해 제한되며, 이는 광범위한 값을 스윕한다.

우리는 언어 모델을 동결하고 약 150M 토큰에 대한 출력 표현에 대해 순위 제한 헤드를 훈련하는 동시에 학습 속도를 훈련 가능한 매개변수 수(부록 B의 더 자세한 내용)로 조정한다.

그림 5: 서로 다른 피티아 모델에 대한 특이 엔트로피의 훈련 역학.

그림 6: 헤드의 병목 차원이 증가함에 따라 여러 모델의 성능.

그림 6에서 우리는 언어 모델링 헤드 \(W\)의 순위가 모델 크기에 관계없이 _1000보다 열등할 때 복잡성이 눈에 띄게 감소하기 시작하는 것을 관찰한다. 이는 헤드가 숨겨진 차원이 더 큰 모델의 경우 주요 성능 병목 현상이 아니라 출력 표현의 품질과 무관하게 더 작은 모델의 성능이 손상될 수 있음을 암시한다.

추정한 또 다른 흥미로운 요소는 데이터 자체에 내재된 차원이다. 특정 귀납적 편향과 관련된 가능한 영향을 피하기 위해 다양한 범위의 데이터 세트(IMDb(Maas et al., 2011), Wikitext(Merity et al., 2016) 및 The Pile(Gao et al., 2020))에 대해 다양한 어휘 크기의 두 토큰자(Llama-2의 경우 30k 토큰 및 피티아의 경우 50k 토큰)를 사용하여 나이브 5-그램 언어 모델을 훈련한다. 5-gram이 관측된 \(\mathcal{C}\)이 주어진 경우, 주어진 4-token 문맥에서 각 행이 가능한 토큰에 대한 확률 분포인 행렬 \(W\in\mathbb{R}^{C\times V}\)을 고려하고 Terashima et al. (2003)에서와 같이 특이값 분포를 계산한다. 그림 7에서 우리는 \(W\)-_error_, \(W\)의 Frobenius norm으로 정규화된 Eckart-Young-Mirsky 정리(Lemma 5.2 참조)에 의해 예측된 순위 \(d\)의 행렬에 대한 \(W\)의 최소 근사 오차를 보고한다.

\[W\text{-error}(d)=\frac{||\sigma_{d+1}||_{2}}{||W||_{F}}\]

우리는 \(W\)의 추정 순위가 일반적인 은닉 차원의 크기에 대해 무시할 수 없음을 발견한다. 다음 절에서는 이상적인 선형 언어 모델링 헤드의 차원성과 성능 간의 연관성을 이론적 관점에서 분석한다.

### 이론적 병목 현상

이 절에서는 문맥 분포의 고유한 차원과 언어 모델의 출력 표현의 하위 차원에 기인할 수 있는 성능 병목 현상 사이의 공식적인 연결을 식별하는 것을 목표로 한다. 이를 위해, 우리는 _이상적_ 맥락적 표현에 최적화된 언어 모델링 헤드를 개념화하고, 그 스펙트럼 특성과 동일한 표현에서 저순위 헤드를 훈련할 때 유도된 성능 격차 사이의 관계를 탐구한다.

사전 훈련 데이터를 나타내는 크기 \(V\)의 어휘에서 가져온 원소의 시퀀스 \((y_{i})_{i\in[1,|y|]}\) 집합 \(\mathcal{T}\)을 생각해 보자. 우리는 주어진 맥락(y_{<i}\)을 완전히 나타내는 함수 \(\phi^{*}\)를 무한 차원의 단일 실수 벡터로 간주한다. 우리는 \(\phi^{*}\)에 초점을 맞추지 않기 때문에 맥락적 표현 \(x_{i}^{*}=\phi^{*}(y_{<i})\)을 도입하여 기호를 단순화할 수 있다.

그림 7: 다른 토큰라이저와 데이터 세트에 대해 \(d\)가 증가함에 따라 \(W\)-오류입니다. 우리는 W-오류가 1000 또는 2000 차원을 사용하여 절반으로 줄어들 수 있지만 10,000-15,000 차원 이후에는 무시할 수 있다는 것을 관찰한다.

선형 언어 모델링 헤드의 작업은 행렬 \(W\): 최적화 문제로 공식화될 수 있다.

\[W^{*}=\operatorname*{argmin}_{W\in\mathbb{R}_{V\times\infty}}\sum_{y\in\mathcal{T}}\sum_{i}\mathcal{L}(W,x_{i}^{*},y_{i}) \tag{1}\]

여기서, \(\mathcal{L}\)는 다음과 같이 소프트맥스 함수 \(\sigma\)를 사용하여 정의된 교차 엔트로피 목적이다:

\[\mathcal{L}(W,x,y)=-\log(\sigma(Wx)_{y})\]

실제로 신경언어 모델 \(\phi_{\theta}\)은 차원 \(d\in\mathbb{N}^{*}\)의 맥락적 표현 \(x_{i}=\phi_{\theta}(y_{<i})\)을 생성한다. 선형 언어 모델링 헤드 \(W_{\theta}\in\mathcal{R}^{V\times d}\)는 식 1과 같은 목적으로 \(\phi_{\theta}\)와 동시에 훈련된다.

우리는 저차원 헤드의 최대 표현성에 초점을 맞춘다: _완전한_ 문맥 표현 \(x_{i}^{*}\)이 제공될 때, 최대 랭크 \(d\)의 선형 언어 모델링 헤드의 최대 성능 수준은 무엇인가? 이 질문은 수학적인 용어로 표현될 수 있다.

\[W_{d}^{*}=\operatorname*{argmin}_{W\in\mathbb{R}_{V\times\infty}}\sum_{y\in \mathcal{T}}\sum_{i}\mathcal{L}(W,x_{i}^{*},y_{i})\text{ s.t. }rank(W)\leq d \tag{2}\]

Lemma 5.1은 \(W^{*}\)에 직접 접근함으로써 점근적으로 성능 격차를 줄일 수 있음을 보여준다.

**Lemma 5.1**: _(부록 A.1의 증명) \(W\in\mathbb{R}^{V\times\infty},M\in\mathcal{H}^{V\times\infty}\) Frobenius norm \(||\cdot||_{F}\) 및 \(\varepsilon\in\mathbb{R}^{*}_{+}\)에 대한 행렬 단위 구를 \(W=W^{*}+\varepsilon M\)로 간주합니다. \(\varepsilon\to 0\)일 때:_

\[|\mathcal{L}(W,x_{i}^{*},y_{i})-\mathcal{L}(W^{*},x_{i}^{*},y_{i})|=O(\varepsilon)\]

따라서 본 논문에서는 스펙트럼 이론과 직접적으로 연결된 저순위 행렬 근사법(Kumar & Schneider, 2017)을 제안한다. 우리의 경우, 우리는 에카트-영-머스키 정리를 사용할 수 있다.

**Lemma 5.2**: _(Eckart-Young-Mirsky 정리) \((\sigma_{i})\) \(W^{*}\)의 특이값을 내림차순으로, \(\mathcal{M}_{d}\) \(\mathbb{R}^{V\times\infty}\) 순위의 행렬 집합 \(d<V=rank(W^{*})\)을 고려하자. 그러면:_

\[\min_{W_{d}\in\mathcal{M}_{d}}||W_{d}-W^{*}||_{F}=\sqrt{\sum_{i=d+1}^{V} \sigma_{i}^{2}}\]

위의 모든 것을 합치면 정리 5.3이 나온다.

**정리 5.3**: _(부록 A.2의 증명) \((\sigma_{i})\) \(W^{*}\)의 특이값을 내림차순으로 고려해 보겠습니다. 그 다음, \(d\to V\)일 때, 선형 LM 헤드 상의 \(d\)-차원 병목 현상에 의해 유도된 손실 갭은 다음과 같다:_

\[\sum_{y\in\mathcal{T}}\sum_{i}\mathcal{L}(W_{d}^{*},x_{i}^{*},y_{i})- \mathcal{L}(W^{*},x_{i}^{*},y_{i})=O\left(\sqrt{\sum_{i=d+1}^{V}\sigma_{i}^{2}}\right)\]

이러한 특성은 LM 헤드가 낮은 순위일 때 이상적인 언어 모델링 헤드의 차원이 성능에 어떤 영향을 미치는지 보여준다. 그러나 정리 5.3에서 얻은 관계는 부록 A.2에서 논의한 것처럼 특별히 강하지 않다.

그림 8에서는 5.1절의 피티아-1B 모델의 머리 병목 실험 결과를 병목 차원 \(d\)이 진화하는 것과 동일한 모델의 머리에서 \(W\)-오류와 비교한다. 이는 \(W\)-오차와 함께 손실 갭이 천천히 증가한다는 것을 보여주며, 허용 순위가 \(W\)의 열악한 근사치로 이어질지라도 성능은 여전히 허용될 수 있음을 암시한다. 우리는 \(W\)-오류가 0.6보다 커지면 성능이 감소하기 시작한다는 것을 알 수 있다.

## 6 Discussion

당면한 문제를 해결하는 한 가지 방법은 얕은 작은 언어 모델을 훈련하여 레이어 카운트 또는 피드포워드 차원과 같은 다른 하이퍼파라미터를 희생시키면서 숨겨진 차원을 증가시키는 것일 수 있다. 그러나 우리는 이러한 맥락에서 그러한 연구 방향이 유망하지 않을 수 있다고 믿는다. 이전 연구에서는 다양한 아키텍처 크기에 대한 하이퍼파라미터 선택을 광범위하게 탐색하고 최적화했다. 폭과 깊이의 영향은 광범위하게 연구되어 왔다(Merrill et al., 2022; Tay et al., 2022; Petty et al., 2023). 종종 최종 성능 및 일반화 능력에서 깊이의 중요성을 보여준다.

또 다른 가능한 방법은 큰 데이터 세트에 작은 언어 모델을 사전 훈련하는 맥락에서 보다 표현적인 소프트맥스 대안(Yang et al., 2018; Chang and McCallum, 2022)을 구현하는 것으로 구성된다. 우리는 그러한 기술에 대한 탐구는 향후 작업을 위해 남겨둔다.

또한 섹션 4.2에서 설명하는 붕괴 후 특이 성분의 특정 특성에 대한 추가 탐사가 LM 포화에 대한 이해도를 향상시킬 수 있다고 믿는다. 토큰 주파수(Gao et al., 2019; Ethayarajh, 2019; Bis et al., 2021)와 이방성을 연결하는 이전 연구를 기반으로 결과적인 지배적인 구성 요소가 토큰 주파수와 상관관계가 있다는 가설을 세우고 LM 헤드 메커니즘에서 토큰 주파수의 중요성을 보여준다(Meister et al., 2023).

이 기사의 범위를 넘어, 우리는 우리의 작업이 마지막 계층 이방성이 성능 포화의 증상이며 따라서 언어 모델의 바람직한 특성이 아닐 가능성이 있음을 입증한다고 주장한다. 우리는 또한 이 작업이 맥락적 확률 분포의 구조에 대한 더 나은 이해를 향한 길을 열어주며, 이는 스케일링 법칙에 대한 우리의 해석도 향상시킬 수 있다고 주장한다.

## Conclusion

소규모 언어 모델은 훈련 중 성능 포화에 영향을 받을 수 있다. 이 현상은 선형 언어 모델링 헤드를 통해 저차원 출력 표현 공간을 높은 순위 컨텍스트 확률 분포에 매핑하는 데 내재된 어려움으로 설명될 수 있음을 발견했다. 실제로, 우리는 더 작은 은닉 차원에 의해 유도된 성능 갭과 맥락적 확률 분포의 스펙트럼 특성 사이의 이론적 연결을 보여준다.

우리는 이러한 매핑의 순위가 일반적인 은닉 차원 선택에 비해 상대적으로 높을 것으로 예상할 수 있음을 경험적으로 확인한다. 또한, LM 헤드의 랭크 제한이 대형 언어 모델의 성능에 미치는 영향을 측정하기 위한 실험을 수행한다. 그 결과 1000 이하의 은닉차원을 사용할 경우 성능이 눈에 띄게 떨어짐을 알 수 있었다. 또한 분광분석의 렌즈를 통해 포화현상을 분석하였고, 작은 모델에만 영향을 미치는 최후층 이방성의 출현은 포화와 상관관계가 있음을 알 수 있었다. 또한 소형 모델의 LM 헤드가 _스펙트럴_ 포화, 즉 퇴화된 상태로 이어지는 특이 값의 균일화로 동시에 고통 받는다는 것을 보여준다.

우리의 연구는 언어 모델링에 대한 소프트맥스 병목 현상의 결과를 더 잘 이해하고 목표 확률 분포의 복잡성을 더 잘 수용하는 언어 모델의 개념을 위한 길을 열어준다.

그림 8: 피티아-1B 모델의 머리에서 순위 \(d\)에 대한 이론적 \(W\)-오류의 함수로서 훈련된 순위 제한 머리를 사용한 최종 손실( \(W^{s}_{d}\))입니다.

### Limitations

이 기사의 주요 한계는 우리가 연구한 비교적 적은 양의 포화 언어 모델이다. 광범위한 양의 중간 체크포인트를 방출하기 위해 관심 범위에서 훈련된 유일한 언어 모델 제품군이기 때문에 작은 피티아 모델의 훈련 역학만 관찰할 수 있었다. 가장 작은 GPT-2 모델에 대해 강력한 마지막 층 이방성을 관찰하지만 포화 상태를 겪었는지 확실하게 구별할 수 없다. OPT-125m 모델은 강한 최후층 이방성을 나타내지 않아 포화 현상에 영향을 받지 않았음을 나타낼 수 있다.

그럼에도 불구하고, 우리는 이 논문이 _모든_ 작은 모델이 포화를 겪어야 한다는 것을 보여주는 것이 아니라 작은 언어 모델의 포화가 상대적으로 작은 숨겨진 차원을 기반으로 하는 언어 모델에 영향을 미칠 수 있는 제한의 징후라고 주장한다.

이 작업의 또 다른 한계는 이상적인 언어 모델링 헤드의 차원과 순위 제한 성능 사이의 확립된 수학적 연결의 느슨한 특성이다(cf. 정리 5.3). 또한 _이상_\(x_{i}^{*}\) 표현을 고려하는 것은 잘못 정의된 개념이라고 주장할 수도 있다. 우리는 논증에서 \(x_{i}^{*}\)의 _이상적_ 속성이 필요하지 않기 때문에 정리 5.3 뒤에 있는 추론은 어떤 맥락적 표현에도 적용될 수 있다고 주장한다. 이데알이라는 단어는 주어진 훈련 집합 \(\mathcal{T}\)에 대한 \(W^{*}\) 행렬에 이러한 표상이 부과하는 구조에 의존하는 정도로 _기본 모델_로부터 얻은 \(x_{i}^{*}\) 표상에 대한 우리의 관찰이 유지된다는 것을 반영한다.

## Acknowledgements

이 글을 꼼꼼히 검토해주시고 소중한 제안을 해주신 송증 씨에게 감사드린다. 이 작업은 프랑스 국가 기관 ANR이 참조 ANR-19-P3IA-0001에 따라 "투자 다베니르" 프로그램의 일부로 자금을 지원한 PRAIRIE 연구소의 마지막 저자의 의장에 의해 지원되었다. 이 작업은 IDRIS(할당 2023-AD011013680R1)에서 운영하는 GENCI의 HPC 자원에 대한 액세스를 허용했다.

## References

* Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Rucandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badeddine Noune, Baptiste Pannier, and Guillherme Penedo. 팔콘 시리즈의 개방형 언어 모델, 2023.
* Biderman 등(2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Afah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In _International Conference on Machine Learning_, pp. 2397-2430. PMLR, 2023.
* Bis et al.(2021) Daniel Bis, Maksim Podkorytov, and Xiuwen Liu. 과도한 공통점: 변압기 언어 모델에서 임베딩의 이동과 그 의미. Kristina Toutanova에서, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkari-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Iannoy Chakraborty, and Yichao Zhou(eds.), _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pp. 5117-5130, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.403. URL [https://aclanthology.org/2021.naacl-main.403](https://aclanthology.org/2021.naacl-main.403).
* Brown 등(2021) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, AdityaRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Illya Sutskever, Dario Amodei. 언어 모델은 2020년에 거의 촬영되지 않은 학습자들이다.
* Camastra and Staiano (2016) Francesco Camastra and Antonino Staiano. 내재적 차원 추정: 진전과 열린 문제. _ 정보 과학_, 328:26-41, 2016. ISSN 0020-0255. doi: [https://doi.org/10.1016/j.ins.2015.08.029](https://doi.org/10.1016/j.ins.2015.08.029). URL [https://www.sciencedirect.com/science/article/pii/S0020025515006179](https://www.sciencedirect.com/science/article/pii/S0020025515006179).
* Chang and McCallum (2020) Haw-Shiuan Chang and Andrew McCallum. 소프트맥스 병목 현상은 언어 모델이 다중 모드 단어 분포를 나타낼 수 없게 만든다. Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 8048-8073, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.554. URL [https://aclanthology.org/2022.acl-long.554](https://aclanthology.org/2022.acl-long.554)
* Devlin 등(2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 언어 이해를 위한 딥 양방향 트랜스포머의 사전 훈련. Jill Burstein, Christy Doran, and Thamar Solorio(eds.), _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1(Long and Short Papers)_, pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).
* Ethayarajh (2019) Kawin Ethayarajh. 문맥화된 단어 표현은 어떻게 맥락화되어 있는가? BERT, ELMo 및 GPT-2 임베딩의 기하학을 비교한다. Inui Kentaro, Jing Jiang, Vincent Ng, and Xiaojun Wan(eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp. 55-65, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1006. URL [https://aclanthology.org/D19-1006](https://aclanthology.org/D19-1006).
* Faysse et al.(2024) Manuel Faysse, Patrick Fernandes, Nuno M. Guerreiro, Antonio Loison, Duarte M. Alves, Caio Corro, Nicolas Boizard, Joao Alves, Ricardo Rei, Pedro H. Martins, Antoni Bigata Casademunt, Francois Yvon, Andre F. T. Martins, Gautier Viaud, Celine Hudelot, Pierre Colombo. 크로와상틀름: 진정한 이중언어 프랑스어-영어 모델, 2024.
* Gao et al.(2019) Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tieyan Liu. 자연어 생성 모델의 학습에서 표현 퇴화 문제 2019년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=SkEYojRqtm](https://openreview.net/forum?id=SkEYojRqtm)입니다.
* Gao 등(2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 파일: 언어 모델링을 위한 다양한 텍스트의 800gb 데이터 세트, 2020.
* Gao 등(2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonnell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 소샷 언어 모델 평가를 위한 프레임워크, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
* Godey et al.(2024) Nathan Godey, Eric de la Clergerie, and Benoit Sagot. 이방성은 트랜스포머의 자기 주의, 2024에 내재되어 있다.
* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. 래, 오리올 빈일스, 로랑 시프르 컴퓨팅 최적화 대용량 언어 모델, 2022를 훈련합니다.
* Hovy et al.(2019)* Jiang et al.(2023) Albert Q. 장, 알렉산드르 사블레이롤, 아서 멘쉬, 크리스 뱀포드, 데벤드라 싱 채플롯, 디에고 드 라스 카사스, 플로리안 브레산드, 지안나 랭글, 기욤 람플, 루실라 사울니에, 렐리오 레나르 라부드, 마리 앤 라쇼, 피에르 스톡, 테벤 르 스카오, 티보 라브릴, 토마스 왕, 티모티 라크루아, 윌리엄 엘 세이드. 2023년 미스트랄 7b
* Jing et al.(2022) Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. 비교적 자기 지도 학습에서 차원 붕괴를 이해한다. 2022년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=YevsQ05DEN7](https://openreview.net/forum?id=YevsQ05DEN7)을 참조 하세요.
* Kanai 등 (2018) Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, and Shuichi Adachi. Sigsoftmax: 소프트맥스 병목 현상의 재분석. In S. 벤지오, H. 왈라치, H. 라로셸, K. 그라우만 Cesa-Bianchi, R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL [https://proceedings.neurips.cc/paper_files/paper/2018/file/9dcb88e0137649590b755372b04oadafad-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2018/file/9dcb88e0137649590b755372b04oadafad-Paper.pdf)
* Kaplan 등(2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. 신경 언어 모델의 법칙을 조정합니다. _ ArXiv_, abs/2001.08361, 2020. URL [https://api.semanticscholar.org/CorpusID:210861095](https://api.semanticscholar.org/CorpusID:210861095).
* Kumar and Schneider (2017) N. 키쇼어 쿠마르와 J. 슈나이더 행렬의 낮은 순위 근사에 대한 문헌 조사 _ Linear and Multilinear Algebra_, 65(11):2212-2244, 2017. doi: 10.1080/03081087.2016.1267104. URL [https://doi.org/10.1080/03081087.2016.1267104](https://doi.org/10.1080/03081087.2016.1267104).
* Lai 등(2023) Wen Lai, Alexandra Chronopoulou, and Alexander Fraser. 다국어 기계 번역에서 데이터 불균형 및 표현 변성을 완화합니다. Houda Bouamor, Juan Pino, and Kalika Bali(eds.), _Findings of the Association for Computational Linguistics: EMNLP 2023_, pp. 14279-14294, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.953. URL [https://aclanthology.org/2023.findings-emnlp.953](https://aclanthology.org/2023.findings-emnlp.953)
* Lin (2021) Ying-Chen Lin. 2021년 드롭아웃 및 디커플링이 있는 순차적 추천 시스템의 소프트맥스 병목 현상을 해결합니다.
* Maas et al.(2011) Andrew L. Maas, Raymond E. Daly, Peter T. 팜 단 황 앤드류 Y 응, 크리스토퍼 팟츠 감성 분석을 위한 단어 벡터를 학습합니다. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, pp. 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL [http://www.aclweb.org/anthology/P11-1015](http://www.aclweb.org/anthology/P11-1015).
* 마이스터 등(2023) 클라라 마이스터, Wojciech Stokowiec, Tiago Pimentel, Lei Yu, Laura Rimell, and Adhiguna Kuncoro. 언어 생성 모델에 대한 자연스러운 편견. Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki(eds.), _Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pp. 243-255, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.22. URL [https://aclanthology.org/2023.acl-short.22](https://aclanthology.org/2023.acl-short.22)
* Merity 등(2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 포인터 센티넬 혼합물 모델, 2016년
* Merrill et al.(2022) William Merrill, Ashish Sabharwal, and Noah A. Smith. 포화 변압기는 일정 깊이 임계값 회로입니다. _ Computational Linguistics_, 10:843-856, 2022의 트랜잭션. doi: 10.1162/tacl_a_00493. URL [https://aclanthology.org/2022.tacl-1.49](https://aclanthology.org/2022.tacl-1.49).
* Paperno 등(2016) Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. LAMBADA 데이터 세트: 광범위한 담화 컨텍스트를 요구하는 단어 예측. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1525-1534, Berlin, Germany, August. Association for Computational Linguistics. URL [http://www.aclweb.org/anthology/P16-1144](http://www.aclweb.org/anthology/P16-1144).
* Petty et al.(2023) Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, and Tal Linzen. 깊이와 너비가 변압기 언어 모델 일반화에 미치는 영향, 2023.
* Petty et al.(2020)* Puccetti et al.(2022) Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice Dell'Orletta. 변압기를 파괴하는 이상치 차원은 주파수에 의해 구동된다. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pp. 1286-1304, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL [https://aclanthology.org/2022.findings-emnlp.93](https://aclanthology.org/2022.findings-emnlp.93)
* Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 언어 모델은 감독되지 않은 다중 작업 학습자입니다. 2019년.
* Rajaee and Pilehvar (2021) Sara Rajaee and Mohammad Taher Pilehvar. 문맥 임베딩 공간에서 등방성을 개선하기 위한 클러스터 기반 접근법. 청칭종에서는 Fei Xia, Wenjie Li, and Roberto Navigli(eds.), _Proceedings of the 59th Annual Meeting of the Association of the Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)_, pp. 575-584, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.73. URL [https://aclanthology.org/2021.acl-short.73](https://aclanthology.org/2021.acl-short.73).
* Rajaee and Pilehvar (2022) Sara Rajaee and Mohammad Taher Pilehvar. 다국어 BERT 임베딩 공간에서의 등방성 분석 Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), _Findings of the Association for Computational Linguistics: ACL 2022_, pp. 1309-1316, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.103. URL [https://aclanthology.org/2022.findings-acl.103](https://aclanthology.org/2022.findings-acl.103).
* Rudman 등(2022) William Rudman, Nate Gillman, Taylor Rayne, and Carsten Eickhoff. IsoScore: 임베딩 공간 활용의 균일성 측정. Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), _Findings of the Association for Computational Linguistics: ACL 2022_, pp. 3325-3339, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.262. URL [https://aclanthology.org/2022.findings-acl.262](https://aclanthology.org/2022.findings-acl.262)
* Sardana and Frankle (2023) Nikhil Sardana and Jonathan Frankle. 친칠라를 넘어 최적: 언어 모델 스케일링 법칙에서 추론을 위한 회계, 2023.
* Sharma and Kaplan (2022) Utkarsh Sharma and Jared Kaplan. 데이터 다양체 차원에서 법칙을 조정합니다. _ Journal of Machine Learning Research_, 23(9):1-34, 2022. URL [http://jmlr.org/papers/v23/20-1111.html](http://jmlr.org/papers/v23/20-1111.html)
* Tay 등(2022) Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. 효율적으로 확장: 사전 훈련 및 미세 조정 변압기의 통찰력을 제공합니다. 2022년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=f20YVDyfIB](https://openreview.net/forum?id=f20YVDyfIB).
* 팀 등 (2021) 젬마 팀, 토마스 메스나드, 캐시 하딘, 로버트 다다시, 수리아 부파티라주, 슈레야 파탁, 로랑 시프레, 모르가리 리비에르, 미히르 산제이 케일, 줄리엣 러브, 야코 오스틴, 제임스 치유, 저스틴 카스트로-로스, 수리아 부파티라주, 슈레야 파탁, 로랑 시프레, 모르가리 리비에르, 미히르 산제이 케일, 줄리엣 러브, 제이콥 오스틴, 제임스 킬링, 제인 라바노프스키, 장-크리스티안 무라루, 이안 텐니, 이반 그리쉬첸코, 아디티아 바푸아, 로랑 시프레, 모르가리 리비에르, 미히르 산제이 케일, 줄리엣 러브, 제이콥 오스틴, 제임스 킬링, 제인 라바노프스키, 제인 라바노프스키, 장-크리스티안 무라루, 이안 텐니, 이반 그리쉬첸코, 아디티아 바푸아, 로랑 시프레, 미히르 산제이 케일 제마: 제미니 연구 및 기술을 기반으로 한 개방형 모델, 2024.
* Terashima et al.(2003) Shiro Terashima, Kazuya Takeda, and Fumitada Itakura. n-gram 행렬의 svd를 통한 언어 확률의 선형 공간 표현 _ 일본 전자 및 통신(Part III: Fundamental Electronic Science)_, 86(8):61-70, 2003. doi: [https://doi.org/10.1002/ecjc.10106](https://doi.org/10.1002/ecjc.10106). URL [https://onlinelibrary.wiley.com/doi/abs/10.1002/ecjc.10106](https://onlinelibrary.wiley.com/doi/abs/10.1002/ecjc.10106).
* 투브론 등(2013) 휴고 투브론, 루이 마틴, 케빈 스톤, 피터 알버트, 암자드 알마하리, 야스민 바바이, 니콜라이 바슐리코프, 수미 바트라, 슈루티 보살레, 단 비켈, 루카스 블레처, 크리스티안 칸톤 페러, 모야 첸, 기옌 쿠쿠룰, 다비드 에시오부, 주드 페르난데스, 제레미 푸, 원린 푸, 브라이언 풀러, 신시아 가오, 제레미 푸, 베다누 호세이니, 나만 고얄, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 마디안 하르트쇼, 사그하르 호세이니, 로베르트 푸, 원린 푸, 브라이언 풀러, 주드 페르난데스, 제레미 푸, 신시아 가오, 사그하르 호세이니, 루이 호우, 하칸 이난, 마르신 카다스, 빅토르 케르케즈, 마디안 하바사, 이사벨 클루만, 아르템 고레네프, 신시아 가오, 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델, 2023.
* Yang et al.(2018) Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. 코헨 소프트맥스 병목 현상 해소: 상위 랭크 RNN 언어 모델. 2018년 _International Conference on Learning Representations_ 에서 URL [https://openreview.net/forum?id=HkwZSG-CZ](https://openreview.net/forum?id=HkwZSG-CZ).
* Zhang 등(2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: 오픈소스 소형 언어 모델, 2024.
* Zhang 등 (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: 사전 훈련된 변압기 언어 모델, 2022를 엽니다.
* Zhou et al.(2021) Kaitlyn Zhou, Kawin Ethayarajh, and Dan Jurafsky. 문맥화된 단어 임베딩의 빈도 기반 왜곡입니다. _ CoRR_, abs/2104.08465, 2021. URL [https://arxiv.org/abs/2104.08465](https://arxiv.org/abs/2104.08465).

## 부록 A Proofs

### Lemma 5.1

증명은 주로 계산과 제한된 개발에 기초한다.

[|\mathcal{L}(W,x_{i}^{*})_{y_{i})\mathcal{L}+\log\frac{\exp((W^{*}_{i})_{y_{i})\exp((W^{*}_{i})_{j})\exp((W^{*}_{i})_{j})\exp((W^{*}_{i})_{j})\exp((W^{*}_{i})_{j})\sum\j\in V}\exp((W^{*}_{i})_{j})}{\sum\j\in V}\exp((W^{*}_{i})_{j})}+o(\varepsilon)\right|\] \[=\left|-\varepsilon(Mx_{i}^{*})_{y_{i}+\epsilon\frac{\sum\j\in V}\exp((W^{*})_{j})}\right|\] \[=\left|-\vare

연속함수 \(M\longrightarrow\left|-(Mx_{i}^{*})_{y_{i}}+\frac{\sum_{j\in V}\exp((Mx_{i}^{*})_{j})}{\sum_{j\in V}\exp((W^{*}x_{i}^{*})_{j})}\right|\)는 콤팩트 매트릭스 단위구(즉, 여기서 \(||M||_{F}=1\))에 경계를 두고 있다.

### Theorem 5.3

Frobenius norm과 관련하여 순위 \(d\)의 \(W^{*}\)의 가장 근사치 \(W_{d}\)에 주목하자. \(W_{d}^{*}\)의 정의에 따르면 다음과 같습니다.

\[\left|\sum_{y\in\mathcal{T}}\sum_{i}\mathcal{L}(W_{d}^{*},x_{i}^{*},y_{i})\right|\leq\sum_{y\in\mathcal{T}}\sum_{i}| \mathcal{L}(W_{d},x_{i}^{*},y_{i})-\mathcal{L}(W_{*},x_{i}^{*},y_{i})| \tag{3}\]

Eckart-Young-Mirsky 정리는 \(d\to V\)일 때,

\[||W_{d}-W^{*}||_{F}=\sqrt{\sum_{i=d+1}^{V}\sigma_{i}^{2}}\to 0\]

\(\varepsilon=W_{d}-W^{*}\)를 정의함으로써, 우리는 Lemma 5.1을 적용하고 다음을 보여줄 수 있다:

\[|\mathcal{L}(W_{d},x_{i}^{*},y_{i})-\mathcal{L}(W^{*},x_{i}^{*},y_{i})|=O(||W_ {d}-W^{*}||_{F})=O\left(\sqrt{\sum_{i=d+1}^{V}\sigma_{i}^{2}}\right)\]

식 (3)으로부터, 우리는 다음과 같다:

\[\left|\sum_{y\in\mathcal{T}}\sum_{i}\mathcal{L}(W_{d}^{*},x_{i}^{*},y_{i})- \Remark the bound of equation (3)은 실제로 다소 느슨할 수 있다. 우리는 \(\mathcal{T}\)의 손실을 최소화하기 위해 \(W^{*}\)에 직접 접근하는 것이 최적의 방법이어야 하는 특별한 이유를 생각할 수 없다. 따라서 제시된 결과는 신중하게 취해야 하며 향후 작업을 위해 이러한 분석의 개선을 남겨둔다.

## 부록 B Hyperparameters

### 제한 된 머리 실험 (그림 6)

Pythia 모델의 경우 4개의 V100 GPU를 사용하고 Llama-7B의 경우 4개의 A100 GPU를 사용하여 The Pile에서 샘플링된 150M 토큰에서 다양한 값의 \(r\)에 대해 \(r\)을 행렬 곱의 내부 차원으로 사용하여 각 순위 제한 머리(\(W=AB\) 형태로 훈련한다. Biderman et al.(2023)의 하이퍼파라미터를 사용하되, 하드웨어 설정에 더 적합하기 때문에 256으로 설정한 배치 크기를 제외한다. 학습 가능한 매개변수 계수가 \(r\)로 진화함에 따라 \(1\cdot10^{-3}\)에서 \(5\cdot10^{-2}\) 범위의 값 중에서 가장 우수한 학습률을 검색한다.

우리는 그림 9에서 선택된 학습률을 보고한다.

그림 9: 각 모델에 대한 순위 제한 머리 실험에 사용된 선택된 피크 학습률이다.

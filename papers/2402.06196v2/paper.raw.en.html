<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>Large Language Models: A Survey</title>
<!--Generated on Tue Feb 20 13:33:27 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.06196v2/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.06196v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.06196v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.06196v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S1" title="I Introduction ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2" title="II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Large Language Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.SS1" title="II-A Early Pre-trained Neural Language Models ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Early Pre-trained Neural Language Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.SS1.SSS1" title="II-A1 Encoder-only PLMs ‣ II-A Early Pre-trained Neural Language Models ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>1 </span>Encoder-only PLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.SS1.SSS2" title="II-A2 Decoder-only PLMs ‣ II-A Early Pre-trained Neural Language Models ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>2 </span>Decoder-only PLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.SS1.SSS3" title="II-A3 Encoder-Decoder PLMs ‣ II-A Early Pre-trained Neural Language Models ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>3 </span>Encoder-Decoder PLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.SS2" title="II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Large Language Model Families</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.SS2.SSS1" title="II-B1 The GPT Family ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>1 </span><span class="ltx_text ltx_font_bold">The GPT Family</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.SS2.SSS2" title="II-B2 The LLaMA Family ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>2 </span><span class="ltx_text ltx_font_bold">The LLaMA Family</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.SS2.SSS3" title="II-B3 The PaLM Family ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span>3 </span><span class="ltx_text ltx_font_bold">The PaLM Family</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.SS3" title="II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Other Representative LLMs</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3" title="III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">How LLMs Are Built</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS1" title="III-A Dominant LLM Architectures ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Dominant LLM Architectures</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS1.SSS1" title="III-A1 Transformer ‣ III-A Dominant LLM Architectures ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span><span class="ltx_text ltx_font_bold">Transformer</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS1.SSS2" title="III-A2 Encoder-Only ‣ III-A Dominant LLM Architectures ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>2 </span><span class="ltx_text ltx_font_bold">Encoder-Only</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS1.SSS3" title="III-A3 Decoder-Only ‣ III-A Dominant LLM Architectures ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>3 </span><span class="ltx_text ltx_font_bold">Decoder-Only</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS1.SSS4" title="III-A4 Encoder-Decoder ‣ III-A Dominant LLM Architectures ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>4 </span><span class="ltx_text ltx_font_bold">Encoder-Decoder</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS2" title="III-B Data Cleaning ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Data Cleaning</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS2.SSS1" title="III-B1 Data Filtering ‣ III-B Data Cleaning ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Data Filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS2.SSS2" title="III-B2 Deduplication ‣ III-B Data Cleaning ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Deduplication</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS3" title="III-C Tokenizations ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Tokenizations</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS3.SSS1" title="III-C1 BytePairEncoding ‣ III-C Tokenizations ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>1 </span><span class="ltx_text ltx_font_bold">BytePairEncoding</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS3.SSS2" title="III-C2 WordPieceEncoding ‣ III-C Tokenizations ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>2 </span><span class="ltx_text ltx_font_bold">WordPieceEncoding</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS3.SSS3" title="III-C3 SentencePieceEncoding ‣ III-C Tokenizations ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>3 </span><span class="ltx_text ltx_font_bold">SentencePieceEncoding</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS4" title="III-D Positional Encoding ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_bold">Positional Encoding</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS4.SSS1" title="III-D1 Absolute Positional Embeddings ‣ III-D Positional Encoding ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>1 </span><span class="ltx_text ltx_font_bold">Absolute Positional Embeddings</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS4.SSS2" title="III-D2 Relative Positional Embeddings ‣ III-D Positional Encoding ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>2 </span><span class="ltx_text ltx_font_bold">Relative Positional Embeddings</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS4.SSS3" title="III-D3 Rotary Position Embeddings ‣ III-D Positional Encoding ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>3 </span><span class="ltx_text ltx_font_bold">Rotary Position Embeddings</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS4.SSS4" title="III-D4 Relative Positional Bias ‣ III-D Positional Encoding ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span>4 </span><span class="ltx_text ltx_font_bold">Relative Positional Bias</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS5" title="III-E Model Pre-training ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Model Pre-training</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS6" title="III-F Fine-tuning and Instruction Tuning ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-F</span> </span><span class="ltx_text ltx_font_italic">Fine-tuning and Instruction Tuning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS7" title="III-G Alignment ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-G</span> </span><span class="ltx_text ltx_font_italic">Alignment</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS8" title="III-H Decoding Strategies ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-H</span> </span><span class="ltx_text ltx_font_italic">Decoding Strategies</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS8.SSS1" title="III-H1 Greedy Search ‣ III-H Decoding Strategies ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-H</span>1 </span><span class="ltx_text ltx_font_bold">Greedy Search</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS8.SSS2" title="III-H2 Beam Search ‣ III-H Decoding Strategies ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-H</span>2 </span><span class="ltx_text ltx_font_bold">Beam Search</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS8.SSS3" title="III-H3 Top-k Sampling ‣ III-H Decoding Strategies ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-H</span>3 </span><span class="ltx_text ltx_font_bold">Top-k Sampling</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS8.SSS4" title="III-H4 Top-p Sampling ‣ III-H Decoding Strategies ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-H</span>4 </span><span class="ltx_text ltx_font_bold">Top-p Sampling</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS9" title="III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-I</span> </span><span class="ltx_text ltx_font_italic">Cost-Effective Training/Inference/Adaptation/Compression</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS9.SSS1" title="III-I1 Optimized Training ‣ III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-I</span>1 </span><span class="ltx_text ltx_font_bold">Optimized Training</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS9.SSS2" title="III-I2 Low-Rank Adaption (LoRA) ‣ III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-I</span>2 </span><span class="ltx_text ltx_font_bold">Low-Rank Adaption (LoRA)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS9.SSS3" title="III-I3 Knowledge Distillation ‣ III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-I</span>3 </span><span class="ltx_text ltx_font_bold">Knowledge Distillation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS9.SSS4" title="III-I4 Quantization ‣ III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-I</span>4 </span><span class="ltx_text ltx_font_bold">Quantization</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4" title="IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">How LLMs Are Used and Augmented</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS1" title="IV-A LLM limitations ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">LLM limitations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2" title="IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Using LLMs: </span><span class="ltx_text ltx_font_bold">Prompt Design and Engineering</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2.SSS1" title="IV-B1 Chain of Thought (CoT) ‣ IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>1 </span>Chain of Thought (CoT)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2.SSS2" title="IV-B2 Tree of Thought (ToT) ‣ IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>2 </span>Tree of Thought (ToT)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2.SSS3" title="IV-B3 Self-Consistency ‣ IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>3 </span>Self-Consistency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2.SSS4" title="IV-B4 Reflection ‣ IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>4 </span>Reflection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2.SSS5" title="IV-B5 Expert Prompting ‣ IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>5 </span>Expert Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2.SSS6" title="IV-B6 Chains ‣ IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>6 </span>Chains</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2.SSS7" title="IV-B7 Rails ‣ IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>7 </span>Rails</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2.SSS8" title="IV-B8 Automatic Prompt Engineering (APE) ‣ IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>8 </span>Automatic Prompt Engineering (APE)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS3" title="IV-C Augmenting LLMs through external knowledge - RAG ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_bold">Augmenting LLMs through external knowledge - RAG</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS3.SSS0.Px1" title="RAG-aware prompting techniques ‣ IV-C Augmenting LLMs through external knowledge - RAG ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">RAG-aware prompting techniques</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS4" title="IV-D Using External Tools ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_bold">Using External Tools</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS4.SSS0.Px1" title="Tool-aware prompting techniques ‣ IV-D Using External Tools ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Tool-aware prompting techniques</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS5" title="IV-E LLM Agents ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_bold">LLM Agents</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS5.SSS0.Px1" title="Prompt engineering techniques for agents ‣ IV-E LLM Agents ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title">Prompt engineering techniques for agents</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S5" title="V Popular Datasets for LLMs ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Popular Datasets for LLMs</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S5.SS1" title="V-A Datasets for Basic Tasks: language modeling/understanding/generation ‣ V Popular Datasets for LLMs ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Datasets for Basic Tasks: language modeling/understanding/generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S5.SS2" title="V-B Datasets for Emergent: ICL, reasoning (CoT), instruction following ‣ V Popular Datasets for LLMs ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Datasets for Emergent: ICL, reasoning (CoT), instruction following</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S5.SS3" title="V-C Datasets for Augmented: using external knowledge/tools ‣ V Popular Datasets for LLMs ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Datasets for Augmented: using external knowledge/tools</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6" title="VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Prominent LLMs’ Performance on Benchmarks</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.SS1" title="VI-A Popular Metrics for Evaluating LLMs ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Popular Metrics for Evaluating LLMs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.SS2" title="VI-B LLMs’ Performance on Different Tasks ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">LLMs’ Performance on Different Tasks</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S7" title="VII Challenges and Future Directions ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Challenges and Future Directions</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S7.SS1" title="VII-A Smaller and more efficient Language Models ‣ VII Challenges and Future Directions ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Smaller and more efficient Language Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S7.SS2" title="VII-B New Post-attention Architectural Paradigms ‣ VII Challenges and Future Directions ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">New Post-attention Architectural Paradigms</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S7.SS3" title="VII-C Multi-modal Models ‣ VII Challenges and Future Directions ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-C</span> </span><span class="ltx_text ltx_font_italic">Multi-modal Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S7.SS4" title="VII-D Improved LLM Usage and Augmentation techniques ‣ VII Challenges and Future Directions ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-D</span> </span><span class="ltx_text ltx_font_italic">Improved LLM Usage and Augmentation techniques</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S7.SS5" title="VII-E Security and Ethical/Responsible AI ‣ VII Challenges and Future Directions ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-E</span> </span><span class="ltx_text ltx_font_italic">Security and Ethical/Responsible AI</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S8" title="VIII Conclusion ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#A0.SS1" title="-A LLM Training/Inference Frameworks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">-A</span> </span><span class="ltx_text ltx_font_italic">LLM Training/Inference Frameworks</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#A0.SS2" title="-B Deployment Tools ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">-B</span> </span><span class="ltx_text ltx_font_italic">Deployment Tools</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#A0.SS3" title="-C Prompting Libraries ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">-C</span> </span><span class="ltx_text ltx_font_italic">Prompting Libraries</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#A0.SS4" title="-D VectorDB ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">-D</span> </span><span class="ltx_text ltx_font_italic">VectorDB</span></span></a></li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2402.06196v2 [cs.CL] 20 Feb 2024</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Large Language Models: A Survey</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu 
<br class="ltx_break">Richard Socher, Xavier Amatriain, Jianfeng Gao 
<br class="ltx_break">
<br class="ltx_break">
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.id1">Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022.
LLMs’ ability of general-purpose language understanding and generation is acquired by training billions of model’s parameters on massive amounts of text data, as predicted by scaling laws <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib2" title="">2</a>]</cite>.
The research area of LLMs, while very recent, is evolving rapidly in many different ways.
In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations.
We also give an overview of techniques developed to build, and augment LLMs.
We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks.
Finally, we conclude the paper by discussing open challenges and future research directions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Language modeling is a long-standing research topic, dating back to the 1950s with Shannon’s application of information theory to human language, where he measured how well simple n-gram language models predict or compress natural language text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib3" title="">3</a>]</cite>.
Since then, statistical language modeling became fundamental to many natural language understanding and generation tasks, ranging from speech recognition, machine translation, to information retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib6" title="">6</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The recent advances on transformer-based large language models (LLMs), pretrained on Web-scale text corpora, significantly extended the capabilities of language models (LLMs). For example, OpenAI’s ChatGPT and GPT-4 can be used not only for natural language processing, but also as general task solvers to power Microsoft’s Co-Pilot systems, for instance, can follow human instructions of complex new tasks performing multi-step reasoning when needed. LLMs are thus becoming the basic building block for the development of general-purpose AI agents or artificial general intelligence (AGI).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">As the field of LLMs is moving fast, with new findings, models and techniques being published in a matter of months or weeks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib11" title="">11</a>]</cite>, AI researchers and practitioners often find it challenging to figure out the best recipes to build LLM-powered AI systems for their tasks. This paper gives a timely survey of the recent advances on LLMs.
We hope this survey will prove a valuable and accessible resource for students, researchers and developers.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">LLMs are large-scale, pre-trained, statistical language models based on neural networks. The recent success of LLMs is an accumulation of decades of research and development of language models, which can be categorized into four waves that have different starting points and velocity: statistical language models, neural language models, pre-trained language models and LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Statistical language models (SLMs) view text as a sequence of words, and estimate the probability of text as the product of their word probabilities. The dominating form of SLMs are Markov chain models known as the n-gram models, which compute the probability of a word conditioned on its immediate proceeding <math alttext="n-1" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mrow id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml"><mi id="S1.p5.1.m1.1.1.2" xref="S1.p5.1.m1.1.1.2.cmml">n</mi><mo id="S1.p5.1.m1.1.1.1" xref="S1.p5.1.m1.1.1.1.cmml">−</mo><mn id="S1.p5.1.m1.1.1.3" xref="S1.p5.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><apply id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"><minus id="S1.p5.1.m1.1.1.1.cmml" xref="S1.p5.1.m1.1.1.1"></minus><ci id="S1.p5.1.m1.1.1.2.cmml" xref="S1.p5.1.m1.1.1.2">𝑛</ci><cn id="S1.p5.1.m1.1.1.3.cmml" type="integer" xref="S1.p5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">n-1</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">italic_n - 1</annotation></semantics></math> words. Since word probabilities are estimated using word and n-gram counts collected from text corpora, the model needs to deal with data sparsity (i.e., assigning zero probabilities to unseen words or n-grams) by using <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">smoothing</em>, where some probability mass of the model is reserved for unseen n-grams <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib12" title="">12</a>]</cite>. N-gram models are widely used in many NLP systems. However, these models are incomplete in that they cannot fully capture the diversity and variability of natural language due to data sparsity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Early neural language models (NLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib16" title="">16</a>]</cite> deal with data sparsity by mapping words to low-dimensional continuous vectors (embedding vectors) and predict the next word based on the aggregation of the embedding vectors of its proceeding words using neural networks. The embedding vectors learned by NLMs define a hidden space where the semantic similarity between vectors can be readily computed as their distance.
This opens the door to computing semantic similarity of any two inputs regardless their forms (e.g., queries vs. documents in Web search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib18" title="">18</a>]</cite>, sentences in different languages in machine translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib20" title="">20</a>]</cite>) or modalities (e.g., image and text in image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib22" title="">22</a>]</cite>). Early NLMs are task-specific models, in that they are trained on task-specific data and their learned hidden space is task-specific.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Pre-trained language models (PLMs), unlike early NLMs, are task-agnostic. This generality also extends to the learned hidden embedding space. The training and inference of PLMs follows the <em class="ltx_emph ltx_font_italic" id="S1.p7.1.1">pre-training and fine-tuning</em> paradigm, where language models with recurrent neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib23" title="">23</a>]</cite> or transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib26" title="">26</a>]</cite> are pre-trained on Web-scale unlabeled text corpora for general tasks such as word prediction, and then finetuned to specific tasks using small amounts of (labeled) task-specific data. Recent surveys on PLMs include <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib28" title="">28</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="307" id="S1.F1.g1" src="x1.png" width="707">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">LLM Capabilities.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Large language models (LLMs) mainly refer to transformer-based neural language models <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Recently, several very promising non-transformer LLMs have been proposed, such as the LLMs based on structured state space models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib30" title="">30</a>]</cite>. See Section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S7" title="VII Challenges and Future Directions ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">VII</span></a> for more details.</span></span></span> that contain tens to hundreds of billions of parameters, which are pre-trained on massive text data, such as PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib31" title="">31</a>]</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib32" title="">32</a>]</cite>, and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib33" title="">33</a>]</cite>, as summarized in Table III.
Compared to PLMs, LLMs are not only much larger in model size, but also exhibit stronger language understanding and generation abilities, and more importantly, emergent abilities that are not present in smaller-scale language models. As illustrated in Fig.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S1.F1" title="Figure 1 ‣ I Introduction ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>, these emergent abilities include (1) in-context learning, where LLMs learn a new task from a small set of examples presented in the prompt at inference time, (2) instruction following, where LLMs, after instruction tuning, can follow the instructions for new types of tasks without using explicit examples, and (3) multi-step reasoning, where LLMs can solve a complex task by breaking down that task into intermediate reasoning steps as demonstrated in the chain-of-thought prompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib34" title="">34</a>]</cite>.
LLMs can also be augmented by using external knowledge and tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib36" title="">36</a>]</cite> so that they can effectively interact with users and environment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib37" title="">37</a>]</cite>, and continually improve itself using feedback data collected through interactions (e.g. via reinforcement learning with human feedback (RLHF)).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">Through advanced usage and augmentation techniques, LLMs can be deployed as so-called AI agents: artificial entities that sense their environment, make decisions, and take actions. Previous research has focused on developing agents for specific tasks and domains. The emergent abilities demonstrated by LLMs make it possible to build general-purpose AI agents based on LLMs.
While LLMs are trained to produce responses in static settings, AI agents need to take actions to interact with dynamic environment. Therefore, LLM-based agents often need to augment LLMs to e.g., obtain updated information from external knowledge bases, verify whether a system action produces the expected result, and cope with when things do not go as expected, etc. We will discuss in detail LLM-based agents in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4" title="IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">In the rest of this paper,
Section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2" title="II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">II</span></a> presents an overview of state of the art of LLMs, focusing on three LLM families (GPT, LLaMA and PaLM) and other representative models.
Section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3" title="III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">III</span></a> discusses how LLMs are built.
Section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4" title="IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">IV</span></a> discusses how LLMs are used, and augmented for real-world applications
Sections <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S5" title="V Popular Datasets for LLMs ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">V</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6" title="VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">VI</span></a> review popular datasets and benchmarks for evaluating LLMs, and summarize the reported LLM evaluation results.
Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S7" title="VII Challenges and Future Directions ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">VII</span></a> concludes the paper by summarizing the challenges and future research directions.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="678" id="S1.F2.g1" src="x2.png" width="487">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">The paper structure.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Large Language Models</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section we start with a review of early pre-trained neural language models as they are the base of LLMs, and then focus our discussion on three families of LLMs: GPT, LlaMA, and PaLM.
Table <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.T1" title="TABLE I ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">I</span></a> provides an overview of some of these models and their characteristics.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.16.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S2.T1.17.2" style="font-size:90%;">High-level Overview of Popular Language Models</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.14">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.14.15.1">
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.14.15.1.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.15.1.1.1">Type</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.14.15.1.2" style="width:48.4pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.15.1.2.1">Model Name</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.14.15.1.3" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.15.1.3.1">#Parameters</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.14.15.1.4" style="width:28.5pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.15.1.4.1">Release</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.14.15.1.5" style="width:42.7pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.15.1.5.1">Base Models</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.14.15.1.6" style="width:22.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.15.1.6.1">Open Source</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.14.15.1.7" style="width:28.5pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.15.1.7.1">#Tokens</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.14.15.1.8" style="width:142.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.15.1.8.1">Training dataset</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.16.2">
<td class="ltx_td ltx_border_t" id="S2.T1.14.16.2.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.16.2.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.16.2.2.1">BERT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.16.2.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.16.2.3.1">110M, 340M</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.16.2.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.16.2.4.1">2018</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.16.2.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.16.2.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.16.2.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.16.2.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.16.2.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.16.2.7.1">137B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.16.2.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.16.2.8.1">BooksCorpus, English Wikipedia</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.17.3">
<td class="ltx_td" id="S2.T1.14.17.3.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.17.3.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.17.3.2.1">RoBERTa</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.17.3.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.17.3.3.1">355M</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.17.3.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.17.3.4.1">2019</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.17.3.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.17.3.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.17.3.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.17.3.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.17.3.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.17.3.7.1">2.2T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.17.3.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.17.3.8.1">BooksCorpus, English Wikipedia, CC-NEWS, STORIES (a subset of Common Crawl), Reddit</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.18.4">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.18.4.1" rowspan="2" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.18.4.1.1">Encoder-Only</span></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.18.4.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.18.4.2.1">ALBERT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.18.4.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.18.4.3.1">12M, 18M, 60M, 235M</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.18.4.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.18.4.4.1">2019</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.18.4.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.18.4.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.18.4.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.18.4.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.18.4.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.18.4.7.1">137B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.18.4.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.18.4.8.1">BooksCorpus, English Wikipedia</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.19.5">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.19.5.1" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.19.5.1.1">DeBERTa</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.19.5.2" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.19.5.2.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.19.5.3" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.19.5.3.1">2020</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.19.5.4" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.19.5.4.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.19.5.5" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.19.5.5.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.19.5.6" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.19.5.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.19.5.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.19.5.7.1">BooksCorpus, English Wikipedia, STORIES, Reddit content</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.20.6">
<td class="ltx_td" id="S2.T1.14.20.6.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.20.6.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.20.6.2.1">XLNet</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.20.6.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.20.6.3.1">110M, 340M</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.20.6.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.20.6.4.1">2019</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.20.6.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.20.6.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.20.6.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.20.6.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.20.6.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.20.6.7.1">32.89B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.20.6.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.20.6.8.1">BooksCorpus, English Wikipedia, Giga5, Common Crawl, ClueWeb 2012-B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.21.7">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.21.7.1" rowspan="2" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.21.7.1.1">Decoder-only</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.21.7.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.21.7.2.1">GPT-1</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.21.7.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.21.7.3.1">120M</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.21.7.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.21.7.4.1">2018</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.21.7.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.21.7.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.21.7.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.21.7.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.21.7.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.21.7.7.1">1.3B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.21.7.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.21.7.8.1">BooksCorpus</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.22.8">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.22.8.1" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.22.8.1.1">GPT-2</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.22.8.2" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.22.8.2.1">1.5B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.22.8.3" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.22.8.3.1">2019</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.22.8.4" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.22.8.4.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.22.8.5" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.22.8.5.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.22.8.6" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.22.8.6.1">10B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.22.8.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.22.8.7.1">Reddit outbound</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.23.9">
<td class="ltx_td ltx_border_t" id="S2.T1.14.23.9.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.23.9.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.23.9.2.1">T5 (Base)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.23.9.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.23.9.3.1">223M</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.23.9.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.23.9.4.1">2019</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.23.9.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.23.9.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.23.9.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.23.9.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.23.9.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.23.9.7.1">156B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.23.9.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.23.9.8.1">Common Crawl</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.24.10">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.24.10.1" rowspan="2" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.24.10.1.1">Encoder-Decoder</span></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.24.10.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.24.10.2.1">MT5 (Base)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.24.10.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.24.10.3.1">300M</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.24.10.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.24.10.4.1">2020</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.24.10.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.24.10.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.24.10.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.24.10.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.24.10.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.24.10.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.24.10.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.24.10.8.1">New Common Crawl-based dataset in 101 languages (m Common Crawl)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.25.11">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.25.11.1" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.25.11.1.1">BART (Base)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.25.11.2" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.25.11.2.1">139M</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.25.11.3" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.25.11.3.1">2019</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.25.11.4" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.25.11.4.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.25.11.5" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.25.11.5.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.25.11.6" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.25.11.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.25.11.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.25.11.7.1">Corrupting text</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_border_t" id="S2.T1.1.1.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.1.1.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.1.1.3.1">GPT-3</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.1.1.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.1.1.4.1">125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13B, 175B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.1.1.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.1.1.5.1">2020</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_border_t" id="S2.T1.1.1.6" style="width:42.7pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.1.1.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.1.1.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.m1.1b"><times id="S2.T1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.1.1.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.1.1.7.1">300B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.1.1.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.1.1.8.1">Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.26.12">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.26.12.1" rowspan="2" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.26.12.1.1">GPT Family</span></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.26.12.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.26.12.2.1">CODEX</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.26.12.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.26.12.3.1">12B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.26.12.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.26.12.4.1">2021</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.26.12.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.26.12.5.1">GPT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.26.12.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.26.12.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.26.12.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.26.12.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.26.12.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.26.12.8.1">Public GitHub software repositories</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2">
<td class="ltx_td ltx_align_justify" id="S2.T1.2.2.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.2.2.2.1">WebGPT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.2.2.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.2.2.3.1">760M, 13B, 175B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.2.2.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.2.2.4.1">2021</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.2.2.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.2.2.5.1">GPT-3</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.2.2.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.2.2.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.2.2.1.1.1.m1.1"><semantics id="S2.T1.2.2.1.1.1.m1.1a"><mo id="S2.T1.2.2.1.1.1.m1.1.1" xref="S2.T1.2.2.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.1.1.1.m1.1b"><times id="S2.T1.2.2.1.1.1.m1.1.1.cmml" xref="S2.T1.2.2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.2.2.6" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.2.2.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.2.2.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.2.2.7.1">ELI5</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.3.3">
<td class="ltx_td" id="S2.T1.3.3.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.3.3.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.3.3.3.1">GPT-4</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.3.3.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.3.3.4.1">1.76T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.3.3.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.3.3.5.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.3.3.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.3.3.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.3.3.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.3.3.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.3.3.1.1.1.m1.1"><semantics id="S2.T1.3.3.1.1.1.m1.1a"><mo id="S2.T1.3.3.1.1.1.m1.1.1" xref="S2.T1.3.3.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.1.1.1.m1.1b"><times id="S2.T1.3.3.1.1.1.m1.1.1.cmml" xref="S2.T1.3.3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.3.3.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.3.3.7.1">13T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.3.3.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.3.3.8.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.27.13">
<td class="ltx_td ltx_border_t" id="S2.T1.14.27.13.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.27.13.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.27.13.2.1">LLaMA1</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.27.13.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.27.13.3.1">7B, 13B, 33B, 65B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.27.13.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.27.13.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.27.13.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.27.13.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.27.13.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.27.13.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.27.13.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.27.13.7.1">1T, 1.4T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.27.13.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.27.13.8.1">Online sources</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.28.14">
<td class="ltx_td" id="S2.T1.14.28.14.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.28.14.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.28.14.2.1">LLaMA2</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.28.14.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.28.14.3.1">7B, 13B, 34B, 70B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.28.14.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.28.14.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.28.14.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.28.14.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.28.14.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.28.14.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.28.14.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.28.14.7.1">2T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.28.14.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.28.14.8.1">Online sources</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.29.15">
<td class="ltx_td" id="S2.T1.14.29.15.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.29.15.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.29.15.2.1">Alpaca</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.29.15.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.29.15.3.1">7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.29.15.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.29.15.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.29.15.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.29.15.5.1">LLaMA1</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.29.15.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.29.15.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.29.15.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.29.15.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.29.15.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.29.15.8.1">GPT-3.5</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.30.16">
<td class="ltx_td" id="S2.T1.14.30.16.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.30.16.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.30.16.2.1">Vicuna-13B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.30.16.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.30.16.3.1">13B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.30.16.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.30.16.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.30.16.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.30.16.5.1">LLaMA1</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.30.16.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.30.16.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.30.16.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.30.16.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.30.16.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.30.16.8.1">GPT-3.5</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.31.17">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.31.17.1" rowspan="2" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.31.17.1.1">LLaMA Family</span></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.31.17.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.31.17.2.1">Koala</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.31.17.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.31.17.3.1">13B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.31.17.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.31.17.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.31.17.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.31.17.5.1">LLaMA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.31.17.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.31.17.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.31.17.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.31.17.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.31.17.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.31.17.8.1">Dialogue data</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.32.18">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.32.18.1" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.32.18.1.1">Mistral-7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.32.18.2" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.32.18.2.1">7.3B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.32.18.3" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.32.18.3.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S2.T1.14.32.18.4" style="width:42.7pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.32.18.5" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.32.18.5.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.32.18.6" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.32.18.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.32.18.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.32.18.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.33.19">
<td class="ltx_td" id="S2.T1.14.33.19.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.33.19.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.33.19.2.1">Code Llama</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.33.19.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.33.19.3.1">34</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.33.19.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.33.19.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.33.19.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.33.19.5.1">LLaMA2</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.33.19.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.33.19.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.33.19.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.33.19.7.1">500B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.33.19.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.33.19.8.1">Publicly available code</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.34.20">
<td class="ltx_td" id="S2.T1.14.34.20.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.34.20.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.34.20.2.1">LongLLaMA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.34.20.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.34.20.3.1">3B, 7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.34.20.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.34.20.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.34.20.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.34.20.5.1">OpenLLaMA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.34.20.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.34.20.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.34.20.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.34.20.7.1">1T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.34.20.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.34.20.8.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.35.21">
<td class="ltx_td" id="S2.T1.14.35.21.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.35.21.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.35.21.2.1">LLaMA-Pro-8B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.35.21.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.35.21.3.1">8.3B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.35.21.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.35.21.4.1">2024</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.35.21.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.35.21.5.1">LLaMA2-7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.35.21.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.35.21.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.35.21.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.35.21.7.1">80B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.35.21.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.35.21.8.1">Code and math corpora</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.36.22">
<td class="ltx_td" id="S2.T1.14.36.22.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.36.22.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.36.22.2.1">TinyLlama-1.1B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.36.22.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.36.22.3.1">1.1B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.36.22.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.36.22.4.1">2024</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.36.22.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.36.22.5.1">LLaMA1.1B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.36.22.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.36.22.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.36.22.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.36.22.7.1">3T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.36.22.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.36.22.8.1">SlimPajama, Starcoderdata</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4">
<td class="ltx_td ltx_border_t" id="S2.T1.4.4.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.4.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.4.4.3.1">PaLM</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.4.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.4.4.4.1">8B, 62B, 540B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.4.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.4.4.5.1">2022</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.4.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.4.4.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.4.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.4.4.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.4.4.1.1.1.m1.1"><semantics id="S2.T1.4.4.1.1.1.m1.1a"><mo id="S2.T1.4.4.1.1.1.m1.1.1" xref="S2.T1.4.4.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.1.1.m1.1b"><times id="S2.T1.4.4.1.1.1.m1.1.1.cmml" xref="S2.T1.4.4.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.4.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.4.4.7.1">780B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.4.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.4.4.8.1">Web documents, books, Wikipedia, conversations, GitHub code</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5">
<td class="ltx_td" id="S2.T1.5.5.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.5.5.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.5.5.3.1">U-PaLM</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.5.5.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.5.5.4.1">8B, 62B, 540B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.5.5.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.5.5.5.1">2022</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.5.5.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.5.5.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.5.5.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.5.5.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.5.5.1.1.1.m1.1"><semantics id="S2.T1.5.5.1.1.1.m1.1a"><mo id="S2.T1.5.5.1.1.1.m1.1.1" xref="S2.T1.5.5.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.1.1.m1.1b"><times id="S2.T1.5.5.1.1.1.m1.1.1.cmml" xref="S2.T1.5.5.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.5.5.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.5.5.7.1">1.3B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.5.5.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.5.5.8.1">Web documents, books, Wikipedia, conversations, GitHub code</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.37.23">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.37.23.1" rowspan="2" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.37.23.1.1">PaLM Family</span></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.37.23.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.37.23.2.1">PaLM-2</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.37.23.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.37.23.3.1">340B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.37.23.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.37.23.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.37.23.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.37.23.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.37.23.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.37.23.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.37.23.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.37.23.7.1">3.6T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.37.23.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.37.23.8.1">Web documents, books, code, mathematics, conversational data</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.6">
<td class="ltx_td ltx_align_justify" id="S2.T1.6.6.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.6.6.2.1">Med-PaLM</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.6.6.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.6.6.3.1">540B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.6.6.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.6.6.4.1">2022</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.6.6.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.6.6.5.1">PaLM</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.6.6.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.6.6.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.6.6.1.1.1.m1.1"><semantics id="S2.T1.6.6.1.1.1.m1.1a"><mo id="S2.T1.6.6.1.1.1.m1.1.1" xref="S2.T1.6.6.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.1.1.1.m1.1b"><times id="S2.T1.6.6.1.1.1.m1.1.1.cmml" xref="S2.T1.6.6.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.6.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.6.6.6" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.6.6.6.1">780B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.6.6.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.6.6.7.1">HealthSearchQA, MedicationQA, LiveQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.7.7">
<td class="ltx_td" id="S2.T1.7.7.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.7.7.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.7.7.3.1">Med-PaLM 2</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.7.7.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.7.7.4.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.7.7.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.7.7.5.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.7.7.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.7.7.6.1">PaLM 2</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.7.7.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.7.7.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.7.7.1.1.1.m1.1"><semantics id="S2.T1.7.7.1.1.1.m1.1a"><mo id="S2.T1.7.7.1.1.1.m1.1.1" xref="S2.T1.7.7.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.1.1.1.m1.1b"><times id="S2.T1.7.7.1.1.1.m1.1.1.cmml" xref="S2.T1.7.7.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.7.7.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.7.7.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.7.7.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.7.7.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.7.7.8.1">MedQA, MedMCQA, HealthSearchQA, LiveQA, MedicationQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.38.24">
<td class="ltx_td ltx_border_t" id="S2.T1.14.38.24.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.38.24.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.38.24.2.1">FLAN</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.38.24.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.38.24.3.1">137B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.38.24.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.38.24.4.1">2021</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.38.24.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.38.24.5.1">LaMDA-PT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.38.24.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.38.24.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.38.24.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.38.24.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.14.38.24.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.38.24.8.1">Web documents, code, dialog data, Wikipedia</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.8">
<td class="ltx_td" id="S2.T1.8.8.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.8.8.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.8.8.3.1">Gopher</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.8.8.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.8.8.4.1">280B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.8.8.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.8.8.5.1">2021</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.8.8.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.8.8.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.8.8.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.8.8.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.8.8.1.1.1.m1.1"><semantics id="S2.T1.8.8.1.1.1.m1.1a"><mo id="S2.T1.8.8.1.1.1.m1.1.1" xref="S2.T1.8.8.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.1.1.1.m1.1b"><times id="S2.T1.8.8.1.1.1.m1.1.1.cmml" xref="S2.T1.8.8.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.8.8.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.8.8.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.8.8.7.1">300B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.8.8.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.8.8.8.1">MassiveText</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.9.9">
<td class="ltx_td" id="S2.T1.9.9.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.9.9.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.9.9.3.1">ERNIE 4.0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.9.9.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.9.9.4.1">10B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.9.9.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.9.9.5.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.9.9.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.9.9.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.9.9.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.9.9.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.9.9.1.1.1.m1.1"><semantics id="S2.T1.9.9.1.1.1.m1.1a"><mo id="S2.T1.9.9.1.1.1.m1.1.1" xref="S2.T1.9.9.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.1.1.1.m1.1b"><times id="S2.T1.9.9.1.1.1.m1.1.1.cmml" xref="S2.T1.9.9.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.9.9.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.9.9.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.9.9.7.1">4TB</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.9.9.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.9.9.8.1">Chinese text</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.10.10">
<td class="ltx_td" id="S2.T1.10.10.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.10.10.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.10.10.3.1">Retro</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.10.10.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.10.10.4.1">7.5B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.10.10.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.10.10.5.1">2021</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.10.10.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.10.10.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.10.10.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.10.10.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.10.10.1.1.1.m1.1"><semantics id="S2.T1.10.10.1.1.1.m1.1a"><mo id="S2.T1.10.10.1.1.1.m1.1.1" xref="S2.T1.10.10.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.1.1.1.m1.1b"><times id="S2.T1.10.10.1.1.1.m1.1.1.cmml" xref="S2.T1.10.10.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.10.10.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.10.10.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.10.10.7.1">600B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.10.10.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.10.10.8.1">MassiveText</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.11.11">
<td class="ltx_td" id="S2.T1.11.11.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.11.11.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.11.11.3.1">LaMDA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.11.11.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.11.11.4.1">137B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.11.11.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.11.11.5.1">2022</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.11.11.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.11.11.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.11.11.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.11.11.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.11.11.1.1.1.m1.1"><semantics id="S2.T1.11.11.1.1.1.m1.1a"><mo id="S2.T1.11.11.1.1.1.m1.1.1" xref="S2.T1.11.11.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.1.1.1.m1.1b"><times id="S2.T1.11.11.1.1.1.m1.1.1.cmml" xref="S2.T1.11.11.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.11.11.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.11.11.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.11.11.7.1">168B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.11.11.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.11.11.8.1">public dialog data and web documents</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.12.12">
<td class="ltx_td" id="S2.T1.12.12.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.12.12.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.12.12.3.1">ChinChilla</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.12.12.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.12.12.4.1">70B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.12.12.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.12.12.5.1">2022</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.12.12.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.12.12.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.12.12.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.12.12.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.12.12.1.1.1.m1.1"><semantics id="S2.T1.12.12.1.1.1.m1.1a"><mo id="S2.T1.12.12.1.1.1.m1.1.1" xref="S2.T1.12.12.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.1.1.1.m1.1b"><times id="S2.T1.12.12.1.1.1.m1.1.1.cmml" xref="S2.T1.12.12.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.12.12.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.12.12.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.12.12.7.1">1.4T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.12.12.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.12.12.8.1">MassiveText</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.39.25">
<td class="ltx_td" id="S2.T1.14.39.25.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.39.25.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.39.25.2.1">Galactia-120B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.39.25.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.39.25.3.1">120B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.39.25.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.39.25.4.1">2022</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.39.25.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.39.25.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S2.T1.14.39.25.6" style="width:22.8pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.39.25.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.39.25.7.1">450B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S2.T1.14.39.25.8" style="width:142.3pt;"></td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.40.26">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.40.26.1" rowspan="2" style="width:68.3pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S2.T1.14.40.26.1.1">Other Popular LLMs</span></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.40.26.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.40.26.2.1">CodeGen</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.40.26.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.40.26.3.1">16.1B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.40.26.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.40.26.4.1">2022</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.40.26.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.40.26.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.40.26.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.40.26.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.40.26.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.40.26.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.40.26.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.40.26.8.1">THE PILE, BIGQUERY, BIGPYTHON</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.41.27">
<td class="ltx_td ltx_align_justify" id="S2.T1.14.41.27.1" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.41.27.1.1">BLOOM</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.41.27.2" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.41.27.2.1">176B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.41.27.3" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.41.27.3.1">2022</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.41.27.4" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.41.27.4.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.41.27.5" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.41.27.5.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.41.27.6" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.41.27.6.1">366B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.41.27.7" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.41.27.7.1">ROOTS</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.42.28">
<td class="ltx_td" id="S2.T1.14.42.28.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.42.28.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.42.28.2.1">Zephyr</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.42.28.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.42.28.3.1">7.24B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.42.28.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.42.28.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.42.28.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.42.28.5.1">Mistral-7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.42.28.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.42.28.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.42.28.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.42.28.7.1">800B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.42.28.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.42.28.8.1">Synthetic data</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.13.13">
<td class="ltx_td" id="S2.T1.13.13.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.13.13.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.13.13.3.1">Grok-0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.13.13.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.13.13.4.1">33B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.13.13.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.13.13.5.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.13.13.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.13.13.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.13.13.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.13.13.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.13.13.1.1.1.m1.1"><semantics id="S2.T1.13.13.1.1.1.m1.1a"><mo id="S2.T1.13.13.1.1.1.m1.1.1" xref="S2.T1.13.13.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.13.13.1.1.1.m1.1b"><times id="S2.T1.13.13.1.1.1.m1.1.1.cmml" xref="S2.T1.13.13.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.13.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.13.13.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.13.13.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.13.13.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.13.13.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.13.13.8.1">Online source</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.43.29">
<td class="ltx_td" id="S2.T1.14.43.29.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.43.29.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.43.29.2.1">ORCA-2</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.43.29.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.43.29.3.1">13B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.43.29.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.43.29.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.43.29.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.43.29.5.1">LLaMA2</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.43.29.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.43.29.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.43.29.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.43.29.7.1">2001B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.43.29.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.43.29.8.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.44.30">
<td class="ltx_td" id="S2.T1.14.44.30.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.44.30.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.44.30.2.1">StartCoder</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.44.30.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.44.30.3.1">15.5B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.44.30.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.44.30.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.44.30.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.44.30.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.44.30.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.44.30.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.44.30.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.44.30.7.1">35B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.44.30.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.44.30.8.1">GitHub</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.45.31">
<td class="ltx_td" id="S2.T1.14.45.31.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.45.31.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.45.31.2.1">MPT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.45.31.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.45.31.3.1">7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.45.31.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.45.31.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.45.31.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.45.31.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.45.31.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.45.31.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.45.31.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.45.31.7.1">1T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.45.31.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.45.31.8.1">RedPajama, m Common Crawl, S2ORC, Common Crawl</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.46.32">
<td class="ltx_td" id="S2.T1.14.46.32.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.46.32.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.46.32.2.1">Mixtral-8x7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.46.32.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.46.32.3.1">46.7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.46.32.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.46.32.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.46.32.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.46.32.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.46.32.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.46.32.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.46.32.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.46.32.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.46.32.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.46.32.8.1">Instruction dataset</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.47.33">
<td class="ltx_td" id="S2.T1.14.47.33.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.47.33.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.47.33.2.1">Falcon 180B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.47.33.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.47.33.3.1">180B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.47.33.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.47.33.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.47.33.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.47.33.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.47.33.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.47.33.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.47.33.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.47.33.7.1">3.5T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.47.33.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.47.33.8.1">RefinedWeb</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.48.34">
<td class="ltx_td" id="S2.T1.14.48.34.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.48.34.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.48.34.2.1">Gemini</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.48.34.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.48.34.3.1">1.8B, 3.25B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.48.34.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.48.34.4.1">2023</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td" id="S2.T1.14.48.34.5" style="width:42.7pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.48.34.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.48.34.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.48.34.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.48.34.7.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.48.34.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.48.34.8.1">Web documents, books, and code, image data, audio data, video data</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.49.35">
<td class="ltx_td" id="S2.T1.14.49.35.1" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.49.35.2" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.49.35.2.1">DeepSeek-Coder</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.49.35.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.49.35.3.1">1.3B, 6.7B, 33B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.49.35.4" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.49.35.4.1">2024</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.49.35.5" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.49.35.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.49.35.6" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.49.35.6.1">✓</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.49.35.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.49.35.7.1">2T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.14.49.35.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.49.35.8.1">GitHub’s Markdown and StackExchange</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.14.14">
<td class="ltx_td ltx_border_bb" id="S2.T1.14.14.2" style="width:68.3pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.14.14.3" style="width:48.4pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.14.3.1">DocLLM</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.14.14.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.14.4.1">1B,7B</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.14.14.5" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.14.5.1">2024</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.14.14.6" style="width:42.7pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.14.6.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.14.14.1" style="width:22.8pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.14.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S2.T1.14.14.1.1.1.m1.1"><semantics id="S2.T1.14.14.1.1.1.m1.1a"><mo id="S2.T1.14.14.1.1.1.m1.1.1" xref="S2.T1.14.14.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T1.14.14.1.1.1.m1.1b"><times id="S2.T1.14.14.1.1.1.m1.1.1.cmml" xref="S2.T1.14.14.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.14.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T1.14.14.1.1.1.m1.1d">×</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.14.14.7" style="width:28.5pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.14.7.1">2T</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.14.14.8" style="width:142.3pt;">
<p class="ltx_p ltx_align_top" id="S2.T1.14.14.8.1">IIT-CDIP Test Collection 1.0, DocBank</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Early Pre-trained Neural Language Models</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Language modeling using neural networks was pioneered by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib40" title="">40</a>]</cite>.
Bengio et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib13" title="">13</a>]</cite> developed one of the first neural language models (NLMs) that are comparable to n-gram models.
Then, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib14" title="">14</a>]</cite> successfully applied NLMs to machine translation.
The release of RNNLM (an open source NLM toolkit) by Mikolov <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib42" title="">42</a>]</cite> helped significantly popularize NLMs.
Afterwards, NLMs based on recurrent neural networks (RNNs) and their variants, such as long short-term memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib19" title="">19</a>]</cite> and gated recurrent unit (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib20" title="">20</a>]</cite>, were widely used for many natural language applications including machine translation, text generation and text classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib43" title="">43</a>]</cite>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Then, the invention of the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib44" title="">44</a>]</cite> marks another milestone in the development of NLMs. By applying self-attention to compute in parallel for every word in a sentence or document an “attention score” to model the influence each word has on another, Transformers allow for much more parallelization than RNNs, which makes it possible to efficiently pre-train very big language models on large amounts of data on GPUs. These pre-trained language models (PLMs) can be fine-tuned for many downstream tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">We group early popular Transformer-based PLMs, based on their neural architectures, into three main categories: encoder-only, decoder-only, and encoder-decoder models.
Comprehensive surveys of early PLMs are provided in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib28" title="">28</a>]</cite>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS1.5.1.1">II-A</span>1 </span>Encoder-only PLMs</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">As the name suggests, the encoder-only models only consist of an encoder network. These models are originally developed for language understanding tasks, such as text classification, where the models need to predict a class label for an input text.
Representative encoder-only models include BERT and its variants, e.g., RoBERTa, ALBERT, DeBERTa, XLM, XLNet, UNILM, as to be described below.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">BERT (Birectional Encoder Representations from Transformers) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib24" title="">24</a>]</cite> is one of the most widely used encoder-only language models. BERT consists of three modules: (1) an embedding module that converts input text into a sequence of embedding vectors, (2) a stack of Transformer encoders that converts embedding vectors into contextual representation vectors, and (3) a fully connected layer that converts the representation vectors (at the final layer) to one-hot vectors.
BERT is pre-trained uses two objectives:
masked language modeling (MLM) and next sentence prediction.
The pre-trained BERT model can be fine-tuned by adding a classifier layer for many language understanding tasks, ranging from text classification, question answering to language inference.
A high-level overview of BERT framework is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F3" title="Figure 3 ‣ II-A1 Encoder-only PLMs ‣ II-A Early Pre-trained Neural Language Models ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">3</span></a>.
As BERT significantly improved state of the art on a wide range of language understanding tasks when it was published, the AI community was inspired to develop many similar encoder-only language models based on BERT.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="154" id="S2.F3.g1" src="extracted/5420339/img/bert_arch.png" width="375">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Overall pre-training and fine-tuning procedures for BERT. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib24" title="">24</a>]</cite></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1">RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib25" title="">25</a>]</cite> significantly improves the robustness of BERT using a set of model design choices and training strategies, such as modifying a few key hyperparameters, removing the next-sentence pre-training objective and training with much larger mini-batches and learning rates.
ALBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib45" title="">45</a>]</cite> uses two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT: (1) splitting the embedding matrix into two smaller matrices, and (2) using repeating layers split among groups.
DeBERTa (Decoding-enhanced BERT with disentangled attention) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib26" title="">26</a>]</cite> improves the BERT and RoBERTa models using two novel techniques.
The first is the disentangled attention mechanism, where
each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training.
In addition, a novel virtual adversarial training method is used for fine-tuning to improve models’ generalization.
ELECTRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib46" title="">46</a>]</cite> uses a new pre-training task, known as replaced token detection (RTD), which is empirically proven to be more sample-efficient than MLM.
Instead of masking the input, RTD corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained to predict whether a token in the corrupted input was replaced by a generated sample or not.
RTD is more sample-efficient than MLM because the former is defined over all input tokens rather than just the small subset being masked out, as illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F4" title="Figure 4 ‣ II-A1 Encoder-only PLMs ‣ II-A Early Pre-trained Neural Language Models ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">4</span></a>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="198" id="S2.F4.g1" src="extracted/5420339/img/electra.png" width="436">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">A comparison between replaced token detection and masked language modeling. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib46" title="">46</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1">XLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib47" title="">47</a>]</cite> extended BERT to cross-lingual language models using two methods: (1) a unsupervised method that only relies on monolingual data, and (2) a supervised method that leverages parallel data with a new cross-lingual language model objective, as illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F5" title="Figure 5 ‣ II-A1 Encoder-only PLMs ‣ II-A Early Pre-trained Neural Language Models ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a>.
XLMs had obtained state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation, at the time they were proposed.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="214" id="S2.F5.g1" src="extracted/5420339/img/xlm.png" width="386">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S2.F5.3.2" style="font-size:90%;"> Cross-lingual language model pretraining. The MLM objective is similar to BERT, but with continuous streams of text as opposed to sentence pairs. The TLM objective extends MLM to pairs of parallel sentences. To
predict a masked English word, the model can attend to both the English sentence and its French translation, and is encouraged to align English and French representations. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib47" title="">47</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS1.p5">
<p class="ltx_p" id="S2.SS1.SSS1.p5.1">There are also encoder-only language models that leverage the advantages
of auto-regressive (decoder) models for model training and inference. Two examples are XLNet and UNILM.
XLNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib48" title="">48</a>]</cite> is based on Transformer-XL, pre-trained using a generalized autoregressive method that enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order.
UNILM (UNIfied pre-trained Language Model) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib49" title="">49</a>]</cite> is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. This is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction is conditioned on, as illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F6" title="Figure 6 ‣ II-A1 Encoder-only PLMs ‣ II-A Early Pre-trained Neural Language Models ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">6</span></a>.
The pre-trained model can be fine-tuned for both natural language understanding and generation tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="244" id="S2.F6.g1" src="extracted/5420339/img/UNILM.png" width="330">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S2.F6.3.2" style="font-size:90%;">Overview of unified LM pre-training. The model parameters are shared across the LM
objectives (i.e., bidirectional LM, unidirectional LM, and sequence-to-sequence LM). Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib49" title="">49</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS2.5.1.1">II-A</span>2 </span>Decoder-only PLMs</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Two of the most widely used decoder-only PLMs are GPT-1 and GPT-2, developed by OpenAI. These models lay the foundation to more powerful LLMs subsequently, i.e., GPT-3 and GPT-4.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1">GPT-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib50" title="">50</a>]</cite> demonstrates for the first time that good performance over a wide range of natural language tasks can be obtained by Generative Pre-Training (GPT) of a decoder-only Transformer model on a diverse corpus of unlabeled text in a self-supervised learning fashion (i.e., next word/token prediction), followed by discriminative fine-tuning on each specific downstream task (with much fewer samples),
as illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F7" title="Figure 7 ‣ II-A2 Decoder-only PLMs ‣ II-A Early Pre-trained Neural Language Models ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">7</span></a>.
GPT-1 paves the way for subsequent GPT models, with each version improving upon the architecture and achieving better performance on various language tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="168" id="S2.F7.g1" src="extracted/5420339/img/GPT.png" width="391">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S2.F7.3.2" style="font-size:90%;">High-level overview of GPT pretraining, and fine-tuning steps. Courtesy of OpenAI.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS2.p3">
<p class="ltx_p" id="S2.SS1.SSS2.p3.1">GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib51" title="">51</a>]</cite> shows that language models are able to learn to perform specific natural language tasks without any explicit supervision when trained on a large WebText dataset consisting of millions of webpages.
The GPT-2 model follows the model designs of
GPT-1 with a few modifications: Layer normalization is moved to the input of each sub-block, additional layer normalization is added after the final self-attention block, initialization is modified to account for the accumulation on the residual path and scaling the weights of residual layers, vocabulary size is expanded to 50,25, and context size is increased from 512 to 1024 tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS3.5.1.1">II-A</span>3 </span>Encoder-Decoder PLMs</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib52" title="">52</a>]</cite>, Raffle et al. shows that almost all NLP tasks can be cast as a sequence-to-sequence generation task. Thus, an encoder-decoder language model, by design, is a unified model in that it can perform all natural language understanding and generation tasks.
Representative encoder-decoder PLMs we will review below are T5, mT5, MASS, and BART.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p2">
<p class="ltx_p" id="S2.SS1.SSS3.p2.1">T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib52" title="">52</a>]</cite> is a Text-to-Text Transfer Transformer (T5) model, where transfer learning is effectively exploited for NLP via an introduction of a unified framework in which all NLP tasks are cast as a text-to-text generation task.
mT5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib53" title="">53</a>]</cite> is a multilingual variant of T5, which is pre-trained on a new Common Crawl-based dataset consisting of texts in 101 languages.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p3">
<p class="ltx_p" id="S2.SS1.SSS3.p3.1">MASS (MAsked Sequence to Sequence pre-training) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib54" title="">54</a>]</cite> adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence. The encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and the decoder predicts the masked fragment. In this way, MASS jointly trains the encoder and decoder for language embedding and generation, respectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p4">
<p class="ltx_p" id="S2.SS1.SSS3.p4.1">BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib55" title="">55</a>]</cite> uses a standard sequence-to-sequence translation model architecture. It is pre-trained by corrupting text with an arbitrary noising function, and then learning to reconstruct the original text.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Large Language Model Families</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Large language models (LLMs) mainly refer to transformer-based PLMs that contain tens to hundreds of billions of parameters. Compared to PLMs reviewed above,
LLMs are not only much larger in model size, but also exhibit stronger language understanding and generation and emergent abilities that are not present in smaller-scale models.
In what follows, we review three LLM families: GPT, LLaMA, and PaLM, as illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F8" title="Figure 8 ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">8</span></a>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S2.F8.g1" src="x3.png" width="708">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S2.F8.3.2" style="font-size:90%;">Popular LLM Families.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS1.5.1.1">II-B</span>1 </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS1.6.2">The GPT Family</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Generative Pre-trained Transformers (GPT) are a family of decoder-only Transformer-based language models, developed by OpenAI. This family consists of
GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4, CODEX, and WebGPT.
Although early GPT models, such as GPT-1 and GPT-2, are open-source, recent models, such as GPT-3 and GPT-4, are close-source and can only be accessed via APIs.
GPT-1 and GPT-2 models have been discussed in the early PLM subsection. We start with GPT-3 below.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib56" title="">56</a>]</cite> is a pre-trained autoregressive language model with 175 billion parameters.
GPT-3 is widely considered as the first LLM in that it not only is much larger than previous PLMs, but also for the first time demonstrates emergent abilities that are not observed in previous smaller PLMs.
GPT-3 shows the emergent ability of in-context learning, which means GPT-3 can be applied to any downstream tasks without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.
GPT-3 achieved strong performance on many NLP tasks, including translation, question-answering, and the cloze tasks, as well as several ones that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, 3-digit arithmetic.
Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F9" title="Figure 9 ‣ II-B1 The GPT Family ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">9</span></a> plots the performance of GPT-3 as a function of the number of examples in in-context prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="S2.F9.g1" src="extracted/5420339/img/gpt3_size.png" width="361">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S2.F9.3.2" style="font-size:90%;">GPT-3 shows that larger models make increasingly efficient use of in-context information. It shows in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a
natural language task description. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib56" title="">56</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1">CODEX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib57" title="">57</a>]</cite>, released by OpenAI in March 2023, is a general-purpose programming model that can parse natural language and generate code in response.
CODEX is a descendant of GPT-3, fine-tuned for programming applications on code corpora collected from GitHub.
CODEX powers Microsoft’s GitHub Copilot.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p4">
<p class="ltx_p" id="S2.SS2.SSS1.p4.1">WebGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib58" title="">58</a>]</cite> is another descendant of GPT-3, fine-tuned to answer open-ended questions using a text-based web browser, facilitating users to search and navigate the web.
Specifically, WebGPT is trained in three steps. The first is for WebGPT to learn to mimic human browsing behaviors using human demonstration data. Then, a reward function is learned to predict human preferences. Finally, WebGPT is refined to optimize the reward function via reinforcement learning and rejection sampling.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p5">
<p class="ltx_p" id="S2.SS2.SSS1.p5.1">To enable LLMs to follow expected human instructions, InstructGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib59" title="">59</a>]</cite> is
proposed to align language models with user intent on a wide range of tasks by fine-tuning with human feedback.
Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, a dataset of labeler demonstrations of the desired model behavior is collected. Then GPT-3 is fine-tuned on this dataset. Then, a dataset of human-ranked model outputs is collected to further fine-tune the model using reinforcement learning. The method is known Reinforcement Learning from Human Feedback (RLHF), as shown in <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F10" title="Figure 10 ‣ II-B1 The GPT Family ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">10</span></a>.
The resultant InstructGPT models have shown improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S2.F10.g1" src="extracted/5420339/img/SFT.png" width="366">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S2.F10.3.2" style="font-size:90%;">The high-level overview of RLHF. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib59" title="">59</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.p6">
<p class="ltx_p" id="S2.SS2.SSS1.p6.1">The most important milestone of LLM development is the launch of ChatGPT (Chat Generative Pre-trained Transformer) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib60" title="">60</a>]</cite> on November 30, 2022.
ChatGPT is chatbot that enables users to steer a conversation to complete a wide range of tasks such as question answering, information seeking, text summarization, and more.
ChatGPT is powered by GPT-3.5 (and later by GPT-4), a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p7">
<p class="ltx_p" id="S2.SS2.SSS1.p7.1">GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib33" title="">33</a>]</cite> is the latest and most powerful LLM in the GPT family. Launched in March, 2023, GPT-4 is a multimodal LLM in that it can take image and text as inputs and produce text outputs.
While still less capable than humans in some of the most challenging real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers, as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F11" title="Figure 11 ‣ II-B1 The GPT Family ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">11</span></a>.
Like early GPT models, GPT-4 was first pre-trained to predict next tokens on large text corpora, and then fine-tuned with RLHF to align model behaviors with human-desired ones.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="315" id="S2.F11.g1" src="extracted/5420339/img/gpt4.png" width="352">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S2.F11.3.2" style="font-size:90%;">GPT-4 performance on academic and professional exams, compared with GPT 3.5. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib33" title="">33</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS2.5.1.1">II-B</span>2 </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS2.6.2">The LLaMA Family</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">LLaMA is a collection of foundation language models, released by Meta.
Unlike GPT models, LLaMA models are open-source, i.e., model weights are released to the research community under a noncommercial license.
Thus, the LLaMA family grows rapidly as these models are widely used by many research groups to develop better open-source LLMs to compete the closed-source ones or to develop task-specific LLMs for mission-critical applications.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">The first set of LLaMA models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib32" title="">32</a>]</cite> was released in February 2023, ranging from 7B to 65B parameters.
These models are pre-trained on trillions of tokens, collected from publicly available datasets.
LLaMA uses the transformer architecture of GPT-3, with a few minor architectural modifications, including (1) using a SwiGLU activation function instead of ReLU,
(2) using rotary positional embeddings instead of absolute positional embedding, and (3) using root-mean-squared layer-normalization instead of standard layer-normalization.
The open-source LLaMA-13B model outperforms the proprietary GPT-3 (175B) model on most benchmarks, making it a good baseline for LLM research.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.p3.1">In July 2023, Meta, in partnership with Microsoft, released the LLaMA-2 collection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib61" title="">61</a>]</cite>, which include both foundation language models and Chat models finetuned for dialog, known as LLaMA-2 Chat.
The LLaMA-2 Chat models were reported to outperform other open-source models on many public benchmarks.
Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F12" title="Figure 12 ‣ II-B2 The LLaMA Family ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">12</span></a> shows the training process of LLaMA-2 Chat. The process begins with pre-training LLaMA-2 using publicly available online data. Then, an initial version of LLaMA-2 Chat is built via supervised fine-tuning. Subsequently, the model is iteratively refined using RLHF, rejection sampling and proximal policy optimization. In the RLHF stage, the accumulation of human feedback for revising the reward model is crucial to prevent the reward model from being changed too much, which could hurt the stability of LLaMA model training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="199" id="S2.F12.g1" src="extracted/5420339/img/llama2.png" width="428">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S2.F12.3.2" style="font-size:90%;">Training of LLaMA-2 Chat. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib61" title="">61</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS2.p4">
<p class="ltx_p" id="S2.SS2.SSS2.p4.1">Alpaca <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib62" title="">62</a>]</cite> is fine-tuned from the LLaMA-7B model using 52K instruction-following demonstrations generated in the style of self-instruct using GPT-3.5 (text-davinci-003).
Alpaca is very cost-effective for training, especially for academic research.
On the self-instruct evaluation set, Alpaca performs similarly to GPT-3.5, despite that Alpaca is much smaller.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p5">
<p class="ltx_p" id="S2.SS2.SSS2.p5.1">The Vicuna team has developed a 13B chat model, Vicuna-13B, by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a evaluator shows that Vicuna-13B achieves more than 90% quality of OpenAI’s ChatGPT, and Google’s Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90% of cases.
<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F13" title="Figure 13 ‣ II-B2 The LLaMA Family ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">13</span></a> shows the relative response quality of Vicuna and a few other well-known models by GPT-4.
Another advantage of Vicuna-13B is its relative limited computational demand for model training. The training cost of Vicuna-13B is merely $300.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="124" id="S2.F13.g1" src="extracted/5420339/img/vicuna.png" width="284">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F13.2.1.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text" id="S2.F13.3.2" style="font-size:90%;">Relative Response Quality of Vicuna and a few other well-known models by GPT-4. Courtesy of Vicuna Team.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS2.p6">
<p class="ltx_p" id="S2.SS2.SSS2.p6.1">Like Alpaca and Vicuna, the Guanaco models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib63" title="">63</a>]</cite> are also finetuned LLaMA models using instruction-following data. But the finetuning is done very efficiently using QLoRA such that
finetuning a 65B parameter model can be done on a single 48GB GPU.
QLoRA back-propagates gradients through a frozen, 4-bit quantized pre-trained language model into Low Rank Adapters (LoRA).
The best Guanaco model outperforms all previously released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of fine-tuning on a single GPU.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p7">
<p class="ltx_p" id="S2.SS2.SSS2.p7.1">Koala <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib64" title="">64</a>]</cite> is yet another instruction-following language model built on LLaMA, but with a specific focus on interaction data that include user inputs and responses generated by highly capable closed-source chat models such as ChatGPT.
The Koala-13B model performs competitively with state-of-the-art chat models according to human evaluation based on real-world user prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p8">
<p class="ltx_p" id="S2.SS2.SSS2.p8.1">Mistral-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib65" title="">65</a>]</cite> is a 7B-parameter language model engineered for superior performance and efficiency. Mistral-7B outperforms the best open-source 13B model (LLaMA-2-13B) across all evaluated benchmarks, and the best open-source 34B model (LLaMA-34B) in reasoning, mathematics, and code generation. This model leverages grouped-query attention for faster inference, coupled with sliding window attention to effectively handle sequences of arbitrary length with a
reduced inference cost.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p9">
<p class="ltx_p" id="S2.SS2.SSS2.p9.1">The LLaMA family is growing rapidly, as more instruction-following models have been built on LLaMA or LLaMA-2, including
Code LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib66" title="">66</a>]</cite>, Gorilla <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib67" title="">67</a>]</cite>, Giraffe <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib68" title="">68</a>]</cite>, Vigogne <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib69" title="">69</a>]</cite>, Tulu 65B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib70" title="">70</a>]</cite>, Long LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib71" title="">71</a>]</cite>, and Stable Beluga2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib72" title="">72</a>]</cite>, just to name a few.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS2.SSS3.5.1.1">II-B</span>3 </span><span class="ltx_text ltx_font_bold" id="S2.SS2.SSS3.6.2">The PaLM Family</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">The PaLM (Pathways Language Model) family are developed by Google.
The first PaLM model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib31" title="">31</a>]</cite> was announced in April 2022 and remained private until March 2023. It is a 540B parameter transformer-based LLM.
The model is pre-trained on a high-quality text corpus consisting of 780 billion tokens that comprise a wide range of natural language tasks and use cases.
PaLM is pre-trained on 6144 TPU v4 chips using the Pathways system, which enables highly efficient training across multiple TPU Pods.
PaLM demonstrates continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks.
PaLM-540B outperforms not only state-of-the-art fine-tuned models on a suite of multi-step reasoning tasks, but also on par with humans on the recently released BIG-bench benchmark.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.1">The U-PaLM models of 8B, 62B, and 540B scales are continually trained on PaLM with UL2R, a method of continue training LLMs on a few steps with UL2’s mixture-of-denoiser objective <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib73" title="">73</a>]</cite>. An approximately 2x computational savings rate is reported.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p3">
<p class="ltx_p" id="S2.SS2.SSS3.p3.1">U-PaLM is later instruction-finetuned as Flan-PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib74" title="">74</a>]</cite>. Compared to other instruction finetuning work mentioned above, Flan-PaLM’s finetuning is performed using a much larger number of tasks, larger model sizes, and chain-of-thought data. As a result, Flan-PaLM substantially outperforms previous instruction-following models.
For instance, Flan-PaLM-540B, which is instruction-finetuned on 1.8K tasks, outperforms PaLM-540B by a large margin (+9.4% on average).
The finetuning data comprises 473 datasets, 146 task categories, and 1,836 total tasks, as illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F14" title="Figure 14 ‣ II-B3 The PaLM Family ‣ II-B Large Language Model Families ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">14</span></a>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="284" id="S2.F14.g1" src="extracted/5420339/img/FlanPaLM.png" width="396">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F14.2.1.1" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text" id="S2.F14.3.2" style="font-size:90%;">Flan-PaLM finetuning consist of 473 datasets in above task categories. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib74" title="">74</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS3.p4">
<p class="ltx_p" id="S2.SS2.SSS3.p4.1">PaLM-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib75" title="">75</a>]</cite> is a more compute-efficient LLM with better multilingual and reasoning capabilities, compared to its predecessor PaLM.
PaLM-2 is trained using a mixture of objectives.
Through extensive evaluations on English, multilingual, and reasoning tasks, PaLM-2 significantly improves the model performance on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference than PaLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p5">
<p class="ltx_p" id="S2.SS2.SSS3.p5.1">Med-PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib76" title="">76</a>]</cite> is a domain-specific PaLM, and is designed to provide high-quality answers to medical questions.
Med-PaLM is finetuned on PaLM using instruction prompt tuning, a parameter-efficient method for aligning LLMs to new domains using a few exemplars.
Med-PaLM obtains very encouraging results on many healthcare tasks, although it is still inferior to human clinicians.
Med-PaLM 2 improves Med-PaLM via med-domain finetuning and ensemble prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib77" title="">77</a>]</cite>. Med-PaLM 2 scored up to 86.5% on the MedQA dataset (i.e., a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries), improving upon Med-PaLM by over 19% and setting a new state-of-the-art.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Other Representative LLMs</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">In addition to the models discussed in the previous subsections, there are other popular LLMs which do not belong to those three model families, yet they have achieved great performance and have pushed the LLMs field forward.
We briefly describe these LLMs in this subsection.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p2.1.1">FLAN:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib78" title="">78</a>]</cite>, Wei et al. explored a simple method for improving the zero-shot learning abilities of language models. They showed that instruction tuning language models on a collection of datasets described via instructions substantially improves zero-shot performance on unseen tasks.
They take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates.
They call this instruction-tuned model FLAN.
Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F15" title="Figure 15 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">15</span></a> provides a comparison of instruction tuning with pretrain–finetune and prompting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="134" id="S2.F15.g1" src="extracted/5420339/img/FLAN.png" width="399">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F15.2.1.1" style="font-size:90%;">Figure 15</span>: </span><span class="ltx_text" id="S2.F15.3.2" style="font-size:90%;">comparison of instruction tuning with pretrain–finetune and prompting. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib78" title="">78</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">Gopher:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib79" title="">79</a>]</cite>, Rae et al. presented an analysis of Transformer-based language model performance across a wide range of model scales — from models with tens of millions of parameters up to a 280 billion parameter model called Gopher.
These models were evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority.
The number of layers, the key/value size, and other hyper-parameters of different model sizes are shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F16" title="Figure 16 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">16</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="111" id="S2.F16.g1" src="extracted/5420339/img/gopher.png" width="404">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F16.2.1.1" style="font-size:90%;">Figure 16</span>: </span><span class="ltx_text" id="S2.F16.3.2" style="font-size:90%;">Model architecture details of Gopher with different number of parameters. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib78" title="">78</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p4.1.1">T0:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib80" title="">80</a>]</cite>, Sanh et al. developed T0, a system for easily mapping any natural language tasks into a human-readable prompted form. They converted a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks.
Then, a T0 encoder-decoder model is developed to consume textual
inputs and produces target responses. The model is trained on a multitask mixture of NLP datasets partitioned into different tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p5.1.1">ERNIE 3.0:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib81" title="">81</a>]</cite>, Sun et al. proposed a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks using zero-shot learning, few-shot learning or fine-tuning. They have trained ERNIE 3.0 with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph.
Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F17" title="Figure 17 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">17</span></a> illustrates the model architecture of Ernie 3.0.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S2.F17.g1" src="extracted/5420339/img/ernie3.png" width="387">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F17.2.1.1" style="font-size:90%;">Figure 17</span>: </span><span class="ltx_text" id="S2.F17.3.2" style="font-size:90%;">High-level model architecture of ERNIE 3.0. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib81" title="">81</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p6">
<p class="ltx_p" id="S2.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p6.1.1">RETRO:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib82" title="">82</a>]</cite>, Borgeaud et al. enhanced auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. Using a 2-trillion-token database, the
Retrieval-Enhanced Transformer (Retro) obtains comparable performance to GPT-3 and Jurassic-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib83" title="">83</a>]</cite> on the Pile, despite using 25% fewer parameters.
As shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F18" title="Figure 18 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">18</span></a>, Retro combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F18"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="166" id="S2.F18.g1" src="extracted/5420339/img/retro.png" width="421">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F18.2.1.1" style="font-size:90%;">Figure 18</span>: </span><span class="ltx_text" id="S2.F18.3.2" style="font-size:90%;">Retro architecture. Left: simplified version where a sequence of length n = 12 is split into l = 3 chunks of size m = 4. For each chunk, we retrieve k = 2 neighbours of r = 5 tokens each. The retrieval pathway is shown on top. Right: Details of the interactions in the CCA operator. Causality is maintained as neighbours of the first chunk only affect the last token of the first chunk and tokens from the second chunk. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib82" title="">82</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p7">
<p class="ltx_p" id="S2.SS3.p7.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p7.1.1">GLaM:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib84" title="">84</a>]</cite>, Du et al. proposed a family of LLMs named GLaM (Generalist Language Model),
which use a sparsely activated mixture-of-experts
architecture to scale the model capacity while also
incurring substantially less training cost compared
to dense variants.
The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero, one and few-shot performance across 29 NLP tasks.
Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F19" title="Figure 19 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">19</span></a> shows the high-level architecture of GLAM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F19"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="249" id="S2.F19.g1" src="extracted/5420339/img/GLAM.png" width="203">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F19.2.1.1" style="font-size:90%;">Figure 19</span>: </span><span class="ltx_text" id="S2.F19.3.2" style="font-size:90%;">GLaM model architecture. Each MoE layer (the bottom
block) is interleaved with a Transformer layer (the upper block). Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib84" title="">84</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p8">
<p class="ltx_p" id="S2.SS3.p8.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p8.1.1">LaMDA:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib85" title="">85</a>]</cite>, Thoppilan et al. presented LaMDA, a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text.
They showed that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p9">
<p class="ltx_p" id="S2.SS3.p9.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p9.1.1">OPT:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib86" title="">86</a>]</cite>, Zhang et al. presented Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which they share with researchers.
The OPT models’ parameters are shown in <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F20" title="Figure 20 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">20</span></a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F20"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="134" id="S2.F20.g1" src="extracted/5420339/img/opt.png" width="192">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F20.2.1.1" style="font-size:90%;">Figure 20</span>: </span><span class="ltx_text" id="S2.F20.3.2" style="font-size:90%;">Different OPT Models’ architecture details. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib86" title="">86</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p10">
<p class="ltx_p" id="S2.SS3.p10.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p10.1.1">Chinchilla:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib2" title="">2</a>]</cite>, Hoffmann et al. investigated the optimal model size and number of tokens for training a transformer language model under a given compute budget.
By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, they found that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled.
They tested this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4% more more data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p11">
<p class="ltx_p" id="S2.SS3.p11.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p11.1.1">Galactica:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib87" title="">87</a>]</cite>, Taylor et al. introduced Galactica, a large language model that can store, combine and reason about scientific knowledge. They trained on a large scientific corpus of papers, reference material, knowledge bases and many other sources.
Galactica performed well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p12">
<p class="ltx_p" id="S2.SS3.p12.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p12.1.1">CodeGen:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib88" title="">88</a>]</cite>, Nijkamp et al. trained and released a family of large
language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open sourced the training library JAXFORMER. They showed the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. They further investigated the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying sub-problems. They also constructed an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p13">
<p class="ltx_p" id="S2.SS3.p13.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p13.1.1">AlexaTM:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib89" title="">89</a>]</cite>, Soltan et al. demonstrated that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various task.
They trained a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and showed that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM consist of 46 encoder layers, 32 decoder layers, 32 attention heads, and <math alttext="d_{model}=4096" class="ltx_Math" display="inline" id="S2.SS3.p13.1.m1.1"><semantics id="S2.SS3.p13.1.m1.1a"><mrow id="S2.SS3.p13.1.m1.1.1" xref="S2.SS3.p13.1.m1.1.1.cmml"><msub id="S2.SS3.p13.1.m1.1.1.2" xref="S2.SS3.p13.1.m1.1.1.2.cmml"><mi id="S2.SS3.p13.1.m1.1.1.2.2" xref="S2.SS3.p13.1.m1.1.1.2.2.cmml">d</mi><mrow id="S2.SS3.p13.1.m1.1.1.2.3" xref="S2.SS3.p13.1.m1.1.1.2.3.cmml"><mi id="S2.SS3.p13.1.m1.1.1.2.3.2" xref="S2.SS3.p13.1.m1.1.1.2.3.2.cmml">m</mi><mo id="S2.SS3.p13.1.m1.1.1.2.3.1" xref="S2.SS3.p13.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S2.SS3.p13.1.m1.1.1.2.3.3" xref="S2.SS3.p13.1.m1.1.1.2.3.3.cmml">o</mi><mo id="S2.SS3.p13.1.m1.1.1.2.3.1a" xref="S2.SS3.p13.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S2.SS3.p13.1.m1.1.1.2.3.4" xref="S2.SS3.p13.1.m1.1.1.2.3.4.cmml">d</mi><mo id="S2.SS3.p13.1.m1.1.1.2.3.1b" xref="S2.SS3.p13.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S2.SS3.p13.1.m1.1.1.2.3.5" xref="S2.SS3.p13.1.m1.1.1.2.3.5.cmml">e</mi><mo id="S2.SS3.p13.1.m1.1.1.2.3.1c" xref="S2.SS3.p13.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S2.SS3.p13.1.m1.1.1.2.3.6" xref="S2.SS3.p13.1.m1.1.1.2.3.6.cmml">l</mi></mrow></msub><mo id="S2.SS3.p13.1.m1.1.1.1" xref="S2.SS3.p13.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS3.p13.1.m1.1.1.3" xref="S2.SS3.p13.1.m1.1.1.3.cmml">4096</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p13.1.m1.1b"><apply id="S2.SS3.p13.1.m1.1.1.cmml" xref="S2.SS3.p13.1.m1.1.1"><eq id="S2.SS3.p13.1.m1.1.1.1.cmml" xref="S2.SS3.p13.1.m1.1.1.1"></eq><apply id="S2.SS3.p13.1.m1.1.1.2.cmml" xref="S2.SS3.p13.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS3.p13.1.m1.1.1.2.1.cmml" xref="S2.SS3.p13.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS3.p13.1.m1.1.1.2.2.cmml" xref="S2.SS3.p13.1.m1.1.1.2.2">𝑑</ci><apply id="S2.SS3.p13.1.m1.1.1.2.3.cmml" xref="S2.SS3.p13.1.m1.1.1.2.3"><times id="S2.SS3.p13.1.m1.1.1.2.3.1.cmml" xref="S2.SS3.p13.1.m1.1.1.2.3.1"></times><ci id="S2.SS3.p13.1.m1.1.1.2.3.2.cmml" xref="S2.SS3.p13.1.m1.1.1.2.3.2">𝑚</ci><ci id="S2.SS3.p13.1.m1.1.1.2.3.3.cmml" xref="S2.SS3.p13.1.m1.1.1.2.3.3">𝑜</ci><ci id="S2.SS3.p13.1.m1.1.1.2.3.4.cmml" xref="S2.SS3.p13.1.m1.1.1.2.3.4">𝑑</ci><ci id="S2.SS3.p13.1.m1.1.1.2.3.5.cmml" xref="S2.SS3.p13.1.m1.1.1.2.3.5">𝑒</ci><ci id="S2.SS3.p13.1.m1.1.1.2.3.6.cmml" xref="S2.SS3.p13.1.m1.1.1.2.3.6">𝑙</ci></apply></apply><cn id="S2.SS3.p13.1.m1.1.1.3.cmml" type="integer" xref="S2.SS3.p13.1.m1.1.1.3">4096</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p13.1.m1.1c">d_{model}=4096</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p13.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT = 4096</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p14">
<p class="ltx_p" id="S2.SS3.p14.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p14.1.1">Sparrow:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib90" title="">90</a>]</cite>, Glaese et al. presented Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. They used reinforcement learning from human feedback to train their models with two new additions to help human raters judge agent behaviour.
The high-level pipeline of Sparrow model is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F21" title="Figure 21 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">21</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F21"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="172" id="S2.F21.g1" src="extracted/5420339/img/sparrow.png" width="329">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F21.2.1.1" style="font-size:90%;">Figure 21</span>: </span><span class="ltx_text" id="S2.F21.3.2" style="font-size:90%;">Sparrow pipeline relies on human participation to continually expand a training set. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib90" title="">90</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p15">
<p class="ltx_p" id="S2.SS3.p15.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p15.1.1">Minerva:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib91" title="">91</a>]</cite>, Lewkowycz et al. introduced Minerva, a large language model pretrained on general natural language data and further trained on technical content, to tackle previous LLM struggle with quantitative reasoning (such as solving mathematics, science, and engineering problems).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p16">
<p class="ltx_p" id="S2.SS3.p16.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p16.1.1">MoD:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib92" title="">92</a>]</cite>, Tay et al. presented a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. They proposed Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together.
This framework is known as Unifying Language Learning (UL2).
An overview of UL2 pretraining paradigm is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F21" title="Figure 21 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">21</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F22"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="208" id="S2.F22.g1" src="extracted/5420339/img/ul2.png" width="328">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F22.2.1.1" style="font-size:90%;">Figure 22</span>: </span><span class="ltx_text" id="S2.F22.3.2" style="font-size:90%;">An overview of UL2 pretraining paradigm. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib92" title="">92</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p17">
<p class="ltx_p" id="S2.SS3.p17.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p17.1.1">BLOOM:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib93" title="">93</a>]</cite>, Scao et al. presented BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total).
An overview of BLOOM architecture is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F23" title="Figure 23 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">23</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F23"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="184" id="S2.F23.g1" src="extracted/5420339/img/bloom.png" width="379">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F23.2.1.1" style="font-size:90%;">Figure 23</span>: </span><span class="ltx_text" id="S2.F23.3.2" style="font-size:90%;">An overview of BLOOM architecture. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib93" title="">93</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p18">
<p class="ltx_p" id="S2.SS3.p18.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p18.1.1">GLM:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib94" title="">94</a>]</cite>, Zeng et al. introduced GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It was an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale
can be successfully pre-trained.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p19">
<p class="ltx_p" id="S2.SS3.p19.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p19.1.1">Pythia:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib95" title="">95</a>]</cite>, Biderman et al. introduced Pythia, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p20">
<p class="ltx_p" id="S2.SS3.p20.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p20.1.1">Orca:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib96" title="">96</a>]</cite>, Mukherjee et al. develop Orca, a 13-billion parameter model that learns to imitate the reasoning process of large foundation models. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p21">
<p class="ltx_p" id="S2.SS3.p21.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p21.1.1">StarCoder:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib97" title="">97</a>]</cite>, Li et al. introduced
StarCoder and StarCoderBase. They are 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention.
StarCoderBase is trained on one trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process.
They fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. They performed the most comprehensive
evaluation of Code LLMs to date and showed that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p22">
<p class="ltx_p" id="S2.SS3.p22.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p22.1.1">KOSMOS:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib98" title="">98</a>]</cite>, Huang et al. introduced
KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e. zero-shot). Specifically, they trained KOSMOS-1 from scratch on web-scale multi-modal corpora, including arbitrarily interleaved text and images, image-caption
pairs, and text data.
Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understanding, generation, and
even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p23">
<p class="ltx_p" id="S2.SS3.p23.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p23.1.1">Gemini:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib99" title="">99</a>]</cite>, Gemini team introduced a new family of multimodal models, that exhibit promising capabilities across image, audio, video, and text understanding. Gemini family includes three versions: Ultra for highly-complex tasks, Pro for enhanced performance and deployability at scale, and Nano for on-device applications.
Gemini architecture is built on top of Transformer decoders, and is trained to support 32k context length (via using efficient attention mechanisms).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S2.F24"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="505" id="S2.F24.g1" src="extracted/5420339/img/timeline.png" width="1062">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F24.6.3.1" style="font-size:90%;">Figure 24</span>: </span><span class="ltx_text" id="S2.F24.4.2" style="font-size:90%;">Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our #parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way for their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models.
<math alttext="\clubsuit" class="ltx_Math" display="inline" id="S2.F24.3.1.m1.1"><semantics id="S2.F24.3.1.m1.1b"><mi id="S2.F24.3.1.m1.1.1" mathvariant="normal" xref="S2.F24.3.1.m1.1.1.cmml">♣</mi><annotation-xml encoding="MathML-Content" id="S2.F24.3.1.m1.1c"><ci id="S2.F24.3.1.m1.1.1.cmml" xref="S2.F24.3.1.m1.1.1">♣</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F24.3.1.m1.1d">\clubsuit</annotation><annotation encoding="application/x-llamapun" id="S2.F24.3.1.m1.1e">♣</annotation></semantics></math> shows entities that serve not only as models but also as approaches. <math alttext="\blacklozenge" class="ltx_Math" display="inline" id="S2.F24.4.2.m2.1"><semantics id="S2.F24.4.2.m2.1b"><mi id="S2.F24.4.2.m2.1.1" mathvariant="normal" xref="S2.F24.4.2.m2.1.1.cmml">◆</mi><annotation-xml encoding="MathML-Content" id="S2.F24.4.2.m2.1c"><ci id="S2.F24.4.2.m2.1.1.cmml" xref="S2.F24.4.2.m2.1.1">◆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F24.4.2.m2.1d">\blacklozenge</annotation><annotation encoding="application/x-llamapun" id="S2.F24.4.2.m2.1e">◆</annotation></semantics></math> shows only approaches.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p24">
<p class="ltx_p" id="S2.SS3.p24.1">Some of the other popular LLM frameworks (or techniques used for efficient developments of LLMs) includes
Inner-Monologue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib100" title="">100</a>]</cite>,
Megatron-Turing NLG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib101" title="">101</a>]</cite>,
LongFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib102" title="">102</a>]</cite>,
OPT-IML <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib103" title="">103</a>]</cite>,
MeTaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib104" title="">104</a>]</cite>,
Dromedary <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib105" title="">105</a>]</cite>,
Palmyra <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib106" title="">106</a>]</cite>,
Camel <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib107" title="">107</a>]</cite>,
Yalm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib108" title="">108</a>]</cite>,
MPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib109" title="">109</a>]</cite>,
ORCA-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib110" title="">110</a>]</cite>,
Gorilla <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib67" title="">67</a>]</cite>,
PAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib111" title="">111</a>]</cite>,
Claude <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib112" title="">112</a>]</cite>,
CodeGen 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib113" title="">113</a>]</cite>,
Zephyr <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib114" title="">114</a>]</cite>,
Grok <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib115" title="">115</a>]</cite>,
Qwen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib116" title="">116</a>]</cite>,
Mamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib30" title="">30</a>]</cite>,
Mixtral-8x7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib117" title="">117</a>]</cite>,
DocLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib118" title="">118</a>]</cite>,
DeepSeek-Coder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib119" title="">119</a>]</cite>,
FuseLLM-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib120" title="">120</a>]</cite>,
TinyLlama-1.1B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib121" title="">121</a>]</cite>,
LLaMA-Pro-8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib122" title="">122</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p25">
<p class="ltx_p" id="S2.SS3.p25.1">Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S2.F24" title="Figure 24 ‣ II-C Other Representative LLMs ‣ II Large Language Models ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">24</span></a> provides an overview of some of the most representative LLM frameworks, and the relevant works that have contributed to the success of LLMs and helped to push the limits of LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">How LLMs Are Built</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first review the popular architectures used for LLMs, and then discuss data and modeling techniques ranging from data preparation, tokenization, to pre-training, instruction tuning, and alignment.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Once the model architecture is chosen, the major steps involved in training an LLM includes: data preparation (collection, cleaning, deduping, etc.), tokenization, model pre-training (in a self-supervised learning fashion), instruction tuning, and alignment.
We will explain each of them in a separate subsection below.
These steps are also illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F25" title="Figure 25 ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">25</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F25"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="824" id="S3.F25.g1" src="x4.png" width="730">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F25.2.1.1" style="font-size:90%;">Figure 25</span>: </span><span class="ltx_text" id="S3.F25.3.2" style="font-size:90%;">This figure shows different components of LLMs.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Dominant LLM Architectures</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The most widely used LLM architectures are encoder-only, decoder-only, and encoder-decoder. Most of them are based on Transformer (as the building block). Therefore we also review the Transformer architecture here.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.5.1.1">III-A</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.6.2">Transformer</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">in a ground-breaking work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib44" title="">44</a>]</cite>, Vaswani et al. proposed the Transformer framework, which was originally designed for effective parallel computing using GPUs. The heart of Transformer is the (self-)attention mechanism, which can capture long-term contextual information much more effectively using GPUs than the recurrence and convolution mechanisms.
Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F26" title="Figure 26 ‣ III-A1 Transformer ‣ III-A Dominant LLM Architectures ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">26</span></a> provides a high-level overview of transformer work. In this section we provide an overview of the main elements and variants, see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib123" title="">123</a>]</cite> for more details.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.5">The Transformer language model architecture, originally proposed for machine translation, consists of an encoder and a decoder.
The encoder is composed of a stack of N = 6 identical Transformer layers. Each layer has two sub-layers. The first one is a multi-head self-attention layer, and the other one is a simple position-wise fully connected feed-forward network.
The decoder is composed of a stack of 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder has a third sub-layer, which performs multi-head attention over the output of the encoder stack.
The attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.
The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.
Instead of performing a single attention function with <math alttext="d_{model}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.1.m1.1"><semantics id="S3.SS1.SSS1.p2.1.m1.1a"><msub id="S3.SS1.SSS1.p2.1.m1.1.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.1.1.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml">d</mi><mrow id="S3.SS1.SSS1.p2.1.m1.1.1.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p2.1.m1.1.1.3.2" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.2.cmml">m</mi><mo id="S3.SS1.SSS1.p2.1.m1.1.1.3.1" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.p2.1.m1.1.1.3.3" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.3.cmml">o</mi><mo id="S3.SS1.SSS1.p2.1.m1.1.1.3.1a" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.p2.1.m1.1.1.3.4" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.4.cmml">d</mi><mo id="S3.SS1.SSS1.p2.1.m1.1.1.3.1b" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.p2.1.m1.1.1.3.5" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.5.cmml">e</mi><mo id="S3.SS1.SSS1.p2.1.m1.1.1.3.1c" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.SSS1.p2.1.m1.1.1.3.6" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.1.m1.1b"><apply id="S3.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.2">𝑑</ci><apply id="S3.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3"><times id="S3.SS1.SSS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.1"></times><ci id="S3.SS1.SSS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.2">𝑚</ci><ci id="S3.SS1.SSS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS1.SSS1.p2.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.4">𝑑</ci><ci id="S3.SS1.SSS1.p2.1.m1.1.1.3.5.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.5">𝑒</ci><ci id="S3.SS1.SSS1.p2.1.m1.1.1.3.6.cmml" xref="S3.SS1.SSS1.p2.1.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.1.m1.1c">d_{model}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT</annotation></semantics></math> dimensional keys, values and queries,
it is found to be beneficial to linearly project the queries, keys and values <math alttext="h" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.2.m2.1"><semantics id="S3.SS1.SSS1.p2.2.m2.1a"><mi id="S3.SS1.SSS1.p2.2.m2.1.1" xref="S3.SS1.SSS1.p2.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.2.m2.1b"><ci id="S3.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p2.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.2.m2.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.2.m2.1d">italic_h</annotation></semantics></math> with different, learned linear projections to <math alttext="d_{k}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.3.m3.1"><semantics id="S3.SS1.SSS1.p2.3.m3.1a"><msub id="S3.SS1.SSS1.p2.3.m3.1.1" xref="S3.SS1.SSS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.SSS1.p2.3.m3.1.1.2" xref="S3.SS1.SSS1.p2.3.m3.1.1.2.cmml">d</mi><mi id="S3.SS1.SSS1.p2.3.m3.1.1.3" xref="S3.SS1.SSS1.p2.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.3.m3.1b"><apply id="S3.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.2">𝑑</ci><ci id="S3.SS1.SSS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p2.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.3.m3.1c">d_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.3.m3.1d">italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="d_{k}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.4.m4.1"><semantics id="S3.SS1.SSS1.p2.4.m4.1a"><msub id="S3.SS1.SSS1.p2.4.m4.1.1" xref="S3.SS1.SSS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.SSS1.p2.4.m4.1.1.2" xref="S3.SS1.SSS1.p2.4.m4.1.1.2.cmml">d</mi><mi id="S3.SS1.SSS1.p2.4.m4.1.1.3" xref="S3.SS1.SSS1.p2.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.4.m4.1b"><apply id="S3.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.2">𝑑</ci><ci id="S3.SS1.SSS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p2.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.4.m4.1c">d_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.4.m4.1d">italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="d_{v}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p2.5.m5.1"><semantics id="S3.SS1.SSS1.p2.5.m5.1a"><msub id="S3.SS1.SSS1.p2.5.m5.1.1" xref="S3.SS1.SSS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.SSS1.p2.5.m5.1.1.2" xref="S3.SS1.SSS1.p2.5.m5.1.1.2.cmml">d</mi><mi id="S3.SS1.SSS1.p2.5.m5.1.1.3" xref="S3.SS1.SSS1.p2.5.m5.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p2.5.m5.1b"><apply id="S3.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1.2">𝑑</ci><ci id="S3.SS1.SSS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.SSS1.p2.5.m5.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p2.5.m5.1c">d_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p2.5.m5.1d">italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> dimensions, respectively.
Positional encoding is incorporated to fuse information about the relative or absolute position of the tokens in the sequence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F26"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="378" id="S3.F26.g1" src="extracted/5420339/img/transformer.png" width="282">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F26.2.1.1" style="font-size:90%;">Figure 26</span>: </span><span class="ltx_text" id="S3.F26.3.2" style="font-size:90%;">High-level overview of transformer work. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib44" title="">44</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS2.5.1.1">III-A</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.6.2">Encoder-Only</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">For this family, at each stage, the attention layers can access all the words in the initial sentence. The pre-training of these models usually consist of somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence. Encoder models are great for tasks requiring an understanding of the full sequence, such as sentence classification, named entity recognition, and extractive question answering.
One prominent encoder only model is BERT (Bidirectional Encoder Representations from Transformers), proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib24" title="">24</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS3.5.1.1">III-A</span>3 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS3.6.2">Decoder-Only</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">For these models, at each stage, for any word, the attention layers can only access the words positioned before that in the sentence. These models are also sometimes called auto-regressive models.
The pretraining of these models is usually formulated as predicting the next word (or token) in the sequence. The decoder-only models are best suited for tasks involving text generation. GPT models are prominent example of this model category.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS4.5.1.1">III-A</span>4 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS4.6.2">Encoder-Decoder</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">These models use both encoder and decoder, and are sometimes called sequence-to-sequence models. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder only accesses the words positioned before a given word in the input.
These models are usually pre-trained using the objectives of encoder or decoder models, but usually involve something a bit more complex. For instance, some models are pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.
Encoder-decoder models are best suited for tasks about generating new sentences conditioned on a given input, such as summarization, translation, or generative question answering.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Data Cleaning</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Data quality is crucial to the performance of language models trained on them. Data cleaning techniques such as filtering, deduplication, are shown to have a big impact on the model performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">As an example, in <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Falcon40B</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib124" title="">124</a>]</cite>, Penedo et al. showed that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, they were able to obtain five trillion tokens from CommonCrawl. They also released an extract of 600 billion tokens from our REFINEDWEB dataset, and 1.3/7.5B parameters language models trained on it. <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F27" title="Figure 27 ‣ III-B Data Cleaning ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">27</span></a> shows the Refinement process of CommonCrawl data by this work.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F27"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="199" id="S3.F27.g1" src="extracted/5420339/img/Falcon40B_filtering.png" width="360">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F27.2.1.1" style="font-size:90%;">Figure 27</span>: </span><span class="ltx_text" id="S3.F27.3.2" style="font-size:90%;">Subsequent stages of Macrodata Refinement remove nearly 90% of the documents originally in CommonCrawl. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib124" title="">124</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.5.1.1">III-B</span>1 </span>Data Filtering</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Data filtering aims to enhance the quality of training data and the effectiveness of the trained LLMs.
Common data filtering techniques include:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.1.1">Removing Noise:</span> refers to eliminating irrelevant or noisy data that might impact the model’s ability to generalize well. As an example, one can think of removing false information from the training data, to lower the chance of model generating false responses.
Two mainstream approaches for quality filtering includes: classifier-based, and heuristic-based frameworks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p3.1.1">Handling Outliers:</span> Identifying and handling outliers or anomalies in the data to prevent them from disproportionately influencing the model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p4.1.1">Addressing Imbalances:</span> Balancing the distribution of classes or categories in the dataset to avoid biases and ensure fair representation. This is specially useful for responsible model training and evaluation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p5.1.1">Text Preprocessing:</span> Cleaning and standardizing text data by removing stop words, punctuation, or other elements that may not contribute significantly to the model’s learning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p6">
<p class="ltx_p" id="S3.SS2.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p6.1.1">Dealing with Ambiguities:</span> Resolving or excluding ambiguous or contradictory data that might confuse the model during training. This can help the model to provide more definite and reliable answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.5.1.1">III-B</span>2 </span>Deduplication</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">De-duplication refers to the process of removing duplicate instances or repeated occurrences of the same data in a dataset.
Duplicate data points can introduce biases in the model training process and reduce the diversity, as the model may learn from the same examples multiple times, potentially leading to overfitting on those particular instances.
Some works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib125" title="">125</a>]</cite> have shown that de-duplication improves models’ ability to generalize to new, unseen data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">The de-duplication process is particularly important when dealing with large datasets, as duplicates can unintentionally inflate the importance of certain patterns or characteristics. This is especially relevant in NLP tasks, where diverse and representative training data is crucial for building robust language models.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">The specific de-duplication method can vary based on the nature of the data and the requirements of the particular language model being trained. It may involve comparing entire data points or specific features to identify and eliminate duplicates.
At the document level, existing works mainly rely on the overlap ratio of high-level features (e.g. n-grams overlap) between documents to detect duplicate samples.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Tokenizations</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Tokenization referes to the process of converting a sequence of text into smaller parts, known as tokens. While the simplest tokenization tool simply chops text into tokens based on white space, most tokenization tools rely on a word dictionary.
However, out-of-vocabulary (OOV) is a problem in this case because the tokenizer only knows words in its dictionary.
To increase the coverage of dictionaries, popular tokenizers used for LLMs are based on sub-words, which can be combined to form a large number of words, including the words unseen in training data or words in different languages.
In what follows, we describe three popular tokenizers.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS1.5.1.1">III-C</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.6.2">BytePairEncoding</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.SS3.SSS1.p1.1.1">BytePairEncoding</span> is originally a type of data compression algorithm that uses frequent patterns at byte level to compress the data. By definition, this algorithm mainly tries to keep the frequent words in their original form and break down ones that are not common. This simple paradigm keeps the vocabulary not very large, but also good enough to represent common words at the same time. Also morphological forms of the frequent words can be represented very well if suffix or prefix is also commonly presented in the training data of the algorithm.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS2.5.1.1">III-C</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.6.2">WordPieceEncoding</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">This algorithm is mainly used for very well-known models such as BERT and Electra. At the beginning of training, the algorithm takes all the alphabet from the training data to make sure that nothing will be left as UNK or <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p1.1.1">unknown</span> from the training dataset. This case happens when the model is given an input that can not be tokenized by the tokenizer. It mostly happens in cases where some characters are not tokenizable by it. Similar to BytePairEncoding, it tries to maximize the likelihood of putting all the tokens in vocabulary based on their frequency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS3.5.1.1">III-C</span>3 </span><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.6.2">SentencePieceEncoding</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Although both tokenizers described before are strong and have many advantages compared to white-space tokenization, they still take assumption of words being always separated by white-space as granted. This assumption is not always true, in fact in some languages, words can be corrupted by many noisy elements such as unwanted spaces or even invented words. SentencePieceEncoding tries to address this issue.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_bold" id="S3.SS4.6.2">Positional Encoding</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS1.5.1.1">III-D</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.6.2">Absolute Positional Embeddings</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">(APE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib44" title="">44</a>]</cite> has been used in the original Transformer model to preserve the information of sequence order. Therefore, the positional information of words is added to the input embeddings at the bottom of both the encoder and decoder stacks. There are various options for positional encodings, either learned or fixed. In the vanilla Transformer, sine and cosine functions are employed for this purpose. The main drawback of using APE in Transformers is the restriction to a certain number of tokens. Additionally, APE fails to account for the relative distances between tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS2.5.1.1">III-D</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.6.2">Relative Positional Embeddings</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">(RPE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib126" title="">126</a>]</cite> involves extending self-attention to take into account the pairwise links between input elements. RPE is added to the model at two levels: first as an additional component to the keys, and subsequently as a sub-component of the values matrix. This approach looks at the input as a fully-connected graph with labels and directed edges. In the case of linear sequences, edges can capture information about the relative position differences between input elements. A clipping distance, represented as k <math alttext="2\leq k\leq n-4" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p1.1.m1.1"><semantics id="S3.SS4.SSS2.p1.1.m1.1a"><mrow id="S3.SS4.SSS2.p1.1.m1.1.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.cmml"><mn id="S3.SS4.SSS2.p1.1.m1.1.1.2" xref="S3.SS4.SSS2.p1.1.m1.1.1.2.cmml">2</mn><mo id="S3.SS4.SSS2.p1.1.m1.1.1.3" xref="S3.SS4.SSS2.p1.1.m1.1.1.3.cmml">≤</mo><mi id="S3.SS4.SSS2.p1.1.m1.1.1.4" xref="S3.SS4.SSS2.p1.1.m1.1.1.4.cmml">k</mi><mo id="S3.SS4.SSS2.p1.1.m1.1.1.5" xref="S3.SS4.SSS2.p1.1.m1.1.1.5.cmml">≤</mo><mrow id="S3.SS4.SSS2.p1.1.m1.1.1.6" xref="S3.SS4.SSS2.p1.1.m1.1.1.6.cmml"><mi id="S3.SS4.SSS2.p1.1.m1.1.1.6.2" xref="S3.SS4.SSS2.p1.1.m1.1.1.6.2.cmml">n</mi><mo id="S3.SS4.SSS2.p1.1.m1.1.1.6.1" xref="S3.SS4.SSS2.p1.1.m1.1.1.6.1.cmml">−</mo><mn id="S3.SS4.SSS2.p1.1.m1.1.1.6.3" xref="S3.SS4.SSS2.p1.1.m1.1.1.6.3.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p1.1.m1.1b"><apply id="S3.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1"><and id="S3.SS4.SSS2.p1.1.m1.1.1a.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1"></and><apply id="S3.SS4.SSS2.p1.1.m1.1.1b.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1"><leq id="S3.SS4.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.3"></leq><cn id="S3.SS4.SSS2.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS4.SSS2.p1.1.m1.1.1.2">2</cn><ci id="S3.SS4.SSS2.p1.1.m1.1.1.4.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.4">𝑘</ci></apply><apply id="S3.SS4.SSS2.p1.1.m1.1.1c.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1"><leq id="S3.SS4.SSS2.p1.1.m1.1.1.5.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.5"></leq><share href="#S3.SS4.SSS2.p1.1.m1.1.1.4.cmml" id="S3.SS4.SSS2.p1.1.m1.1.1d.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1"></share><apply id="S3.SS4.SSS2.p1.1.m1.1.1.6.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.6"><minus id="S3.SS4.SSS2.p1.1.m1.1.1.6.1.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.6.1"></minus><ci id="S3.SS4.SSS2.p1.1.m1.1.1.6.2.cmml" xref="S3.SS4.SSS2.p1.1.m1.1.1.6.2">𝑛</ci><cn id="S3.SS4.SSS2.p1.1.m1.1.1.6.3.cmml" type="integer" xref="S3.SS4.SSS2.p1.1.m1.1.1.6.3">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p1.1.m1.1c">2\leq k\leq n-4</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS2.p1.1.m1.1d">2 ≤ italic_k ≤ italic_n - 4</annotation></semantics></math>, specifies the maximum limit on relative locations. This allows the model to make reasonable predictions for sequence lengths that are not part of the training data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS3.5.1.1">III-D</span>3 </span><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS3.6.2">Rotary Position Embeddings</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">Rotary Positional Embedding (RoPE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib127" title="">127</a>]</cite> tackles problems with existing approaches.
Learned absolute positional encodings can lack generalizability and meaningfulness, particularly when sentences are short. Moreover, current methods like T5’s positional embedding face challenges with constructing a full attention matrix between positions. RoPE uses a rotation matrix to encode the absolute position of words and simultaneously includes explicit relative position details in self-attention. RoPE brings useful features like flexibility with sentence lengths, a decrease in word dependency as relative distances increase, and the ability to improve linear self-attention with relative position encoding. GPT-NeoX-20B, PaLM, CODEGEN, and LLaMA are among models that take advantage of RoPE in their architectures.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS4.SSS4.5.1.1">III-D</span>4 </span><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS4.6.2">Relative Positional Bias</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.SSS4.p1">
<p class="ltx_p" id="S3.SS4.SSS4.p1.1">The concept behind this type of positional embedding is to facilitate extrapolation during inference for sequences longer than those encountered in training.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib128" title="">128</a>]</cite> Press et al. proposed Attention with Linear Biases (ALiBi). Instead of simply adding positional embeddings to word embeddings, they introduced a bias to the attention scores of query-key pairs, imposing a penalty proportional to their distance. In the BLOOM model, ALiBi is leveraged.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F28">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F27.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="436" id="S3.F27.sf1.g1" src="extracted/5420339/img/APE.png" width="299">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F27.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F27.sf1.3.2" style="font-size:90%;">Absolute Positional Embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib129" title="">129</a>]</cite></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F27.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="382" id="S3.F27.sf2.g1" src="extracted/5420339/img/RPE.png" width="538">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F27.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F27.sf2.3.2" style="font-size:90%;">Relative Positional Embeddings</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F27.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="311" id="S3.F27.sf3.g1" src="extracted/5420339/img/RoPE.png" width="538">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F27.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F27.sf3.3.2" style="font-size:90%;">Rotary Positional Embedding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib127" title="">127</a>]</cite></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2 ltx_align_center" id="S3.F27.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="184" id="S3.F27.sf4.g1" src="extracted/5420339/img/alibi.png" width="419">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F27.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S3.F27.sf4.3.2" style="font-size:90%;">Relative Positional Bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib128" title="">128</a>]</cite></span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F28.2.1.1" style="font-size:90%;">Figure 28</span>: </span><span class="ltx_text" id="S3.F28.3.2" style="font-size:90%;">Various positional encodings are employed in LLMs.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.5.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.6.2">Model Pre-training</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Pre-training is the very first step in large language model training pipeline, and it helps LLMs to acquire fundamental language understanding capabilities, which can be useful in a wide range of language related tasks.
During pre-training, the LLM is trained on a massive amount of (usually) unlabeled texts, usually in a self-supervised manner.
There are different approaches used for pre-training like next sentence prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib24" title="">24</a>]</cite>, two most common ones include, next token prediction (autoregressive language modeling), and masked language modeling.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.4">In <span class="ltx_text ltx_font_bold" id="S3.SS5.p2.4.1">Autoregressive Language Modeling</span> framework, given a sequence of <math alttext="n" class="ltx_Math" display="inline" id="S3.SS5.p2.1.m1.1"><semantics id="S3.SS5.p2.1.m1.1a"><mi id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.1.m1.1d">italic_n</annotation></semantics></math> tokens <math alttext="x_{1}" class="ltx_Math" display="inline" id="S3.SS5.p2.2.m2.1"><semantics id="S3.SS5.p2.2.m2.1a"><msub id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mi id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">x</mi><mn id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2">𝑥</ci><cn id="S3.SS5.p2.2.m2.1.1.3.cmml" type="integer" xref="S3.SS5.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">x_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.2.m2.1d">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, …, <math alttext="x_{n}" class="ltx_Math" display="inline" id="S3.SS5.p2.3.m3.1"><semantics id="S3.SS5.p2.3.m3.1a"><msub id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml"><mi id="S3.SS5.p2.3.m3.1.1.2" xref="S3.SS5.p2.3.m3.1.1.2.cmml">x</mi><mi id="S3.SS5.p2.3.m3.1.1.3" xref="S3.SS5.p2.3.m3.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><apply id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.3.m3.1.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p2.3.m3.1.1.2.cmml" xref="S3.SS5.p2.3.m3.1.1.2">𝑥</ci><ci id="S3.SS5.p2.3.m3.1.1.3.cmml" xref="S3.SS5.p2.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">x_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, the model tries to predict next token <math alttext="x_{n+1}" class="ltx_Math" display="inline" id="S3.SS5.p2.4.m4.1"><semantics id="S3.SS5.p2.4.m4.1a"><msub id="S3.SS5.p2.4.m4.1.1" xref="S3.SS5.p2.4.m4.1.1.cmml"><mi id="S3.SS5.p2.4.m4.1.1.2" xref="S3.SS5.p2.4.m4.1.1.2.cmml">x</mi><mrow id="S3.SS5.p2.4.m4.1.1.3" xref="S3.SS5.p2.4.m4.1.1.3.cmml"><mi id="S3.SS5.p2.4.m4.1.1.3.2" xref="S3.SS5.p2.4.m4.1.1.3.2.cmml">n</mi><mo id="S3.SS5.p2.4.m4.1.1.3.1" xref="S3.SS5.p2.4.m4.1.1.3.1.cmml">+</mo><mn id="S3.SS5.p2.4.m4.1.1.3.3" xref="S3.SS5.p2.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.4.m4.1b"><apply id="S3.SS5.p2.4.m4.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.4.m4.1.1.1.cmml" xref="S3.SS5.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS5.p2.4.m4.1.1.2.cmml" xref="S3.SS5.p2.4.m4.1.1.2">𝑥</ci><apply id="S3.SS5.p2.4.m4.1.1.3.cmml" xref="S3.SS5.p2.4.m4.1.1.3"><plus id="S3.SS5.p2.4.m4.1.1.3.1.cmml" xref="S3.SS5.p2.4.m4.1.1.3.1"></plus><ci id="S3.SS5.p2.4.m4.1.1.3.2.cmml" xref="S3.SS5.p2.4.m4.1.1.3.2">𝑛</ci><cn id="S3.SS5.p2.4.m4.1.1.3.3.cmml" type="integer" xref="S3.SS5.p2.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.4.m4.1c">x_{n+1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT</annotation></semantics></math> (and sometimes next sequence of tokens) in an auto-regressive fashion.
One popular loss function in this case is the log-likelihood of predicted tokens as shown in Eq <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.E2" title="2 ‣ III-E Model Pre-training ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathscr{L}_{ALM}(x)=\sum_{i=1}^{N}p(x_{i+n}|x_{i},...,x_{i+n-1})" class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><msub id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.3.2.cmml"><mi class="ltx_font_mathscript" id="S3.E1.m1.3.3.3.2.2" xref="S3.E1.m1.3.3.3.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.3.3.3.2.3" xref="S3.E1.m1.3.3.3.2.3.cmml"><mi id="S3.E1.m1.3.3.3.2.3.2" xref="S3.E1.m1.3.3.3.2.3.2.cmml">A</mi><mo id="S3.E1.m1.3.3.3.2.3.1" xref="S3.E1.m1.3.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.3.3.3.2.3.3" xref="S3.E1.m1.3.3.3.2.3.3.cmml">L</mi><mo id="S3.E1.m1.3.3.3.2.3.1a" xref="S3.E1.m1.3.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.3.3.3.2.3.4" xref="S3.E1.m1.3.3.3.2.3.4.cmml">M</mi></mrow></msub><mo id="S3.E1.m1.3.3.3.1" xref="S3.E1.m1.3.3.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.3.3.2" xref="S3.E1.m1.3.3.3.cmml"><mo id="S3.E1.m1.3.3.3.3.2.1" stretchy="false" xref="S3.E1.m1.3.3.3.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">x</mi><mo id="S3.E1.m1.3.3.3.3.2.2" stretchy="false" xref="S3.E1.m1.3.3.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.2" rspace="0.111em" xref="S3.E1.m1.3.3.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><munderover id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml"><mo id="S3.E1.m1.3.3.1.2.2.2" movablelimits="false" xref="S3.E1.m1.3.3.1.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.3.3.1.2.2.3" xref="S3.E1.m1.3.3.1.2.2.3.cmml"><mi id="S3.E1.m1.3.3.1.2.2.3.2" xref="S3.E1.m1.3.3.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.2.2.3.1" xref="S3.E1.m1.3.3.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.3.3.1.2.2.3.3" xref="S3.E1.m1.3.3.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.3.3.1.2.3" xref="S3.E1.m1.3.3.1.2.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml">p</mi><mo id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.4" xref="S3.E1.m1.3.3.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.4.2" xref="S3.E1.m1.3.3.1.1.1.1.1.4.2.cmml">x</mi><mrow id="S3.E1.m1.3.3.1.1.1.1.1.4.3" xref="S3.E1.m1.3.3.1.1.1.1.1.4.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.4.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.4.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.4.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.4.3.1.cmml">+</mo><mi id="S3.E1.m1.3.3.1.1.1.1.1.4.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.4.3.3.cmml">n</mi></mrow></msub><mo fence="false" id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.3.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.1.1.1.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2" mathvariant="normal" xref="S3.E1.m1.2.2.cmml">…</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.2.2.4" xref="S3.E1.m1.3.3.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.2.cmml">x</mi><mrow id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.1" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.1.cmml">+</mo><mi id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.3" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.3.cmml">n</mi></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.1.cmml">−</mo><mn id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"></eq><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><times id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.1"></times><apply id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.2">subscript</csymbol><ci id="S3.E1.m1.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2">ℒ</ci><apply id="S3.E1.m1.3.3.3.2.3.cmml" xref="S3.E1.m1.3.3.3.2.3"><times id="S3.E1.m1.3.3.3.2.3.1.cmml" xref="S3.E1.m1.3.3.3.2.3.1"></times><ci id="S3.E1.m1.3.3.3.2.3.2.cmml" xref="S3.E1.m1.3.3.3.2.3.2">𝐴</ci><ci id="S3.E1.m1.3.3.3.2.3.3.cmml" xref="S3.E1.m1.3.3.3.2.3.3">𝐿</ci><ci id="S3.E1.m1.3.3.3.2.3.4.cmml" xref="S3.E1.m1.3.3.3.2.3.4">𝑀</ci></apply></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑥</ci></apply><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><apply id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.2.1.cmml" xref="S3.E1.m1.3.3.1.2">superscript</csymbol><apply id="S3.E1.m1.3.3.1.2.2.cmml" xref="S3.E1.m1.3.3.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.2">subscript</csymbol><sum id="S3.E1.m1.3.3.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.2.2.2"></sum><apply id="S3.E1.m1.3.3.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.2.2.3"><eq id="S3.E1.m1.3.3.1.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.2.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.2.2.3.2">𝑖</ci><cn id="S3.E1.m1.3.3.1.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.3.3.1.2.3.cmml" xref="S3.E1.m1.3.3.1.2.3">𝑁</ci></apply><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1"><times id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"></times><ci id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3">𝑝</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3">conditional</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.4.2">𝑥</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.4.3"><plus id="S3.E1.m1.3.3.1.1.1.1.1.4.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.4.3.1"></plus><ci id="S3.E1.m1.3.3.1.1.1.1.1.4.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.4.3.2">𝑖</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.4.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.4.3.3">𝑛</ci></apply></apply><list id="S3.E1.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2"><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">…</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.2">𝑥</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3"><minus id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.1"></minus><apply id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2"><plus id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.1"></plus><ci id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.2">𝑖</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.2.3">𝑛</ci></apply><cn id="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.1.1.1.2.2.2.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\mathscr{L}_{ALM}(x)=\sum_{i=1}^{N}p(x_{i+n}|x_{i},...,x_{i+n-1})</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">script_L start_POSTSUBSCRIPT italic_A italic_L italic_M end_POSTSUBSCRIPT ( italic_x ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p ( italic_x start_POSTSUBSCRIPT italic_i + italic_n end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i + italic_n - 1 end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS5.p2.5">Given the auto-regressive nature of this framework, the decoder-only models are naturally better suited to learn how to accomplish these task.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.2">In <span class="ltx_text ltx_font_bold" id="S3.SS5.p3.2.1">Masked Language Modeling</span>, some words are masked in a sequence and the model is trained to predict the masked words based on the surrounding context. Sometimes people refer to this approach as denoising autoencoding, too.
If we denote the masked/corrupted samples in the sequence <math alttext="x" class="ltx_Math" display="inline" id="S3.SS5.p3.1.m1.1"><semantics id="S3.SS5.p3.1.m1.1a"><mi id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><ci id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.1.m1.1d">italic_x</annotation></semantics></math>, as <math alttext="\tilde{x}" class="ltx_Math" display="inline" id="S3.SS5.p3.2.m2.1"><semantics id="S3.SS5.p3.2.m2.1a"><mover accent="true" id="S3.SS5.p3.2.m2.1.1" xref="S3.SS5.p3.2.m2.1.1.cmml"><mi id="S3.SS5.p3.2.m2.1.1.2" xref="S3.SS5.p3.2.m2.1.1.2.cmml">x</mi><mo id="S3.SS5.p3.2.m2.1.1.1" xref="S3.SS5.p3.2.m2.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><apply id="S3.SS5.p3.2.m2.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1"><ci id="S3.SS5.p3.2.m2.1.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1.1">~</ci><ci id="S3.SS5.p3.2.m2.1.1.2.cmml" xref="S3.SS5.p3.2.m2.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">\tilde{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.2.m2.1d">over~ start_ARG italic_x end_ARG</annotation></semantics></math>, then the training objective of this approach can be written as:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathscr{L}_{MLM}(x)=\sum_{i=1}^{N}p(\tilde{x}|x\backslash\tilde{x})" class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mrow id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3.cmml"><msub id="S3.E2.m1.2.2.3.2" xref="S3.E2.m1.2.2.3.2.cmml"><mi class="ltx_font_mathscript" id="S3.E2.m1.2.2.3.2.2" xref="S3.E2.m1.2.2.3.2.2.cmml">ℒ</mi><mrow id="S3.E2.m1.2.2.3.2.3" xref="S3.E2.m1.2.2.3.2.3.cmml"><mi id="S3.E2.m1.2.2.3.2.3.2" xref="S3.E2.m1.2.2.3.2.3.2.cmml">M</mi><mo id="S3.E2.m1.2.2.3.2.3.1" xref="S3.E2.m1.2.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E2.m1.2.2.3.2.3.3" xref="S3.E2.m1.2.2.3.2.3.3.cmml">L</mi><mo id="S3.E2.m1.2.2.3.2.3.1a" xref="S3.E2.m1.2.2.3.2.3.1.cmml">⁢</mo><mi id="S3.E2.m1.2.2.3.2.3.4" xref="S3.E2.m1.2.2.3.2.3.4.cmml">M</mi></mrow></msub><mo id="S3.E2.m1.2.2.3.1" xref="S3.E2.m1.2.2.3.1.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.3.3.2" xref="S3.E2.m1.2.2.3.cmml"><mo id="S3.E2.m1.2.2.3.3.2.1" stretchy="false" xref="S3.E2.m1.2.2.3.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">x</mi><mo id="S3.E2.m1.2.2.3.3.2.2" stretchy="false" xref="S3.E2.m1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.2" rspace="0.111em" xref="S3.E2.m1.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><munderover id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml"><mo id="S3.E2.m1.2.2.1.2.2.2" movablelimits="false" xref="S3.E2.m1.2.2.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.2.2.1.2.2.3" xref="S3.E2.m1.2.2.1.2.2.3.cmml"><mi id="S3.E2.m1.2.2.1.2.2.3.2" xref="S3.E2.m1.2.2.1.2.2.3.2.cmml">i</mi><mo id="S3.E2.m1.2.2.1.2.2.3.1" xref="S3.E2.m1.2.2.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.2.2.1.2.2.3.3" xref="S3.E2.m1.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.2.2.1.2.3" xref="S3.E2.m1.2.2.1.2.3.cmml">N</mi></munderover><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml">p</mi><mo id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.2.2.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.2.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2.cmml">x</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.2.1" xref="S3.E2.m1.2.2.1.1.1.1.1.2.1.cmml">~</mo></mover><mo fence="false" id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">|</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2.cmml">x</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E2.m1.2.2.1.1.1.1.1.3.1.cmml">\</mo><mover accent="true" id="S3.E2.m1.2.2.1.1.1.1.1.3.3" xref="S3.E2.m1.2.2.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.2.2.1.1.1.1.1.3.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.3.3.2.cmml">x</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.3.3.1" xref="S3.E2.m1.2.2.1.1.1.1.1.3.3.1.cmml">~</mo></mover></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"></eq><apply id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3"><times id="S3.E2.m1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.3.1"></times><apply id="S3.E2.m1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.3.2.1.cmml" xref="S3.E2.m1.2.2.3.2">subscript</csymbol><ci id="S3.E2.m1.2.2.3.2.2.cmml" xref="S3.E2.m1.2.2.3.2.2">ℒ</ci><apply id="S3.E2.m1.2.2.3.2.3.cmml" xref="S3.E2.m1.2.2.3.2.3"><times id="S3.E2.m1.2.2.3.2.3.1.cmml" xref="S3.E2.m1.2.2.3.2.3.1"></times><ci id="S3.E2.m1.2.2.3.2.3.2.cmml" xref="S3.E2.m1.2.2.3.2.3.2">𝑀</ci><ci id="S3.E2.m1.2.2.3.2.3.3.cmml" xref="S3.E2.m1.2.2.3.2.3.3">𝐿</ci><ci id="S3.E2.m1.2.2.3.2.3.4.cmml" xref="S3.E2.m1.2.2.3.2.3.4">𝑀</ci></apply></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑥</ci></apply><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><apply id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.2.1.cmml" xref="S3.E2.m1.2.2.1.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.2.2.cmml" xref="S3.E2.m1.2.2.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.2">subscript</csymbol><sum id="S3.E2.m1.2.2.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.2.2.2"></sum><apply id="S3.E2.m1.2.2.1.2.2.3.cmml" xref="S3.E2.m1.2.2.1.2.2.3"><eq id="S3.E2.m1.2.2.1.2.2.3.1.cmml" xref="S3.E2.m1.2.2.1.2.2.3.1"></eq><ci id="S3.E2.m1.2.2.1.2.2.3.2.cmml" xref="S3.E2.m1.2.2.1.2.2.3.2">𝑖</ci><cn id="S3.E2.m1.2.2.1.2.2.3.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.2.2.1.2.3.cmml" xref="S3.E2.m1.2.2.1.2.3">𝑁</ci></apply><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><times id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3">𝑝</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2"><ci id="S3.E2.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.1">~</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2.2">𝑥</ci></apply><apply id="S3.E2.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3"><ci id="S3.E2.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.1">\</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.2">𝑥</ci><apply id="S3.E2.m1.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.3"><ci id="S3.E2.m1.2.2.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.3.1">~</ci><ci id="S3.E2.m1.2.2.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.3.3.2">𝑥</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathscr{L}_{MLM}(x)=\sum_{i=1}^{N}p(\tilde{x}|x\backslash\tilde{x})</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">script_L start_POSTSUBSCRIPT italic_M italic_L italic_M end_POSTSUBSCRIPT ( italic_x ) = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p ( over~ start_ARG italic_x end_ARG | italic_x \ over~ start_ARG italic_x end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.1">And more recently, <span class="ltx_text ltx_font_bold" id="S3.SS5.p4.1.1">Mixture of Experts (MoE)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib130" title="">130</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib131" title="">131</a>]</cite> have become very popular in LLM space too. MoEs enable models to be pre-trained with much less compute, which means one can dramatically scale up the model or dataset size with the same compute budget as a dense model.
MoE consists of two main elements:
<span class="ltx_text ltx_font_bold" id="S3.SS5.p4.1.2">Sparse MoE layers</span>, which are used instead of dense feed-forward network (FFN) layers, and have a certain number of “experts” (e.g. 8), in which each expert is a neural network. In practice, the experts are FFNs, but they can also be more complex networks.
<span class="ltx_text ltx_font_bold" id="S3.SS5.p4.1.3">A gate network or router</span>, that determines which tokens are sent to which expert. It is worth noting that, one can send a token to more than one expert. How to route a token to an expert is one of the big decisions when working with MoEs - the router is composed of learned parameters and is pretrained at the same time as the rest of the network.
Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F29" title="Figure 29 ‣ III-E Model Pre-training ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">29</span></a> provides an illustration of a Switch Transformer encoder block, which are used in MoE.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F29"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="S3.F29.g1" src="extracted/5420339/img/moe.png" width="415">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F29.2.1.1" style="font-size:90%;">Figure 29</span>: </span><span class="ltx_text" id="S3.F29.3.2" style="font-size:90%;">: Illustration of a Switch Transformer encoder block. They replaced the dense feed forward network (FFN) layer present in the Transformer with a sparse Switch FFN layer (light blue). . Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib131" title="">131</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS6.5.1.1">III-F</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS6.6.2">Fine-tuning and Instruction Tuning</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">Early language models such as BERT trained using self-supervision as explained in section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS5" title="III-E Model Pre-training ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-E</span></span></a> were not able to perform specific tasks. In order for the foundation model to be useful it needed to be fine-tuned to a specific task with labeled data (so-called supervised fine-tuning or SFT for short). For example, in the original BERT paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib24" title="">24</a>]</cite>, the model was fine-tuned to 11 different tasks. While more recent LLMs no longer require fine-tuning to be used, they can still benefit from task or data-specific fine-tuning. For example, OpenAI reports that the much smaller GPT-3.5 Turbo model can outperform GPT-4 when fine-tuned with task specific data <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://platform.openai.com/docs/guides/fine-tuning</span></span></span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">Fine-tuning does not need to be performed to a single task though, and there are different approaches to multi-task fine-tuning (see e.g. Mahabi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib132" title="">132</a>]</cite>). Fine-tuning to one or more tasks is known to improve results and reduce the complexity of prompt engineering, and it can serve as an alternative to retrieval augmented generation. Furthermore, there are other reasons why it might be advisable to fine-tune. For example, one might want to fine-tune to expose the model to new or proprietary data that it has not been exposed to during pre-training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS6.p3">
<p class="ltx_p" id="S3.SS6.p3.1">An important reason to fine-tune LLMs is to align the responses to the expectations humans will have when providing instructions through prompts. This is the so-called <span class="ltx_text ltx_font_bold" id="S3.SS6.p3.1.1">instruction tuning</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib133" title="">133</a>]</cite>. We dive into the details of how to design and engineer prompts in section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2" title="IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>, but in the context of instruction tuning, it is important to understand that the instruction is a prompt that specifies the task that the LLM should accomplish. Instruction tuning datasets such as Natural Instructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib134" title="">134</a>]</cite> include not only the task definition but other components such as positive/negative examples or things to avoid.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS6.p4">
<p class="ltx_p" id="S3.SS6.p4.1">The specific approach and instruction datasets used to instruction-tune an LLM varies, but, generally speaking, instruction tuned models outperform their original foundation models they are based on. For example, InstructGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib59" title="">59</a>]</cite> outperforms GPT-3 on most benchmarks. The same is true for Alpaca <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib62" title="">62</a>]</cite> when compared to LLaMA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS6.p5">
<p class="ltx_p" id="S3.SS6.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS6.p5.1.1">Self-Instruct</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib135" title="">135</a>]</cite>, proposed by Wang et al. is also a popular approach along this line, in which they introduced a framework for improving the instruction-following capabilities of pre-trained language models by bootstrapping their own generations. Their pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to fine tune the original model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS7.5.1.1">III-G</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS7.6.2">Alignment</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">AI Alignment is the process of steering AI systems towards human goals, preferences, and principles. LLMs, pre-trained for word prediction, often exhibit unintended behaviors. For example, they might generate contents that are toxic, harmful, misleading and biased.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS7.p2">
<p class="ltx_p" id="S3.SS7.p2.1">Instruction tuning, discussed above, gets LLMs a step closer to being aligned. However, in many cases, it is important to include further steps to improve the alignment of the model and avoid unintended behaviors <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>According to very recent research by Ethayarajh et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib136" title="">136</a>]</cite>, further alignment besides SFT mainly improves models of at least 7B parameters. For smaller models, SFT is sufficient.</span></span></span>. We review the most popular approaches to alignment in this subsection.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS7.p3">
<p class="ltx_p" id="S3.SS7.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS7.p3.1.1">RLHF</span> (reinforcement learning from human feedback) and <span class="ltx_text ltx_font_bold" id="S3.SS7.p3.1.2">RLAIF</span> (reinforcement learning from AI feedback) are two popular approaches. RLHF uses a reward model to learn alignment from human feedback. This reward model, after being tuned, is able to rate different outputs and score them according to their alignment preferences given by humans. The reward model gives feedback to the original LLM and this feedback is used to tune the LLM further <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib137" title="">137</a>]</cite>. Reinforcement learning from AI feedback on the other hand, directly connects a pretrained and well-aligned model to the LLM and helps it to learn from larger and more aligned models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib138" title="">138</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS7.p4">
<p class="ltx_p" id="S3.SS7.p4.1">In another recent work (known as <span class="ltx_text ltx_font_bold" id="S3.SS7.p4.1.1">DPO</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib139" title="">139</a>]</cite>, Rafailov et al. discussed that RLHF is a complex and often unstable procedure, and tried to address this with a new approach. They leveraged a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which they called Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant
hyperparameter tuning.
They observed that fine-tuning with DPO exceeds RLHF’s ability to control sentiment of generations and improves response quality in summarization.
Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F30" title="Figure 30 ‣ III-G Alignment ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">30</span></a> shows the high-level comparison between DPO vs RLHF.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F30"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="86" id="S3.F30.g1" src="extracted/5420339/img/DPO.png" width="408">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F30.2.1.1" style="font-size:90%;">Figure 30</span>: </span><span class="ltx_text" id="S3.F30.3.2" style="font-size:90%;">DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification objective, without an explicit reward function or RL. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib139" title="">139</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS7.p5">
<p class="ltx_p" id="S3.SS7.p5.6">Even more recently Ethayarajh et al. proposed a new alignment approach called the Kahneman-Tversky Optimization (KTO) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib136" title="">136</a>]</cite>. Unlike existing state-of-the-art approaches, KTO does not require paired preference data (<math alttext="x" class="ltx_Math" display="inline" id="S3.SS7.p5.1.m1.1"><semantics id="S3.SS7.p5.1.m1.1a"><mi id="S3.SS7.p5.1.m1.1.1" xref="S3.SS7.p5.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p5.1.m1.1b"><ci id="S3.SS7.p5.1.m1.1.1.cmml" xref="S3.SS7.p5.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p5.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p5.1.m1.1d">italic_x</annotation></semantics></math>, <math alttext="y_{w}" class="ltx_Math" display="inline" id="S3.SS7.p5.2.m2.1"><semantics id="S3.SS7.p5.2.m2.1a"><msub id="S3.SS7.p5.2.m2.1.1" xref="S3.SS7.p5.2.m2.1.1.cmml"><mi id="S3.SS7.p5.2.m2.1.1.2" xref="S3.SS7.p5.2.m2.1.1.2.cmml">y</mi><mi id="S3.SS7.p5.2.m2.1.1.3" xref="S3.SS7.p5.2.m2.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS7.p5.2.m2.1b"><apply id="S3.SS7.p5.2.m2.1.1.cmml" xref="S3.SS7.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS7.p5.2.m2.1.1.1.cmml" xref="S3.SS7.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS7.p5.2.m2.1.1.2.cmml" xref="S3.SS7.p5.2.m2.1.1.2">𝑦</ci><ci id="S3.SS7.p5.2.m2.1.1.3.cmml" xref="S3.SS7.p5.2.m2.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p5.2.m2.1c">y_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p5.2.m2.1d">italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="y_{l}" class="ltx_Math" display="inline" id="S3.SS7.p5.3.m3.1"><semantics id="S3.SS7.p5.3.m3.1a"><msub id="S3.SS7.p5.3.m3.1.1" xref="S3.SS7.p5.3.m3.1.1.cmml"><mi id="S3.SS7.p5.3.m3.1.1.2" xref="S3.SS7.p5.3.m3.1.1.2.cmml">y</mi><mi id="S3.SS7.p5.3.m3.1.1.3" xref="S3.SS7.p5.3.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS7.p5.3.m3.1b"><apply id="S3.SS7.p5.3.m3.1.1.cmml" xref="S3.SS7.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS7.p5.3.m3.1.1.1.cmml" xref="S3.SS7.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS7.p5.3.m3.1.1.2.cmml" xref="S3.SS7.p5.3.m3.1.1.2">𝑦</ci><ci id="S3.SS7.p5.3.m3.1.1.3.cmml" xref="S3.SS7.p5.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p5.3.m3.1c">y_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p5.3.m3.1d">italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>), and it only needs (x,y) and knowledge of whether <math alttext="y" class="ltx_Math" display="inline" id="S3.SS7.p5.4.m4.1"><semantics id="S3.SS7.p5.4.m4.1a"><mi id="S3.SS7.p5.4.m4.1.1" xref="S3.SS7.p5.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS7.p5.4.m4.1b"><ci id="S3.SS7.p5.4.m4.1.1.cmml" xref="S3.SS7.p5.4.m4.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p5.4.m4.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p5.4.m4.1d">italic_y</annotation></semantics></math> is desirable or undesirable. KTO-aligned models are shown to be good or better than DPO-aligned models at scales from 1B to 30B, despite not using paired preferences.
KTO is also far easier to use in the real world than preference optimization methods, as the kind of data it needs is far more abundant. As an example, every retail company has a lot of customer interaction data and whether that interaction was successful (e.g., purchase made) or unsuccessful (e.g., no purchase made).
However, They have little to no counterfactual data (i.e., what would have made an unsuccessful customer interaction <math alttext="y_{l}" class="ltx_Math" display="inline" id="S3.SS7.p5.5.m5.1"><semantics id="S3.SS7.p5.5.m5.1a"><msub id="S3.SS7.p5.5.m5.1.1" xref="S3.SS7.p5.5.m5.1.1.cmml"><mi id="S3.SS7.p5.5.m5.1.1.2" xref="S3.SS7.p5.5.m5.1.1.2.cmml">y</mi><mi id="S3.SS7.p5.5.m5.1.1.3" xref="S3.SS7.p5.5.m5.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS7.p5.5.m5.1b"><apply id="S3.SS7.p5.5.m5.1.1.cmml" xref="S3.SS7.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS7.p5.5.m5.1.1.1.cmml" xref="S3.SS7.p5.5.m5.1.1">subscript</csymbol><ci id="S3.SS7.p5.5.m5.1.1.2.cmml" xref="S3.SS7.p5.5.m5.1.1.2">𝑦</ci><ci id="S3.SS7.p5.5.m5.1.1.3.cmml" xref="S3.SS7.p5.5.m5.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p5.5.m5.1c">y_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p5.5.m5.1d">italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math>
into a successful one <math alttext="y_{w}" class="ltx_Math" display="inline" id="S3.SS7.p5.6.m6.1"><semantics id="S3.SS7.p5.6.m6.1a"><msub id="S3.SS7.p5.6.m6.1.1" xref="S3.SS7.p5.6.m6.1.1.cmml"><mi id="S3.SS7.p5.6.m6.1.1.2" xref="S3.SS7.p5.6.m6.1.1.2.cmml">y</mi><mi id="S3.SS7.p5.6.m6.1.1.3" xref="S3.SS7.p5.6.m6.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS7.p5.6.m6.1b"><apply id="S3.SS7.p5.6.m6.1.1.cmml" xref="S3.SS7.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS7.p5.6.m6.1.1.1.cmml" xref="S3.SS7.p5.6.m6.1.1">subscript</csymbol><ci id="S3.SS7.p5.6.m6.1.1.2.cmml" xref="S3.SS7.p5.6.m6.1.1.2">𝑦</ci><ci id="S3.SS7.p5.6.m6.1.1.3.cmml" xref="S3.SS7.p5.6.m6.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS7.p5.6.m6.1c">y_{w}</annotation><annotation encoding="application/x-llamapun" id="S3.SS7.p5.6.m6.1d">italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT</annotation></semantics></math>).
Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F31" title="Figure 31 ‣ III-G Alignment ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">31</span></a> shows a high-level comparison between KTO and other alignment approaches discussed above.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F31"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="165" id="S3.F31.g1" src="extracted/5420339/img/KTO.png" width="406">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F31.2.1.1" style="font-size:90%;">Figure 31</span>: </span><span class="ltx_text" id="S3.F31.3.2" style="font-size:90%;">LLM alignment involves supervised finetuning followed by optimizing a human-centered loss (HALO). However, the paired preferences that existing approaches need are hard-to-obtain. In contrast, KTO uses a far more abundant kind of data, making it much easier to use in the real world. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib136" title="">136</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS8.5.1.1">III-H</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS8.6.2">Decoding Strategies</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS8.p1">
<p class="ltx_p" id="S3.SS8.p1.1">Decoding refers to the process of text generation using pre-trained LLMs.
Given an input prompt, the tokenizer translates each token in the input text into a corresponding token ID.
Then, the language model uses these token IDs as input and predicts the next most likely token (or a sequence of tokens).
Finally, the model generates logits, which are converted to probabilities using a softmax function.
Different decoding strategies have been proposed. Some of the most popular ones are greedy search, beam search, as well as different sample techniques such as top-K, top-P (Nucleus sampling).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S3.SS8.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS8.SSS1.5.1.1">III-H</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS8.SSS1.6.2">Greedy Search</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS8.SSS1.p1">
<p class="ltx_p" id="S3.SS8.SSS1.p1.1">Greedy search takes the most probable token at each step as the next token in the sequence, discarding all other potential options. As you can imagine, this is a simple approach and can loose a lot of temporal consistency and coherency.
It only considers the most probable token at each step, without considering the overall effect on the sequence. This property makes it fast, but it also means that it can miss out on better sequences that might have appeared with slightly less probable next tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS8.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS8.SSS2.5.1.1">III-H</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS8.SSS2.6.2">Beam Search</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS8.SSS2.p1">
<p class="ltx_p" id="S3.SS8.SSS2.p1.1">Unlike greedy search that only considers the next most probable token, beam search takes into account the <span class="ltx_text ltx_font_bold" id="S3.SS8.SSS2.p1.1.1">N</span> most likely tokens, where <span class="ltx_text ltx_font_bold" id="S3.SS8.SSS2.p1.1.2">N</span> denotes the number of beams. This procedure is repeated until a predefined maximum sequence length is reached or an end-of-sequence token appears. At this point, the sequence of tokens (AKA “beam”) with the highest overall score is chosen as the output.
For example for beam size of 2 and maximum length of 5, the beam search needs to keep track of <math alttext="2^{5}=32" class="ltx_Math" display="inline" id="S3.SS8.SSS2.p1.1.m1.1"><semantics id="S3.SS8.SSS2.p1.1.m1.1a"><mrow id="S3.SS8.SSS2.p1.1.m1.1.1" xref="S3.SS8.SSS2.p1.1.m1.1.1.cmml"><msup id="S3.SS8.SSS2.p1.1.m1.1.1.2" xref="S3.SS8.SSS2.p1.1.m1.1.1.2.cmml"><mn id="S3.SS8.SSS2.p1.1.m1.1.1.2.2" xref="S3.SS8.SSS2.p1.1.m1.1.1.2.2.cmml">2</mn><mn id="S3.SS8.SSS2.p1.1.m1.1.1.2.3" xref="S3.SS8.SSS2.p1.1.m1.1.1.2.3.cmml">5</mn></msup><mo id="S3.SS8.SSS2.p1.1.m1.1.1.1" xref="S3.SS8.SSS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS8.SSS2.p1.1.m1.1.1.3" xref="S3.SS8.SSS2.p1.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS8.SSS2.p1.1.m1.1b"><apply id="S3.SS8.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS8.SSS2.p1.1.m1.1.1"><eq id="S3.SS8.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS8.SSS2.p1.1.m1.1.1.1"></eq><apply id="S3.SS8.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS8.SSS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS8.SSS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS8.SSS2.p1.1.m1.1.1.2">superscript</csymbol><cn id="S3.SS8.SSS2.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S3.SS8.SSS2.p1.1.m1.1.1.2.2">2</cn><cn id="S3.SS8.SSS2.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS8.SSS2.p1.1.m1.1.1.2.3">5</cn></apply><cn id="S3.SS8.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS8.SSS2.p1.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS8.SSS2.p1.1.m1.1c">2^{5}=32</annotation><annotation encoding="application/x-llamapun" id="S3.SS8.SSS2.p1.1.m1.1d">2 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT = 32</annotation></semantics></math> possible sequences. So it is more computationally intensive than greedy search.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS8.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS8.SSS3.5.1.1">III-H</span>3 </span><span class="ltx_text ltx_font_bold" id="S3.SS8.SSS3.6.2">Top-k Sampling</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS8.SSS3.p1">
<p class="ltx_p" id="S3.SS8.SSS3.p1.1">Top-k sampling is a technique that uses the probability distribution generated by the language model to select a token randomly from the k most likely options.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS8.SSS3.p2">
<p class="ltx_p" id="S3.SS8.SSS3.p2.1">Suppose we have 6 tokens (A, B, C, D, E, F) and k=2, and P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)= 12.5%. In top-k sampling, tokens C, D, E, F are disregarded, and the model outputs A 60% of the time, and B, 40% of the time.
This approach ensures that we prioritize the most probable tokens while introducing an element of randomness in the selection process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS8.SSS3.p3">
<p class="ltx_p" id="S3.SS8.SSS3.p3.1">The randomness is usually introduced via the concept of temperature. The temperature T is a parameter that ranges from 0 to 1, which affects the probabilities generated by the softmax function, making the most likely tokens more influential. In practice, it simply consists of dividing the input logits by temperature value:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="softmax(x_{i})=\frac{e^{x_{i}/T}}{\sum_{j}e^{x_{j}/T}}" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml">s</mi><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.4" xref="S3.E3.m1.1.1.1.4.cmml">o</mi><mo id="S3.E3.m1.1.1.1.2a" xref="S3.E3.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.5" xref="S3.E3.m1.1.1.1.5.cmml">f</mi><mo id="S3.E3.m1.1.1.1.2b" xref="S3.E3.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.6" xref="S3.E3.m1.1.1.1.6.cmml">t</mi><mo id="S3.E3.m1.1.1.1.2c" xref="S3.E3.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.7" xref="S3.E3.m1.1.1.1.7.cmml">m</mi><mo id="S3.E3.m1.1.1.1.2d" xref="S3.E3.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.8" xref="S3.E3.m1.1.1.1.8.cmml">a</mi><mo id="S3.E3.m1.1.1.1.2e" xref="S3.E3.m1.1.1.1.2.cmml">⁢</mo><mi id="S3.E3.m1.1.1.1.9" xref="S3.E3.m1.1.1.1.9.cmml">x</mi><mo id="S3.E3.m1.1.1.1.2f" xref="S3.E3.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mfrac id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><msup id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.3.2.2" xref="S3.E3.m1.1.1.3.2.2.cmml">e</mi><mrow id="S3.E3.m1.1.1.3.2.3" xref="S3.E3.m1.1.1.3.2.3.cmml"><msub id="S3.E3.m1.1.1.3.2.3.2" xref="S3.E3.m1.1.1.3.2.3.2.cmml"><mi id="S3.E3.m1.1.1.3.2.3.2.2" xref="S3.E3.m1.1.1.3.2.3.2.2.cmml">x</mi><mi id="S3.E3.m1.1.1.3.2.3.2.3" xref="S3.E3.m1.1.1.3.2.3.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.1.1.3.2.3.1" xref="S3.E3.m1.1.1.3.2.3.1.cmml">/</mo><mi id="S3.E3.m1.1.1.3.2.3.3" xref="S3.E3.m1.1.1.3.2.3.3.cmml">T</mi></mrow></msup><mrow id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><msub id="S3.E3.m1.1.1.3.3.1" xref="S3.E3.m1.1.1.3.3.1.cmml"><mo id="S3.E3.m1.1.1.3.3.1.2" xref="S3.E3.m1.1.1.3.3.1.2.cmml">∑</mo><mi id="S3.E3.m1.1.1.3.3.1.3" xref="S3.E3.m1.1.1.3.3.1.3.cmml">j</mi></msub><msup id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml"><mi id="S3.E3.m1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.3.3.2.2.cmml">e</mi><mrow id="S3.E3.m1.1.1.3.3.2.3" xref="S3.E3.m1.1.1.3.3.2.3.cmml"><msub id="S3.E3.m1.1.1.3.3.2.3.2" xref="S3.E3.m1.1.1.3.3.2.3.2.cmml"><mi id="S3.E3.m1.1.1.3.3.2.3.2.2" xref="S3.E3.m1.1.1.3.3.2.3.2.2.cmml">x</mi><mi id="S3.E3.m1.1.1.3.3.2.3.2.3" xref="S3.E3.m1.1.1.3.3.2.3.2.3.cmml">j</mi></msub><mo id="S3.E3.m1.1.1.3.3.2.3.1" xref="S3.E3.m1.1.1.3.3.2.3.1.cmml">/</mo><mi id="S3.E3.m1.1.1.3.3.2.3.3" xref="S3.E3.m1.1.1.3.3.2.3.3.cmml">T</mi></mrow></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">𝑠</ci><ci id="S3.E3.m1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.4">𝑜</ci><ci id="S3.E3.m1.1.1.1.5.cmml" xref="S3.E3.m1.1.1.1.5">𝑓</ci><ci id="S3.E3.m1.1.1.1.6.cmml" xref="S3.E3.m1.1.1.1.6">𝑡</ci><ci id="S3.E3.m1.1.1.1.7.cmml" xref="S3.E3.m1.1.1.1.7">𝑚</ci><ci id="S3.E3.m1.1.1.1.8.cmml" xref="S3.E3.m1.1.1.1.8">𝑎</ci><ci id="S3.E3.m1.1.1.1.9.cmml" xref="S3.E3.m1.1.1.1.9">𝑥</ci><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><divide id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3"></divide><apply id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2">superscript</csymbol><ci id="S3.E3.m1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.2">𝑒</ci><apply id="S3.E3.m1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3"><divide id="S3.E3.m1.1.1.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.2.3.1"></divide><apply id="S3.E3.m1.1.1.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.2.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.2.3.2.1.cmml" xref="S3.E3.m1.1.1.3.2.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.3.2.2.cmml" xref="S3.E3.m1.1.1.3.2.3.2.2">𝑥</ci><ci id="S3.E3.m1.1.1.3.2.3.2.3.cmml" xref="S3.E3.m1.1.1.3.2.3.2.3">𝑖</ci></apply><ci id="S3.E3.m1.1.1.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.2.3.3">𝑇</ci></apply></apply><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><apply id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.1.cmml" xref="S3.E3.m1.1.1.3.3.1">subscript</csymbol><sum id="S3.E3.m1.1.1.3.3.1.2.cmml" xref="S3.E3.m1.1.1.3.3.1.2"></sum><ci id="S3.E3.m1.1.1.3.3.1.3.cmml" xref="S3.E3.m1.1.1.3.3.1.3">𝑗</ci></apply><apply id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.3.3.2">superscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.2">𝑒</ci><apply id="S3.E3.m1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3"><divide id="S3.E3.m1.1.1.3.3.2.3.1.cmml" xref="S3.E3.m1.1.1.3.3.2.3.1"></divide><apply id="S3.E3.m1.1.1.3.3.2.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.2.3.2.1.cmml" xref="S3.E3.m1.1.1.3.3.2.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.3.2.2.cmml" xref="S3.E3.m1.1.1.3.3.2.3.2.2">𝑥</ci><ci id="S3.E3.m1.1.1.3.3.2.3.2.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3.2.3">𝑗</ci></apply><ci id="S3.E3.m1.1.1.3.3.2.3.3.cmml" xref="S3.E3.m1.1.1.3.3.2.3.3">𝑇</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">softmax(x_{i})=\frac{e^{x_{i}/T}}{\sum_{j}e^{x_{j}/T}}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_s italic_o italic_f italic_t italic_m italic_a italic_x ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = divide start_ARG italic_e start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_T end_POSTSUPERSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT / italic_T end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS8.SSS3.p4">
<p class="ltx_p" id="S3.SS8.SSS3.p4.1">A low temperature setting significantly alters the probability distribution (and is commonly used in text generation to control the level of “creativity” in the generated output), while a large temperature prioritizes the tokens with higher probabilities.
Top-k is a creative way of sampling, and can be used along with beam search.
The sequence chosen by top-k sampling may not be the sequence with highest probability in beam search. But it’s important to remember that highest scores do not always lead to more realistic or meaningful sequences.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS8.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS8.SSS4.5.1.1">III-H</span>4 </span><span class="ltx_text ltx_font_bold" id="S3.SS8.SSS4.6.2">Top-p Sampling</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS8.SSS4.p1">
<p class="ltx_p" id="S3.SS8.SSS4.p1.1">Top-p sampling, also known as Nucleus sampling, takes a slightly different approach from top-k sampling. Instead of selecting the top k most probable tokens, nucleus sampling chooses a cutoff value p such that the sum of the probabilities of the selected tokens exceeds p. This forms a “nucleus” of tokens from which to randomly choose the next token. In other words, in top-p sampling the language model examines the most probable tokens in descending order and keeps adding them to the list until the sum of probabilities surpasses the threshold p.
As you can imagine, this could be better specially for scenarios in which top-k tokens do not have a large probability mass.
Unlike top-k sampling, the number of tokens included in the nucleus sampling is not fixed. This variability often results in a more diverse and creative output, making nucleus sampling popular for text generation related tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS9">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS9.5.1.1">III-I</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS9.6.2">Cost-Effective Training/Inference/Adaptation/Compression</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS9.p1">
<p class="ltx_p" id="S3.SS9.p1.1">In this part, we review some of the popular approaches used for more cost-friendly (and compute-friendly) training and usage of LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S3.SS9.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS9.SSS1.5.1.1">III-I</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS9.SSS1.6.2">Optimized Training</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS9.SSS1.p1">
<p class="ltx_p" id="S3.SS9.SSS1.p1.1">There are many frameworks developed for optimized training of LLMs, here we introduce some of the prominent ones.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS9.SSS1.p2">
<p class="ltx_p" id="S3.SS9.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS9.SSS1.p2.1.1">ZeRO: </span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib140" title="">140</a>]</cite>, Rajbhandari et al. developed a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed of LLMs while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing one to scale the model size proportional to the number of devices with sustained high efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS9.SSS1.p3">
<p class="ltx_p" id="S3.SS9.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS9.SSS1.p3.1.1">RWKV:</span> In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib141" title="">141</a>]</cite>, Peng et al. proposed a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Their approach leverages a linear attention mechanism and allows them to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters.
RWKV architecture is shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F32" title="Figure 32 ‣ III-I1 Optimized Training ‣ III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">32</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F32">
<br class="ltx_break"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="381" id="S3.F32.g1" src="extracted/5420339/img/rwkv.png" width="387">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F32.2.1.1" style="font-size:90%;">Figure 32</span>: </span><span class="ltx_text" id="S3.F32.3.2" style="font-size:90%;">RWKV architecture. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib141" title="">141</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS9.SSS1.p4">
<p class="ltx_p" id="S3.SS9.SSS1.p4.1">The Time Complexity comparison of RWKV with different Transformers are provided in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F33" title="Figure 33 ‣ III-I1 Optimized Training ‣ III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">33</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F33"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S3.F33.g1" src="extracted/5420339/img/rwkv_time.png" width="327">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F33.2.1.1" style="font-size:90%;">Figure 33</span>: </span><span class="ltx_text" id="S3.F33.3.2" style="font-size:90%;">Time Complexity comparison of RWKV with different Transformers. Here T denotes the sequence length, d the feature dimension, and c is MEGA’s chunk size of quadratic attention. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib141" title="">141</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S3.SS9.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS9.SSS2.5.1.1">III-I</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS9.SSS2.6.2">Low-Rank Adaption (LoRA)</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS9.SSS2.p1">
<p class="ltx_p" id="S3.SS9.SSS2.p1.1">Low-Rank Adaptation is a popular and lightweight training technique that significantly reduces the number of trainable parameters, and is based on a crucial insight that the difference between the fine-tuned weights for a specialized task and the initial pre-trained weights often exhibits “low intrinsic rank” - meaning that it can be approximated well by a low rank matrix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib142" title="">142</a>]</cite>.
Training with LoRA is much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), that are easier to store and share.
One property of low-rank matrices is that they can be represented as the product of two smaller matrices. This realization leads to the hypothesis that this delta between fine-tuned weights and initial pre-trained weights can be represented as the matrix product of two much smaller matrices. By focusing on updating these two smaller matrices rather than the entire original weight matrix, computational efficiency can be substantially improved.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS9.SSS2.p2">
<p class="ltx_p" id="S3.SS9.SSS2.p2.18">Specifically, for a pre-trained weight matrix <math alttext="W_{0}\in R^{d\times k}" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.1.m1.1"><semantics id="S3.SS9.SSS2.p2.1.m1.1a"><mrow id="S3.SS9.SSS2.p2.1.m1.1.1" xref="S3.SS9.SSS2.p2.1.m1.1.1.cmml"><msub id="S3.SS9.SSS2.p2.1.m1.1.1.2" xref="S3.SS9.SSS2.p2.1.m1.1.1.2.cmml"><mi id="S3.SS9.SSS2.p2.1.m1.1.1.2.2" xref="S3.SS9.SSS2.p2.1.m1.1.1.2.2.cmml">W</mi><mn id="S3.SS9.SSS2.p2.1.m1.1.1.2.3" xref="S3.SS9.SSS2.p2.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS9.SSS2.p2.1.m1.1.1.1" xref="S3.SS9.SSS2.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS9.SSS2.p2.1.m1.1.1.3" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS9.SSS2.p2.1.m1.1.1.3.2" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.2.cmml">R</mi><mrow id="S3.SS9.SSS2.p2.1.m1.1.1.3.3" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS9.SSS2.p2.1.m1.1.1.3.3.2" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.3.2.cmml">d</mi><mo id="S3.SS9.SSS2.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS9.SSS2.p2.1.m1.1.1.3.3.3" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.1.m1.1b"><apply id="S3.SS9.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1"><in id="S3.SS9.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.1"></in><apply id="S3.SS9.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS9.SSS2.p2.1.m1.1.1.2.2.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.2.2">𝑊</ci><cn id="S3.SS9.SSS2.p2.1.m1.1.1.2.3.cmml" type="integer" xref="S3.SS9.SSS2.p2.1.m1.1.1.2.3">0</cn></apply><apply id="S3.SS9.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS9.SSS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.2">𝑅</ci><apply id="S3.SS9.SSS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.3"><times id="S3.SS9.SSS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS9.SSS2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.3.2">𝑑</ci><ci id="S3.SS9.SSS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS9.SSS2.p2.1.m1.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.1.m1.1c">W_{0}\in R^{d\times k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.1.m1.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ italic_R start_POSTSUPERSCRIPT italic_d × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, LoRA constrains its update by representing the latter with a low-rank decomposition <math alttext="W_{0}+\Delta W=W_{0}+BA" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.2.m2.1"><semantics id="S3.SS9.SSS2.p2.2.m2.1a"><mrow id="S3.SS9.SSS2.p2.2.m2.1.1" xref="S3.SS9.SSS2.p2.2.m2.1.1.cmml"><mrow id="S3.SS9.SSS2.p2.2.m2.1.1.2" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.cmml"><msub id="S3.SS9.SSS2.p2.2.m2.1.1.2.2" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.2.cmml"><mi id="S3.SS9.SSS2.p2.2.m2.1.1.2.2.2" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.2.2.cmml">W</mi><mn id="S3.SS9.SSS2.p2.2.m2.1.1.2.2.3" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.2.3.cmml">0</mn></msub><mo id="S3.SS9.SSS2.p2.2.m2.1.1.2.1" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.1.cmml">+</mo><mrow id="S3.SS9.SSS2.p2.2.m2.1.1.2.3" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.3.cmml"><mi id="S3.SS9.SSS2.p2.2.m2.1.1.2.3.2" mathvariant="normal" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.3.2.cmml">Δ</mi><mo id="S3.SS9.SSS2.p2.2.m2.1.1.2.3.1" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.2.m2.1.1.2.3.3" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.3.3.cmml">W</mi></mrow></mrow><mo id="S3.SS9.SSS2.p2.2.m2.1.1.1" xref="S3.SS9.SSS2.p2.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS9.SSS2.p2.2.m2.1.1.3" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.cmml"><msub id="S3.SS9.SSS2.p2.2.m2.1.1.3.2" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.2.cmml"><mi id="S3.SS9.SSS2.p2.2.m2.1.1.3.2.2" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.2.2.cmml">W</mi><mn id="S3.SS9.SSS2.p2.2.m2.1.1.3.2.3" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.2.3.cmml">0</mn></msub><mo id="S3.SS9.SSS2.p2.2.m2.1.1.3.1" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.1.cmml">+</mo><mrow id="S3.SS9.SSS2.p2.2.m2.1.1.3.3" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS9.SSS2.p2.2.m2.1.1.3.3.2" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.3.2.cmml">B</mi><mo id="S3.SS9.SSS2.p2.2.m2.1.1.3.3.1" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.2.m2.1.1.3.3.3" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.3.3.cmml">A</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.2.m2.1b"><apply id="S3.SS9.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1"><eq id="S3.SS9.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.1"></eq><apply id="S3.SS9.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.2"><plus id="S3.SS9.SSS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.1"></plus><apply id="S3.SS9.SSS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.2.m2.1.1.2.2.1.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.2">subscript</csymbol><ci id="S3.SS9.SSS2.p2.2.m2.1.1.2.2.2.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.2.2">𝑊</ci><cn id="S3.SS9.SSS2.p2.2.m2.1.1.2.2.3.cmml" type="integer" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.2.3">0</cn></apply><apply id="S3.SS9.SSS2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.3"><times id="S3.SS9.SSS2.p2.2.m2.1.1.2.3.1.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.3.1"></times><ci id="S3.SS9.SSS2.p2.2.m2.1.1.2.3.2.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.3.2">Δ</ci><ci id="S3.SS9.SSS2.p2.2.m2.1.1.2.3.3.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.2.3.3">𝑊</ci></apply></apply><apply id="S3.SS9.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.3"><plus id="S3.SS9.SSS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.1"></plus><apply id="S3.SS9.SSS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.2.m2.1.1.3.2.1.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.2">subscript</csymbol><ci id="S3.SS9.SSS2.p2.2.m2.1.1.3.2.2.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.2.2">𝑊</ci><cn id="S3.SS9.SSS2.p2.2.m2.1.1.3.2.3.cmml" type="integer" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.2.3">0</cn></apply><apply id="S3.SS9.SSS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.3"><times id="S3.SS9.SSS2.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.3.1"></times><ci id="S3.SS9.SSS2.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.3.2">𝐵</ci><ci id="S3.SS9.SSS2.p2.2.m2.1.1.3.3.3.cmml" xref="S3.SS9.SSS2.p2.2.m2.1.1.3.3.3">𝐴</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.2.m2.1c">W_{0}+\Delta W=W_{0}+BA</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.2.m2.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + roman_Δ italic_W = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_B italic_A</annotation></semantics></math>, where <math alttext="B\in R^{d\times r}" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.3.m3.1"><semantics id="S3.SS9.SSS2.p2.3.m3.1a"><mrow id="S3.SS9.SSS2.p2.3.m3.1.1" xref="S3.SS9.SSS2.p2.3.m3.1.1.cmml"><mi id="S3.SS9.SSS2.p2.3.m3.1.1.2" xref="S3.SS9.SSS2.p2.3.m3.1.1.2.cmml">B</mi><mo id="S3.SS9.SSS2.p2.3.m3.1.1.1" xref="S3.SS9.SSS2.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS9.SSS2.p2.3.m3.1.1.3" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS9.SSS2.p2.3.m3.1.1.3.2" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.2.cmml">R</mi><mrow id="S3.SS9.SSS2.p2.3.m3.1.1.3.3" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS9.SSS2.p2.3.m3.1.1.3.3.2" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.3.2.cmml">d</mi><mo id="S3.SS9.SSS2.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS9.SSS2.p2.3.m3.1.1.3.3.3" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.3.m3.1b"><apply id="S3.SS9.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1"><in id="S3.SS9.SSS2.p2.3.m3.1.1.1.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1.1"></in><ci id="S3.SS9.SSS2.p2.3.m3.1.1.2.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1.2">𝐵</ci><apply id="S3.SS9.SSS2.p2.3.m3.1.1.3.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS9.SSS2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.2">𝑅</ci><apply id="S3.SS9.SSS2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.3"><times id="S3.SS9.SSS2.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.3.1"></times><ci id="S3.SS9.SSS2.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.3.2">𝑑</ci><ci id="S3.SS9.SSS2.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS9.SSS2.p2.3.m3.1.1.3.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.3.m3.1c">B\in R^{d\times r}</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.3.m3.1d">italic_B ∈ italic_R start_POSTSUPERSCRIPT italic_d × italic_r end_POSTSUPERSCRIPT</annotation></semantics></math> , <math alttext="A\in R^{r\times k}" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.4.m4.1"><semantics id="S3.SS9.SSS2.p2.4.m4.1a"><mrow id="S3.SS9.SSS2.p2.4.m4.1.1" xref="S3.SS9.SSS2.p2.4.m4.1.1.cmml"><mi id="S3.SS9.SSS2.p2.4.m4.1.1.2" xref="S3.SS9.SSS2.p2.4.m4.1.1.2.cmml">A</mi><mo id="S3.SS9.SSS2.p2.4.m4.1.1.1" xref="S3.SS9.SSS2.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS9.SSS2.p2.4.m4.1.1.3" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.cmml"><mi id="S3.SS9.SSS2.p2.4.m4.1.1.3.2" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.2.cmml">R</mi><mrow id="S3.SS9.SSS2.p2.4.m4.1.1.3.3" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.3.cmml"><mi id="S3.SS9.SSS2.p2.4.m4.1.1.3.3.2" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.3.2.cmml">r</mi><mo id="S3.SS9.SSS2.p2.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.SS9.SSS2.p2.4.m4.1.1.3.3.3" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.4.m4.1b"><apply id="S3.SS9.SSS2.p2.4.m4.1.1.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1"><in id="S3.SS9.SSS2.p2.4.m4.1.1.1.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1.1"></in><ci id="S3.SS9.SSS2.p2.4.m4.1.1.2.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1.2">𝐴</ci><apply id="S3.SS9.SSS2.p2.4.m4.1.1.3.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.4.m4.1.1.3.1.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS9.SSS2.p2.4.m4.1.1.3.2.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.2">𝑅</ci><apply id="S3.SS9.SSS2.p2.4.m4.1.1.3.3.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.3"><times id="S3.SS9.SSS2.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.3.1"></times><ci id="S3.SS9.SSS2.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.3.2">𝑟</ci><ci id="S3.SS9.SSS2.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS9.SSS2.p2.4.m4.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.4.m4.1c">A\in R^{r\times k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.4.m4.1d">italic_A ∈ italic_R start_POSTSUPERSCRIPT italic_r × italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, and the rank <math alttext="r\ll min(d,k)" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.5.m5.2"><semantics id="S3.SS9.SSS2.p2.5.m5.2a"><mrow id="S3.SS9.SSS2.p2.5.m5.2.3" xref="S3.SS9.SSS2.p2.5.m5.2.3.cmml"><mi id="S3.SS9.SSS2.p2.5.m5.2.3.2" xref="S3.SS9.SSS2.p2.5.m5.2.3.2.cmml">r</mi><mo id="S3.SS9.SSS2.p2.5.m5.2.3.1" xref="S3.SS9.SSS2.p2.5.m5.2.3.1.cmml">≪</mo><mrow id="S3.SS9.SSS2.p2.5.m5.2.3.3" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.cmml"><mi id="S3.SS9.SSS2.p2.5.m5.2.3.3.2" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.2.cmml">m</mi><mo id="S3.SS9.SSS2.p2.5.m5.2.3.3.1" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.5.m5.2.3.3.3" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.3.cmml">i</mi><mo id="S3.SS9.SSS2.p2.5.m5.2.3.3.1a" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.5.m5.2.3.3.4" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.4.cmml">n</mi><mo id="S3.SS9.SSS2.p2.5.m5.2.3.3.1b" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.1.cmml">⁢</mo><mrow id="S3.SS9.SSS2.p2.5.m5.2.3.3.5.2" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.5.1.cmml"><mo id="S3.SS9.SSS2.p2.5.m5.2.3.3.5.2.1" stretchy="false" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.5.1.cmml">(</mo><mi id="S3.SS9.SSS2.p2.5.m5.1.1" xref="S3.SS9.SSS2.p2.5.m5.1.1.cmml">d</mi><mo id="S3.SS9.SSS2.p2.5.m5.2.3.3.5.2.2" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.5.1.cmml">,</mo><mi id="S3.SS9.SSS2.p2.5.m5.2.2" xref="S3.SS9.SSS2.p2.5.m5.2.2.cmml">k</mi><mo id="S3.SS9.SSS2.p2.5.m5.2.3.3.5.2.3" stretchy="false" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.5.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.5.m5.2b"><apply id="S3.SS9.SSS2.p2.5.m5.2.3.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.3"><csymbol cd="latexml" id="S3.SS9.SSS2.p2.5.m5.2.3.1.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.3.1">much-less-than</csymbol><ci id="S3.SS9.SSS2.p2.5.m5.2.3.2.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.3.2">𝑟</ci><apply id="S3.SS9.SSS2.p2.5.m5.2.3.3.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.3.3"><times id="S3.SS9.SSS2.p2.5.m5.2.3.3.1.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.1"></times><ci id="S3.SS9.SSS2.p2.5.m5.2.3.3.2.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.2">𝑚</ci><ci id="S3.SS9.SSS2.p2.5.m5.2.3.3.3.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.3">𝑖</ci><ci id="S3.SS9.SSS2.p2.5.m5.2.3.3.4.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.4">𝑛</ci><interval closure="open" id="S3.SS9.SSS2.p2.5.m5.2.3.3.5.1.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.3.3.5.2"><ci id="S3.SS9.SSS2.p2.5.m5.1.1.cmml" xref="S3.SS9.SSS2.p2.5.m5.1.1">𝑑</ci><ci id="S3.SS9.SSS2.p2.5.m5.2.2.cmml" xref="S3.SS9.SSS2.p2.5.m5.2.2">𝑘</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.5.m5.2c">r\ll min(d,k)</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.5.m5.2d">italic_r ≪ italic_m italic_i italic_n ( italic_d , italic_k )</annotation></semantics></math>.
During training, <math alttext="W_{0}" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.6.m6.1"><semantics id="S3.SS9.SSS2.p2.6.m6.1a"><msub id="S3.SS9.SSS2.p2.6.m6.1.1" xref="S3.SS9.SSS2.p2.6.m6.1.1.cmml"><mi id="S3.SS9.SSS2.p2.6.m6.1.1.2" xref="S3.SS9.SSS2.p2.6.m6.1.1.2.cmml">W</mi><mn id="S3.SS9.SSS2.p2.6.m6.1.1.3" xref="S3.SS9.SSS2.p2.6.m6.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.6.m6.1b"><apply id="S3.SS9.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS9.SSS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.6.m6.1.1.1.cmml" xref="S3.SS9.SSS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS9.SSS2.p2.6.m6.1.1.2.cmml" xref="S3.SS9.SSS2.p2.6.m6.1.1.2">𝑊</ci><cn id="S3.SS9.SSS2.p2.6.m6.1.1.3.cmml" type="integer" xref="S3.SS9.SSS2.p2.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.6.m6.1c">W_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.6.m6.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> is frozen and does not receive gradient updates, while <math alttext="A" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.7.m7.1"><semantics id="S3.SS9.SSS2.p2.7.m7.1a"><mi id="S3.SS9.SSS2.p2.7.m7.1.1" xref="S3.SS9.SSS2.p2.7.m7.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.7.m7.1b"><ci id="S3.SS9.SSS2.p2.7.m7.1.1.cmml" xref="S3.SS9.SSS2.p2.7.m7.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.7.m7.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.7.m7.1d">italic_A</annotation></semantics></math> and <math alttext="B" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.8.m8.1"><semantics id="S3.SS9.SSS2.p2.8.m8.1a"><mi id="S3.SS9.SSS2.p2.8.m8.1.1" xref="S3.SS9.SSS2.p2.8.m8.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.8.m8.1b"><ci id="S3.SS9.SSS2.p2.8.m8.1.1.cmml" xref="S3.SS9.SSS2.p2.8.m8.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.8.m8.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.8.m8.1d">italic_B</annotation></semantics></math> contain trainable parameters.
It is worth mentioning that both <math alttext="W_{0}" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.9.m9.1"><semantics id="S3.SS9.SSS2.p2.9.m9.1a"><msub id="S3.SS9.SSS2.p2.9.m9.1.1" xref="S3.SS9.SSS2.p2.9.m9.1.1.cmml"><mi id="S3.SS9.SSS2.p2.9.m9.1.1.2" xref="S3.SS9.SSS2.p2.9.m9.1.1.2.cmml">W</mi><mn id="S3.SS9.SSS2.p2.9.m9.1.1.3" xref="S3.SS9.SSS2.p2.9.m9.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.9.m9.1b"><apply id="S3.SS9.SSS2.p2.9.m9.1.1.cmml" xref="S3.SS9.SSS2.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.9.m9.1.1.1.cmml" xref="S3.SS9.SSS2.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS9.SSS2.p2.9.m9.1.1.2.cmml" xref="S3.SS9.SSS2.p2.9.m9.1.1.2">𝑊</ci><cn id="S3.SS9.SSS2.p2.9.m9.1.1.3.cmml" type="integer" xref="S3.SS9.SSS2.p2.9.m9.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.9.m9.1c">W_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.9.m9.1d">italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\Delta W=BA" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.10.m10.1"><semantics id="S3.SS9.SSS2.p2.10.m10.1a"><mrow id="S3.SS9.SSS2.p2.10.m10.1.1" xref="S3.SS9.SSS2.p2.10.m10.1.1.cmml"><mrow id="S3.SS9.SSS2.p2.10.m10.1.1.2" xref="S3.SS9.SSS2.p2.10.m10.1.1.2.cmml"><mi id="S3.SS9.SSS2.p2.10.m10.1.1.2.2" mathvariant="normal" xref="S3.SS9.SSS2.p2.10.m10.1.1.2.2.cmml">Δ</mi><mo id="S3.SS9.SSS2.p2.10.m10.1.1.2.1" xref="S3.SS9.SSS2.p2.10.m10.1.1.2.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.10.m10.1.1.2.3" xref="S3.SS9.SSS2.p2.10.m10.1.1.2.3.cmml">W</mi></mrow><mo id="S3.SS9.SSS2.p2.10.m10.1.1.1" xref="S3.SS9.SSS2.p2.10.m10.1.1.1.cmml">=</mo><mrow id="S3.SS9.SSS2.p2.10.m10.1.1.3" xref="S3.SS9.SSS2.p2.10.m10.1.1.3.cmml"><mi id="S3.SS9.SSS2.p2.10.m10.1.1.3.2" xref="S3.SS9.SSS2.p2.10.m10.1.1.3.2.cmml">B</mi><mo id="S3.SS9.SSS2.p2.10.m10.1.1.3.1" xref="S3.SS9.SSS2.p2.10.m10.1.1.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.10.m10.1.1.3.3" xref="S3.SS9.SSS2.p2.10.m10.1.1.3.3.cmml">A</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.10.m10.1b"><apply id="S3.SS9.SSS2.p2.10.m10.1.1.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1"><eq id="S3.SS9.SSS2.p2.10.m10.1.1.1.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1.1"></eq><apply id="S3.SS9.SSS2.p2.10.m10.1.1.2.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1.2"><times id="S3.SS9.SSS2.p2.10.m10.1.1.2.1.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1.2.1"></times><ci id="S3.SS9.SSS2.p2.10.m10.1.1.2.2.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1.2.2">Δ</ci><ci id="S3.SS9.SSS2.p2.10.m10.1.1.2.3.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1.2.3">𝑊</ci></apply><apply id="S3.SS9.SSS2.p2.10.m10.1.1.3.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1.3"><times id="S3.SS9.SSS2.p2.10.m10.1.1.3.1.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1.3.1"></times><ci id="S3.SS9.SSS2.p2.10.m10.1.1.3.2.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1.3.2">𝐵</ci><ci id="S3.SS9.SSS2.p2.10.m10.1.1.3.3.cmml" xref="S3.SS9.SSS2.p2.10.m10.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.10.m10.1c">\Delta W=BA</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.10.m10.1d">roman_Δ italic_W = italic_B italic_A</annotation></semantics></math> are multiplied with the same input, and their respective
output vectors are summed coordinate-wise. For <math alttext="h=W_{0}x" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.11.m11.1"><semantics id="S3.SS9.SSS2.p2.11.m11.1a"><mrow id="S3.SS9.SSS2.p2.11.m11.1.1" xref="S3.SS9.SSS2.p2.11.m11.1.1.cmml"><mi id="S3.SS9.SSS2.p2.11.m11.1.1.2" xref="S3.SS9.SSS2.p2.11.m11.1.1.2.cmml">h</mi><mo id="S3.SS9.SSS2.p2.11.m11.1.1.1" xref="S3.SS9.SSS2.p2.11.m11.1.1.1.cmml">=</mo><mrow id="S3.SS9.SSS2.p2.11.m11.1.1.3" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.cmml"><msub id="S3.SS9.SSS2.p2.11.m11.1.1.3.2" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.2.cmml"><mi id="S3.SS9.SSS2.p2.11.m11.1.1.3.2.2" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.2.2.cmml">W</mi><mn id="S3.SS9.SSS2.p2.11.m11.1.1.3.2.3" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.2.3.cmml">0</mn></msub><mo id="S3.SS9.SSS2.p2.11.m11.1.1.3.1" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.11.m11.1.1.3.3" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.3.cmml">x</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.11.m11.1b"><apply id="S3.SS9.SSS2.p2.11.m11.1.1.cmml" xref="S3.SS9.SSS2.p2.11.m11.1.1"><eq id="S3.SS9.SSS2.p2.11.m11.1.1.1.cmml" xref="S3.SS9.SSS2.p2.11.m11.1.1.1"></eq><ci id="S3.SS9.SSS2.p2.11.m11.1.1.2.cmml" xref="S3.SS9.SSS2.p2.11.m11.1.1.2">ℎ</ci><apply id="S3.SS9.SSS2.p2.11.m11.1.1.3.cmml" xref="S3.SS9.SSS2.p2.11.m11.1.1.3"><times id="S3.SS9.SSS2.p2.11.m11.1.1.3.1.cmml" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.1"></times><apply id="S3.SS9.SSS2.p2.11.m11.1.1.3.2.cmml" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.11.m11.1.1.3.2.1.cmml" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.2">subscript</csymbol><ci id="S3.SS9.SSS2.p2.11.m11.1.1.3.2.2.cmml" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.2.2">𝑊</ci><cn id="S3.SS9.SSS2.p2.11.m11.1.1.3.2.3.cmml" type="integer" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.2.3">0</cn></apply><ci id="S3.SS9.SSS2.p2.11.m11.1.1.3.3.cmml" xref="S3.SS9.SSS2.p2.11.m11.1.1.3.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.11.m11.1c">h=W_{0}x</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.11.m11.1d">italic_h = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x</annotation></semantics></math>, their modified forward pass yields:
<math alttext="h=W_{0}x+\Delta Wx=W_{0}x+BAx" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.12.m12.1"><semantics id="S3.SS9.SSS2.p2.12.m12.1a"><mrow id="S3.SS9.SSS2.p2.12.m12.1.1" xref="S3.SS9.SSS2.p2.12.m12.1.1.cmml"><mi id="S3.SS9.SSS2.p2.12.m12.1.1.2" xref="S3.SS9.SSS2.p2.12.m12.1.1.2.cmml">h</mi><mo id="S3.SS9.SSS2.p2.12.m12.1.1.3" xref="S3.SS9.SSS2.p2.12.m12.1.1.3.cmml">=</mo><mrow id="S3.SS9.SSS2.p2.12.m12.1.1.4" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.cmml"><mrow id="S3.SS9.SSS2.p2.12.m12.1.1.4.2" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.cmml"><msub id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.cmml"><mi id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.2" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.2.cmml">W</mi><mn id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.3" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.3.cmml">0</mn></msub><mo id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.1" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.3" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.3.cmml">x</mi></mrow><mo id="S3.SS9.SSS2.p2.12.m12.1.1.4.1" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.1.cmml">+</mo><mrow id="S3.SS9.SSS2.p2.12.m12.1.1.4.3" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.cmml"><mi id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.2" mathvariant="normal" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.2.cmml">Δ</mi><mo id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.1" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.3" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.3.cmml">W</mi><mo id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.1a" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.4" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.4.cmml">x</mi></mrow></mrow><mo id="S3.SS9.SSS2.p2.12.m12.1.1.5" xref="S3.SS9.SSS2.p2.12.m12.1.1.5.cmml">=</mo><mrow id="S3.SS9.SSS2.p2.12.m12.1.1.6" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.cmml"><mrow id="S3.SS9.SSS2.p2.12.m12.1.1.6.2" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.cmml"><msub id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.cmml"><mi id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.2" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.2.cmml">W</mi><mn id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.3" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.3.cmml">0</mn></msub><mo id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.1" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.3" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.3.cmml">x</mi></mrow><mo id="S3.SS9.SSS2.p2.12.m12.1.1.6.1" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.1.cmml">+</mo><mrow id="S3.SS9.SSS2.p2.12.m12.1.1.6.3" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.cmml"><mi id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.2" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.2.cmml">B</mi><mo id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.1" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.3" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.3.cmml">A</mi><mo id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.1a" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.4" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.4.cmml">x</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.12.m12.1b"><apply id="S3.SS9.SSS2.p2.12.m12.1.1.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1"><and id="S3.SS9.SSS2.p2.12.m12.1.1a.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1"></and><apply id="S3.SS9.SSS2.p2.12.m12.1.1b.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1"><eq id="S3.SS9.SSS2.p2.12.m12.1.1.3.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.3"></eq><ci id="S3.SS9.SSS2.p2.12.m12.1.1.2.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.2">ℎ</ci><apply id="S3.SS9.SSS2.p2.12.m12.1.1.4.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4"><plus id="S3.SS9.SSS2.p2.12.m12.1.1.4.1.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.1"></plus><apply id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2"><times id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.1.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.1"></times><apply id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.1.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2">subscript</csymbol><ci id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.2.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.2">𝑊</ci><cn id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.3.cmml" type="integer" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.2.3">0</cn></apply><ci id="S3.SS9.SSS2.p2.12.m12.1.1.4.2.3.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.2.3">𝑥</ci></apply><apply id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3"><times id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.1.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.1"></times><ci id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.2.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.2">Δ</ci><ci id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.3.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.3">𝑊</ci><ci id="S3.SS9.SSS2.p2.12.m12.1.1.4.3.4.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.4.3.4">𝑥</ci></apply></apply></apply><apply id="S3.SS9.SSS2.p2.12.m12.1.1c.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1"><eq id="S3.SS9.SSS2.p2.12.m12.1.1.5.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.5"></eq><share href="#S3.SS9.SSS2.p2.12.m12.1.1.4.cmml" id="S3.SS9.SSS2.p2.12.m12.1.1d.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1"></share><apply id="S3.SS9.SSS2.p2.12.m12.1.1.6.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6"><plus id="S3.SS9.SSS2.p2.12.m12.1.1.6.1.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.1"></plus><apply id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2"><times id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.1.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.1"></times><apply id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.1.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2">subscript</csymbol><ci id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.2.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.2">𝑊</ci><cn id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.3.cmml" type="integer" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.2.3">0</cn></apply><ci id="S3.SS9.SSS2.p2.12.m12.1.1.6.2.3.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.2.3">𝑥</ci></apply><apply id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3"><times id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.1.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.1"></times><ci id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.2.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.2">𝐵</ci><ci id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.3.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.3">𝐴</ci><ci id="S3.SS9.SSS2.p2.12.m12.1.1.6.3.4.cmml" xref="S3.SS9.SSS2.p2.12.m12.1.1.6.3.4">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.12.m12.1c">h=W_{0}x+\Delta Wx=W_{0}x+BAx</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.12.m12.1d">italic_h = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + roman_Δ italic_W italic_x = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_x + italic_B italic_A italic_x</annotation></semantics></math>.
Usually a random Gaussian initialization is used for <math alttext="A" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.13.m13.1"><semantics id="S3.SS9.SSS2.p2.13.m13.1a"><mi id="S3.SS9.SSS2.p2.13.m13.1.1" xref="S3.SS9.SSS2.p2.13.m13.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.13.m13.1b"><ci id="S3.SS9.SSS2.p2.13.m13.1.1.cmml" xref="S3.SS9.SSS2.p2.13.m13.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.13.m13.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.13.m13.1d">italic_A</annotation></semantics></math>, and zero initialization for <math alttext="B" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.14.m14.1"><semantics id="S3.SS9.SSS2.p2.14.m14.1a"><mi id="S3.SS9.SSS2.p2.14.m14.1.1" xref="S3.SS9.SSS2.p2.14.m14.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.14.m14.1b"><ci id="S3.SS9.SSS2.p2.14.m14.1.1.cmml" xref="S3.SS9.SSS2.p2.14.m14.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.14.m14.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.14.m14.1d">italic_B</annotation></semantics></math>, so <math alttext="\Delta W=BA" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.15.m15.1"><semantics id="S3.SS9.SSS2.p2.15.m15.1a"><mrow id="S3.SS9.SSS2.p2.15.m15.1.1" xref="S3.SS9.SSS2.p2.15.m15.1.1.cmml"><mrow id="S3.SS9.SSS2.p2.15.m15.1.1.2" xref="S3.SS9.SSS2.p2.15.m15.1.1.2.cmml"><mi id="S3.SS9.SSS2.p2.15.m15.1.1.2.2" mathvariant="normal" xref="S3.SS9.SSS2.p2.15.m15.1.1.2.2.cmml">Δ</mi><mo id="S3.SS9.SSS2.p2.15.m15.1.1.2.1" xref="S3.SS9.SSS2.p2.15.m15.1.1.2.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.15.m15.1.1.2.3" xref="S3.SS9.SSS2.p2.15.m15.1.1.2.3.cmml">W</mi></mrow><mo id="S3.SS9.SSS2.p2.15.m15.1.1.1" xref="S3.SS9.SSS2.p2.15.m15.1.1.1.cmml">=</mo><mrow id="S3.SS9.SSS2.p2.15.m15.1.1.3" xref="S3.SS9.SSS2.p2.15.m15.1.1.3.cmml"><mi id="S3.SS9.SSS2.p2.15.m15.1.1.3.2" xref="S3.SS9.SSS2.p2.15.m15.1.1.3.2.cmml">B</mi><mo id="S3.SS9.SSS2.p2.15.m15.1.1.3.1" xref="S3.SS9.SSS2.p2.15.m15.1.1.3.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.15.m15.1.1.3.3" xref="S3.SS9.SSS2.p2.15.m15.1.1.3.3.cmml">A</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.15.m15.1b"><apply id="S3.SS9.SSS2.p2.15.m15.1.1.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1"><eq id="S3.SS9.SSS2.p2.15.m15.1.1.1.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1.1"></eq><apply id="S3.SS9.SSS2.p2.15.m15.1.1.2.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1.2"><times id="S3.SS9.SSS2.p2.15.m15.1.1.2.1.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1.2.1"></times><ci id="S3.SS9.SSS2.p2.15.m15.1.1.2.2.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1.2.2">Δ</ci><ci id="S3.SS9.SSS2.p2.15.m15.1.1.2.3.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1.2.3">𝑊</ci></apply><apply id="S3.SS9.SSS2.p2.15.m15.1.1.3.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1.3"><times id="S3.SS9.SSS2.p2.15.m15.1.1.3.1.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1.3.1"></times><ci id="S3.SS9.SSS2.p2.15.m15.1.1.3.2.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1.3.2">𝐵</ci><ci id="S3.SS9.SSS2.p2.15.m15.1.1.3.3.cmml" xref="S3.SS9.SSS2.p2.15.m15.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.15.m15.1c">\Delta W=BA</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.15.m15.1d">roman_Δ italic_W = italic_B italic_A</annotation></semantics></math> is zero at the beginning of training.
They then scale <math alttext="\Delta Wx" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.16.m16.1"><semantics id="S3.SS9.SSS2.p2.16.m16.1a"><mrow id="S3.SS9.SSS2.p2.16.m16.1.1" xref="S3.SS9.SSS2.p2.16.m16.1.1.cmml"><mi id="S3.SS9.SSS2.p2.16.m16.1.1.2" mathvariant="normal" xref="S3.SS9.SSS2.p2.16.m16.1.1.2.cmml">Δ</mi><mo id="S3.SS9.SSS2.p2.16.m16.1.1.1" xref="S3.SS9.SSS2.p2.16.m16.1.1.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.16.m16.1.1.3" xref="S3.SS9.SSS2.p2.16.m16.1.1.3.cmml">W</mi><mo id="S3.SS9.SSS2.p2.16.m16.1.1.1a" xref="S3.SS9.SSS2.p2.16.m16.1.1.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.16.m16.1.1.4" xref="S3.SS9.SSS2.p2.16.m16.1.1.4.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.16.m16.1b"><apply id="S3.SS9.SSS2.p2.16.m16.1.1.cmml" xref="S3.SS9.SSS2.p2.16.m16.1.1"><times id="S3.SS9.SSS2.p2.16.m16.1.1.1.cmml" xref="S3.SS9.SSS2.p2.16.m16.1.1.1"></times><ci id="S3.SS9.SSS2.p2.16.m16.1.1.2.cmml" xref="S3.SS9.SSS2.p2.16.m16.1.1.2">Δ</ci><ci id="S3.SS9.SSS2.p2.16.m16.1.1.3.cmml" xref="S3.SS9.SSS2.p2.16.m16.1.1.3">𝑊</ci><ci id="S3.SS9.SSS2.p2.16.m16.1.1.4.cmml" xref="S3.SS9.SSS2.p2.16.m16.1.1.4">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.16.m16.1c">\Delta Wx</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.16.m16.1d">roman_Δ italic_W italic_x</annotation></semantics></math> by <math alttext="\alpha r" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.17.m17.1"><semantics id="S3.SS9.SSS2.p2.17.m17.1a"><mrow id="S3.SS9.SSS2.p2.17.m17.1.1" xref="S3.SS9.SSS2.p2.17.m17.1.1.cmml"><mi id="S3.SS9.SSS2.p2.17.m17.1.1.2" xref="S3.SS9.SSS2.p2.17.m17.1.1.2.cmml">α</mi><mo id="S3.SS9.SSS2.p2.17.m17.1.1.1" xref="S3.SS9.SSS2.p2.17.m17.1.1.1.cmml">⁢</mo><mi id="S3.SS9.SSS2.p2.17.m17.1.1.3" xref="S3.SS9.SSS2.p2.17.m17.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.17.m17.1b"><apply id="S3.SS9.SSS2.p2.17.m17.1.1.cmml" xref="S3.SS9.SSS2.p2.17.m17.1.1"><times id="S3.SS9.SSS2.p2.17.m17.1.1.1.cmml" xref="S3.SS9.SSS2.p2.17.m17.1.1.1"></times><ci id="S3.SS9.SSS2.p2.17.m17.1.1.2.cmml" xref="S3.SS9.SSS2.p2.17.m17.1.1.2">𝛼</ci><ci id="S3.SS9.SSS2.p2.17.m17.1.1.3.cmml" xref="S3.SS9.SSS2.p2.17.m17.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.17.m17.1c">\alpha r</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.17.m17.1d">italic_α italic_r</annotation></semantics></math>, where <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p2.18.m18.1"><semantics id="S3.SS9.SSS2.p2.18.m18.1a"><mi id="S3.SS9.SSS2.p2.18.m18.1.1" xref="S3.SS9.SSS2.p2.18.m18.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p2.18.m18.1b"><ci id="S3.SS9.SSS2.p2.18.m18.1.1.cmml" xref="S3.SS9.SSS2.p2.18.m18.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p2.18.m18.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p2.18.m18.1d">italic_α</annotation></semantics></math> is a constant in r.
This reparametrization is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F34" title="Figure 34 ‣ III-I2 Low-Rank Adaption (LoRA) ‣ III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">34</span></a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F34"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="209" id="S3.F34.g1" src="extracted/5420339/img/lora.png" width="218">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F34.6.3.1" style="font-size:90%;">Figure 34</span>: </span><span class="ltx_text" id="S3.F34.4.2" style="font-size:90%;">An illustration of LoRA reparametrizan. Only <math alttext="A" class="ltx_Math" display="inline" id="S3.F34.3.1.m1.1"><semantics id="S3.F34.3.1.m1.1b"><mi id="S3.F34.3.1.m1.1.1" xref="S3.F34.3.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.F34.3.1.m1.1c"><ci id="S3.F34.3.1.m1.1.1.cmml" xref="S3.F34.3.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F34.3.1.m1.1d">A</annotation><annotation encoding="application/x-llamapun" id="S3.F34.3.1.m1.1e">italic_A</annotation></semantics></math> and <math alttext="B" class="ltx_Math" display="inline" id="S3.F34.4.2.m2.1"><semantics id="S3.F34.4.2.m2.1b"><mi id="S3.F34.4.2.m2.1.1" xref="S3.F34.4.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.F34.4.2.m2.1c"><ci id="S3.F34.4.2.m2.1.1.cmml" xref="S3.F34.4.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F34.4.2.m2.1d">B</annotation><annotation encoding="application/x-llamapun" id="S3.F34.4.2.m2.1e">italic_B</annotation></semantics></math> trained during this process. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib142" title="">142</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS9.SSS2.p3">
<p class="ltx_p" id="S3.SS9.SSS2.p3.4">It is worth mentioning that LoRA can be applied to any a subset of weight matrices in a neural network to reduce the number of trainable parameters.
In the Transformer architecture, there are four weight matrices in the self-attention module (<math alttext="W_{q}" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p3.1.m1.1"><semantics id="S3.SS9.SSS2.p3.1.m1.1a"><msub id="S3.SS9.SSS2.p3.1.m1.1.1" xref="S3.SS9.SSS2.p3.1.m1.1.1.cmml"><mi id="S3.SS9.SSS2.p3.1.m1.1.1.2" xref="S3.SS9.SSS2.p3.1.m1.1.1.2.cmml">W</mi><mi id="S3.SS9.SSS2.p3.1.m1.1.1.3" xref="S3.SS9.SSS2.p3.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p3.1.m1.1b"><apply id="S3.SS9.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS9.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS9.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS9.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS9.SSS2.p3.1.m1.1.1.2">𝑊</ci><ci id="S3.SS9.SSS2.p3.1.m1.1.1.3.cmml" xref="S3.SS9.SSS2.p3.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p3.1.m1.1c">W_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p3.1.m1.1d">italic_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> , <math alttext="W_{k}" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p3.2.m2.1"><semantics id="S3.SS9.SSS2.p3.2.m2.1a"><msub id="S3.SS9.SSS2.p3.2.m2.1.1" xref="S3.SS9.SSS2.p3.2.m2.1.1.cmml"><mi id="S3.SS9.SSS2.p3.2.m2.1.1.2" xref="S3.SS9.SSS2.p3.2.m2.1.1.2.cmml">W</mi><mi id="S3.SS9.SSS2.p3.2.m2.1.1.3" xref="S3.SS9.SSS2.p3.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p3.2.m2.1b"><apply id="S3.SS9.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS9.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p3.2.m2.1.1.1.cmml" xref="S3.SS9.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS9.SSS2.p3.2.m2.1.1.2.cmml" xref="S3.SS9.SSS2.p3.2.m2.1.1.2">𝑊</ci><ci id="S3.SS9.SSS2.p3.2.m2.1.1.3.cmml" xref="S3.SS9.SSS2.p3.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p3.2.m2.1c">W_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p3.2.m2.1d">italic_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="W_{v}" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p3.3.m3.1"><semantics id="S3.SS9.SSS2.p3.3.m3.1a"><msub id="S3.SS9.SSS2.p3.3.m3.1.1" xref="S3.SS9.SSS2.p3.3.m3.1.1.cmml"><mi id="S3.SS9.SSS2.p3.3.m3.1.1.2" xref="S3.SS9.SSS2.p3.3.m3.1.1.2.cmml">W</mi><mi id="S3.SS9.SSS2.p3.3.m3.1.1.3" xref="S3.SS9.SSS2.p3.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p3.3.m3.1b"><apply id="S3.SS9.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS9.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p3.3.m3.1.1.1.cmml" xref="S3.SS9.SSS2.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS9.SSS2.p3.3.m3.1.1.2.cmml" xref="S3.SS9.SSS2.p3.3.m3.1.1.2">𝑊</ci><ci id="S3.SS9.SSS2.p3.3.m3.1.1.3.cmml" xref="S3.SS9.SSS2.p3.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p3.3.m3.1c">W_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p3.3.m3.1d">italic_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> , <math alttext="W_{o}" class="ltx_Math" display="inline" id="S3.SS9.SSS2.p3.4.m4.1"><semantics id="S3.SS9.SSS2.p3.4.m4.1a"><msub id="S3.SS9.SSS2.p3.4.m4.1.1" xref="S3.SS9.SSS2.p3.4.m4.1.1.cmml"><mi id="S3.SS9.SSS2.p3.4.m4.1.1.2" xref="S3.SS9.SSS2.p3.4.m4.1.1.2.cmml">W</mi><mi id="S3.SS9.SSS2.p3.4.m4.1.1.3" xref="S3.SS9.SSS2.p3.4.m4.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS9.SSS2.p3.4.m4.1b"><apply id="S3.SS9.SSS2.p3.4.m4.1.1.cmml" xref="S3.SS9.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS9.SSS2.p3.4.m4.1.1.1.cmml" xref="S3.SS9.SSS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS9.SSS2.p3.4.m4.1.1.2.cmml" xref="S3.SS9.SSS2.p3.4.m4.1.1.2">𝑊</ci><ci id="S3.SS9.SSS2.p3.4.m4.1.1.3.cmml" xref="S3.SS9.SSS2.p3.4.m4.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS9.SSS2.p3.4.m4.1c">W_{o}</annotation><annotation encoding="application/x-llamapun" id="S3.SS9.SSS2.p3.4.m4.1d">italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>), and two in the MLP module. Most of the time, LoRA is focused on adapting the attention weights only for downstream tasks, and freezes the MLP modules, so they are not trained in downstream tasks both for simplicity and parameter-efficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS9.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS9.SSS3.5.1.1">III-I</span>3 </span><span class="ltx_text ltx_font_bold" id="S3.SS9.SSS3.6.2">Knowledge Distillation</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS9.SSS3.p1">
<p class="ltx_p" id="S3.SS9.SSS3.p1.1">Knowledge distillation is the process of learning from a larger model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib143" title="">143</a>]</cite>. Earlier days of best-performing models release have proven that this approach is very useful even if it is used in an API distillation approach. It is also referred to as an approach to distill the knowledge of not a single model but in fact multiple models into a smaller one. Creating smaller models by this approach yields smaller model sizes that can be used even on edge devices. Knowledge distillation as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.F35" title="Figure 35 ‣ III-I3 Knowledge Distillation ‣ III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">35</span></a>, illustrates a general setup of this training scheme.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F35"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="46" id="S3.F35.g1" src="x5.jpg" width="104">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F35.2.1.1" style="font-size:90%;">Figure 35</span>: </span><span class="ltx_text" id="S3.F35.3.2" style="font-size:90%;">A generic knowledge distillation framework with student and teacher (Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib144" title="">144</a>]</cite>).</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS9.SSS3.p2">
<p class="ltx_p" id="S3.SS9.SSS3.p2.1">Knowledge can be transferred by different forms of learning: response distillation, feature distillation, and API distillation. Response distillation is concerned only with the outputs of the teacher model and tries to teach the student model how to exactly or at least similarly perform (in the sense of prediction) as the teacher. Feature distillation not only uses the last layer but also intermediate layers as well to create a better inner representation for the student model. This helps the smaller model to have a similar representation as the teacher model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS9.SSS3.p3">
<p class="ltx_p" id="S3.SS9.SSS3.p3.1">API distillation is the process of using an API (typically from an LLM provider such as OpenAI) to train smaller models. In the case of LLMs, it is used to train the model from the direct output of the larger model which makes it very similar to response distillation. Many concerns are raised by this type of distillation because in cases where the model itself is not openly available, a (usually) paid API is exposed for end users. On the other hand, while users pay for each call, how to use the predictions is limited, for example, OpenAI prohibits usage of its API to create LLMs that later will be used to compete with it. The main value in such case is training data.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS9.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS9.SSS4.5.1.1">III-I</span>4 </span><span class="ltx_text ltx_font_bold" id="S3.SS9.SSS4.6.2">Quantization</span>
</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS9.SSS4.p1">
<p class="ltx_p" id="S3.SS9.SSS4.p1.1">deep learning in its core, is a set of mathematical functions applied to matrices, with a specific precision for model weights. Reducing the precision of the weights can be used to reduce the size of the model and also make it faster. As an example, Float-32 operations compared to Int-8 operations are slower. This process, which is called quantization, can be applied in different phases.
Main approaches for model quantization can be categorized as: post training quantization and quantization-aware training. Post-training quantization is concerned with quantized trained models in two well-known methods: dynamic and static. Dynamic post-training quantization computes the range of quantization on the runtime and is slower compared to static. Quantization-aware training adds quantization criteria into training, and a quantized model is trained and optimized during training process. This approach ensures that the end model will have good performance and also does not need to be quantized after training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">How LLMs Are Used and Augmented</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Once the LLMs are trained, we can use them to generate desired outputs for a variety of tasks.
LLMs can be used directly through basic prompting. However, in order to exploit their full potential or to address some of the shortcomings, we need to augment the models through some external means.
In this section we first provide a brief overview of the main shortcoming of LLMs, with a deeper look at the issue of hallucination. We then describe how prompting and some augmentation approaches can not only address those limitations but also be used to augment the capabilities of LLMs going as far as turning an LLM into a full-blown AI agent with the ability to interface with the external world.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F36"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="521" id="S4.F36.g1" src="x6.png" width="700">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F36.2.1.1" style="font-size:90%;">Figure 36</span>: </span><span class="ltx_text" id="S4.F36.3.2" style="font-size:90%;">How LLMs Are Used and Augmented.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">LLM limitations</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">It is important to remember that LLMs are trained to predict a token. While fine-tuning and alignment improves their performance and adds different dimensions to their abilities, there are still some important limitations that come up, particularly if they are used naively. Some of them include the following:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">They don’t have state/memory. LLMs on their own cannot remember even what was sent to them in the previous prompt. That is an important limitation for many of the uses cases that require some form of state.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">They are stochastic/probabilistic. If you send the same prompt to an LLM several times, you are likely to get different responses. While there are parameters, and in particular the temperature, to limit the variability in the response, this is an inherent property of their training that can create issues.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">They have stale information and, on their own, don’t have access to external data. An LLM on its own does not even know about the current time or day and does not have access to any information that was not present in its training set.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">They are generally very large. This means that many costly GPU machines are needed for training and serving. In some cases, largest models have poor SLAs, particularly in terms of latency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">They hallucinate. LLMs do not have a notion of ”truth” and they have usually been trained on a mix of good and bad content. They can produce very plausible but untruthful answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">While the previous limitations can all become important for some applications, it is worth for us to dive a bit into the last one, hallucinations, since it has gathered a lot of interest over the past few months and it has also sparked many of the prompt approaches and LLM augmentation methods we will later describe.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Hallucination:</span>
In the realm of Large Language Models (LLMs), the phenomenon of ”hallucinations” has garnered significant attention. Defined in the literature, notably in the ”Survey of Hallucination in Natural Language Generation” paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib145" title="">145</a>]</cite>, hallucination in an LLM is characterized as ”the generation of content that is nonsensical or unfaithful to the provided source.” This terminology, although rooted in psychological parlance, has been appropriated within the field of artificial intelligence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Hallucinations in LLMs can be broadly categorized into two types:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Intrinsic Hallucinations</span>: These directly conflict with the source material, introducing factual inaccuracies or logical inconsistencies.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Extrinsic Hallucinations</span>: These, while not contradicting, are unverifiable against the source, encompassing speculative or unconfirmable elements.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">The definition of ’source’ in LLM contexts varies with the task. In dialogue-based tasks, it refers to ’world knowledge’, whereas in text summarization, it pertains to the input text itself. This distinction plays a crucial role in evaluating and interpreting hallucinations. The impact of hallucinations is also highly context-dependent. For instance, in creative endeavors like poem writing, hallucinations might be deemed acceptable or even beneficial.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1">LLMs, trained on diverse datasets including the internet, books, and Wikipedia, generate text based on probabilistic models without an inherent understanding of truth or falsity. Recent advancements like instruct tuning and Reinforcement Learning from Human Feedback (RLHF) have attempted to steer LLMs towards more factual outputs, but the fundamental probabilistic nature and its inherent limitations remain. A recent study, “Sources of Hallucination by Large Language Models on Inference Tasks” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib146" title="">146</a>]</cite>, highlights two key aspects contributing to hallucinations in LLMs: the veracity prior and the relative frequency heuristic, underscoring the complexities inherent in LLM training and output generation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.1">Effective automated measurement of hallucinations in LLMs requires a combination of statistical and model-based metrics.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p10">
<p class="ltx_p" id="S4.SS1.p10.1"><em class="ltx_emph ltx_font_italic" id="S4.SS1.p10.1.1">Statistical Metrics</em>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">Metrics like ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib147" title="">147</a>]</cite> and BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib148" title="">148</a>]</cite> are common for assessing text similarity, focusing on intrinsic hallucinations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">Advanced metrics such as PARENT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib149" title="">149</a>]</cite>, PARENT-T <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib150" title="">150</a>]</cite>, and Knowledge F1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib151" title="">151</a>]</cite> are utilized when structured knowledge sources are available. These metrics, while effective, have limitations in capturing syntactic and semantic nuances.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p11">
<p class="ltx_p" id="S4.SS1.p11.1"><em class="ltx_emph ltx_font_italic" id="S4.SS1.p11.1.1">Model-Based Metrics</em>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I4.i1.p1.1.1">IE-Based Metrics</span>: Utilize Information Extraction models to simplify knowledge into relational tuples, then compare these with the source.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I4.i2.p1.1.1">QA-Based Metrics</span>: Assess the overlap between generated content and the source through a question-answering framework (see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib152" title="">152</a>]</cite>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i3.p1">
<p class="ltx_p" id="S4.I4.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I4.i3.p1.1.1">NLI-Based Metrics</span>: Use Natural Language Inference datasets to evaluate the truthfulness of a generated hypothesis based on a given premise (see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib153" title="">153</a>]</cite>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i4.p1">
<p class="ltx_p" id="S4.I4.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I4.i4.p1.1.1">Faithfulness Classification Metrics</span>: Offer a refined assessment by creating task-specific datasets for a nuanced evaluation (see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib154" title="">154</a>]</cite>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p12">
<p class="ltx_p" id="S4.SS1.p12.1">Despite advances in automated metrics, human judgment remains a vital piece. It typically involves two methodologies:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p13">
<ol class="ltx_enumerate" id="S4.I5">
<li class="ltx_item" id="S4.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I5.i1.p1">
<p class="ltx_p" id="S4.I5.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I5.i1.p1.1.1">Scoring</span>: Human evaluators rate the level of hallucination within a predefined scale.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I5.i2.p1">
<p class="ltx_p" id="S4.I5.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I5.i2.p1.1.1">Comparative Analysis</span>: Evaluators compare generated content against baseline or ground-truth references, adding an essential layer of subjective assessment.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS1.p14">
<p class="ltx_p" id="S4.SS1.p14.1">FactScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib155" title="">155</a>]</cite> is a recent example of a metric that can be used both for human and model-based evaluation. The metric breaks an LLM generation into “atomic facts”. The final score is computed as the sum of the accuracy of each atomic fact, giving each of them equal weight. Accuracy is a binary number that simply states whether the atomic fact is supported by the source. The authors implement different automation strategies that use LLMs to estimate this metric.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p15">
<p class="ltx_p" id="S4.SS1.p15.1">Finally, mitigating hallucinations in LLMs is a multifaceted challenge, requiring tailored strategies to suit various applications. Those include:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I6">
<li class="ltx_item" id="S4.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i1.p1">
<p class="ltx_p" id="S4.I6.i1.p1.1">Product Design and User Interaction Strategies such as use case design, structuring the input/output, or providing mechanisms for user feedback.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i2.p1">
<p class="ltx_p" id="S4.I6.i2.p1.1">Data Management and Continuous Improvement. Maintaining and analyzing a tracking set of hallucinations is essential for ongoing model improvement.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I6.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i3.p1">
<p class="ltx_p" id="S4.I6.i3.p1.1">Prompt Engineering and Metaprompt Design. Many of the advanced prompt techniques described in <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS2" title="IV-B Using LLMs: Prompt Design and Engineering ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a> such as Retrieval Augmented Generation directly address hallucination risks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I6.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I6.i4.p1">
<p class="ltx_p" id="S4.I6.i4.p1.1">Model Selection and Configuration for Hallucination Mitigation. For exemple, larger models with lower temperature settings usually perform better. Also, techniques such as RLHF or domain-sepcific fine-tuning can mitigate hallucination risks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Using LLMs: </span><span class="ltx_text ltx_font_bold" id="S4.SS2.7.3">Prompt Design and Engineering</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">A prompt in generative AI models is the textual input provided by users to guide the model’s output. This could range from simple questions to detailed descriptions or specific tasks. Prompts generally consist of instructions, questions, input data, and examples. In practice, to elicit a desired response from an AI model, a prompt must contain either instructions or questions, with other elements being optional. Advanced prompts involve more complex structures, such as ”chain of thought” prompting, where the model is guided to follow a logical reasoning process to arrive at an answer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Prompt engineering is a rapidly evolving discipline that shapes the interactions and outputs of LLMs and other generative AI models. The essence of prompt engineering lies in crafting the optimal prompt to achieve a specific goal with a generative model. This process is not only about instructing the model but also involves some understanding of the model’s capabilities and limitations, and the context within which it operates.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Prompt engineering transcends the mere construction of prompts; it requires a blend of domain knowledge, understanding of the AI model, and a methodical approach to tailor prompts for different contexts. This might involve creating templates that can be programmatically modified based on a given dataset or context. For example, generating personalized responses based on user data might use a template that is dynamically filled with relevant user information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Furthermore, prompt engineering is an iterative and exploratory process, akin to traditional machine learning practices such as model evaluation or hyperparameter tuning. The rapid growth of this field suggests its potential to revolutionize certain aspects of machine learning, moving beyond traditional methods like feature or architecture engineering. On the other hand, traditional engineering practices such as version control and regression testing need to be adapted to this new paradigm just like they were adapted to other machine learning approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib156" title="">156</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">In the following paragraphs we detail some of the most interesting and popular prompt engineering approaches.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS1.5.1.1">IV-B</span>1 </span>Chain of Thought (CoT)</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">The Chain of Thought (CoT) technique, initially described in the paper “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib34" title="">34</a>]</cite> by Google researchers, represents a pivotal advancement in prompt engineering for Large Language Models (LLMs). This approach hinges on the understanding that LLMs, while proficient in token prediction, are not inherently designed for explicit reasoning. CoT addresses this by guiding the model through essential reasoning steps.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">CoT is based on making the implicit reasoning process of LLMs explicit. By outlining the steps required for reasoning, the model is directed closer to a logical and reasoned output, especially in scenarios demanding more than simple information retrieval or pattern recognition.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">CoT prompting manifests in two primary forms:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S4.I7">
<li class="ltx_item" id="S4.I7.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I7.i1.p1">
<p class="ltx_p" id="S4.I7.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I7.i1.p1.1.1">Zero-Shot CoT:</span> This form involves instructing the LLM to “think step by step”, prompting it to deconstruct the problem and articulate each stage of reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I7.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I7.i2.p1">
<p class="ltx_p" id="S4.I7.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I7.i2.p1.1.1">Manual CoT:</span> A more complex variant, it requires providing step-by-step reasoning examples as templates for the model. While yielding more effective results, it poses challenges in scalability and maintenance.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">Manual CoT is more effective than zero-shot. However, the effectiveness of this example-based CoT depends on the choice of diverse examples, and constructing prompts with such examples of step by step reasoning by hand is hard and error prone. That is where automatic CoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib157" title="">157</a>]</cite> comes into play.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS2.5.1.1">IV-B</span>2 </span>Tree of Thought (ToT)</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">The Tree of Thought (ToT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib158" title="">158</a>]</cite> prompting technique is inspired by the concept of considering various alternative solutions or thought processes before converging on the most plausible one.
ToT is based on the idea of branching out into multiple ”thought trees” where each branch represents a different line of reasoning. This method allows the LLM to explore various possibilities and hypotheses, much like human cognitive processes where multiple scenarios are considered before determining the most likely one.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">A critical aspect of ToT is the evaluation of these reasoning paths. As the LLM generates different branches of thought, each is assessed for its validity and relevance to the query. This process involves real-time analysis and comparison of the branches, leading to a selection of the most coherent and logical outcome.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">ToT is particularly useful in complex problem-solving scenarios where a single line of reasoning might not suffice. It allows LLMs to mimic a more human-like problem-solving approach, considering a range of possibilities before arriving at a conclusion. This technique enhances the model’s ability to handle ambiguity, complexity, and nuanced tasks, making it a valuable tool in advanced AI applications.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS3.5.1.1">IV-B</span>3 </span>Self-Consistency</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Self-Consistency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib159" title="">159</a>]</cite> utilizes an ensemble-based method, where the LLM is prompted to generate multiple responses to the same query. The consistency among these responses serves as an indicator of their accuracy and reliability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">The Self-Consistency approach is grounded in the principle that if an LLM generates multiple, similar responses to the same prompt, it is more likely that the response is accurate. This method involves asking the LLM to tackle a query multiple times, each time analyzing the response for consistency. This technique is especially useful in scenarios where factual accuracy and precision are paramount.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">The consistency of responses can be measured using various methods. One common approach is to analyze the overlap in the content of the responses. Other methods may include comparing the semantic similarity of responses or employing more sophisticated techniques like BERT-scores or n-gram overlaps. These measures help in quantifying the level of agreement among the responses generated by the LLM.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.1">Self-Consistency has significant applications in fields where the veracity of information is critical. It is particularly relevant in scenarios like fact-checking, where ensuring the accuracy of information provided by AI models is essential. By employing this technique, prompt engineers can enhance the trustworthiness of LLMs, making them more reliable for tasks that require high levels of factual accuracy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS4.5.1.1">IV-B</span>4 </span>Reflection</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">Reflection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib160" title="">160</a>]</cite> involves prompting LLMs to assess and potentially revise their own outputs based on reasoning about the correctness and coherence of their responses. The concept of Reflection centers on the ability of LLMs to engage in a form of self-evaluation. After generating an initial response, the model is prompted to reflect on its own output, considering factors like factual accuracy, logical consistency, and relevance. This introspective process can lead to the generation of revised or improved responses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS4.p2">
<p class="ltx_p" id="S4.SS2.SSS4.p2.1">A key aspect of Reflection is the LLM’s capacity for self-editing. By evaluating its initial response, the model can identify potential errors or areas of improvement. This iterative process of generation, reflection, and revision enables the LLM to refine its output, enhancing the overall quality and reliability of its responses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS5.5.1.1">IV-B</span>5 </span>Expert Prompting</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS5.p1">
<p class="ltx_p" id="S4.SS2.SSS5.p1.1">Expert Prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib161" title="">161</a>]</cite> enhances the capabilities of Large Language Models (LLMs) by simulating the responses of experts in various fields. This method involves prompting the LLMs to assume the role of an expert and respond accordingly, providing high-quality, informed answers. A key strategy within Expert Prompting is the multi-expert approach. The LLM is prompted to consider responses from multiple expert perspectives, which are then synthesized to form a comprehensive and well-rounded answer. This technique not only enhances the depth of the response but also incorporates a range of viewpoints, reflecting a more holistic understanding of the subject matter.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS6.5.1.1">IV-B</span>6 </span>Chains</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS6.p1">
<p class="ltx_p" id="S4.SS2.SSS6.p1.1">Chains refer to the method of linking multiple components in a sequence to handle complex tasks with Large Language Models (LLMs). This approach involves creating a series of interconnected steps or processes, each contributing to the final outcome. The concept of Chains is based on the idea of constructing a workflow where different stages or components are sequentially arranged. Each component in a Chain performs a specific function, and the output of one serves as the input for the next. This end-to-end arrangement allows for more complex and nuanced processing, as each stage can be tailored to handle a specific aspect of the task. Chains can vary in complexity and structure, depending on the requirements. In “PromptChainer: Chaining Large Language Model Prompts through Visual Programming” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib162" title="">162</a>]</cite>, the authors not only describe the main challenges in designing chains, but also describe a visual tool to support those tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS7.5.1.1">IV-B</span>7 </span>Rails</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS7.p1">
<p class="ltx_p" id="S4.SS2.SSS7.p1.1">Rails in advanced prompt engineering refer to a method of guiding and controlling the output of Large Language Models (LLMs) through predefined rules or templates. This approach is designed to ensure that the model’s responses adhere to certain standards or criteria, enhancing the relevance, safety, and accuracy of the output. The concept of Rails involves setting up a framework or a set of guidelines that the LLM must follow while generating responses. These guidelines are typically defined using a modeling language or templates known as Canonical Forms, which standardize the way natural language sentences are structured and delivered.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS7.p2">
<p class="ltx_p" id="S4.SS2.SSS7.p2.1">Rails can be designed for various purposes, depending on the specific needs of the application:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I8">
<li class="ltx_item" id="S4.I8.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I8.i1.p1">
<p class="ltx_p" id="S4.I8.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I8.i1.p1.1.1">Topical Rails:</span> Ensure that the LLM sticks to a particular topic or domain.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I8.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I8.i2.p1">
<p class="ltx_p" id="S4.I8.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I8.i2.p1.1.1">Fact-Checking Rails:</span> Aimed at minimizing the generation of false or misleading information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I8.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I8.i3.p1">
<p class="ltx_p" id="S4.I8.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I8.i3.p1.1.1">Jailbreaking Rails:</span> Prevent the LLM from generating responses that attempt to bypass its own operational constraints or guidelines.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS8.5.1.1">IV-B</span>8 </span>Automatic Prompt Engineering (APE)</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS8.p1">
<p class="ltx_p" id="S4.SS2.SSS8.p1.1">Automatic Prompt Engineering (APE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib163" title="">163</a>]</cite> focuses on automating the process of prompt creation for Large Language Models (LLMs). APE seeks to streamline and optimize the prompt design process, leveraging the capabilities of LLMs themselves to generate and evaluate prompts. APE involves using LLMs in a self-referential manner where the model is employed to generate, score, and refine prompts. This recursive use of LLMs enables the creation of high-quality prompts that are more likely to elicit the desired response or outcome.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS2.SSS8.p2">
<p class="ltx_p" id="S4.SS2.SSS8.p2.1">The methodology of APE can be broken down into several key steps:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I9">
<li class="ltx_item" id="S4.I9.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I9.i1.p1">
<p class="ltx_p" id="S4.I9.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I9.i1.p1.1.1">Prompt Generation:</span> The LLM generates a range of potential prompts based on a given task or objective.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I9.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I9.i2.p1">
<p class="ltx_p" id="S4.I9.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I9.i2.p1.1.1">Prompt Scoring:</span> Each generated prompt is then evaluated for its effectiveness, often using criteria like clarity, specificity, and likelihood of eliciting the desired response.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I9.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I9.i3.p1">
<p class="ltx_p" id="S4.I9.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I9.i3.p1.1.1">Refinement and Iteration:</span> Based on these evaluations, prompts can be refined and iterated upon, further enhancing their quality and effectiveness.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS3.6.2">Augmenting LLMs through external knowledge - RAG</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">One of the main limitations of pre-trained LLMs is their lack of up-to-date knowledge or access to private or use-case-specific information. This is where retrieval augmented generation (RAG) comes into the picture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib164" title="">164</a>]</cite>. RAG, illustrated in figure <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.F37" title="Figure 37 ‣ IV-C Augmenting LLMs through external knowledge - RAG ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">37</span></a>, involves extracting a query from the input prompt and using that query to retrieve relevant information from an external knowledge source (e.g. a search engine or a knowledge graph, see figure <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.F38" title="Figure 38 ‣ IV-C Augmenting LLMs through external knowledge - RAG ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">38</span></a> ). The relevant information is then added to the original prompt and fed to the LLM in order for the model to generate the final response. A RAG system includes three important components: Retrieval, Generation, Augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib165" title="">165</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F37"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="S4.F37.g1" src="extracted/5420339/img/rag.png" width="449">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F37.2.1.1" style="font-size:90%;">Figure 37</span>: </span><span class="ltx_text" id="S4.F37.3.2" style="font-size:90%;">An example of synthesizing RAG with LLMs for question answering application <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib166" title="">166</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S4.F38"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S4.F38.g1" src="extracted/5420339/img/rag_kg.png" width="500">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F38.2.1.1" style="font-size:90%;">Figure 38</span>: </span><span class="ltx_text" id="S4.F38.3.2" style="font-size:90%;">This is one example of synthesizing the KG as a retriever with LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib167" title="">167</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">RAG-aware prompting techniques</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">Because of the importance of RAG to build advanced LLM systems, several RAG-aware prompting techniques have been developed recently. One such technique is Forward-looking Active Retrieval Augmented Generation (FLARE)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p2.1">Forward-looking Active Retrieval Augmented Generation (FLARE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib168" title="">168</a>]</cite> enhances the capabilities of Large Language Models (LLMs) by iteratively combining prediction and information retrieval. FLARE represents an evolution in the use of retrieval-augmented generation, aimed at improving the accuracy and relevance of LLM responses.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p3.1">FLARE involves an iterative process where the LLM actively predicts upcoming content and uses these predictions as queries to retrieve relevant information. This method contrasts with traditional retrieval-augmented models that typically retrieve information once and then proceed with generation. In FLARE, this process is dynamic and ongoing throughout the generation phase. In FLARE, each sentence or segment generated by the LLM is evaluated for confidence. If the confidence level is below a certain threshold, the model uses the generated content as a query to retrieve relevant information, which is then used to regenerate or refine the sentence. This iterative process ensures that each part of the response is informed by the most relevant and current information available.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p4">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p4.1">For more details on RAG framework and its relevant works, we refer the readers to this survey of retrieval augmented generations
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib165" title="">165</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS4.6.2">Using External Tools</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Retrieving information from an external knowledge source as described above is only one of the potential ways to augment an LLM. More generally, an LLM can access any number of external tools (e.g. an API to a service) to augment its functionality. In that regards, RAG can be seen as a specific instance of the broader category of the so called ”tools”.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Tools in this context are external functions or services that LLMs can utilize. These tools extend the range of tasks an LLM can perform, from basic information retrieval to complex interactions with external databases or APIs.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">In the paper ”Toolformer: Language Models Can Teach Themselves to Use Tools”
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib169" title="">169</a>]</cite>, the authors go beyond simple tool usage by training an LLM to decide what tool to use when, and even what parameters the API needs. Tools include two different search engines, or a calculator. In the following examples, the LLM decides to call an external Q&amp;A tool, a calculator, and a Wikipedia Search Engine More recently, researchers at Berkeley have trained a new LLM called Gorilla <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib67" title="">67</a>]</cite> that beats GPT-4 at the use of APIs, a specific but quite general tool.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Tool-aware prompting techniques</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p1.1">Similarly to what was described with RAG, several tool-aware prompting approaches have been developed to make usage of tools more scalable. A popular technique is the so called
Automatic Multi-step Reasoning and Tool-use (ART).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p2.1">Automatic Multi-step Reasoning and Tool-use (ART) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib170" title="">170</a>]</cite> is a prompt engineering technique that combines automated chain of thought prompting with the use of external tools. ART represents a convergence of multiple prompt engineering strategies, enhancing the ability of Large Language Models (LLMs) to handle complex tasks that require both reasoning and interaction with external data sources or tools.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p3.1">ART involves a systematic approach where, given a task and input, the system first identifies similar tasks from a task library. These tasks are then used as examples in the prompt, guiding the LLM on how to approach and execute the current task. This method is particularly effective when tasks require a combination of internal reasoning and external data processing or retrieval.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.5.1.1">IV-E</span> </span><span class="ltx_text ltx_font_bold" id="S4.SS5.6.2">LLM Agents</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">The idea of AI agents has been well-explored in the history of AI. An agent is typically an autonomous entity that can perceive the environment using its sensors, make a judgment based on the state it currently is, and accordingly act based on the actions that are available to it.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">In the context of LLMs, an agent refers to a system based on a specialized instantiation of an (augmented) LLM that is capable of performing specific tasks autonomously. These agents are designed to interact with users and environment to
make decisions based on the input and the intended goal of the interaction. Agents are based on LLMs equipped with the ability to access and use tools, and to make decisions based on the given input. They are designed to handle tasks that require a degree of autonomy and decision-making, typically beyond simple response generation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">The functionalities of a generic LLM-based agent include:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I10">
<li class="ltx_item" id="S4.I10.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I10.i1.p1">
<p class="ltx_p" id="S4.I10.i1.p1.1">Tool Access and Utilization: Agents have the capability to access external tools and services, and to utilize these resources effectively to accomplish tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I10.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I10.i2.p1">
<p class="ltx_p" id="S4.I10.i2.p1.1">Decision Making: They can make decisions based on the input, context, and the tools available to them, often employing complex reasoning processes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1">As an example, an LLM that has access to a function (or an API) such as weather API, can answer any question related to the weather of the specific place. In other words, it can use APIs to solve problems. Furthermore, if that LLM has access to an API that allows to make purchases, a purchasing agent can be built to not only have capabilities to read information from the external world, but also act on it <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib171" title="">171</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F39"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="197" id="S4.F39.g1" src="extracted/5420339/img/jarvis.png" width="445">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F39.2.1.1" style="font-size:90%;">Figure 39</span>: </span><span class="ltx_text" id="S4.F39.3.2" style="font-size:90%;">HuggingGPT: An agent-based approach to use tools and planning [image courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib171" title="">171</a>]</cite>]</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS5.p5">
<p class="ltx_p" id="S4.SS5.p5.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.F40" title="Figure 40 ‣ IV-E LLM Agents ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">40</span></a> shows another example of LLM-based agents for conversational information seeking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib36" title="">36</a>]</cite>, where an LLM is augmented with a set of plug-and-play modules, including a
<em class="ltx_emph ltx_font_italic" id="S4.SS5.p5.1.1">working memory</em> that tracks the dialog state, a
<em class="ltx_emph ltx_font_italic" id="S4.SS5.p5.1.2">policy</em> that makes an execution plan for the task and selects next system action, an
<em class="ltx_emph ltx_font_italic" id="S4.SS5.p5.1.3">action executor</em> that performs an action selected by the policy (consolidating evidence from external knowledge, or prompting the LLM to generate responses), and a
<em class="ltx_emph ltx_font_italic" id="S4.SS5.p5.1.4">utility</em> that accesses the alignment of the LLM’s responses with user expectations or specific business requirements, and generate feedback to improve agent performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F40"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="401" id="S4.F40.g1" src="extracted/5420339/img/llm-augmenter.png" width="565">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F40.2.1.1" style="font-size:90%;">Figure 40</span>: </span><span class="ltx_text" id="S4.F40.3.2" style="font-size:90%;">A LLM-based agent for conversational information seeking. Courtesy of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib36" title="">36</a>]</cite>.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS5.p6">
<p class="ltx_p" id="S4.SS5.p6.1">For more details on LLM-based AI agents see recent survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib173" title="">173</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib174" title="">174</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS5.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Prompt engineering techniques for agents</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS5.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS5.SSS0.Px1.p1.1">Like RAG and Tools, prompt engineering techniques that specifically address the needs of LLM-based agents have been developed. Three such examples are Reasoning without Observation (ReWOO), Reason and Act (ReAct), and Dialog-Enabled Resolving Agents (DERA).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS5.SSS0.Px1.p2.1">Reasoning without Observation (ReWOO) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib175" title="">175</a>]</cite> aims to decouple reasoning from direct observations. ReWOO operates by enabling LLMs to formulate comprehensive reasoning plans or meta-plans without immediate reliance on external data or tools. This approach allows the agent to create a structured framework for reasoning that can be executed once the necessary data or observations are available. In ReWOO, the LLM initially develops a plan (a series of steps) that outlines how to approach and solve a given problem. This meta-planning phase is crucial as it sets the stage for the agent to process information once it becomes available. The execution phase then involves integrating actual data or observations into the pre-specified plan, leading to coherent and contextually relevant responses. ReWOO offers significant advantages in terms of token efficiency and robustness to tool failure. It enables LLMs to handle tasks where immediate access to external data is not available, relying instead on a well-structured reasoning framework. This method is particularly advantageous in scenarios where data retrieval is costly, slow, or uncertain, allowing the LLM-based agent to maintain a high level of performance and reliability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS5.SSS0.Px1.p3.1">Reason and Act (ReAct)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib176" title="">176</a>]</cite> prompts LLMs to generate not only verbal reasoning but also actionable steps, thus enhancing the model’s dynamic problem-solving capabilities. ReAct is grounded in the principle of integrating reasoning with action. In this approach, the LLM is prompted to alternate between generating reasoning traces (explanations) and taking actions (steps or commands) in an interleaved manner. This approach allows the model to dynamically reason about a problem, and propose and take concrete actions simultaneously.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS5.SSS0.Px1.p4">
<p class="ltx_p" id="S4.SS5.SSS0.Px1.p4.1">Dialog-Enabled Resolving Agents (DERA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib177" title="">177</a>]</cite> are specialized AI agents that can engage in dialogue, resolve queries, and make decisions based on interactive exchanges. DERA is developed based on the idea of utilizing multiple agents within a dialog context, each with specific roles and functions. These agents can include Researchers, who gather and analyze information, and Deciders, who make final judgments based on the information provided. This division of roles allows for a well-organized and efficient approach to problem-solving and decision-making. DERA is particularly advantageous in scenarios requiring complex decision-making and problem-solving, such as those in medical diagnostics or customer service. The collaborative and interactive nature of DERA agents allows them to handle intricate queries with a level of depth and nuance that single-agent systems might struggle with. Moreover, this approach aligns well with human decision-making processes, making AI reasoning more relatable and trustworthy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Popular Datasets for LLMs</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Large language models exhibit promising accomplishments, but the main question that arises is how effectively they function and how their performance can be assessed in specific tasks or applications.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The evaluation of LLMs poses particular challenges due to the evolving landscape of their applications. The original intent behind developing LLMs was to boost the performance of NLP tasks such as translation, summarization, question-answering, and so on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib178" title="">178</a>]</cite>. However, it is evident today that these models are finding utility across diverse domains including code generation and finance. Moreover, the evaluation of LLMs encompasses several critical considerations such as fairness and bias, fact-checking, and reasoning. In this section, we outline the commonly used benchmarks for assessing LLMs. These benchmarks are categorized based on training or evaluating the LLM Capabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S5.F41"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="492" id="S5.F41.g1" src="extracted/5420339/img/db_visu.png" width="492">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F41.2.1.1" style="font-size:90%;">Figure 41</span>: </span><span class="ltx_text" id="S5.F41.3.2" style="font-size:90%;">Dataset applications.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S5.F42"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="409" id="S5.F42.g1" src="extracted/5420339/img/dataset_hist_ver.png" width="450">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F42.2.1.1" style="font-size:90%;">Figure 42</span>: </span><span class="ltx_text" id="S5.F42.3.2" style="font-size:90%;">Datasets licensed under different licenses.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Datasets for Basic Tasks: language modeling/understanding/generation</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">This section provides an overview of the benchmarks and datasets suited to evaluate the basic abilities of LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Natural Questions</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib179" title="">179</a>]</cite> is a QA dataset that consists of real anonymized, aggregated queries submitted to the Google search engine as questions. An annotator is presented with a question along with a Wikipedia page from the top <math alttext="5" class="ltx_Math" display="inline" id="S5.I1.i1.p1.1.m1.1"><semantics id="S5.I1.i1.p1.1.m1.1a"><mn id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.1b"><cn id="S5.I1.i1.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i1.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.1.m1.1d">5</annotation></semantics></math> search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">MMLU</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib180" title="">180</a>]</cite> is intended to evaluate the knowledge gained in zero-shot and few-shot scenarios. That means that MMLU assesses both the general knowledge and problem-solving ability of a model. It covers 57 subjects in STEM, humanities, social sciences, and other areas. The benchmark varies in complexity, ranging from elementary to advanced professional. It is worth mentioning that the main contribution of this dataset is for multi-task language understanding, question answering, and arithmetic reasoning.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">MBPP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib181" title="">181</a>]</cite> stands for “Mostly Basic Python Problems” and provides a benchmark for evaluating the performance of models designed for code generation. The benchmark encompasses <math alttext="974" class="ltx_Math" display="inline" id="S5.I1.i3.p1.1.m1.1"><semantics id="S5.I1.i3.p1.1.m1.1a"><mn id="S5.I1.i3.p1.1.m1.1.1" xref="S5.I1.i3.p1.1.m1.1.1.cmml">974</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.1.m1.1b"><cn id="S5.I1.i3.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i3.p1.1.m1.1.1">974</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.1.m1.1c">974</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i3.p1.1.m1.1d">974</annotation></semantics></math> short Python programs including a wide range of topics, including fundamental programming concepts and standard library usage, and more. Each challenge comprises a task description, a code solution, and three automated test cases.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">HumanEval</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib182" title="">182</a>]</cite> is a dataset for code generation task. This dataset consists of <math alttext="164" class="ltx_Math" display="inline" id="S5.I1.i4.p1.1.m1.1"><semantics id="S5.I1.i4.p1.1.m1.1a"><mn id="S5.I1.i4.p1.1.m1.1.1" xref="S5.I1.i4.p1.1.m1.1.1.cmml">164</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i4.p1.1.m1.1b"><cn id="S5.I1.i4.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i4.p1.1.m1.1.1">164</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i4.p1.1.m1.1c">164</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i4.p1.1.m1.1d">164</annotation></semantics></math> hand-crafted programming challenges. Each challenge is accompanied by a function signature, docstring, code body, and multiple unit tests. The main intuition behind developing this dataset is to guarantee the exclusion of its contents from training datasets for code generation models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.3"><span class="ltx_text ltx_font_bold" id="S5.I1.i5.p1.3.1">APPS</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib183" title="">183</a>]</cite> is designed for code generation task focusing on the Python programming language. The APPS dataset contains a collection of <math alttext="232,444" class="ltx_Math" display="inline" id="S5.I1.i5.p1.1.m1.2"><semantics id="S5.I1.i5.p1.1.m1.2a"><mrow id="S5.I1.i5.p1.1.m1.2.3.2" xref="S5.I1.i5.p1.1.m1.2.3.1.cmml"><mn id="S5.I1.i5.p1.1.m1.1.1" xref="S5.I1.i5.p1.1.m1.1.1.cmml">232</mn><mo id="S5.I1.i5.p1.1.m1.2.3.2.1" xref="S5.I1.i5.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I1.i5.p1.1.m1.2.2" xref="S5.I1.i5.p1.1.m1.2.2.cmml">444</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i5.p1.1.m1.2b"><list id="S5.I1.i5.p1.1.m1.2.3.1.cmml" xref="S5.I1.i5.p1.1.m1.2.3.2"><cn id="S5.I1.i5.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i5.p1.1.m1.1.1">232</cn><cn id="S5.I1.i5.p1.1.m1.2.2.cmml" type="integer" xref="S5.I1.i5.p1.1.m1.2.2">444</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i5.p1.1.m1.2c">232,444</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i5.p1.1.m1.2d">232 , 444</annotation></semantics></math> Python programs. Each program in the dataset has an average of <math alttext="18" class="ltx_Math" display="inline" id="S5.I1.i5.p1.2.m2.1"><semantics id="S5.I1.i5.p1.2.m2.1a"><mn id="S5.I1.i5.p1.2.m2.1.1" xref="S5.I1.i5.p1.2.m2.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i5.p1.2.m2.1b"><cn id="S5.I1.i5.p1.2.m2.1.1.cmml" type="integer" xref="S5.I1.i5.p1.2.m2.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i5.p1.2.m2.1c">18</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i5.p1.2.m2.1d">18</annotation></semantics></math> lines of Python code. Additionally, APPS offers access to a repository of <math alttext="10,000" class="ltx_Math" display="inline" id="S5.I1.i5.p1.3.m3.2"><semantics id="S5.I1.i5.p1.3.m3.2a"><mrow id="S5.I1.i5.p1.3.m3.2.3.2" xref="S5.I1.i5.p1.3.m3.2.3.1.cmml"><mn id="S5.I1.i5.p1.3.m3.1.1" xref="S5.I1.i5.p1.3.m3.1.1.cmml">10</mn><mo id="S5.I1.i5.p1.3.m3.2.3.2.1" xref="S5.I1.i5.p1.3.m3.2.3.1.cmml">,</mo><mn id="S5.I1.i5.p1.3.m3.2.2" xref="S5.I1.i5.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i5.p1.3.m3.2b"><list id="S5.I1.i5.p1.3.m3.2.3.1.cmml" xref="S5.I1.i5.p1.3.m3.2.3.2"><cn id="S5.I1.i5.p1.3.m3.1.1.cmml" type="integer" xref="S5.I1.i5.p1.3.m3.1.1">10</cn><cn id="S5.I1.i5.p1.3.m3.2.2.cmml" type="integer" xref="S5.I1.i5.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i5.p1.3.m3.2c">10,000</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i5.p1.3.m3.2d">10 , 000</annotation></semantics></math> unique programming exercises, each with text-based problem descriptions. The final aspect to highlight is that the it includes test cases.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.3"><span class="ltx_text ltx_font_bold" id="S5.I1.i6.p1.3.1">WikiSQL</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib184" title="">184</a>]</cite> is crafted for code generation task and it has 87,726 carefully labeled pairs of SQL queries and corresponding natural language questions from Wikipedia tables. The SQL queries comprise three subsets: test sets (<math alttext="17,284" class="ltx_Math" display="inline" id="S5.I1.i6.p1.1.m1.2"><semantics id="S5.I1.i6.p1.1.m1.2a"><mrow id="S5.I1.i6.p1.1.m1.2.3.2" xref="S5.I1.i6.p1.1.m1.2.3.1.cmml"><mn id="S5.I1.i6.p1.1.m1.1.1" xref="S5.I1.i6.p1.1.m1.1.1.cmml">17</mn><mo id="S5.I1.i6.p1.1.m1.2.3.2.1" xref="S5.I1.i6.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I1.i6.p1.1.m1.2.2" xref="S5.I1.i6.p1.1.m1.2.2.cmml">284</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i6.p1.1.m1.2b"><list id="S5.I1.i6.p1.1.m1.2.3.1.cmml" xref="S5.I1.i6.p1.1.m1.2.3.2"><cn id="S5.I1.i6.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i6.p1.1.m1.1.1">17</cn><cn id="S5.I1.i6.p1.1.m1.2.2.cmml" type="integer" xref="S5.I1.i6.p1.1.m1.2.2">284</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i6.p1.1.m1.2c">17,284</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i6.p1.1.m1.2d">17 , 284</annotation></semantics></math> examples), development (<math alttext="9,145" class="ltx_Math" display="inline" id="S5.I1.i6.p1.2.m2.2"><semantics id="S5.I1.i6.p1.2.m2.2a"><mrow id="S5.I1.i6.p1.2.m2.2.3.2" xref="S5.I1.i6.p1.2.m2.2.3.1.cmml"><mn id="S5.I1.i6.p1.2.m2.1.1" xref="S5.I1.i6.p1.2.m2.1.1.cmml">9</mn><mo id="S5.I1.i6.p1.2.m2.2.3.2.1" xref="S5.I1.i6.p1.2.m2.2.3.1.cmml">,</mo><mn id="S5.I1.i6.p1.2.m2.2.2" xref="S5.I1.i6.p1.2.m2.2.2.cmml">145</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i6.p1.2.m2.2b"><list id="S5.I1.i6.p1.2.m2.2.3.1.cmml" xref="S5.I1.i6.p1.2.m2.2.3.2"><cn id="S5.I1.i6.p1.2.m2.1.1.cmml" type="integer" xref="S5.I1.i6.p1.2.m2.1.1">9</cn><cn id="S5.I1.i6.p1.2.m2.2.2.cmml" type="integer" xref="S5.I1.i6.p1.2.m2.2.2">145</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i6.p1.2.m2.2c">9,145</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i6.p1.2.m2.2d">9 , 145</annotation></semantics></math> examples), and training (<math alttext="61,297" class="ltx_Math" display="inline" id="S5.I1.i6.p1.3.m3.2"><semantics id="S5.I1.i6.p1.3.m3.2a"><mrow id="S5.I1.i6.p1.3.m3.2.3.2" xref="S5.I1.i6.p1.3.m3.2.3.1.cmml"><mn id="S5.I1.i6.p1.3.m3.1.1" xref="S5.I1.i6.p1.3.m3.1.1.cmml">61</mn><mo id="S5.I1.i6.p1.3.m3.2.3.2.1" xref="S5.I1.i6.p1.3.m3.2.3.1.cmml">,</mo><mn id="S5.I1.i6.p1.3.m3.2.2" xref="S5.I1.i6.p1.3.m3.2.2.cmml">297</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i6.p1.3.m3.2b"><list id="S5.I1.i6.p1.3.m3.2.3.1.cmml" xref="S5.I1.i6.p1.3.m3.2.3.2"><cn id="S5.I1.i6.p1.3.m3.1.1.cmml" type="integer" xref="S5.I1.i6.p1.3.m3.1.1">61</cn><cn id="S5.I1.i6.p1.3.m3.2.2.cmml" type="integer" xref="S5.I1.i6.p1.3.m3.2.2">297</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i6.p1.3.m3.2c">61,297</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i6.p1.3.m3.2d">61 , 297</annotation></semantics></math> examples).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i7.p1">
<p class="ltx_p" id="S5.I1.i7.p1.2"><span class="ltx_text ltx_font_bold" id="S5.I1.i7.p1.2.1">TriviaQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib185" title="">185</a>]</cite> is designed for QA task. This dataset comprises more than <math alttext="650,000" class="ltx_Math" display="inline" id="S5.I1.i7.p1.1.m1.2"><semantics id="S5.I1.i7.p1.1.m1.2a"><mrow id="S5.I1.i7.p1.1.m1.2.3.2" xref="S5.I1.i7.p1.1.m1.2.3.1.cmml"><mn id="S5.I1.i7.p1.1.m1.1.1" xref="S5.I1.i7.p1.1.m1.1.1.cmml">650</mn><mo id="S5.I1.i7.p1.1.m1.2.3.2.1" xref="S5.I1.i7.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I1.i7.p1.1.m1.2.2" xref="S5.I1.i7.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i7.p1.1.m1.2b"><list id="S5.I1.i7.p1.1.m1.2.3.1.cmml" xref="S5.I1.i7.p1.1.m1.2.3.2"><cn id="S5.I1.i7.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i7.p1.1.m1.1.1">650</cn><cn id="S5.I1.i7.p1.1.m1.2.2.cmml" type="integer" xref="S5.I1.i7.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i7.p1.1.m1.2c">650,000</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i7.p1.1.m1.2d">650 , 000</annotation></semantics></math> question-answer-evidence triples. There are <math alttext="95,000" class="ltx_Math" display="inline" id="S5.I1.i7.p1.2.m2.2"><semantics id="S5.I1.i7.p1.2.m2.2a"><mrow id="S5.I1.i7.p1.2.m2.2.3.2" xref="S5.I1.i7.p1.2.m2.2.3.1.cmml"><mn id="S5.I1.i7.p1.2.m2.1.1" xref="S5.I1.i7.p1.2.m2.1.1.cmml">95</mn><mo id="S5.I1.i7.p1.2.m2.2.3.2.1" xref="S5.I1.i7.p1.2.m2.2.3.1.cmml">,</mo><mn id="S5.I1.i7.p1.2.m2.2.2" xref="S5.I1.i7.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i7.p1.2.m2.2b"><list id="S5.I1.i7.p1.2.m2.2.3.1.cmml" xref="S5.I1.i7.p1.2.m2.2.3.2"><cn id="S5.I1.i7.p1.2.m2.1.1.cmml" type="integer" xref="S5.I1.i7.p1.2.m2.1.1">95</cn><cn id="S5.I1.i7.p1.2.m2.2.2.cmml" type="integer" xref="S5.I1.i7.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i7.p1.2.m2.2c">95,000</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i7.p1.2.m2.2d">95 , 000</annotation></semantics></math> question-answer pairs in this dataset, each authored by trivia enthusiasts and supported by an average of six independently sourced evidence documents. These documents are automatically acquired from Wikipedia or broader web search results. The dataset is categorized into two segments, including those with authentic answers from Wikipedia and web domains, and verified sets embody the accurately answered questions along with their associated documents from both Wikipedia and online.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i8.p1">
<p class="ltx_p" id="S5.I1.i8.p1.4"><span class="ltx_text ltx_font_bold" id="S5.I1.i8.p1.4.1">RACE</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib186" title="">186</a>]</cite> suits for reading comprehension task. This dataset is based on English tests completed by Chinese students from middle school and high school, aged <math alttext="12" class="ltx_Math" display="inline" id="S5.I1.i8.p1.1.m1.1"><semantics id="S5.I1.i8.p1.1.m1.1a"><mn id="S5.I1.i8.p1.1.m1.1.1" xref="S5.I1.i8.p1.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i8.p1.1.m1.1b"><cn id="S5.I1.i8.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i8.p1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i8.p1.1.m1.1c">12</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i8.p1.1.m1.1d">12</annotation></semantics></math> to <math alttext="18" class="ltx_Math" display="inline" id="S5.I1.i8.p1.2.m2.1"><semantics id="S5.I1.i8.p1.2.m2.1a"><mn id="S5.I1.i8.p1.2.m2.1.1" xref="S5.I1.i8.p1.2.m2.1.1.cmml">18</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i8.p1.2.m2.1b"><cn id="S5.I1.i8.p1.2.m2.1.1.cmml" type="integer" xref="S5.I1.i8.p1.2.m2.1.1">18</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i8.p1.2.m2.1c">18</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i8.p1.2.m2.1d">18</annotation></semantics></math>, and it contains roughly <math alttext="28,000" class="ltx_Math" display="inline" id="S5.I1.i8.p1.3.m3.2"><semantics id="S5.I1.i8.p1.3.m3.2a"><mrow id="S5.I1.i8.p1.3.m3.2.3.2" xref="S5.I1.i8.p1.3.m3.2.3.1.cmml"><mn id="S5.I1.i8.p1.3.m3.1.1" xref="S5.I1.i8.p1.3.m3.1.1.cmml">28</mn><mo id="S5.I1.i8.p1.3.m3.2.3.2.1" xref="S5.I1.i8.p1.3.m3.2.3.1.cmml">,</mo><mn id="S5.I1.i8.p1.3.m3.2.2" xref="S5.I1.i8.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i8.p1.3.m3.2b"><list id="S5.I1.i8.p1.3.m3.2.3.1.cmml" xref="S5.I1.i8.p1.3.m3.2.3.2"><cn id="S5.I1.i8.p1.3.m3.1.1.cmml" type="integer" xref="S5.I1.i8.p1.3.m3.1.1">28</cn><cn id="S5.I1.i8.p1.3.m3.2.2.cmml" type="integer" xref="S5.I1.i8.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i8.p1.3.m3.2c">28,000</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i8.p1.3.m3.2d">28 , 000</annotation></semantics></math> texts and <math alttext="100,000" class="ltx_Math" display="inline" id="S5.I1.i8.p1.4.m4.2"><semantics id="S5.I1.i8.p1.4.m4.2a"><mrow id="S5.I1.i8.p1.4.m4.2.3.2" xref="S5.I1.i8.p1.4.m4.2.3.1.cmml"><mn id="S5.I1.i8.p1.4.m4.1.1" xref="S5.I1.i8.p1.4.m4.1.1.cmml">100</mn><mo id="S5.I1.i8.p1.4.m4.2.3.2.1" xref="S5.I1.i8.p1.4.m4.2.3.1.cmml">,</mo><mn id="S5.I1.i8.p1.4.m4.2.2" xref="S5.I1.i8.p1.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i8.p1.4.m4.2b"><list id="S5.I1.i8.p1.4.m4.2.3.1.cmml" xref="S5.I1.i8.p1.4.m4.2.3.2"><cn id="S5.I1.i8.p1.4.m4.1.1.cmml" type="integer" xref="S5.I1.i8.p1.4.m4.1.1">100</cn><cn id="S5.I1.i8.p1.4.m4.2.2.cmml" type="integer" xref="S5.I1.i8.p1.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i8.p1.4.m4.2c">100,000</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i8.p1.4.m4.2d">100 , 000</annotation></semantics></math> questions rigorously prepared by human specialists, primarily English instructors. This dataset contains a wide range of subjects that were purposefully chosen to assess students’ comprehension and reasoning abilities. This dataset is available in three subgroups: RACE-M, RACE-H, and RACE. RACE-M refers to the middle school examinations, whereas RACE-H denotes the high school tests. Finally, RACE is the synthesis of RACE-M and RACE-H.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i9.p1">
<p class="ltx_p" id="S5.I1.i9.p1.5"><span class="ltx_text ltx_font_bold" id="S5.I1.i9.p1.5.1">SQuAD</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib187" title="">187</a>]</cite> stands for “Stanford Question Answering Dataset” and is a crowdsourced reading comprehension dataset based on Wikipedia articles. It has approximately <math alttext="100,000" class="ltx_Math" display="inline" id="S5.I1.i9.p1.1.m1.2"><semantics id="S5.I1.i9.p1.1.m1.2a"><mrow id="S5.I1.i9.p1.1.m1.2.3.2" xref="S5.I1.i9.p1.1.m1.2.3.1.cmml"><mn id="S5.I1.i9.p1.1.m1.1.1" xref="S5.I1.i9.p1.1.m1.1.1.cmml">100</mn><mo id="S5.I1.i9.p1.1.m1.2.3.2.1" xref="S5.I1.i9.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I1.i9.p1.1.m1.2.2" xref="S5.I1.i9.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i9.p1.1.m1.2b"><list id="S5.I1.i9.p1.1.m1.2.3.1.cmml" xref="S5.I1.i9.p1.1.m1.2.3.2"><cn id="S5.I1.i9.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i9.p1.1.m1.1.1">100</cn><cn id="S5.I1.i9.p1.1.m1.2.2.cmml" type="integer" xref="S5.I1.i9.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i9.p1.1.m1.2c">100,000</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i9.p1.1.m1.2d">100 , 000</annotation></semantics></math> question-answer pairs connected to more than <math alttext="500" class="ltx_Math" display="inline" id="S5.I1.i9.p1.2.m2.1"><semantics id="S5.I1.i9.p1.2.m2.1a"><mn id="S5.I1.i9.p1.2.m2.1.1" xref="S5.I1.i9.p1.2.m2.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i9.p1.2.m2.1b"><cn id="S5.I1.i9.p1.2.m2.1.1.cmml" type="integer" xref="S5.I1.i9.p1.2.m2.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i9.p1.2.m2.1c">500</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i9.p1.2.m2.1d">500</annotation></semantics></math> articles. The answers to these questions are typically text fragments or spans taken from the corresponding reading passages. The questions may be unanswerable in some cases. The dataset is divided into three sets: an <math alttext="80\%" class="ltx_Math" display="inline" id="S5.I1.i9.p1.3.m3.1"><semantics id="S5.I1.i9.p1.3.m3.1a"><mrow id="S5.I1.i9.p1.3.m3.1.1" xref="S5.I1.i9.p1.3.m3.1.1.cmml"><mn id="S5.I1.i9.p1.3.m3.1.1.2" xref="S5.I1.i9.p1.3.m3.1.1.2.cmml">80</mn><mo id="S5.I1.i9.p1.3.m3.1.1.1" xref="S5.I1.i9.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i9.p1.3.m3.1b"><apply id="S5.I1.i9.p1.3.m3.1.1.cmml" xref="S5.I1.i9.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.I1.i9.p1.3.m3.1.1.1.cmml" xref="S5.I1.i9.p1.3.m3.1.1.1">percent</csymbol><cn id="S5.I1.i9.p1.3.m3.1.1.2.cmml" type="integer" xref="S5.I1.i9.p1.3.m3.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i9.p1.3.m3.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i9.p1.3.m3.1d">80 %</annotation></semantics></math> training set, a <math alttext="10\%" class="ltx_Math" display="inline" id="S5.I1.i9.p1.4.m4.1"><semantics id="S5.I1.i9.p1.4.m4.1a"><mrow id="S5.I1.i9.p1.4.m4.1.1" xref="S5.I1.i9.p1.4.m4.1.1.cmml"><mn id="S5.I1.i9.p1.4.m4.1.1.2" xref="S5.I1.i9.p1.4.m4.1.1.2.cmml">10</mn><mo id="S5.I1.i9.p1.4.m4.1.1.1" xref="S5.I1.i9.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i9.p1.4.m4.1b"><apply id="S5.I1.i9.p1.4.m4.1.1.cmml" xref="S5.I1.i9.p1.4.m4.1.1"><csymbol cd="latexml" id="S5.I1.i9.p1.4.m4.1.1.1.cmml" xref="S5.I1.i9.p1.4.m4.1.1.1">percent</csymbol><cn id="S5.I1.i9.p1.4.m4.1.1.2.cmml" type="integer" xref="S5.I1.i9.p1.4.m4.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i9.p1.4.m4.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i9.p1.4.m4.1d">10 %</annotation></semantics></math> development set, and a <math alttext="10\%" class="ltx_Math" display="inline" id="S5.I1.i9.p1.5.m5.1"><semantics id="S5.I1.i9.p1.5.m5.1a"><mrow id="S5.I1.i9.p1.5.m5.1.1" xref="S5.I1.i9.p1.5.m5.1.1.cmml"><mn id="S5.I1.i9.p1.5.m5.1.1.2" xref="S5.I1.i9.p1.5.m5.1.1.2.cmml">10</mn><mo id="S5.I1.i9.p1.5.m5.1.1.1" xref="S5.I1.i9.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i9.p1.5.m5.1b"><apply id="S5.I1.i9.p1.5.m5.1.1.cmml" xref="S5.I1.i9.p1.5.m5.1.1"><csymbol cd="latexml" id="S5.I1.i9.p1.5.m5.1.1.1.cmml" xref="S5.I1.i9.p1.5.m5.1.1.1">percent</csymbol><cn id="S5.I1.i9.p1.5.m5.1.1.2.cmml" type="integer" xref="S5.I1.i9.p1.5.m5.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i9.p1.5.m5.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i9.p1.5.m5.1d">10 %</annotation></semantics></math> hidden test set.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i10.p1">
<p class="ltx_p" id="S5.I1.i10.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i10.p1.1.1">BoolQ</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib188" title="">188</a>]</cite> is a yes/no question-answering dataset where the goal is reading comprehension task. BoolQ includes <math alttext="15,942" class="ltx_Math" display="inline" id="S5.I1.i10.p1.1.m1.2"><semantics id="S5.I1.i10.p1.1.m1.2a"><mrow id="S5.I1.i10.p1.1.m1.2.3.2" xref="S5.I1.i10.p1.1.m1.2.3.1.cmml"><mn id="S5.I1.i10.p1.1.m1.1.1" xref="S5.I1.i10.p1.1.m1.1.1.cmml">15</mn><mo id="S5.I1.i10.p1.1.m1.2.3.2.1" xref="S5.I1.i10.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I1.i10.p1.1.m1.2.2" xref="S5.I1.i10.p1.1.m1.2.2.cmml">942</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i10.p1.1.m1.2b"><list id="S5.I1.i10.p1.1.m1.2.3.1.cmml" xref="S5.I1.i10.p1.1.m1.2.3.2"><cn id="S5.I1.i10.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i10.p1.1.m1.1.1">15</cn><cn id="S5.I1.i10.p1.1.m1.2.2.cmml" type="integer" xref="S5.I1.i10.p1.1.m1.2.2">942</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i10.p1.1.m1.2c">15,942</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i10.p1.1.m1.2d">15 , 942</annotation></semantics></math> examples. Each example is a triplet that includes a question, a relevant paragraph, and the solution. Although the main intuition behind this dataset is for reading comprehension, it can be used for reasoning, natural language inference, and question-answering tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i11.p1">
<p class="ltx_p" id="S5.I1.i11.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i11.p1.1.1">MultiRC</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib189" title="">189</a>]</cite> is another dataset that fits reading comprehension task. MultiRC contains brief paragraphs as well as multi-sentence questions that can be answered using the information in the paragraph. The paragraphs in this dataset come from a variety of sources, including news, fiction, historical texts, Wikipedia articles, discussions on society and law, elementary school science textbooks, and 9/11 reports. Each question has many response choices, with one or more of them being correct. Answering the questions requires reasoning across several sentences. MultiRC dataset encompasses around <math alttext="6,000" class="ltx_Math" display="inline" id="S5.I1.i11.p1.1.m1.2"><semantics id="S5.I1.i11.p1.1.m1.2a"><mrow id="S5.I1.i11.p1.1.m1.2.3.2" xref="S5.I1.i11.p1.1.m1.2.3.1.cmml"><mn id="S5.I1.i11.p1.1.m1.1.1" xref="S5.I1.i11.p1.1.m1.1.1.cmml">6</mn><mo id="S5.I1.i11.p1.1.m1.2.3.2.1" xref="S5.I1.i11.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I1.i11.p1.1.m1.2.2" xref="S5.I1.i11.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i11.p1.1.m1.2b"><list id="S5.I1.i11.p1.1.m1.2.3.1.cmml" xref="S5.I1.i11.p1.1.m1.2.3.2"><cn id="S5.I1.i11.p1.1.m1.1.1.cmml" type="integer" xref="S5.I1.i11.p1.1.m1.1.1">6</cn><cn id="S5.I1.i11.p1.1.m1.2.2.cmml" type="integer" xref="S5.I1.i11.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i11.p1.1.m1.2c">6,000</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i11.p1.1.m1.2d">6 , 000</annotation></semantics></math> multi-sentence questions gathered from over 800 paragraphs. On average, each question offers about two valid answer alternatives out of a total of five.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Datasets for Emergent: ICL, reasoning (CoT), instruction following</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">This section centers on the benchmarks and datasets employed to evaluate the emergent abilities of LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.4"><span class="ltx_text ltx_font_bold" id="S5.I2.i1.p1.4.1">GSM8K</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib190" title="">190</a>]</cite> is designed to evaluate the model’s ability for multi-step mathematical reasoning. GSM8K includes 8.5K linguistically diverse grade school math word problems written by humans. The dataset is split into two sets: a training set with <math alttext="7.5K" class="ltx_Math" display="inline" id="S5.I2.i1.p1.1.m1.1"><semantics id="S5.I2.i1.p1.1.m1.1a"><mrow id="S5.I2.i1.p1.1.m1.1.1" xref="S5.I2.i1.p1.1.m1.1.1.cmml"><mn id="S5.I2.i1.p1.1.m1.1.1.2" xref="S5.I2.i1.p1.1.m1.1.1.2.cmml">7.5</mn><mo id="S5.I2.i1.p1.1.m1.1.1.1" xref="S5.I2.i1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.I2.i1.p1.1.m1.1.1.3" xref="S5.I2.i1.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i1.p1.1.m1.1b"><apply id="S5.I2.i1.p1.1.m1.1.1.cmml" xref="S5.I2.i1.p1.1.m1.1.1"><times id="S5.I2.i1.p1.1.m1.1.1.1.cmml" xref="S5.I2.i1.p1.1.m1.1.1.1"></times><cn id="S5.I2.i1.p1.1.m1.1.1.2.cmml" type="float" xref="S5.I2.i1.p1.1.m1.1.1.2">7.5</cn><ci id="S5.I2.i1.p1.1.m1.1.1.3.cmml" xref="S5.I2.i1.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i1.p1.1.m1.1c">7.5K</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i1.p1.1.m1.1d">7.5 italic_K</annotation></semantics></math> problems, and a test set with <math alttext="1" class="ltx_Math" display="inline" id="S5.I2.i1.p1.2.m2.1"><semantics id="S5.I2.i1.p1.2.m2.1a"><mn id="S5.I2.i1.p1.2.m2.1.1" xref="S5.I2.i1.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i1.p1.2.m2.1b"><cn id="S5.I2.i1.p1.2.m2.1.1.cmml" type="integer" xref="S5.I2.i1.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i1.p1.2.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i1.p1.2.m2.1d">1</annotation></semantics></math>K problems. These problems need <math alttext="2" class="ltx_Math" display="inline" id="S5.I2.i1.p1.3.m3.1"><semantics id="S5.I2.i1.p1.3.m3.1a"><mn id="S5.I2.i1.p1.3.m3.1.1" xref="S5.I2.i1.p1.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i1.p1.3.m3.1b"><cn id="S5.I2.i1.p1.3.m3.1.1.cmml" type="integer" xref="S5.I2.i1.p1.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i1.p1.3.m3.1c">2</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i1.p1.3.m3.1d">2</annotation></semantics></math> to <math alttext="8" class="ltx_Math" display="inline" id="S5.I2.i1.p1.4.m4.1"><semantics id="S5.I2.i1.p1.4.m4.1a"><mn id="S5.I2.i1.p1.4.m4.1.1" xref="S5.I2.i1.p1.4.m4.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i1.p1.4.m4.1b"><cn id="S5.I2.i1.p1.4.m4.1.1.cmml" type="integer" xref="S5.I2.i1.p1.4.m4.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i1.p1.4.m4.1c">8</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i1.p1.4.m4.1d">8</annotation></semantics></math> steps to be solved. Solutions mainly are a series of elementary calculations using basic arithmetic operations.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.6"><span class="ltx_text ltx_font_bold" id="S5.I2.i2.p1.6.1">MATH</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib191" title="">191</a>]</cite> enables to assess how well models can solve math problems. MATH dataset hast <math alttext="12" class="ltx_Math" display="inline" id="S5.I2.i2.p1.1.m1.1"><semantics id="S5.I2.i2.p1.1.m1.1a"><mn id="S5.I2.i2.p1.1.m1.1.1" xref="S5.I2.i2.p1.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i2.p1.1.m1.1b"><cn id="S5.I2.i2.p1.1.m1.1.1.cmml" type="integer" xref="S5.I2.i2.p1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.p1.1.m1.1c">12</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i2.p1.1.m1.1d">12</annotation></semantics></math>, <math alttext="500" class="ltx_Math" display="inline" id="S5.I2.i2.p1.2.m2.1"><semantics id="S5.I2.i2.p1.2.m2.1a"><mn id="S5.I2.i2.p1.2.m2.1.1" xref="S5.I2.i2.p1.2.m2.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i2.p1.2.m2.1b"><cn id="S5.I2.i2.p1.2.m2.1.1.cmml" type="integer" xref="S5.I2.i2.p1.2.m2.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.p1.2.m2.1c">500</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i2.p1.2.m2.1d">500</annotation></semantics></math> problems from high school math competitions. Each problem in the dataset has a step-by-step solution and a final answer enclosed in a box. The problems cover a wide range of topics and have different levels of complexity. There are seven subjects in total. Furthermore, the difficulty of each problem is rated based on the AoPS standards on a scale from <math alttext="{}^{\prime}1^{\prime}" class="ltx_math_unparsed" display="inline" id="S5.I2.i2.p1.3.m3.1"><semantics id="S5.I2.i2.p1.3.m3.1a"><mmultiscripts id="S5.I2.i2.p1.3.m3.1.1"><mn id="S5.I2.i2.p1.3.m3.1.1.2.2">1</mn><mrow id="S5.I2.i2.p1.3.m3.1.1a"></mrow><mo id="S5.I2.i2.p1.3.m3.1.1.2.3">′</mo><mprescripts id="S5.I2.i2.p1.3.m3.1.1b"></mprescripts><mrow id="S5.I2.i2.p1.3.m3.1.1c"></mrow><mo id="S5.I2.i2.p1.3.m3.1.1.3">′</mo></mmultiscripts><annotation encoding="application/x-tex" id="S5.I2.i2.p1.3.m3.1b">{}^{\prime}1^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i2.p1.3.m3.1c">start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT 1 start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="{}^{\prime}5^{\prime}" class="ltx_math_unparsed" display="inline" id="S5.I2.i2.p1.4.m4.1"><semantics id="S5.I2.i2.p1.4.m4.1a"><mmultiscripts id="S5.I2.i2.p1.4.m4.1.1"><mn id="S5.I2.i2.p1.4.m4.1.1.2.2">5</mn><mrow id="S5.I2.i2.p1.4.m4.1.1a"></mrow><mo id="S5.I2.i2.p1.4.m4.1.1.2.3">′</mo><mprescripts id="S5.I2.i2.p1.4.m4.1.1b"></mprescripts><mrow id="S5.I2.i2.p1.4.m4.1.1c"></mrow><mo id="S5.I2.i2.p1.4.m4.1.1.3">′</mo></mmultiscripts><annotation encoding="application/x-tex" id="S5.I2.i2.p1.4.m4.1b">{}^{\prime}5^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i2.p1.4.m4.1c">start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT 5 start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>. A <math alttext="{}^{\prime}1^{\prime}" class="ltx_math_unparsed" display="inline" id="S5.I2.i2.p1.5.m5.1"><semantics id="S5.I2.i2.p1.5.m5.1a"><mmultiscripts id="S5.I2.i2.p1.5.m5.1.1"><mn id="S5.I2.i2.p1.5.m5.1.1.2.2">1</mn><mrow id="S5.I2.i2.p1.5.m5.1.1a"></mrow><mo id="S5.I2.i2.p1.5.m5.1.1.2.3">′</mo><mprescripts id="S5.I2.i2.p1.5.m5.1.1b"></mprescripts><mrow id="S5.I2.i2.p1.5.m5.1.1c"></mrow><mo id="S5.I2.i2.p1.5.m5.1.1.3">′</mo></mmultiscripts><annotation encoding="application/x-tex" id="S5.I2.i2.p1.5.m5.1b">{}^{\prime}1^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i2.p1.5.m5.1c">start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT 1 start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> shows the easiest problems in a subject, while <math alttext="{}^{\prime}5^{\prime}" class="ltx_math_unparsed" display="inline" id="S5.I2.i2.p1.6.m6.1"><semantics id="S5.I2.i2.p1.6.m6.1a"><mmultiscripts id="S5.I2.i2.p1.6.m6.1.1"><mn id="S5.I2.i2.p1.6.m6.1.1.2.2">5</mn><mrow id="S5.I2.i2.p1.6.m6.1.1a"></mrow><mo id="S5.I2.i2.p1.6.m6.1.1.2.3">′</mo><mprescripts id="S5.I2.i2.p1.6.m6.1.1b"></mprescripts><mrow id="S5.I2.i2.p1.6.m6.1.1c"></mrow><mo id="S5.I2.i2.p1.6.m6.1.1.3">′</mo></mmultiscripts><annotation encoding="application/x-tex" id="S5.I2.i2.p1.6.m6.1b">{}^{\prime}5^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i2.p1.6.m6.1c">start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT 5 start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> represents the most difficult. In terms of formatting, all problems and solutions are presented using LATEX and the Asymptote vector graphics language.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i3.p1.1.1">HellaSwag</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib192" title="">192</a>]</cite> is designed to assess commonsense reasoning in LLMs. This benchmark includes <math alttext="70,000" class="ltx_Math" display="inline" id="S5.I2.i3.p1.1.m1.2"><semantics id="S5.I2.i3.p1.1.m1.2a"><mrow id="S5.I2.i3.p1.1.m1.2.3.2" xref="S5.I2.i3.p1.1.m1.2.3.1.cmml"><mn id="S5.I2.i3.p1.1.m1.1.1" xref="S5.I2.i3.p1.1.m1.1.1.cmml">70</mn><mo id="S5.I2.i3.p1.1.m1.2.3.2.1" xref="S5.I2.i3.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I2.i3.p1.1.m1.2.2" xref="S5.I2.i3.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i3.p1.1.m1.2b"><list id="S5.I2.i3.p1.1.m1.2.3.1.cmml" xref="S5.I2.i3.p1.1.m1.2.3.2"><cn id="S5.I2.i3.p1.1.m1.1.1.cmml" type="integer" xref="S5.I2.i3.p1.1.m1.1.1">70</cn><cn id="S5.I2.i3.p1.1.m1.2.2.cmml" type="integer" xref="S5.I2.i3.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i3.p1.1.m1.2c">70,000</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i3.p1.1.m1.2d">70 , 000</annotation></semantics></math> multiple-choice questions. Each question is derived from one of two domains: ActivityNet or WikiHow, and presents four answer choices regarding what might happen in the following situation. The correct answer provides an actual statement describing the upcoming event, but the three wrong answers are created to confuse machines.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i4.p1">
<p class="ltx_p" id="S5.I2.i4.p1.2"><span class="ltx_text ltx_font_bold" id="S5.I2.i4.p1.2.1">AI2 Reasoning Challenge (ARC)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib193" title="">193</a>]</cite> is used for commonsense reasoning. This benchmark encompasses <math alttext="7,787" class="ltx_Math" display="inline" id="S5.I2.i4.p1.1.m1.2"><semantics id="S5.I2.i4.p1.1.m1.2a"><mrow id="S5.I2.i4.p1.1.m1.2.3.2" xref="S5.I2.i4.p1.1.m1.2.3.1.cmml"><mn id="S5.I2.i4.p1.1.m1.1.1" xref="S5.I2.i4.p1.1.m1.1.1.cmml">7</mn><mo id="S5.I2.i4.p1.1.m1.2.3.2.1" xref="S5.I2.i4.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I2.i4.p1.1.m1.2.2" xref="S5.I2.i4.p1.1.m1.2.2.cmml">787</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.1.m1.2b"><list id="S5.I2.i4.p1.1.m1.2.3.1.cmml" xref="S5.I2.i4.p1.1.m1.2.3.2"><cn id="S5.I2.i4.p1.1.m1.1.1.cmml" type="integer" xref="S5.I2.i4.p1.1.m1.1.1">7</cn><cn id="S5.I2.i4.p1.1.m1.2.2.cmml" type="integer" xref="S5.I2.i4.p1.1.m1.2.2">787</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.1.m1.2c">7,787</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.1.m1.2d">7 , 787</annotation></semantics></math> science examination questions. These questions are in English, and most of them are set up in a multiple-choice format. The questions have been divided into two groups: a Challenge Set with <math alttext="2,590" class="ltx_Math" display="inline" id="S5.I2.i4.p1.2.m2.2"><semantics id="S5.I2.i4.p1.2.m2.2a"><mrow id="S5.I2.i4.p1.2.m2.2.3.2" xref="S5.I2.i4.p1.2.m2.2.3.1.cmml"><mn id="S5.I2.i4.p1.2.m2.1.1" xref="S5.I2.i4.p1.2.m2.1.1.cmml">2</mn><mo id="S5.I2.i4.p1.2.m2.2.3.2.1" xref="S5.I2.i4.p1.2.m2.2.3.1.cmml">,</mo><mn id="S5.I2.i4.p1.2.m2.2.2" xref="S5.I2.i4.p1.2.m2.2.2.cmml">590</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i4.p1.2.m2.2b"><list id="S5.I2.i4.p1.2.m2.2.3.1.cmml" xref="S5.I2.i4.p1.2.m2.2.3.2"><cn id="S5.I2.i4.p1.2.m2.1.1.cmml" type="integer" xref="S5.I2.i4.p1.2.m2.1.1">2</cn><cn id="S5.I2.i4.p1.2.m2.2.2.cmml" type="integer" xref="S5.I2.i4.p1.2.m2.2.2">590</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i4.p1.2.m2.2c">2,590</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i4.p1.2.m2.2d">2 , 590</annotation></semantics></math> difficult questions and an Easy Set with 5,197 questions. Each collection has also been pre-divided into Train, Development, and Test subsets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i5.p1">
<p class="ltx_p" id="S5.I2.i5.p1.2"><span class="ltx_text ltx_font_bold" id="S5.I2.i5.p1.2.1">PIQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib194" title="">194</a>]</cite> is intended to evaluate the language representations on their knowledge of physical commonsense. In this dataset, the focus is on everyday situations with a preference for uncommon solutions. The central task is a multiple-choice question answering, where a question <math alttext="(q)" class="ltx_Math" display="inline" id="S5.I2.i5.p1.1.m1.1"><semantics id="S5.I2.i5.p1.1.m1.1a"><mrow id="S5.I2.i5.p1.1.m1.1.2.2"><mo id="S5.I2.i5.p1.1.m1.1.2.2.1" stretchy="false">(</mo><mi id="S5.I2.i5.p1.1.m1.1.1" xref="S5.I2.i5.p1.1.m1.1.1.cmml">q</mi><mo id="S5.I2.i5.p1.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.1.m1.1b"><ci id="S5.I2.i5.p1.1.m1.1.1.cmml" xref="S5.I2.i5.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.1.m1.1c">(q)</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.1.m1.1d">( italic_q )</annotation></semantics></math> is provided along with two potential solutions <math alttext="(s1,s2)" class="ltx_Math" display="inline" id="S5.I2.i5.p1.2.m2.2"><semantics id="S5.I2.i5.p1.2.m2.2a"><mrow id="S5.I2.i5.p1.2.m2.2.2.2" xref="S5.I2.i5.p1.2.m2.2.2.3.cmml"><mo id="S5.I2.i5.p1.2.m2.2.2.2.3" stretchy="false" xref="S5.I2.i5.p1.2.m2.2.2.3.cmml">(</mo><mrow id="S5.I2.i5.p1.2.m2.1.1.1.1" xref="S5.I2.i5.p1.2.m2.1.1.1.1.cmml"><mi id="S5.I2.i5.p1.2.m2.1.1.1.1.2" xref="S5.I2.i5.p1.2.m2.1.1.1.1.2.cmml">s</mi><mo id="S5.I2.i5.p1.2.m2.1.1.1.1.1" xref="S5.I2.i5.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mn id="S5.I2.i5.p1.2.m2.1.1.1.1.3" xref="S5.I2.i5.p1.2.m2.1.1.1.1.3.cmml">1</mn></mrow><mo id="S5.I2.i5.p1.2.m2.2.2.2.4" xref="S5.I2.i5.p1.2.m2.2.2.3.cmml">,</mo><mrow id="S5.I2.i5.p1.2.m2.2.2.2.2" xref="S5.I2.i5.p1.2.m2.2.2.2.2.cmml"><mi id="S5.I2.i5.p1.2.m2.2.2.2.2.2" xref="S5.I2.i5.p1.2.m2.2.2.2.2.2.cmml">s</mi><mo id="S5.I2.i5.p1.2.m2.2.2.2.2.1" xref="S5.I2.i5.p1.2.m2.2.2.2.2.1.cmml">⁢</mo><mn id="S5.I2.i5.p1.2.m2.2.2.2.2.3" xref="S5.I2.i5.p1.2.m2.2.2.2.2.3.cmml">2</mn></mrow><mo id="S5.I2.i5.p1.2.m2.2.2.2.5" stretchy="false" xref="S5.I2.i5.p1.2.m2.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i5.p1.2.m2.2b"><interval closure="open" id="S5.I2.i5.p1.2.m2.2.2.3.cmml" xref="S5.I2.i5.p1.2.m2.2.2.2"><apply id="S5.I2.i5.p1.2.m2.1.1.1.1.cmml" xref="S5.I2.i5.p1.2.m2.1.1.1.1"><times id="S5.I2.i5.p1.2.m2.1.1.1.1.1.cmml" xref="S5.I2.i5.p1.2.m2.1.1.1.1.1"></times><ci id="S5.I2.i5.p1.2.m2.1.1.1.1.2.cmml" xref="S5.I2.i5.p1.2.m2.1.1.1.1.2">𝑠</ci><cn id="S5.I2.i5.p1.2.m2.1.1.1.1.3.cmml" type="integer" xref="S5.I2.i5.p1.2.m2.1.1.1.1.3">1</cn></apply><apply id="S5.I2.i5.p1.2.m2.2.2.2.2.cmml" xref="S5.I2.i5.p1.2.m2.2.2.2.2"><times id="S5.I2.i5.p1.2.m2.2.2.2.2.1.cmml" xref="S5.I2.i5.p1.2.m2.2.2.2.2.1"></times><ci id="S5.I2.i5.p1.2.m2.2.2.2.2.2.cmml" xref="S5.I2.i5.p1.2.m2.2.2.2.2.2">𝑠</ci><cn id="S5.I2.i5.p1.2.m2.2.2.2.2.3.cmml" type="integer" xref="S5.I2.i5.p1.2.m2.2.2.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i5.p1.2.m2.2c">(s1,s2)</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i5.p1.2.m2.2d">( italic_s 1 , italic_s 2 )</annotation></semantics></math>. Then, the best solution is chosen by whether a model or a human. For each question, only one of the solutions is the correct answer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i6.p1">
<p class="ltx_p" id="S5.I2.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i6.p1.1.1">SIQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib195" title="">195</a>]</cite> provides a framework for evaluating models’ ability for commonsense reasoning about social situations. SIQA dataset has <math alttext="38,000" class="ltx_Math" display="inline" id="S5.I2.i6.p1.1.m1.2"><semantics id="S5.I2.i6.p1.1.m1.2a"><mrow id="S5.I2.i6.p1.1.m1.2.3.2" xref="S5.I2.i6.p1.1.m1.2.3.1.cmml"><mn id="S5.I2.i6.p1.1.m1.1.1" xref="S5.I2.i6.p1.1.m1.1.1.cmml">38</mn><mo id="S5.I2.i6.p1.1.m1.2.3.2.1" xref="S5.I2.i6.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I2.i6.p1.1.m1.2.2" xref="S5.I2.i6.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I2.i6.p1.1.m1.2b"><list id="S5.I2.i6.p1.1.m1.2.3.1.cmml" xref="S5.I2.i6.p1.1.m1.2.3.2"><cn id="S5.I2.i6.p1.1.m1.1.1.cmml" type="integer" xref="S5.I2.i6.p1.1.m1.1.1">38</cn><cn id="S5.I2.i6.p1.1.m1.2.2.cmml" type="integer" xref="S5.I2.i6.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i6.p1.1.m1.2c">38,000</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i6.p1.1.m1.2d">38 , 000</annotation></semantics></math> multiple-choice questions designed to assess emotional and social intelligence in everyday circumstances. This dataset covers a wide variety of social scenarios. In SIQA, the potential answers is a mixture of human-selected responses and machine-generated ones that have been filtered through adversarial processes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i7.p1">
<p class="ltx_p" id="S5.I2.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i7.p1.1.1">OpenBookQA (OBQA)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib196" title="">196</a>]</cite> is a new kind of question-answering dataset where answering its questions requires additional common and commonsense knowledge not contained in the book and rich text comprehension. This dataset includes around 6,000 multiple-choice questions. Each question is linked to one core fact, as well as an additional collection of over <math alttext="6000" class="ltx_Math" display="inline" id="S5.I2.i7.p1.1.m1.1"><semantics id="S5.I2.i7.p1.1.m1.1a"><mn id="S5.I2.i7.p1.1.m1.1.1" xref="S5.I2.i7.p1.1.m1.1.1.cmml">6000</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i7.p1.1.m1.1b"><cn id="S5.I2.i7.p1.1.m1.1.1.cmml" type="integer" xref="S5.I2.i7.p1.1.m1.1.1">6000</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i7.p1.1.m1.1c">6000</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i7.p1.1.m1.1d">6000</annotation></semantics></math> facts. The questions were developed using a multi-stage crowdsourcing and expert filtering procedure. OpenBookQA questions are difficult because they need multi-hop reasoning with limited background.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i8.p1">
<p class="ltx_p" id="S5.I2.i8.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i8.p1.1.1">TruthfulQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib197" title="">197</a>]</cite> is designed specifically to evaluate the truthfulness of language models in generating answers to questions. This dataset includes 817 questions, written by authors, from <math alttext="38" class="ltx_Math" display="inline" id="S5.I2.i8.p1.1.m1.1"><semantics id="S5.I2.i8.p1.1.m1.1a"><mn id="S5.I2.i8.p1.1.m1.1.1" xref="S5.I2.i8.p1.1.m1.1.1.cmml">38</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i8.p1.1.m1.1b"><cn id="S5.I2.i8.p1.1.m1.1.1.cmml" type="integer" xref="S5.I2.i8.p1.1.m1.1.1">38</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i8.p1.1.m1.1c">38</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i8.p1.1.m1.1d">38</annotation></semantics></math> different categories, including health, law, finance, and politics. These questions are purposefully designed to challenge human responders, as they may contain common misunderstandings that lead to incorrect answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i9.p1">
<p class="ltx_p" id="S5.I2.i9.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I2.i9.p1.1.1">OPT-IML Bench</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib103" title="">103</a>]</cite> is a comprehensive benchmark for Instruction Meta-Learning. It covers 2000 NLP tasks from 8 existing benchmarks. The OPT-IML Bench consists of a training set with 17.9 M examples, a dev set with 145K samples, and a test set with 321K samples.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Datasets for Augmented: using external knowledge/tools</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">This section focuses on datasets designed for the augmented abilities of LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<ul class="ltx_itemize" id="S5.I3">
<li class="ltx_item" id="S5.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i1.p1">
<p class="ltx_p" id="S5.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i1.p1.1.1">HotpotQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib198" title="">198</a>]</cite> is designed to cover a diverse and explainable question-answering dataset that necessitates multi-hop reasoning. This dataset is derived from the English Wikipedia. It consists of roughly <math alttext="113,000" class="ltx_Math" display="inline" id="S5.I3.i1.p1.1.m1.2"><semantics id="S5.I3.i1.p1.1.m1.2a"><mrow id="S5.I3.i1.p1.1.m1.2.3.2" xref="S5.I3.i1.p1.1.m1.2.3.1.cmml"><mn id="S5.I3.i1.p1.1.m1.1.1" xref="S5.I3.i1.p1.1.m1.1.1.cmml">113</mn><mo id="S5.I3.i1.p1.1.m1.2.3.2.1" xref="S5.I3.i1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I3.i1.p1.1.m1.2.2" xref="S5.I3.i1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I3.i1.p1.1.m1.2b"><list id="S5.I3.i1.p1.1.m1.2.3.1.cmml" xref="S5.I3.i1.p1.1.m1.2.3.2"><cn id="S5.I3.i1.p1.1.m1.1.1.cmml" type="integer" xref="S5.I3.i1.p1.1.m1.1.1">113</cn><cn id="S5.I3.i1.p1.1.m1.2.2.cmml" type="integer" xref="S5.I3.i1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I3.i1.p1.1.m1.2c">113,000</annotation><annotation encoding="application/x-llamapun" id="S5.I3.i1.p1.1.m1.2d">113 , 000</annotation></semantics></math> questions. Each question in the dataset comes with two paragraphs, called gold paragraphs, from two Wikipedia articles. Also, there is a list of sentences in those paragraphs that crowdworkers have picked as important for answering the question.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i2.p1">
<p class="ltx_p" id="S5.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i2.p1.1.1">ToolQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib199" title="">199</a>]</cite> is a question answering benchmark to evaluate LLMs’ ability to use external tools for answering questions.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I3.i3.p1">
<p class="ltx_p" id="S5.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I3.i3.p1.1.1">GPT4Tools</span> serves as an instructional dataset, generated by instructing advanced teachers (such as ChatGPT), with instructions conditioned on visual content and tool descriptions. This process results in the generation of instructions related to the use of tools. There are three versions of this dataset. The first version comprises 71,000 instruction-following data points utilized to fine-tune the GPT4Tools model. The next version consists of manually cleaned instruction data used for validation, covering instructions related to the tools from the first version. The last version is cleaned instruction data used for testing and includes instructions related to some tools that are not present in the first version.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.2.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S5.T2.3.2" style="font-size:90%;">LLM Datasets Overview.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.4.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T2.4.1.1.1.1">Benchmark Name</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.1.1.2" style="width:113.8pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T2.4.1.1.2.1">Evaluation Metric</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.1.1.3" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T2.4.1.1.3.1">Leaderboard</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.1.1.4" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T2.4.1.1.4.1">Source</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.1.1.5" style="width:56.9pt;"><span class="ltx_text ltx_font_bold ltx_align_top" id="S5.T2.4.1.1.5.1">paperswithcode</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.2.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.2.2.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.2.2.1.1">HumanEval</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.2.2.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.2.2.2.1">PASS@k</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.2.2.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.2.2.3.1"><a class="ltx_ref ltx_href" href="https://llm-leaderboard.streamlit.app" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.2.2.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.2.2.4.1"><a class="ltx_ref ltx_href" href="https://github.com/openai/human-eval" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.2.2.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.2.2.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/code-generation-on-humaneval" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.3.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.3.3.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.3.3.1.1">MBPP</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.3.3.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.3.3.2.1">PASS@k, Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.3.3.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.3.3.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.3.3.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.3.3.4.1"><a class="ltx_ref ltx_href" href="https://github.com/google-research/google-research/tree/master/mbpp" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.3.3.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.3.3.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/code-generation-on-mbpp" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.4.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.4.4.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.4.4.1.1">APPS</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.4.4.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.4.4.2.1">PASS@k, Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.4.4.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.4.4.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.4.4.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.4.4.4.1"><a class="ltx_ref ltx_href" href="https://github.com/hendrycks/apps" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.4.4.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.4.4.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/code-generation-on-apps" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.5.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.5.5.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.5.5.1.1">WikiSQL</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.5.5.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.5.5.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.5.5.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.5.5.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.5.5.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.5.5.4.1"><a class="ltx_ref ltx_href" href="https://github.com/IBM/SQL-to-Text" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.5.5.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.5.5.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/code-generation-on-wikisql" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.6.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.6.6.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.6.6.1.1">CoNaLa</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.6.6.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.6.6.2.1">BLEU</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.6.6.3" style="width:56.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.6.6.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.6.6.4.1"><a class="ltx_ref ltx_href" href="https://conala-corpus.github.io/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.6.6.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.6.6.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/code-generation-on-conala" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.7.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.7.7.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.7.7.1.1">CodeParrot</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.7.7.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.7.7.2.1">PASS@k</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.7.7.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.7.7.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.7.7.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.7.7.4.1"><a class="ltx_ref ltx_href" href="https://github.com/huggingface/blog/blob/main/codeparrot.md" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.7.7.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.7.7.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.8.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.8.8.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.8.8.1.1">HellaSwag</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.8.8.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.8.8.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.8.8.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.8.8.3.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.8.8.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.8.8.4.1"><a class="ltx_ref ltx_href" href="https://rowanzellers.com/hellaswag" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.8.8.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.8.8.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/sentence-completion-on-hellaswag" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.9.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.9.9.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.9.9.1.1">AI2 Reasoning Challenge (ARC)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.9.9.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.9.9.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.9.9.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.9.9.3.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.9.9.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.9.9.4.1"><a class="ltx_ref ltx_href" href="https://github.com/fchollet/ARC/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.9.9.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.9.9.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/common-sense-reasoning-on-arc-challenge" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.10.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.10.10.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.10.10.1.1">BoolQ</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.10.10.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.10.10.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.10.10.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.10.10.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.10.10.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.10.10.4.1"><a class="ltx_ref ltx_href" href="https://github.com/google-research-datasets/boolean-questions" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.10.10.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.10.10.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-boolq" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.11.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.11.11.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.11.11.1.1">MultiRC</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.11.11.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.11.11.2.1">F1-score, Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.11.11.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.11.11.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.11.11.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.11.11.4.1"><a class="ltx_ref ltx_href" href="https://cogcomp.seas.upenn.edu/multirc/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.11.11.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.11.11.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-multirc" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.12.12">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.12.12.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.12.12.1.1">CNN/Daily Mail <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib200" title="">200</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.12.12.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.12.12.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.12.12.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.12.12.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.12.12.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.12.12.4.1"><a class="ltx_ref ltx_href" href="https://github.com/danqi/rc-cnn-dailymail" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.12.12.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.12.12.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.13.13">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.13.13.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.13.13.1.1">SQuAD</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.13.13.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.13.13.2.1">F1-score, EM</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.13.13.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.13.13.3.1"><a class="ltx_ref ltx_href" href="https://rajpurkar.github.io/SQuAD-explorer/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.13.13.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.13.13.4.1"><a class="ltx_ref ltx_href" href="https://rajpurkar.github.io/SQuAD-explorer/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.13.13.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.13.13.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/dataset/squad" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.14.14">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.14.14.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.14.14.1.1">RACE</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.14.14.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.14.14.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.14.14.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.14.14.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.14.14.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.14.14.4.1"><a class="ltx_ref ltx_href" href="https://www.cs.cmu.edu/~glai1/data/race/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.14.14.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.14.14.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/reading-comprehension-on-race" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.15.15">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.15.15.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.15.15.1.1">CNN/Daily Mail <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib201" title="">201</a>]</cite></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.15.15.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.15.15.2.1">ROUGE</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.15.15.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.15.15.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.15.15.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.15.15.4.1"><a class="ltx_ref ltx_href" href="https://github.com/abisee/cnn-dailymail" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.15.15.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.15.15.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/abstractive-text-summarization-on-cnn-daily" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.16.16">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.16.16.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.16.16.1.1">Drop</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.16.16.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.16.16.2.1">F1-score, EM</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.16.16.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.16.16.3.1"><a class="ltx_ref ltx_href" href="https://leaderboard.allenai.org/drop/submissions/public" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.16.16.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.16.16.4.1"><a class="ltx_ref ltx_href" href="https://allenai.org/data/drop" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.16.16.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.16.16.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-drop-test" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.17.17">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.17.17.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.17.17.1.1">QuAC</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.17.17.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.17.17.2.1">F1-score, HEQ-Q, HEQ-D</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.17.17.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.17.17.3.1"><a class="ltx_ref ltx_href" href="https://quac.ai/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.17.17.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.17.17.4.1"><a class="ltx_ref ltx_href" href="https://quac.ai/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.17.17.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.17.17.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-quac" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.18.18">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.18.18.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.18.18.1.1">TriviaQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.18.18.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.18.18.2.1">EM, F1-score, Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.18.18.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.18.18.3.1"><a class="ltx_ref ltx_href" href="https://competitions.codalab.org/competitions/17208#results" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.18.18.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.18.18.4.1"><a class="ltx_ref ltx_href" href="https://nlp.cs.washington.edu/triviaqa/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.18.18.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.18.18.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-triviaqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.19.19">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.19.19.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.19.19.1.1">Natural Questions</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.19.19.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.19.19.2.1">EM, F1-score, Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.19.19.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.19.19.3.1"><a class="ltx_ref ltx_href" href="https://ai.google.com/research/NaturalQuestions" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.19.19.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.19.19.4.1"><a class="ltx_ref ltx_href" href="https://ai.google.com/research/NaturalQuestions" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.19.19.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.19.19.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-natural-questions" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.20.20">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.20.20.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.20.20.1.1">StrategyQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.20.20.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.20.20.2.1">Accuracy, Recall@10, SARI</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.20.20.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.20.20.3.1"><a class="ltx_ref ltx_href" href="https://leaderboard.allenai.org/strategyqa/submissions/public" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.20.20.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.20.20.4.1"><a class="ltx_ref ltx_href" href="https://allenai.org/data/strategyqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.20.20.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.20.20.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-strategyqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.21.21">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.21.21.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.21.21.1.1">CoQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.21.21.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.21.21.2.1">F1-score</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.21.21.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.21.21.3.1"><a class="ltx_ref ltx_href" href="https://stanfordnlp.github.io/coqa/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.21.21.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.21.21.4.1"><a class="ltx_ref ltx_href" href="https://stanfordnlp.github.io/coqa/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.21.21.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.21.21.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/dataset/coqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.22.22">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.22.22.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.22.22.1.1">XSum</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.22.22.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.22.22.2.1">ROUGE</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.22.22.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.22.22.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.22.22.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.22.22.4.1"><a class="ltx_ref ltx_href" href="https://github.com/EdinburghNLP/XSum" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.22.22.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.22.22.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/text-summarization-on-x-sum" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.23.23">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.23.23.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.23.23.1.1">SAMSum</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.23.23.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.23.23.2.1">ROUGE</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.23.23.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.23.23.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.23.23.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.23.23.4.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.23.23.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.23.23.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/dataset/samsum-corpus" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.24.24">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.24.24.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.24.24.1.1">WikiSum</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.24.24.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.24.24.2.1">ROUGE</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.24.24.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.24.24.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.24.24.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.24.24.4.1"><a class="ltx_ref ltx_href" href="https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/wikisum" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.24.24.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.24.24.5.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.25.25">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.25.25.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.25.25.1.1">DialogSum</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.25.25.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.25.25.2.1">ROUGE</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.25.25.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.25.25.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.25.25.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.25.25.4.1"><a class="ltx_ref ltx_href" href="https://github.com/cylnlp/dialogsum" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.25.25.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.25.25.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/dataset/dialogsum" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.26.26">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.26.26.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.26.26.1.1">TruthfulQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.26.26.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.26.26.2.1">MC1 , MC2, % true, % info, BLEURT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.26.26.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.26.26.3.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.26.26.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.26.26.4.1"><a class="ltx_ref ltx_href" href="https://github.com/sylinrl/TruthfulQA" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.26.26.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.26.26.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-truthfulqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.27.27">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.27.27.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.27.27.1.1">MMLU</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.27.27.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.27.27.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.27.27.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.27.27.3.1"><a class="ltx_ref ltx_href" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.27.27.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.27.27.4.1"><a class="ltx_ref ltx_href" href="https://github.com/hendrycks/test" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.27.27.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.27.27.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/dataset/mmlu" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.28.28">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.28.28.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.28.28.1.1">GSM8K</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.28.28.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.28.28.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.28.28.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.28.28.3.1"><a class="ltx_ref ltx_href" href="https://opencompass.org.cn/dataset-detail/GSM8K" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.28.28.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.28.28.4.1"><a class="ltx_ref ltx_href" href="https://github.com/openai/grade-school-math" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.28.28.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.28.28.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.29.29">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.29.29.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.29.29.1.1">PIQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.29.29.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.29.29.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.29.29.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.29.29.3.1"><a class="ltx_ref ltx_href" href="https://leaderboard.allenai.org/physicaliqa/submissions/public" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.29.29.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.29.29.4.1"><a class="ltx_ref ltx_href" href="https://leaderboard.allenai.org/physicaliqa/submissions/about" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.29.29.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.29.29.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-piqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.30.30">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.30.30.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.30.30.1.1">SIQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.30.30.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.30.30.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.30.30.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.30.30.3.1"><a class="ltx_ref ltx_href" href="https://leaderboard.allenai.org/socialiqa/submissions/public" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.30.30.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.30.30.4.1"><a class="ltx_ref ltx_href" href="https://leaderboard.allenai.org/socialiqa/submissions/about" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.30.30.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.30.30.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-social-iqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.31.31">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.31.31.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.31.31.1.1">OpenBookQA (OBQA)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.31.31.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.31.31.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.31.31.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.31.31.3.1"><a class="ltx_ref ltx_href" href="https://leaderboard.allenai.org/open_book_qa/submissions/public" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.31.31.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.31.31.4.1"><a class="ltx_ref ltx_href" href="https://github.com/allenai/OpenBookQA" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.31.31.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.31.31.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-openbookqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.32.32">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.32.32.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.32.32.1.1">HotpotQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.32.32.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.32.32.2.1">EM, F1-score, Joint EM, Joint F1-score,</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.32.32.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.32.32.3.1"><a class="ltx_ref ltx_href" href="https://hotpotqa.github.io/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.32.32.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.32.32.4.1"><a class="ltx_ref ltx_href" href="https://hotpotqa.github.io/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.32.32.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.32.32.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/question-answering-on-hotpotqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.33.33">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.33.33.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.33.33.1.1">MATH</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.33.33.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.33.33.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.33.33.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.33.33.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.33.33.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.33.33.4.1"><a class="ltx_ref ltx_href" href="https://github.com/hendrycks/math" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.33.33.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.33.33.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/math-word-problem-solving-on-math" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.34.34">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.34.34.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.34.34.1.1">CommonsenseQA</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.34.34.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.34.34.2.1">Accuracy</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.34.34.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.34.34.3.1"><a class="ltx_ref ltx_href" href="https://www.tau-nlp.sites.tau.ac.il/csqa-leaderboard2" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.34.34.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.34.34.4.1"><a class="ltx_ref ltx_href" href="https://www.tau-nlp.sites.tau.ac.il/commonsenseqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.34.34.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.34.34.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/sota/common-sense-reasoning-on-commonsenseqa" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.35.35">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.35.35.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.35.35.1.1">Natural Instructions</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.35.35.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.35.35.2.1">ROUGE-L, Human</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.35.35.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.35.35.3.1"><a class="ltx_ref ltx_href" href="https://leaderboard.allenai.org/natural-instructions/submissions/public" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.35.35.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.35.35.4.1"><a class="ltx_ref ltx_href" href="https://instructions.apps.allenai.org/" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.35.35.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.35.35.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/dataset/natural-instructions" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.36.36">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.36.36.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.36.36.1.1">BIG-bench</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.36.36.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.36.36.2.1">Accuracy, Average</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.36.36.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.36.36.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.36.36.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.36.36.4.1"><a class="ltx_ref ltx_href" href="https://github.com/google/BIG-bench" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.36.36.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.36.36.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/dataset/big-bench" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.37.37">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.37.37.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.37.37.1.1">ToolTalk</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.37.37.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.37.37.2.1">Success rate, Precision, Recall, Incorrect action rate, Percent of failing error types</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.37.37.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.37.37.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.37.37.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.37.37.4.1"><a class="ltx_ref ltx_href" href="https://github.com/microsoft/ToolTalk" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.37.37.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.37.37.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/paper/tooltlk-evaluating-tool-usage-in-a" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.38.38">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.38.38.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.38.38.1.1">MetaTool</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.38.38.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.38.38.2.1">Accuracy, Precision, Recall, F1-score</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.38.38.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.38.38.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.38.38.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.38.38.4.1"><a class="ltx_ref ltx_href" href="https://github.com/HowieHwong/MetaTool?tab=readme-ov-file" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.38.38.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.38.38.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/paper/metatool-benchmark-deciding-whether-to-use" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.39.39">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.39.39.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.39.39.1.1">GPT4Tools</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.39.39.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.39.39.2.1">Successful Rate of Thought, Successful Rate of Action, Successful Rate of Arguments, Success Rate</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.39.39.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.39.39.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.39.39.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.39.39.4.1"><a class="ltx_ref ltx_href" href="https://github.com/AILab-CVC/GPT4Tools" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.39.39.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.39.39.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/paper/gpt4tools-teaching-large-language-model-to" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.40.40">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.40.40.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.40.40.1.1">API-Bank</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.40.40.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.40.40.2.1">Correctness, ROUGE, Error(API Hallucination, Has Exception, Invalid Input Parameters, False API Call Format, API Call, Miss Input Parameters)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.40.40.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.40.40.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.40.40.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.40.40.4.1"><a class="ltx_ref ltx_href" href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S5.T2.4.40.40.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.40.40.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/paper/api-bank-a-benchmark-for-tool-augmented-llms" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.41.41">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T2.4.41.41.1" style="width:71.1pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.41.41.1.1">Alpaca-CoT</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.4.41.41.2" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.41.41.2.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.4.41.41.3" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.41.41.3.1">-</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.4.41.41.4" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.41.41.4.1"><a class="ltx_ref ltx_href" href="https://github.com/PhoebusSi/alpaca-CoT" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r ltx_border_t" id="S5.T2.4.41.41.5" style="width:56.9pt;">
<p class="ltx_p ltx_align_top" id="S5.T2.4.41.41.5.1"><a class="ltx_ref ltx_href" href="https://paperswithcode.com/paper/an-empirical-study-of-instruction-tuning" title="">Link</a></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Prominent LLMs’ Performance on Benchmarks</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section we first provide an overview of some of popular metrics used for evaluating the performance of LLMs under different scenarios. We then look at the performance of prominent large language models on some of the popular datasets and benchmarks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Popular Metrics for Evaluating LLMs</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Evaluating the performance of generative language models depends on the underlying task they are going to be used for. Tasks that are mostly about selecting a choice out of given ones (such as sentiment analysis), can be seen as simple as classification and their performance can be evaluated using classification metrics. Metrics such as accuracy, precision, recall, F1, etc are applicable in this case. It is also important to note that the answers generated by the model for specific tasks such as multi-choice question answering are always either True or False. If the answer is not in a set of options, it can be seen as False as well.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">However, some tasks that are purely open-ended text generation cannot be evaluated in the same way as for categorization. Different metrics are required for the specific purpose of the evaluation. Code generation is a very different case in open-ended generative evaluations. The generated code must pass the test suite but on the other hand, it is also important to understand if a model is capable of generating different solutions as a code, what is the probability of selecting the correct one among them. Pass@k is a very good metric in this case. It works in this manner that given a problem, different solutions as code are generated. They are tested for correctness using different functionality tests. Afterward, from generated n solutions, and the respective c number of them being correct equation <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.E4" title="4 ‣ VI-A Popular Metrics for Evaluating LLMs ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">4</span></a> provides the final value.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<table class="ltx_equation ltx_eqn_table" id="S6.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{pass@$k$}:=\mathop{\mathbb{E}}_{\text{Problems}}\left[1-\frac{{\binom{n-%
c}{k}}}{\binom{n}{k}}\right]" class="ltx_Math" display="block" id="S6.E4.m1.6"><semantics id="S6.E4.m1.6a"><mrow id="S6.E4.m1.6.6" xref="S6.E4.m1.6.6.cmml"><mrow id="S6.E4.m1.1.1.1" xref="S6.E4.m1.1.1.1b.cmml"><mtext id="S6.E4.m1.1.1.1a" xref="S6.E4.m1.1.1.1b.cmml">pass@</mtext><mi id="S6.E4.m1.1.1.1.m1.1.1" xref="S6.E4.m1.1.1.1.m1.1.1.cmml">k</mi></mrow><mo id="S6.E4.m1.6.6.2" lspace="0.278em" xref="S6.E4.m1.6.6.2.cmml">:=</mo><mrow id="S6.E4.m1.6.6.1" xref="S6.E4.m1.6.6.1.cmml"><munder id="S6.E4.m1.6.6.1.2" xref="S6.E4.m1.6.6.1.2.cmml"><mo id="S6.E4.m1.6.6.1.2.2" lspace="0.111em" movablelimits="false" rspace="0em" xref="S6.E4.m1.6.6.1.2.2.cmml">𝔼</mo><mtext id="S6.E4.m1.6.6.1.2.3" xref="S6.E4.m1.6.6.1.2.3a.cmml">Problems</mtext></munder><mrow id="S6.E4.m1.6.6.1.1.1" xref="S6.E4.m1.6.6.1.1.2.cmml"><mo id="S6.E4.m1.6.6.1.1.1.2" xref="S6.E4.m1.6.6.1.1.2.1.cmml">[</mo><mrow id="S6.E4.m1.6.6.1.1.1.1" xref="S6.E4.m1.6.6.1.1.1.1.cmml"><mn id="S6.E4.m1.6.6.1.1.1.1.2" xref="S6.E4.m1.6.6.1.1.1.1.2.cmml">1</mn><mo id="S6.E4.m1.6.6.1.1.1.1.1" xref="S6.E4.m1.6.6.1.1.1.1.1.cmml">−</mo><mfrac id="S6.E4.m1.5.5" xref="S6.E4.m1.5.5.cmml"><mrow id="S6.E4.m1.3.3.2.4" xref="S6.E4.m1.3.3.2.3.cmml"><mo id="S6.E4.m1.3.3.2.4.1" xref="S6.E4.m1.3.3.2.3.1.cmml">(</mo><mfrac id="S6.E4.m1.3.3.2.2.2.2" linethickness="0pt" xref="S6.E4.m1.3.3.2.3.cmml"><mrow id="S6.E4.m1.2.2.1.1.1.1.1.1" xref="S6.E4.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S6.E4.m1.2.2.1.1.1.1.1.1.3" xref="S6.E4.m1.2.2.1.1.1.1.1.1.3.cmml">n</mi><mo id="S6.E4.m1.2.2.1.1.1.1.1.1.2" xref="S6.E4.m1.2.2.1.1.1.1.1.1.2.cmml">−</mo><mi id="S6.E4.m1.2.2.1.1.1.1.1.1.4" xref="S6.E4.m1.2.2.1.1.1.1.1.1.4.cmml">c</mi></mrow><mi id="S6.E4.m1.3.3.2.2.2.2.2.1" xref="S6.E4.m1.3.3.2.2.2.2.2.1.cmml">k</mi></mfrac><mo id="S6.E4.m1.3.3.2.4.2" xref="S6.E4.m1.3.3.2.3.1.cmml">)</mo></mrow><mrow id="S6.E4.m1.5.5.4.4" xref="S6.E4.m1.5.5.4.3.cmml"><mo id="S6.E4.m1.5.5.4.4.1" xref="S6.E4.m1.5.5.4.3.1.cmml">(</mo><mfrac id="S6.E4.m1.5.5.4.2.2.2" linethickness="0pt" xref="S6.E4.m1.5.5.4.3.cmml"><mi id="S6.E4.m1.4.4.3.1.1.1.1.1" xref="S6.E4.m1.4.4.3.1.1.1.1.1.cmml">n</mi><mi id="S6.E4.m1.5.5.4.2.2.2.2.1" xref="S6.E4.m1.5.5.4.2.2.2.2.1.cmml">k</mi></mfrac><mo id="S6.E4.m1.5.5.4.4.2" xref="S6.E4.m1.5.5.4.3.1.cmml">)</mo></mrow></mfrac></mrow><mo id="S6.E4.m1.6.6.1.1.1.3" xref="S6.E4.m1.6.6.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.E4.m1.6b"><apply id="S6.E4.m1.6.6.cmml" xref="S6.E4.m1.6.6"><csymbol cd="latexml" id="S6.E4.m1.6.6.2.cmml" xref="S6.E4.m1.6.6.2">assign</csymbol><ci id="S6.E4.m1.1.1.1b.cmml" xref="S6.E4.m1.1.1.1"><mrow id="S6.E4.m1.1.1.1.cmml" xref="S6.E4.m1.1.1.1"><mtext id="S6.E4.m1.1.1.1a.cmml" xref="S6.E4.m1.1.1.1">pass@</mtext><mi id="S6.E4.m1.1.1.1.m1.1.1.cmml" xref="S6.E4.m1.1.1.1.m1.1.1">k</mi></mrow></ci><apply id="S6.E4.m1.6.6.1.cmml" xref="S6.E4.m1.6.6.1"><apply id="S6.E4.m1.6.6.1.2.cmml" xref="S6.E4.m1.6.6.1.2"><csymbol cd="ambiguous" id="S6.E4.m1.6.6.1.2.1.cmml" xref="S6.E4.m1.6.6.1.2">subscript</csymbol><ci id="S6.E4.m1.6.6.1.2.2.cmml" xref="S6.E4.m1.6.6.1.2.2">𝔼</ci><ci id="S6.E4.m1.6.6.1.2.3a.cmml" xref="S6.E4.m1.6.6.1.2.3"><mtext id="S6.E4.m1.6.6.1.2.3.cmml" mathsize="70%" xref="S6.E4.m1.6.6.1.2.3">Problems</mtext></ci></apply><apply id="S6.E4.m1.6.6.1.1.2.cmml" xref="S6.E4.m1.6.6.1.1.1"><csymbol cd="latexml" id="S6.E4.m1.6.6.1.1.2.1.cmml" xref="S6.E4.m1.6.6.1.1.1.2">delimited-[]</csymbol><apply id="S6.E4.m1.6.6.1.1.1.1.cmml" xref="S6.E4.m1.6.6.1.1.1.1"><minus id="S6.E4.m1.6.6.1.1.1.1.1.cmml" xref="S6.E4.m1.6.6.1.1.1.1.1"></minus><cn id="S6.E4.m1.6.6.1.1.1.1.2.cmml" type="integer" xref="S6.E4.m1.6.6.1.1.1.1.2">1</cn><apply id="S6.E4.m1.5.5.cmml" xref="S6.E4.m1.5.5"><divide id="S6.E4.m1.5.5.5.cmml" xref="S6.E4.m1.5.5"></divide><apply id="S6.E4.m1.3.3.2.3.cmml" xref="S6.E4.m1.3.3.2.4"><csymbol cd="latexml" id="S6.E4.m1.3.3.2.3.1.cmml" xref="S6.E4.m1.3.3.2.4.1">binomial</csymbol><apply id="S6.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S6.E4.m1.2.2.1.1.1.1.1.1"><minus id="S6.E4.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S6.E4.m1.2.2.1.1.1.1.1.1.2"></minus><ci id="S6.E4.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S6.E4.m1.2.2.1.1.1.1.1.1.3">𝑛</ci><ci id="S6.E4.m1.2.2.1.1.1.1.1.1.4.cmml" xref="S6.E4.m1.2.2.1.1.1.1.1.1.4">𝑐</ci></apply><ci id="S6.E4.m1.3.3.2.2.2.2.2.1.cmml" xref="S6.E4.m1.3.3.2.2.2.2.2.1">𝑘</ci></apply><apply id="S6.E4.m1.5.5.4.3.cmml" xref="S6.E4.m1.5.5.4.4"><csymbol cd="latexml" id="S6.E4.m1.5.5.4.3.1.cmml" xref="S6.E4.m1.5.5.4.4.1">binomial</csymbol><ci id="S6.E4.m1.4.4.3.1.1.1.1.1.cmml" xref="S6.E4.m1.4.4.3.1.1.1.1.1">𝑛</ci><ci id="S6.E4.m1.5.5.4.2.2.2.2.1.cmml" xref="S6.E4.m1.5.5.4.2.2.2.2.1">𝑘</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E4.m1.6c">\text{pass@$k$}:=\mathop{\mathbb{E}}_{\text{Problems}}\left[1-\frac{{\binom{n-%
c}{k}}}{\binom{n}{k}}\right]</annotation><annotation encoding="application/x-llamapun" id="S6.E4.m1.6d">pass@ italic_k := blackboard_E start_POSTSUBSCRIPT Problems end_POSTSUBSCRIPT [ 1 - divide start_ARG ( FRACOP start_ARG italic_n - italic_c end_ARG start_ARG italic_k end_ARG ) end_ARG start_ARG ( FRACOP start_ARG italic_n end_ARG start_ARG italic_k end_ARG ) end_ARG ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">Exact match (EM) is another metric that is mostly concerned with exact matches from (pre-defined) answers. It counts a prediction as correct if it exactly matches one of more than one desired reference text token by token. In some cases, it can be the same as accuracy and the equation <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.E5" title="5 ‣ VI-A Popular Metrics for Evaluating LLMs ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a> shows the mathematical definition. Here M is total number of correct answers and N is the total number of questions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib202" title="">202</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p5">
<table class="ltx_equation ltx_eqn_table" id="S6.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="EM=\frac{M}{N}" class="ltx_Math" display="block" id="S6.E5.m1.1"><semantics id="S6.E5.m1.1a"><mrow id="S6.E5.m1.1.1" xref="S6.E5.m1.1.1.cmml"><mrow id="S6.E5.m1.1.1.2" xref="S6.E5.m1.1.1.2.cmml"><mi id="S6.E5.m1.1.1.2.2" xref="S6.E5.m1.1.1.2.2.cmml">E</mi><mo id="S6.E5.m1.1.1.2.1" xref="S6.E5.m1.1.1.2.1.cmml">⁢</mo><mi id="S6.E5.m1.1.1.2.3" xref="S6.E5.m1.1.1.2.3.cmml">M</mi></mrow><mo id="S6.E5.m1.1.1.1" xref="S6.E5.m1.1.1.1.cmml">=</mo><mfrac id="S6.E5.m1.1.1.3" xref="S6.E5.m1.1.1.3.cmml"><mi id="S6.E5.m1.1.1.3.2" xref="S6.E5.m1.1.1.3.2.cmml">M</mi><mi id="S6.E5.m1.1.1.3.3" xref="S6.E5.m1.1.1.3.3.cmml">N</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S6.E5.m1.1b"><apply id="S6.E5.m1.1.1.cmml" xref="S6.E5.m1.1.1"><eq id="S6.E5.m1.1.1.1.cmml" xref="S6.E5.m1.1.1.1"></eq><apply id="S6.E5.m1.1.1.2.cmml" xref="S6.E5.m1.1.1.2"><times id="S6.E5.m1.1.1.2.1.cmml" xref="S6.E5.m1.1.1.2.1"></times><ci id="S6.E5.m1.1.1.2.2.cmml" xref="S6.E5.m1.1.1.2.2">𝐸</ci><ci id="S6.E5.m1.1.1.2.3.cmml" xref="S6.E5.m1.1.1.2.3">𝑀</ci></apply><apply id="S6.E5.m1.1.1.3.cmml" xref="S6.E5.m1.1.1.3"><divide id="S6.E5.m1.1.1.3.1.cmml" xref="S6.E5.m1.1.1.3"></divide><ci id="S6.E5.m1.1.1.3.2.cmml" xref="S6.E5.m1.1.1.3.2">𝑀</ci><ci id="S6.E5.m1.1.1.3.3.cmml" xref="S6.E5.m1.1.1.3.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E5.m1.1c">EM=\frac{M}{N}</annotation><annotation encoding="application/x-llamapun" id="S6.E5.m1.1d">italic_E italic_M = divide start_ARG italic_M end_ARG start_ARG italic_N end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS1.p6">
<p class="ltx_p" id="S6.SS1.p6.1">Human equivalence score (HEQ) on the other hand, is an alternative to F1 score <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib203" title="">203</a>]</cite>. HEQ-Q represents the precision of individual questions, wherein an answer is deemed correct if the model’s F1 score surpasses the average human F1 score. Likewise, HEQ-D denotes the precision of each dialogue; it is deemed accurate when all questions within the dialogue meet the criteria of HEQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib182" title="">182</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T3.8.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S6.T3.9.2" style="font-size:90%;">LLM categories and respective definitions.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T3.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.6.7.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.6.7.1.1">Classification</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T3.6.7.1.2">Category</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T3.6.7.1.3">Description</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.1.1.2" rowspan="4">
<span class="ltx_text" id="S6.T3.1.1.2.1">Size</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.1.1.3">Small</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.1.1.1">Number of parameters <math alttext="\leq" class="ltx_Math" display="inline" id="S6.T3.1.1.1.m1.1"><semantics id="S6.T3.1.1.1.m1.1a"><mo id="S6.T3.1.1.1.m1.1.1" xref="S6.T3.1.1.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S6.T3.1.1.1.m1.1b"><leq id="S6.T3.1.1.1.m1.1.1.cmml" xref="S6.T3.1.1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.1.1.1.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S6.T3.1.1.1.m1.1d">≤</annotation></semantics></math> 1B</td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.3.3.3">Medium</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.3.3.2">1B <math alttext="<" class="ltx_Math" display="inline" id="S6.T3.2.2.1.m1.1"><semantics id="S6.T3.2.2.1.m1.1a"><mo id="S6.T3.2.2.1.m1.1.1" xref="S6.T3.2.2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.1.m1.1b"><lt id="S6.T3.2.2.1.m1.1.1.cmml" xref="S6.T3.2.2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S6.T3.2.2.1.m1.1d">&lt;</annotation></semantics></math> Number of parameters <math alttext="\leq" class="ltx_Math" display="inline" id="S6.T3.3.3.2.m2.1"><semantics id="S6.T3.3.3.2.m2.1a"><mo id="S6.T3.3.3.2.m2.1.1" xref="S6.T3.3.3.2.m2.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S6.T3.3.3.2.m2.1b"><leq id="S6.T3.3.3.2.m2.1.1.cmml" xref="S6.T3.3.3.2.m2.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.3.3.2.m2.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S6.T3.3.3.2.m2.1d">≤</annotation></semantics></math> 10B</td>
</tr>
<tr class="ltx_tr" id="S6.T3.5.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.5.5.3">Large</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.5.5.2">10B <math alttext="<" class="ltx_Math" display="inline" id="S6.T3.4.4.1.m1.1"><semantics id="S6.T3.4.4.1.m1.1a"><mo id="S6.T3.4.4.1.m1.1.1" xref="S6.T3.4.4.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T3.4.4.1.m1.1b"><lt id="S6.T3.4.4.1.m1.1.1.cmml" xref="S6.T3.4.4.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.4.4.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S6.T3.4.4.1.m1.1d">&lt;</annotation></semantics></math> Number of parameters <math alttext="\leq" class="ltx_Math" display="inline" id="S6.T3.5.5.2.m2.1"><semantics id="S6.T3.5.5.2.m2.1a"><mo id="S6.T3.5.5.2.m2.1.1" xref="S6.T3.5.5.2.m2.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S6.T3.5.5.2.m2.1b"><leq id="S6.T3.5.5.2.m2.1.1.cmml" xref="S6.T3.5.5.2.m2.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.5.5.2.m2.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S6.T3.5.5.2.m2.1d">≤</annotation></semantics></math> 100B</td>
</tr>
<tr class="ltx_tr" id="S6.T3.6.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.6.2">Very Large</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.6.1">100B <math alttext="<" class="ltx_Math" display="inline" id="S6.T3.6.6.1.m1.1"><semantics id="S6.T3.6.6.1.m1.1a"><mo id="S6.T3.6.6.1.m1.1.1" xref="S6.T3.6.6.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.T3.6.6.1.m1.1b"><lt id="S6.T3.6.6.1.m1.1.1.cmml" xref="S6.T3.6.6.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.6.6.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S6.T3.6.6.1.m1.1d">&lt;</annotation></semantics></math> Number of parameters</td>
</tr>
<tr class="ltx_tr" id="S6.T3.6.8.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.6.8.1.1" rowspan="3">
<span class="ltx_text" id="S6.T3.6.8.1.1.1">Type</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.8.1.2">Foundation model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.8.1.3">Pretrained language model</td>
</tr>
<tr class="ltx_tr" id="S6.T3.6.9.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.9.2.1">Instruction model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.9.2.2">Pretrained and instruction fine-tuned language model</td>
</tr>
<tr class="ltx_tr" id="S6.T3.6.10.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.10.3.1">Chat model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.10.3.2">Pretrained, instruction fine-tuned, and chat fine-tuned language model</td>
</tr>
<tr class="ltx_tr" id="S6.T3.6.11.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.6.11.4.1" rowspan="2">
<span class="ltx_text" id="S6.T3.6.11.4.1.1">Origin</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.11.4.2">Original model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.11.4.3">An original model released with either Foundation, Instruction, or Chat model</td>
</tr>
<tr class="ltx_tr" id="S6.T3.6.12.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.12.5.1">Tuned model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.12.5.2">Fine-tuned version of an original model</td>
</tr>
<tr class="ltx_tr" id="S6.T3.6.13.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T3.6.13.6.1" rowspan="2">
<span class="ltx_text" id="S6.T3.6.13.6.1.1">Availability</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.13.6.2">Publicly available</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T3.6.13.6.3">Model and weights are available due to request to without request</td>
</tr>
<tr class="ltx_tr" id="S6.T3.6.14.7">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T3.6.14.7.1">Publicly unavailable</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T3.6.14.7.2">Model and weights are not publicly available</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T4.2.1.1" style="font-size:90%;">TABLE IV</span>: </span><span class="ltx_text" id="S6.T4.3.2" style="font-size:90%;">Different LLM categorization.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.1.1.1">Model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.1.1.2">Size</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.1.1.3">#Params (B)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.1.1.4">Type</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.1.1.5">Availability</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.1.1.6">Origin</td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.2.2.1">Davinci-002</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.2.2.2" style="background-color:#FFBFBF;"><span class="ltx_text" id="S6.T4.4.2.2.2.1" style="background-color:#FFBFBF;">Very Large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.2.2.3">175</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.2.2.4" style="background-color:#BFBFFF;"><span class="ltx_text" id="S6.T4.4.2.2.4.1" style="background-color:#BFBFFF;">Instruction</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.2.2.5" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.2.2.5.1" style="background-color:#FFFFBF;">Unavailable</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.2.2.6" style="background-color:#FFEFEF;"><span class="ltx_text" id="S6.T4.4.2.2.6.1" style="background-color:#FFEFEF;">Tuned</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.3.3.1">Davinci-003</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.3.3.2" style="background-color:#FFBFBF;"><span class="ltx_text" id="S6.T4.4.3.3.2.1" style="background-color:#FFBFBF;">Very Large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.3.3.3">175</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.3.3.4" style="background-color:#BFBFFF;"><span class="ltx_text" id="S6.T4.4.3.3.4.1" style="background-color:#BFBFFF;">Instruction</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.3.3.5" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.3.3.5.1" style="background-color:#FFFFBF;">Unavailable</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.3.3.6" style="background-color:#FFEFEF;"><span class="ltx_text" id="S6.T4.4.3.3.6.1" style="background-color:#FFEFEF;">Tuned</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.4.4.1">GPT 3.5-turbo</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.4.4.2" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.4.4.2.1" style="background-color:#FFFFBF;">Large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.4.4.3">20</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.4.4.4" style="background-color:#EFBFCF;"><span class="ltx_text" id="S6.T4.4.4.4.4.1" style="background-color:#EFBFCF;">Chat</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.4.4.5" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.4.4.5.1" style="background-color:#FFFFBF;">Unavailable</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.4.4.6" style="background-color:#FFEFEF;"><span class="ltx_text" id="S6.T4.4.4.4.6.1" style="background-color:#FFEFEF;">Tuned</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.5.5.1">Falcon 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.5.5.2" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.5.5.2.1" style="background-color:#BFFFBF;">Medium</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.5.5.3">7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.5.5.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S6.T4.4.5.5.4.1" style="background-color:#DFDFDF;">Foundation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.5.5.5" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.5.5.5.1" style="background-color:#BFFFBF;">Public</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.5.5.6" style="background-color:#BFDFDF;"><span class="ltx_text" id="S6.T4.4.5.5.6.1" style="background-color:#BFDFDF;">Original</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.6.6.1">Alpaca</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.6.6.2" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.6.6.2.1" style="background-color:#FFFFBF;">Large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.6.6.3">13</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.6.6.4" style="background-color:#EFBFCF;"><span class="ltx_text" id="S6.T4.4.6.6.4.1" style="background-color:#EFBFCF;">Chat</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.6.6.5" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.6.6.5.1" style="background-color:#BFFFBF;">Public</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.6.6.6" style="background-color:#FFEFEF;"><span class="ltx_text" id="S6.T4.4.6.6.6.1" style="background-color:#FFEFEF;">Tuned</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.7.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.7.7.1">Pythia 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.7.7.2" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.7.7.2.1" style="background-color:#BFFFBF;">Medium</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.7.7.3">7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.7.7.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S6.T4.4.7.7.4.1" style="background-color:#DFDFDF;">Foundation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.7.7.5" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.7.7.5.1" style="background-color:#BFFFBF;">Public</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.7.7.6" style="background-color:#BFDFDF;"><span class="ltx_text" id="S6.T4.4.7.7.6.1" style="background-color:#BFDFDF;">Original</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.8.8.1">Pythia 12B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.8.8.2" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.8.8.2.1" style="background-color:#FFFFBF;">Large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.8.8.3">12</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.8.8.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S6.T4.4.8.8.4.1" style="background-color:#DFDFDF;">Foundation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.8.8.5" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.8.8.5.1" style="background-color:#BFFFBF;">Public</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.8.8.6" style="background-color:#BFDFDF;"><span class="ltx_text" id="S6.T4.4.8.8.6.1" style="background-color:#BFDFDF;">Original</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.9.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.9.9.1">LLAMA 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.9.9.2" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.9.9.2.1" style="background-color:#BFFFBF;">Medium</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.9.9.3">7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.9.9.4" style="background-color:#EFBFCF;"><span class="ltx_text" id="S6.T4.4.9.9.4.1" style="background-color:#EFBFCF;">Chat</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.9.9.5" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.9.9.5.1" style="background-color:#BFFFBF;">Public</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.9.9.6" style="background-color:#BFDFDF;"><span class="ltx_text" id="S6.T4.4.9.9.6.1" style="background-color:#BFDFDF;">Original</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.10.10">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.10.10.1">LLAMA 2 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.10.10.2" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.10.10.2.1" style="background-color:#BFFFBF;">Medium</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.10.10.3">7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.10.10.4" style="background-color:#EFBFCF;"><span class="ltx_text" id="S6.T4.4.10.10.4.1" style="background-color:#EFBFCF;">Chat</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.10.10.5" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.10.10.5.1" style="background-color:#BFFFBF;">Public</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.10.10.6" style="background-color:#FFEFEF;"><span class="ltx_text" id="S6.T4.4.10.10.6.1" style="background-color:#FFEFEF;">Tuned</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.11.11">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.11.11.1">LLAMA 2 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.11.11.2" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.11.11.2.1" style="background-color:#BFFFBF;">Medium</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.11.11.3">7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.11.11.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S6.T4.4.11.11.4.1" style="background-color:#DFDFDF;">Foundation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.11.11.5" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.11.11.5.1" style="background-color:#BFFFBF;">Public</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.11.11.6" style="background-color:#BFDFDF;"><span class="ltx_text" id="S6.T4.4.11.11.6.1" style="background-color:#BFDFDF;">Original</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.12.12">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.12.12.1">Vicuna 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.12.12.2" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.12.12.2.1" style="background-color:#FFFFBF;">Large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.12.12.3">13</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.12.12.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S6.T4.4.12.12.4.1" style="background-color:#DFDFDF;">Foundation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.12.12.5" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.12.12.5.1" style="background-color:#BFFFBF;">Public</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.12.12.6" style="background-color:#FFEFEF;"><span class="ltx_text" id="S6.T4.4.12.12.6.1" style="background-color:#FFEFEF;">Tuned</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.13.13">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.13.13.1">Vicuna 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.13.13.2" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.13.13.2.1" style="background-color:#BFFFBF;">Medium</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.13.13.3">7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.13.13.4" style="background-color:#DFDFDF;"><span class="ltx_text" id="S6.T4.4.13.13.4.1" style="background-color:#DFDFDF;">Foundation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.13.13.5" style="background-color:#BFFFBF;"><span class="ltx_text" id="S6.T4.4.13.13.5.1" style="background-color:#BFFFBF;">Public</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.13.13.6" style="background-color:#FFEFEF;"><span class="ltx_text" id="S6.T4.4.13.13.6.1" style="background-color:#FFEFEF;">Tuned</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.14.14">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.14.14.1">Claude</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.14.14.2" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.14.14.2.1" style="background-color:#FFFFBF;">Large</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.14.14.3">93</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.14.14.4" style="background-color:#EFBFCF;"><span class="ltx_text" id="S6.T4.4.14.14.4.1" style="background-color:#EFBFCF;">Chat</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.14.14.5" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.14.14.5.1" style="background-color:#FFFFBF;">Unavailable</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.4.14.14.6" style="background-color:#BFDFDF;"><span class="ltx_text" id="S6.T4.4.14.14.6.1" style="background-color:#BFDFDF;">Original</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.4.15.15">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T4.4.15.15.1">Claude 2</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T4.4.15.15.2" style="background-color:#FFBFBF;"><span class="ltx_text" id="S6.T4.4.15.15.2.1" style="background-color:#FFBFBF;">Very Large</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T4.4.15.15.3">137</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T4.4.15.15.4" style="background-color:#EFBFCF;"><span class="ltx_text" id="S6.T4.4.15.15.4.1" style="background-color:#EFBFCF;">Chat</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T4.4.15.15.5" style="background-color:#FFFFBF;"><span class="ltx_text" id="S6.T4.4.15.15.5.1" style="background-color:#FFFFBF;">Unavailable</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T4.4.15.15.6" style="background-color:#BFDFDF;"><span class="ltx_text" id="S6.T4.4.15.15.6.1" style="background-color:#BFDFDF;">Original</span></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p7">
<p class="ltx_p" id="S6.SS1.p7.1">Evaluation of other generative tasks such as machine translation are based on metrics such as Rouge and BLEU. These scores work well when there is a reference text as ground truth (such as translation) and a hypothesis that is generated by the generative model, in our case the LLM. These scores are mostly used for cases where the goal is to detect the similarity of the answer and ground truth in a computation manner. In a computation manner, it meant that nothing more than N-Grams would be used. However, metrics such as BERT-Score are also good for these cases but they are also heavily erroneous because another model is used to judge. Still, even today, evaluating purely generated content is very hard and no completely fitting metric is not found, metrics are either looking for simplistic features such as N-Gram, SkipGram, etc, or they are models with unknown accuracy and preciseness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib204" title="">204</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p8">
<p class="ltx_p" id="S6.SS1.p8.1">Generative evaluation metrics are also another type of evaluation metric for LLMs that use another LLM for evaluating the answer. However, depending on the task itself, evaluation can be possible in this way or not. Another dependency that makes generative evaluation error-prone is reliance on the prompt itself. <span class="ltx_ref ltx_ref_self">RAGAS</span> is one of the good examples that incorporate the usage of generative evaluation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p9">
<p class="ltx_p" id="S6.SS1.p9.1">Various benchmarks and leaderboards have been proposed to address the most challenging question in the world of large language models: Which one is better? However not a simple answer can address this question. The answer depends on various aspects of large language models. Section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S5" title="V Popular Datasets for LLMs ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">V</span></a> shows the categorical presentation of different tasks and the most important datasets in each category. We will follow the same categorization and provide a comparison based on each category. After providing comparison for each category, we will provide a broad overview of aggregated performance by averaging the reported performance metric on different tasks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.p10">
<p class="ltx_p" id="S6.SS1.p10.1">Evaluating different LLMs can be seen also from different perspectives. For example, a LLM with a drastically fewer number of parameters is not completely comparable to one with a larger number of parameters. From this perspective, we will categorize LLMs in four categories as well: <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.1">small</span> (less than or equal to 1 billion parameters), <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.2">medium</span> (between 1 and 10 billion), <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.3">large</span> (between 10 and 100 billion), and <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.4">very large</span> (more than 100 billion). Another classification for the LLMs we use is their primary use case. We consider each LLM to be either: <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.5">Foundation</span> model (pretrained language model with no instruction fine-tuning and chat fine-tuning), <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.6">Instruction</span> model (pretrained language model with only instruction fine-tuning), and <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.7">Chat</span> model (pretrained language model with instruction and chat fine-tuning). Apart from all the categorization described, another category is required to distinguish between original models and tuned ones. <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.8">Original</span> models are those that have been released as a foundation model or a fine-tuned one. <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.9">Tuned</span> models are those that grasped the original model and tuned it with different datasets or even different training approaches. It is also good to note that original models are usually foundation models that have been fine-tuned on specific datasets or even different approaches. Availability of the model weights regardless of the license is another category in our classification. Models that have their weights publicly available (even through request) are noted as <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.10">Public</span> models while others are noted as <span class="ltx_text ltx_font_bold" id="S6.SS1.p10.1.11">Private</span>. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.T3" title="TABLE III ‣ VI-A Popular Metrics for Evaluating LLMs ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">III</span></a> shows all of these definitions and abbreviations used in the rest of the article.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.F43" title="Figure 43 ‣ VI-A Popular Metrics for Evaluating LLMs ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">43</span></a> illustrate these visually.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S6.F43"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="400" id="S6.F43.g1" src="x7.png" width="612">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F43.2.1.1" style="font-size:90%;">Figure 43</span>: </span><span class="ltx_text" id="S6.F43.3.2" style="font-size:90%;">LLM categorizations.</span></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.p11">
<p class="ltx_p" id="S6.SS1.p11.1">According to the provided categorizations, we can categorize and label each notable LLM as shown in table <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.T4" title="TABLE IV ‣ VI-A Popular Metrics for Evaluating LLMs ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">IV</span></a>. As can be seen from this table, models categorized as very large are also unavailable as well.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">LLMs’ Performance on Different Tasks</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Commonsense reasoning is one of the important capabilities each model can obtain. This capability denotes the ability of the model to use prior knowledge in combination with reasoning skills. In the case of HellaSwag for example, finding the continuation of text is challenging because the given text contains a partial part of the story while the given choices as continuation are tricky to select, and without having prior knowledge about the world it is not possible. This specific kind of reasoning deserves high attention because it is related to utilizing previous knowledge with open text-described scenes or facts. As can be seen from table <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.T5" title="TABLE V ‣ VI-B LLMs’ Performance on Different Tasks ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">V</span></a> not just Unavailable models but also Public ones can achieve good results on various tests.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T5.2.1.1" style="font-size:90%;">TABLE V</span>: </span><span class="ltx_text" id="S6.T5.3.2" style="font-size:90%;">Commonsense reasoning comparison.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T5.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.1.1.1">Model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.1.1.2">OBQA</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.1.1.3">HellaSwag</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.2.2.1">Davinci-003</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T5.4.2.2.2.1">51</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.2.2.3">83.4</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.3.3.1">Falcon 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.3.3.2">44.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.3.3.3">76.3</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.4.4.1">Alpaca</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.4.4.2">43.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.4.4.3">73.9</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.5.5.1">Pythia 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.5.5.2">37.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.5.5.3">64</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.6.6.1">Pythia 12B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.6.6.2">43.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.6.6.3">68.1</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.7.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.7.7.1">LLAMA 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.7.7.2">42.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.7.7.3">73</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.8.8.1">Dolly 6B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.8.8.2">41.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.8.8.3">67.6</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.9.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.9.9.1">Dolly 12B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.9.9.2">40.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.9.9.3">71</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.10.10">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.10.10.1">Alpaca 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.10.10.2">43.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.10.10.3">73.9</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.11.11">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.11.11.1">Alpaca Lora 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.11.11.2">42.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.11.11.3">74</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.12.12">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.12.12.1">GPT-J 6.7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.12.12.2">38.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.12.12.3">66.2</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.13.13">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.13.13.1">LLama 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.13.13.2">42.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.13.13.3">73</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.14.14">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.14.14.1">LLama 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.14.14.2">42.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.14.14.3">76.2</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.15.15">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.15.15.1">Pythia 6.7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.15.15.2">37.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.15.15.3">64</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.16.16">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.16.16.1">Pythia 12B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.16.16.2">38</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.16.16.3">67.3</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.17.17">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.17.17.1">StableLM Tuned</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.17.17.2">33.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.17.17.3">53.6</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.18.18">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.18.18.1">Koala 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.18.18.2">42.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.18.18.3">72.6</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.19.19">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.19.19.1">Mosaic mpt-7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.19.19.2">42.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.19.19.3">76.3</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.20.20">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt" id="S6.T5.4.20.20.1">LLAMA 2 70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T5.4.20.20.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T5.4.20.20.3">87.33</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.21.21">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.21.21.1">LLAMA 65B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.21.21.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.21.21.3">86.09</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.22.22">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.22.22.1">Falcon 40B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.22.22.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.22.22.3">85.3</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.23.23">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.23.23.1">Falcon 180B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.23.23.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.23.23.3">88.86</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.24.24">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.24.24.1">MPT Instruct 30B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.24.24.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.24.24.3">84.31</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.25.25">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.25.25.1">MPT Instruct 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.25.25.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.25.25.3">77.91</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.26.26">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.26.26.1">Yi 6B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.26.26.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.26.26.3">76.42</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.27.27">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.27.27.1">Yi 34B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.27.27.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.27.27.3">85.69</td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.28.28">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.28.28.1">GPT-4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.28.28.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T5.4.28.28.3"><span class="ltx_text ltx_font_bold" id="S6.T5.4.28.28.3.1">95.3</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.4.29.29">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T5.4.29.29.1">Gemini Ultra</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T5.4.29.29.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T5.4.29.29.3">87.8</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">From the results presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.T5" title="TABLE V ‣ VI-B LLMs’ Performance on Different Tasks ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">V</span></a> it is clear that GPT-4 achieves best results for HellaSwag while Davinci-003 is best model for OBQA. It is also good to note that results for OBQA are not reported for all of the models and possibly davinci-003 is not the best model achieving highest results on OBQA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">Not all models report their performance on all datasets, and because of that, the number of models for which performance is reported in different tables varies.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T6.2.1.1" style="font-size:90%;">TABLE VI</span>: </span><span class="ltx_text" id="S6.T6.3.2" style="font-size:90%;">Symbolic reasoning comparison.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T6.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T6.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.4.1.1.1">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T6.4.1.1.2">Cobjects</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T6.4.1.1.3">Penguins</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T6.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.4.2.1.1">GPT-NeoX</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.2.1.2">26</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.2.1.3">33.56</td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.4.3.2.1">OPT 66B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.3.2.2">31.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.3.2.3">28.08</td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.4.4.3.1">Bloomberg GPT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.4.3.2">34.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.4.3.3">37.67</td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.4.5.4.1">BLOOM 176B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.5.4.2">36.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.5.4.3">40.41</td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.6.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.4.6.5.1">PaLM 540B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.6.5.2">38</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.6.5.3">44.5</td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.7.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.4.7.6.1">Gopher-280B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.7.6.2">49.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.7.6.3">40.6</td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.8.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.4.8.7.1">Chinchilla-70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.8.7.2">59.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T6.4.8.7.3">48.7</td>
</tr>
<tr class="ltx_tr" id="S6.T6.4.9.8">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T6.4.9.8.1">PaLM 2</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T6.4.9.8.2">61.2</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T6.4.9.8.3">65.8</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">World knowledge is mostly about general knowledge questions, for example, in Wikifact dataset questions such as ”Who is the author of a specific well-known book” can be found and references are also provided. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.T7" title="TABLE VII ‣ VI-B LLMs’ Performance on Different Tasks ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">VII</span></a> shows the results.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T7.2.1.1" style="font-size:90%;">TABLE VII</span>: </span><span class="ltx_text" id="S6.T7.3.2" style="font-size:90%;">World knowledge comparison.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T7.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T7.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.1.1.1">Model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.1.1.2">TriviaQA</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.1.1.3">NaturalQ</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.1.1.4">WebQ</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.1.1.5">ARC</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.2.2.1">BLOOM</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.2.2.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.2.2.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.2.2.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.2.2.5">32.9</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.3.3.1">BLOOM 176B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.3.3.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.3.3.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.3.3.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.3.3.5">50.85</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.4.4.1">Bloomberg GPT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.4.4.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.4.4.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.4.4.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.4.4.5">48.63</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.5.5.1">Chinchilla</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.5.5.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.5.5.3">35.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.5.5.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.5.5.5">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.6.6.1">Codex + REPLUG</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.6.6.2">76.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.6.6.3">44.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.6.6.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.6.6.5">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.7.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.7.7.1">GAL 120B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.7.7.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.7.7.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.7.7.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.7.7.5">67.9</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.8.8.1">GLaM 62B/64E</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.8.8.2">75.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.8.8.3">32.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.8.8.4">15.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.8.8.5">50.3</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.9.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.9.9.1">Gopher</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.9.9.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.9.9.3">28.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.9.9.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.9.9.5">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.10.10">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.10.10.1">GPT-3 175B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.10.10.2">71.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.10.10.3">29.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.10.10.4">41.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.10.10.5">85.2</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.11.11">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.11.11.1">GPT-4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.11.11.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.11.11.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.11.11.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.11.11.5">96.4</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.12.12">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.12.12.1">GPT-NeoX</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.12.12.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.12.12.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.12.12.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.12.12.5">45.39</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.13.13">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.13.13.1">LLaMA 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.13.13.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.13.13.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.13.13.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.13.13.5">52.7</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.14.14">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.14.14.1">LLaMA 2 70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.14.14.2">85</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.14.14.3">33</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.14.14.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.14.14.5">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.15.15">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.15.15.1">LLaMA 33B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.15.15.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.15.15.3">24.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.15.15.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.15.15.5">57.8</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.16.16">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.16.16.1">LLaMA 65B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.16.16.2">72.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.16.16.3">39.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.16.16.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.16.16.5">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.17.17">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.17.17.1">LLaMA 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.17.17.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.17.17.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.17.17.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.17.17.5">47.6</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.18.18">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.18.18.1">Mistral 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.18.18.2">69.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.18.18.3">28.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.18.18.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.18.18.5">55.5</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.19.19">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.19.19.1">Neo-6B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.19.19.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.19.19.3">13.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.19.19.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.19.19.5">-</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.20.20">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.20.20.1">OPT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.20.20.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.20.20.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.20.20.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.20.20.5">31.1</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.21.21">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.21.21.1">OPT 66B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.21.21.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.21.21.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.21.21.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.21.21.5">44.54</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.22.22">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.22.22.1">OPT-175B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.22.22.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.22.22.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.22.22.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.22.22.5">43.94</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.23.23">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.23.23.1">OPT-175B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.23.23.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.23.23.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.23.23.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.23.23.5">25.6</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.24.24">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.24.24.1">PaLM 2-L</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.24.24.2">86.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.24.24.3">37.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.24.24.4">28.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.24.24.5">95.1</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.25.25">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.25.25.1">PaLM 2-M</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.25.25.2">81.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.25.25.3">32</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.25.25.4">26.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.25.25.5">64.9</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.26.26">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.26.26.1">PaLM 2-S</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.26.26.2">75.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.26.26.3">25.3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.26.26.4">21.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.26.26.5">59.6</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.27.27">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.27.27.1">PaLM-540B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.27.27.2">81.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.27.27.3">39.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.27.27.4">43.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.27.27.5">87.1</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.28.28">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.28.28.1">phi-1.5-web 1.3B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.28.28.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.28.28.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.28.28.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.28.28.5">44.9</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.29.29">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.29.29.1">SparseGPT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.29.29.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.29.29.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.29.29.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.29.29.5">38.99</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.30.30">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.30.30.1">SparseGPT</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.30.30.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.30.30.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.30.30.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.4.30.30.5">39.85</td>
</tr>
<tr class="ltx_tr" id="S6.T7.4.31.31">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T7.4.31.31.1">SparseGPT</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T7.4.31.31.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T7.4.31.31.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T7.4.31.31.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T7.4.31.31.5">41.3</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p5">
<p class="ltx_p" id="S6.SS2.p5.1">For some specific use-case models, it is highly demanded to have coding and code-generation capability. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.T8" title="TABLE VIII ‣ VI-B LLMs’ Performance on Different Tasks ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">VIII</span></a> shows the results of different models on coding capability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T8.2.1.1" style="font-size:90%;">TABLE VIII</span>: </span><span class="ltx_text" id="S6.T8.3.2" style="font-size:90%;">Coding capability comparison.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T8.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T8.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.1.1.1">Model</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.1.1.2">HumanEval</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.2.2.1">Gemini Ultra</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.2.2.2">74.4</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.3.3.1">Gemini Pro</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.3.3.2">67.7</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.4.4.1">GPT-4</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.4.4.2">67</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.5.5.1">WizardCoder 15B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.5.5.2">57.3</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.6.6.1">phi-1 1.3B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.6.6.2">50.6</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.7.7.1">Code Llama</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.7.7.2">48.8</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.8.8.1">GPT-3.5</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.8.8.2">48.1</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.9.9.1">OctoCoder</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.9.9.2">46.2</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.10.10.1">phi-1-small</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.10.10.2">45</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.11.11.1">PaLM 2-S</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.11.11.2">37.6</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.12.12.1">InstructCodeT5+ 16B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.12.12.2">35</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.13.13.1">Mistral 7B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.13.13.2">30.5</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.14.14.1">LLaMA 2</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.14.14.2">29.9</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.15.15.1">phi-1-base</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.15.15.2">29</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.16.16.1">Codex-12B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.16.16.2">28.81</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.17.17.1">PaLM 540B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.17.17.2">26.2</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.18.18.1">CodeT5+ 2B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.18.18.2">24.2</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.19.19.1">LLaMA 65B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.19.19.2">23.7</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.20.20.1">LLaMA 33B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.20.20.2">21.7</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.21.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.21.21.1">PaLM 62B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.21.21.2">15.9</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.22.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.22.22.1">LLaMA 13B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.22.22.2">15.8</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.23.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.23.23.1">LaMDA 137B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.23.23.2">14</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.24.24.1">MIM-350M</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.24.24.2">13.7</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.25.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.25.25.1">LLaMA 7B</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T8.4.25.25.2">10.5</td>
</tr>
<tr class="ltx_tr" id="S6.T8.4.26.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T8.4.26.26.1">PaLM 8B</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T8.4.26.26.2">3.6</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p6">
<p class="ltx_p" id="S6.SS2.p6.1">Arithmetic reasoning is another challenging reasoning capability to achieve. GSM8K for example contains grade school mathematical questions with respect to their answers. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.T9" title="TABLE IX ‣ VI-B LLMs’ Performance on Different Tasks ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">IX</span></a> provides an insight for different model comparisons.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T9.2.1.1" style="font-size:90%;">TABLE IX</span>: </span><span class="ltx_text" id="S6.T9.3.2" style="font-size:90%;">Arithmetic reasoning comparison.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T9.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T9.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.1.1.1">Model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.1.1.2">GSM8k</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.1.1.3">MATH</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.2.2.1">Gemini Ultra</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.2.2.2">94.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.2.2.3">53.2</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.3.3.1">GPT-4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.3.3.2">87.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.3.3.3">42.5</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.4.4.1">Gemini Pro</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.4.4.2">86.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.4.4.3">32.6</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.5.5.1">ToRA 70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.5.5.2">84.3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.5.5.3">49.7</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.6.6.1">MathCoder-L-70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.6.6.2">83.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.6.6.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.7.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.7.7.1">MetaMath 70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.7.7.2">82.3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.7.7.3">26</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.8.8.1">MuggleMATH 70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.8.8.2">82.3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.8.8.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.9.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.9.9.1">MathCoder-CL-34B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.9.9.2">81.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.9.9.3">45.2</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.10.10">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.10.10.1">ToRA-Code 34B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.10.10.2">80.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.10.10.3">50.8</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.11.11">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.11.11.1">MetaMath-Mistral-7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.11.11.2">77.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.11.11.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.12.12">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.12.12.1">Arithmo2-Mistral-7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.12.12.2">76.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.12.12.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.13.13">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.13.13.1">ToRA-Code 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.13.13.2">75.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.13.13.3">48.1</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.14.14">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.14.14.1">Arithmo-Mistral-7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.14.14.2">74.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.14.14.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.15.15">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.15.15.1">MathCoder-CL-13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.15.15.2">74.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.15.15.3">35.9</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.16.16">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.16.16.1">MuggleMATH 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.16.16.2">74</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.16.16.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.17.17">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.17.17.1">CodeT5+</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.17.17.2">73.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.17.17.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.18.18">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.18.18.1">KwaiYiiMath 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.18.18.2">73.3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.18.18.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.19.19">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.19.19.1">ToRA-Code 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.19.19.2">72.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.19.19.3">44.6</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.20.20">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.20.20.1">MathCoder-L-13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.20.20.2">72.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.20.20.3">29.9</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.21.21">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.21.21.1">MetaMath 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.21.21.2">71</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.21.21.3">22.5</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.22.22">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.22.22.1">LLaMA 65B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.22.22.2">69.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.22.22.3">10.6</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.23.23">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.23.23.1">MuggleMATH 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.23.23.2">68.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.23.23.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.24.24">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.24.24.1">MathCoder-CL-7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.24.24.2">67.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.24.24.3">23.3</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.25.25">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.25.25.1">MetaMath 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.25.25.2">66.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.25.25.3">19.4</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.26.26">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.26.26.1">RFT 70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.26.26.2">64.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.26.26.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.27.27">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.27.27.1">MathCoder-L-7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.27.27.2">64.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.27.27.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.28.28">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.28.28.1">Orca 2-13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.28.28.2">59.14</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.28.28.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.29.29">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.29.29.1">U-PaLM</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.29.29.2">58.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.29.29.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.30.30">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.30.30.1">PaLM-540B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.30.30.2">58.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.30.30.3">8.8</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.31.31">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.31.31.1">LLaMA 2 70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.31.31.2">56.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.31.31.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.32.32">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.32.32.1">RFT 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.32.32.2">55.3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.32.32.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.33.33">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.33.33.1">LLaMA 33B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.33.33.2">53.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.33.33.3">7.1</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.34.34">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.34.34.1">Mistral 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.34.34.2">52.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.34.34.3">13.1</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.35.35">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.35.35.1">RFT 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.35.35.2">51.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.35.35.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.36.36">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.36.36.1">LLaMA 65B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.36.36.2">50.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.36.36.3">20.5</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.37.37">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.37.37.1">Orca 2-7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.37.37.2">47.23</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.37.37.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.38.38">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.38.38.1">Text-davinci-002</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.38.38.2">40.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.38.38.3">19.1</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.39.39">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.39.39.1">LLaMA 33B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.39.39.2">35.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.39.39.3">3.9</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.40.40">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.40.40.1">GPT-Neo-2.7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.40.40.2">19.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.40.40.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.41.41">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.41.41.1">LLaMA 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.41.41.2">18.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.41.41.3">2.9</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.42.42">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.42.42.1">PaLM 540B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.42.42.2">17.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.42.42.3">8.8</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.43.43">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.43.43.1">LLaMA 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.43.43.2">17.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.43.43.3">3.9</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.44.44">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.44.44.1">LLaMA 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.44.44.2">11</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.44.44.3">2.9</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.45.45">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.45.45.1">GPT-Neo-125M</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.45.45.2">7.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.45.45.3">-</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.46.46">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.46.46.1">PaLM 8B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.46.46.2">4.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.46.46.3">1.5</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.47.47">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.47.47.1">GPT-2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.47.47.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.47.47.3">5.4</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.48.48">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.48.48.1">GPT-3 175B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.48.48.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.48.48.3">5.2</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.49.49">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.49.49.1">PaLM 62B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.49.49.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.49.49.3">4.4</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.50.50">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.50.50.1">GPT-3-13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.50.50.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.50.50.3">3</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.51.51">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.51.51.1">LLaMA 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.51.51.2">11</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T9.4.51.51.3">2.9</td>
</tr>
<tr class="ltx_tr" id="S6.T9.4.52.52">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T9.4.52.52.1">PaLM 8B</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T9.4.52.52.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T9.4.52.52.3">1.5</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p7">
<p class="ltx_p" id="S6.SS2.p7.1">Large language models in some cases are hallucinating answers simply because they are next-token prediction machines. Hallucination is one of the important factors in measuring how much a large language model is trustworthy and reliable. Measuring hallucination on the other hand is also not easy as it seems because each fact can be written in different styles and even the smallest changes in writing make it hard to detect. It is fair to assume if any particular LLM is more capable to detect hallucination of false information in text, it is also more trustworthy. HaluEval is one of the datasets that aims to measure hallucination in this field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib205" title="">205</a>]</cite>. Evaluation can also be performed by another model judging the response with regard to the actual answer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib206" title="">206</a>]</cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S6.T10" title="TABLE X ‣ VI-B LLMs’ Performance on Different Tasks ‣ VI Prominent LLMs’ Performance on Benchmarks ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">X</span></a> shows the evaluation of different models based on these datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T10.2.1.1" style="font-size:90%;">TABLE X</span>: </span><span class="ltx_text" id="S6.T10.3.2" style="font-size:90%;">Hallucination evaluation</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T10.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T10.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.1.1.1">Model</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.1.1.2">HHEM</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.1.1.3">HaluEval QA</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.1.1.4">HaluEval Dialogue</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.1.1.5">HaluEval Sum.</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.1.1.6">HaluEval General</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.2.2.1">GPT 4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.2.2.2">97</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.2.2.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.2.2.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.2.2.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.2.2.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.3.3.1">GPT 4 Turbo</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.3.3.2">97</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.3.3.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.3.3.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.3.3.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.3.3.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.4.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.4.4.1">GPT 3.5 Turbo</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.4.4.2">96.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.4.4.3">62.59</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.4.4.4">72.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.4.4.5">58.53</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.4.4.6">79.44</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.5.5.1">Davinci002</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.5.5.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.5.5.3">60.05</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.5.5.4">60.81</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.5.5.5">47.77</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.5.5.6">80.42</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.6.6.1">Davinci003</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.6.6.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.6.6.3">49.65</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.6.6.4">68.37</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.6.6.5">48.07</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.6.6.6">80.4</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.7.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.7.7.1">GPT-3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.7.7.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.7.7.3">49.21</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.7.7.4">50.02</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.7.7.5">51.23</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.7.7.6">72.72</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.8.8.1">Google Gemini Pro</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.8.8.2">95.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.8.8.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.8.8.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.8.8.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.8.8.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.9.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.9.9.1">Llama 2 70B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.9.9.2">94.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.9.9.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.9.9.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.9.9.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.9.9.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.10.10">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.10.10.1">Llama 2 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.10.10.2">94.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.10.10.3">49.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.10.10.4">43.99</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.10.10.5">49.55</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.10.10.6">20.46</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.11.11">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.11.11.1">Llama 2 13B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.11.11.2">94.1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.11.11.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.11.11.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.11.11.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.11.11.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.12.12">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.12.12.1">Cohere-Chat</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.12.12.2">92.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.12.12.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.12.12.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.12.12.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.12.12.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.13.13">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.13.13.1">Cohere</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.13.13.2">91.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.13.13.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.13.13.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.13.13.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.13.13.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.14.14">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.14.14.1">Claude 2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.14.14.2">91.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.14.14.3">69.78</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.14.14.4">64.73</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.14.14.5">57.75</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.14.14.6">75</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.15.15">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.15.15.1">Claude 1</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S6.T10.4.15.15.2"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.15.15.3">67.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.15.15.4">64.83</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.15.15.5">53.76</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.15.15.6">73.88</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.16.16">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.16.16.1">Microsoft Phi 2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.16.16.2">91.5</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.16.16.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.16.16.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.16.16.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.16.16.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.17.17">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.17.17.1">Google Palm 2 (beta)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.17.17.2">91.4</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.17.17.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.17.17.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.17.17.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.17.17.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.18.18">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.18.18.1">Mixtral 8x7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.18.18.2">90.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.18.18.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.18.18.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.18.18.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.18.18.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.19.19">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.19.19.1">Amazon Titan Express</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.19.19.2">90.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.19.19.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.19.19.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.19.19.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.19.19.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.20.20">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.20.20.1">Mistral 7B</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.20.20.2">90.6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.20.20.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.20.20.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.20.20.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.20.20.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.21.21">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.21.21.1">Google Palm 2 Chat (beta)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.21.21.2">90</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.21.21.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.21.21.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.21.21.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.21.21.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.22.22">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.22.22.1">Google Palm 2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.22.22.2">87.9</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.22.22.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.22.22.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.22.22.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.22.22.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.23.23">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.23.23.1">Google Palm 2 Chat</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.23.23.2">72.8</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.23.23.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.23.23.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.23.23.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.23.23.6">-</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.24.24">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.24.24.1">ChatGLM</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.24.24.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.24.24.3">47.93</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.24.24.4">44.41</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.24.24.5">48.57</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.24.24.6">30.92</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.25.25">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.25.25.1">Falcon</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.25.25.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.25.25.3">39.66</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.25.25.4">29.08</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.25.25.5">42.71</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.25.25.6">18.98</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.26.26">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.26.26.1">Vicuna</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.26.26.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.26.26.3">60.34</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.26.26.4">46.35</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.26.26.5">45.62</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T10.4.26.26.6">19.48</td>
</tr>
<tr class="ltx_tr" id="S6.T10.4.27.27">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T10.4.27.27.1">Alpaca</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.4.27.27.2">-</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.4.27.27.3">6.68</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.4.27.27.4">17.55</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.4.27.27.5">20.63</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T10.4.27.27.6">9.54</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Challenges and Future Directions</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">As we have seen in the previous sections, large language models have achieved impressive results in the past 1-2 years. At the same time this is still a new and extremely active research area where the pace of innovation is increasing rather than slowing down. As in any other evolving area though, there are still numerous challenges ahead.
Here we briefly mention some of the challenges and main active areas which are known so far.
It is worth noting that LLM challenges are discussed in details in a work by Kaddour et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib207" title="">207</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.5.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.6.2">Smaller and more efficient Language Models</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">This is a survey on <em class="ltx_emph ltx_font_italic" id="S7.SS1.p1.1.1">large</em> language models, and there has been an initial push towards ”larger is better” that has clearly been rewarded with ever larger models like GPT-4 getting better accuracy and performance in benchmarks. However, those large models are costly and inefficient in several dimensions (e.g. high latency). In response to all of this, there is a current research trend to come up with Small Language Models (SLMs) as a cost-effective alternative to LLMs, particularly when used on specific tasks that might not require the full generality of larger models.
Prominent works in this direction include Phi-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib208" title="">208</a>]</cite>, Phi-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib209" title="">209</a>]</cite>, and Phi-2 from Microsoft.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">More generally, we should expect many research efforts in this area of how to train smaller and more efficient models. Techniques such as parameter-efficient fine-tuning (PEFT), teacher/student, and other forms of distillation – see section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S3.SS9" title="III-I Cost-Effective Training/Inference/Adaptation/Compression ‣ III How LLMs Are Built ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-I</span></span></a> – will continue to be used to build a smaller model out of larger ones.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.5.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.6.2">New Post-attention Architectural Paradigms</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Transformer blocks have been a crucial and constant part of most of current LLM frameworks, and it’s a big question mark how much longer this architecture will be in vogue, and what will be the next big architectural break-through in the field of deep learning (and NLP).
Since AlexNet in 2012, we have seen many architectures go in and out of fashion, including LSTM, GRU, seq2seq, but Transformers have been the dominant approach since its inception. As described earlier, attention is the main mechanism driving transformers. More recently, there has been promising research in alternative approaches that are being labelled as post-attention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">An important class of such class of post-attention models are the so called State Space Models (SSMs). While the notion of State Space Models has a long history in machine learning, it should be noted that in the context of language models, SSM is usually used in reference to the newer Structure State Space Model architecture or S4 for short (see Gu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib29" title="">29</a>]</cite>). Some recent models in this category are Mamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib30" title="">30</a>]</cite>, Hyena <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib210" title="">210</a>]</cite>, and Striped Hyena <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib211" title="">211</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">While all of those models are very competitive in terms of performance in leaderboards and efficiency, they also address an important challenge in more traditional attention-based architectures: <em class="ltx_emph ltx_font_italic" id="S7.SS2.p3.1.1">the lack of support for larger context windows</em>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1">Having a good answer to many prompts requires context. For example, the response to ”Recommend some good movies for me” requires a lot of context about ”me” as well as what movies are available and which ones I have not watched. Context length is especially important for RAG, where large portions of text might be retrieved and injected into the prompt for generation (see section <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4.SS3" title="IV-C Augmenting LLMs through external knowledge - RAG ‣ IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS2.p5">
<p class="ltx_p" id="S7.SS2.p5.1">The longer the context length, the more tokens we can squeeze into the context. The more information the model has access to, the better its response will be. But on the other hand, with very long context, it would be hard for the model to remember everything and efficiently process all the information. Attention-based models are highly inefficient for longer contexts and that is why we should expect more research in different mechanisms that enable processing longer contexts and generally come up with more efficient architectures.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS2.p6">
<p class="ltx_p" id="S7.SS2.p6.1">That being said, new architectures might not only propose alternatives for the attention mechanism but rather rethink the whole Transformer architecture. As an early example of this, Monarch Mixer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib212" title="">212</a>]</cite> proposes a new architecture that uses the same sub-quadratic primitive that achieves high hardware efficiency on GPUs – Monarch matrices – along both sequence length and model dimension.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS2.p7">
<p class="ltx_p" id="S7.SS2.p7.1">On the other end of the spectrum, it is worth mentioning that there are some attention-compatible architectural mechanisms that have been recently gaining steam and proving their value in creating better and more powerful LLMs. Probably the best example of such mechanism is Mixture of Experts (MoE). MoEs have been around in machine learning for years, even before the Deep Learning Era <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib213" title="">213</a>]</cite>, but they have been gaining popularity since then, and particularly in the context of Transformer models and LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS2.p8">
<p class="ltx_p" id="S7.SS2.p8.1">In LLMs, MoEs allow to train an extremely large model than is then only partially instantiated during inference when some of the experts are turned off wherever the gating/weighting function has a low weight assigned to them. As an example, the GLaM model has 1.2 trillion parameters, but during inference only 2 out of the 64 experts are used <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib84" title="">84</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS2.p9">
<p class="ltx_p" id="S7.SS2.p9.1">MoEs are nowadays an important component of the so-called frontier LLMs (i.e. the most advanced and capable models). GPT-4 itself is rumored to be based on a MoE architecture, and some of the best performing LLMs such as Mixtral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib117" title="">117</a>]</cite>, are basically an MoE version of pre-existing LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS2.p10">
<p class="ltx_p" id="S7.SS2.p10.1">Finally, it is important to note that MoEs can be used as a component of any architecture regardless of whether it is based on attention or not. In fact, MoEs have also been applied to SSM-based LLMs like Mamba&nbsp;citepioro2024moemamba. We should continue to see MoE-driven improvements in the future regardless of the underlying architecture.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS3.5.1.1">VII-C</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS3.6.2">Multi-modal Models</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">Future LLMs are expected to be multi-modal and handle a variety of data types, such as text, images, and videos, audio, in a unified manner.
This opens up possibilities for more diverse applications in fields like question answering, content generation, creative arts, and healthcare, robotics, and beyond.
There are already several prominent multi-modal LLMs out there, including: LLAVA
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib214" title="">214</a>]</cite>, LLAVA-Plus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib215" title="">215</a>]</cite>, GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib33" title="">33</a>]</cite>, Qwen-vl <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib116" title="">116</a>]</cite>, Next-GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib216" title="">216</a>]</cite>, but the trend is expected to be continued. Evaluation of these models also is a new research topic, especially conversational generative vision models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib217" title="">217</a>]</cite>.
Multi-modal LLMs can unlock huge potentials in a variety of tasks, and there has already been a descent progress in this direction, which needs a dedicated paper to discuss all its details.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS4.5.1.1">VII-D</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS4.6.2">Improved LLM Usage and Augmentation techniques</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1">As we described in section<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#S4" title="IV How LLMs Are Used and Augmented ‣ Large Language Models: A Survey"><span class="ltx_text ltx_ref_tag">IV</span></a>, many of the shortcomings and limitations of LLMs such as <em class="ltx_emph ltx_font_italic" id="S7.SS4.p1.1.1">hallucination</em> can be addressed through advanced prompt engineering, use of tools, or other augmentation techniques. We should expect not only continued, but accelerated research in this area.
It is worth mentioning that, in the specific case of software engineering, some works (<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib218" title="">218</a>]</cite>) tried to automatically eliminate this issue from the overall software engineering workflow</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS4.p2">
<p class="ltx_p" id="S7.SS4.p2.1">LLM-based systems are already starting to replace machine learning systems that were until recently using other approaches. As a clear example of this, LLMs are now being deployed to better understand people preference and interests, and provide more personalized interactions, whether in customer service, content recommendation, or other applications. This involves better understanding of user preferences, and analyzing their past interactions and using them as the context. We will continue to see research in the application and usage of LLMs for not only <em class="ltx_emph ltx_font_italic" id="S7.SS4.p2.1.1">personalization and recommendations</em>, but many other application areas using other machine learning techniques.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS4.p3">
<p class="ltx_p" id="S7.SS4.p3.1">Finally, another important area of research we expect to gather increased attention is that of <em class="ltx_emph ltx_font_italic" id="S7.SS4.p3.1.1">LLM-based agents and multi-agent systems</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib172" title="">172</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib173" title="">173</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib174" title="">174</a>]</cite>. The development of LLM systems with access to external tools and decision-making capabilities is both exciting and challenging. We will see continued research and progress in this important area that some argue could lead to Artificial General Intelligence (AGI).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S7.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS5.5.1.1">VII-E</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS5.6.2">Security and Ethical/Responsible AI</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.SS5.p1">
<p class="ltx_p" id="S7.SS5.p1.1">Ensuring the robustness and security of LLMs against adversarial attacks and other vulnerabilities is a critical area of research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib219" title="">219</a>]</cite>. As LLMs are increasingly deployed in real-world applications, they need to be protected from potential threats, to prevent them being used to manipulate people or spread mis-information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.SS5.p2">
<p class="ltx_p" id="S7.SS5.p2.1">Addressing ethical concerns and biases in LLMs is another active area of research. Efforts are being made to ensure that LLMs are fair, unbiased, and capable of handling sensitive information responsibly. As LLMs are being used more and more by a large number of people on a daily basis, making sure they are unbiased and behave responsibly is crucial.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusion</span>
</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">This paper present a survey of LLMs developed in the past few years.
We first provide an overview of early pre-trained language models (e.g., as BERT), then review three popular LLM families (GPT, LLaMA, PaLM), and other representative LLMs.
We then survey methods and techniques of building, augmenting, and using LLMs.
We review popular LLM datasets and benchmarks, and compare performance of a set of prominent models on public benchmarks.
Finally, we present open challenges and future research directions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Kaplan, S.&nbsp;McCandlish, T.&nbsp;Henighan, T.&nbsp;B. Brown, B.&nbsp;Chess, R.&nbsp;Child, S.&nbsp;Gray, A.&nbsp;Radford, J.&nbsp;Wu, and D.&nbsp;Amodei, “Scaling laws for neural language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Hoffmann, S.&nbsp;Borgeaud, A.&nbsp;Mensch, E.&nbsp;Buchatskaya, T.&nbsp;Cai, E.&nbsp;Rutherford, D.&nbsp;d.&nbsp;L. Casas, L.&nbsp;A. Hendricks, J.&nbsp;Welbl, A.&nbsp;Clark <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et&nbsp;al.</em>, “Training compute-optimal large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">arXiv preprint arXiv:2203.15556</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;E. Shannon, “Prediction and entropy of printed english,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Bell system technical journal</em>, vol.&nbsp;30, no.&nbsp;1, pp. 50–64, 1951.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Jelinek, <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Statistical methods for speech recognition</em>.&nbsp;&nbsp;&nbsp;MIT press, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Manning and H.&nbsp;Schutze, <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Foundations of statistical natural language processing</em>.&nbsp;&nbsp;&nbsp;MIT press, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;D. Manning, <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">An introduction to information retrieval</em>.&nbsp;&nbsp;&nbsp;Cambridge university press, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;X. Zhao, K.&nbsp;Zhou, J.&nbsp;Li, T.&nbsp;Tang, X.&nbsp;Wang, Y.&nbsp;Hou, Y.&nbsp;Min, B.&nbsp;Zhang, J.&nbsp;Zhang, Z.&nbsp;Dong <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et&nbsp;al.</em>, “A survey of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">arXiv preprint arXiv:2303.18223</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Zhou, Q.&nbsp;Li, C.&nbsp;Li, J.&nbsp;Yu, Y.&nbsp;Liu, G.&nbsp;Wang, K.&nbsp;Zhang, C.&nbsp;Ji, Q.&nbsp;Yan, L.&nbsp;He <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et&nbsp;al.</em>, “A comprehensive survey on pretrained foundation models: A history from bert to chatgpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">arXiv preprint arXiv:2302.09419</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Liu, W.&nbsp;Yuan, J.&nbsp;Fu, Z.&nbsp;Jiang, H.&nbsp;Hayashi, and G.&nbsp;Neubig, “Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">ACM Computing Surveys</em>, vol.&nbsp;55, no.&nbsp;9, pp. 1–35, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Q.&nbsp;Dong, L.&nbsp;Li, D.&nbsp;Dai, C.&nbsp;Zheng, Z.&nbsp;Wu, B.&nbsp;Chang, X.&nbsp;Sun, J.&nbsp;Xu, and Z.&nbsp;Sui, “A survey for in-context learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2301.00234</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Huang and K.&nbsp;C.-C. Chang, “Towards reasoning in large language models: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2212.10403</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;F. Chen and J.&nbsp;Goodman, “An empirical study of smoothing techniques for language modeling,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Computer Speech &amp; Language</em>, vol.&nbsp;13, no.&nbsp;4, pp. 359–394, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Bengio, R.&nbsp;Ducharme, and P.&nbsp;Vincent, “A neural probabilistic language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Advances in neural information processing systems</em>, vol.&nbsp;13, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Schwenk, D.&nbsp;Déchelotte, and J.-L. Gauvain, “Continuous space language models for statistical machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions</em>, 2006, pp. 723–730.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Mikolov, M.&nbsp;Karafiát, L.&nbsp;Burget, J.&nbsp;Cernockỳ, and S.&nbsp;Khudanpur, “Recurrent neural network based language model.” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Interspeech</em>, vol.&nbsp;2, no.&nbsp;3.&nbsp;&nbsp;&nbsp;Makuhari, 2010, pp. 1045–1048.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Graves, “Generating sequences with recurrent neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1308.0850</em>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.-S. Huang, X.&nbsp;He, J.&nbsp;Gao, L.&nbsp;Deng, A.&nbsp;Acero, and L.&nbsp;Heck, “Learning deep structured semantic models for web search using clickthrough data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</em>, 2013, pp. 2333–2338.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Gao, C.&nbsp;Xiong, P.&nbsp;Bennett, and N.&nbsp;Craswell, <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Neural Approaches to Conversational Information Retrieval</em>.&nbsp;&nbsp;&nbsp;Springer Nature, 2023, vol.&nbsp;44.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Sutskever, O.&nbsp;Vinyals, and Q.&nbsp;V. Le, “Sequence to sequence learning with neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in neural information processing systems</em>, vol.&nbsp;27, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Cho, B.&nbsp;Van&nbsp;Merriënboer, D.&nbsp;Bahdanau, and Y.&nbsp;Bengio, “On the properties of neural machine translation: Encoder-decoder approaches,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1409.1259</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Fang, S.&nbsp;Gupta, F.&nbsp;Iandola, R.&nbsp;K. Srivastava, L.&nbsp;Deng, P.&nbsp;Dollár, J.&nbsp;Gao, X.&nbsp;He, M.&nbsp;Mitchell, J.&nbsp;C. Platt <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et&nbsp;al.</em>, “From captions to visual concepts and back,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2015, pp. 1473–1482.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Vinyals, A.&nbsp;Toshev, S.&nbsp;Bengio, and D.&nbsp;Erhan, “Show and tell: A neural image caption generator,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2015, pp. 3156–3164.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;E. Peters, M.&nbsp;Neumann, M.&nbsp;Iyyer, M.&nbsp;Gardner, C.&nbsp;Clark, K.&nbsp;Lee, and L.&nbsp;Zettlemoyer, “Deep contextualized word representations. corr abs/1802.05365 (2018),” <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:1802.05365</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Devlin, M.-W. Chang, K.&nbsp;Lee, and K.&nbsp;Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Liu, M.&nbsp;Ott, N.&nbsp;Goyal, J.&nbsp;Du, M.&nbsp;Joshi, D.&nbsp;Chen, O.&nbsp;Levy, M.&nbsp;Lewis, L.&nbsp;Zettlemoyer, and V.&nbsp;Stoyanov, “Roberta: A robustly optimized bert pretraining approach,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;He, X.&nbsp;Liu, J.&nbsp;Gao, and W.&nbsp;Chen, “Deberta: Decoding-enhanced bert with disentangled attention,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2006.03654</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Han, Z.&nbsp;Zhang, N.&nbsp;Ding, Y.&nbsp;Gu, X.&nbsp;Liu, Y.&nbsp;Huo, J.&nbsp;Qiu, Y.&nbsp;Yao, A.&nbsp;Zhang, L.&nbsp;Zhang <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">et&nbsp;al.</em>, “Pre-trained models: Past, present and future,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">AI Open</em>, vol.&nbsp;2, pp. 225–250, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Qiu, T.&nbsp;Sun, Y.&nbsp;Xu, Y.&nbsp;Shao, N.&nbsp;Dai, and X.&nbsp;Huang, “Pre-trained models for natural language processing: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Science China Technological Sciences</em>, vol.&nbsp;63, no.&nbsp;10, pp. 1872–1897, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Gu, K.&nbsp;Goel, and C.&nbsp;Ré, “Efficiently modeling long sequences with structured state spaces,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Gu and T.&nbsp;Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2312.00752</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Chowdhery, S.&nbsp;Narang, J.&nbsp;Devlin, M.&nbsp;Bosma, G.&nbsp;Mishra, A.&nbsp;Roberts, P.&nbsp;Barham, H.&nbsp;W. Chung, C.&nbsp;Sutton, S.&nbsp;Gehrmann <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">et&nbsp;al.</em>, “Palm: Scaling language modeling with pathways,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.2.2">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, T.&nbsp;Lavril, G.&nbsp;Izacard, X.&nbsp;Martinet, M.-A. Lachaux, T.&nbsp;Lacroix, B.&nbsp;Rozière, N.&nbsp;Goyal, E.&nbsp;Hambro, F.&nbsp;Azhar <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">et&nbsp;al.</em>, “Llama: Open and efficient foundation language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI, “GPT-4 Technical Report,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2303.08774v3.pdf" title="">https://arxiv.org/pdf/2303.08774v3.pdf</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, X.&nbsp;Wang, D.&nbsp;Schuurmans, M.&nbsp;Bosma, b.&nbsp;ichter, F.&nbsp;Xia, E.&nbsp;Chi, Q.&nbsp;V. Le, and D.&nbsp;Zhou, “Chain-of-thought prompting elicits reasoning in large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Advances in Neural Information Processing Systems</em>, S.&nbsp;Koyejo, S.&nbsp;Mohamed, A.&nbsp;Agarwal, D.&nbsp;Belgrave, K.&nbsp;Cho, and A.&nbsp;Oh, Eds., vol.&nbsp;35.&nbsp;&nbsp;&nbsp;Curran Associates, Inc., 2022, pp. 24 824–24 837. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Mialon, R.&nbsp;Dessì, M.&nbsp;Lomeli, C.&nbsp;Nalmpantis, R.&nbsp;Pasunuru, R.&nbsp;Raileanu, B.&nbsp;Rozière, T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, A.&nbsp;Celikyilmaz <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">et&nbsp;al.</em>, “Augmented language models: a survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.2.2">arXiv preprint arXiv:2302.07842</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Peng, M.&nbsp;Galley, P.&nbsp;He, H.&nbsp;Cheng, Y.&nbsp;Xie, Y.&nbsp;Hu, Q.&nbsp;Huang, L.&nbsp;Liden, Z.&nbsp;Yu, W.&nbsp;Chen, and J.&nbsp;Gao, “Check your facts and try again: Improving large language models with external knowledge and automated feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2302.12813</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Yao, J.&nbsp;Zhao, D.&nbsp;Yu, N.&nbsp;Du, I.&nbsp;Shafran, K.&nbsp;Narasimhan, and Y.&nbsp;Cao, “React: Synergizing reasoning and acting in language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2210.03629</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;E. Rumelhart, G.&nbsp;E. Hinton, R.&nbsp;J. Williams <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">et&nbsp;al.</em>, “Learning internal representations by error propagation,” 1985.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;L. Elman, “Finding structure in time,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Cognitive science</em>, vol.&nbsp;14, no.&nbsp;2, pp. 179–211, 1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;V. Mahoney, “Fast text compression with neural networks.” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">FLAIRS conference</em>, 2000, pp. 230–234.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Mikolov, A.&nbsp;Deoras, D.&nbsp;Povey, L.&nbsp;Burget, and J.&nbsp;Černockỳ, “Strategies for training large scale neural network language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">2011 IEEE Workshop on Automatic Speech Recognition &amp; Understanding</em>.&nbsp;&nbsp;&nbsp;IEEE, 2011, pp. 196–201.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
tmikolov. rnnlm. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.fit.vutbr.cz/~imikolov/rnnlm/" title="">https://www.fit.vutbr.cz/~imikolov/rnnlm/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Minaee, N.&nbsp;Kalchbrenner, E.&nbsp;Cambria, N.&nbsp;Nikzad, M.&nbsp;Chenaghlu, and J.&nbsp;Gao, “Deep learning–based text classification: a comprehensive review,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">ACM computing surveys (CSUR)</em>, vol.&nbsp;54, no.&nbsp;3, pp. 1–40, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Vaswani, N.&nbsp;Shazeer, N.&nbsp;Parmar, J.&nbsp;Uszkoreit, L.&nbsp;Jones, A.&nbsp;N. Gomez, <span class="ltx_text ltx_font_caligraphic" id="bib.bib44.1.1">L</span>.&nbsp;Kaiser, and I.&nbsp;Polosukhin, “Attention is all you need,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.2.2">Advances in neural information processing systems</em>, vol.&nbsp;30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Lan, M.&nbsp;Chen, S.&nbsp;Goodman, K.&nbsp;Gimpel, P.&nbsp;Sharma, and R.&nbsp;Soricut, “Albert: A lite bert for self-supervised learning of language representations,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:1909.11942</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Clark, M.-T. Luong, Q.&nbsp;V. Le, and C.&nbsp;D. Manning, “Electra: Pre-training text encoders as discriminators rather than generators,” <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2003.10555</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Lample and A.&nbsp;Conneau, “Cross-lingual language model pretraining,” <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:1901.07291</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Yang, Z.&nbsp;Dai, Y.&nbsp;Yang, J.&nbsp;Carbonell, R.&nbsp;R. Salakhutdinov, and Q.&nbsp;V. Le, “Xlnet: Generalized autoregressive pretraining for language understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Advances in neural information processing systems</em>, vol.&nbsp;32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Dong, N.&nbsp;Yang, W.&nbsp;Wang, F.&nbsp;Wei, X.&nbsp;Liu, Y.&nbsp;Wang, J.&nbsp;Gao, M.&nbsp;Zhou, and H.-W. Hon, “Unified language model pre-training for natural language understanding and generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Advances in neural information processing systems</em>, vol.&nbsp;32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Radford, K.&nbsp;Narasimhan, T.&nbsp;Salimans, I.&nbsp;Sutskever <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">et&nbsp;al.</em>, “Improving language understanding by generative pre-training,” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Radford, J.&nbsp;Wu, R.&nbsp;Child, D.&nbsp;Luan, D.&nbsp;Amodei, I.&nbsp;Sutskever <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">et&nbsp;al.</em>, “Language models are unsupervised multitask learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib51.2.2">OpenAI blog</em>, vol.&nbsp;1, no.&nbsp;8, p.&nbsp;9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Raffel, N.&nbsp;Shazeer, A.&nbsp;Roberts, K.&nbsp;Lee, S.&nbsp;Narang, M.&nbsp;Matena, Y.&nbsp;Zhou, W.&nbsp;Li, and P.&nbsp;J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">The Journal of Machine Learning Research</em>, vol.&nbsp;21, no.&nbsp;1, pp. 5485–5551, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Xue, N.&nbsp;Constant, A.&nbsp;Roberts, M.&nbsp;Kale, R.&nbsp;Al-Rfou, A.&nbsp;Siddhant, A.&nbsp;Barua, and C.&nbsp;Raffel, “mt5: A massively multilingual pre-trained text-to-text transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2010.11934</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Song, X.&nbsp;Tan, T.&nbsp;Qin, J.&nbsp;Lu, and T.-Y. Liu, “Mass: Masked sequence to sequence pre-training for language generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:1905.02450</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Lewis, Y.&nbsp;Liu, N.&nbsp;Goyal, M.&nbsp;Ghazvininejad, A.&nbsp;Mohamed, O.&nbsp;Levy, V.&nbsp;Stoyanov, and L.&nbsp;Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:1910.13461</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Brown, B.&nbsp;Mann, N.&nbsp;Ryder, M.&nbsp;Subbiah, J.&nbsp;D. Kaplan, P.&nbsp;Dhariwal, A.&nbsp;Neelakantan, P.&nbsp;Shyam, G.&nbsp;Sastry, A.&nbsp;Askell <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">et&nbsp;al.</em>, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib56.2.2">Advances in neural information processing systems</em>, vol.&nbsp;33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Chen, J.&nbsp;Tworek, H.&nbsp;Jun, Q.&nbsp;Yuan, H.&nbsp;P. d.&nbsp;O. Pinto, J.&nbsp;Kaplan, H.&nbsp;Edwards, Y.&nbsp;Burda, N.&nbsp;Joseph, G.&nbsp;Brockman <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">et&nbsp;al.</em>, “Evaluating large language models trained on code,” <em class="ltx_emph ltx_font_italic" id="bib.bib57.2.2">arXiv preprint arXiv:2107.03374</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Nakano, J.&nbsp;Hilton, S.&nbsp;Balaji, J.&nbsp;Wu, L.&nbsp;Ouyang, C.&nbsp;Kim, C.&nbsp;Hesse, S.&nbsp;Jain, V.&nbsp;Kosaraju, W.&nbsp;Saunders <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">et&nbsp;al.</em>, “Webgpt: Browser-assisted question-answering with human feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib58.2.2">arXiv preprint arXiv:2112.09332</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Ouyang, J.&nbsp;Wu, X.&nbsp;Jiang, D.&nbsp;Almeida, C.&nbsp;Wainwright, P.&nbsp;Mishkin, C.&nbsp;Zhang, S.&nbsp;Agarwal, K.&nbsp;Slama, A.&nbsp;Ray <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">et&nbsp;al.</em>, “Training language models to follow instructions with human feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib59.2.2">Advances in Neural Information Processing Systems</em>, vol.&nbsp;35, pp. 27 730–27 744, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI. (2022) Introducing chatgpt. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Touvron, L.&nbsp;Martin, K.&nbsp;Stone, P.&nbsp;Albert, A.&nbsp;Almahairi, Y.&nbsp;Babaei, N.&nbsp;Bashlykov, S.&nbsp;Batra, P.&nbsp;Bhargava, S.&nbsp;Bhosale <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">et&nbsp;al.</em>, “Llama 2: Open foundation and fine-tuned chat models,” <em class="ltx_emph ltx_font_italic" id="bib.bib61.2.2">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Taori, I.&nbsp;Gulrajani, T.&nbsp;Zhang, Y.&nbsp;Dubois, X.&nbsp;Li, C.&nbsp;Guestrin, P.&nbsp;Liang, and T.&nbsp;B. Hashimoto, “Alpaca: A strong, replicable instruction-following model,” <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html</em>, vol.&nbsp;3, no.&nbsp;6, p.&nbsp;7, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Dettmers, A.&nbsp;Pagnoni, A.&nbsp;Holtzman, and L.&nbsp;Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2305.14314</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Geng, A.&nbsp;Gudibande, H.&nbsp;Liu, E.&nbsp;Wallace, P.&nbsp;Abbeel, S.&nbsp;Levine, and D.&nbsp;Song, “Koala: A dialogue model for academic research,” <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Blog post, April</em>, vol.&nbsp;1, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Q. Jiang, A.&nbsp;Sablayrolles, A.&nbsp;Mensch, C.&nbsp;Bamford, D.&nbsp;S. Chaplot, D.&nbsp;d.&nbsp;l. Casas, F.&nbsp;Bressand, G.&nbsp;Lengyel, G.&nbsp;Lample, L.&nbsp;Saulnier <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">et&nbsp;al.</em>, “Mistral 7b,” <em class="ltx_emph ltx_font_italic" id="bib.bib65.2.2">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Roziere, J.&nbsp;Gehring, F.&nbsp;Gloeckle, S.&nbsp;Sootla, I.&nbsp;Gat, X.&nbsp;E. Tan, Y.&nbsp;Adi, J.&nbsp;Liu, T.&nbsp;Remez, J.&nbsp;Rapin <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">et&nbsp;al.</em>, “Code llama: Open foundation models for code,” <em class="ltx_emph ltx_font_italic" id="bib.bib66.2.2">arXiv preprint arXiv:2308.12950</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;G. Patil, T.&nbsp;Zhang, X.&nbsp;Wang, and J.&nbsp;E. Gonzalez, “Gorilla: Large language model connected with massive apis,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Pal, D.&nbsp;Karkhanis, M.&nbsp;Roberts, S.&nbsp;Dooley, A.&nbsp;Sundararajan, and S.&nbsp;Naidu, “Giraffe: Adventures in expanding context lengths in llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2308.10882</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Huang, “Vigogne: French instruction-following and chat models,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/bofenghuang/vigogne" title="">https://github.com/bofenghuang/vigogne</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, H.&nbsp;Ivison, P.&nbsp;Dasigi, J.&nbsp;Hessel, T.&nbsp;Khot, K.&nbsp;R. Chandu, D.&nbsp;Wadden, K.&nbsp;MacMillan, N.&nbsp;A. Smith, I.&nbsp;Beltagy <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">et&nbsp;al.</em>, “How far can camels go? exploring the state of instruction tuning on open resources,” <em class="ltx_emph ltx_font_italic" id="bib.bib70.2.2">arXiv preprint arXiv:2306.04751</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Tworkowski, K.&nbsp;Staniszewski, M.&nbsp;Pacek, Y.&nbsp;Wu, H.&nbsp;Michalewski, and P.&nbsp;Miłoś, “Focused transformer: Contrastive training for context scaling,” <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2307.03170</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Mahan, R.&nbsp;Carlow, L.&nbsp;Castricato, N.&nbsp;Cooper, and C.&nbsp;Laforte, “Stable beluga models.” [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="%5Bhttps://huggingface.co/stabilityai/StableBeluga2%5D(https://huggingface.co/stabilityai/StableBeluga2)" title="">[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Tay, J.&nbsp;Wei, H.&nbsp;W. Chung, V.&nbsp;Q. Tran, D.&nbsp;R. So, S.&nbsp;Shakeri, X.&nbsp;Garcia, H.&nbsp;S. Zheng, J.&nbsp;Rao, A.&nbsp;Chowdhery <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">et&nbsp;al.</em>, “Transcending scaling laws with 0.1% extra compute,” <em class="ltx_emph ltx_font_italic" id="bib.bib73.2.2">arXiv preprint arXiv:2210.11399</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;W. Chung, L.&nbsp;Hou, S.&nbsp;Longpre, B.&nbsp;Zoph, Y.&nbsp;Tay, W.&nbsp;Fedus, Y.&nbsp;Li, X.&nbsp;Wang, M.&nbsp;Dehghani, S.&nbsp;Brahma <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">et&nbsp;al.</em>, “Scaling instruction-finetuned language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib74.2.2">arXiv preprint arXiv:2210.11416</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Anil, A.&nbsp;M. Dai, O.&nbsp;Firat, M.&nbsp;Johnson, D.&nbsp;Lepikhin, A.&nbsp;Passos, S.&nbsp;Shakeri, E.&nbsp;Taropa, P.&nbsp;Bailey, Z.&nbsp;Chen <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">et&nbsp;al.</em>, “Palm 2 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib75.2.2">arXiv preprint arXiv:2305.10403</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Singhal, S.&nbsp;Azizi, T.&nbsp;Tu, S.&nbsp;S. Mahdavi, J.&nbsp;Wei, H.&nbsp;W. Chung, N.&nbsp;Scales, A.&nbsp;Tanwani, H.&nbsp;Cole-Lewis, S.&nbsp;Pfohl <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">et&nbsp;al.</em>, “Large language models encode clinical knowledge,” <em class="ltx_emph ltx_font_italic" id="bib.bib76.2.2">arXiv preprint arXiv:2212.13138</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Singhal, T.&nbsp;Tu, J.&nbsp;Gottweis, R.&nbsp;Sayres, E.&nbsp;Wulczyn, L.&nbsp;Hou, K.&nbsp;Clark, S.&nbsp;Pfohl, H.&nbsp;Cole-Lewis, D.&nbsp;Neal <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">et&nbsp;al.</em>, “Towards expert-level medical question answering with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib77.2.2">arXiv preprint arXiv:2305.09617</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Wei, M.&nbsp;Bosma, V.&nbsp;Y. Zhao, K.&nbsp;Guu, A.&nbsp;W. Yu, B.&nbsp;Lester, N.&nbsp;Du, A.&nbsp;M. Dai, and Q.&nbsp;V. Le, “Finetuned language models are zero-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2109.01652</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;W. Rae, S.&nbsp;Borgeaud, T.&nbsp;Cai, K.&nbsp;Millican, J.&nbsp;Hoffmann, F.&nbsp;Song, J.&nbsp;Aslanides, S.&nbsp;Henderson, R.&nbsp;Ring, S.&nbsp;Young <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">et&nbsp;al.</em>, “Scaling language models: Methods, analysis &amp; insights from training gopher,” <em class="ltx_emph ltx_font_italic" id="bib.bib79.2.2">arXiv preprint arXiv:2112.11446</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Sanh, A.&nbsp;Webson, C.&nbsp;Raffel, S.&nbsp;H. Bach, L.&nbsp;Sutawika, Z.&nbsp;Alyafeai, A.&nbsp;Chaffin, A.&nbsp;Stiegler, T.&nbsp;L. Scao, A.&nbsp;Raja <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">et&nbsp;al.</em>, “Multitask prompted training enables zero-shot task generalization,” <em class="ltx_emph ltx_font_italic" id="bib.bib80.2.2">arXiv preprint arXiv:2110.08207</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Sun, S.&nbsp;Wang, S.&nbsp;Feng, S.&nbsp;Ding, C.&nbsp;Pang, J.&nbsp;Shang, J.&nbsp;Liu, X.&nbsp;Chen, Y.&nbsp;Zhao, Y.&nbsp;Lu <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">et&nbsp;al.</em>, “Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib81.2.2">arXiv preprint arXiv:2107.02137</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Borgeaud, A.&nbsp;Mensch, J.&nbsp;Hoffmann, T.&nbsp;Cai, E.&nbsp;Rutherford, K.&nbsp;Millican, G.&nbsp;B. Van Den&nbsp;Driessche, J.-B. Lespiau, B.&nbsp;Damoc, A.&nbsp;Clark <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">et&nbsp;al.</em>, “Improving language models by retrieving from trillions of tokens,” in <em class="ltx_emph ltx_font_italic" id="bib.bib82.2.2">International conference on machine learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2022, pp. 2206–2240.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Lieber, O.&nbsp;Sharir, B.&nbsp;Lenz, and Y.&nbsp;Shoham, “Jurassic-1: Technical details and evaluation,” <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">White Paper. AI21 Labs</em>, vol.&nbsp;1, p.&nbsp;9, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Du, Y.&nbsp;Huang, A.&nbsp;M. Dai, S.&nbsp;Tong, D.&nbsp;Lepikhin, Y.&nbsp;Xu, M.&nbsp;Krikun, Y.&nbsp;Zhou, A.&nbsp;W. Yu, O.&nbsp;Firat <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">et&nbsp;al.</em>, “Glam: Efficient scaling of language models with mixture-of-experts,” in <em class="ltx_emph ltx_font_italic" id="bib.bib84.2.2">International Conference on Machine Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2022, pp. 5547–5569.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Thoppilan, D.&nbsp;De&nbsp;Freitas, J.&nbsp;Hall, N.&nbsp;Shazeer, A.&nbsp;Kulshreshtha, H.-T. Cheng, A.&nbsp;Jin, T.&nbsp;Bos, L.&nbsp;Baker, Y.&nbsp;Du <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">et&nbsp;al.</em>, “Lamda: Language models for dialog applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib85.2.2">arXiv preprint arXiv:2201.08239</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, S.&nbsp;Roller, N.&nbsp;Goyal, M.&nbsp;Artetxe, M.&nbsp;Chen, S.&nbsp;Chen, C.&nbsp;Dewan, M.&nbsp;Diab, X.&nbsp;Li, X.&nbsp;V. Lin <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">et&nbsp;al.</em>, “Opt: Open pre-trained transformer language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib86.2.2">arXiv preprint arXiv:2205.01068</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Taylor, M.&nbsp;Kardas, G.&nbsp;Cucurull, T.&nbsp;Scialom, A.&nbsp;Hartshorn, E.&nbsp;Saravia, A.&nbsp;Poulton, V.&nbsp;Kerkez, and R.&nbsp;Stojnic, “Galactica: A large language model for science,” <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">arXiv preprint arXiv:2211.09085</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Nijkamp, B.&nbsp;Pang, H.&nbsp;Hayashi, L.&nbsp;Tu, H.&nbsp;Wang, Y.&nbsp;Zhou, S.&nbsp;Savarese, and C.&nbsp;Xiong, “Codegen: An open large language model for code with multi-turn program synthesis,” <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2203.13474</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Soltan, S.&nbsp;Ananthakrishnan, J.&nbsp;FitzGerald, R.&nbsp;Gupta, W.&nbsp;Hamza, H.&nbsp;Khan, C.&nbsp;Peris, S.&nbsp;Rawls, A.&nbsp;Rosenbaum, A.&nbsp;Rumshisky <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">et&nbsp;al.</em>, “Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model,” <em class="ltx_emph ltx_font_italic" id="bib.bib89.2.2">arXiv preprint arXiv:2208.01448</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Glaese, N.&nbsp;McAleese, M.&nbsp;Trebacz, J.&nbsp;Aslanides, V.&nbsp;Firoiu, T.&nbsp;Ewalds, M.&nbsp;Rauh, L.&nbsp;Weidinger, M.&nbsp;Chadwick, P.&nbsp;Thacker <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">et&nbsp;al.</em>, “Improving alignment of dialogue agents via targeted human judgements,” <em class="ltx_emph ltx_font_italic" id="bib.bib90.2.2">arXiv preprint arXiv:2209.14375</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Lewkowycz, A.&nbsp;Andreassen, D.&nbsp;Dohan, E.&nbsp;Dyer, H.&nbsp;Michalewski, V.&nbsp;Ramasesh, A.&nbsp;Slone, C.&nbsp;Anil, I.&nbsp;Schlag, T.&nbsp;Gutman-Solo <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">et&nbsp;al.</em>, “Solving quantitative reasoning problems with language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib91.2.2">Advances in Neural Information Processing Systems</em>, vol.&nbsp;35, pp. 3843–3857, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Tay, M.&nbsp;Dehghani, V.&nbsp;Q. Tran, X.&nbsp;Garcia, D.&nbsp;Bahri, T.&nbsp;Schuster, H.&nbsp;S. Zheng, N.&nbsp;Houlsby, and D.&nbsp;Metzler, “Unifying language learning paradigms,” <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">arXiv preprint arXiv:2205.05131</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;L. Scao, A.&nbsp;Fan, C.&nbsp;Akiki, E.&nbsp;Pavlick, S.&nbsp;Ilić, D.&nbsp;Hesslow, R.&nbsp;Castagné, A.&nbsp;S. Luccioni, F.&nbsp;Yvon, M.&nbsp;Gallé <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">et&nbsp;al.</em>, “Bloom: A 176b-parameter open-access multilingual language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib93.2.2">arXiv preprint arXiv:2211.05100</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Zeng, X.&nbsp;Liu, Z.&nbsp;Du, Z.&nbsp;Wang, H.&nbsp;Lai, M.&nbsp;Ding, Z.&nbsp;Yang, Y.&nbsp;Xu, W.&nbsp;Zheng, X.&nbsp;Xia <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">et&nbsp;al.</em>, “Glm-130b: An open bilingual pre-trained model,” <em class="ltx_emph ltx_font_italic" id="bib.bib94.2.2">arXiv preprint arXiv:2210.02414</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Biderman, H.&nbsp;Schoelkopf, Q.&nbsp;G. Anthony, H.&nbsp;Bradley, K.&nbsp;O’Brien, E.&nbsp;Hallahan, M.&nbsp;A. Khan, S.&nbsp;Purohit, U.&nbsp;S. Prashanth, E.&nbsp;Raff <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">et&nbsp;al.</em>, “Pythia: A suite for analyzing large language models across training and scaling,” in <em class="ltx_emph ltx_font_italic" id="bib.bib95.2.2">International Conference on Machine Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2023, pp. 2397–2430.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Mukherjee, A.&nbsp;Mitra, G.&nbsp;Jawahar, S.&nbsp;Agarwal, H.&nbsp;Palangi, and A.&nbsp;Awadallah, “Orca: Progressive learning from complex explanation traces of gpt-4,” <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">arXiv preprint arXiv:2306.02707</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Li, L.&nbsp;B. Allal, Y.&nbsp;Zi, N.&nbsp;Muennighoff, D.&nbsp;Kocetkov, C.&nbsp;Mou, M.&nbsp;Marone, C.&nbsp;Akiki, J.&nbsp;Li, J.&nbsp;Chim <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">et&nbsp;al.</em>, “Starcoder: may the source be with you!” <em class="ltx_emph ltx_font_italic" id="bib.bib97.2.2">arXiv preprint arXiv:2305.06161</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Huang, L.&nbsp;Dong, W.&nbsp;Wang, Y.&nbsp;Hao, S.&nbsp;Singhal, S.&nbsp;Ma, T.&nbsp;Lv, L.&nbsp;Cui, O.&nbsp;K. Mohammed, Q.&nbsp;Liu <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">et&nbsp;al.</em>, “Language is not all you need: Aligning perception with language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib98.2.2">arXiv preprint arXiv:2302.14045</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Team, R.&nbsp;Anil, S.&nbsp;Borgeaud, Y.&nbsp;Wu, J.-B. Alayrac, J.&nbsp;Yu, R.&nbsp;Soricut, J.&nbsp;Schalkwyk, A.&nbsp;M. Dai, A.&nbsp;Hauth <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">et&nbsp;al.</em>, “Gemini: a family of highly capable multimodal models,” <em class="ltx_emph ltx_font_italic" id="bib.bib99.2.2">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Huang, F.&nbsp;Xia, T.&nbsp;Xiao, H.&nbsp;Chan, J.&nbsp;Liang, P.&nbsp;Florence, A.&nbsp;Zeng, J.&nbsp;Tompson, I.&nbsp;Mordatch, Y.&nbsp;Chebotar <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">et&nbsp;al.</em>, “Inner monologue: Embodied reasoning through planning with language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib100.2.2">arXiv preprint arXiv:2207.05608</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Smith, M.&nbsp;Patwary, B.&nbsp;Norick, P.&nbsp;LeGresley, S.&nbsp;Rajbhandari, J.&nbsp;Casper, Z.&nbsp;Liu, S.&nbsp;Prabhumoye, G.&nbsp;Zerveas, V.&nbsp;Korthikanti <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">et&nbsp;al.</em>, “Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib101.2.2">arXiv preprint arXiv:2201.11990</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
I.&nbsp;Beltagy, M.&nbsp;E. Peters, and A.&nbsp;Cohan, “Longformer: The long-document transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">arXiv preprint arXiv:2004.05150</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Iyer, X.&nbsp;V. Lin, R.&nbsp;Pasunuru, T.&nbsp;Mihaylov, D.&nbsp;Simig, P.&nbsp;Yu, K.&nbsp;Shuster, T.&nbsp;Wang, Q.&nbsp;Liu, P.&nbsp;S. Koura <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">et&nbsp;al.</em>, “Opt-iml: Scaling language model instruction meta learning through the lens of generalization,” <em class="ltx_emph ltx_font_italic" id="bib.bib103.2.2">arXiv preprint arXiv:2212.12017</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Hao, H.&nbsp;Song, L.&nbsp;Dong, S.&nbsp;Huang, Z.&nbsp;Chi, W.&nbsp;Wang, S.&nbsp;Ma, and F.&nbsp;Wei, “Language models are general-purpose interfaces,” <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">arXiv preprint arXiv:2206.06336</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Sun, Y.&nbsp;Shen, Q.&nbsp;Zhou, H.&nbsp;Zhang, Z.&nbsp;Chen, D.&nbsp;Cox, Y.&nbsp;Yang, and C.&nbsp;Gan, “Principle-driven self-alignment of language models from scratch with minimal human supervision,” <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">arXiv preprint arXiv:2305.03047</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;E. team, “Palmyra-base Parameter Autoregressive Language Model,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dev.writer.com" title="">https://dev.writer.com</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
——, “Camel-5b instructgpt,” <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dev.writer.com" title="">https://dev.writer.com</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yandex. Yalm. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yandex/YaLM-100B" title="">https://github.com/yandex/YaLM-100B</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Team <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">et&nbsp;al.</em>, “Introducing mpt-7b: a new standard for open-source, commercially usable llms,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;Mitra, L.&nbsp;D. Corro, S.&nbsp;Mahajan, A.&nbsp;Codas, C.&nbsp;Simoes, S.&nbsp;Agarwal, X.&nbsp;Chen, A.&nbsp;Razdaibiedina, E.&nbsp;Jones, K.&nbsp;Aggarwal, H.&nbsp;Palangi, G.&nbsp;Zheng, C.&nbsp;Rosset, H.&nbsp;Khanpour, and A.&nbsp;Awadallah, “Orca 2: Teaching small language models how to reason,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Gao, A.&nbsp;Madaan, S.&nbsp;Zhou, U.&nbsp;Alon, P.&nbsp;Liu, Y.&nbsp;Yang, J.&nbsp;Callan, and G.&nbsp;Neubig, “Pal: Program-aided language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">International Conference on Machine Learning</em>.&nbsp;&nbsp;&nbsp;PMLR, 2023, pp. 10 764–10 799.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic. claude. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/introducing-claude" title="">https://www.anthropic.com/news/introducing-claude</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Nijkamp, H.&nbsp;Hayashi, C.&nbsp;Xiong, S.&nbsp;Savarese, and Y.&nbsp;Zhou, “Codegen2: Lessons for training llms on programming and natural languages,” <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">arXiv preprint arXiv:2305.02309</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Tunstall, E.&nbsp;Beeching, N.&nbsp;Lambert, N.&nbsp;Rajani, K.&nbsp;Rasul, Y.&nbsp;Belkada, S.&nbsp;Huang, L.&nbsp;von Werra, C.&nbsp;Fourrier, N.&nbsp;Habib <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">et&nbsp;al.</em>, “Zephyr: Direct distillation of lm alignment,” <em class="ltx_emph ltx_font_italic" id="bib.bib114.2.2">arXiv preprint arXiv:2310.16944</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;team. Grok. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://grok.x.ai/" title="">https://grok.x.ai/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Bai, S.&nbsp;Bai, S.&nbsp;Yang, S.&nbsp;Wang, S.&nbsp;Tan, P.&nbsp;Wang, J.&nbsp;Lin, C.&nbsp;Zhou, and J.&nbsp;Zhou, “Qwen-vl: A frontier large vision-language model with versatile abilities,” <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">arXiv preprint arXiv:2308.12966</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
mixtral. mixtral. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mistral.ai/news/mixtral-of-experts/" title="">https://mistral.ai/news/mixtral-of-experts/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Wang, N.&nbsp;Raman, M.&nbsp;Sibue, Z.&nbsp;Ma, P.&nbsp;Babkin, S.&nbsp;Kaur, Y.&nbsp;Pei, A.&nbsp;Nourbakhsh, and X.&nbsp;Liu, “Docllm: A layout-aware generative language model for multimodal document understanding,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Guo, Q.&nbsp;Zhu, D.&nbsp;Yang, Z.&nbsp;Xie, K.&nbsp;Dong, W.&nbsp;Zhang, G.&nbsp;Chen, X.&nbsp;Bi, Y.&nbsp;Wu, Y.&nbsp;K. Li, F.&nbsp;Luo, Y.&nbsp;Xiong, and W.&nbsp;Liang, “Deepseek-coder: When the large language model meets programming – the rise of code intelligence,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
F.&nbsp;Wan, X.&nbsp;Huang, D.&nbsp;Cai, X.&nbsp;Quan, W.&nbsp;Bi, and S.&nbsp;Shi, “Knowledge fusion of large language models,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Zhang, G.&nbsp;Zeng, T.&nbsp;Wang, and W.&nbsp;Lu, “Tinyllama: An open-source small language model,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Wu, Y.&nbsp;Gan, Y.&nbsp;Ge, Z.&nbsp;Lu, J.&nbsp;Wang, Y.&nbsp;Feng, P.&nbsp;Luo, and Y.&nbsp;Shan, “Llama pro: Progressive llama with block expansion,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
X.&nbsp;Amatriain, A.&nbsp;Sankar, J.&nbsp;Bing, P.&nbsp;K. Bodigutla, T.&nbsp;J. Hazen, and M.&nbsp;Kazi, “Transformer models: an introduction and catalog,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Penedo, Q.&nbsp;Malartic, D.&nbsp;Hesslow, R.&nbsp;Cojocaru, A.&nbsp;Cappelli, H.&nbsp;Alobeidli, B.&nbsp;Pannier, E.&nbsp;Almazrouei, and J.&nbsp;Launay, “The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only,” <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">arXiv preprint arXiv:2306.01116</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Hernandez, T.&nbsp;Brown, T.&nbsp;Conerly, N.&nbsp;DasSarma, D.&nbsp;Drain, S.&nbsp;El-Showk, N.&nbsp;Elhage, Z.&nbsp;Hatfield-Dodds, T.&nbsp;Henighan, T.&nbsp;Hume <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">et&nbsp;al.</em>, “Scaling laws and interpretability of learning from repeated data,” <em class="ltx_emph ltx_font_italic" id="bib.bib125.2.2">arXiv preprint arXiv:2205.10487</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Shaw, J.&nbsp;Uszkoreit, and A.&nbsp;Vaswani, “Self-attention with relative position representations,” <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">arXiv preprint arXiv:1803.02155</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Su, Y.&nbsp;Lu, S.&nbsp;Pan, B.&nbsp;Wen, and Y.&nbsp;Liu, “Roformer: Enhanced transformer with rotary position embedding,” <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">arXiv preprint arXiv:2104.09864</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Press, N.&nbsp;A. Smith, and M.&nbsp;Lewis, “Train short, test long: Attention with linear biases enables input length extrapolation,” <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">arXiv preprint arXiv:2108.12409</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Ke, D.&nbsp;He, and T.-Y. Liu, “Rethinking positional encoding in language pre-training,” <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">arXiv preprint arXiv:2006.15595</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Shazeer, A.&nbsp;Mirhoseini, K.&nbsp;Maziarz, A.&nbsp;Davis, Q.&nbsp;Le, G.&nbsp;Hinton, and J.&nbsp;Dean, “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer,” <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">arXiv preprint arXiv:1701.06538</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
W.&nbsp;Fedus, B.&nbsp;Zoph, and N.&nbsp;Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">The Journal of Machine Learning Research</em>, vol.&nbsp;23, no.&nbsp;1, pp. 5232–5270, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;K. Mahabadi, S.&nbsp;Ruder, M.&nbsp;Dehghani, and J.&nbsp;Henderson, “Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Zhang, L.&nbsp;Dong, X.&nbsp;Li, S.&nbsp;Zhang, X.&nbsp;Sun, S.&nbsp;Wang, J.&nbsp;Li, R.&nbsp;Hu, T.&nbsp;Zhang, F.&nbsp;Wu, and G.&nbsp;Wang, “Instruction tuning for large language models: A survey,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Mishra, D.&nbsp;Khashabi, C.&nbsp;Baral, and H.&nbsp;Hajishirzi, “Cross-task generalization via natural language crowdsourcing instructions,” <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">arXiv preprint arXiv:2104.08773</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Wang, Y.&nbsp;Kordi, S.&nbsp;Mishra, A.&nbsp;Liu, N.&nbsp;A. Smith, D.&nbsp;Khashabi, and H.&nbsp;Hajishirzi, “Self-instruct: Aligning language model with self generated instructions,” <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">arXiv preprint arXiv:2212.10560</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Ethayarajh, W.&nbsp;Xu, D.&nbsp;Jurafsky, and D.&nbsp;Kiela. Kto. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf" title="">https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;F. Christiano, J.&nbsp;Leike, T.&nbsp;Brown, M.&nbsp;Martic, S.&nbsp;Legg, and D.&nbsp;Amodei, “Deep reinforcement learning from human preferences,” <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">Advances in neural information processing systems</em>, vol.&nbsp;30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Lee, S.&nbsp;Phatale, H.&nbsp;Mansoor, K.&nbsp;Lu, T.&nbsp;Mesnard, C.&nbsp;Bishop, V.&nbsp;Carbune, and A.&nbsp;Rastogi, “Rlaif: Scaling reinforcement learning from human feedback with ai feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">arXiv preprint arXiv:2309.00267</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Rafailov, A.&nbsp;Sharma, E.&nbsp;Mitchell, S.&nbsp;Ermon, C.&nbsp;D. Manning, and C.&nbsp;Finn, “Direct preference optimization: Your language model is secretly a reward model,” <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">arXiv preprint arXiv:2305.18290</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Rajbhandari, J.&nbsp;Rasley, O.&nbsp;Ruwase, and Y.&nbsp;He, “Zero: Memory optimizations toward training trillion parameter models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</em>.&nbsp;&nbsp;&nbsp;IEEE, 2020, pp. 1–16.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Peng, E.&nbsp;Alcaide, Q.&nbsp;Anthony, A.&nbsp;Albalak, S.&nbsp;Arcadinho, H.&nbsp;Cao, X.&nbsp;Cheng, M.&nbsp;Chung, M.&nbsp;Grella, K.&nbsp;K. GV <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">et&nbsp;al.</em>, “Rwkv: Reinventing rnns for the transformer era,” <em class="ltx_emph ltx_font_italic" id="bib.bib141.2.2">arXiv preprint arXiv:2305.13048</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;J. Hu, Y.&nbsp;Shen, P.&nbsp;Wallis, Z.&nbsp;Allen-Zhu, Y.&nbsp;Li, S.&nbsp;Wang, L.&nbsp;Wang, and W.&nbsp;Chen, “Lora: Low-rank adaptation of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Hinton, O.&nbsp;Vinyals, and J.&nbsp;Dean, “Distilling the knowledge in a neural network,” <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">arXiv preprint arXiv:1503.02531</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Gou, B.&nbsp;Yu, S.&nbsp;J. Maybank, and D.&nbsp;Tao, “Knowledge distillation: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">International Journal of Computer Vision</em>, vol. 129, pp. 1789–1819, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Ji, N.&nbsp;Lee, R.&nbsp;Frieske, T.&nbsp;Yu, D.&nbsp;Su, Y.&nbsp;Xu, E.&nbsp;Ishii, Y.&nbsp;J. Bang, A.&nbsp;Madotto, and P.&nbsp;Fung, “Survey of hallucination in natural language generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">ACM Comput. Surv.</em>, vol.&nbsp;55, no.&nbsp;12, mar 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3571730" title="">https://doi.org/10.1145/3571730</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;McKenna, T.&nbsp;Li, L.&nbsp;Cheng, M.&nbsp;J. Hosseini, M.&nbsp;Johnson, and M.&nbsp;Steedman, “Sources of hallucination by large language models on inference tasks,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.-Y. Lin, “ROUGE: A package for automatic evaluation of summaries,” in <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">Text Summarization Branches Out</em>.&nbsp;&nbsp;&nbsp;Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74–81. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W04-1013" title="">https://aclanthology.org/W04-1013</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Papineni, S.&nbsp;Roukos, T.&nbsp;Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, P.&nbsp;Isabelle, E.&nbsp;Charniak, and D.&nbsp;Lin, Eds.&nbsp;&nbsp;&nbsp;Philadelphia, Pennsylvania, USA: Association for Computational Linguistics, Jul. 2002, pp. 311–318. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P02-1040" title="">https://aclanthology.org/P02-1040</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Dhingra, M.&nbsp;Faruqui, A.&nbsp;Parikh, M.-W. Chang, D.&nbsp;Das, and W.&nbsp;Cohen, “Handling divergent reference texts when evaluating table-to-text generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, A.&nbsp;Korhonen, D.&nbsp;Traum, and L.&nbsp;Màrquez, Eds.&nbsp;&nbsp;&nbsp;Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1483" title="">https://aclanthology.org/P19-1483</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Wang, X.&nbsp;Wang, B.&nbsp;An, D.&nbsp;Yu, and C.&nbsp;Chen, “Towards faithful neural table-to-text generation with content-matching constraints,” in <em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, D.&nbsp;Jurafsky, J.&nbsp;Chai, N.&nbsp;Schluter, and J.&nbsp;Tetreault, Eds.&nbsp;&nbsp;&nbsp;Online: Association for Computational Linguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.101" title="">https://aclanthology.org/2020.acl-main.101</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Song, W.-N. Zhang, J.&nbsp;Hu, and T.&nbsp;Liu, “Generating persona consistent dialogues by exploiting natural language inference,” <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol.&nbsp;34, no.&nbsp;05, pp. 8878–8885, Apr. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
O.&nbsp;Honovich, L.&nbsp;Choshen, R.&nbsp;Aharoni, E.&nbsp;Neeman, I.&nbsp;Szpektor, and O.&nbsp;Abend, “<math alttext="q^{2}" class="ltx_Math" display="inline" id="bib.bib152.1.m1.1"><semantics id="bib.bib152.1.m1.1a"><msup id="bib.bib152.1.m1.1.1" xref="bib.bib152.1.m1.1.1.cmml"><mi id="bib.bib152.1.m1.1.1.2" xref="bib.bib152.1.m1.1.1.2.cmml">q</mi><mn id="bib.bib152.1.m1.1.1.3" xref="bib.bib152.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib152.1.m1.1b"><apply id="bib.bib152.1.m1.1.1.cmml" xref="bib.bib152.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib152.1.m1.1.1.1.cmml" xref="bib.bib152.1.m1.1.1">superscript</csymbol><ci id="bib.bib152.1.m1.1.1.2.cmml" xref="bib.bib152.1.m1.1.1.2">𝑞</ci><cn id="bib.bib152.1.m1.1.1.3.cmml" type="integer" xref="bib.bib152.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib152.1.m1.1c">q^{2}</annotation><annotation encoding="application/x-llamapun" id="bib.bib152.1.m1.1d">italic_q start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib152.2.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, M.-F. Moens, X.&nbsp;Huang, L.&nbsp;Specia, and S.&nbsp;W.-t. Yih, Eds.&nbsp;&nbsp;&nbsp;Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 7856–7870. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.619" title="">https://aclanthology.org/2021.emnlp-main.619</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Dziri, H.&nbsp;Rashkin, T.&nbsp;Linzen, and D.&nbsp;Reitter, “Evaluating attribution in dialogue systems: The BEGIN benchmark,” <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">Transactions of the Association for Computational Linguistics</em>, vol.&nbsp;10, pp. 1066–1083, 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.tacl-1.62" title="">https://aclanthology.org/2022.tacl-1.62</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Santhanam, B.&nbsp;Hedayatnia, S.&nbsp;Gella, A.&nbsp;Padmakumar, S.&nbsp;Kim, Y.&nbsp;Liu, and D.&nbsp;Z. Hakkani-Tür, “Rome was built in 1776: A case study on factual correctness in knowledge-grounded response generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">ArXiv</em>, vol. abs/2110.05456, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Min, K.&nbsp;Krishna, X.&nbsp;Lyu, M.&nbsp;Lewis, W.&nbsp;tau Yih, P.&nbsp;W. Koh, M.&nbsp;Iyyer, L.&nbsp;Zettlemoyer, and H.&nbsp;Hajishirzi, “Factscore: Fine-grained atomic evaluation of factual precision in long form text generation,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Sculley, G.&nbsp;Holt, D.&nbsp;Golovin, E.&nbsp;Davydov, T.&nbsp;Phillips, D.&nbsp;Ebner, V.&nbsp;Chaudhary, and M.&nbsp;Young, “Machine learning: The high interest credit card of technical debt,” in <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)</em>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Zhang, A.&nbsp;Zhang, M.&nbsp;Li, and A.&nbsp;Smola, “Automatic chain of thought prompting in large language models,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Yao, D.&nbsp;Yu, J.&nbsp;Zhao, I.&nbsp;Shafran, T.&nbsp;L. Griffiths, Y.&nbsp;Cao, and K.&nbsp;Narasimhan, “Tree of thoughts: Deliberate problem solving with large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Manakul, A.&nbsp;Liusie, and M.&nbsp;J.&nbsp;F. Gales, “Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Shinn, F.&nbsp;Cassano, E.&nbsp;Berman, A.&nbsp;Gopinath, K.&nbsp;Narasimhan, and S.&nbsp;Yao, “Reflexion: Language agents with verbal reinforcement learning,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;J. Zhang, S.&nbsp;Florin, A.&nbsp;N. Lee, E.&nbsp;Niknafs, A.&nbsp;Marginean, A.&nbsp;Wang, K.&nbsp;Tyser, Z.&nbsp;Chin, Y.&nbsp;Hicke, N.&nbsp;Singh, M.&nbsp;Udell, Y.&nbsp;Kim, T.&nbsp;Buonassisi, A.&nbsp;Solar-Lezama, and I.&nbsp;Drori, “Exploring the mit mathematics and eecs curriculum using large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Wu, E.&nbsp;Jiang, A.&nbsp;Donsbach, J.&nbsp;Gray, A.&nbsp;Molina, M.&nbsp;Terry, and C.&nbsp;J. Cai, “Promptchainer: Chaining large language model prompts through visual programming,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zhou, A.&nbsp;I. Muresanu, Z.&nbsp;Han, K.&nbsp;Paster, S.&nbsp;Pitis, H.&nbsp;Chan, and J.&nbsp;Ba, “Large language models are human-level prompt engineers,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;S.&nbsp;H. Lewis, E.&nbsp;Perez, A.&nbsp;Piktus, F.&nbsp;Petroni, V.&nbsp;Karpukhin, N.&nbsp;Goyal, H.&nbsp;Küttler, M.&nbsp;Lewis, W.&nbsp;Yih, T.&nbsp;Rocktäschel, S.&nbsp;Riedel, and D.&nbsp;Kiela, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">CoRR</em>, vol. abs/2005.11401, 2020. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.11401" title="">https://arxiv.org/abs/2005.11401</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Gao, Y.&nbsp;Xiong, X.&nbsp;Gao, K.&nbsp;Jia, J.&nbsp;Pan, Y.&nbsp;Bi, Y.&nbsp;Dai, J.&nbsp;Sun, and H.&nbsp;Wang, “Retrieval-augmented generation for large language models: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
A.&nbsp;W. Services. (Year of publication, e.g., 2023) Question answering using retrieval augmented generation with foundation models in amazon sagemaker jumpstart. Accessed: Date of access, e.g., December 5, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://shorturl.at/dSV47" title="">https://shorturl.at/dSV47</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Pan, L.&nbsp;Luo, Y.&nbsp;Wang, C.&nbsp;Chen, J.&nbsp;Wang, and X.&nbsp;Wu, “Unifying large language models and knowledge graphs: A roadmap,” <em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">arXiv preprint arXiv:2306.08302</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Jiang, F.&nbsp;F. Xu, L.&nbsp;Gao, Z.&nbsp;Sun, Q.&nbsp;Liu, J.&nbsp;Dwivedi-Yu, Y.&nbsp;Yang, J.&nbsp;Callan, and G.&nbsp;Neubig, “Active retrieval augmented generation,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Schick, J.&nbsp;Dwivedi-Yu, R.&nbsp;Dessì, R.&nbsp;Raileanu, M.&nbsp;Lomeli, L.&nbsp;Zettlemoyer, N.&nbsp;Cancedda, and T.&nbsp;Scialom, “Toolformer: Language models can teach themselves to use tools,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Paranjape, S.&nbsp;Lundberg, S.&nbsp;Singh, H.&nbsp;Hajishirzi, L.&nbsp;Zettlemoyer, and M.&nbsp;T. Ribeiro, “Art: Automatic multi-step reasoning and tool-use for large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Shen, K.&nbsp;Song, X.&nbsp;Tan, D.&nbsp;Li, W.&nbsp;Lu, and Y.&nbsp;Zhuang, “Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface,” <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">arXiv preprint arXiv:2303.17580</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Xi, W.&nbsp;Chen, X.&nbsp;Guo, W.&nbsp;He, Y.&nbsp;Ding, B.&nbsp;Hong, M.&nbsp;Zhang, J.&nbsp;Wang, S.&nbsp;Jin, E.&nbsp;Zhou <em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">et&nbsp;al.</em>, “The rise and potential of large language model based agents: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib172.2.2">arXiv preprint arXiv:2309.07864</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Wang, C.&nbsp;Ma, X.&nbsp;Feng, Z.&nbsp;Zhang, H.&nbsp;Yang, J.&nbsp;Zhang, Z.&nbsp;Chen, J.&nbsp;Tang, X.&nbsp;Chen, Y.&nbsp;Lin <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">et&nbsp;al.</em>, “A survey on large language model based autonomous agents,” <em class="ltx_emph ltx_font_italic" id="bib.bib173.2.2">arXiv preprint arXiv:2308.11432</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Durante, Q.&nbsp;Huang, N.&nbsp;Wake, R.&nbsp;Gong, J.&nbsp;S. Park, B.&nbsp;Sarkar, R.&nbsp;Taori, Y.&nbsp;Noda, D.&nbsp;Terzopoulos, Y.&nbsp;Choi, K.&nbsp;Ikeuchi, H.&nbsp;Vo, L.&nbsp;Fei-Fei, and J.&nbsp;Gao, “Agent ai: Surveying the horizons of multimodal interaction,” <em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">arXiv preprint arXiv:2401.03568</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
B.&nbsp;Xu, Z.&nbsp;Peng, B.&nbsp;Lei, S.&nbsp;Mukherjee, Y.&nbsp;Liu, and D.&nbsp;Xu, “Rewoo: Decoupling reasoning from observations for efficient augmented language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Yao, J.&nbsp;Zhao, D.&nbsp;Yu, N.&nbsp;Du, I.&nbsp;Shafran, K.&nbsp;Narasimhan, and Y.&nbsp;Cao, “React: Synergizing reasoning and acting in language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Nair, E.&nbsp;Schumacher, G.&nbsp;Tso, and A.&nbsp;Kannan, “Dera: Enhancing large language model completions with dialog-enabled resolving agents,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Chang, X.&nbsp;Wang, J.&nbsp;Wang, Y.&nbsp;Wu, L.&nbsp;Yang, K.&nbsp;Zhu, H.&nbsp;Chen, X.&nbsp;Yi, C.&nbsp;Wang, Y.&nbsp;Wang, W.&nbsp;Ye, Y.&nbsp;Zhang, Y.&nbsp;Chang, P.&nbsp;S. Yu, Q.&nbsp;Yang, and X.&nbsp;Xie, “A survey on evaluation of large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Kwiatkowski, J.&nbsp;Palomaki, O.&nbsp;Redfield, M.&nbsp;Collins, A.&nbsp;Parikh, C.&nbsp;Alberti, D.&nbsp;Epstein, I.&nbsp;Polosukhin, J.&nbsp;Devlin, K.&nbsp;Lee, K.&nbsp;Toutanova, L.&nbsp;Jones, M.&nbsp;Kelcey, M.-W. Chang, A.&nbsp;M. Dai, J.&nbsp;Uszkoreit, Q.&nbsp;Le, and S.&nbsp;Petrov, “Natural questions: A benchmark for question answering research,” <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">Transactions of the Association for Computational Linguistics</em>, vol.&nbsp;7, pp. 452–466, 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/Q19-1026" title="">https://aclanthology.org/Q19-1026</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Hendrycks, C.&nbsp;Burns, S.&nbsp;Basart, A.&nbsp;Zou, M.&nbsp;Mazeika, D.&nbsp;Song, and J.&nbsp;Steinhardt, “Measuring massive multitask language understanding,” 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Austin, A.&nbsp;Odena, M.&nbsp;Nye, M.&nbsp;Bosma, H.&nbsp;Michalewski, D.&nbsp;Dohan, E.&nbsp;Jiang, C.&nbsp;Cai, M.&nbsp;Terry, Q.&nbsp;Le <em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">et&nbsp;al.</em>, “Program synthesis with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib181.2.2">arXiv preprint arXiv:2108.07732</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
E.&nbsp;Choi, H.&nbsp;He, M.&nbsp;Iyyer, M.&nbsp;Yatskar, W.-t. Yih, Y.&nbsp;Choi, P.&nbsp;Liang, and L.&nbsp;Zettlemoyer, “QuAC: Question answering in context,” in <em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, E.&nbsp;Riloff, D.&nbsp;Chiang, J.&nbsp;Hockenmaier, and J.&nbsp;Tsujii, Eds.&nbsp;&nbsp;&nbsp;Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-1241" title="">https://aclanthology.org/D18-1241</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Hendrycks, S.&nbsp;Basart, S.&nbsp;Kadavath, M.&nbsp;Mazeika, A.&nbsp;Arora, E.&nbsp;Guo, C.&nbsp;Burns, S.&nbsp;Puranik, H.&nbsp;He, D.&nbsp;Song, and J.&nbsp;Steinhardt, “Measuring coding challenge competence with apps,” <em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">NeurIPS</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
V.&nbsp;Zhong, C.&nbsp;Xiong, and R.&nbsp;Socher, “Seq2sql: Generating structured queries from natural language using reinforcement learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">arXiv preprint arXiv:1709.00103</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Joshi, E.&nbsp;Choi, D.&nbsp;Weld, and L.&nbsp;Zettlemoyer, “TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension,” in <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, R.&nbsp;Barzilay and M.-Y. Kan, Eds.&nbsp;&nbsp;&nbsp;Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P17-1147" title="">https://aclanthology.org/P17-1147</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;Lai, Q.&nbsp;Xie, H.&nbsp;Liu, Y.&nbsp;Yang, and E.&nbsp;Hovy, “RACE: Large-scale ReAding comprehension dataset from examinations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, M.&nbsp;Palmer, R.&nbsp;Hwa, and S.&nbsp;Riedel, Eds.&nbsp;&nbsp;&nbsp;Copenhagen, Denmark: Association for Computational Linguistics, Sep. 2017, pp. 785–794. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D17-1082" title="">https://aclanthology.org/D17-1082</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Rajpurkar, J.&nbsp;Zhang, K.&nbsp;Lopyrev, and P.&nbsp;Liang, “SQuAD: 100,000+ questions for machine comprehension of text,” in <em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, J.&nbsp;Su, K.&nbsp;Duh, and X.&nbsp;Carreras, Eds.&nbsp;&nbsp;&nbsp;Austin, Texas: Association for Computational Linguistics, Nov. 2016, pp. 2383–2392. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D16-1264" title="">https://aclanthology.org/D16-1264</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
C.&nbsp;Clark, K.&nbsp;Lee, M.&nbsp;Chang, T.&nbsp;Kwiatkowski, M.&nbsp;Collins, and K.&nbsp;Toutanova, “Boolq: Exploring the surprising difficulty of natural yes/no questions,” <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">CoRR</em>, vol. abs/1905.10044, 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1905.10044" title="">http://arxiv.org/abs/1905.10044</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Khashabi, S.&nbsp;Chaturvedi, M.&nbsp;Roth, S.&nbsp;Upadhyay, and D.&nbsp;Roth, “Looking beyond the surface:a challenge set for reading comprehension over multiple sentences,” in <em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
K.&nbsp;Cobbe, V.&nbsp;Kosaraju, M.&nbsp;Bavarian, M.&nbsp;Chen, H.&nbsp;Jun, L.&nbsp;Kaiser, M.&nbsp;Plappert, J.&nbsp;Tworek, J.&nbsp;Hilton, R.&nbsp;Nakano, C.&nbsp;Hesse, and J.&nbsp;Schulman, “Training verifiers to solve math word problems,” <em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">CoRR</em>, vol. abs/2110.14168, 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2110.14168" title="">https://arxiv.org/abs/2110.14168</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Hendrycks, C.&nbsp;Burns, S.&nbsp;Kadavath, A.&nbsp;Arora, S.&nbsp;Basart, E.&nbsp;Tang, D.&nbsp;Song, and J.&nbsp;Steinhardt, “Measuring mathematical problem solving with the MATH dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">CoRR</em>, vol. abs/2103.03874, 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2103.03874" title="">https://arxiv.org/abs/2103.03874</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Zellers, A.&nbsp;Holtzman, Y.&nbsp;Bisk, A.&nbsp;Farhadi, and Y.&nbsp;Choi, “Hellaswag: Can a machine really finish your sentence?” 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
P.&nbsp;Clark, I.&nbsp;Cowhey, O.&nbsp;Etzioni, T.&nbsp;Khot, A.&nbsp;Sabharwal, C.&nbsp;Schoenick, and O.&nbsp;Tafjord, “Think you have solved question answering? try arc, the AI2 reasoning challenge,” <em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">CoRR</em>, vol. abs/1803.05457, 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1803.05457" title="">http://arxiv.org/abs/1803.05457</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Bisk, R.&nbsp;Zellers, R.&nbsp;L. Bras, J.&nbsp;Gao, and Y.&nbsp;Choi, “PIQA: reasoning about physical commonsense in natural language,” <em class="ltx_emph ltx_font_italic" id="bib.bib194.1.1">CoRR</em>, vol. abs/1911.11641, 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1911.11641" title="">http://arxiv.org/abs/1911.11641</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Sap, H.&nbsp;Rashkin, D.&nbsp;Chen, R.&nbsp;L. Bras, and Y.&nbsp;Choi, “Socialiqa: Commonsense reasoning about social interactions,” <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">CoRR</em>, vol. abs/1904.09728, 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1904.09728" title="">http://arxiv.org/abs/1904.09728</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
T.&nbsp;Mihaylov, P.&nbsp;Clark, T.&nbsp;Khot, and A.&nbsp;Sabharwal, “Can a suit of armor conduct electricity? A new dataset for open book question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">CoRR</em>, vol. abs/1809.02789, 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1809.02789" title="">http://arxiv.org/abs/1809.02789</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Lin, J.&nbsp;Hilton, and O.&nbsp;Evans, “Truthfulqa: Measuring how models mimic human falsehoods,” <em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">arXiv preprint arXiv:2109.07958</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Z.&nbsp;Yang, P.&nbsp;Qi, S.&nbsp;Zhang, Y.&nbsp;Bengio, W.&nbsp;W. Cohen, R.&nbsp;Salakhutdinov, and C.&nbsp;D. Manning, “Hotpotqa: A dataset for diverse, explainable multi-hop question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib198.1.1">CoRR</em>, vol. abs/1809.09600, 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1809.09600" title="">http://arxiv.org/abs/1809.09600</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Zhuang, Y.&nbsp;Yu, K.&nbsp;Wang, H.&nbsp;Sun, and C.&nbsp;Zhang, “Toolqa: A dataset for llm question answering with external tools,” <em class="ltx_emph ltx_font_italic" id="bib.bib199.1.1">arXiv preprint arXiv:2306.13304</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Chen, J.&nbsp;Bolton, and C.&nbsp;D. Manning, “A thorough examination of the cnn/daily mail reading comprehension task,” in <em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">Association for Computational Linguistics (ACL)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
R.&nbsp;Nallapati, B.&nbsp;Zhou, C.&nbsp;Gulcehre, B.&nbsp;Xiang <em class="ltx_emph ltx_font_italic" id="bib.bib201.1.1">et&nbsp;al.</em>, “Abstractive text summarization using sequence-to-sequence rnns and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib201.2.2">arXiv preprint arXiv:1602.06023</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Bai and D.&nbsp;Z. Wang, “More than reading comprehension: A survey on datasets and metrics of textual question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">arXiv preprint arXiv:2109.12264</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.-Y. Huang, E.&nbsp;Choi, and W.-t. Yih, “Flowqa: Grasping flow in history for conversational machine comprehension,” <em class="ltx_emph ltx_font_italic" id="bib.bib203.1.1">arXiv preprint arXiv:1810.06683</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_tag_bibitem">[204]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Lee, J.&nbsp;Lee, H.&nbsp;Moon, C.&nbsp;Park, J.&nbsp;Seo, S.&nbsp;Eo, S.&nbsp;Koo, and H.&nbsp;Lim, “A survey on evaluation metrics for machine translation,” <em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">Mathematics</em>, vol.&nbsp;11, no.&nbsp;4, p. 1006, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_tag_bibitem">[205]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Li, X.&nbsp;Cheng, W.&nbsp;X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval: A large-scale hallucination evaluation benchmark for large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib205.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 2023, pp. 6449–6464.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_tag_bibitem">[206]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Simon Mark Hughes, “Hughes hallucination evaluation model (hhem) leaderboard,” 2024, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/vectara/Hallucination-evaluation-leaderboard" title="">https://huggingface.co/spaces/vectara/Hallucination-evaluation-leaderboard</a>, Last accessed on 2024-01-21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_tag_bibitem">[207]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
J.&nbsp;Kaddour, J.&nbsp;Harris, M.&nbsp;Mozes, H.&nbsp;Bradley, R.&nbsp;Raileanu, and R.&nbsp;McHardy, “Challenges and applications of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib207.1.1">arXiv preprint arXiv:2307.10169</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_tag_bibitem">[208]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Gunasekar, Y.&nbsp;Zhang, J.&nbsp;Aneja, C.&nbsp;C.&nbsp;T. Mendes, A.&nbsp;Del&nbsp;Giorno, S.&nbsp;Gopi, M.&nbsp;Javaheripi, P.&nbsp;Kauffmann, G.&nbsp;de&nbsp;Rosa, O.&nbsp;Saarikivi <em class="ltx_emph ltx_font_italic" id="bib.bib208.1.1">et&nbsp;al.</em>, “Textbooks are all you need,” <em class="ltx_emph ltx_font_italic" id="bib.bib208.2.2">arXiv preprint arXiv:2306.11644</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_tag_bibitem">[209]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Y.&nbsp;Li, S.&nbsp;Bubeck, R.&nbsp;Eldan, A.&nbsp;Del&nbsp;Giorno, S.&nbsp;Gunasekar, and Y.&nbsp;T. Lee, “Textbooks are all you need ii: phi-1.5 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib209.1.1">arXiv preprint arXiv:2309.05463</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_tag_bibitem">[210]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Poli, S.&nbsp;Massaroli, E.&nbsp;Nguyen, D.&nbsp;Y. Fu, T.&nbsp;Dao, S.&nbsp;Baccus, Y.&nbsp;Bengio, S.&nbsp;Ermon, and C.&nbsp;Ré, “Hyena hierarchy: Towards larger convolutional language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_tag_bibitem">[211]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
M.&nbsp;Poli, J.&nbsp;Wang, S.&nbsp;Massaroli, J.&nbsp;Quesnelle, E.&nbsp;Nguyen, and A.&nbsp;Thomas, “StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models,” 12 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/togethercomputer/stripedhyena" title="">https://github.com/togethercomputer/stripedhyena</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_tag_bibitem">[212]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
D.&nbsp;Y. Fu, S.&nbsp;Arora, J.&nbsp;Grogan, I.&nbsp;Johnson, S.&nbsp;Eyuboglu, A.&nbsp;W. Thomas, B.&nbsp;Spector, M.&nbsp;Poli, A.&nbsp;Rudra, and C.&nbsp;Ré, “Monarch mixer: A simple sub-quadratic gemm-based architecture,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_tag_bibitem">[213]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
G.&nbsp;J. McLachlan, S.&nbsp;X. Lee, and S.&nbsp;I. Rathnayake, “Finite mixture models,” <em class="ltx_emph ltx_font_italic" id="bib.bib213.1.1">Annual review of statistics and its application</em>, vol.&nbsp;6, pp. 355–378, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_tag_bibitem">[214]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
H.&nbsp;Liu, C.&nbsp;Li, Q.&nbsp;Wu, and Y.&nbsp;J. Lee, “Visual instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib214.1.1">arXiv preprint arXiv:2304.08485</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_tag_bibitem">[215]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Liu, H.&nbsp;Cheng, H.&nbsp;Liu, H.&nbsp;Zhang, F.&nbsp;Li, T.&nbsp;Ren, X.&nbsp;Zou, J.&nbsp;Yang, H.&nbsp;Su, J.&nbsp;Zhu, L.&nbsp;Zhang, J.&nbsp;Gao, and C.&nbsp;Li, “Llava-plus: Learning to use tools for creating multimodal agents,” <em class="ltx_emph ltx_font_italic" id="bib.bib215.1.1">arXiv preprint arXiv:2311.05437</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_tag_bibitem">[216]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
S.&nbsp;Wu, H.&nbsp;Fei, L.&nbsp;Qu, W.&nbsp;Ji, and T.-S. Chua, “Next-gpt: Any-to-any multimodal llm,” <em class="ltx_emph ltx_font_italic" id="bib.bib216.1.1">arXiv preprint arXiv:2309.05519</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_tag_bibitem">[217]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;N. Khasmakhi, M.&nbsp;Asgari-Chenaghlu, N.&nbsp;Asghar, P.&nbsp;Schaer, and D.&nbsp;Zühlke, “Convgenvismo: Evaluation of conversational generative vision models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_tag_bibitem">[218]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
N.&nbsp;Alshahwan, J.&nbsp;Chheda, A.&nbsp;Finegenova, B.&nbsp;Gokkaya, M.&nbsp;Harman, I.&nbsp;Harper, A.&nbsp;Marginean, S.&nbsp;Sengupta, and E.&nbsp;Wang, “Automated unit test improvement using large language models at meta,” <em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">arXiv preprint arXiv:2402.09171</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_tag_bibitem">[219]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
L.&nbsp;Sun, Y.&nbsp;Huang, H.&nbsp;Wang, S.&nbsp;Wu, Q.&nbsp;Zhang, C.&nbsp;Gao, Y.&nbsp;Huang, W.&nbsp;Lyu, Y.&nbsp;Zhang, X.&nbsp;Li <em class="ltx_emph ltx_font_italic" id="bib.bib219.1.1">et&nbsp;al.</em>, “Trustllm: Trustworthiness in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib219.2.2">arXiv preprint arXiv:2401.05561</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_tag_bibitem">[220]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Microsoft. Deepspeed. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/microsoft/DeepSpeed" title="">https://github.com/microsoft/DeepSpeed</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_tag_bibitem">[221]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
HuggingFace. Transformers. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/transformers" title="">https://github.com/huggingface/transformers</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_tag_bibitem">[222]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nvidia. Megatron. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA/Megatron-LM" title="">https://github.com/NVIDIA/Megatron-LM</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_tag_bibitem">[223]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
BMTrain. Bmtrain. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenBMB/BMTrain" title="">https://github.com/OpenBMB/BMTrain</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_tag_bibitem">[224]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
EleutherAI. gpt-neox. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/EleutherAI/gpt-neox" title="">https://github.com/EleutherAI/gpt-neox</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_tag_bibitem">[225]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
microsoft. Lora. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/microsoft/LoRA" title="">https://github.com/microsoft/LoRA</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_tag_bibitem">[226]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
ColossalAI. Colossalai. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hpcaitech/ColossalAI" title="">https://github.com/hpcaitech/ColossalAI</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_tag_bibitem">[227]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
FastChat. Fastchat. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/lm-sys/FastChat" title="">https://github.com/lm-sys/FastChat</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_tag_bibitem">[228]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
skypilot. skypilot. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/skypilot-org/skypilot" title="">https://github.com/skypilot-org/skypilot</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_tag_bibitem">[229]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
vllm. vllm. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/vllm-project/vllm" title="">https://github.com/vllm-project/vllm</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_tag_bibitem">[230]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
huggingface. text-generation-inference. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/text-generation-inference" title="">https://github.com/huggingface/text-generation-inference</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_tag_bibitem">[231]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
langchain. langchain. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/langchain-ai/langchain" title="">https://github.com/langchain-ai/langchain</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_tag_bibitem">[232]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
bentoml. Openllm. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/bentoml/OpenLLM" title="">https://github.com/bentoml/OpenLLM</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_tag_bibitem">[233]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
embedchain. embedchain. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/embedchain/embedchain" title="">https://github.com/embedchain/embedchain</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_tag_bibitem">[234]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
microsoft. autogen. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/microsoft/autogen" title="">https://github.com/microsoft/autogen</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_tag_bibitem">[235]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
babyagi. babyagi. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yoheinakajima/babyagi" title="">https://github.com/yoheinakajima/babyagi</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_tag_bibitem">[236]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
guidance. guidance. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/guidance-ai/guidance" title="">https://github.com/guidance-ai/guidance</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_tag_bibitem">[237]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
prompttools. prompttools. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hegelai/prompttools" title="">https://github.com/hegelai/prompttools</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_tag_bibitem">[238]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
promptfoo. promptfoo. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/promptfoo/promptfoo" title="">https://github.com/promptfoo/promptfoo</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_tag_bibitem">[239]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
facebook. faiss. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/faiss" title="">https://github.com/facebookresearch/faiss</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_tag_bibitem">[240]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
milvus. milvus. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/milvus-io/milvus" title="">https://github.com/milvus-io/milvus</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_tag_bibitem">[241]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
qdrant. qdrant. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/qdrant/qdrant" title="">https://github.com/qdrant/qdrant</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_tag_bibitem">[242]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
weaviate. weaviate. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/weaviate/weaviate" title="">https://github.com/weaviate/weaviate</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_tag_bibitem">[243]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
llama index. llama-index. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/run-llama/llama_index" title="">https://github.com/run-llama/llama_index</a>
</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">1. Open Source Toolkits For LLM Development and Deployment</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">There are various frameworks and libraries developed for LLM training, evaluation, and deployment, and covering every single framework is out of this paper’s scope. But we try to provide a brief introduction of some of the most popular ones, grouped into different categories.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A0.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="A0.SS1.5.1.1">-A</span> </span><span class="ltx_text ltx_font_italic" id="A0.SS1.6.2">LLM Training/Inference Frameworks</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A0.SS1.p1">
<p class="ltx_p" id="A0.SS1.p1.1">Some of the popular frameworks which are useful for LLM training includes (note that some of them can be used beyond LLM training too):</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS1.p2">
<p class="ltx_p" id="A0.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="A0.SS1.p2.1.1">DeepSpeed</span>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib220" title="">220</a>]</cite> is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.
DeepSpeed enables world’s most powerful language models like MT-530B and BLOOM. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS1.p3">
<p class="ltx_p" id="A0.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="A0.SS1.p3.1.1">Transformers</span>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib221" title="">221</a>]</cite> is library by HuggingFace which provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. Using pretrained models one can reduce compute costs, carbon footprint, and save the time and resources required to train a model from scratch.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS1.p4">
<p class="ltx_p" id="A0.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="A0.SS1.p4.1.1">Megatron-LM</span>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib222" title="">222</a>]</cite> is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA.
It contains efficient, model-parallel (tensor, sequence, and pipeline), and multi-node pre-training of transformer based models such as GPT, BERT, and T5 using mixed precision.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS1.p5">
<p class="ltx_p" id="A0.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="A0.SS1.p5.1.1">BMTrain</span>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib223" title="">223</a>]</cite> is an efficient large model training toolkit that can be used to train large models with tens of billions of parameters. It can train models in a distributed manner while keeping the code as simple as stand-alone training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS1.p6">
<p class="ltx_p" id="A0.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="A0.SS1.p6.1.1">GPT-NeoX</span>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib224" title="">224</a>]</cite> leverages many of the same features and technologies as the popular Megatron-DeepSpeed library but with substantially increased usability and novel optimizations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS1.p7">
<p class="ltx_p" id="A0.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="A0.SS1.p7.1.1">LoRA</span>
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib225" title="">225</a>]</cite> library provides the support for Low-Rank Adaptation of Large Language Models. It reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights. This vastly reduces the storage requirement for large language models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency. LoRA also outperforms several other adaptation methods including adapter, prefix-tuning, and fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS1.p8">
<p class="ltx_p" id="A0.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="A0.SS1.p8.1.1">ColossalAI</span>
library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib226" title="">226</a>]</cite> provides a collection of parallel components. It aims to support developers to write their distributed deep learning models just like how they write their model on their laptop. They provide user-friendly tools to kickstart distributed training and inference in a few lines.
In terms of Parallelism strategies, they support: Data Parallelism, Pipeline Parallelism, Sequence Parallelism, Zero Redundancy Optimizer (ZeRO) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib140" title="">140</a>]</cite>, and Auto-Parallelism.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A0.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="A0.SS2.5.1.1">-B</span> </span><span class="ltx_text ltx_font_italic" id="A0.SS2.6.2">Deployment Tools</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A0.SS2.p1">
<p class="ltx_p" id="A0.SS2.p1.1">We provide an overview of some of the most popular LLM deployment tools here.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS2.p2">
<p class="ltx_p" id="A0.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A0.SS2.p2.1.1">FastChat</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib227" title="">227</a>]</cite> is an open platform for training, serving, and evaluating large language model based chatbots.
FastChat’s core features include:
The training and evaluation code for state-of-the-art models (e.g., Vicuna, MT-Bench), and a distributed multi-model serving system with web UI and OpenAI-compatible RESTful APIs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS2.p3">
<p class="ltx_p" id="A0.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="A0.SS2.p3.1.1">Skypilot</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib228" title="">228</a>]</cite> is a framework for running LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS2.p4">
<p class="ltx_p" id="A0.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="A0.SS2.p4.1.1">vLLM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib229" title="">229</a>]</cite> is a fast and easy-to-use library for LLM inference and serving.
vLLM seamlessly supports many Hugging Face models, including the following architectures: Aquila, Baichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT BigCode, LLaMA, LLaMA 2, Mistral, Mixtral, MPT, OPT, Qwen, Yi, and many more.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS2.p5">
<p class="ltx_p" id="A0.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="A0.SS2.p5.1.1">text-generation-inference</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib230" title="">230</a>]</cite> is a toolkit for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and more.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS2.p6">
<p class="ltx_p" id="A0.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="A0.SS2.p6.1.1">LangChain</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib231" title="">231</a>]</cite> is a framework for developing applications powered by language models. It enables applications that:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A0.I1">
<li class="ltx_item" id="A0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A0.I1.i1.p1">
<p class="ltx_p" id="A0.I1.i1.p1.1">Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A0.I1.i2.p1">
<p class="ltx_p" id="A0.I1.i2.p1.1">Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A0.SS2.p7">
<p class="ltx_p" id="A0.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="A0.SS2.p7.1.1">OpenLLM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib232" title="">232</a>]</cite> is an open-source platform designed to facilitate the deployment and operation of large language models (LLMs) in real-world applications. With OpenLLM, you can run inference on any open-source LLM, deploy them on the cloud or on-premises, and build powerful AI applications.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS2.p8">
<p class="ltx_p" id="A0.SS2.p8.1"><span class="ltx_text ltx_font_bold" id="A0.SS2.p8.1.1">Embedchain</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib233" title="">233</a>]</cite> is an Open Source RAG Framework that makes it easy to create and deploy AI apps.
Embedchain streamlines the creation of RAG applications, offering a seamless process for managing various types of unstructured data. It efficiently segments data into manageable chunks, generates relevant embeddings, and stores them in a vector database for optimized retrieval.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS2.p9">
<p class="ltx_p" id="A0.SS2.p9.1"><span class="ltx_text ltx_font_bold" id="A0.SS2.p9.1.1">Autogen</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib234" title="">234</a>]</cite> is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS2.p10">
<p class="ltx_p" id="A0.SS2.p10.1"><span class="ltx_text ltx_font_bold" id="A0.SS2.p10.1.1">BabyAGI</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib235" title="">235</a>]</cite> is an autonomous Artificial Intelligence agent, that is designed to generate and execute tasks based on given objectives. It harnesses cutting-edge technologies from OpenAI, Pinecone, LangChain, and Chroma to automate tasks and achieve specific goals. In this blog post, we will dive into the unique features of BabyAGI and explore how it can streamline task automation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A0.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="A0.SS3.5.1.1">-C</span> </span><span class="ltx_text ltx_font_italic" id="A0.SS3.6.2">Prompting Libraries</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A0.SS3.p1">
<p class="ltx_p" id="A0.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="A0.SS3.p1.1.1">Guidance</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib236" title="">236</a>]</cite> is a programming paradigm that offers superior control and efficiency compared to conventional prompting and chaining. It allows users to constrain generation (e.g. with regex and CFGs) as well as to interleave control (conditional, loops) and generation seamlessly.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS3.p2">
<p class="ltx_p" id="A0.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="A0.SS3.p2.1.1">PromptTools</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib237" title="">237</a>]</cite> offers a set of open-source, self-hostable tools for experimenting with, testing, and evaluating LLMs, vector databases, and prompts. The core idea is to enable developers to evaluate using familiar interfaces like code, notebooks, and a local playground.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS3.p3">
<p class="ltx_p" id="A0.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="A0.SS3.p3.1.1">PromptBench</span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">promptbench</span>]</cite> is a Pytorch-based Python package for Evaluation of Large Language Models (LLMs). It provides user-friendly APIs for researchers to conduct evaluation on LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS3.p4">
<p class="ltx_p" id="A0.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="A0.SS3.p4.1.1">Promptfoo</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib238" title="">238</a>]</cite> is a tool for testing and evaluating LLM output quality. It systematically test prompts, models, and RAGs with predefined test cases.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A0.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="A0.SS4.5.1.1">-D</span> </span><span class="ltx_text ltx_font_italic" id="A0.SS4.6.2">VectorDB</span>
</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A0.SS4.p1">
<p class="ltx_p" id="A0.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="A0.SS4.p1.1.1">Faiss</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib239" title="">239</a>]</cite> is a library developed by Facebook AI Research that provides efficient similarity search and clustering of dense vectors. It is designed for use with large-scale, high-dimensional data and supports several index types and algorithms for various use cases.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS4.p2">
<p class="ltx_p" id="A0.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="A0.SS4.p2.1.1">Milvus</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib240" title="">240</a>]</cite> is an open-source vector database built to power embedding similarity search and AI applications. Milvus makes unstructured data search more accessible, and provides a consistent user experience regardless of the deployment environment.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS4.p3">
<p class="ltx_p" id="A0.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="A0.SS4.p3.1.1">Qdrant</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib241" title="">241</a>]</cite> is a vector similarity search engine and vector database. It provides a production-ready service with a convenient API to store, search, and manage points—vectors with an additional payload Qdrant is tailored to extended filtering support. environment.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS4.p4">
<p class="ltx_p" id="A0.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="A0.SS4.p4.1.1">Weaviate</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib242" title="">242</a>]</cite> is an open-source, GraphQL-based vector search engine that enables similarity search on high-dimensional data. While it is open-source, the commercial version offers additional features, support, and managed services.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A0.SS4.p5">
<p class="ltx_p" id="A0.SS4.p5.1">Some of the other popular options includes <span class="ltx_text ltx_font_bold" id="A0.SS4.p5.1.1">LlamaIndex</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2402.06196v2#bib.bib243" title="">243</a>]</cite> and <span class="ltx_text ltx_font_bold" id="A0.SS4.p5.1.2">Pinecone</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
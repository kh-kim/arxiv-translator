<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>OLMo : Accelerating the Science of Language Models</title>
<!--Generated on Wed Feb 28 02:26:01 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on February 28, 2024.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.00838v3/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.00838v3">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.00838v3">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.00838v3" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S1" title="1 Introduction ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2" title="2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>OLMo Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS1" title="2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>OLMo&nbsp;Model and Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS2" title="2.2 Pretraining Data: Dolma ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Pretraining Data: Dolma</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS3" title="2.3 Adaptation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Adaptation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4" title="2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px1" title="In-Loop Training Ablations ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">In-Loop Training Ablations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px2" title="Downstream Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Downstream Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px3" title="Intrinsic Language Modeling Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Intrinsic Language Modeling Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px4" title="Adaptation Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Adaptation Evaluation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3" title="3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Training OLMo</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS1" title="3.1 Distributed Training Framework ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Distributed Training Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS2" title="3.2 Optimizer ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Optimizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS3" title="3.3 Data ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS4" title="3.4 Hardware ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Hardware</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4" title="4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1" title="4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Downstream evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1.SSS0.Px1" title="Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1.SSS0.Px2" title="Results ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS2" title="4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Intrinsic language modeling evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS2.SSS0.Px1" title="Setup ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS2.SSS0.Px2" title="Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS3" title="4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Adaptation Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS3.SSS0.Px1" title="Setup ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS3.SSS0.Px2" title="Results ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS4" title="4.4 Power Consumption and Carbon Footprint ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Power Consumption and Carbon Footprint</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S5" title="5 Artifacts Released ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Artifacts Released</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S6" title="6 License ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>License</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S7" title="7 Conclusion and Future Work ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1" title="Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.SS0.SSS0.Px1" title="Additional perplexity results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Additional perplexity results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.SS0.SSS0.Px2" title="Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Additional end-task results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A2" title="Appendix B Adaptation Training Details ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Adaptation Training Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A3" title="Appendix C Adaptation Evaluation and Model details ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Adaptation Evaluation and Model details</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: fdsymbol</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2402.00838v3 [cs.CL] 28 Feb 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text" id="id1.1" style="position:relative; bottom:-3.0pt;"><span class="ltx_text" id="id1.1.1" style="width:0.0pt;position:relative; bottom:3.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text" id="id1.1.1.1" style="font-size:70%;">OLMo</span></span><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="29" id="id1.1.g1" src="x1.png" width="55"></span>&nbsp;: Accelerating the Science of Language Models
</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id3.2.2">Dirk Groeneveld

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id2.1.1.m1.1"><semantics id="id2.1.1.m1.1a"><msup id="id2.1.1.m1.1.1" xref="id2.1.1.m1.1.1.cmml"><mi id="id2.1.1.m1.1.1a" xref="id2.1.1.m1.1.1.cmml"></mi><mi id="id2.1.1.m1.1.1.1" mathcolor="#265ED4" xref="id2.1.1.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id2.1.1.m1.1b"><apply id="id2.1.1.m1.1.1.cmml" xref="id2.1.1.m1.1.1"><ci id="id2.1.1.m1.1.1.1.cmml" xref="id2.1.1.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.1.1.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id2.1.1.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Iz Beltagy

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id3.2.2.m2.1"><semantics id="id3.2.2.m2.1a"><msup id="id3.2.2.m2.1.1" xref="id3.2.2.m2.1.1.cmml"><mi id="id3.2.2.m2.1.1a" xref="id3.2.2.m2.1.1.cmml"></mi><mi id="id3.2.2.m2.1.1.1" mathcolor="#265ED4" xref="id3.2.2.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id3.2.2.m2.1b"><apply id="id3.2.2.m2.1.1.cmml" xref="id3.2.2.m2.1.1"><ci id="id3.2.2.m2.1.1.1.cmml" xref="id3.2.2.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.2.2.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id3.2.2.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id7.6.6">Pete Walsh

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id4.3.3.m1.1"><semantics id="id4.3.3.m1.1a"><msup id="id4.3.3.m1.1.1" xref="id4.3.3.m1.1.1.cmml"><mi id="id4.3.3.m1.1.1a" xref="id4.3.3.m1.1.1.cmml"></mi><mi id="id4.3.3.m1.1.1.1" mathcolor="#265ED4" xref="id4.3.3.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id4.3.3.m1.1b"><apply id="id4.3.3.m1.1.1.cmml" xref="id4.3.3.m1.1.1"><ci id="id4.3.3.m1.1.1.1.cmml" xref="id4.3.3.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.3.3.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id4.3.3.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Akshita Bhagia

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id5.4.4.m2.1"><semantics id="id5.4.4.m2.1a"><msup id="id5.4.4.m2.1.1" xref="id5.4.4.m2.1.1.cmml"><mi id="id5.4.4.m2.1.1a" xref="id5.4.4.m2.1.1.cmml"></mi><mi id="id5.4.4.m2.1.1.1" mathcolor="#265ED4" xref="id5.4.4.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id5.4.4.m2.1b"><apply id="id5.4.4.m2.1.1.cmml" xref="id5.4.4.m2.1.1"><ci id="id5.4.4.m2.1.1.1.cmml" xref="id5.4.4.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.4.4.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id5.4.4.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Rodney Kinney

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id6.5.5.m3.1"><semantics id="id6.5.5.m3.1a"><msup id="id6.5.5.m3.1.1" xref="id6.5.5.m3.1.1.cmml"><mi id="id6.5.5.m3.1.1a" xref="id6.5.5.m3.1.1.cmml"></mi><mi id="id6.5.5.m3.1.1.1" mathcolor="#265ED4" xref="id6.5.5.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id6.5.5.m3.1b"><apply id="id6.5.5.m3.1.1.cmml" xref="id6.5.5.m3.1.1"><ci id="id6.5.5.m3.1.1.1.cmml" xref="id6.5.5.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.5.5.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id6.5.5.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Oyvind Tafjord

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id7.6.6.m4.1"><semantics id="id7.6.6.m4.1a"><msup id="id7.6.6.m4.1.1" xref="id7.6.6.m4.1.1.cmml"><mi id="id7.6.6.m4.1.1a" xref="id7.6.6.m4.1.1.cmml"></mi><mi id="id7.6.6.m4.1.1.1" mathcolor="#265ED4" xref="id7.6.6.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id7.6.6.m4.1b"><apply id="id7.6.6.m4.1.1.cmml" xref="id7.6.6.m4.1.1"><ci id="id7.6.6.m4.1.1.1.cmml" xref="id7.6.6.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.6.6.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id7.6.6.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id13.12.12">Ananya Harsh Jha

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id8.7.7.m1.1"><semantics id="id8.7.7.m1.1a"><msup id="id8.7.7.m1.1.1" xref="id8.7.7.m1.1.1.cmml"><mi id="id8.7.7.m1.1.1a" xref="id8.7.7.m1.1.1.cmml"></mi><mi id="id8.7.7.m1.1.1.1" mathcolor="#265ED4" xref="id8.7.7.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id8.7.7.m1.1b"><apply id="id8.7.7.m1.1.1.cmml" xref="id8.7.7.m1.1.1"><ci id="id8.7.7.m1.1.1.1.cmml" xref="id8.7.7.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.7.7.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id8.7.7.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Hamish Ivison

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id9.8.8.m2.1"><semantics id="id9.8.8.m2.1a"><msup id="id9.8.8.m2.1.1" xref="id9.8.8.m2.1.1.cmml"><mi id="id9.8.8.m2.1.1a" xref="id9.8.8.m2.1.1.cmml"></mi><mi id="id9.8.8.m2.1.1.1" mathcolor="#265ED4" xref="id9.8.8.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id9.8.8.m2.1b"><apply id="id9.8.8.m2.1.1.cmml" xref="id9.8.8.m2.1.1"><ci id="id9.8.8.m2.1.1.1.cmml" xref="id9.8.8.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.8.8.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id9.8.8.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id10.9.9.m3.1"><semantics id="id10.9.9.m3.1a"><msup id="id10.9.9.m3.1.1" xref="id10.9.9.m3.1.1.cmml"><mi id="id10.9.9.m3.1.1a" xref="id10.9.9.m3.1.1.cmml"></mi><mi id="id10.9.9.m3.1.1.1" mathcolor="#265ED4" xref="id10.9.9.m3.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id10.9.9.m3.1b"><apply id="id10.9.9.m3.1.1.cmml" xref="id10.9.9.m3.1.1"><ci id="id10.9.9.m3.1.1.1.cmml" xref="id10.9.9.m3.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.9.9.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id10.9.9.m3.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Ian Magnusson

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id11.10.10.m4.1"><semantics id="id11.10.10.m4.1a"><msup id="id11.10.10.m4.1.1" xref="id11.10.10.m4.1.1.cmml"><mi id="id11.10.10.m4.1.1a" xref="id11.10.10.m4.1.1.cmml"></mi><mi id="id11.10.10.m4.1.1.1" mathcolor="#265ED4" xref="id11.10.10.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id11.10.10.m4.1b"><apply id="id11.10.10.m4.1.1.cmml" xref="id11.10.10.m4.1.1"><ci id="id11.10.10.m4.1.1.1.cmml" xref="id11.10.10.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.10.10.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id11.10.10.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Yizhong Wang

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id12.11.11.m5.1"><semantics id="id12.11.11.m5.1a"><msup id="id12.11.11.m5.1.1" xref="id12.11.11.m5.1.1.cmml"><mi id="id12.11.11.m5.1.1a" xref="id12.11.11.m5.1.1.cmml"></mi><mi id="id12.11.11.m5.1.1.1" mathcolor="#265ED4" xref="id12.11.11.m5.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id12.11.11.m5.1b"><apply id="id12.11.11.m5.1.1.cmml" xref="id12.11.11.m5.1.1"><ci id="id12.11.11.m5.1.1.1.cmml" xref="id12.11.11.m5.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.11.11.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id12.11.11.m5.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id13.12.12.m6.1"><semantics id="id13.12.12.m6.1a"><msup id="id13.12.12.m6.1.1" xref="id13.12.12.m6.1.1.cmml"><mi id="id13.12.12.m6.1.1a" xref="id13.12.12.m6.1.1.cmml"></mi><mi id="id13.12.12.m6.1.1.1" mathcolor="#265ED4" xref="id13.12.12.m6.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id13.12.12.m6.1b"><apply id="id13.12.12.m6.1.1.cmml" xref="id13.12.12.m6.1.1"><ci id="id13.12.12.m6.1.1.1.cmml" xref="id13.12.12.m6.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id13.12.12.m6.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id13.12.12.m6.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id17.16.16">Shane Arora

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id14.13.13.m1.1"><semantics id="id14.13.13.m1.1a"><msup id="id14.13.13.m1.1.1" xref="id14.13.13.m1.1.1.cmml"><mi id="id14.13.13.m1.1.1a" xref="id14.13.13.m1.1.1.cmml"></mi><mi id="id14.13.13.m1.1.1.1" mathcolor="#265ED4" xref="id14.13.13.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id14.13.13.m1.1b"><apply id="id14.13.13.m1.1.1.cmml" xref="id14.13.13.m1.1.1"><ci id="id14.13.13.m1.1.1.1.cmml" xref="id14.13.13.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id14.13.13.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id14.13.13.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 David Atkinson

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id15.14.14.m2.1"><semantics id="id15.14.14.m2.1a"><msup id="id15.14.14.m2.1.1" xref="id15.14.14.m2.1.1.cmml"><mi id="id15.14.14.m2.1.1a" xref="id15.14.14.m2.1.1.cmml"></mi><mi id="id15.14.14.m2.1.1.1" mathcolor="#265ED4" xref="id15.14.14.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id15.14.14.m2.1b"><apply id="id15.14.14.m2.1.1.cmml" xref="id15.14.14.m2.1.1"><ci id="id15.14.14.m2.1.1.1.cmml" xref="id15.14.14.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id15.14.14.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id15.14.14.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Russell Authur

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id16.15.15.m3.1"><semantics id="id16.15.15.m3.1a"><msup id="id16.15.15.m3.1.1" xref="id16.15.15.m3.1.1.cmml"><mi id="id16.15.15.m3.1.1a" xref="id16.15.15.m3.1.1.cmml"></mi><mi id="id16.15.15.m3.1.1.1" mathcolor="#265ED4" xref="id16.15.15.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id16.15.15.m3.1b"><apply id="id16.15.15.m3.1.1.cmml" xref="id16.15.15.m3.1.1"><ci id="id16.15.15.m3.1.1.1.cmml" xref="id16.15.15.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id16.15.15.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id16.15.15.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Khyathi Raghavi Chandu

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id17.16.16.m4.1"><semantics id="id17.16.16.m4.1a"><msup id="id17.16.16.m4.1.1" xref="id17.16.16.m4.1.1.cmml"><mi id="id17.16.16.m4.1.1a" xref="id17.16.16.m4.1.1.cmml"></mi><mi id="id17.16.16.m4.1.1.1" mathcolor="#265ED4" xref="id17.16.16.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id17.16.16.m4.1b"><apply id="id17.16.16.m4.1.1.cmml" xref="id17.16.16.m4.1.1"><ci id="id17.16.16.m4.1.1.1.cmml" xref="id17.16.16.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id17.16.16.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id17.16.16.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id23.22.22">Arman Cohan

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\gamma}}}" class="ltx_Math" display="inline" id="id18.17.17.m1.1"><semantics id="id18.17.17.m1.1a"><msup id="id18.17.17.m1.1.1" xref="id18.17.17.m1.1.1.cmml"><mi id="id18.17.17.m1.1.1a" xref="id18.17.17.m1.1.1.cmml"></mi><mi id="id18.17.17.m1.1.1.1" mathcolor="#265ED4" xref="id18.17.17.m1.1.1.1.cmml">γ</mi></msup><annotation-xml encoding="MathML-Content" id="id18.17.17.m1.1b"><apply id="id18.17.17.m1.1.1.cmml" xref="id18.17.17.m1.1.1"><ci id="id18.17.17.m1.1.1.1.cmml" xref="id18.17.17.m1.1.1.1">𝛾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id18.17.17.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\gamma}}}</annotation><annotation encoding="application/x-llamapun" id="id18.17.17.m1.1d">start_FLOATSUPERSCRIPT bold_italic_γ end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id19.18.18.m2.1"><semantics id="id19.18.18.m2.1a"><msup id="id19.18.18.m2.1.1" xref="id19.18.18.m2.1.1.cmml"><mi id="id19.18.18.m2.1.1a" xref="id19.18.18.m2.1.1.cmml"></mi><mi id="id19.18.18.m2.1.1.1" mathcolor="#265ED4" xref="id19.18.18.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id19.18.18.m2.1b"><apply id="id19.18.18.m2.1.1.cmml" xref="id19.18.18.m2.1.1"><ci id="id19.18.18.m2.1.1.1.cmml" xref="id19.18.18.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id19.18.18.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id19.18.18.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Jennifer Dumas

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id20.19.19.m3.1"><semantics id="id20.19.19.m3.1a"><msup id="id20.19.19.m3.1.1" xref="id20.19.19.m3.1.1.cmml"><mi id="id20.19.19.m3.1.1a" xref="id20.19.19.m3.1.1.cmml"></mi><mi id="id20.19.19.m3.1.1.1" mathcolor="#265ED4" xref="id20.19.19.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id20.19.19.m3.1b"><apply id="id20.19.19.m3.1.1.cmml" xref="id20.19.19.m3.1.1"><ci id="id20.19.19.m3.1.1.1.cmml" xref="id20.19.19.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id20.19.19.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id20.19.19.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Yanai Elazar

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id21.20.20.m4.1"><semantics id="id21.20.20.m4.1a"><msup id="id21.20.20.m4.1.1" xref="id21.20.20.m4.1.1.cmml"><mi id="id21.20.20.m4.1.1a" xref="id21.20.20.m4.1.1.cmml"></mi><mi id="id21.20.20.m4.1.1.1" mathcolor="#265ED4" xref="id21.20.20.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id21.20.20.m4.1b"><apply id="id21.20.20.m4.1.1.cmml" xref="id21.20.20.m4.1.1"><ci id="id21.20.20.m4.1.1.1.cmml" xref="id21.20.20.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id21.20.20.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id21.20.20.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id22.21.21.m5.1"><semantics id="id22.21.21.m5.1a"><msup id="id22.21.21.m5.1.1" xref="id22.21.21.m5.1.1.cmml"><mi id="id22.21.21.m5.1.1a" xref="id22.21.21.m5.1.1.cmml"></mi><mi id="id22.21.21.m5.1.1.1" mathcolor="#265ED4" xref="id22.21.21.m5.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id22.21.21.m5.1b"><apply id="id22.21.21.m5.1.1.cmml" xref="id22.21.21.m5.1.1"><ci id="id22.21.21.m5.1.1.1.cmml" xref="id22.21.21.m5.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id22.21.21.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id22.21.21.m5.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Yuling Gu

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id23.22.22.m6.1"><semantics id="id23.22.22.m6.1a"><msup id="id23.22.22.m6.1.1" xref="id23.22.22.m6.1.1.cmml"><mi id="id23.22.22.m6.1.1a" xref="id23.22.22.m6.1.1.cmml"></mi><mi id="id23.22.22.m6.1.1.1" mathcolor="#265ED4" xref="id23.22.22.m6.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id23.22.22.m6.1b"><apply id="id23.22.22.m6.1.1.cmml" xref="id23.22.22.m6.1.1"><ci id="id23.22.22.m6.1.1.1.cmml" xref="id23.22.22.m6.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id23.22.22.m6.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id23.22.22.m6.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id27.26.26">Jack Hessel

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id24.23.23.m1.1"><semantics id="id24.23.23.m1.1a"><msup id="id24.23.23.m1.1.1" xref="id24.23.23.m1.1.1.cmml"><mi id="id24.23.23.m1.1.1a" xref="id24.23.23.m1.1.1.cmml"></mi><mi id="id24.23.23.m1.1.1.1" mathcolor="#265ED4" xref="id24.23.23.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id24.23.23.m1.1b"><apply id="id24.23.23.m1.1.1.cmml" xref="id24.23.23.m1.1.1"><ci id="id24.23.23.m1.1.1.1.cmml" xref="id24.23.23.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id24.23.23.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id24.23.23.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Tushar Khot

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id25.24.24.m2.1"><semantics id="id25.24.24.m2.1a"><msup id="id25.24.24.m2.1.1" xref="id25.24.24.m2.1.1.cmml"><mi id="id25.24.24.m2.1.1a" xref="id25.24.24.m2.1.1.cmml"></mi><mi id="id25.24.24.m2.1.1.1" mathcolor="#265ED4" xref="id25.24.24.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id25.24.24.m2.1b"><apply id="id25.24.24.m2.1.1.cmml" xref="id25.24.24.m2.1.1"><ci id="id25.24.24.m2.1.1.1.cmml" xref="id25.24.24.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id25.24.24.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id25.24.24.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 William Merrill

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\delta}}}" class="ltx_Math" display="inline" id="id26.25.25.m3.1"><semantics id="id26.25.25.m3.1a"><msup id="id26.25.25.m3.1.1" xref="id26.25.25.m3.1.1.cmml"><mi id="id26.25.25.m3.1.1a" xref="id26.25.25.m3.1.1.cmml"></mi><mi id="id26.25.25.m3.1.1.1" mathcolor="#265ED4" xref="id26.25.25.m3.1.1.1.cmml">δ</mi></msup><annotation-xml encoding="MathML-Content" id="id26.25.25.m3.1b"><apply id="id26.25.25.m3.1.1.cmml" xref="id26.25.25.m3.1.1"><ci id="id26.25.25.m3.1.1.1.cmml" xref="id26.25.25.m3.1.1.1">𝛿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id26.25.25.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\delta}}}</annotation><annotation encoding="application/x-llamapun" id="id26.25.25.m3.1d">start_FLOATSUPERSCRIPT bold_italic_δ end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Jacob Morrison

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id27.26.26.m4.1"><semantics id="id27.26.26.m4.1a"><msup id="id27.26.26.m4.1.1" xref="id27.26.26.m4.1.1.cmml"><mi id="id27.26.26.m4.1.1a" xref="id27.26.26.m4.1.1.cmml"></mi><mi id="id27.26.26.m4.1.1.1" mathcolor="#265ED4" xref="id27.26.26.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id27.26.26.m4.1b"><apply id="id27.26.26.m4.1.1.cmml" xref="id27.26.26.m4.1.1"><ci id="id27.26.26.m4.1.1.1.cmml" xref="id27.26.26.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id27.26.26.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id27.26.26.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id31.30.30">
Niklas Muennighoff


 Aakanksha Naik

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id29.28.28.m2.1"><semantics id="id29.28.28.m2.1a"><msup id="id29.28.28.m2.1.1" xref="id29.28.28.m2.1.1.cmml"><mi id="id29.28.28.m2.1.1a" xref="id29.28.28.m2.1.1.cmml"></mi><mi id="id29.28.28.m2.1.1.1" mathcolor="#265ED4" xref="id29.28.28.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id29.28.28.m2.1b"><apply id="id29.28.28.m2.1.1.cmml" xref="id29.28.28.m2.1.1"><ci id="id29.28.28.m2.1.1.1.cmml" xref="id29.28.28.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id29.28.28.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id29.28.28.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Crystal Nam

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id30.29.29.m3.1"><semantics id="id30.29.29.m3.1a"><msup id="id30.29.29.m3.1.1" xref="id30.29.29.m3.1.1.cmml"><mi id="id30.29.29.m3.1.1a" xref="id30.29.29.m3.1.1.cmml"></mi><mi id="id30.29.29.m3.1.1.1" mathcolor="#265ED4" xref="id30.29.29.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id30.29.29.m3.1b"><apply id="id30.29.29.m3.1.1.cmml" xref="id30.29.29.m3.1.1"><ci id="id30.29.29.m3.1.1.1.cmml" xref="id30.29.29.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id30.29.29.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id30.29.29.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Matthew E. Peters

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id31.30.30.m4.1"><semantics id="id31.30.30.m4.1a"><msup id="id31.30.30.m4.1.1" xref="id31.30.30.m4.1.1.cmml"><mi id="id31.30.30.m4.1.1a" xref="id31.30.30.m4.1.1.cmml"></mi><mi id="id31.30.30.m4.1.1.1" mathcolor="#265ED4" xref="id31.30.30.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id31.30.30.m4.1b"><apply id="id31.30.30.m4.1.1.cmml" xref="id31.30.30.m4.1.1"><ci id="id31.30.30.m4.1.1.1.cmml" xref="id31.30.30.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id31.30.30.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id31.30.30.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id36.35.35">Valentina Pyatkin

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id32.31.31.m1.1"><semantics id="id32.31.31.m1.1a"><msup id="id32.31.31.m1.1.1" xref="id32.31.31.m1.1.1.cmml"><mi id="id32.31.31.m1.1.1a" xref="id32.31.31.m1.1.1.cmml"></mi><mi id="id32.31.31.m1.1.1.1" mathcolor="#265ED4" xref="id32.31.31.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id32.31.31.m1.1b"><apply id="id32.31.31.m1.1.1.cmml" xref="id32.31.31.m1.1.1"><ci id="id32.31.31.m1.1.1.1.cmml" xref="id32.31.31.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id32.31.31.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id32.31.31.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id33.32.32.m2.1"><semantics id="id33.32.32.m2.1a"><msup id="id33.32.32.m2.1.1" xref="id33.32.32.m2.1.1.cmml"><mi id="id33.32.32.m2.1.1a" xref="id33.32.32.m2.1.1.cmml"></mi><mi id="id33.32.32.m2.1.1.1" mathcolor="#265ED4" xref="id33.32.32.m2.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id33.32.32.m2.1b"><apply id="id33.32.32.m2.1.1.cmml" xref="id33.32.32.m2.1.1"><ci id="id33.32.32.m2.1.1.1.cmml" xref="id33.32.32.m2.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id33.32.32.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id33.32.32.m2.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Abhilasha Ravichander

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id34.33.33.m3.1"><semantics id="id34.33.33.m3.1a"><msup id="id34.33.33.m3.1.1" xref="id34.33.33.m3.1.1.cmml"><mi id="id34.33.33.m3.1.1a" xref="id34.33.33.m3.1.1.cmml"></mi><mi id="id34.33.33.m3.1.1.1" mathcolor="#265ED4" xref="id34.33.33.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id34.33.33.m3.1b"><apply id="id34.33.33.m3.1.1.cmml" xref="id34.33.33.m3.1.1"><ci id="id34.33.33.m3.1.1.1.cmml" xref="id34.33.33.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id34.33.33.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id34.33.33.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Dustin Schwenk

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id35.34.34.m4.1"><semantics id="id35.34.34.m4.1a"><msup id="id35.34.34.m4.1.1" xref="id35.34.34.m4.1.1.cmml"><mi id="id35.34.34.m4.1.1a" xref="id35.34.34.m4.1.1.cmml"></mi><mi id="id35.34.34.m4.1.1.1" mathcolor="#265ED4" xref="id35.34.34.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id35.34.34.m4.1b"><apply id="id35.34.34.m4.1.1.cmml" xref="id35.34.34.m4.1.1"><ci id="id35.34.34.m4.1.1.1.cmml" xref="id35.34.34.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id35.34.34.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id35.34.34.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Saurabh Shah

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id36.35.35.m5.1"><semantics id="id36.35.35.m5.1a"><msup id="id36.35.35.m5.1.1" xref="id36.35.35.m5.1.1.cmml"><mi id="id36.35.35.m5.1.1a" xref="id36.35.35.m5.1.1.cmml"></mi><mi id="id36.35.35.m5.1.1.1" mathcolor="#265ED4" xref="id36.35.35.m5.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id36.35.35.m5.1b"><apply id="id36.35.35.m5.1.1.cmml" xref="id36.35.35.m5.1.1"><ci id="id36.35.35.m5.1.1.1.cmml" xref="id36.35.35.m5.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id36.35.35.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id36.35.35.m5.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id41.40.40">Will Smith

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id37.36.36.m1.1"><semantics id="id37.36.36.m1.1a"><msup id="id37.36.36.m1.1.1" xref="id37.36.36.m1.1.1.cmml"><mi id="id37.36.36.m1.1.1a" xref="id37.36.36.m1.1.1.cmml"></mi><mi id="id37.36.36.m1.1.1.1" mathcolor="#265ED4" xref="id37.36.36.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id37.36.36.m1.1b"><apply id="id37.36.36.m1.1.1.cmml" xref="id37.36.36.m1.1.1"><ci id="id37.36.36.m1.1.1.1.cmml" xref="id37.36.36.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id37.36.36.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id37.36.36.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Emma Strubell

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id38.37.37.m2.1"><semantics id="id38.37.37.m2.1a"><msup id="id38.37.37.m2.1.1" xref="id38.37.37.m2.1.1.cmml"><mi id="id38.37.37.m2.1.1a" xref="id38.37.37.m2.1.1.cmml"></mi><mi id="id38.37.37.m2.1.1.1" mathcolor="#265ED4" xref="id38.37.37.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id38.37.37.m2.1b"><apply id="id38.37.37.m2.1.1.cmml" xref="id38.37.37.m2.1.1"><ci id="id38.37.37.m2.1.1.1.cmml" xref="id38.37.37.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id38.37.37.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id38.37.37.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\mu}}}" class="ltx_Math" display="inline" id="id39.38.38.m3.1"><semantics id="id39.38.38.m3.1a"><msup id="id39.38.38.m3.1.1" xref="id39.38.38.m3.1.1.cmml"><mi id="id39.38.38.m3.1.1a" xref="id39.38.38.m3.1.1.cmml"></mi><mi id="id39.38.38.m3.1.1.1" mathcolor="#265ED4" xref="id39.38.38.m3.1.1.1.cmml">μ</mi></msup><annotation-xml encoding="MathML-Content" id="id39.38.38.m3.1b"><apply id="id39.38.38.m3.1.1.cmml" xref="id39.38.38.m3.1.1"><ci id="id39.38.38.m3.1.1.1.cmml" xref="id39.38.38.m3.1.1.1">𝜇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id39.38.38.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\mu}}}</annotation><annotation encoding="application/x-llamapun" id="id39.38.38.m3.1d">start_FLOATSUPERSCRIPT bold_italic_μ end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Nishant Subramani

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id40.39.39.m4.1"><semantics id="id40.39.39.m4.1a"><msup id="id40.39.39.m4.1.1" xref="id40.39.39.m4.1.1.cmml"><mi id="id40.39.39.m4.1.1a" xref="id40.39.39.m4.1.1.cmml"></mi><mi id="id40.39.39.m4.1.1.1" mathcolor="#265ED4" xref="id40.39.39.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id40.39.39.m4.1b"><apply id="id40.39.39.m4.1.1.cmml" xref="id40.39.39.m4.1.1"><ci id="id40.39.39.m4.1.1.1.cmml" xref="id40.39.39.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id40.39.39.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id40.39.39.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Mitchell Wortsman

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id41.40.40.m5.1"><semantics id="id41.40.40.m5.1a"><msup id="id41.40.40.m5.1.1" xref="id41.40.40.m5.1.1.cmml"><mi id="id41.40.40.m5.1.1a" xref="id41.40.40.m5.1.1.cmml"></mi><mi id="id41.40.40.m5.1.1.1" mathcolor="#265ED4" xref="id41.40.40.m5.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id41.40.40.m5.1b"><apply id="id41.40.40.m5.1.1.cmml" xref="id41.40.40.m5.1.1"><ci id="id41.40.40.m5.1.1.1.cmml" xref="id41.40.40.m5.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id41.40.40.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id41.40.40.m5.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id44.43.43">Pradeep Dasigi

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id42.41.41.m1.1"><semantics id="id42.41.41.m1.1a"><msup id="id42.41.41.m1.1.1" xref="id42.41.41.m1.1.1.cmml"><mi id="id42.41.41.m1.1.1a" xref="id42.41.41.m1.1.1.cmml"></mi><mi id="id42.41.41.m1.1.1.1" mathcolor="#265ED4" xref="id42.41.41.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id42.41.41.m1.1b"><apply id="id42.41.41.m1.1.1.cmml" xref="id42.41.41.m1.1.1"><ci id="id42.41.41.m1.1.1.1.cmml" xref="id42.41.41.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id42.41.41.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id42.41.41.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Nathan Lambert

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id43.42.42.m2.1"><semantics id="id43.42.42.m2.1a"><msup id="id43.42.42.m2.1.1" xref="id43.42.42.m2.1.1.cmml"><mi id="id43.42.42.m2.1.1a" xref="id43.42.42.m2.1.1.cmml"></mi><mi id="id43.42.42.m2.1.1.1" mathcolor="#265ED4" xref="id43.42.42.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id43.42.42.m2.1b"><apply id="id43.42.42.m2.1.1.cmml" xref="id43.42.42.m2.1.1"><ci id="id43.42.42.m2.1.1.1.cmml" xref="id43.42.42.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id43.42.42.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id43.42.42.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Kyle Richardson

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id44.43.43.m3.1"><semantics id="id44.43.43.m3.1a"><msup id="id44.43.43.m3.1.1" xref="id44.43.43.m3.1.1.cmml"><mi id="id44.43.43.m3.1.1a" xref="id44.43.43.m3.1.1.cmml"></mi><mi id="id44.43.43.m3.1.1.1" mathcolor="#265ED4" xref="id44.43.43.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id44.43.43.m3.1b"><apply id="id44.43.43.m3.1.1.cmml" xref="id44.43.43.m3.1.1"><ci id="id44.43.43.m3.1.1.1.cmml" xref="id44.43.43.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id44.43.43.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id44.43.43.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id48.47.47">Luke Zettlemoyer

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id45.44.44.m1.1"><semantics id="id45.44.44.m1.1a"><msup id="id45.44.44.m1.1.1" xref="id45.44.44.m1.1.1.cmml"><mi id="id45.44.44.m1.1.1a" xref="id45.44.44.m1.1.1.cmml"></mi><mi id="id45.44.44.m1.1.1.1" mathcolor="#265ED4" xref="id45.44.44.m1.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id45.44.44.m1.1b"><apply id="id45.44.44.m1.1.1.cmml" xref="id45.44.44.m1.1.1"><ci id="id45.44.44.m1.1.1.1.cmml" xref="id45.44.44.m1.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id45.44.44.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id45.44.44.m1.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
Jesse Dodge

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id46.45.45.m2.1"><semantics id="id46.45.45.m2.1a"><msup id="id46.45.45.m2.1.1" xref="id46.45.45.m2.1.1.cmml"><mi id="id46.45.45.m2.1.1a" xref="id46.45.45.m2.1.1.cmml"></mi><mi id="id46.45.45.m2.1.1.1" mathcolor="#265ED4" xref="id46.45.45.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id46.45.45.m2.1b"><apply id="id46.45.45.m2.1.1.cmml" xref="id46.45.45.m2.1.1"><ci id="id46.45.45.m2.1.1.1.cmml" xref="id46.45.45.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id46.45.45.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id46.45.45.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Kyle Lo

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id47.46.46.m3.1"><semantics id="id47.46.46.m3.1a"><msup id="id47.46.46.m3.1.1" xref="id47.46.46.m3.1.1.cmml"><mi id="id47.46.46.m3.1.1a" xref="id47.46.46.m3.1.1.cmml"></mi><mi id="id47.46.46.m3.1.1.1" mathcolor="#265ED4" xref="id47.46.46.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id47.46.46.m3.1b"><apply id="id47.46.46.m3.1.1.cmml" xref="id47.46.46.m3.1.1"><ci id="id47.46.46.m3.1.1.1.cmml" xref="id47.46.46.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id47.46.46.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id47.46.46.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Luca Soldaini

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id48.47.47.m4.1"><semantics id="id48.47.47.m4.1a"><msup id="id48.47.47.m4.1.1" xref="id48.47.47.m4.1.1.cmml"><mi id="id48.47.47.m4.1.1a" xref="id48.47.47.m4.1.1.cmml"></mi><mi id="id48.47.47.m4.1.1.1" mathcolor="#265ED4" xref="id48.47.47.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id48.47.47.m4.1b"><apply id="id48.47.47.m4.1.1.cmml" xref="id48.47.47.m4.1.1"><ci id="id48.47.47.m4.1.1.1.cmml" xref="id48.47.47.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id48.47.47.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id48.47.47.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id52.51.51">Noah A. Smith

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id49.48.48.m1.1"><semantics id="id49.48.48.m1.1a"><msup id="id49.48.48.m1.1.1" xref="id49.48.48.m1.1.1.cmml"><mi id="id49.48.48.m1.1.1a" xref="id49.48.48.m1.1.1.cmml"></mi><mi id="id49.48.48.m1.1.1.1" mathcolor="#265ED4" xref="id49.48.48.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id49.48.48.m1.1b"><apply id="id49.48.48.m1.1.1.cmml" xref="id49.48.48.m1.1.1"><ci id="id49.48.48.m1.1.1.1.cmml" xref="id49.48.48.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id49.48.48.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id49.48.48.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id50.49.49.m2.1"><semantics id="id50.49.49.m2.1a"><msup id="id50.49.49.m2.1.1" xref="id50.49.49.m2.1.1.cmml"><mi id="id50.49.49.m2.1.1a" xref="id50.49.49.m2.1.1.cmml"></mi><mi id="id50.49.49.m2.1.1.1" mathcolor="#265ED4" xref="id50.49.49.m2.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id50.49.49.m2.1b"><apply id="id50.49.49.m2.1.1.cmml" xref="id50.49.49.m2.1.1"><ci id="id50.49.49.m2.1.1.1.cmml" xref="id50.49.49.m2.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id50.49.49.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id50.49.49.m2.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Hannaneh Hajishirzi

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id51.50.50.m3.1"><semantics id="id51.50.50.m3.1a"><msup id="id51.50.50.m3.1.1" xref="id51.50.50.m3.1.1.cmml"><mi id="id51.50.50.m3.1.1a" xref="id51.50.50.m3.1.1.cmml"></mi><mi id="id51.50.50.m3.1.1.1" mathcolor="#265ED4" xref="id51.50.50.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id51.50.50.m3.1b"><apply id="id51.50.50.m3.1.1.cmml" xref="id51.50.50.m3.1.1"><ci id="id51.50.50.m3.1.1.1.cmml" xref="id51.50.50.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id51.50.50.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id51.50.50.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id52.51.51.m4.1"><semantics id="id52.51.51.m4.1a"><msup id="id52.51.51.m4.1.1" xref="id52.51.51.m4.1.1.cmml"><mi id="id52.51.51.m4.1.1a" xref="id52.51.51.m4.1.1.cmml"></mi><mi id="id52.51.51.m4.1.1.1" mathcolor="#265ED4" xref="id52.51.51.m4.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id52.51.51.m4.1b"><apply id="id52.51.51.m4.1.1.cmml" xref="id52.51.51.m4.1.1"><ci id="id52.51.51.m4.1.1.1.cmml" xref="id52.51.51.m4.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id52.51.51.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id52.51.51.m4.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break">
<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id53.52.m1.1"><semantics id="id53.52.m1.1a"><msup id="id53.52.m1.1.1" xref="id53.52.m1.1.1.cmml"><mi id="id53.52.m1.1.1a" xref="id53.52.m1.1.1.cmml"></mi><mi id="id53.52.m1.1.1.1" mathcolor="#265ED4" xref="id53.52.m1.1.1.1.cmml">𝜶</mi></msup><annotation-xml encoding="MathML-Content" id="id53.52.m1.1b"><apply id="id53.52.m1.1.1.cmml" xref="id53.52.m1.1.1"><ci id="id53.52.m1.1.1.1.cmml" xref="id53.52.m1.1.1.1">𝜶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id53.52.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id53.52.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>Allen Institute for Artificial Intelligence
 
<br class="ltx_break">
<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id54.53.m2.1"><semantics id="id54.53.m2.1a"><msup id="id54.53.m2.1.1" xref="id54.53.m2.1.1.cmml"><mi id="id54.53.m2.1.1a" xref="id54.53.m2.1.1.cmml"></mi><mi id="id54.53.m2.1.1.1" mathcolor="#265ED4" xref="id54.53.m2.1.1.1.cmml">𝜷</mi></msup><annotation-xml encoding="MathML-Content" id="id54.53.m2.1b"><apply id="id54.53.m2.1.1.cmml" xref="id54.53.m2.1.1"><ci id="id54.53.m2.1.1.1.cmml" xref="id54.53.m2.1.1.1">𝜷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id54.53.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id54.53.m2.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Washington  <math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\gamma}}}" class="ltx_Math" display="inline" id="id55.54.m3.1"><semantics id="id55.54.m3.1a"><msup id="id55.54.m3.1.1" xref="id55.54.m3.1.1.cmml"><mi id="id55.54.m3.1.1a" xref="id55.54.m3.1.1.cmml"></mi><mi id="id55.54.m3.1.1.1" mathcolor="#265ED4" xref="id55.54.m3.1.1.1.cmml">𝜸</mi></msup><annotation-xml encoding="MathML-Content" id="id55.54.m3.1b"><apply id="id55.54.m3.1.1.cmml" xref="id55.54.m3.1.1"><ci id="id55.54.m3.1.1.1.cmml" xref="id55.54.m3.1.1.1">𝜸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id55.54.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\gamma}}}</annotation><annotation encoding="application/x-llamapun" id="id55.54.m3.1d">start_FLOATSUPERSCRIPT bold_italic_γ end_FLOATSUPERSCRIPT</annotation></semantics></math>Yale University
 
<br class="ltx_break">
<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\delta}}}" class="ltx_Math" display="inline" id="id56.55.m4.1"><semantics id="id56.55.m4.1a"><msup id="id56.55.m4.1.1" xref="id56.55.m4.1.1.cmml"><mi id="id56.55.m4.1.1a" xref="id56.55.m4.1.1.cmml"></mi><mi id="id56.55.m4.1.1.1" mathcolor="#265ED4" xref="id56.55.m4.1.1.1.cmml">𝜹</mi></msup><annotation-xml encoding="MathML-Content" id="id56.55.m4.1b"><apply id="id56.55.m4.1.1.cmml" xref="id56.55.m4.1.1"><ci id="id56.55.m4.1.1.1.cmml" xref="id56.55.m4.1.1.1">𝜹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id56.55.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\delta}}}</annotation><annotation encoding="application/x-llamapun" id="id56.55.m4.1d">start_FLOATSUPERSCRIPT bold_italic_δ end_FLOATSUPERSCRIPT</annotation></semantics></math>New York University  <math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\mu}}}" class="ltx_Math" display="inline" id="id57.56.m5.1"><semantics id="id57.56.m5.1a"><msup id="id57.56.m5.1.1" xref="id57.56.m5.1.1.cmml"><mi id="id57.56.m5.1.1a" xref="id57.56.m5.1.1.cmml"></mi><mi id="id57.56.m5.1.1.1" mathcolor="#265ED4" xref="id57.56.m5.1.1.1.cmml">𝝁</mi></msup><annotation-xml encoding="MathML-Content" id="id57.56.m5.1b"><apply id="id57.56.m5.1.1.cmml" xref="id57.56.m5.1.1"><ci id="id57.56.m5.1.1.1.cmml" xref="id57.56.m5.1.1.1">𝝁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id57.56.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\mu}}}</annotation><annotation encoding="application/x-llamapun" id="id57.56.m5.1d">start_FLOATSUPERSCRIPT bold_italic_μ end_FLOATSUPERSCRIPT</annotation></semantics></math>Carnegie Mellon University

<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id64.57.id1">olmo@allenai.org</span>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_dates">(February 28, 2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id65.id1">Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings.
As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed.
Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs.
To this end, this technical report details the first release of OLMo, a state-of-the-art, truly <span class="ltx_text ltx_font_bold" id="id65.id1.1">O</span>pen <span class="ltx_text ltx_font_bold" id="id65.id1.2">L</span>anguage <span class="ltx_text ltx_font_bold" id="id65.id1.3">Mo</span>del and its framework to build and study the science of language modeling.
Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="id63.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="id58.1.1">
<td class="ltx_td ltx_align_right" id="id58.1.1.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id58.1.1.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id58.1.1.1.1.g1" src="x2.png" width="22"></span></td>
<td class="ltx_td ltx_align_center" id="id58.1.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id58.1.1.2.1">Weights</span></td>
<td class="ltx_td ltx_align_left" id="id58.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B" title="">https://huggingface.co/allenai/OLMo-7B</a></td>
</tr>
<tr class="ltx_tr" id="id59.2.2">
<td class="ltx_td ltx_align_right" id="id59.2.2.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id59.2.2.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id59.2.2.1.1.g1" src="x3.png" width="20"></span></td>
<td class="ltx_td ltx_align_center" id="id59.2.2.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id59.2.2.2.1">Code</span></td>
<td class="ltx_td ltx_align_left" id="id59.2.2.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo" title="">https://github.com/allenai/OLMo</a></td>
</tr>
<tr class="ltx_tr" id="id60.3.3">
<td class="ltx_td ltx_align_right" id="id60.3.3.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id60.3.3.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id60.3.3.1.1.g1" src="x2.png" width="22"></span></td>
<td class="ltx_td ltx_align_center" id="id60.3.3.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id60.3.3.2.1">Data</span></td>
<td class="ltx_td ltx_align_left" id="id60.3.3.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/allenai/dolma" title="">https://huggingface.co/datasets/allenai/dolma</a></td>
</tr>
<tr class="ltx_tr" id="id61.4.4">
<td class="ltx_td ltx_align_right" id="id61.4.4.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id61.4.4.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id61.4.4.1.1.g1" src="x4.png" width="20"></span></td>
<td class="ltx_td ltx_align_center" id="id61.4.4.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id61.4.4.2.1">Evaluation</span></td>
<td class="ltx_td ltx_align_left" id="id61.4.4.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo-Eval" title="">https://github.com/allenai/OLMo-Eval</a></td>
</tr>
<tr class="ltx_tr" id="id62.5.5">
<td class="ltx_td ltx_align_right" id="id62.5.5.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id62.5.5.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id62.5.5.1.1.g1" src="x5.png" width="20"></span></td>
<td class="ltx_td ltx_align_center" id="id62.5.5.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id62.5.5.2.1">Adaptation</span></td>
<td class="ltx_td ltx_align_left" id="id62.5.5.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/open-instruct" title="">https://github.com/allenai/open-instruct</a></td>
</tr>
<tr class="ltx_tr" id="id63.6.6">
<td class="ltx_td ltx_align_right" id="id63.6.6.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id63.6.6.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="id63.6.6.1.1.g1" src="extracted/5436352/figures/wandb-logo.png" width="16"></span></td>
<td class="ltx_td ltx_align_center" id="id63.6.6.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id63.6.6.2.1">W&amp;B Logs</span></td>
<td class="ltx_td ltx_align_left" id="id63.6.6.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5" title="">https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5</a></td>
</tr>
</tbody>
</table>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Language models have been at the center of NLP technologies for many years <cite class="ltx_cite ltx_citemacro_citep">(Rosenfeld, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib67" title="">2000</a>; Bengio et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib6" title="">2003</a>; Mikolov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib48" title="">2013</a>; Peters et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib58" title="">2018</a>; Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib12" title="">2020</a>)</cite>.
Recently, due to large-scale pretraining and human annotation for alignment, they have become commercially valuable <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib53" title="">2023</a>)</cite>.
However, as their commercial value has increased, the largest models have become gated behind proprietary interfaces, with important details left undisclosed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">We believe that full access to open language models for the research community is critical to the scientific study of these models, their strengths and weaknesses, and their biases and risks.
Accordingly, we introduce <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">OLMo</span>, a state-of-the-art, truly open language model and framework to build, study, and advance LMs, along with the training data, training and evaluation code, intermediate model checkpoints, and training logs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recent LM releases have varied in their degree of openness. For example, Mistral 8x7B provided model weights and a brief report&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib34" title="">2024</a>)</cite>,
while LLaMA came with in-depth adaptation training instructions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite>, and Mosaic Pretrained Transformer came with many details, including the dataset distribution, though not the data itself&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(MosaicML NLP Team, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib50" title="">2023</a>)</cite>. Falcon’s pretraining data was partially released&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib2" title="">2023</a>)</cite>,
and the most open models—the Pythia suite&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Biderman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib7" title="">2023</a>)</cite> and BLOOM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(BigScience et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib8" title="">2022</a>)</cite>—released training code, model checkpoints, training data and more. </p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">With OLMo, we release the whole framework from data to training to evaluation tools: multiple training checkpoints across multiple hardware types, training logs, and exact datasets used, with a permissive license.
We are not the only team to do this; recent work from LLM360 targets similar goals&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib40" title="">2023</a>)</cite>.
OLMo&nbsp;narrows the gap from their models to state-of-the-art capabilities of models like LLaMA2. This project has benefited from lessons learned from all of these previous efforts with their varying degrees of openness, and we believe that a large, diverse population of open models is the best hope for scientific progress on understanding language models and engineering progress on improving their utility.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The OLMo framework encompasses the tools and resources required for building and researching language models. For training and modeling, it includes
full model weights, training code, training logs, ablations, training metrics in the form of Weights &amp; Biases logs, and inference code. This first release includes four variants of our language model at the 7B scale corresponding to different architectures, optimizers, and training hardware, and one model at the 1B scale, all trained on at least 2T tokens. We are also releasing hundreds of intermediate checkpoints available as revisions on HuggingFace.
For dataset building and analysis, it includes the full training data used for these models, including code that produces the training data, from AI2’s Dolma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Soldaini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>, and WIMBD&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Elazar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib24" title="">2023</a>)</cite> for analyzing pretraining data.
For evaluation, it includes AI2’s Catwalk&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Groeneveld et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib28" title="">2023</a>)</cite> for downstream evaluation and Paloma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Magnusson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite> for perplexity-based evaluation.
For instruction-tuning, we released Open Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ivison et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>; Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib83" title="">2023</a>)</cite>, and
we are currently using it to produce an adapted (instruction-tuned and RLHFed) version of OLMo, which we will release soon.
Finally, all code and weights are released under the Apache 2.0 License.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.apache.org/licenses/LICENSE-2.0" title="">http://www.apache.org/licenses/LICENSE-2.0</a></span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">This is the first step in a long series of planned releases, continuing with larger models, instruction-tuned models, and more modalities and variants down the line. We therefore hope to catalyze research into as-yet poorly understood aspects of these models, for example, the relationship between pretraining data and model capabilities, the impact of design and hyperparameter choices, and various optimization methods and their impact on model training. In addition, we report on the lessons learned and important details necessary to successfully train language models at this scale.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>OLMo Framework</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section describes the OLMo&nbsp;framework, consisting of the OLMo models (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS1" title="2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.1</span></a>), our pre-training dataset, Dolma (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS2" title="2.2 Pretraining Data: Dolma ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.2</span></a>), and our evaluation framework (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4" title="2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>OLMo&nbsp;Model and Architecture</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We adopt a decoder-only transformer architecture based on <cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib80" title="">2017</a>)</cite>,
and deliver 1B and 7B variants as described in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.T1" title="Table 1 ‣ 2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, with a 65B version coming soon.
Our specific architecture includes several improvements over the vanilla transformer from <cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib80" title="">2017</a>)</cite> following other recent large language models
like PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib14" title="">2022</a>)</cite>, the LLaMA family <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">b</a>)</cite>, OpenLM <cite class="ltx_cite ltx_citemacro_citep">(Gururangan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib30" title="">2023</a>)</cite>, and Falcon <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib2" title="">2023</a>)</cite>.
Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.T2" title="Table 2 ‣ 2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> gives a comprehensive comparison of our 7B architecture to the similarly-sized models from these other families.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Layers</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">Hidden Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1">Attention Heads</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.5.1">Tokens Trained</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1" style="padding-top:1pt;padding-bottom:1pt;">1B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.2" style="padding-top:1pt;padding-bottom:1pt;">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.3" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.4" style="padding-top:1pt;padding-bottom:1pt;">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.5" style="padding-top:1pt;padding-bottom:1pt;">2T</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.3.2.1" style="padding-top:1pt;padding-bottom:1pt;">7B</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.2" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.3" style="padding-top:1pt;padding-bottom:1pt;">4086</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.4" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.5" style="padding-top:1pt;padding-bottom:1pt;">2.46T</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S2.T1.1.4.3.1" style="padding-top:1pt;padding-bottom:1pt;">65B*</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.4.3.2" style="padding-top:1pt;padding-bottom:1pt;">80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.4.3.3" style="padding-top:1pt;padding-bottom:1pt;">8192</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.4.3.4" style="padding-top:1pt;padding-bottom:1pt;">64</td>
<td class="ltx_td ltx_border_bb" id="S2.T1.1.4.3.5" style="padding-top:1pt;padding-bottom:1pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>OLMo model sizes and the maximum number of tokens trained to. 
<br class="ltx_break">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
* <em class="ltx_emph ltx_font_italic" id="S2.T1.3.1">At the time of writing our 65B model is still training.</em></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">We generally select hyperparameters by optimizing for training throughput on our hardware while minimizing the risk of loss spikes and slow divergence. We ablate choices through our in-loop evaluation setting, given available computational sources (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px1" title="In-Loop Training Ablations ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>). Table <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.T2" title="Table 2 ‣ 2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> compares our design choices with recent state-of-the-art open language models.
Our main changes over the vanilla transformer architecture can be summarized as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">No biases.</span> Following LLaMA, PaLM, and others, we exclude all bias terms from our architecture in order to improve training stability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Non-parametric layer norm.</span> We use the non-parametric formulation of layer norm <cite class="ltx_cite ltx_citemacro_citep">(Ba et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib4" title="">2016</a>)</cite> in which there is no affine transformation within the norm, i.e. no “adaptive gain” (or bias). We believe this was the safest option and it was also the fastest compared to the other variants we considered: parametric layer norm and RMSNorm <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Sennrich, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib91" title="">2019</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">SwiGLU activation function.</span> Like LLaMA, PaLM, and others we use the SwiGLU activation function <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib70" title="">2020</a>)</cite> instead of ReLU, and following LLaMA the activation hidden size is approximately <math alttext="\frac{8}{3}d" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mrow id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mfrac id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml"><mn id="S2.I1.i3.p1.1.m1.1.1.2.2" xref="S2.I1.i3.p1.1.m1.1.1.2.2.cmml">8</mn><mn id="S2.I1.i3.p1.1.m1.1.1.2.3" xref="S2.I1.i3.p1.1.m1.1.1.2.3.cmml">3</mn></mfrac><mo id="S2.I1.i3.p1.1.m1.1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><times id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.1"></times><apply id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2"><divide id="S2.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2"></divide><cn id="S2.I1.i3.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S2.I1.i3.p1.1.m1.1.1.2.2">8</cn><cn id="S2.I1.i3.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S2.I1.i3.p1.1.m1.1.1.2.3">3</cn></apply><ci id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">\frac{8}{3}d</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">divide start_ARG 8 end_ARG start_ARG 3 end_ARG italic_d</annotation></semantics></math>, but increased to the closest multiple of 128 (e.g. 11,008 for our 7B model) to improve throughput.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Since SwiGLU is a “gated” activation function, the output is half the size of the input. So technically our inputs to SwiGLU have a dimensionality of 2 <math alttext="\times" class="ltx_Math" display="inline" id="footnote2.m1.1"><semantics id="footnote2.m1.1b"><mo id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><times id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.1e">×</annotation></semantics></math> 11,008 = 22,016 for our 7B model.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">Rotary positional embeddings (RoPE).</span> Like LLaMA, PaLM, and others we replace absolute positional embeddings with rotary positional embeddings (RoPE; <cite class="ltx_cite ltx_citemacro_citep">Su et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib73" title="">2021</a></cite>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i5.p1.1.1">Vocabulary.</span> We use a modified version of the BPE-based tokenizer from GPT-NeoX-20B <cite class="ltx_cite ltx_citemacro_citep">(Black et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib10" title="">2022</a>)</cite> with additional tokens for masking personal identifiable information (PII). The final vocabulary size is 50,280.
However, to maximize training throughput we increase the size of the corresponding embedding matrix in our model to 50,304 so that it’s a multiple of 128.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.8.9.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S2.T2.8.9.1.1" style="padding-top:1pt;padding-bottom:1pt;"></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.2.1">OLMo-7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.3.1">LLaMA2-7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.4.1">OpenLM-7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.5.1">Falcon-7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.6" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.6.1">PaLM-8B</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.10.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.8.10.2.1" style="padding-top:1pt;padding-bottom:1pt;">Dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.2" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.3" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.4" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.5" style="padding-top:1pt;padding-bottom:1pt;">4544</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.6" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.11.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.11.3.1" style="padding-top:1pt;padding-bottom:1pt;">Num heads</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.2" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.3" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.4" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.5" style="padding-top:1pt;padding-bottom:1pt;">71</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.6" style="padding-top:1pt;padding-bottom:1pt;">16</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.12.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.12.4.1" style="padding-top:1pt;padding-bottom:1pt;">Num layers</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.2" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.3" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.4" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.5" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.6" style="padding-top:1pt;padding-bottom:1pt;">32</td>
</tr>
<tr class="ltx_tr" id="S2.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.3.3.4" style="padding-top:1pt;padding-bottom:1pt;">MLP ratio</th>
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.m1.1.1" xref="S2.T2.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T2.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.m1.1d">∼</annotation></semantics></math>8/3</td>
<td class="ltx_td ltx_align_left" id="S2.T2.2.2.2" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.2.2.2.m1.1"><semantics id="S2.T2.2.2.2.m1.1a"><mo id="S2.T2.2.2.2.m1.1.1" xref="S2.T2.2.2.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.2.m1.1b"><csymbol cd="latexml" id="S2.T2.2.2.2.m1.1.1.cmml" xref="S2.T2.2.2.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.2.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.2.2.2.m1.1d">∼</annotation></semantics></math>8/3</td>
<td class="ltx_td ltx_align_left" id="S2.T2.3.3.3" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.3.3.3.m1.1"><semantics id="S2.T2.3.3.3.m1.1a"><mo id="S2.T2.3.3.3.m1.1.1" xref="S2.T2.3.3.3.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.m1.1b"><csymbol cd="latexml" id="S2.T2.3.3.3.m1.1.1.cmml" xref="S2.T2.3.3.3.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.3.3.3.m1.1d">∼</annotation></semantics></math>8/3</td>
<td class="ltx_td ltx_align_left" id="S2.T2.3.3.5" style="padding-top:1pt;padding-bottom:1pt;">4</td>
<td class="ltx_td ltx_align_left" id="S2.T2.3.3.6" style="padding-top:1pt;padding-bottom:1pt;">4</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.13.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.13.5.1" style="padding-top:1pt;padding-bottom:1pt;">Layer norm type</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.2" style="padding-top:1pt;padding-bottom:1pt;">non-parametric</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.3" style="padding-top:1pt;padding-bottom:1pt;">RMSNorm</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.4" style="padding-top:1pt;padding-bottom:1pt;">parametric</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.5" style="padding-top:1pt;padding-bottom:1pt;">parametric</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.6" style="padding-top:1pt;padding-bottom:1pt;">parametric</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.14.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.14.6.1" style="padding-top:1pt;padding-bottom:1pt;">Positional embeddings</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.2" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.3" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.4" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.5" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.6" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.15.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.15.7.1" style="padding-top:1pt;padding-bottom:1pt;">Attention variant</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.2" style="padding-top:1pt;padding-bottom:1pt;">full</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.3" style="padding-top:1pt;padding-bottom:1pt;">GQA</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.4" style="padding-top:1pt;padding-bottom:1pt;">full</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.5" style="padding-top:1pt;padding-bottom:1pt;">MQA</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.6" style="padding-top:1pt;padding-bottom:1pt;">MQA</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.16.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.16.8.1" style="padding-top:1pt;padding-bottom:1pt;">Biases</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.2" style="padding-top:1pt;padding-bottom:1pt;">none</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.3" style="padding-top:1pt;padding-bottom:1pt;">none</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.4" style="padding-top:1pt;padding-bottom:1pt;">in LN only</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.5" style="padding-top:1pt;padding-bottom:1pt;">in LN only</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.6" style="padding-top:1pt;padding-bottom:1pt;">none</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.17.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.17.9.1" style="padding-top:1pt;padding-bottom:1pt;">Block type</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.2" style="padding-top:1pt;padding-bottom:1pt;">sequential</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.3" style="padding-top:1pt;padding-bottom:1pt;">sequential</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.4" style="padding-top:1pt;padding-bottom:1pt;">sequential</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.5" style="padding-top:1pt;padding-bottom:1pt;">parallel</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.6" style="padding-top:1pt;padding-bottom:1pt;">parallel</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.18.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.18.10.1" style="padding-top:1pt;padding-bottom:1pt;">Activation</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.2" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.3" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.4" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.5" style="padding-top:1pt;padding-bottom:1pt;">GeLU</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.6" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.19.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.19.11.1" style="padding-top:1pt;padding-bottom:1pt;">Sequence length</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.2" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.3" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.4" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.5" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.6" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.20.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.20.12.1" style="padding-top:1pt;padding-bottom:1pt;">Batch size (instances)</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.2" style="padding-top:1pt;padding-bottom:1pt;">2160</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.3" style="padding-top:1pt;padding-bottom:1pt;">1024</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.4" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.5" style="padding-top:1pt;padding-bottom:1pt;">2304</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.6" style="padding-top:1pt;padding-bottom:1pt;">512</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.6" style="padding-top:1pt;padding-bottom:1pt;">Batch size (tokens)</th>
<td class="ltx_td ltx_align_left" id="S2.T2.4.4.1" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.4.4.1.m1.1"><semantics id="S2.T2.4.4.1.m1.1a"><mo id="S2.T2.4.4.1.m1.1.1" xref="S2.T2.4.4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.4.4.1.m1.1b"><csymbol cd="latexml" id="S2.T2.4.4.1.m1.1.1.cmml" xref="S2.T2.4.4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.4.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.4.4.1.m1.1d">∼</annotation></semantics></math>4M</td>
<td class="ltx_td ltx_align_left" id="S2.T2.5.5.2" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.5.5.2.m1.1"><semantics id="S2.T2.5.5.2.m1.1a"><mo id="S2.T2.5.5.2.m1.1.1" xref="S2.T2.5.5.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.5.5.2.m1.1b"><csymbol cd="latexml" id="S2.T2.5.5.2.m1.1.1.cmml" xref="S2.T2.5.5.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.5.2.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.5.5.2.m1.1d">∼</annotation></semantics></math>4M</td>
<td class="ltx_td ltx_align_left" id="S2.T2.6.6.3" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.6.6.3.m1.1"><semantics id="S2.T2.6.6.3.m1.1a"><mo id="S2.T2.6.6.3.m1.1.1" xref="S2.T2.6.6.3.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.6.6.3.m1.1b"><csymbol cd="latexml" id="S2.T2.6.6.3.m1.1.1.cmml" xref="S2.T2.6.6.3.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.6.6.3.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.6.6.3.m1.1d">∼</annotation></semantics></math>4M</td>
<td class="ltx_td ltx_align_left" id="S2.T2.7.7.4" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.7.7.4.m1.1"><semantics id="S2.T2.7.7.4.m1.1a"><mo id="S2.T2.7.7.4.m1.1.1" xref="S2.T2.7.7.4.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.7.7.4.m1.1b"><csymbol cd="latexml" id="S2.T2.7.7.4.m1.1.1.cmml" xref="S2.T2.7.7.4.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.7.4.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.7.7.4.m1.1d">∼</annotation></semantics></math>4M</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.8.5" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.8.8.5.m1.1"><semantics id="S2.T2.8.8.5.m1.1a"><mo id="S2.T2.8.8.5.m1.1.1" xref="S2.T2.8.8.5.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.8.8.5.m1.1b"><csymbol cd="latexml" id="S2.T2.8.8.5.m1.1.1.cmml" xref="S2.T2.8.8.5.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.8.5.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.8.8.5.m1.1d">∼</annotation></semantics></math>1M</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.21.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S2.T2.8.21.13.1" style="padding-top:1pt;padding-bottom:1pt;">Weight tying</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.2" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.3" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.4" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.5" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.6" style="padding-top:1pt;padding-bottom:1pt;">yes</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>LM architecture comparison at the 7–8B scale. In the “layer norm type” row, “parametric” and “non-parametric” refer to the usual layer norm implementation with and without adaptive gain and bias, respectively.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pretraining Data: Dolma</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Despite progress in access to model parameters, pretraining datasets are still not as open.
Pretraining data are often not released alongside open models (let alone closed models) and documentation about such data is often lacking in detail that would be needed to reproduce or fully understand the work.
This has made it difficult to support certain threads of language model research, such as understanding how training data impacts model capabilities and limitations.
To facilitate open research on language model pretraining, we built and released our pretraining dataset, Dolma—a diverse, multi-source corpus of 3T tokens across 5B documents acquired from 7 different data sources that are (1) commonly seen in large-scale language model pretraining and (2) accessible to the general public <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>.
Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.T3" title="Table 3 ‣ 2.2 Pretraining Data: Dolma ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> provides a high-level overview of the amount of data from each source.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Dolma is built using a pipeline of
(1) language filtering,
(2) quality filtering,
(3) content filtering,
(4) deduplication,
(5) multi-source mixing, and
(6) tokenization.
We refer the reader to the Dolma report&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Soldaini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite> for more details about its design principles, details about its construction, and a more detailed summary of its contents.
The report provides additional analyses and experimental results from training language models on intermediate states of Dolma to share what we learned about important data curation practices, including the role of content or quality filters, deduplication, and mixing data from multiple sources.
We keep documents from each source separate, both during curation as well as in the final release.
We open-sourced our high-performance data curation tools;
this toolkit can be used to further experiment on Dolma, reproduce our work, and enable fast and easy curation of pretraining corpora.
Finally, we also open-sourced our WIMBD tool&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Elazar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib24" title="">2023</a>)</cite> to help with dataset analysis.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S2.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.2.1">Doc Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.1.1.3.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.1.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.3.1.1.1.1">UTF-8</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.3.1.2.1.1">bytes</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.3.1.3">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.3.1.3.1"><span class="ltx_text ltx_font_italic" id="S2.T3.1.1.1.3.1.3.1.1">(GB)</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.1.1.4.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.1.1.4.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.4.1.1.1.1">Documents</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.4.1.2">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.4.1.2.1"><span class="ltx_text ltx_font_italic" id="S2.T3.1.1.1.4.1.2.1.1">(millions)</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.1.1.5.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.1.1.5.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.5.1.1.1.1">GPT-NeoX</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.5.1.2">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.5.1.2.1.1">tokens</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.5.1.3">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.5.1.3.1"><span class="ltx_text ltx_font_italic" id="S2.T3.1.1.1.5.1.3.1.1">(billions)</span></td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T3.1.2.1.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.2.1.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.2.1.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.2.1.1.1.1.1">Common Crawl</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.2.1.2">web pages</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.2.1.3">9,022</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.2.1.4">3,370</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.2.1.5">2,006</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.3.2.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.3.2.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.3.2.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.1.1.1.1">The Stack</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.2">code</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.3">1,043</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.4">210</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.5">342</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.4.3.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.4.3.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.4.3.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.1.1.1.1">C4</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.2">web pages</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.3">790</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.4">364</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.5">174</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.5.4.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.5.4.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.5.4.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.1.1.1.1">Reddit</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.2">social media</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.3">339</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.4">377</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.5">80</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.6.5.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.6.5.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.6.5.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.1.1.1.1">peS2o</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.2">STEM papers</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.3">268</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.4">38.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.5">57</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.7.6.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.7.6.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.7.6.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.1.1.1.1">Project Gutenberg</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.2">books</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.3">20.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.4">0.056</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.5">5.2</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.8.7.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.8.7.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.8.7.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.1.1.1.1">Wikipedia, Wikibooks</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.2">encyclopedic</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.3">16.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.4">6.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.5">3.7</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" colspan="2" id="S2.T3.1.9.8.1">
<span class="ltx_text ltx_font_bold" id="S2.T3.1.9.8.1.1">Total</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T3.1.9.8.2"><span class="ltx_text ltx_font_bold" id="S2.T3.1.9.8.2.1">11,519</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T3.1.9.8.3"><span class="ltx_text ltx_font_bold" id="S2.T3.1.9.8.3.1">4,367</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T3.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S2.T3.1.9.8.4.1">2,668</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Composition of Dolma.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Adaptation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Pretrained models are not always used as-is, but rather further fine-tuned to improve their performance, safety, and usability. Often models are first trained to follow instructions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Mishra et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib49" title="">2022</a>; Wei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib84" title="">2022</a>; Sanh et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib69" title="">2022</a>)</cite>, and then further trained on human preferences&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ouyang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib54" title="">2022</a>)</cite> to improve the quality of their generations. We showcase the efficacy of using OLMo as a base model for further fine-tuning by training OLMo to be a general chat assistant following our Open Instruct (<span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p1.1.1">Tülu</span>) data and training setup&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ivison et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>)</cite>. Our approach involves first performing instruction fine-tuning with a mixture of distilled and human-written instruction data and then further aligning the model with distilled preference data using Direct Preference Optimization (DPO)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rafailov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib62" title="">2023</a>)</cite>.
We experimented with mixing the Tulu instruction data at the end of pretraining, as done in recent models such as <cite class="ltx_cite ltx_citemacro_citet">DeepSeek-AI et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib21" title="">2024</a>)</cite>, but did not have conclusive findings.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">We perform base model evaluation at two stages: <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.1">online</em> evaluation to make decisions for model design and <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.2">offline</em> evaluation to evaluate model checkpoints.
For the offline stage, we use the Catwalk framework <cite class="ltx_cite ltx_citemacro_citep">(Groeneveld et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib28" title="">2023</a>)</cite>, a publicly available evaluation tool with access to a wide range of datasets and task formats. Using Catwalk, we perform downstream evaluation as well as intrinsic language modeling evaluation on the new perplexity benchmark, Paloma <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">For both downstream and perplexity evaluation, we use our fixed evaluation pipeline
to compare results against publicly available models. We also report a separate evaluation of our adapted model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">In-Loop Training Ablations</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px1.p1.1">Throughout model training, we perform downstream evaluations to make decisions around model architecture, initialization, optimizers, learning rate schedule, and data mixtures. We call this our <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS0.Px1.p1.1.1">online</em> evaluation as it runs in-loop every 1000 training steps (or <math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS4.SSS0.Px1.p1.1.m1.1a"><mo id="S2.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS0.Px1.p1.1.m1.1d">∼</annotation></semantics></math>4B training tokens) and provides an early and continuous signal on the quality of the model being trained. These evaluations rely on many of the core tasks and experiment settings used for our <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS0.Px1.p1.1.2">offline</em> evaluation detailed in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1" title="4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>, which also mirrors the task and evaluation structure of the EleutherAI eval harness&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib26" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Downstream Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p1.1">Following much previous work <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib12" title="">2020</a>; Black et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib10" title="">2022</a>; Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">b</a>, <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS0.Px2.p1.1.1.1">inter alia</em>)</cite>, we report zero-shot performance on a set of downstream tasks. Our evaluation suite consists of 8 core tasks corresponding closely to the commonsense reasoning task set reported by <cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite> (see Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T6" title="Table 6 ‣ Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a> for a list of tasks). Given the scale of the models being evaluated, such tasks were selected at the beginning of model development due to their naturalness (e.g., all can formulated as text completion scoring tasks) and ability to provide meaningful signals throughout training (see Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F1" title="Figure 1 ‣ Results ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Intrinsic Language Modeling Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px3.p1.1">To measure how OLMo-7B fits distributions of language beyond held-out training data, we use Paloma <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>, a new perplexity benchmark that includes 585 different domains of text. Domains range from nytimes.com to r/depression on Reddit and are drawn from 18 separate data sources, such as C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib63" title="">2020</a>)</cite>, in stratified samples. This allows for more equal inclusion of text domains that are under-represented in their source corpora.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS4.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS4.SSS0.Px3.p2.1">We aim not just to compare OLMo-7B against other models for best performance, but also to demonstrate how it enables fuller and more controlled scientific evaluations. OLMo-7B is the largest LM with explicit decontamination for perplexity evaluation. Following the approach described in Paloma, we remove any pretraining document with paragraphs leaked from Paloma evaluation data. Without decontamination, other models risk underestimating perplexity (i.e., overestimating the model’s out-of-sample fit). We also release intermediate checkpoints, allowing richer comparisons with two other models that release checkpoints, Pythia-6.9B <cite class="ltx_cite ltx_citemacro_citep">(Biderman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib7" title="">2023</a>)</cite> and RPJ-INCITE-7B <cite class="ltx_cite ltx_citemacro_citep">(Together Computer, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib76" title="">2023</a>)</cite> (see Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Adaptation Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px4.p1.1">We also follow our Open Instruct evaluation suite <cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib83" title="">2023</a>); Ivison et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>)</cite> to evaluate OLMo after instruction fine-tuning and DPO training using our
We focus on evaluations around model chat capabilities and safety to showcase the efficacy of using OLMo as a base for further fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Training OLMo</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section describes our pretraining setup, including our distributed training framework (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS1" title="3.1 Distributed Training Framework ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.1</span></a>), optimizer settings (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS2" title="3.2 Optimizer ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>), data preparation (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS3" title="3.3 Data ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.3</span></a>), and hardware (Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS4" title="3.4 Hardware ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.4</span></a>).
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Distributed Training Framework</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We train our models using the <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">ZeRO</em> optimizer strategy <cite class="ltx_cite ltx_citemacro_citep">(Rajbhandari et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib64" title="">2019</a>)</cite> via PyTorch’s <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.2">FSDP</span> framework <cite class="ltx_cite ltx_citemacro_citep">(Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib93" title="">2023</a>)</cite>, which reduces memory consumption by sharding the model weights and their corresponding optimizer state across GPUs.
At the 7B scale, this enables training with a micro-batch size of 4096 tokens per GPU on our hardware (see Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS4" title="3.4 Hardware ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.4</span></a>).
For OLMo-1B and -7B models, we use a constant global batch size of approximately 4M tokens (2048 instances, each with a sequence length of 2048 tokens).
For OLMo-65B model (currently training), we use a batch size warmup that starts at approximately 2M tokens (1024 instances), then doubles every 100B tokens until reaching approximately 16M tokens (8192 instances).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To improve throughput, we employ mixed-precision training <cite class="ltx_cite ltx_citemacro_citep">(Micikevicius et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib46" title="">2017</a>)</cite> through <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.1">FSDP</span>’s built-in settings and PyTorch’s <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.2">amp</span> module.
The latter ensures that certain operations like the softmax always run in full precision to improve stability, while all other operations run in half-precision with the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.3">bfloat16</span> format. Under our specific settings, the sharded model weights and optimizer state local to each GPU are kept in full precision.
The weights within each transformer block are only cast to <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.4">bfloat16</span> when the full-sized parameters are materialized on each GPU during the forward and backward passes.
Gradients are reduced across GPUs in full precision.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Optimizer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.3">We use the AdamW optimizer <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib41" title="">2019</a>)</cite> with the hyperparameters shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.T4" title="Table 4 ‣ 3.3 Data ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>.
For all model sizes, we warm up the learning rate over 5000 steps (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">∼</annotation></semantics></math>21B tokens) and then decay it linearly from there down to a tenth of the peak learning rate over the remainder of training.
After the warm-up period, we clip gradients such that the total <math alttext="l^{2}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">l</mi><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑙</ci><cn id="S3.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">l^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_l start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm of the parameter gradients<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>During gradient clipping all of the model’s parameters are treated as a single big vector (as if all parameters were flattened and concatenated together), and we take the <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="footnote3.m1.1"><semantics id="footnote3.m1.1b"><msub id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml"><mi id="footnote3.m1.1.1.2" mathvariant="normal" xref="footnote3.m1.1.1.2.cmml">ℓ</mi><mn id="footnote3.m1.1.1.3" xref="footnote3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><apply id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1"><csymbol cd="ambiguous" id="footnote3.m1.1.1.1.cmml" xref="footnote3.m1.1.1">subscript</csymbol><ci id="footnote3.m1.1.1.2.cmml" xref="footnote3.m1.1.1.2">ℓ</ci><cn id="footnote3.m1.1.1.3.cmml" type="integer" xref="footnote3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">\ell_{2}</annotation><annotation encoding="application/x-llamapun" id="footnote3.m1.1e">roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>-norm over the corresponding single gradient vector. This is the standard way to clip gradients in PyTorch.</span></span></span> does not exceed <math alttext="1.0" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn id="S3.SS2.p1.3.m3.1.1.cmml" type="float" xref="S3.SS2.p1.3.m3.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">1.0</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">1.0</annotation></semantics></math>. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.T5" title="Table 5 ‣ 3.3 Data ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">5</span></a> gives a comparison of our optimizer settings at the 7B scale to those of other recent LMs that also used AdamW.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We built our training dataset out of a 2T-token sample from our open dataset, Dolma <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>, which we describe in Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS2" title="2.2 Pretraining Data: Dolma ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.2</span></a>.
The tokens from every document are concatenated together after appending a special <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.1">EOS</span> token to the end of each document, and then we group consecutive chunks of 2048 tokens to form training instances.
The training instances are shuffled in the exact same way for each training run.
The data order and exact composition of each training batch can be reconstructed from the artifacts we release.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">All of our released models have been trained to at least 2T tokens (a single epoch over our training data), and some have been trained beyond that by starting a second epoch over the data with a different shuffling order. The impact of repeating this small amount of data should be negligible according to prior work&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib51" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T4.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.9.10.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T4.9.10.1.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.1.1">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.2.1">Peak LR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.3.1">Betas</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.4.1">Epsilon</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.5.1">Weight Decay</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.6" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.6.1">Batch Size (tokens)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T4.1.1.2" style="padding-top:1pt;padding-bottom:1pt;">1B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.1.1.3.1">4.0E-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.4" style="padding-top:1pt;padding-bottom:1pt;">(0.9, 0.95)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.1.1.5.1">1.0E-5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.6" style="padding-top:1pt;padding-bottom:1pt;">0.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.1.1.1.m1.1"><semantics id="S3.T4.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.m1.1d">∼</annotation></semantics></math>4M</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T4.2.2.2" style="padding-top:1pt;padding-bottom:1pt;">7B</th>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.2.2.3.1">3.0E-4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.4" style="padding-top:1pt;padding-bottom:1pt;">(0.9, 0.95)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.2.2.5.1">1.0E-5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.6" style="padding-top:1pt;padding-bottom:1pt;">0.1</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.1" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.2.2.1.m1.1"><semantics id="S3.T4.2.2.1.m1.1a"><mo id="S3.T4.2.2.1.m1.1.1" xref="S3.T4.2.2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.1.m1.1b"><csymbol cd="latexml" id="S3.T4.2.2.1.m1.1.1.cmml" xref="S3.T4.2.2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.2.2.1.m1.1d">∼</annotation></semantics></math>4M</td>
</tr>
<tr class="ltx_tr" id="S3.T4.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T4.9.9.8" style="padding-top:1pt;padding-bottom:1pt;">65B*</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.9" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.9.9.9.1">1.5E-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.10" style="padding-top:1pt;padding-bottom:1pt;">(0.9, 0.95)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.11" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.9.9.11.1">1.0E-5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.12" style="padding-top:1pt;padding-bottom:1pt;">0.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.7" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.3.3.1.m1.1"><semantics id="S3.T4.3.3.1.m1.1a"><mo id="S3.T4.3.3.1.m1.1.1" xref="S3.T4.3.3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.1.m1.1b"><csymbol cd="latexml" id="S3.T4.3.3.1.m1.1.1.cmml" xref="S3.T4.3.3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.3.3.1.m1.1d">∼</annotation></semantics></math>2M <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T4.4.4.2.m2.1"><semantics id="S3.T4.4.4.2.m2.1a"><mo id="S3.T4.4.4.2.m2.1.1" stretchy="false" xref="S3.T4.4.4.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.2.m2.1b"><ci id="S3.T4.4.4.2.m2.1.1.cmml" xref="S3.T4.4.4.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.4.4.2.m2.1d">→</annotation></semantics></math> <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.5.5.3.m3.1"><semantics id="S3.T4.5.5.3.m3.1a"><mo id="S3.T4.5.5.3.m3.1.1" xref="S3.T4.5.5.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.3.m3.1b"><csymbol cd="latexml" id="S3.T4.5.5.3.m3.1.1.cmml" xref="S3.T4.5.5.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.5.5.3.m3.1d">∼</annotation></semantics></math>4M <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T4.6.6.4.m4.1"><semantics id="S3.T4.6.6.4.m4.1a"><mo id="S3.T4.6.6.4.m4.1.1" stretchy="false" xref="S3.T4.6.6.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T4.6.6.4.m4.1b"><ci id="S3.T4.6.6.4.m4.1.1.cmml" xref="S3.T4.6.6.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.6.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.6.6.4.m4.1d">→</annotation></semantics></math> <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.7.7.5.m5.1"><semantics id="S3.T4.7.7.5.m5.1a"><mo id="S3.T4.7.7.5.m5.1.1" xref="S3.T4.7.7.5.m5.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.7.7.5.m5.1b"><csymbol cd="latexml" id="S3.T4.7.7.5.m5.1.1.cmml" xref="S3.T4.7.7.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.7.5.m5.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.7.7.5.m5.1d">∼</annotation></semantics></math>8M <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T4.8.8.6.m6.1"><semantics id="S3.T4.8.8.6.m6.1a"><mo id="S3.T4.8.8.6.m6.1.1" stretchy="false" xref="S3.T4.8.8.6.m6.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T4.8.8.6.m6.1b"><ci id="S3.T4.8.8.6.m6.1.1.cmml" xref="S3.T4.8.8.6.m6.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.8.8.6.m6.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.8.8.6.m6.1d">→</annotation></semantics></math> <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.9.9.7.m7.1"><semantics id="S3.T4.9.9.7.m7.1a"><mo id="S3.T4.9.9.7.m7.1.1" xref="S3.T4.9.9.7.m7.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.9.9.7.m7.1b"><csymbol cd="latexml" id="S3.T4.9.9.7.m7.1.1.cmml" xref="S3.T4.9.9.7.m7.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.9.9.7.m7.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.9.9.7.m7.1d">∼</annotation></semantics></math>16M</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>AdamW pretraining hyperparameters for OLMo models.
<br class="ltx_break">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
* <em class="ltx_emph ltx_font_italic" id="S3.T4.11.1">At the time of writing our 65B model is still training.</em></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T5.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T5.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.2.1">OLMo-7B</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.3.1">LLaMA2-7B</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.4.1">OpenLM-7B</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.5.1">Falcon-7B</span></th>
</tr>
<tr class="ltx_tr" id="S3.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T5.1.2.2.1">warmup steps</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T5.1.2.2.2">5000</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T5.1.2.2.3">2000</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T5.1.2.2.4">2000</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T5.1.2.2.5">1000</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T5.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.3.1.1">peak LR</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.3.1.2"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.3.1.2.1">3.0E-04</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.3.1.3"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.3.1.3.1">3.0E-04</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.3.1.4"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.3.1.4.1">3.0E-04</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.3.1.5"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.3.1.5.1">6.0E-04</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.4.2.1">minimum LR</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.4.2.2"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.4.2.2.1">3.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.4.2.3"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.4.2.3.1">3.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.4.2.4"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.4.2.4.1">3.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.4.2.5"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.4.2.5.1">1.2E-05</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.5.3.1">weight decay</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.5.3.2">0.1</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.5.3.3">0.1</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.5.3.4">0.1</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.5.3.5">0.1</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.6.4.1">beta1</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.6.4.2">0.9</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.6.4.3">0.9</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.6.4.4">0.9</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.6.4.5">0.99</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.7.5.1">beta2</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.7.5.2">0.95</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.7.5.3">0.95</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.7.5.4">0.95</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.7.5.5">0.999</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.8.6.1">epsilon</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.8.6.2"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.8.6.2.1">1.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.8.6.3"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.8.6.3.1">1.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.8.6.4"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.8.6.4.1">1.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.8.6.5"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.8.6.5.1">1.0E-05</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.9.7.1">LR schedule</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.9.7.2">linear</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.9.7.3">cosine</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.9.7.4">cosine</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.9.7.5">cosine</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.10.8.1">gradient clipping</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.10.8.2">global 1.0</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.10.8.3">global 1.0</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.10.8.4">global 1.0</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.10.8.5">global 1.0</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.11.9.1">gradient reduce dtype</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.11.9.2">FP32</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.11.9.3">FP32</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.11.9.4">FP32</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.11.9.5">BF16</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T5.1.12.10.1">optimizer state dtype</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.12.10.2">FP32</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.12.10.3">most likely FP32</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.12.10.4">FP32</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.12.10.5">FP32</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of pretraining optimizer settings at the 7B scale. Each model in this table used AdamW as its optimizer.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Hardware</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In order to verify that our codebase could be used on both NVIDIA and AMD GPUs without any loss in performance, we trained models on two different clusters:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">LUMI:</span> Provided by the LUMI supercomputer,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lumi-supercomputer.eu" title="">https://www.lumi-supercomputer.eu</a></span></span></span> we used up to 256 nodes on this cluster,
where each node consists of 4x AMD MI250X GPUs with 128GB of memory<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>The MI250X is a dual-chip module, meaning in practice that each physical device consists of two logical devices, so each node has 8 logical GPU devices with 64GB of memory each.</span></span></span> and 800Gbps of interconnect.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">MosaicML:</span> Provided by MosaicML<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mosaicml.com" title="">https://www.mosaicml.com</a></span></span></span> (Databricks), we used 27 nodes on this cluster, where each node consists of 8x NVIDIA A100 GPUs with 40GB of memory and 800Gbps interconnect.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS4.p1.2">Despite minor differences in batch size to optimize for training throughput, both runs resulted in nearly identical performance on our evaluation suite by 2T tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The checkpoint used for evaluating OLMo-7B is trained until 2.46T
tokens on the Dolma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Soldaini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite> dataset with a linear learning rate decay schedule mentioned in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS2" title="3.2 Optimizer ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>. In our experiments, we find that tuning this checkpoint further on the Dolma dataset for 1000 steps with the learning rate linearly decayed to 0 boosts model performance on perplexity and end-task evaluation suites described in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4" title="2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>. We compare OLMo with other publicly available models including LLaMA-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>)</cite>, LLaMA2-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite>, MPT-7B <cite class="ltx_cite ltx_citemacro_citep">(MosaicML NLP Team, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib50" title="">2023</a>)</cite>, Pythia-6.9B <cite class="ltx_cite ltx_citemacro_citep">(Biderman et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib7" title="">2023</a>)</cite>, Falcon-7B <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib2" title="">2023</a>)</cite> and RPJ-INCITE-7B <cite class="ltx_cite ltx_citemacro_citep">(Together Computer, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib76" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Downstream evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">Our core <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.1">downstream evaluation suite</span> (see Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T6" title="Table 6 ‣ Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>) consists of: arc (both <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.2">arc_easy</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.3">arc_challenge</span>) <cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib18" title="">2018</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.4">boolq</span> <cite class="ltx_cite ltx_citemacro_citep">(Clark et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib17" title="">2019</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.5">openbookqa</span> <cite class="ltx_cite ltx_citemacro_citep">(Mihaylov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib47" title="">2018</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.6">sciq</span> <cite class="ltx_cite ltx_citemacro_citep">(Welbl et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib85" title="">2017</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.7">hellaswag</span> <cite class="ltx_cite ltx_citemacro_citep">(Zellers et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib90" title="">2019</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.8">piqa</span> <cite class="ltx_cite ltx_citemacro_citep">(Bisk et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib9" title="">2020</a>)</cite>,
and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.9">winogrande</span> <cite class="ltx_cite ltx_citemacro_citep">(Sakaguchi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib68" title="">2021</a>)</cite>. In Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1" title="Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>, we also report results on an additional set of auxiliary tasks outside of our core evaluation set that we found to have less stable performance trends (see Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.F4" title="Figure 4 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.1">In all cases, we perform zero-shot evaluation using the rank classification approach popularized by <cite class="ltx_cite ltx_citemacro_citet">Brown et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib12" title="">2020</a>)</cite>. Under this approach, candidate text completions (e.g., different multiple-choice options) are ranked by likelihood (usually normalized by some normalization factor), and prediction accuracy is reported. While Catwalk implements several common likelihood normalization strategies, including normalizing by number of tokens (per-token normalization) <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib12" title="">2020</a>; Liang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib37" title="">2022</a>)</cite>, by number of characters (per-character normalization) <cite class="ltx_cite ltx_citemacro_citep">(Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib26" title="">2023</a>)</cite>, as well as incorporating an answer’s unconditional likelihood <cite class="ltx_cite ltx_citemacro_citep">(Brown et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib12" title="">2020</a>)</cite>, we selected the normalization strategies for each dataset separately. Specifically, we used unconditional normalization for <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.1">arc</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.2">openbookqa</span>, per-token normalization for <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.3">hellaswag</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.4">piqa</span>, and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.5">winogrande</span> and no normalization for <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.6">boolq</span>, and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.7">sciq</span> (i.e., tasks formulated as single token prediction tasks).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1.1">7B Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.2.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.2.1.1.1">arc</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.2.1.2.1">challenge</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.3.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.3.1.1.1">arc</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.3.1.2.1">easy</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.4">boolq</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.5.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.5.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.5.1.1.1">hella-</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.5.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.5.1.2.1">swag</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.6.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.6.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.6.1.1.1">open</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.6.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.6.1.2.1">bookqa</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.7">piqa</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.8">sciq</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T6.1.1.1.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.9.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.9.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.9.1.1.1">wino-</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.9.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.9.1.2.1">grande</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.10">avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T6.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.2.1.1.1">Falcon</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.2">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.3">70.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.4">74.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.5">75.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.6">53.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.7">78.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.8">93.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.1.2.1.9">68.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.10">70.3</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.3.2.1.1">LLaMA</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.2">44.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.3">67.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.4">75.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.5">76.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.6">51.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.7">77.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.8">93.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.3.2.9">70.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.10">69.6</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.4.3.1.1">Llama 2</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.2">48.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.3">69.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.4">80.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.5">76.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.6">48.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.7">76.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.8">94.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.4.3.9">69.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.10">70.5</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.5.4.1.1">MPT</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.2">46.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.3">70.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.4">74.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.5">77.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.6">48.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.7">77.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.8">93.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.5.4.9">69.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.10">69.8</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.6.5.1.1">Pythia</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.2">44.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.3">61.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.4">61.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.5">63.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.6">45.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.7">75.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.8">91.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.6.5.9">62.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.10">63.0</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.7.6.1.1">RPJ-INCITE</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.2">42.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.3">68.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.4">68.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.5">70.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.6">49.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.7">76.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.8">92.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.7.6.9">64.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.10">66.6</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.8.7" style="background-color:#40C4DF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T6.1.8.7.1"><span class="ltx_text" id="S4.T6.1.8.7.1.1" style="background-color:#40C4DF;">[] <span class="ltx_text ltx_font_bold" id="S4.T6.1.8.7.1.1.1">OLMo-7B</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.2"><span class="ltx_text" id="S4.T6.1.8.7.2.1" style="background-color:#40C4DF;">48.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.3"><span class="ltx_text" id="S4.T6.1.8.7.3.1" style="background-color:#40C4DF;">65.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.4"><span class="ltx_text" id="S4.T6.1.8.7.4.1" style="background-color:#40C4DF;">73.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.5"><span class="ltx_text" id="S4.T6.1.8.7.5.1" style="background-color:#40C4DF;">76.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.6"><span class="ltx_text" id="S4.T6.1.8.7.6.1" style="background-color:#40C4DF;">50.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.7"><span class="ltx_text" id="S4.T6.1.8.7.7.1" style="background-color:#40C4DF;">78.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.8"><span class="ltx_text" id="S4.T6.1.8.7.8.1" style="background-color:#40C4DF;">93.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.1.8.7.9"><span class="ltx_text" id="S4.T6.1.8.7.9.1" style="background-color:#40C4DF;">67.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.10"><span class="ltx_text" id="S4.T6.1.8.7.10.1" style="background-color:#40C4DF;">69.3</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Zero-shot evaluation of OLMo-7B and 6 other publicly available comparable model checkpoints on 8 core tasks from the downstream evaluation suite described in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px2" title="Downstream Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>. For OLMo-7B, we report results for the 2.46T token checkpoint.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T6" title="Table 6 ‣ Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes the result of zero-shot evaluation of OLMo-7B and compares it against 6 other publicly available models of comparable size. We report results on 8 core tasks from our evaluation suite described in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px2" title="Downstream Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>. On aggregate, OLMo-7B is competitive against all 6 publicly available model checkpoints in our comparison table.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p2.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F1" title="Figure 1 ‣ Results ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> we plot the accuracy score progression of 8 core end-tasks. All tasks, except OBQA, show an upward trend in accuracy numbers as OLMo-7B is trained on more tokens. A sharp upward tick in accuracy of many tasks between the last and the second to last step shows us the benefit of linearly reducing the LR to 0 over the final 1000 training steps. See Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.T9" title="Table 9 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">9</span></a> in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1" title="Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">A</span></a> for additional evaluation results and discussion.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F1">
<p class="ltx_p ltx_align_center" id="S4.F1.1"><span class="ltx_text" id="S4.F1.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="502" id="S4.F1.1.1.g1" src="x6.png" width="830">
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Accuracy score progression of OLMo-7B on 8 core end-tasks score from Catwalk evaluation suite described in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px2" title="Downstream Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>. We can see the benefit of decaying LR to 0 in the final 1000 steps of training on most tasks.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Intrinsic language modeling evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">For intrinsic evaluations, Paloma proposes a range of analyses, from inspection of performance in each domain separately to more summarized results over combinations of domains. We report results at two levels of granularity: the aggregate performance over 11 of the 18 sources in Paloma as in <cite class="ltx_cite ltx_citemacro_cite">Magnusson et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>, as well as more fine-grained results over each of these sources individually.
This particular subset of 11 sources from Paloma excludes sources that are not publicly available, involve fringe or toxic text, or consist of code data not supported by Paloma’s decontamination approach.
This leaves C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib63" title="">2020</a>)</cite>, mC4-en <cite class="ltx_cite ltx_citemacro_citep">(Chung et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib16" title="">2023</a>)</cite>, Wikitext 103 <cite class="ltx_cite ltx_citemacro_citep">(Merity et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib45" title="">2016</a>)</cite>, Penn Treebank <cite class="ltx_cite ltx_citemacro_citep">(Marcus et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib44" title="">1999</a>; Nunes, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib52" title="">2020</a>)</cite>, RedPajama <cite class="ltx_cite ltx_citemacro_citep">(Together Computer, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib76" title="">2023</a>)</cite>, Falcon-RefinedWeb <cite class="ltx_cite ltx_citemacro_citep">(Penedo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib57" title="">2023</a>)</cite>, Dolma <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>, M2D2 S2ORC <cite class="ltx_cite ltx_citemacro_citep">(Reid et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib65" title="">2022</a>)</cite>, M2D2 Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Reid et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib65" title="">2022</a>)</cite>, C4 100 domains <cite class="ltx_cite ltx_citemacro_citep">(Chronopoulou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib15" title="">2022</a>)</cite>, and Dolma 100 Subreddits <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>. To allow for a fair comparison between models with different vocabularies, we report bits per byte as defined by <cite class="ltx_cite ltx_citemacro_citet">Gao et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib25" title="">2020</a>)</cite> over the test sets of these sources.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">In the <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS0.Px2.p1.1.1">Sources Combined</span> subplot of Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>, we show the performance of OLMo-7B against 6 comparably-sized language models on the combination of 11 data sources from Paloma. Overall we find OLMo to have a competitive fit, especially given its training data was explicitly decontaminated against Paloma. As seen through the comparison of final models (see shapes) as well intermediate checkpoints (see dashed lines), the OLMo results follow similar scaling trends of other models. Note that the performance of intermediate checkpoints is influenced by where that checkpoint occurs in the learning rate schedule. So models trained for fewer steps will tend to have steeper training curves without necessarily being more sample efficient if training duration were fixed across all models. MPT-7B, nevertheless, stands out as improving ahead of the other models in this subplot. This could be due to a number of factors, including pretraining data composition and its match to the domains in Paloma (e.g., MPT trains on 27% non-Common Crawl data rather than 18% for LLaMA, 12.2% for RedPajama, and 11.2% for OLMo) as well as various data preprocessing decisions (e.g., MPT’s use of semantic deduplication by <cite class="ltx_cite ltx_citemacro_citep">Abbas et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib1" title="">2023</a></cite>, on C4).
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="617" id="S4.F2.g1" src="x7.png" width="831">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Bits per byte on 11 evaluation data sources from Paloma and their combination <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>, decontaminated from OLMo’s pretraining data. While models follow a general data scaling trend, sample efficiency is most favorable on in-distribution data. For example, OLMo-7B overtakes all other models on C4, perhaps from having 88.8% Common Crawl pretraining data.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">The remaining subplots in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> provide more fine-grained analysis by reporting bits per byte separately for each of the 11 data sources that are combined in the aggregated Paloma metric. From this we see greater variation in sample efficiency, largely driven by the similarity of training and evaluation distributions. Notably, OLMo-7B fares well on evaluations predominated by Common Crawl, such as C4, though different ways of postprocessing Common Crawl are best fit by models trained with that specific data, such as Falcon-7B on Falcon RefinedWeb. Meanwhile, OLMo-7B is less sample efficient compared to other models on sources less related to scraped web text, such as WikiText-103, M2D2 S2ORC, and M2D2 Wikipedia. The RedPajama evaluation shows a similar pattern, perhaps as only 2 of its 7 domains are from Common Crawl, and Paloma weights domains within each source equally. Since heterogeneous data from curated sources like Wikipedia and ArXiv papers is much less abundant than scraped web text, maintaining sample efficiency for fit to these distributions of language will be challenging as pretraining corpora are scaled.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Adaptation Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T7">
<p class="ltx_p" id="S4.T7.4"><span class="ltx_text" id="S4.T7.4.4">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T7.4.4.4" style="width:296.3pt;height:198pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T7.4.4.4.4"><span class="ltx_text" id="S4.T7.4.4.4.4.4" style="color:#000000;">
<span class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T7.4.4.4.4.4.4">
<span class="ltx_tbody">
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.5.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.1.1">Model</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.2.1">MMLU</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.3"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.3.1">AlpacaEval</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.4"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.4.1">ToxiGen</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.5"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.5.1">TruthfulQA</span></span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.4">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.4.5"></span>
<span class="ltx_td ltx_align_center" id="S4.T7.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.1.1.1.1.1.1">0-shot</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T7.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T7.1.1.1.1.1.1.1.1.m1.1a"><mo id="S4.T7.1.1.1.1.1.1.1.1.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T7.1.1.1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T7.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.1.1.1.1.1.1.1.1.m1.1d">↑</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S4.T7.2.2.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T7.2.2.2.2.2.2.2.2.1">%win</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T7.2.2.2.2.2.2.2.2.m1.1"><semantics id="S4.T7.2.2.2.2.2.2.2.2.m1.1a"><mo id="S4.T7.2.2.2.2.2.2.2.2.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T7.2.2.2.2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.2.2.2.2.2.m1.1b"><ci id="S4.T7.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T7.2.2.2.2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.2.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.2.2.2.2.2.2.2.2.m1.1d">↑</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S4.T7.3.3.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T7.3.3.3.3.3.3.3.3.1">% Toxic</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T7.3.3.3.3.3.3.3.3.m1.1"><semantics id="S4.T7.3.3.3.3.3.3.3.3.m1.1a"><mo id="S4.T7.3.3.3.3.3.3.3.3.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T7.3.3.3.3.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.3.3.3.3.3.m1.1b"><ci id="S4.T7.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S4.T7.3.3.3.3.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.3.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.3.3.3.3.3.3.3.3.m1.1d">↓</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.4.4.1">%Info+True</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T7.4.4.4.4.4.4.4.4.m1.1"><semantics id="S4.T7.4.4.4.4.4.4.4.4.m1.1a"><mo id="S4.T7.4.4.4.4.4.4.4.4.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T7.4.4.4.4.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.4.4.4.4.4.m1.1b"><ci id="S4.T7.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S4.T7.4.4.4.4.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.4.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.4.4.4.4.4.4.4.4.m1.1d">↑</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.6.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.6.2.1.1">OLMo (base)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.2">28.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.3">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.4">81.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.5">31.6</span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.7.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.7.3.1.1">MPT Chat</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.2">33.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.3">46.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.4">0.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.5">42.7</span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.8.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.8.4.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.8.4.1.1">Falcon Instruct</span></span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.8.4.2">25.2</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.8.4.3">14.0</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.8.4.4">70.7</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.8.4.5">27.2</span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.9.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.9.5.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.9.5.1.1">RPJ-INCITE Chat</span></span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.9.5.2">27.0</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.9.5.3">38.0</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.9.5.4">46.4</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.9.5.5">53.0</span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.10.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.10.6.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.10.6.1.1">Llama-2-Chat</span></span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.10.6.2">46.8</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.10.6.3">87.3</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.10.6.4">0.0</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.10.6.5">26.3</span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.11.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S4.T7.4.4.4.4.4.4.11.7.1.1">Tülu<span class="ltx_text ltx_font_upright" id="S4.T7.4.4.4.4.4.4.11.7.1.1.1"> 2</span></span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.2">50.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.3">73.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.4">7.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.5">51.7</span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.12.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.12.8.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S4.T7.4.4.4.4.4.4.12.8.1.1">Tülu<span class="ltx_text ltx_font_upright" id="S4.T7.4.4.4.4.4.4.12.8.1.1.1"> 2+DPO</span></span></span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.12.8.2">50.7</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.12.8.3">85.1</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.12.8.4">0.5</span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.12.8.5">- *</span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.13.9" style="background-color:#40C4DF;">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.13.9.1"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.1.1" style="background-color:#40C4DF;">[] <span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.13.9.1.1.1">OLMo +SFT</span></span></span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.13.9.2"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.2.1" style="background-color:#40C4DF;">47.3</span></span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.13.9.3"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.3.1" style="background-color:#40C4DF;">57.0</span></span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.13.9.4"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.4.1" style="background-color:#40C4DF;">14.4</span></span>
<span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.13.9.5"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.5.1" style="background-color:#40C4DF;">41.2</span></span></span>
<span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.14.10" style="background-color:#40C4DF;">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T7.4.4.4.4.4.4.14.10.1"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.1.1" style="background-color:#40C4DF;">[] <span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.14.10.1.1.1">OLMo +SFT+DPO</span></span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.4.4.4.4.4.14.10.2"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.2.1" style="background-color:#40C4DF;">46.2</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.4.4.4.4.4.14.10.3"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.3.1" style="background-color:#40C4DF;">69.3</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.4.4.4.4.4.14.10.4"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.4.1" style="background-color:#40C4DF;">1.7</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.4.4.4.4.4.14.10.5"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.5.1" style="background-color:#40C4DF;">52.0</span></span></span>
</span>
</span>
</span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Evaluation of various instruction-tuned 7B models, including OLMo-7B and before and after adaptation training. Lower is better for ToxiGen and higher is better for other metrics. We provide a detailed description of models and metrics in Appendix.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A3" title="Appendix C Adaptation Evaluation and Model details ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">C</span></a>. * Following <cite class="ltx_cite ltx_citemacro_citet">Ivison et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>)</cite>, we do not report <span class="ltx_text ltx_font_smallcaps" id="S4.T7.6.1">Tülu</span> 2 TruthfulQA scores due to test set contamination.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">We evaluate OLMo before adaptation, and after both the supervised fine-tuning and DPO training stage, focusing on the safety and chat evaluations used by <cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib83" title="">2023</a>)</cite>. We additionally compare to officially released instruction-tuned variants of the models from Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T6" title="Table 6 ‣ Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>. We finally also compare to <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px1.p1.1.1">Tülu</span> 2 models to compare against models trained using the same post-training data mixes and procedures.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">We find that instruction tuning considerably improves the performance and safety of OLMo, increasing MMLU performance by a wide margin and improving ToxiGen and TruthfulQA scores - especially after DPO training. Additionally, we find that OLMo outperforms most other chat variants after both initial instruction tuning (OLMo +SFT) and additional preference alignment (OLMo +SFT+DPO), highlighting both the strength of OLMo as a base model and the strength of the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.1">Tülu</span> mix used to perform adaptation training. However, we find there is still a gap with <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.2">Tülu</span> 2, which is trained by applying the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.3">Tülu</span> mix on Llama 2. This gap may be due to test set contamination in Llama 2<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite> report that Llama 2 was pretrained on data contaminated with MMLU test data.</span></span></span> and because the <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.4">Tülu</span> mix was primarily designed for Llama models - we will investigate the cause of this gap in future work. Overall, we see that OLMo greatly benefits from additional tuning and serves as a strong base model for downstream applications.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Power Consumption and Carbon Footprint</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Following previous literature <cite class="ltx_cite ltx_citemacro_citep">(Strubell et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib72" title="">2019</a>; Patterson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib56" title="">2021</a>; Wu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib86" title="">2022</a>; Dodge et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib22" title="">2022</a>)</cite>, we estimate the total energy consumed and carbon released while pretraining our models by calculating the total power consumption required for training, and then multiplying it by the carbon emission intensity of the power grid where the model was trained. While reporting these operational emissions is standard practice, it does not account for other sources of emissions such as the embodied emissions due to the manufacturing, transportation and disposal of hardware and datacenter infrastructure, lifetime operational emissions due to use, rebound effects, or other environmental impacts such as water consumption or mining. Thus our estimates should be viewed as lower bounds.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">We calculate the total power consumption for our models by measuring the power consumption of a single node every 25ms, calculating an average across the entire training run, and multiplying by the total number of nodes. We then account for the energy efficiency of the data center by multiplying the previous total by a power usage effectiveness (PUE) factor, which we set to 1.1, representing a conservative 10% energy consumption overhead typical of energy efficient datacenters.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nrel.gov/computational-science/measuring-efficiency-pue.html" title="">https://www.nrel.gov/computational-science/measuring-efficiency-pue.html</a></span></span></span><span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.google.com/about/datacenters/efficiency/" title="">https://www.google.com/about/datacenters/efficiency/</a></span></span></span> We estimate that pretraining our 7B models consumed <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">239 MWh</span> of energy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.3">To calculate carbon emissions, we multiply the total power consumption by a carbon intensity factor, measured in kg CO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><msub id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mi id="S4.SS4.p3.1.m1.1.1a" xref="S4.SS4.p3.1.m1.1.1.cmml"></mi><mn id="S4.SS4.p3.1.m1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><cn id="S4.SS4.p3.1.m1.1.1.1.cmml" type="integer" xref="S4.SS4.p3.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math> emitted per KWh, based on the physical location of the data center where each model was trained. The model trained on A100-40GB GPUs was trained in Australia, so we assume a carbon intensity factor of 0.610, the national average for Australia in 2022.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/december-quarter-2022/Emissions-Reduction.aspx" title="">https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/december-quarter-2022/Emissions-Reduction.aspx</a></span></span></span> The model trained on MI250X GPUs was trained in the LUMI supercomputer, which runs on 100% renewable, carbon-neutral energy, so we assume a carbon intensity factor of 0. LUMI is powered entirely by hydroelectric power and some sources <cite class="ltx_cite ltx_citemacro_citep">(Ubierna et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib79" title="">2022</a>)</cite> measure the carbon intensity factor of hydroelectric power to be 0.024, which would imply total carbon emissions of 3.54 tCO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><msub id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi id="S4.SS4.p3.2.m2.1.1a" xref="S4.SS4.p3.2.m2.1.1.cmml"></mi><mn id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><cn id="S4.SS4.p3.2.m2.1.1.1.cmml" type="integer" xref="S4.SS4.p3.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>eq.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lumi-supercomputer.eu" title="">https://www.lumi-supercomputer.eu</a></span></span></span> However, we rely on the official LUMI data for our calculations, and thus we estimate total pretraining emissions of <span class="ltx_text ltx_font_bold" id="S4.SS4.p3.3.1">69.78 tCO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.SS4.p3.3.1.m1.1"><semantics id="S4.SS4.p3.3.1.m1.1a"><msub id="S4.SS4.p3.3.1.m1.1.1" xref="S4.SS4.p3.3.1.m1.1.1.cmml"><mi id="S4.SS4.p3.3.1.m1.1.1a" xref="S4.SS4.p3.3.1.m1.1.1.cmml"></mi><mn id="S4.SS4.p3.3.1.m1.1.1.1" mathvariant="normal" xref="S4.SS4.p3.3.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.1.m1.1b"><apply id="S4.SS4.p3.3.1.m1.1.1.cmml" xref="S4.SS4.p3.3.1.m1.1.1"><cn id="S4.SS4.p3.3.1.m1.1.1.1.cmml" type="integer" xref="S4.SS4.p3.3.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.1.m1.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.3.1.m1.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>eq</span>.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>These metrics were in part collected using Carbonara’s AI agent and monitoring platform. Learn more at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trycarbonara.com" title="">https://trycarbonara.com</a></span></span></span> In Table <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#footnote12b" title="footnote 12 ‣ Table 8 ‣ 4.4 Power Consumption and Carbon Footprint ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">12</span></a> we compare our models with other previously released models based on publicly available information.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">We hope that openly releasing our models can reduce future emissions by allowing others to avoid the need to pretrain models from scratch, and give insights into the true cost of developing state of the art models. We also highlight that our estimates are lower bounds, because they do not include other critical pieces of development such as debugging, hyperparameter tuning, and downtime.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T8">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T8.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T8.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T8.2.2.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.2.2.4">GPU Type</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.2.2.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.2.2.5.1">
<tbody><tr class="ltx_tr" id="S4.T8.2.2.5.1.1">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.5.1.1.1">GPU Power</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.5.1.2">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.5.1.2.1">Consumption</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.5.1.3">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.5.1.3.1">(MWh)</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.2.2.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.2.2.6.1">
<tbody><tr class="ltx_tr" id="S4.T8.2.2.6.1.1">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.6.1.1.1">Power</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.6.1.2">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.6.1.2.1">Usage</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.6.1.3">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.6.1.3.1">Effectiveness</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.1.1.1.1">
<tbody><tr class="ltx_tr" id="S4.T8.1.1.1.1.2">
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.1.2.1">Carbon</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.1.3.1">Intensity</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.1.1.1">
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.1.1.1">(kg CO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.T8.1.1.1.1.1.1.m1.1"><semantics id="S4.T8.1.1.1.1.1.1.m1.1a"><msub id="S4.T8.1.1.1.1.1.1.m1.1.1" xref="S4.T8.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T8.1.1.1.1.1.1.m1.1.1a" xref="S4.T8.1.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T8.1.1.1.1.1.1.m1.1.1.1" xref="S4.T8.1.1.1.1.1.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T8.1.1.1.1.1.1.m1.1b"><apply id="S4.T8.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T8.1.1.1.1.1.1.m1.1.1"><cn id="S4.T8.1.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T8.1.1.1.1.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.1.1.1.1.1.1.m1.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T8.1.1.1.1.1.1.m1.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>e/KWh)</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.2.2.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.2.2.2.1">
<tbody><tr class="ltx_tr" id="S4.T8.2.2.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.2.1.2.1">Carbon</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.2.1.3">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.2.1.3.1">Emissions</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.2.1.1.1">(tCO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.T8.2.2.2.1.1.1.m1.1"><semantics id="S4.T8.2.2.2.1.1.1.m1.1a"><msub id="S4.T8.2.2.2.1.1.1.m1.1.1" xref="S4.T8.2.2.2.1.1.1.m1.1.1.cmml"><mi id="S4.T8.2.2.2.1.1.1.m1.1.1a" xref="S4.T8.2.2.2.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T8.2.2.2.1.1.1.m1.1.1.1" xref="S4.T8.2.2.2.1.1.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T8.2.2.2.1.1.1.m1.1b"><apply id="S4.T8.2.2.2.1.1.1.m1.1.1.cmml" xref="S4.T8.2.2.2.1.1.1.m1.1.1"><cn id="S4.T8.2.2.2.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T8.2.2.2.1.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.2.2.2.1.1.1.m1.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T8.2.2.2.1.1.1.m1.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>eq)</td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T8.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.3.1.1.1">Gopher-280B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.2">TPU v3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.3">1,066</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.4">1.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.5">0.330</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.6">380</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.4.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.4.2.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.4.2.1.1">BLOOM-176B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.2">A100-80GB</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.3">433</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.4">1.2</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.5">0.057</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.6">30</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.5.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.5.3.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.5.3.1.1">OPT-175B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.2">A100-80GB</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.3">324</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.4">1.1</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.5">0.231</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.6">82</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.6.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.6.4.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.6.4.1.1">T5-11B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.2">TPU v3</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.3">77</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.4">1.12</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.5">0.545</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.6">47</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.7.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.7.5.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.7.5.1.1">LLaMA-7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.2">A100-80GB</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.3">33</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.4">1.1</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.5">0.385</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.6">14</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.8.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.8.6.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.8.6.1.1">LLaMA2-7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.2">A100-80GB</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.3">74</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.4">1.1</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.5">0.385</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.6">31</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.9.7" style="background-color:#40C4DF;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.9.7.1"><span class="ltx_text" id="S4.T8.2.9.7.1.1" style="background-color:#40C4DF;">[]
<span class="ltx_text ltx_font_bold" id="S4.T8.2.9.7.1.1.1">OLMo-7B</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.2"><span class="ltx_text" id="S4.T8.2.9.7.2.1" style="background-color:#40C4DF;">MI250X</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.3"><span class="ltx_text" id="S4.T8.2.9.7.3.1" style="background-color:#40C4DF;">135</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.4"><span class="ltx_text" id="S4.T8.2.9.7.4.1" style="background-color:#40C4DF;">1.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.5"><span class="ltx_text" id="S4.T8.2.9.7.5.1" style="background-color:#40C4DF;">0.000*</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.6"><span class="ltx_text" id="S4.T8.2.9.7.6.1" style="background-color:#40C4DF;">0*</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.10.8" style="background-color:#40C4DF;">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T8.2.10.8.1"><span class="ltx_text" id="S4.T8.2.10.8.1.1" style="background-color:#40C4DF;">[]
<span class="ltx_text ltx_font_bold" id="S4.T8.2.10.8.1.1.1">OLMo-7B</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.2"><span class="ltx_text" id="S4.T8.2.10.8.2.1" style="background-color:#40C4DF;">A100-40GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.3"><span class="ltx_text" id="S4.T8.2.10.8.3.1" style="background-color:#40C4DF;">104</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.4"><span class="ltx_text" id="S4.T8.2.10.8.4.1" style="background-color:#40C4DF;">1.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.5"><span class="ltx_text" id="S4.T8.2.10.8.5.1" style="background-color:#40C4DF;">0.610</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.6"><span class="ltx_text" id="S4.T8.2.10.8.6.1" style="background-color:#40C4DF;">70</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>CO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.T8.5.m1.1"><semantics id="S4.T8.5.m1.1b"><msub id="S4.T8.5.m1.1.1" xref="S4.T8.5.m1.1.1.cmml"><mi id="S4.T8.5.m1.1.1b" xref="S4.T8.5.m1.1.1.cmml"></mi><mn id="S4.T8.5.m1.1.1.1" xref="S4.T8.5.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T8.5.m1.1c"><apply id="S4.T8.5.m1.1.1.cmml" xref="S4.T8.5.m1.1.1"><cn id="S4.T8.5.m1.1.1.1.cmml" type="integer" xref="S4.T8.5.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.5.m1.1d">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T8.5.m1.1e">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math> emissions during pretraining. We estimate the total carbon emissions for various models using publicly available data on PUE, carbon intensity of local power grid, and reported power consumption. Numbers for Gopher-280B <cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib61" title="">2022</a>)</cite>, BLOOM-176B <cite class="ltx_cite ltx_citemacro_citep">(Luccioni et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib42" title="">2022</a>)</cite>, OPT-175B <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib92" title="">2022</a>)</cite>, T5-11B <cite class="ltx_cite ltx_citemacro_citep">(Patterson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib56" title="">2021</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>)</cite>, and LLaMA2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite> are taken from their respective papers. See Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS4" title="4.4 Power Consumption and Carbon Footprint ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.4</span></a> for details on how tCO2eq was calculated.
<br class="ltx_break">* LUMI runs entirely on hydroelectric power<span class="ltx_note ltx_role_footnotemark" id="footnote12b"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">12</span></span></span></span>and some estimates <cite class="ltx_cite ltx_citemacro_citep">(Ubierna et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib79" title="">2022</a>)</cite> measure the intensity factor of hydroelectric power to be 0.024, implying total emissions of 3.54 tCO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.T8.6.m2.1"><semantics id="S4.T8.6.m2.1b"><msub id="S4.T8.6.m2.1.1" xref="S4.T8.6.m2.1.1.cmml"><mi id="S4.T8.6.m2.1.1b" xref="S4.T8.6.m2.1.1.cmml"></mi><mn id="S4.T8.6.m2.1.1.1" xref="S4.T8.6.m2.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T8.6.m2.1c"><apply id="S4.T8.6.m2.1.1.cmml" xref="S4.T8.6.m2.1.1"><cn id="S4.T8.6.m2.1.1.1.cmml" type="integer" xref="S4.T8.6.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.6.m2.1d">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T8.6.m2.1e">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>eq.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Artifacts Released</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">By sharing artifacts from all pipeline stages, we aim to encourage open research and reduce duplicated, often costly efforts, by academics and practitioners. We release the following:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.p2">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">The training and modeling code.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo" title="">https://github.com/allenai/OLMo</a></span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">The trained model weights for the 7B model,<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B" title="">https://huggingface.co/allenai/OLMo-7B</a></span></span></span> 7B-twin-2T,<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B-Twin-2T" title="">https://huggingface.co/allenai/OLMo-7B-Twin-2T</a></span></span></span> and the 1B model.<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-1B" title="">https://huggingface.co/allenai/OLMo-1B</a></span></span></span> For all the models, we release not only the final model weights but also 500+ intermediate checkpoints at intervals of 1000 steps. </p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Adapted OLMo-7B with instruction-tuning, 7B-SFT<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B-SFT" title="">https://huggingface.co/allenai/OLMo-7B-SFT</a></span></span></span>, and RLHF, 7B-Instruct<span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B-Instruct" title="">https://huggingface.co/allenai/OLMo-7B-Instruct</a></span></span></span> including its training and evaluation code and data using our Open Instruct<span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/open-instruct" title="">https://github.com/allenai/open-instruct</a></span></span></span> library&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib83" title="">2023</a>; Ivison et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">The training data Dolma&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Soldaini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/allenai/dolma" title="">https://huggingface.co/datasets/allenai/dolma</a></span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">Dolma’s toolkit to construct new datasets,<span class="ltx_note ltx_role_footnote" id="footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/dolma" title="">https://github.com/allenai/dolma</a></span></span></span> and WIMBD&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Elazar et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib24" title="">2023</a>)</cite> for dataset analysis.<span class="ltx_note ltx_role_footnote" id="footnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/wimbd" title="">https://github.com/allenai/wimbd</a></span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.1">The evaluation code<span class="ltx_note ltx_role_footnote" id="footnote23"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo-Eval" title="">https://github.com/allenai/OLMo-Eval</a></span></span></span> using Catwalk<span class="ltx_note ltx_role_footnote" id="footnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/catwalk" title="">https://github.com/allenai/catwalk</a></span></span></span> for downstream evaluation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Groeneveld et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib28" title="">2023</a>)</cite> and Paloma<span class="ltx_note ltx_role_footnote" id="footnote25"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://paloma.allen.ai" title="">https://paloma.allen.ai</a></span></span></span> for perplexity-based evaluation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Magnusson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S5.I1.i7.p1">
<p class="ltx_p" id="S5.I1.i7.p1.1">The complete set of metrics logged to Weights &amp; Biases during training.<span class="ltx_note ltx_role_footnote" id="footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5" title="">https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5</a></span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">We intend to follow up on this release with further training logs, ablations, and findings.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>License</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our goal is to facilitate scientific development and empower the scientific community, so we favor permissive licenses that give users flexibility in using our resources and artifacts.
As such, all code and weights are released under the Apache 2.0 License.<span class="ltx_note ltx_role_footnote" id="footnote27"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.apache.org/licenses/LICENSE-2.0" title="">http://www.apache.org/licenses/LICENSE-2.0</a></span></span></span>
Some licenses used by other organizations for recent model releases prohibit using the outputs from their models to train artificial intelligence or machine learning systems, while we expressly allow users to do so. We also do not limit commercial use. We hope that our models can make other models better.
We recognize that the risk for misuse of our models is relatively low since they are mainly designed as scientific artifacts not as products with broad public adoption (our models have not been adapted as chatbots). In addition, over the past year there have been a number of comparable models released with very permissive licenses, so using a more strict license for our work will not remove the overall risk in the field. We believe this tradeoff on the side of being more open is the best option.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This technical report presents our first release of OLMo, a state-of-the-art, truly open language model and its framework to build and study the science of language modeling.
Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code.
Soon, we will also release training logs, ablations, findings and Weights &amp; Biases logs.
We are also exploring the adaptation of OLMo with instruction tuning and different flavors of RLHF. We are going to release the adapted models as well as all of our model adaptation code and data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">We intend to continuously support and extend OLMo and its framework, and continue to push the boundaries of open LMs to empower the open research community. To that end, we look forward to bringing different model sizes, modalities, datasets, safety measures, and evaluations into the OLMo family.
We hope this and future releases will empower and strengthen the open research community and inspire a new wave of innovation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Author Contributions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">OLMo would not have been possible without the help of our many teammates and collaborators. We list author contributions (in alphabetical order) below:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Contributors to <span class="ltx_text ltx_font_bold" id="Sx1.p2.1.1">pretraining dataset construction and tooling</span> (Dolma) include Russell Authur, Iz Beltagy, Akshita Bhagia, Khyathi Chandu, Jesse Dodge, Yanai Elazar, Dirk Groeneveld, Rodney Kinney, Kyle Lo, Aakanksha Naik, Abhilasha Ravichander, Dustin Schwenk, Luca Soldaini, and Nishant Subramani.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Contributors to <span class="ltx_text ltx_font_bold" id="Sx1.p3.1.1">model training and architecture</span> include Shane Arora, Iz Beltagy, Akshita Bhagia, Matthew E. Peters, Dirk Groeneveld, Ananya Harsh Jha, William Merrill, Jacob Morrison, Niklas Muennighoff, Dustin Schwenk, Saurabh Shah, Pete Walsh, and Mitchell Wortsman.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">Contributors to <span class="ltx_text ltx_font_bold" id="Sx1.p4.1.1">evaluation suite and tooling</span> include Akshita Bhagia, Arman Cohan, Pradeep Dasigi, Jesse Dodge, Dirk Groeneveld, Yuling Gu, Tushar Khot, Ian Magnusson, Kyle Richardson, Oyvind Tajford, and Pete Walsh.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">Contributors to <span class="ltx_text ltx_font_bold" id="Sx1.p5.1.1">model adaptation</span> include Iz Beltagy, Pradeep Dasigi, Jack Hessel, Hamish Ivison, Nathan Lambert, Valentina Pyatkin, Pete Walsh, and Yizhong Wang.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">Contributors to <span class="ltx_text ltx_font_bold" id="Sx1.p6.1.1">license creation and risk assessment</span> include David Atkinson, Jesse Dodge, Jennifer Dumas, Crystal Nam, and Will Smith.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx1.p7">
<p class="ltx_p" id="Sx1.p7.1">The OLMo project was led by Hannaneh Hajishirzi and Noah A. Smith.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">OLMo would not have been possible without the support of many individuals and institutions.
The experimental components of this work were made possible through a partnership with AMD and CSC, enabling use of the LUMI supercomputer, and Kempner Institute at Harvard University.
We thank Jonathan Frankle and the team at MosaicML (now Databricks) for sharing their experiences with FSDP, and building the code base that OLMo is based on.
We thank our teammates Taira Anderson, Michelle Benedict, Jon Borchardt, Evie Cheng, Arnavi Chheda, Johann Dahm, Matt Latzke, Kelsey MacMillan, Aaron Sarnat, Carissa Schoenick, Sam Skjonsberg, Michael Schmitz, Michael Wilson, Caitlin Wittlif, and the entire IT team, for their help with the website, design, internal and external communications, budgeting, and other activities that supported smooth progress on this project.
Finally, we also express gratitude for the helpful discussions and feedback from our teammates at AI2 and close collaborators, including Prithviraj (Raj) Ammanabrolu, Peter Clark, Nicole DeCario, Doug Downey, Ali Farhadi, Ian Ferreira, Väinö Hatanpää, Sham M. Kakade, Julien Launay, Sydney Levine, Pekka Manninen, Franzi Roessner, Maarten Sap, Ludwig Schmidt, Yulia Tsvetkov, and Daniel S. Weld.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbas et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari&nbsp;S Morcos.

</span>
<span class="ltx_bibblock">Semdedup: Data-efficient learning at web-scale through semantic
deduplication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.09540</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.09540" title="">https://arxiv.org/abs/2303.09540</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almazrouei et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
Ruxandra-Aimée Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic,
Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo.

</span>
<span class="ltx_bibblock">The falcon series of open language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">ArXiv</em>, abs/2311.16867, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:265466629" title="">https://api.semanticscholar.org/CorpusID:265466629</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anand et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy
Mulyar.

</span>
<span class="ltx_bibblock">Gpt4all: Training an assistant-style chatbot with large scale data
distillation from gpt-3.5-turbo.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nomic-ai/gpt4all" title="">https://github.com/nomic-ai/gpt4all</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba et&nbsp;al. [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jimmy Ba, Jamie&nbsp;Ryan Kiros, and Geoffrey&nbsp;E. Hinton.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ArXiv</em>, abs/1607.06450, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:8236317" title="">https://api.semanticscholar.org/CorpusID:8236317</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,
Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage,
Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,
Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning
from human feedback, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et&nbsp;al. [2003]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">J. Mach. Learn. Res.</em>, 3:1137–1155, 2003.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:221275765" title="">https://api.semanticscholar.org/CorpusID:221275765</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stella Biderman, Hailey Schoelkopf, Quentin&nbsp;Gregory Anthony, Herbie Bradley,
Kyle O’Brien, Eric Hallahan, Mohammad&nbsp;Aflah Khan, Shivanshu Purohit,
Usvsn&nbsp;Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar
Van Der&nbsp;Wal.

</span>
<span class="ltx_bibblock">Pythia: A suite for analyzing large language models across training
and scaling.

</span>
<span class="ltx_bibblock">In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 40th
International Conference on Machine Learning</em>, volume 202 of
<em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">Proceedings of Machine Learning Research</em>, pages 2397–2430. PMLR,
23–29 Jul 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v202/biderman23a.html" title="">https://proceedings.mlr.press/v202/biderman23a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BigScience et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
BigScience, Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana
Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni,
François Yvon, et&nbsp;al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2211.05100</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the AAAI conference on artificial
intelligence</em>, volume&nbsp;34, pages 7432–7439, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/6239" title="">https://ojs.aaai.org/index.php/AAAI/article/view/6239</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,
USVSN&nbsp;Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben
Wang, and Samuel Weinbach.

</span>
<span class="ltx_bibblock">GPT-NeoX-20B: An open-source autoregressive language model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the ACL Workshop on Challenges &amp;
Perspectives in Creating Large Language Models</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.06745" title="">https://arxiv.org/abs/2204.06745</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blodgett et&nbsp;al. [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Su&nbsp;Lin Blodgett, Lisa Green, and Brendan O’Connor.

</span>
<span class="ltx_bibblock">Demographic dialectal variation in social media: A case study of
African-American English.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1119–1130, Austin, Texas, November 2016.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/D16-1120" title="">10.18653/v1/D16-1120</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D16-1120" title="">https://aclanthology.org/D16-1120</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T.&nbsp;J.
Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeff Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">ArXiv</em>, abs/2005.14165, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:218971783" title="">https://api.semanticscholar.org/CorpusID:218971783</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E. Gonzalez, Ion Stoica, and
Eric&nbsp;P. Xing.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality, March 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">https://lmsys.org/blog/2023-03-30-vicuna/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
Abhishek Rao, Parker Barnes, Yi&nbsp;Tay, Noam Shazeer, Vinodkumar Prabhakaran,
Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
Dohan, Shivani Agrawal, Mark Omernick, Andrew&nbsp;M. Dai,
Thanumalayan&nbsp;Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.02311" title="">https://arxiv.org/abs/2204.02311</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chronopoulou et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexandra Chronopoulou, Matthew Peters, and Jesse Dodge.

</span>
<span class="ltx_bibblock">Efficient hierarchical domain adaptation for pretrained language
models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 1336–1351, Seattle, United States, July 2022.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.naacl-main.96" title="">10.18653/v1/2022.naacl-main.96</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.naacl-main.96" title="">https://aclanthology.org/2022.naacl-main.96</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Noah Constant, Xavier García, Adam Roberts, Yi&nbsp;Tay, Sharan
Narang, and Orhan Firat.

</span>
<span class="ltx_bibblock">Unimax: Fairer and more effective language sampling for large-scale
multilingual pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">ArXiv</em>, abs/2304.09151, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258187051" title="">https://api.semanticscholar.org/CorpusID:258187051</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1905.10044</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning
challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1803.05457" title="">https://arxiv.org/abs/1803.05457</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali
Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.

</span>
<span class="ltx_bibblock">Free dolly: Introducing the world’s first truly open
instruction-tuned llm, 2023.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm" title="">https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,
Zhiyuan Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">Ultrafeedback: Boosting language models with high-quality feedback,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai,
Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige
Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao,
Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi
Li, Yao Li, Y.&nbsp;K. Li, Wenfeng Liang, Fangyun Lin, A.&nbsp;X. Liu, Bo&nbsp;Liu, Wen Liu,
Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong
Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren,
Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su,
Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu
Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y.&nbsp;Wu, Xin Xie, Zhenda Xie, Ziwei
Xie, Yiliang Xiong, Hanwei Xu, R.&nbsp;X. Xu, Yanhong Xu, Dejian Yang, Yuxiang
You, Shuiping Yu, Xingkai Yu, B.&nbsp;Zhang, Haowei Zhang, Lecong Zhang, Liyue
Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang
Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou.

</span>
<span class="ltx_bibblock">Deepseek llm: Scaling open-source language models with longtermism,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jesse Dodge, Taylor Prewitt, Remi Tachet&nbsp;Des Combes, Erika Odmark, Roy
Schwartz, Emma Strubell, Alexandra&nbsp;Sasha Luccioni, Noah&nbsp;A. Smith, Nicole
DeCario, and Will Buchanan.

</span>
<span class="ltx_bibblock">Measuring the carbon intensity of ai in cloud instances, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3531146.3533234" title="">https://dl.acm.org/doi/10.1145/3531146.3533234</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dolan and Brockett [2005]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William&nbsp;B. Dolan and Chris Brockett.

</span>
<span class="ltx_bibblock">Automatically constructing a corpus of sentential paraphrases.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">International Joint Conference on Natural Language
Processing</em>, 2005.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/research/publication/automatically-constructing-a-corpus-of-sentential-paraphrases/" title="">https://www.microsoft.com/en-us/research/publication/automatically-constructing-a-corpus-of-sentential-paraphrases/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elazar et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yanai Elazar, Akshita Bhagia, Ian&nbsp;H. Magnusson, Abhilasha Ravichander, Dustin
Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer
Singh, Hanna Hajishirzi, Noah&nbsp;A. Smith, and Jesse Dodge.

</span>
<span class="ltx_bibblock">What’s in my big data?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ArXiv</em>, abs/2310.20707, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:264803575" title="">https://api.semanticscholar.org/CorpusID:264803575</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et&nbsp;al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2101.00027</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2101.00027" title="">https://arxiv.org/abs/2101.00027</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noac’h,
Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang,
Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric
Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation, 12 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/10256836" title="">https://zenodo.org/records/10256836</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Greenbaum and Nelson [1996]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sidney Greenbaum and Gerald Nelson.

</span>
<span class="ltx_bibblock">The international corpus of english (ICE) project.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">World Englishes</em>, 15(1):3–15, mar 1996.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1111/j.1467-971x.1996.tb00088.x" title="">10.1111/j.1467-971x.1996.tb00088.x</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1111%2Fj.1467-971x.1996.tb00088.x" title="">https://doi.org/10.1111%2Fj.1467-971x.1996.tb00088.x</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groeneveld et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dirk Groeneveld, Anas Awadalla, Iz&nbsp;Beltagy, Akshita Bhagia, Ian Magnusson, Hao
Peng, Oyvind Tafjord, Pete Walsh, Kyle Richardson, and Jesse Dodge.

</span>
<span class="ltx_bibblock">Catwalk: A unified language model evaluation framework for many
datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2312.10253</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.10253" title="">https://arxiv.org/abs/2312.10253</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding,
Jianwei Yue, and Yupeng Wu.

</span>
<span class="ltx_bibblock">How close is chatgpt to human experts? comparison corpus, evaluation,
and detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arxiv:2301.07597</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suchin Gururangan, Mitchell Wortsman, Samir&nbsp;Yitzhak Gadre, Achal Dave, Maciej
Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt
Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and
Ludwig Schmidt.

</span>
<span class="ltx_bibblock">OpenLM: a minimal but performative language modeling (lm)
repository, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mlfoundations/open_lm/" title="">https://github.com/mlfoundations/open_lm/</a>.

</span>
<span class="ltx_bibblock">GitHub repository.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartvigsen et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
and Ece Kamar.

</span>
<span class="ltx_bibblock">TOXIGEN: Controlling Language Models to Generate Implied and
Adversarial Toxicity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">ACL</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.09509" title="">https://arxiv.org/abs/2203.09509</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the International Conference on Learning
Representations (ICLR)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivison et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters,
Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A. Smith, Iz&nbsp;Beltagy, and
Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Camels in a changing climate: Enhancing lm adaptation with tulu 2,
2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.10702" title="">https://arxiv.org/abs/2311.10702</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
Savary, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Emma&nbsp;Bou
Hanna, Florian Bressand, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2401.04088</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.04088" title="">https://arxiv.org/abs/2401.04088</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis,
Zhi&nbsp;Rui Tam, Keith Stevens, Abdullah Barhoum, Duc&nbsp;Minh Nguyen, Oliver
Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David&nbsp;Alexandrovich
Glushkov, Arnav&nbsp;Varma Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu
Nguyen, and Alexander&nbsp;Julian Mattick.

</span>
<span class="ltx_bibblock">Openassistant conversations - democratizing large language model
alignment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Thirty-seventh Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=VSJotgbPHF" title="">https://openreview.net/forum?id=VSJotgbPHF</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock">Github repository, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/alpaca_eval" title="">https://github.com/tatsu-lab/alpaca_eval</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2211.09110</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.09110" title="">https://arxiv.org/abs/2211.09110</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.

</span>
<span class="ltx_bibblock">Logiqa: A challenge dataset for machine reading comprehension with
logical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">CoRR</em>, abs/2007.08124, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2007.08124" title="">https://arxiv.org/abs/2007.08124</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua
Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llm360: Towards fully transparent open-source llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2312.06550</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.06550" title="">https://arxiv.org/abs/2312.06550</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">International Conference on Learning Representations</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">https://openreview.net/forum?id=Bkg6RiCqY7</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luccioni et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexandra&nbsp;Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.

</span>
<span class="ltx_bibblock">Estimating the carbon footprint of bloom, a 176b parameter language
model, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.02001" title="">https://arxiv.org/abs/2211.02001</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Magnusson et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya&nbsp;Harsh
Jha, Oyvind Tafjord, Dustin Schwenk, Evan&nbsp;Pete Walsh, Yanai Elazar, Kyle Lo,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Paloma: A benchmark for evaluating language model fit.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2312.10523</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcus et&nbsp;al. [1999]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mitchell&nbsp;P. Marcus, Beatrice Santorini, Mary&nbsp;Ann Marcinkiewicz, and Ann Taylor.

</span>
<span class="ltx_bibblock">Treebank-3, 1999.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/LDC99T42" title="">https://catalog.ldc.upenn.edu/LDC99T42</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merity et&nbsp;al. [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ArXiv</em>, abs/1609.07843, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:16299141" title="">https://api.semanticscholar.org/CorpusID:16299141</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micikevicius et&nbsp;al. [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory&nbsp;Frederick Diamos,
Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii
Kuchaiev, Ganesh Venkatesh, and Hao Wu.

</span>
<span class="ltx_bibblock">Mixed precision training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">ArXiv</em>, abs/1710.03740, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:3297437" title="">https://api.semanticscholar.org/CorpusID:3297437</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book
question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:1809.02789</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1809.02789" title="">https://arxiv.org/abs/1809.02789</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et&nbsp;al. [2013]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory&nbsp;S. Corrado, and Jeffrey Dean.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Neural Information Processing Systems</em>, 2013.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:16447573" title="">https://api.semanticscholar.org/CorpusID:16447573</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing
instructions.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, pages 3470–3487, Dublin,
Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.acl-long.244" title="">10.18653/v1/2022.acl-long.244</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.244" title="">https://aclanthology.org/2022.acl-long.244</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MosaicML NLP Team [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
MosaicML NLP Team.

</span>
<span class="ltx_bibblock">Introducing mpt-7b: A new standard for open-source, commercially
usable llms, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="www.mosaicml.com/blog/mpt-7b" title="">www.mosaicml.com/blog/mpt-7b</a>.

</span>
<span class="ltx_bibblock">Accessed: 2023-05-05.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Niklas Muennighoff, Alexander&nbsp;M Rush, Boaz Barak, Teven&nbsp;Le Scao, Aleksandra
Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2305.16264</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nunes [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Davide Nunes.

</span>
<span class="ltx_bibblock">Preprocessed penn tree bank, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/record/3910021" title="">https://zenodo.org/record/3910021</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ArXiv</em>, abs/2303.08774, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257532815" title="">https://api.semanticscholar.org/CorpusID:257532815</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul&nbsp;F Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In S.&nbsp;Koyejo, S.&nbsp;Mohamed, A.&nbsp;Agarwal, D.&nbsp;Belgrave, K.&nbsp;Cho, and A.&nbsp;Oh,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Advances in Neural Information Processing Systems</em>, volume&nbsp;35,
pages 27730–27744. Curran Associates, Inc., 2022.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papasavva et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Antonis Papasavva, Savvas Zannettou, Emiliano&nbsp;De Cristofaro, Gianluca
Stringhini, and Jeremy Blackburn.

</span>
<span class="ltx_bibblock">Raiders of the lost kek: 3.5 years of augmented 4chan posts from the
politically incorrect board.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the International AAAI Conference on Web and
Social Media</em>, 14:885–894, may 2020.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1609/icwsm.v14i1.7354" title="">10.1609/icwsm.v14i1.7354</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1609%2Ficwsm.v14i1.7354" title="">https://doi.org/10.1609%2Ficwsm.v14i1.7354</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
Daniel Rothchild, David So, Maud Texier, and Jeff Dean.

</span>
<span class="ltx_bibblock">Carbon emissions and large neural network training, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2104.10350" title="">https://arxiv.org/abs/2104.10350</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aimée
Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam
Almazrouei, and Julien Launay.

</span>
<span class="ltx_bibblock">The refinedweb dataset for falcon llm: Outperforming curated corpora
with web data, and web data only.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">ArXiv</em>, abs/2306.01116, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:259063761" title="">https://api.semanticscholar.org/CorpusID:259063761</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matthew&nbsp;E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Deep contextualized word representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ArXiv</em>, abs/1802.05365, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:3626819" title="">https://api.semanticscholar.org/CorpusID:3626819</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pilehvar and Camacho-Collados [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mohammad&nbsp;Taher Pilehvar and José Camacho-Collados.

</span>
<span class="ltx_bibblock">Wic: 10, 000 example pairs for evaluating context-sensitive
representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">CoRR</em>, abs/1808.09121, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1808.09121" title="">http://arxiv.org/abs/1808.09121</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press and Wolf [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ofir Press and Lior Wolf.

</span>
<span class="ltx_bibblock">Using the output embedding to improve language models.

</span>
<span class="ltx_bibblock">In Mirella Lapata, Phil Blunsom, and Alexander Koller, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume 2, Short Papers</em>, pages
157–163, Valencia, Spain, April 2017. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/E17-2025" title="">https://aclanthology.org/E17-2025</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jack&nbsp;W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
George van&nbsp;den Driessche, Lisa&nbsp;Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme
Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
Xiang&nbsp;Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
de&nbsp;Masson&nbsp;d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor
Babuschkin, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Chris Jones, James
Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel,
William Isaac, Ed&nbsp;Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray
Kavukcuoglu, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training
gopher, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.11446" title="">https://arxiv.org/abs/2112.11446</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher&nbsp;D Manning, Stefano
Ermon, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a
reward model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Thirty-seventh Conference on Neural Information Processing
Systems</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HPuSIXJaa9" title="">https://openreview.net/forum?id=HPuSIXJaa9</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">J. Mach. Learn. Res.</em>, 21(1), jan 2020.

</span>
<span class="ltx_bibblock">ISSN 1532-4435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">Zero: Memory optimizations toward training trillion parameter models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">SC20: International Conference for High Performance Computing,
Networking, Storage and Analysis</em>, pages 1–16, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:203736482" title="">https://api.semanticscholar.org/CorpusID:203736482</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Machel Reid, Victor Zhong, Suchin Gururangan, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">M2D2: A massively multi-domain language modeling dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 964–975, Abu Dhabi, United Arab
Emirates, December 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.63" title="">https://aclanthology.org/2022.emnlp-main.63</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Manoel&nbsp;Horta Ribeiro, Jeremy Blackburn, Barry Bradlyn, Emiliano&nbsp;De Cristofaro,
Gianluca Stringhini, Summer Long, Stephanie Greenberg, and Savvas Zannettou.

</span>
<span class="ltx_bibblock">The evolution of the manosphere across the web.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the International AAAI Conference on Web and
Social Media</em>, 15:196–207, may 2021.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1609/icwsm.v15i1.18053" title="">10.1609/icwsm.v15i1.18053</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1609%2Ficwsm.v15i1.18053" title="">https://doi.org/10.1609%2Ficwsm.v15i1.18053</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenfeld [2000]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ronald Rosenfeld.

</span>
<span class="ltx_bibblock">Two decades of statistical language modeling: Where do we go from
here?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the IEEE</em>, 88(8):1270–1278,
2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Communications of the ACM</em>, 64(9):99–106,
2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/abs/10.1145/3474381" title="">https://dl.acm.org/doi/abs/10.1145/3474381</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M&nbsp;Saiful
Bari, Canwen Xu, Urmish Thakker, Shanya&nbsp;Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang,
Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng&nbsp;Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason&nbsp;Alan Fries, Ryan
Teehan, Teven&nbsp;Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander&nbsp;M
Rush.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=9Vrb9D0WI4" title="">https://openreview.net/forum?id=9Vrb9D0WI4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Noam&nbsp;M. Shazeer.

</span>
<span class="ltx_bibblock">Glu variants improve transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">ArXiv</em>, abs/2002.05202, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:211096588" title="">https://api.semanticscholar.org/CorpusID:211096588</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soldaini et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson,
Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,
Valentin Hofmann, Ananya&nbsp;Harsh Jha, Sachin Kumar, Li&nbsp;Lucy, Xinxi Lyu, Nathan
Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik,
Crystal Nam, Matthew&nbsp;E. Peters, Abhilasha Ravichander, Kyle Richardson,
Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh,
Luke Zettlemoyer, Noah&nbsp;A. Smith, Hannaneh Hajishirzi, Iz&nbsp;Beltagy, Dirk
Groeneveld, Jesse Dodge, and Kyle Lo.

</span>
<span class="ltx_bibblock">Dolma: an Open Corpus of Three Trillion Tokens for Language Model
Pretraining Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Emma Strubell, Ananya Ganesh, and Andrew McCallum.

</span>
<span class="ltx_bibblock">Energy and policy considerations for deep learning in NLP.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>, pages 3645–3650, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/P19-1355" title="">10.18653/v1/P19-1355</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1355" title="">https://aclanthology.org/P19-1355</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">ArXiv</em>, abs/2104.09864, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:233307138" title="">https://api.semanticscholar.org/CorpusID:233307138</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teknium1 [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Teknium1.

</span>
<span class="ltx_bibblock">Gpteacher.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/teknium1/GPTeacher" title="">https://github.com/teknium1/GPTeacher</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Together Computer [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Together Computer.

</span>
<span class="ltx_bibblock">RedPajama: An Open Source Recipe to Reproduce LLaMA training
dataset, April 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/togethercomputer/RedPajama-Data" title="">https://github.com/togethercomputer/RedPajama-Data</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023a]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">ArXiv</em>, abs/2302.13971, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257219404" title="">https://api.semanticscholar.org/CorpusID:257219404</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023b]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models,
2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09288" title="">https://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ubierna et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
María Ubierna, Cristina&nbsp;Díez Santos, and Sara Mercier-Blais.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Water Security and Climate Change: Hydropower Reservoir
Greenhouse Gas Emissions</em>, pages 69–94.

</span>
<span class="ltx_bibblock">Springer Singapore, Singapore, 2022.

</span>
<span class="ltx_bibblock">ISBN 978-981-16-5493-0.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1007/978-981-16-5493-0_5" title="">10.1007/978-981-16-5493-0˙5</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-981-16-5493-0_5" title="">https://doi.org/10.1007/978-981-16-5493-0_5</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Ł&nbsp;ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;Von Luxburg, S.&nbsp;Bengio, H.&nbsp;Wallach, R.&nbsp;Fergus,
S.&nbsp;Vishwanathan, and R.&nbsp;Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Advances in Neural
Information Processing Systems</em>, volume&nbsp;30. Curran Associates, Inc., 2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilares and Gómez-Rodríguez [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
David Vilares and Carlos Gómez-Rodríguez.

</span>
<span class="ltx_bibblock">HEAD-QA: A healthcare dataset for complex reasoning.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>, pages 960–966, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/P19-1092" title="">10.18653/v1/P19-1092</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1092" title="">https://aclanthology.org/P19-1092</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
Samuel&nbsp;R. Bowman.

</span>
<span class="ltx_bibblock">Glue: A multi-task benchmark and analysis platform for natural
language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">ArXiv</em>, abs/1804.07461, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1804.07461" title="">https://arxiv.org/abs/1804.07461</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot,
Khyathi&nbsp;Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah&nbsp;A. Smith,
Iz&nbsp;Beltagy, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">How far can camels go? exploring the state of instruction tuning on
open resources, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.04751" title="">https://arxiv.org/abs/2306.04751</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams&nbsp;Wei Yu, Brian Lester,
Nan Du, Andrew&nbsp;M. Dai, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gEZrGCozdqR" title="">https://openreview.net/forum?id=gEZrGCozdqR</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et&nbsp;al. [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Johannes Welbl, Nelson&nbsp;F Liu, and Matt Gardner.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:1707.06209</em>, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1707.06209" title="">https://arxiv.org/abs/1707.06209</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,
Kiwan Maeng, Gloria Chang, Fiona&nbsp;Aga Behram, James Huang, Charles Bai,
Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore
Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin&nbsp;S. Lee, Bugra
Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, and Kim
Hazelwood.

</span>
<span class="ltx_bibblock">Sustainable ai: Environmental implications, challenges and
opportunities, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2111.00364" title="">https://arxiv.org/abs/2111.00364</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu&nbsp;Zhao, Jiazhan Feng, Chongyang
Tao, Qingwei Lin, and Daxin Jiang.

</span>
<span class="ltx_bibblock">WizardLM: Empowering large pre-trained language models to follow
complex instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">The Twelfth International Conference on Learning
Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=CfXh93NDgH" title="">https://openreview.net/forum?id=CfXh93NDgH</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.

</span>
<span class="ltx_bibblock">Baize: An open-source chat model with parameter-efficient tuning on
self-chat data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2304.01196</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zannettou et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Savvas Zannettou, Barry Bradlyn, Emiliano De&nbsp;Cristofaro, Haewoon Kwak, Michael
Sirivianos, Gianluca Stringini, and Jeremy Blackburn.

</span>
<span class="ltx_bibblock">What is gab: A bastion of free speech or an alt-right echo chamber.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Companion Proceedings of the The Web Conference 2018</em>, WWW
’18, page 1007–1014, Republic and Canton of Geneva, CHE, 2018.
International World Wide Web Conferences Steering Committee.

</span>
<span class="ltx_bibblock">ISBN 9781450356404.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1145/3184558.3191531" title="">10.1145/3184558.3191531</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3184558.3191531" title="">https://doi.org/10.1145/3184558.3191531</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">arXiv preprint arXiv:1905.07830</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1905.07830" title="">https://arxiv.org/abs/1905.07830</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Sennrich [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">ArXiv</em>, abs/1910.07467, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:113405151" title="">https://api.semanticscholar.org/CorpusID:113405151</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit&nbsp;Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.01068" title="">https://arxiv.org/abs/2205.01068</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu,
Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can
Balioglu, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li.

</span>
<span class="ltx_bibblock">Pytorch fsdp: Experiences on scaling fully sharded data parallel.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">Proc. VLDB Endow.</em>, 16:3848–3860, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258297871" title="">https://api.semanticscholar.org/CorpusID:258297871</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="453" id="A1.F3.g1" src="x8.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Bits per byte for each of the 7 remaining Paloma data sources not aggregated in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Additional perplexity results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.1">In Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.F3" title="Figure 3 ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> we provide results for each of the 7 data sources in Paloma <cite class="ltx_cite ltx_citemacro_citep">[Magnusson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>]</cite> that are excluded from the combined metric in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>. Some of these sources such as Pile <cite class="ltx_cite ltx_citemacro_citep">[Gao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib25" title="">2020</a>]</cite> and ICE <cite class="ltx_cite ltx_citemacro_citep">[Greenbaum and Nelson, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib27" title="">1996</a>]</cite> are not publicly available at this time. Dolma 100 Programming Languages <cite class="ltx_cite ltx_citemacro_citep">[Soldaini et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>]</cite> consists of code data that is not supported by the decontamination approach used in Paloma. TwitterAAE <cite class="ltx_cite ltx_citemacro_citep">[Blodgett et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib11" title="">2016</a>]</cite>, along with ICE, are datasets for targeted analyses of disparities in performance between different dialects and as such should be evaluated separately. And finally, the Manosphere, Gab, and 4chan corpora <cite class="ltx_cite ltx_citemacro_citep">[Ribeiro et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib66" title="">2021</a>, Zannettou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib89" title="">2018</a>, Papasavva et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib55" title="">2020</a>]</cite> are intended to examine model fit to language from fringe online communities that are studied for prevalent hate speech and toxicity. Thus minimizing perplexity on these fringe corpora is not always desirable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p2.1">One notable result here is that OLMo-7B is much farther ahead of the other models on Dolma 100 Programming Languages (100 PLs). Note that this effect may be due in part to underestimation from contamination, as decontaminating code data is beyond the scope of the method in Paloma. At the same time other models that are trained on code data from GitHub such as RPJ-INCITE-7B, that are just as likely to have contamination, fair much worse. Another factor then is that OLMo-7B trains on code data with exactly the same post-processing as that in 100 PLs while the code data in other models will have been processed differently. Similarly, Pile evaluation demonstrates these in-distribution and potential contamination effects as Pythia-6.9B achieves top performance despite being trained on almost an order of magnitude fewer tokens than OLMo-7B.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p3.1">The results on the remaining 5 targeted sources should be interpreted with care, as Paloma often finds that perplexity on these sources is dominated by superficial features such as low average document length rather than fit to that which would actually be salient to members of these speech communities. TwitterAAE and Gab have among the shortest documents in Paloma contributing to unusually high bits per byte in this figure. Other than these two, the models are notably very closely grouped in a data scaling trend in ICE, Manosphere, and 4chan.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Additional end-task results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.1">Next, in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.T9" title="Table 9 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">9</span></a>, we provide results from zero-shot evaluation of OLMo-7B on 6 additional end-tasks apart from the 8 in our core evaluation suite. These tasks are <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.1">headqa_en</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Vilares and Gómez-Rodríguez, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib81" title="">2019</a>]</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.2">logiqa</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Liu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib39" title="">2020</a>]</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.3">mrpc</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Dolan and Brockett, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib23" title="">2005</a>]</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.4">qnli</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib82" title="">2018</a>]</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.5">wic</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Pilehvar and Camacho-Collados, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib59" title="">2018</a>]</cite>, and <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.6">wnli</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib82" title="">2018</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A1.T9">
<p class="ltx_p ltx_align_center" id="A1.T9.1"><span class="ltx_text" id="A1.T9.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A1.T9.1.1.1" style="width:249.1pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="A1.T9.1.1.1.1"><span class="ltx_text" id="A1.T9.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T9.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.2">headqa_en</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.3">logiqa</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.4">mrpc</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.5">qnli</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.6">wic</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.7">wnli</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.8">avg.</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.2.1.1.1">Falcon-7B</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.2">38.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.3">23.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.4">62.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.5">49.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.6">49.5</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.7">47.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.8">45.4</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.3.2.1.1">LLaMA-7B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.2">38.7</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.3">19.5</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.4">68.6</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.5">50.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.6">49.1</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.3.2.7">52.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.8">46.4</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.4.3.1.1">LLaMA2-7B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.2">39.5</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.3">26.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.4">69.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.5">49.4</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.6">49.8</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.4.3.7">45.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.8">46.5</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.5.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.5.4.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.5.4.1.1">MPT-7B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.2">37.4</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.3">22.9</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.4">67.7</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.5">52.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.6">48.1</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.5.4.7">47.9</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.8">46.0</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.6.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.6.5.1.1">Pythia-6.9B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.2">40.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.3">21.5</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.4">65.4</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.5">53.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.6">55.0</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.6.5.7">38.0</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.8">45.6</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.7.6">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.7.6.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.7.6.1.1">RPJ-INCITE-7B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.2">36.9</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.3">27.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.4">58.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.5">53.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.6">48.9</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.7.6.7">57.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.8">47.3</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.8.7" style="background-color:#40C4DF;">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T9.1.1.1.1.1.1.8.7.1"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.1.1" style="background-color:#40C4DF;">[]
<span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.8.7.1.1.1">OLMo-7B</span></span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.2"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.2.1" style="background-color:#40C4DF;">37.3</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.3"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.3.1" style="background-color:#40C4DF;">23.4</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.4"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.4.1" style="background-color:#40C4DF;">68.4</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.5"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.5.1" style="background-color:#40C4DF;">49.1</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.6"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.6.1" style="background-color:#40C4DF;">50.2</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T9.1.1.1.1.1.1.8.7.7"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.7.1" style="background-color:#40C4DF;">56.3</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.8"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.8.1" style="background-color:#40C4DF;">47.5</span></span></span>
</span>
</span></span></span>
</span></span></span>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Zero-shot evaluation of OLMo-7B on 6 additional end-tasks apart from the 8 present in our core evaluation suite. Once again, we compare OLMo-7B to 6 other model checkpoints which are publicly available. We find that OLMo-7B outperforms the other models on aggregate taken over 6 additional end-tasks from this table, however these tasks were also found to provide limited signal during training (see Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.F4" title="Figure 4 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p2.1">We note, however, that in contrast to our core evaluation set described in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1" title="4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>, we found these additional end-tasks to have less stable performance during model development, and to provide a limited signal. This is illustrated in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.F4" title="Figure 4 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>, where we see the progress of task performance throughout training to be more random (compare with the more stable upward trends in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F1" title="Figure 1 ‣ Results ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>). While tasks such as <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.1">mrpc</span> and <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.2">wic</span> appear more stable, they offered additional difficulties related to performance being tied to random chance (e.g., <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.3">wic</span>) or the tendency of models to make spurious predictions (e.g., always predicting a single label) that either inflate or deflate performance due to dataset class imbalances (e.g., <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.4">mrpc</span>). We therefore caution against relying too heavily on these tasks when measuring model performance throughout training and comparing models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A1.F4">
<p class="ltx_p ltx_align_center" id="A1.F4.1"><span class="ltx_text" id="A1.F4.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="502" id="A1.F4.1.1.g1" src="x9.png" width="830">
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Accuracy score progression of OLMo-7B on 6 additional end-tasks. The performance of these additional end-tasks was unstable and provided limited signal during model development.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Adaptation Training Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We use the following hyperparameters when instruction tuning OLMo. These were chosen through small pilot experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.p2">
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1">Learning Rate: <math alttext="2\times 10^{-6}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.1.m1.1"><semantics id="A2.I1.i1.p1.1.m1.1a"><mrow id="A2.I1.i1.p1.1.m1.1.1" xref="A2.I1.i1.p1.1.m1.1.1.cmml"><mn id="A2.I1.i1.p1.1.m1.1.1.2" xref="A2.I1.i1.p1.1.m1.1.1.2.cmml">2</mn><mo id="A2.I1.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.I1.i1.p1.1.m1.1.1.1.cmml">×</mo><msup id="A2.I1.i1.p1.1.m1.1.1.3" xref="A2.I1.i1.p1.1.m1.1.1.3.cmml"><mn id="A2.I1.i1.p1.1.m1.1.1.3.2" xref="A2.I1.i1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A2.I1.i1.p1.1.m1.1.1.3.3" xref="A2.I1.i1.p1.1.m1.1.1.3.3.cmml"><mo id="A2.I1.i1.p1.1.m1.1.1.3.3a" xref="A2.I1.i1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="A2.I1.i1.p1.1.m1.1.1.3.3.2" xref="A2.I1.i1.p1.1.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.I1.i1.p1.1.m1.1b"><apply id="A2.I1.i1.p1.1.m1.1.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1"><times id="A2.I1.i1.p1.1.m1.1.1.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1.1"></times><cn id="A2.I1.i1.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.I1.i1.p1.1.m1.1.1.2">2</cn><apply id="A2.I1.i1.p1.1.m1.1.1.3.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3">superscript</csymbol><cn id="A2.I1.i1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A2.I1.i1.p1.1.m1.1.1.3.2">10</cn><apply id="A2.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3.3"><minus id="A2.I1.i1.p1.1.m1.1.1.3.3.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3.3"></minus><cn id="A2.I1.i1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A2.I1.i1.p1.1.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.i1.p1.1.m1.1c">2\times 10^{-6}</annotation><annotation encoding="application/x-llamapun" id="A2.I1.i1.p1.1.m1.1d">2 × 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1">Epochs: 3</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p" id="A2.I1.i3.p1.1">Warmup: Linear warmup for the first 3% of total training time, and then linear cooldown to a learning rate of 0 over the remaining steps.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i4.p1">
<p class="ltx_p" id="A2.I1.i4.p1.1">Weight Decay: 0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i5.p1">
<p class="ltx_p" id="A2.I1.i5.p1.1">Gradient clipping: 0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i6.p1">
<p class="ltx_p" id="A2.I1.i6.p1.1">Maximum sequence length: 2048</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">After instruction finetuning, we then use the following hyperparameters for DPO training, following <cite class="ltx_cite ltx_citemacro_citet">Ivison et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>]</cite>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.p4">
<ul class="ltx_itemize" id="A2.I2">
<li class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i1.p1">
<p class="ltx_p" id="A2.I2.i1.p1.1">Learning Rate: <math alttext="5\times 10^{-7}" class="ltx_Math" display="inline" id="A2.I2.i1.p1.1.m1.1"><semantics id="A2.I2.i1.p1.1.m1.1a"><mrow id="A2.I2.i1.p1.1.m1.1.1" xref="A2.I2.i1.p1.1.m1.1.1.cmml"><mn id="A2.I2.i1.p1.1.m1.1.1.2" xref="A2.I2.i1.p1.1.m1.1.1.2.cmml">5</mn><mo id="A2.I2.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.I2.i1.p1.1.m1.1.1.1.cmml">×</mo><msup id="A2.I2.i1.p1.1.m1.1.1.3" xref="A2.I2.i1.p1.1.m1.1.1.3.cmml"><mn id="A2.I2.i1.p1.1.m1.1.1.3.2" xref="A2.I2.i1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A2.I2.i1.p1.1.m1.1.1.3.3" xref="A2.I2.i1.p1.1.m1.1.1.3.3.cmml"><mo id="A2.I2.i1.p1.1.m1.1.1.3.3a" xref="A2.I2.i1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="A2.I2.i1.p1.1.m1.1.1.3.3.2" xref="A2.I2.i1.p1.1.m1.1.1.3.3.2.cmml">7</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.I2.i1.p1.1.m1.1b"><apply id="A2.I2.i1.p1.1.m1.1.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1"><times id="A2.I2.i1.p1.1.m1.1.1.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1.1"></times><cn id="A2.I2.i1.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.I2.i1.p1.1.m1.1.1.2">5</cn><apply id="A2.I2.i1.p1.1.m1.1.1.3.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.I2.i1.p1.1.m1.1.1.3.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3">superscript</csymbol><cn id="A2.I2.i1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A2.I2.i1.p1.1.m1.1.1.3.2">10</cn><apply id="A2.I2.i1.p1.1.m1.1.1.3.3.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3.3"><minus id="A2.I2.i1.p1.1.m1.1.1.3.3.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3.3"></minus><cn id="A2.I2.i1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A2.I2.i1.p1.1.m1.1.1.3.3.2">7</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I2.i1.p1.1.m1.1c">5\times 10^{-7}</annotation><annotation encoding="application/x-llamapun" id="A2.I2.i1.p1.1.m1.1d">5 × 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i2.p1">
<p class="ltx_p" id="A2.I2.i2.p1.1"><math alttext="\beta" class="ltx_Math" display="inline" id="A2.I2.i2.p1.1.m1.1"><semantics id="A2.I2.i2.p1.1.m1.1a"><mi id="A2.I2.i2.p1.1.m1.1.1" xref="A2.I2.i2.p1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A2.I2.i2.p1.1.m1.1b"><ci id="A2.I2.i2.p1.1.m1.1.1.cmml" xref="A2.I2.i2.p1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.I2.i2.p1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="A2.I2.i2.p1.1.m1.1d">italic_β</annotation></semantics></math>: 0.1</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i3.p1">
<p class="ltx_p" id="A2.I2.i3.p1.1">Epochs: 3</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i4.p1">
<p class="ltx_p" id="A2.I2.i4.p1.1">Warmup: Linear warmup for the first 10% of total training time, and then linear cooldown to a learning rate of 0 over the remaining steps.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i5.p1">
<p class="ltx_p" id="A2.I2.i5.p1.1">Weight Decay: 0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i6.p1">
<p class="ltx_p" id="A2.I2.i6.p1.1">Gradient clipping: 0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i7.p1">
<p class="ltx_p" id="A2.I2.i7.p1.1">Maximum sequence length: 2048</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Adaptation Evaluation and Model details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We choose the models in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T7" title="Table 7 ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">7</span></a> by choosing the ‘canonical’ best versions (that is, the best instruction-tuned or otherwise adapted models released by the same organisation) of the base models we compare against in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T6" title="Table 6 ‣ Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>. We additionally compare to <span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.1">Tülu</span> 2 to show the current best models trained using the <span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.2">Tülu</span> mix used to finetune OLMo. We display evaluations on MMLU, AlpacaEval, ToxiGen, and Truthfulness to focus on displaying how instruction tuning can generally help capabilities (MMLU), how the models perform in an open-ended chat setting (AlpacaEval), and to test how instruction tuning aids in model safety and truthfulness (AlpacaEval, ToxiGen). We additionally report OLMo’s performance over the entire <span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.3">Tülu</span> evaluation suite in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A3.T10" title="Table 10 ‣ Appendix C Adaptation Evaluation and Model details ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">10</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="A3.T10">
<p class="ltx_p" id="A3.T10.1"><span class="ltx_text" id="A3.T10.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A3.T10.1.1.1" style="width:431.3pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="A3.T10.1.1.1.1"><span class="ltx_text" id="A3.T10.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T10.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.1.1">Model</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.2.1">MMLU</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.3.1">GSM8k</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.4.1">BBH</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.5.1">TydiQA</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.6.1">Codex-Eval</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.7.1">AlpacaEval</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.8.1">ToxiGen</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.9.1">TruthfulQA</span></span></span>
<span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.2.2">
<span class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="A3.T10.1.1.1.1.1.1.2.2.1"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.2.1">0-shot</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.3.1">8-shot CoT</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.4.1">3-shot CoT</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.5.1">1-shot</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.6.1">Pass@10</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.7"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.7.1">%win</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.8"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.8.1">% Toxic</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.9"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.9.1">% Info + True</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.3.1.1.1">OLMo-7B</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.2">28.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.3">8.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.4">31.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.5">32.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.6">21.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.7">-</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.8">81.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.9">31.6</span></span>
<span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.4.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T10.1.1.1.1.1.1.4.2.1"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.4.2.1.1">+SFT</span></span>
<span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.2">47.3</span>
<span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.3">15.5</span>
<span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.4">36.9</span>
<span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.5">35.2</span>
<span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.6">28.6</span>
<span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.7">57.0</span>
<span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.8">14.4</span>
<span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.9">41.2</span></span>
<span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.5.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A3.T10.1.1.1.1.1.1.5.3.1"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.5.3.1.1">+SFT+DPO</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.2">46.1</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.3">11.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.4">35.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.5">21.7</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.6">27.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.7">69.3</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.8">1.7</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.9">52.0</span></span>
</span>
</span></span></span>
</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Evaluation of OLMo-7B models before and after instruction finetuning and DPO training on the full <span class="ltx_text ltx_font_smallcaps" id="A3.T10.3.1">Tülu</span> evaluation suite. Lower is better for ToxiGen and higher is better for other metrics.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">We provide a brief description of each model evaluated in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T7" title="Table 7 ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">7</span></a> below. For all models, we use the provided chat template for prompt formatting when available.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1">MPT Chat: A version of MPT 7B finetuned on the ShareGPT-Vicuna&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Chiang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib13" title="">2023</a>]</cite>, HC3&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Guo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib29" title="">2023</a>]</cite>, Alpaca&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Taori et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib74" title="">2023</a>]</cite>, HH-RLHF&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Bai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib5" title="">2022</a>]</cite>, and Evol-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib87" title="">2024</a>]</cite> datasets. Retrieved from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/mosaicml/mpt-7b-chat" title="">https://huggingface.co/mosaicml/mpt-7b-chat</a>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p" id="A3.I1.i2.p1.1">Falcon Instruct: A version of Falcon 7B finetuned on the Baize&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Xu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib88" title="">2023</a>]</cite>, GPT4All&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Anand et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib3" title="">2023</a>]</cite>, GPTeacher&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Teknium1, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib75" title="">2023</a>]</cite>, and Refined-Web English&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Penedo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib57" title="">2023</a>]</cite> datasets. Retrieved from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/tiiuae/falcon-7b-instruct" title="">https://huggingface.co/tiiuae/falcon-7b-instruct</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i3.p1">
<p class="ltx_p" id="A3.I1.i3.p1.1">RPJ-INCITE Chat: A version of RPJ-INCITE 7B finetuned on the OASST1&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Köpf et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib35" title="">2023</a>]</cite> and Dolly V2&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Conover et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib19" title="">2023</a>]</cite> datasets. Retrieved from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat" title="">https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i4.p1">
<p class="ltx_p" id="A3.I1.i4.p1.1">Llama-2 Chat: A version of Llama 2 7B finetuned on a mixture of instruction datasets and further trained with RLHF. We refer the reader to <cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>]</cite> for further details.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i5.p1">
<p class="ltx_p" id="A3.I1.i5.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A3.I1.i5.p1.1.1">Tülu</span> 2: A version of Llama 2 7B finetuned on a mixture of instruction datasets (the <span class="ltx_text ltx_font_smallcaps" id="A3.I1.i5.p1.1.2">Tülu</span> 2 mix). We refer the reader to <cite class="ltx_cite ltx_citemacro_citet">Ivison et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>]</cite> for further details.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i6.p1">
<p class="ltx_p" id="A3.I1.i6.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A3.I1.i6.p1.1.1">Tülu</span> 2+DPO: <span class="ltx_text ltx_font_smallcaps" id="A3.I1.i6.p1.1.2">Tülu</span> 2 further trained with DPO on the UltraFeedback dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Cui et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib20" title="">2023</a>]</cite>. We refer the reader to <cite class="ltx_cite ltx_citemacro_citet">Ivison et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>]</cite> for further details.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i7.p1">
<p class="ltx_p" id="A3.I1.i7.p1.1">OLMo +SFT: A version of OLMo 7B fintuned on the same data as <span class="ltx_text ltx_font_smallcaps" id="A3.I1.i7.p1.1.1">Tülu</span> 2.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i8.p1">
<p class="ltx_p" id="A3.I1.i8.p1.1">OLMo +SFT+DPO: OLMo +SFT further trained with DPO on the UltraFeedback dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Cui et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib20" title="">2023</a>]</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p" id="A3.p3.1">We additionally provide a brief description of each evaluation setting from Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T7" title="Table 7 ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A3.I2">
<li class="ltx_item" id="A3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i1.p1">
<p class="ltx_p" id="A3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i1.p1.1.1">MMLU</span>: We use the official MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Hendrycks et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib32" title="">2021</a>]</cite> evaluation script and prompts available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hendrycks/test" title="">https://github.com/hendrycks/test</a>, with modifications to allow for batch processing. We evaluate using 0 few-shot examples, following the original setup of MMLU. We report average accuracy across test examples.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i2.p1">
<p class="ltx_p" id="A3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i2.p1.1.1">ToxiGen</span>: We follow the setup in <cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>]</cite>, but use the original set of prompts from <cite class="ltx_cite ltx_citemacro_citet">Hartvigsen et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib31" title="">2022</a>]</cite>, which are designed to elicit toxic generations for certain groups. We take only the prompts designed to produce toxic language (‘hateful’ prompts) and use 500 prompts per group to reduce evaluation costs. For base language models, we pass in the original ToxiGen prompts unchanged and greedily decode up to the first new line (or a maximum of 512 tokens). For instruction-tuned models, we place the prompt in the corresponding template, and ask the model to complete the prompt, until the model generates a stop token (or a maximum of 512 tokens). We pass the generated text into a roberta-large model trained to detect toxic content finetuned as part of <cite class="ltx_cite ltx_citemacro_citet">Hartvigsen et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib31" title="">2022</a>]</cite><span class="ltx_note ltx_role_footnote" id="footnote28"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/tomh/toxigen_roberta" title="">https://huggingface.co/tomh/toxigen_roberta</a></span></span></span>. We then report the percentage of generations deemed toxic by the classifier.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i3.p1">
<p class="ltx_p" id="A3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i3.p1.1.1">TruthfulQA</span>: Following <cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>]</cite>, we mainly use the generation setting of TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">[Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib38" title="">2022</a>]</cite>. The TruthfulQA dataset contains 818 questions, which are used to prompt the tested model to generate answers. We use the default QA prompt format with 6 in-context QA examples. We follow the official script in their official implemention<span class="ltx_note ltx_role_footnote" id="footnote29"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><span class="ltx_tag ltx_tag_note">29</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sylinrl/TruthfulQA/" title="">https://github.com/sylinrl/TruthfulQA/</a></span></span></span> to do greedy decoding and answer postprocessing. We train two LLaMA 2-based classifiers for judging the truthfulness and informativeness of the model response, due to the deprecation of GPT-3 making exact replication of the original TruthfulQA evaluation infeasible. We find that the LLaMA 2 judges are generally able to match the performance of the original GPT-3-based judges used by <cite class="ltx_cite ltx_citemacro_citet">Lin et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib38" title="">2022</a>]</cite>. We report the rate of the responses being truthful and informative (% Informative and Truthful) following <cite class="ltx_cite ltx_citemacro_citet">Touvron et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>]</cite>. We only report the % Informative and Truthful as our primary metric.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i4.p1">
<p class="ltx_p" id="A3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i4.p1.1.1">AlpacaEval</span>: We use the package provided by <cite class="ltx_cite ltx_citemacro_citet">Li et&nbsp;al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib36" title="">2023</a>]</cite>, following the default setup which asks the evaluated model to generate responses for 805 prompts and employ GPT-4 to compare the response with <span class="ltx_text" id="A3.I2.i4.p1.1.2">Davinci</span>-<span class="ltx_text" id="A3.I2.i4.p1.1.3">003</span>. We employ the “alpaca_eval_gpt4” annotator. We allow the evaluated model to generate up to 2048 tokens, without specifying special stop sequences. The reported win-rate is the percentage of model generations that GPT-4 reports as being preferred over the generations from <span class="ltx_text" id="A3.I2.i4.p1.1.4">Davinci</span>-<span class="ltx_text" id="A3.I2.i4.p1.1.5">003</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="A3.p4">
<p class="ltx_p" id="A3.p4.1"></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>OLMo : Accelerating the Science of Language Models</title>
<!--Generated on Wed Feb 28 02:26:01 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on February 28, 2024.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="https://arxiv.org/html/2402.00838v3/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2402.00838v3">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2402.00838v3">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2402.00838v3" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S1" title="1 Introduction ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2" title="2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>OLMo Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS1" title="2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>OLMo&nbsp;Model and Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS2" title="2.2 Pretraining Data: Dolma ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Pretraining Data: Dolma</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS3" title="2.3 Adaptation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Adaptation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4" title="2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px1" title="In-Loop Training Ablations ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">In-Loop Training Ablations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px2" title="Downstream Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Downstream Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px3" title="Intrinsic Language Modeling Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Intrinsic Language Modeling Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px4" title="Adaptation Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Adaptation Evaluation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3" title="3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Training OLMo</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS1" title="3.1 Distributed Training Framework ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Distributed Training Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS2" title="3.2 Optimizer ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Optimizer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS3" title="3.3 Data ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS4" title="3.4 Hardware ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Hardware</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4" title="4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1" title="4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Downstream evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1.SSS0.Px1" title="Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1.SSS0.Px2" title="Results ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS2" title="4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Intrinsic language modeling evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS2.SSS0.Px1" title="Setup ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS2.SSS0.Px2" title="Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS3" title="4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Adaptation Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS3.SSS0.Px1" title="Setup ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS3.SSS0.Px2" title="Results ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS4" title="4.4 Power Consumption and Carbon Footprint ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Power Consumption and Carbon Footprint</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S5" title="5 Artifacts Released ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Artifacts Released</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S6" title="6 License ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>License</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S7" title="7 Conclusion and Future Work ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1" title="Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Additional Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.SS0.SSS0.Px1" title="Additional perplexity results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Additional perplexity results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.SS0.SSS0.Px2" title="Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title">Additional end-task results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A2" title="Appendix B Adaptation Training Details ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Adaptation Training Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A3" title="Appendix C Adaptation Evaluation and Model details ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Adaptation Evaluation and Model details</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: fdsymbol</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2402.00838v3 [cs.CL] 28 Feb 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text" id="id1.1" style="position:relative; bottom:-3.0pt;"><span class="ltx_text" id="id1.1.1" style="width:0.0pt;position:relative; bottom:3.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="ltx_text" id="id1.1.1.1" style="font-size:70%;">OLMo</span></span><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="29" id="id1.1.g1" src="https://arxiv.org/html/2402.00838v3/x1.png" width="55"></span>&nbsp;: Accelerating the Science of Language Models
</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id3.2.2">Dirk Groeneveld

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id2.1.1.m1.1"><semantics id="id2.1.1.m1.1a"><msup id="id2.1.1.m1.1.1" xref="id2.1.1.m1.1.1.cmml"><mi id="id2.1.1.m1.1.1a" xref="id2.1.1.m1.1.1.cmml"></mi><mi id="id2.1.1.m1.1.1.1" mathcolor="#265ED4" xref="id2.1.1.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id2.1.1.m1.1b"><apply id="id2.1.1.m1.1.1.cmml" xref="id2.1.1.m1.1.1"><ci id="id2.1.1.m1.1.1.1.cmml" xref="id2.1.1.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.1.1.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id2.1.1.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Iz Beltagy

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id3.2.2.m2.1"><semantics id="id3.2.2.m2.1a"><msup id="id3.2.2.m2.1.1" xref="id3.2.2.m2.1.1.cmml"><mi id="id3.2.2.m2.1.1a" xref="id3.2.2.m2.1.1.cmml"></mi><mi id="id3.2.2.m2.1.1.1" mathcolor="#265ED4" xref="id3.2.2.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id3.2.2.m2.1b"><apply id="id3.2.2.m2.1.1.cmml" xref="id3.2.2.m2.1.1"><ci id="id3.2.2.m2.1.1.1.cmml" xref="id3.2.2.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.2.2.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id3.2.2.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id7.6.6">Pete Walsh

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id4.3.3.m1.1"><semantics id="id4.3.3.m1.1a"><msup id="id4.3.3.m1.1.1" xref="id4.3.3.m1.1.1.cmml"><mi id="id4.3.3.m1.1.1a" xref="id4.3.3.m1.1.1.cmml"></mi><mi id="id4.3.3.m1.1.1.1" mathcolor="#265ED4" xref="id4.3.3.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id4.3.3.m1.1b"><apply id="id4.3.3.m1.1.1.cmml" xref="id4.3.3.m1.1.1"><ci id="id4.3.3.m1.1.1.1.cmml" xref="id4.3.3.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.3.3.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id4.3.3.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Akshita Bhagia

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id5.4.4.m2.1"><semantics id="id5.4.4.m2.1a"><msup id="id5.4.4.m2.1.1" xref="id5.4.4.m2.1.1.cmml"><mi id="id5.4.4.m2.1.1a" xref="id5.4.4.m2.1.1.cmml"></mi><mi id="id5.4.4.m2.1.1.1" mathcolor="#265ED4" xref="id5.4.4.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id5.4.4.m2.1b"><apply id="id5.4.4.m2.1.1.cmml" xref="id5.4.4.m2.1.1"><ci id="id5.4.4.m2.1.1.1.cmml" xref="id5.4.4.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.4.4.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id5.4.4.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Rodney Kinney

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id6.5.5.m3.1"><semantics id="id6.5.5.m3.1a"><msup id="id6.5.5.m3.1.1" xref="id6.5.5.m3.1.1.cmml"><mi id="id6.5.5.m3.1.1a" xref="id6.5.5.m3.1.1.cmml"></mi><mi id="id6.5.5.m3.1.1.1" mathcolor="#265ED4" xref="id6.5.5.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id6.5.5.m3.1b"><apply id="id6.5.5.m3.1.1.cmml" xref="id6.5.5.m3.1.1"><ci id="id6.5.5.m3.1.1.1.cmml" xref="id6.5.5.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.5.5.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id6.5.5.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Oyvind Tafjord

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id7.6.6.m4.1"><semantics id="id7.6.6.m4.1a"><msup id="id7.6.6.m4.1.1" xref="id7.6.6.m4.1.1.cmml"><mi id="id7.6.6.m4.1.1a" xref="id7.6.6.m4.1.1.cmml"></mi><mi id="id7.6.6.m4.1.1.1" mathcolor="#265ED4" xref="id7.6.6.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id7.6.6.m4.1b"><apply id="id7.6.6.m4.1.1.cmml" xref="id7.6.6.m4.1.1"><ci id="id7.6.6.m4.1.1.1.cmml" xref="id7.6.6.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.6.6.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id7.6.6.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id13.12.12">Ananya Harsh Jha

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id8.7.7.m1.1"><semantics id="id8.7.7.m1.1a"><msup id="id8.7.7.m1.1.1" xref="id8.7.7.m1.1.1.cmml"><mi id="id8.7.7.m1.1.1a" xref="id8.7.7.m1.1.1.cmml"></mi><mi id="id8.7.7.m1.1.1.1" mathcolor="#265ED4" xref="id8.7.7.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id8.7.7.m1.1b"><apply id="id8.7.7.m1.1.1.cmml" xref="id8.7.7.m1.1.1"><ci id="id8.7.7.m1.1.1.1.cmml" xref="id8.7.7.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.7.7.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id8.7.7.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Hamish Ivison

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id9.8.8.m2.1"><semantics id="id9.8.8.m2.1a"><msup id="id9.8.8.m2.1.1" xref="id9.8.8.m2.1.1.cmml"><mi id="id9.8.8.m2.1.1a" xref="id9.8.8.m2.1.1.cmml"></mi><mi id="id9.8.8.m2.1.1.1" mathcolor="#265ED4" xref="id9.8.8.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id9.8.8.m2.1b"><apply id="id9.8.8.m2.1.1.cmml" xref="id9.8.8.m2.1.1"><ci id="id9.8.8.m2.1.1.1.cmml" xref="id9.8.8.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.8.8.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id9.8.8.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id10.9.9.m3.1"><semantics id="id10.9.9.m3.1a"><msup id="id10.9.9.m3.1.1" xref="id10.9.9.m3.1.1.cmml"><mi id="id10.9.9.m3.1.1a" xref="id10.9.9.m3.1.1.cmml"></mi><mi id="id10.9.9.m3.1.1.1" mathcolor="#265ED4" xref="id10.9.9.m3.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id10.9.9.m3.1b"><apply id="id10.9.9.m3.1.1.cmml" xref="id10.9.9.m3.1.1"><ci id="id10.9.9.m3.1.1.1.cmml" xref="id10.9.9.m3.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id10.9.9.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id10.9.9.m3.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Ian Magnusson

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id11.10.10.m4.1"><semantics id="id11.10.10.m4.1a"><msup id="id11.10.10.m4.1.1" xref="id11.10.10.m4.1.1.cmml"><mi id="id11.10.10.m4.1.1a" xref="id11.10.10.m4.1.1.cmml"></mi><mi id="id11.10.10.m4.1.1.1" mathcolor="#265ED4" xref="id11.10.10.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id11.10.10.m4.1b"><apply id="id11.10.10.m4.1.1.cmml" xref="id11.10.10.m4.1.1"><ci id="id11.10.10.m4.1.1.1.cmml" xref="id11.10.10.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id11.10.10.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id11.10.10.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Yizhong Wang

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id12.11.11.m5.1"><semantics id="id12.11.11.m5.1a"><msup id="id12.11.11.m5.1.1" xref="id12.11.11.m5.1.1.cmml"><mi id="id12.11.11.m5.1.1a" xref="id12.11.11.m5.1.1.cmml"></mi><mi id="id12.11.11.m5.1.1.1" mathcolor="#265ED4" xref="id12.11.11.m5.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id12.11.11.m5.1b"><apply id="id12.11.11.m5.1.1.cmml" xref="id12.11.11.m5.1.1"><ci id="id12.11.11.m5.1.1.1.cmml" xref="id12.11.11.m5.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id12.11.11.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id12.11.11.m5.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id13.12.12.m6.1"><semantics id="id13.12.12.m6.1a"><msup id="id13.12.12.m6.1.1" xref="id13.12.12.m6.1.1.cmml"><mi id="id13.12.12.m6.1.1a" xref="id13.12.12.m6.1.1.cmml"></mi><mi id="id13.12.12.m6.1.1.1" mathcolor="#265ED4" xref="id13.12.12.m6.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id13.12.12.m6.1b"><apply id="id13.12.12.m6.1.1.cmml" xref="id13.12.12.m6.1.1"><ci id="id13.12.12.m6.1.1.1.cmml" xref="id13.12.12.m6.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id13.12.12.m6.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id13.12.12.m6.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id17.16.16">Shane Arora

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id14.13.13.m1.1"><semantics id="id14.13.13.m1.1a"><msup id="id14.13.13.m1.1.1" xref="id14.13.13.m1.1.1.cmml"><mi id="id14.13.13.m1.1.1a" xref="id14.13.13.m1.1.1.cmml"></mi><mi id="id14.13.13.m1.1.1.1" mathcolor="#265ED4" xref="id14.13.13.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id14.13.13.m1.1b"><apply id="id14.13.13.m1.1.1.cmml" xref="id14.13.13.m1.1.1"><ci id="id14.13.13.m1.1.1.1.cmml" xref="id14.13.13.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id14.13.13.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id14.13.13.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 David Atkinson

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id15.14.14.m2.1"><semantics id="id15.14.14.m2.1a"><msup id="id15.14.14.m2.1.1" xref="id15.14.14.m2.1.1.cmml"><mi id="id15.14.14.m2.1.1a" xref="id15.14.14.m2.1.1.cmml"></mi><mi id="id15.14.14.m2.1.1.1" mathcolor="#265ED4" xref="id15.14.14.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id15.14.14.m2.1b"><apply id="id15.14.14.m2.1.1.cmml" xref="id15.14.14.m2.1.1"><ci id="id15.14.14.m2.1.1.1.cmml" xref="id15.14.14.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id15.14.14.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id15.14.14.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Russell Authur

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id16.15.15.m3.1"><semantics id="id16.15.15.m3.1a"><msup id="id16.15.15.m3.1.1" xref="id16.15.15.m3.1.1.cmml"><mi id="id16.15.15.m3.1.1a" xref="id16.15.15.m3.1.1.cmml"></mi><mi id="id16.15.15.m3.1.1.1" mathcolor="#265ED4" xref="id16.15.15.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id16.15.15.m3.1b"><apply id="id16.15.15.m3.1.1.cmml" xref="id16.15.15.m3.1.1"><ci id="id16.15.15.m3.1.1.1.cmml" xref="id16.15.15.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id16.15.15.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id16.15.15.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Khyathi Raghavi Chandu

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id17.16.16.m4.1"><semantics id="id17.16.16.m4.1a"><msup id="id17.16.16.m4.1.1" xref="id17.16.16.m4.1.1.cmml"><mi id="id17.16.16.m4.1.1a" xref="id17.16.16.m4.1.1.cmml"></mi><mi id="id17.16.16.m4.1.1.1" mathcolor="#265ED4" xref="id17.16.16.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id17.16.16.m4.1b"><apply id="id17.16.16.m4.1.1.cmml" xref="id17.16.16.m4.1.1"><ci id="id17.16.16.m4.1.1.1.cmml" xref="id17.16.16.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id17.16.16.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id17.16.16.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id23.22.22">Arman Cohan

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\gamma}}}" class="ltx_Math" display="inline" id="id18.17.17.m1.1"><semantics id="id18.17.17.m1.1a"><msup id="id18.17.17.m1.1.1" xref="id18.17.17.m1.1.1.cmml"><mi id="id18.17.17.m1.1.1a" xref="id18.17.17.m1.1.1.cmml"></mi><mi id="id18.17.17.m1.1.1.1" mathcolor="#265ED4" xref="id18.17.17.m1.1.1.1.cmml">γ</mi></msup><annotation-xml encoding="MathML-Content" id="id18.17.17.m1.1b"><apply id="id18.17.17.m1.1.1.cmml" xref="id18.17.17.m1.1.1"><ci id="id18.17.17.m1.1.1.1.cmml" xref="id18.17.17.m1.1.1.1">𝛾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id18.17.17.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\gamma}}}</annotation><annotation encoding="application/x-llamapun" id="id18.17.17.m1.1d">start_FLOATSUPERSCRIPT bold_italic_γ end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id19.18.18.m2.1"><semantics id="id19.18.18.m2.1a"><msup id="id19.18.18.m2.1.1" xref="id19.18.18.m2.1.1.cmml"><mi id="id19.18.18.m2.1.1a" xref="id19.18.18.m2.1.1.cmml"></mi><mi id="id19.18.18.m2.1.1.1" mathcolor="#265ED4" xref="id19.18.18.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id19.18.18.m2.1b"><apply id="id19.18.18.m2.1.1.cmml" xref="id19.18.18.m2.1.1"><ci id="id19.18.18.m2.1.1.1.cmml" xref="id19.18.18.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id19.18.18.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id19.18.18.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Jennifer Dumas

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id20.19.19.m3.1"><semantics id="id20.19.19.m3.1a"><msup id="id20.19.19.m3.1.1" xref="id20.19.19.m3.1.1.cmml"><mi id="id20.19.19.m3.1.1a" xref="id20.19.19.m3.1.1.cmml"></mi><mi id="id20.19.19.m3.1.1.1" mathcolor="#265ED4" xref="id20.19.19.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id20.19.19.m3.1b"><apply id="id20.19.19.m3.1.1.cmml" xref="id20.19.19.m3.1.1"><ci id="id20.19.19.m3.1.1.1.cmml" xref="id20.19.19.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id20.19.19.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id20.19.19.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Yanai Elazar

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id21.20.20.m4.1"><semantics id="id21.20.20.m4.1a"><msup id="id21.20.20.m4.1.1" xref="id21.20.20.m4.1.1.cmml"><mi id="id21.20.20.m4.1.1a" xref="id21.20.20.m4.1.1.cmml"></mi><mi id="id21.20.20.m4.1.1.1" mathcolor="#265ED4" xref="id21.20.20.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id21.20.20.m4.1b"><apply id="id21.20.20.m4.1.1.cmml" xref="id21.20.20.m4.1.1"><ci id="id21.20.20.m4.1.1.1.cmml" xref="id21.20.20.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id21.20.20.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id21.20.20.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id22.21.21.m5.1"><semantics id="id22.21.21.m5.1a"><msup id="id22.21.21.m5.1.1" xref="id22.21.21.m5.1.1.cmml"><mi id="id22.21.21.m5.1.1a" xref="id22.21.21.m5.1.1.cmml"></mi><mi id="id22.21.21.m5.1.1.1" mathcolor="#265ED4" xref="id22.21.21.m5.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id22.21.21.m5.1b"><apply id="id22.21.21.m5.1.1.cmml" xref="id22.21.21.m5.1.1"><ci id="id22.21.21.m5.1.1.1.cmml" xref="id22.21.21.m5.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id22.21.21.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id22.21.21.m5.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Yuling Gu

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id23.22.22.m6.1"><semantics id="id23.22.22.m6.1a"><msup id="id23.22.22.m6.1.1" xref="id23.22.22.m6.1.1.cmml"><mi id="id23.22.22.m6.1.1a" xref="id23.22.22.m6.1.1.cmml"></mi><mi id="id23.22.22.m6.1.1.1" mathcolor="#265ED4" xref="id23.22.22.m6.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id23.22.22.m6.1b"><apply id="id23.22.22.m6.1.1.cmml" xref="id23.22.22.m6.1.1"><ci id="id23.22.22.m6.1.1.1.cmml" xref="id23.22.22.m6.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id23.22.22.m6.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id23.22.22.m6.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id27.26.26">Jack Hessel

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id24.23.23.m1.1"><semantics id="id24.23.23.m1.1a"><msup id="id24.23.23.m1.1.1" xref="id24.23.23.m1.1.1.cmml"><mi id="id24.23.23.m1.1.1a" xref="id24.23.23.m1.1.1.cmml"></mi><mi id="id24.23.23.m1.1.1.1" mathcolor="#265ED4" xref="id24.23.23.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id24.23.23.m1.1b"><apply id="id24.23.23.m1.1.1.cmml" xref="id24.23.23.m1.1.1"><ci id="id24.23.23.m1.1.1.1.cmml" xref="id24.23.23.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id24.23.23.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id24.23.23.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Tushar Khot

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id25.24.24.m2.1"><semantics id="id25.24.24.m2.1a"><msup id="id25.24.24.m2.1.1" xref="id25.24.24.m2.1.1.cmml"><mi id="id25.24.24.m2.1.1a" xref="id25.24.24.m2.1.1.cmml"></mi><mi id="id25.24.24.m2.1.1.1" mathcolor="#265ED4" xref="id25.24.24.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id25.24.24.m2.1b"><apply id="id25.24.24.m2.1.1.cmml" xref="id25.24.24.m2.1.1"><ci id="id25.24.24.m2.1.1.1.cmml" xref="id25.24.24.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id25.24.24.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id25.24.24.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 William Merrill

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\delta}}}" class="ltx_Math" display="inline" id="id26.25.25.m3.1"><semantics id="id26.25.25.m3.1a"><msup id="id26.25.25.m3.1.1" xref="id26.25.25.m3.1.1.cmml"><mi id="id26.25.25.m3.1.1a" xref="id26.25.25.m3.1.1.cmml"></mi><mi id="id26.25.25.m3.1.1.1" mathcolor="#265ED4" xref="id26.25.25.m3.1.1.1.cmml">δ</mi></msup><annotation-xml encoding="MathML-Content" id="id26.25.25.m3.1b"><apply id="id26.25.25.m3.1.1.cmml" xref="id26.25.25.m3.1.1"><ci id="id26.25.25.m3.1.1.1.cmml" xref="id26.25.25.m3.1.1.1">𝛿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id26.25.25.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\delta}}}</annotation><annotation encoding="application/x-llamapun" id="id26.25.25.m3.1d">start_FLOATSUPERSCRIPT bold_italic_δ end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Jacob Morrison

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id27.26.26.m4.1"><semantics id="id27.26.26.m4.1a"><msup id="id27.26.26.m4.1.1" xref="id27.26.26.m4.1.1.cmml"><mi id="id27.26.26.m4.1.1a" xref="id27.26.26.m4.1.1.cmml"></mi><mi id="id27.26.26.m4.1.1.1" mathcolor="#265ED4" xref="id27.26.26.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id27.26.26.m4.1b"><apply id="id27.26.26.m4.1.1.cmml" xref="id27.26.26.m4.1.1"><ci id="id27.26.26.m4.1.1.1.cmml" xref="id27.26.26.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id27.26.26.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id27.26.26.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id31.30.30">
Niklas Muennighoff


 Aakanksha Naik

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id29.28.28.m2.1"><semantics id="id29.28.28.m2.1a"><msup id="id29.28.28.m2.1.1" xref="id29.28.28.m2.1.1.cmml"><mi id="id29.28.28.m2.1.1a" xref="id29.28.28.m2.1.1.cmml"></mi><mi id="id29.28.28.m2.1.1.1" mathcolor="#265ED4" xref="id29.28.28.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id29.28.28.m2.1b"><apply id="id29.28.28.m2.1.1.cmml" xref="id29.28.28.m2.1.1"><ci id="id29.28.28.m2.1.1.1.cmml" xref="id29.28.28.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id29.28.28.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id29.28.28.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Crystal Nam

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id30.29.29.m3.1"><semantics id="id30.29.29.m3.1a"><msup id="id30.29.29.m3.1.1" xref="id30.29.29.m3.1.1.cmml"><mi id="id30.29.29.m3.1.1a" xref="id30.29.29.m3.1.1.cmml"></mi><mi id="id30.29.29.m3.1.1.1" mathcolor="#265ED4" xref="id30.29.29.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id30.29.29.m3.1b"><apply id="id30.29.29.m3.1.1.cmml" xref="id30.29.29.m3.1.1"><ci id="id30.29.29.m3.1.1.1.cmml" xref="id30.29.29.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id30.29.29.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id30.29.29.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Matthew E. Peters

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id31.30.30.m4.1"><semantics id="id31.30.30.m4.1a"><msup id="id31.30.30.m4.1.1" xref="id31.30.30.m4.1.1.cmml"><mi id="id31.30.30.m4.1.1a" xref="id31.30.30.m4.1.1.cmml"></mi><mi id="id31.30.30.m4.1.1.1" mathcolor="#265ED4" xref="id31.30.30.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id31.30.30.m4.1b"><apply id="id31.30.30.m4.1.1.cmml" xref="id31.30.30.m4.1.1"><ci id="id31.30.30.m4.1.1.1.cmml" xref="id31.30.30.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id31.30.30.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id31.30.30.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id36.35.35">Valentina Pyatkin

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id32.31.31.m1.1"><semantics id="id32.31.31.m1.1a"><msup id="id32.31.31.m1.1.1" xref="id32.31.31.m1.1.1.cmml"><mi id="id32.31.31.m1.1.1a" xref="id32.31.31.m1.1.1.cmml"></mi><mi id="id32.31.31.m1.1.1.1" mathcolor="#265ED4" xref="id32.31.31.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id32.31.31.m1.1b"><apply id="id32.31.31.m1.1.1.cmml" xref="id32.31.31.m1.1.1"><ci id="id32.31.31.m1.1.1.1.cmml" xref="id32.31.31.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id32.31.31.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id32.31.31.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id33.32.32.m2.1"><semantics id="id33.32.32.m2.1a"><msup id="id33.32.32.m2.1.1" xref="id33.32.32.m2.1.1.cmml"><mi id="id33.32.32.m2.1.1a" xref="id33.32.32.m2.1.1.cmml"></mi><mi id="id33.32.32.m2.1.1.1" mathcolor="#265ED4" xref="id33.32.32.m2.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id33.32.32.m2.1b"><apply id="id33.32.32.m2.1.1.cmml" xref="id33.32.32.m2.1.1"><ci id="id33.32.32.m2.1.1.1.cmml" xref="id33.32.32.m2.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id33.32.32.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id33.32.32.m2.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Abhilasha Ravichander

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id34.33.33.m3.1"><semantics id="id34.33.33.m3.1a"><msup id="id34.33.33.m3.1.1" xref="id34.33.33.m3.1.1.cmml"><mi id="id34.33.33.m3.1.1a" xref="id34.33.33.m3.1.1.cmml"></mi><mi id="id34.33.33.m3.1.1.1" mathcolor="#265ED4" xref="id34.33.33.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id34.33.33.m3.1b"><apply id="id34.33.33.m3.1.1.cmml" xref="id34.33.33.m3.1.1"><ci id="id34.33.33.m3.1.1.1.cmml" xref="id34.33.33.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id34.33.33.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id34.33.33.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Dustin Schwenk

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id35.34.34.m4.1"><semantics id="id35.34.34.m4.1a"><msup id="id35.34.34.m4.1.1" xref="id35.34.34.m4.1.1.cmml"><mi id="id35.34.34.m4.1.1a" xref="id35.34.34.m4.1.1.cmml"></mi><mi id="id35.34.34.m4.1.1.1" mathcolor="#265ED4" xref="id35.34.34.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id35.34.34.m4.1b"><apply id="id35.34.34.m4.1.1.cmml" xref="id35.34.34.m4.1.1"><ci id="id35.34.34.m4.1.1.1.cmml" xref="id35.34.34.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id35.34.34.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id35.34.34.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Saurabh Shah

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id36.35.35.m5.1"><semantics id="id36.35.35.m5.1a"><msup id="id36.35.35.m5.1.1" xref="id36.35.35.m5.1.1.cmml"><mi id="id36.35.35.m5.1.1a" xref="id36.35.35.m5.1.1.cmml"></mi><mi id="id36.35.35.m5.1.1.1" mathcolor="#265ED4" xref="id36.35.35.m5.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id36.35.35.m5.1b"><apply id="id36.35.35.m5.1.1.cmml" xref="id36.35.35.m5.1.1"><ci id="id36.35.35.m5.1.1.1.cmml" xref="id36.35.35.m5.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id36.35.35.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id36.35.35.m5.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id41.40.40">Will Smith

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id37.36.36.m1.1"><semantics id="id37.36.36.m1.1a"><msup id="id37.36.36.m1.1.1" xref="id37.36.36.m1.1.1.cmml"><mi id="id37.36.36.m1.1.1a" xref="id37.36.36.m1.1.1.cmml"></mi><mi id="id37.36.36.m1.1.1.1" mathcolor="#265ED4" xref="id37.36.36.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id37.36.36.m1.1b"><apply id="id37.36.36.m1.1.1.cmml" xref="id37.36.36.m1.1.1"><ci id="id37.36.36.m1.1.1.1.cmml" xref="id37.36.36.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id37.36.36.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id37.36.36.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Emma Strubell

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id38.37.37.m2.1"><semantics id="id38.37.37.m2.1a"><msup id="id38.37.37.m2.1.1" xref="id38.37.37.m2.1.1.cmml"><mi id="id38.37.37.m2.1.1a" xref="id38.37.37.m2.1.1.cmml"></mi><mi id="id38.37.37.m2.1.1.1" mathcolor="#265ED4" xref="id38.37.37.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id38.37.37.m2.1b"><apply id="id38.37.37.m2.1.1.cmml" xref="id38.37.37.m2.1.1"><ci id="id38.37.37.m2.1.1.1.cmml" xref="id38.37.37.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id38.37.37.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id38.37.37.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\mu}}}" class="ltx_Math" display="inline" id="id39.38.38.m3.1"><semantics id="id39.38.38.m3.1a"><msup id="id39.38.38.m3.1.1" xref="id39.38.38.m3.1.1.cmml"><mi id="id39.38.38.m3.1.1a" xref="id39.38.38.m3.1.1.cmml"></mi><mi id="id39.38.38.m3.1.1.1" mathcolor="#265ED4" xref="id39.38.38.m3.1.1.1.cmml">μ</mi></msup><annotation-xml encoding="MathML-Content" id="id39.38.38.m3.1b"><apply id="id39.38.38.m3.1.1.cmml" xref="id39.38.38.m3.1.1"><ci id="id39.38.38.m3.1.1.1.cmml" xref="id39.38.38.m3.1.1.1">𝜇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id39.38.38.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\mu}}}</annotation><annotation encoding="application/x-llamapun" id="id39.38.38.m3.1d">start_FLOATSUPERSCRIPT bold_italic_μ end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Nishant Subramani

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id40.39.39.m4.1"><semantics id="id40.39.39.m4.1a"><msup id="id40.39.39.m4.1.1" xref="id40.39.39.m4.1.1.cmml"><mi id="id40.39.39.m4.1.1a" xref="id40.39.39.m4.1.1.cmml"></mi><mi id="id40.39.39.m4.1.1.1" mathcolor="#265ED4" xref="id40.39.39.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id40.39.39.m4.1b"><apply id="id40.39.39.m4.1.1.cmml" xref="id40.39.39.m4.1.1"><ci id="id40.39.39.m4.1.1.1.cmml" xref="id40.39.39.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id40.39.39.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id40.39.39.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Mitchell Wortsman

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id41.40.40.m5.1"><semantics id="id41.40.40.m5.1a"><msup id="id41.40.40.m5.1.1" xref="id41.40.40.m5.1.1.cmml"><mi id="id41.40.40.m5.1.1a" xref="id41.40.40.m5.1.1.cmml"></mi><mi id="id41.40.40.m5.1.1.1" mathcolor="#265ED4" xref="id41.40.40.m5.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id41.40.40.m5.1b"><apply id="id41.40.40.m5.1.1.cmml" xref="id41.40.40.m5.1.1"><ci id="id41.40.40.m5.1.1.1.cmml" xref="id41.40.40.m5.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id41.40.40.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id41.40.40.m5.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id44.43.43">Pradeep Dasigi

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id42.41.41.m1.1"><semantics id="id42.41.41.m1.1a"><msup id="id42.41.41.m1.1.1" xref="id42.41.41.m1.1.1.cmml"><mi id="id42.41.41.m1.1.1a" xref="id42.41.41.m1.1.1.cmml"></mi><mi id="id42.41.41.m1.1.1.1" mathcolor="#265ED4" xref="id42.41.41.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id42.41.41.m1.1b"><apply id="id42.41.41.m1.1.1.cmml" xref="id42.41.41.m1.1.1"><ci id="id42.41.41.m1.1.1.1.cmml" xref="id42.41.41.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id42.41.41.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id42.41.41.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Nathan Lambert

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id43.42.42.m2.1"><semantics id="id43.42.42.m2.1a"><msup id="id43.42.42.m2.1.1" xref="id43.42.42.m2.1.1.cmml"><mi id="id43.42.42.m2.1.1a" xref="id43.42.42.m2.1.1.cmml"></mi><mi id="id43.42.42.m2.1.1.1" mathcolor="#265ED4" xref="id43.42.42.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id43.42.42.m2.1b"><apply id="id43.42.42.m2.1.1.cmml" xref="id43.42.42.m2.1.1"><ci id="id43.42.42.m2.1.1.1.cmml" xref="id43.42.42.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id43.42.42.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id43.42.42.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Kyle Richardson

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id44.43.43.m3.1"><semantics id="id44.43.43.m3.1a"><msup id="id44.43.43.m3.1.1" xref="id44.43.43.m3.1.1.cmml"><mi id="id44.43.43.m3.1.1a" xref="id44.43.43.m3.1.1.cmml"></mi><mi id="id44.43.43.m3.1.1.1" mathcolor="#265ED4" xref="id44.43.43.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id44.43.43.m3.1b"><apply id="id44.43.43.m3.1.1.cmml" xref="id44.43.43.m3.1.1"><ci id="id44.43.43.m3.1.1.1.cmml" xref="id44.43.43.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id44.43.43.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id44.43.43.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id48.47.47">Luke Zettlemoyer

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id45.44.44.m1.1"><semantics id="id45.44.44.m1.1a"><msup id="id45.44.44.m1.1.1" xref="id45.44.44.m1.1.1.cmml"><mi id="id45.44.44.m1.1.1a" xref="id45.44.44.m1.1.1.cmml"></mi><mi id="id45.44.44.m1.1.1.1" mathcolor="#265ED4" xref="id45.44.44.m1.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id45.44.44.m1.1b"><apply id="id45.44.44.m1.1.1.cmml" xref="id45.44.44.m1.1.1"><ci id="id45.44.44.m1.1.1.1.cmml" xref="id45.44.44.m1.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id45.44.44.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id45.44.44.m1.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
Jesse Dodge

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id46.45.45.m2.1"><semantics id="id46.45.45.m2.1a"><msup id="id46.45.45.m2.1.1" xref="id46.45.45.m2.1.1.cmml"><mi id="id46.45.45.m2.1.1a" xref="id46.45.45.m2.1.1.cmml"></mi><mi id="id46.45.45.m2.1.1.1" mathcolor="#265ED4" xref="id46.45.45.m2.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id46.45.45.m2.1b"><apply id="id46.45.45.m2.1.1.cmml" xref="id46.45.45.m2.1.1"><ci id="id46.45.45.m2.1.1.1.cmml" xref="id46.45.45.m2.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id46.45.45.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id46.45.45.m2.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Kyle Lo

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id47.46.46.m3.1"><semantics id="id47.46.46.m3.1a"><msup id="id47.46.46.m3.1.1" xref="id47.46.46.m3.1.1.cmml"><mi id="id47.46.46.m3.1.1a" xref="id47.46.46.m3.1.1.cmml"></mi><mi id="id47.46.46.m3.1.1.1" mathcolor="#265ED4" xref="id47.46.46.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id47.46.46.m3.1b"><apply id="id47.46.46.m3.1.1.cmml" xref="id47.46.46.m3.1.1"><ci id="id47.46.46.m3.1.1.1.cmml" xref="id47.46.46.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id47.46.46.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id47.46.46.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Luca Soldaini

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id48.47.47.m4.1"><semantics id="id48.47.47.m4.1a"><msup id="id48.47.47.m4.1.1" xref="id48.47.47.m4.1.1.cmml"><mi id="id48.47.47.m4.1.1a" xref="id48.47.47.m4.1.1.cmml"></mi><mi id="id48.47.47.m4.1.1.1" mathcolor="#265ED4" xref="id48.47.47.m4.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id48.47.47.m4.1b"><apply id="id48.47.47.m4.1.1.cmml" xref="id48.47.47.m4.1.1"><ci id="id48.47.47.m4.1.1.1.cmml" xref="id48.47.47.m4.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id48.47.47.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id48.47.47.m4.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break"><span class="ltx_text ltx_font_bold" id="id52.51.51">Noah A. Smith

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id49.48.48.m1.1"><semantics id="id49.48.48.m1.1a"><msup id="id49.48.48.m1.1.1" xref="id49.48.48.m1.1.1.cmml"><mi id="id49.48.48.m1.1.1a" xref="id49.48.48.m1.1.1.cmml"></mi><mi id="id49.48.48.m1.1.1.1" mathcolor="#265ED4" xref="id49.48.48.m1.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id49.48.48.m1.1b"><apply id="id49.48.48.m1.1.1.cmml" xref="id49.48.48.m1.1.1"><ci id="id49.48.48.m1.1.1.1.cmml" xref="id49.48.48.m1.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id49.48.48.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id49.48.48.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id50.49.49.m2.1"><semantics id="id50.49.49.m2.1a"><msup id="id50.49.49.m2.1.1" xref="id50.49.49.m2.1.1.cmml"><mi id="id50.49.49.m2.1.1a" xref="id50.49.49.m2.1.1.cmml"></mi><mi id="id50.49.49.m2.1.1.1" mathcolor="#265ED4" xref="id50.49.49.m2.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id50.49.49.m2.1b"><apply id="id50.49.49.m2.1.1.cmml" xref="id50.49.49.m2.1.1"><ci id="id50.49.49.m2.1.1.1.cmml" xref="id50.49.49.m2.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id50.49.49.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id50.49.49.m2.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
 Hannaneh Hajishirzi

<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id51.50.50.m3.1"><semantics id="id51.50.50.m3.1a"><msup id="id51.50.50.m3.1.1" xref="id51.50.50.m3.1.1.cmml"><mi id="id51.50.50.m3.1.1a" xref="id51.50.50.m3.1.1.cmml"></mi><mi id="id51.50.50.m3.1.1.1" mathcolor="#265ED4" xref="id51.50.50.m3.1.1.1.cmml">α</mi></msup><annotation-xml encoding="MathML-Content" id="id51.50.50.m3.1b"><apply id="id51.50.50.m3.1.1.cmml" xref="id51.50.50.m3.1.1"><ci id="id51.50.50.m3.1.1.1.cmml" xref="id51.50.50.m3.1.1.1">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id51.50.50.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id51.50.50.m3.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math><math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id52.51.51.m4.1"><semantics id="id52.51.51.m4.1a"><msup id="id52.51.51.m4.1.1" xref="id52.51.51.m4.1.1.cmml"><mi id="id52.51.51.m4.1.1a" xref="id52.51.51.m4.1.1.cmml"></mi><mi id="id52.51.51.m4.1.1.1" mathcolor="#265ED4" xref="id52.51.51.m4.1.1.1.cmml">β</mi></msup><annotation-xml encoding="MathML-Content" id="id52.51.51.m4.1b"><apply id="id52.51.51.m4.1.1.cmml" xref="id52.51.51.m4.1.1"><ci id="id52.51.51.m4.1.1.1.cmml" xref="id52.51.51.m4.1.1.1">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id52.51.51.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id52.51.51.m4.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>
</span>
<br class="ltx_break">
<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}" class="ltx_Math" display="inline" id="id53.52.m1.1"><semantics id="id53.52.m1.1a"><msup id="id53.52.m1.1.1" xref="id53.52.m1.1.1.cmml"><mi id="id53.52.m1.1.1a" xref="id53.52.m1.1.1.cmml"></mi><mi id="id53.52.m1.1.1.1" mathcolor="#265ED4" xref="id53.52.m1.1.1.1.cmml">𝜶</mi></msup><annotation-xml encoding="MathML-Content" id="id53.52.m1.1b"><apply id="id53.52.m1.1.1.cmml" xref="id53.52.m1.1.1"><ci id="id53.52.m1.1.1.1.cmml" xref="id53.52.m1.1.1.1">𝜶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id53.52.m1.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\alpha}}}</annotation><annotation encoding="application/x-llamapun" id="id53.52.m1.1d">start_FLOATSUPERSCRIPT bold_italic_α end_FLOATSUPERSCRIPT</annotation></semantics></math>Allen Institute for Artificial Intelligence
 
<br class="ltx_break">
<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}" class="ltx_Math" display="inline" id="id54.53.m2.1"><semantics id="id54.53.m2.1a"><msup id="id54.53.m2.1.1" xref="id54.53.m2.1.1.cmml"><mi id="id54.53.m2.1.1a" xref="id54.53.m2.1.1.cmml"></mi><mi id="id54.53.m2.1.1.1" mathcolor="#265ED4" xref="id54.53.m2.1.1.1.cmml">𝜷</mi></msup><annotation-xml encoding="MathML-Content" id="id54.53.m2.1b"><apply id="id54.53.m2.1.1.cmml" xref="id54.53.m2.1.1"><ci id="id54.53.m2.1.1.1.cmml" xref="id54.53.m2.1.1.1">𝜷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id54.53.m2.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\beta}}}</annotation><annotation encoding="application/x-llamapun" id="id54.53.m2.1d">start_FLOATSUPERSCRIPT bold_italic_β end_FLOATSUPERSCRIPT</annotation></semantics></math>University of Washington  <math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\gamma}}}" class="ltx_Math" display="inline" id="id55.54.m3.1"><semantics id="id55.54.m3.1a"><msup id="id55.54.m3.1.1" xref="id55.54.m3.1.1.cmml"><mi id="id55.54.m3.1.1a" xref="id55.54.m3.1.1.cmml"></mi><mi id="id55.54.m3.1.1.1" mathcolor="#265ED4" xref="id55.54.m3.1.1.1.cmml">𝜸</mi></msup><annotation-xml encoding="MathML-Content" id="id55.54.m3.1b"><apply id="id55.54.m3.1.1.cmml" xref="id55.54.m3.1.1"><ci id="id55.54.m3.1.1.1.cmml" xref="id55.54.m3.1.1.1">𝜸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id55.54.m3.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\gamma}}}</annotation><annotation encoding="application/x-llamapun" id="id55.54.m3.1d">start_FLOATSUPERSCRIPT bold_italic_γ end_FLOATSUPERSCRIPT</annotation></semantics></math>Yale University
 
<br class="ltx_break">
<math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\delta}}}" class="ltx_Math" display="inline" id="id56.55.m4.1"><semantics id="id56.55.m4.1a"><msup id="id56.55.m4.1.1" xref="id56.55.m4.1.1.cmml"><mi id="id56.55.m4.1.1a" xref="id56.55.m4.1.1.cmml"></mi><mi id="id56.55.m4.1.1.1" mathcolor="#265ED4" xref="id56.55.m4.1.1.1.cmml">𝜹</mi></msup><annotation-xml encoding="MathML-Content" id="id56.55.m4.1b"><apply id="id56.55.m4.1.1.cmml" xref="id56.55.m4.1.1"><ci id="id56.55.m4.1.1.1.cmml" xref="id56.55.m4.1.1.1">𝜹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id56.55.m4.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\delta}}}</annotation><annotation encoding="application/x-llamapun" id="id56.55.m4.1d">start_FLOATSUPERSCRIPT bold_italic_δ end_FLOATSUPERSCRIPT</annotation></semantics></math>New York University  <math alttext="{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\mu}}}" class="ltx_Math" display="inline" id="id57.56.m5.1"><semantics id="id57.56.m5.1a"><msup id="id57.56.m5.1.1" xref="id57.56.m5.1.1.cmml"><mi id="id57.56.m5.1.1a" xref="id57.56.m5.1.1.cmml"></mi><mi id="id57.56.m5.1.1.1" mathcolor="#265ED4" xref="id57.56.m5.1.1.1.cmml">𝝁</mi></msup><annotation-xml encoding="MathML-Content" id="id57.56.m5.1b"><apply id="id57.56.m5.1.1.cmml" xref="id57.56.m5.1.1"><ci id="id57.56.m5.1.1.1.cmml" xref="id57.56.m5.1.1.1">𝝁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id57.56.m5.1c">{}^{{\color[rgb]{0.1484375,0.3671875,0.83203125}\boldsymbol{\mu}}}</annotation><annotation encoding="application/x-llamapun" id="id57.56.m5.1d">start_FLOATSUPERSCRIPT bold_italic_μ end_FLOATSUPERSCRIPT</annotation></semantics></math>Carnegie Mellon University

<br class="ltx_break"><span class="ltx_text ltx_font_typewriter" id="id64.57.id1">olmo@allenai.org</span>
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_dates">(February 28, 2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id65.id1">언어 모델(LMs)은 NLP 연구와 상업용 제품 제공 모두에서 유비쿼터스화되었다. 상업적 중요성이 급증함에 따라 가장 강력한 모델은 독점 인터페이스 뒤에서 폐쇄되고, 훈련 데이터, 아키텍처 및 개발의 중요한 세부 정보가 공개되지 않았다. 편향과 잠재적 위험을 포함하여 이러한 모델을 과학적으로 연구하는 데 이러한 세부 사항이 중요하다는 점을 감안할 때 연구 커뮤니티가 강력하고 진정으로 열린 LMs에 접근하는 것이 필수적이라고 믿는다. 이를 위해 이 기술 보고서는 OLMo의 첫 번째 릴리스에 대해 자세히 설명하며, 진정한 <span class="ltx_text ltx_font_bold" id="id65.id1.1">O</span>pen <span class="ltx_text ltx_font_bold" id="id65.id1.2">L</span>anguage <span class="ltx_text ltx_font_bold" id="id65.id1.3">Mo</span>del 및 그 프레임워크를 사용하여 언어 모델링 과학을 빌드하고 연구합니다. 기존의 모델 가중치 및 추론 코드만을 발표하던 기존 연구들과는 달리 OLMo와 학습 데이터, 학습 및 평가 코드를 포함한 전체 프레임워크를 발표한다. 이번 발표가 열린 연구 커뮤니티에 힘을 실어주고 강화하며 새로운 혁신 물결을 불러일으키기를 바랍니다.</p>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="id63.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="id58.1.1">
<td class="ltx_td ltx_align_right" id="id58.1.1.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id58.1.1.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id58.1.1.1.1.g1" src="https://arxiv.org/html/2402.00838v3/x2.png" width="22"></span></td>
<td class="ltx_td ltx_align_center" id="id58.1.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id58.1.1.2.1">Weights</span></td>
<td class="ltx_td ltx_align_left" id="id58.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B" title="">https://huggingface.co/allenai/OLMo-7B</a></td>
</tr>
<tr class="ltx_tr" id="id59.2.2">
<td class="ltx_td ltx_align_right" id="id59.2.2.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id59.2.2.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id59.2.2.1.1.g1" src="https://arxiv.org/html/2402.00838v3/x3.png" width="20"></span></td>
<td class="ltx_td ltx_align_center" id="id59.2.2.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id59.2.2.2.1">Code</span></td>
<td class="ltx_td ltx_align_left" id="id59.2.2.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo" title="">https://github.com/allenai/OLMo</a></td>
</tr>
<tr class="ltx_tr" id="id60.3.3">
<td class="ltx_td ltx_align_right" id="id60.3.3.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id60.3.3.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id60.3.3.1.1.g1" src="https://arxiv.org/html/2402.00838v3/x2.png" width="22"></span></td>
<td class="ltx_td ltx_align_center" id="id60.3.3.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id60.3.3.2.1">Data</span></td>
<td class="ltx_td ltx_align_left" id="id60.3.3.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/allenai/dolma" title="">https://huggingface.co/datasets/allenai/dolma</a></td>
</tr>
<tr class="ltx_tr" id="id61.4.4">
<td class="ltx_td ltx_align_right" id="id61.4.4.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id61.4.4.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id61.4.4.1.1.g1" src="https://arxiv.org/html/2402.00838v3/x4.png" width="20"></span></td>
<td class="ltx_td ltx_align_center" id="id61.4.4.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id61.4.4.2.1">Evaluation</span></td>
<td class="ltx_td ltx_align_left" id="id61.4.4.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo-Eval" title="">https://github.com/allenai/OLMo-Eval</a></td>
</tr>
<tr class="ltx_tr" id="id62.5.5">
<td class="ltx_td ltx_align_right" id="id62.5.5.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id62.5.5.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="20" id="id62.5.5.1.1.g1" src="https://arxiv.org/html/2402.00838v3/x5.png" width="20"></span></td>
<td class="ltx_td ltx_align_center" id="id62.5.5.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id62.5.5.2.1">Adaptation</span></td>
<td class="ltx_td ltx_align_left" id="id62.5.5.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/open-instruct" title="">https://github.com/allenai/open-instruct</a></td>
</tr>
<tr class="ltx_tr" id="id63.6.6">
<td class="ltx_td ltx_align_right" id="id63.6.6.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="id63.6.6.1.1" style="position:relative; bottom:-1.5pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="15" id="id63.6.6.1.1.g1" src="https://arxiv.org/html/2402.00838v3/extracted/5436352/figures/wandb-logo.png" width="16"></span></td>
<td class="ltx_td ltx_align_center" id="id63.6.6.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="id63.6.6.2.1">W&amp;B Logs</span></td>
<td class="ltx_td ltx_align_left" id="id63.6.6.3" style="padding-top:1pt;padding-bottom:1pt;"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5" title="">https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5</a></td>
</tr>
</tbody>
</table>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">언어 모델은 수년 동안 NLP 기술의 중심에 있었다 <cite class="ltx_cite ltx_citemacro_citep">(Rosenfeld, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib67" title="">2000</a>; Bengio et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib6" title="">2003</a>; Mikolov et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib48" title="">2013</a>; Peters et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib58" title="">2018</a>; Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib12" title="">2020</a>)</cite> 최근 대규모 사전 훈련과 정렬에 대한 인간 주석으로 인해 상업적으로 가치가 있는 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib53" title="">2023</a>)</cite>가 되었다. 그러나 상업적 가치가 증가함에 따라 가장 큰 모델은 독점 인터페이스 뒤에 문이 닫혔으며 중요한 세부 사항은 공개되지 않았다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">우리는 연구 커뮤니티를 위한 개방형 언어 모델에 대한 완전한 액세스가 이러한 모델의 강점과 약점, 편향과 위험에 대한 과학적 연구에 중요하다고 믿는다. 따라서 우리는 학습 데이터, 훈련 및 평가 코드, 중간 모델 체크포인트 및 훈련 로그와 함께 LM을 구축, 연구 및 발전시키기 위한 최첨단 개방형 언어 모델 및 프레임워크인 <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">OLMo</span>을 소개한다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">최근 LM 릴리즈는 개방도가 다양합니다. 예를 들어, Mistral 8x7B는 모델 가중치 및 간략 보고서 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib34" title="">2024</a>)</cite>를 제공했고, LLaMA는 심층 적응 훈련 지침 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite>를 제공했으며, 모자이크 사전 훈련 트랜스포머는 데이터 자체 <cite class="ltx_cite ltx_citemacro_citep">(MosaicML NLP Team, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib50" title="">2023</a>)</cite>는 아니지만 데이터 세트 분포를 포함한 많은 세부 사항을 제공했습니다. 팔콘의 사전 훈련 데이터는 부분적으로 릴리스되었다. <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib2" title="">2023</a>)</cite>, 그리고 가장 개방된 모델인 피티아 제품군 <cite class="ltx_cite ltx_citemacro_citep">(Biderman et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib7" title="">2023</a>)</cite>와 BLOOM <cite class="ltx_cite ltx_citemacro_citep">(BigScience et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib8" title="">2022</a>)</cite>는 훈련 코드, 모델 체크포인트, 훈련 데이터 등을 릴리스했다.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">OLMo를 사용하여 우리는 허용 라이선스를 사용하여 여러 하드웨어 유형, 훈련 로그 및 사용된 정확한 데이터 세트에 걸쳐 여러 훈련 체크포인트인 데이터에서 훈련으로 평가 도구로 전체 프레임워크를 릴리스합니다. 우리는 이것을 한 유일한 팀이 아니다; LLM360의 최근 연구는 유사한 목표 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib40" title="">2023</a>)</cite>를 목표로 한다. OLMo는 그들의 모델에서 LLaMA2와 같은 모델의 최첨단 능력으로의 격차를 좁히고 있다. 이 프로젝트는 다양한 개방성의 정도로 이전의 모든 노력에서 얻은 교훈으로부터 이익을 얻었으며, 우리는 크고 다양한 개방 모델의 모집단이 언어 모델을 이해하는 과학적 진전과 유용성을 향상시키는 공학적 진전에 가장 좋은 희망이라고 믿는다.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The OLMo framework encompasses the tools and resources required for building and researching language models. For training and modeling, it includes full model weights, training code, training logs, ablations, training metrics in the form of Weights &amp; Biases logs, and inference code. This first release includes four variants of our language model at the 7B scale corresponding to different architectures, optimizers, and training hardware, and one model at the 1B scale, all trained on at least 2T tokens. We are also releasing hundreds of intermediate checkpoints available as revisions on HuggingFace. For dataset building and analysis, it includes the full training data used for these models, including code that produces the training data, from AI2’s Dolma <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>, and WIMBD <cite class="ltx_cite ltx_citemacro_citep">(Elazar et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib24" title="">2023</a>)</cite> for analyzing pretraining data. For evaluation, it includes AI2’s Catwalk <cite class="ltx_cite ltx_citemacro_citep">(Groeneveld et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib28" title="">2023</a>)</cite> for downstream evaluation and Paloma <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite> for perplexity-based evaluation. For instruction-tuning, we released Open Instruct <cite class="ltx_cite ltx_citemacro_citep">(Ivison et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib83" title="">2023</a>)</cite>, and we are currently using it to produce an adapted (instruction-tuned and RLHFed) version of OLMo, which we will release soon. Finally, all code and weights are released under the Apache 2.0 License.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.apache.org/licenses/LICENSE-2.0" title="">http://www.apache.org/licenses/LICENSE-2.0</a></span></span></span></p>OLMo 프레임워크는 언어 모델을 구축하고 연구하는 데 필요한 도구와 자원을 포괄한다. 훈련 및 모델링을 위해 전체 모델 가중치, 훈련 코드, 훈련 로그, 삭마, Weights & Biases 로그 형태의 훈련 메트릭, 추론 코드를 포함한다. 이 첫 번째 릴리스에는 서로 다른 아키텍처, 최적화기 및 훈련 하드웨어에 해당하는 7B 규모의 언어 모델 4개와 1B 규모의 모델 1개가 포함되어 있으며 모두 최소 2T 토큰으로 훈련되었다. 휴지페이스에서 수정할 수 있는 수백 개의 중간 검문소도 출시합니다. 데이터세트 구축 및 분석을 위해 AI2의 Dolma<cite class="ltx_cite ltx_citemacro_citep">(Soldaini et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>에서 학습 데이터를 생성하는 코드를 포함하여 이러한 모델에 사용되는 전체 학습 데이터와 사전 학습 데이터 분석을 위한 WIMBD<cite class="ltx_cite ltx_citemacro_citep">(Elazar et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib24" title="">2023</a>)</cite>를 포함한다. 평가를 위해 다운스트림 평가를 위해 AI2의 Catwalk <cite class="ltx_cite ltx_citemacro_citep">(Groeneveld et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib28" title="">2023</a>)</cite>와 퍼플렉시티 기반 평가를 위해 Paloma <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>를 포함한다. 인스트럭션 튜닝을 위해 Open Instruct <cite class="ltx_cite ltx_citemacro_citep">(Ivison et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib83" title="">2023</a>)</cite>를 출시하였으며, 현재 OLMo의 각색(인스트럭션 튜닝 및 RLHFed) 버전을 제작하는 데 사용하고 있으며, 곧 출시할 예정이다. 마지막으로 모든 코드 및 가중치는 Apache 2.0 라이선스에 따라 릴리스됩니다. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.apache.org/licenses/LICENSE-2.0" title="">http://www.apache.org/licenses/LICENSE-2.0</a></span></span></span></p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">이것은 더 큰 모델, 명령어 조정 모델, 그리고 더 많은 양식 및 변형으로 이어지는 긴 일련의 계획된 릴리즈의 첫 번째 단계이다. 따라서 우리는 사전 훈련 데이터와 모델 능력 간의 관계, 설계 및 하이퍼파라미터 선택의 영향, 다양한 최적화 방법 및 모델 훈련에 미치는 영향과 같은 이러한 모델의 아직 잘 이해되지 않은 측면에 대한 연구를 촉매하기를 희망한다. 또한 이 척도에서 언어 모델을 성공적으로 훈련하는 데 필요한 교훈과 중요한 세부 사항에 대해 보고한다.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>OLMo Framework</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">이 섹션에서는 OLMo 모델(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS1" title="2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.1</span></a>), 사전 훈련 데이터 세트인 돌마(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS2" title="2.2 Pretraining Data: Dolma ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.2</span></a>), 평가 프레임워크(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4" title="2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>)로 구성된 OLMo 프레임워크를 설명한다.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>OLMo&nbsp;Model and Architecture</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">우리는 <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib80" title="">2017</a>)</cite>를 기반으로 하는 디코더 전용 변압기 아키텍처를 채택하고 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.T1" title="Table 1 ‣ 2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>에 설명된 대로 1B 및 7B 변형을 전달하며 65B 버전이 곧 출시된다. 우리의 특정 아키텍처는 PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib14" title="">2022</a>)</cite>, LLaMA 계열 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">b</a>)</cite>, OpenLM <cite class="ltx_cite ltx_citemacro_citep">(Gururangan et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib30" title="">2023</a>)</cite>, Falcon <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib2" title="">2023</a>)</cite>와 같은 다른 최근 대형 언어 모델에 이어 <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib80" title="">2017</a>)</cite>의 바닐라 변압기에 대한 몇 가지 개선 사항을 포함한다. 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.T2" title="Table 2 ‣ 2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>는 7B 아키텍처를 이러한 다른 패밀리의 유사한 크기의 모델과 종합적으로 비교한다.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Layers</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">Hidden Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1">Attention Heads</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.5.1">Tokens Trained</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1" style="padding-top:1pt;padding-bottom:1pt;">1B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.2" style="padding-top:1pt;padding-bottom:1pt;">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.3" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.4" style="padding-top:1pt;padding-bottom:1pt;">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.5" style="padding-top:1pt;padding-bottom:1pt;">2T</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.3.2.1" style="padding-top:1pt;padding-bottom:1pt;">7B</th>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.2" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.3" style="padding-top:1pt;padding-bottom:1pt;">4086</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.4" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2.5" style="padding-top:1pt;padding-bottom:1pt;">2.46T</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S2.T1.1.4.3.1" style="padding-top:1pt;padding-bottom:1pt;">65B*</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.4.3.2" style="padding-top:1pt;padding-bottom:1pt;">80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.4.3.3" style="padding-top:1pt;padding-bottom:1pt;">8192</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.4.3.4" style="padding-top:1pt;padding-bottom:1pt;">64</td>
<td class="ltx_td ltx_border_bb" id="S2.T1.1.4.3.5" style="padding-top:1pt;padding-bottom:1pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>OLMo 모델 크기 및 훈련된 토큰의 최대 개수.</figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>OLMo model sizes and the maximum number of tokens trained to. 
<br class="ltx_break">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
* <em class="ltx_emph ltx_font_italic" id="S2.T1.3.1">At the time of writing our 65B model is still training.</em></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">일반적으로 손실 스파이크 및 느린 발산의 위험을 최소화하면서 하드웨어에서 처리량을 훈련하기 위해 최적화하여 하이퍼파라미터를 선택한다. 우리는 사용 가능한 계산 소스가 주어진 루프 내 평가 설정을 통해 선택을 제거한다(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px1" title="In-Loop Training Ablations ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>). 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.T2" title="Table 2 ‣ 2.1 OLMo Model and Architecture ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>는 우리의 디자인 선택을 최근의 최신 개방형 언어 모델과 비교한다. 바닐라 변압기 아키텍처의 주요 변경 사항은 다음과 같이 요약할 수 있습니다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">No biases. </span> LLaMA, PaLM 및 기타에 이어 훈련 안정성을 향상시키기 위해 아키텍처에서 모든 편향 용어를 제외한다.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Non-parametric layer norm. </span> 우리는 규범 내에서 어파인 변환이 없는, 즉 "적응적 이득"(또는 바이어스)이 없는 층 규범 <cite class="ltx_cite ltx_citemacro_citep">(Ba et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib4" title="">2016</a>)</cite>의 비모수적 공식을 사용한다. 이것이 가장 안전한 옵션이었고 매개변수 계층 규범 및 RMSNorm <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Sennrich, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib91" title="">2019</a>)</cite>와 같은 다른 변형과 비교하여 가장 빨랐다고 믿는다.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">SwiGLU activation function. </span> LLaMA, PaLM 등과 같이 ReLU 대신 SwiGLU 활성화 함수 <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib70" title="">2020</a>)</cite>를 사용하고, LLaMA에 따라 활성화 숨김 크기는 대략 <math alttext="\frac{8}{3}d" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mrow id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mfrac id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml"><mn id="S2.I1.i3.p1.1.m1.1.1.2.2" xref="S2.I1.i3.p1.1.m1.1.1.2.2.cmml">8</mn><mn id="S2.I1.i3.p1.1.m1.1.1.2.3" xref="S2.I1.i3.p1.1.m1.1.1.2.3.cmml">3</mn></mfrac><mo id="S2.I1.i3.p1.1.m1.1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><times id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.1"></times><apply id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2"><divide id="S2.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2"></divide><cn id="S2.I1.i3.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S2.I1.i3.p1.1.m1.1.1.2.2">8</cn><cn id="S2.I1.i3.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S2.I1.i3.p1.1.m1.1.1.2.3">3</cn></apply><ci id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">\frac{8}{3}d</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">divide start_ARG 8 end_ARG start_ARG 3 end_ARG italic_d</annotation></semantics></math>이지만 처리량을 개선하기 위해 128의 가장 가까운 배수(예: 7B 모델의 경우 11,008)로 증가했다. <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>SwiGLU는 "gated" 활성화 함수이므로 출력은 입력의 절반 크기입니다. 따라서 기술적으로 SwiGLU에 대한 입력은 7B 모델에 대해 2 <math alttext="\times" class="ltx_Math" display="inline" id="footnote2.m1.1"><semantics id="footnote2.m1.1b"><mo id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><times id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.1e">×</annotation></semantics></math> 11,008 = 22,016의 차원을 갖는다. </span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">Rotary positional embeddings (RoPE). </span> Like LLaMA, PaLM, and others we replace absolute position embeddings with rotary position embeddings (RoPE; <cite class="ltx_cite ltx_citemacro_citep">Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib73" title="">2021</a></cite>).</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i5.p1.1.1">Vocabulary. </span> 우리는 개인 식별 정보(PII)를 마스킹하기 위한 추가 토큰과 함께 GPT-NeoX-20B <cite class="ltx_cite ltx_citemacro_citep">(Black et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib10" title="">2022</a>)</cite>의 수정된 버전의 BPE 기반 토큰화기를 사용한다. 최종 어휘의 크기는 50,280이다. 그러나 훈련 처리량을 최대화하기 위해 모델의 해당 임베딩 행렬의 크기를 50,304로 증가시켜 128의 배수가 되도록 한다.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.8.9.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S2.T2.8.9.1.1" style="padding-top:1pt;padding-bottom:1pt;"></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.2.1">OLMo-7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.3.1">LLaMA2-7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.4.1">OpenLM-7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.5.1">Falcon-7B</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.8.9.1.6" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S2.T2.8.9.1.6.1">PaLM-8B</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.10.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.8.10.2.1" style="padding-top:1pt;padding-bottom:1pt;">Dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.2" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.3" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.4" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.5" style="padding-top:1pt;padding-bottom:1pt;">4544</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.8.10.2.6" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.11.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.11.3.1" style="padding-top:1pt;padding-bottom:1pt;">Num heads</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.2" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.3" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.4" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.5" style="padding-top:1pt;padding-bottom:1pt;">71</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.11.3.6" style="padding-top:1pt;padding-bottom:1pt;">16</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.12.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.12.4.1" style="padding-top:1pt;padding-bottom:1pt;">Num layers</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.2" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.3" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.4" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.5" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.12.4.6" style="padding-top:1pt;padding-bottom:1pt;">32</td>
</tr>
<tr class="ltx_tr" id="S2.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.3.3.4" style="padding-top:1pt;padding-bottom:1pt;">MLP ratio</th>
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.m1.1.1" xref="S2.T2.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T2.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.m1.1d">∼</annotation></semantics></math>8/3</td>
<td class="ltx_td ltx_align_left" id="S2.T2.2.2.2" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.2.2.2.m1.1"><semantics id="S2.T2.2.2.2.m1.1a"><mo id="S2.T2.2.2.2.m1.1.1" xref="S2.T2.2.2.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.2.m1.1b"><csymbol cd="latexml" id="S2.T2.2.2.2.m1.1.1.cmml" xref="S2.T2.2.2.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.2.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.2.2.2.m1.1d">∼</annotation></semantics></math>8/3</td>
<td class="ltx_td ltx_align_left" id="S2.T2.3.3.3" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.3.3.3.m1.1"><semantics id="S2.T2.3.3.3.m1.1a"><mo id="S2.T2.3.3.3.m1.1.1" xref="S2.T2.3.3.3.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.m1.1b"><csymbol cd="latexml" id="S2.T2.3.3.3.m1.1.1.cmml" xref="S2.T2.3.3.3.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.3.3.3.m1.1d">∼</annotation></semantics></math>8/3</td>
<td class="ltx_td ltx_align_left" id="S2.T2.3.3.5" style="padding-top:1pt;padding-bottom:1pt;">4</td>
<td class="ltx_td ltx_align_left" id="S2.T2.3.3.6" style="padding-top:1pt;padding-bottom:1pt;">4</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.13.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.13.5.1" style="padding-top:1pt;padding-bottom:1pt;">Layer norm type</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.2" style="padding-top:1pt;padding-bottom:1pt;">non-parametric</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.3" style="padding-top:1pt;padding-bottom:1pt;">RMSNorm</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.4" style="padding-top:1pt;padding-bottom:1pt;">parametric</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.5" style="padding-top:1pt;padding-bottom:1pt;">parametric</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.13.5.6" style="padding-top:1pt;padding-bottom:1pt;">parametric</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.14.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.14.6.1" style="padding-top:1pt;padding-bottom:1pt;">Positional embeddings</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.2" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.3" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.4" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.5" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.14.6.6" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.15.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.15.7.1" style="padding-top:1pt;padding-bottom:1pt;">Attention variant</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.2" style="padding-top:1pt;padding-bottom:1pt;">full</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.3" style="padding-top:1pt;padding-bottom:1pt;">GQA</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.4" style="padding-top:1pt;padding-bottom:1pt;">full</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.5" style="padding-top:1pt;padding-bottom:1pt;">MQA</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.15.7.6" style="padding-top:1pt;padding-bottom:1pt;">MQA</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.16.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.16.8.1" style="padding-top:1pt;padding-bottom:1pt;">Biases</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.2" style="padding-top:1pt;padding-bottom:1pt;">none</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.3" style="padding-top:1pt;padding-bottom:1pt;">none</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.4" style="padding-top:1pt;padding-bottom:1pt;">in LN only</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.5" style="padding-top:1pt;padding-bottom:1pt;">in LN only</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.16.8.6" style="padding-top:1pt;padding-bottom:1pt;">none</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.17.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.17.9.1" style="padding-top:1pt;padding-bottom:1pt;">Block type</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.2" style="padding-top:1pt;padding-bottom:1pt;">sequential</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.3" style="padding-top:1pt;padding-bottom:1pt;">sequential</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.4" style="padding-top:1pt;padding-bottom:1pt;">sequential</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.5" style="padding-top:1pt;padding-bottom:1pt;">parallel</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.17.9.6" style="padding-top:1pt;padding-bottom:1pt;">parallel</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.18.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.18.10.1" style="padding-top:1pt;padding-bottom:1pt;">Activation</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.2" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.3" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.4" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.5" style="padding-top:1pt;padding-bottom:1pt;">GeLU</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.18.10.6" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.19.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.19.11.1" style="padding-top:1pt;padding-bottom:1pt;">Sequence length</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.2" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.3" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.4" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.5" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.19.11.6" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.20.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.20.12.1" style="padding-top:1pt;padding-bottom:1pt;">Batch size (instances)</th>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.2" style="padding-top:1pt;padding-bottom:1pt;">2160</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.3" style="padding-top:1pt;padding-bottom:1pt;">1024</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.4" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.5" style="padding-top:1pt;padding-bottom:1pt;">2304</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.20.12.6" style="padding-top:1pt;padding-bottom:1pt;">512</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.8.8.6" style="padding-top:1pt;padding-bottom:1pt;">Batch size (tokens)</th>
<td class="ltx_td ltx_align_left" id="S2.T2.4.4.1" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.4.4.1.m1.1"><semantics id="S2.T2.4.4.1.m1.1a"><mo id="S2.T2.4.4.1.m1.1.1" xref="S2.T2.4.4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.4.4.1.m1.1b"><csymbol cd="latexml" id="S2.T2.4.4.1.m1.1.1.cmml" xref="S2.T2.4.4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.4.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.4.4.1.m1.1d">∼</annotation></semantics></math>4M</td>
<td class="ltx_td ltx_align_left" id="S2.T2.5.5.2" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.5.5.2.m1.1"><semantics id="S2.T2.5.5.2.m1.1a"><mo id="S2.T2.5.5.2.m1.1.1" xref="S2.T2.5.5.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.5.5.2.m1.1b"><csymbol cd="latexml" id="S2.T2.5.5.2.m1.1.1.cmml" xref="S2.T2.5.5.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.5.2.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.5.5.2.m1.1d">∼</annotation></semantics></math>4M</td>
<td class="ltx_td ltx_align_left" id="S2.T2.6.6.3" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.6.6.3.m1.1"><semantics id="S2.T2.6.6.3.m1.1a"><mo id="S2.T2.6.6.3.m1.1.1" xref="S2.T2.6.6.3.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.6.6.3.m1.1b"><csymbol cd="latexml" id="S2.T2.6.6.3.m1.1.1.cmml" xref="S2.T2.6.6.3.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.6.6.3.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.6.6.3.m1.1d">∼</annotation></semantics></math>4M</td>
<td class="ltx_td ltx_align_left" id="S2.T2.7.7.4" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.7.7.4.m1.1"><semantics id="S2.T2.7.7.4.m1.1a"><mo id="S2.T2.7.7.4.m1.1.1" xref="S2.T2.7.7.4.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.7.7.4.m1.1b"><csymbol cd="latexml" id="S2.T2.7.7.4.m1.1.1.cmml" xref="S2.T2.7.7.4.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.7.4.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.7.7.4.m1.1d">∼</annotation></semantics></math>4M</td>
<td class="ltx_td ltx_align_left" id="S2.T2.8.8.5" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S2.T2.8.8.5.m1.1"><semantics id="S2.T2.8.8.5.m1.1a"><mo id="S2.T2.8.8.5.m1.1.1" xref="S2.T2.8.8.5.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.8.8.5.m1.1b"><csymbol cd="latexml" id="S2.T2.8.8.5.m1.1.1.cmml" xref="S2.T2.8.8.5.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.8.5.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.T2.8.8.5.m1.1d">∼</annotation></semantics></math>1M</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.21.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S2.T2.8.21.13.1" style="padding-top:1pt;padding-bottom:1pt;">Weight tying</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.2" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.3" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.4" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.5" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.8.21.13.6" style="padding-top:1pt;padding-bottom:1pt;">yes</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span>7-8B 스케일에서의 LM 아키텍처 비교. "계층 규범 유형" 행에서, "모수적" 및 "비모수적"은 각각 적응 이득 및 바이어스가 있거나 없는 통상적인 계층 규범 구현을 지칭한다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pretraining Data: Dolma</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">모델 매개변수에 대한 액세스가 진행되었음에도 불구하고 사전 훈련 데이터 세트는 여전히 열려 있지 않습니다. 사전 훈련 데이터는 종종 열린 모델(닫힌 모델은 고사하고)과 함께 공개되지 않으며 이러한 데이터에 대한 문서는 작업을 재현하거나 완전히 이해하는 데 필요한 세부 정보가 부족한 경우가 많다. 이것은 훈련 데이터가 모델 역량과 한계에 어떻게 영향을 미치는지 이해하는 것과 같은 언어 모델 연구의 특정 스레드를 지원하는 것을 어렵게 만들었다. 언어 모델 사전 훈련에 대한 공개 연구를 용이하게 하기 위해 우리는 대규모 언어 모델 사전 훈련에서 흔히 볼 수 있는 7개의 다른 데이터 소스에서 얻은 5B 문서에 걸쳐 3T 토큰의 다양한 다중 소스 코퍼스인 돌마를 구축하고 출시했다. Table <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.T3" title="Table 3 ‣ 2.2 Pretraining Data: Dolma ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>는 각 소스의 데이터 양에 대한 높은 수준의 개요를 제공한다.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">돌마는 (1) 언어 필터링, (2) 품질 필터링, (3) 콘텐츠 필터링, (4) 중복 제거, (5) 다중 소스 혼합 및 (6) 토큰화의 파이프라인을 사용하여 구축된다. 우리는 독자에게 돌마 보고서 <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>를 참조하여 디자인 원리에 대한 자세한 내용, 구성에 대한 자세한 내용 및 내용에 대한 더 자세한 요약을 설명한다. 이 보고서는 콘텐츠 또는 품질 필터의 역할, 중복 제거 및 여러 소스의 데이터 혼합을 포함하여 중요한 데이터 큐레이션 관행에 대해 배운 내용을 공유하기 위해 돌마의 중간 상태에 대한 학습 언어 모델의 추가 분석 및 실험 결과를 제공한다. 큐레이션과 최종 릴리스 모두에서 각 소스의 문서를 별도로 보관합니다. 고성능 데이터 큐레이션 도구를 오픈소싱하였으며, 이 툴킷은 돌마에 대한 추가 실험, 작업 재현, 사전 훈련 말뭉치의 빠르고 쉬운 큐레이션을 가능하게 하는 데 사용될 수 있다. 마지막으로 데이터세트 분석을 돕기 위해 WIMBD 도구 <cite class="ltx_cite ltx_citemacro_citep">(Elazar et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib24" title="">2023</a>)</cite>를 오픈소싱했다.</p>
</div>
<figure class="ltx_table" id="S2.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.1.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.2.1">Doc Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.1.1.3.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.1.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.3.1.1.1.1">UTF-8</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.3.1.2.1.1">bytes</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.3.1.3">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.3.1.3.1"><span class="ltx_text ltx_font_italic" id="S2.T3.1.1.1.3.1.3.1.1">(GB)</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.1.1.4.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.1.1.4.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.4.1.1.1.1">Documents</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.4.1.2">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.4.1.2.1"><span class="ltx_text ltx_font_italic" id="S2.T3.1.1.1.4.1.2.1.1">(millions)</span></td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T3.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.1.1.5.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.1.1.5.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.5.1.1.1.1">GPT-NeoX</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.5.1.2">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.5.1.2.1"><span class="ltx_text ltx_font_bold" id="S2.T3.1.1.1.5.1.2.1.1">tokens</span></td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.1.1.5.1.3">
<td class="ltx_td ltx_align_center" id="S2.T3.1.1.1.5.1.3.1"><span class="ltx_text ltx_font_italic" id="S2.T3.1.1.1.5.1.3.1.1">(billions)</span></td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T3.1.2.1.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.2.1.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.2.1.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.2.1.1.1.1.1">Common Crawl</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.2.1.2">web pages</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.2.1.3">9,022</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.2.1.4">3,370</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T3.1.2.1.5">2,006</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.3.2.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.3.2.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.3.2.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.1.1.1.1">The Stack</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.2">code</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.3">1,043</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.4">210</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.3.2.5">342</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.4.3.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.4.3.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.4.3.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.1.1.1.1">C4</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.2">web pages</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.3">790</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.4">364</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.4.3.5">174</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.5.4.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.5.4.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.5.4.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.1.1.1.1">Reddit</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.2">social media</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.3">339</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.4">377</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.5.4.5">80</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.6.5.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.6.5.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.6.5.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.1.1.1.1">peS2o</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.2">STEM papers</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.3">268</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.4">38.8</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.6.5.5">57</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.7.6.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.7.6.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.7.6.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.1.1.1.1">Project Gutenberg</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.2">books</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.3">20.4</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.4">0.056</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.7.6.5">5.2</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T3.1.8.7.1">
<table class="ltx_tabular ltx_align_middle" id="S2.T3.1.8.7.1.1">
<tbody><tr class="ltx_tr" id="S2.T3.1.8.7.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.1.1.1.1">Wikipedia, Wikibooks</td>
</tr>
</tbody></table>
</th>
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.2">encyclopedic</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.3">16.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.4">6.2</td>
<td class="ltx_td ltx_align_center" id="S2.T3.1.8.7.5">3.7</td>
</tr>
<tr class="ltx_tr" id="S2.T3.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" colspan="2" id="S2.T3.1.9.8.1">
<span class="ltx_text ltx_font_bold" id="S2.T3.1.9.8.1.1">Total</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T3.1.9.8.2"><span class="ltx_text ltx_font_bold" id="S2.T3.1.9.8.2.1">11,519</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T3.1.9.8.3"><span class="ltx_text ltx_font_bold" id="S2.T3.1.9.8.3.1">4,367</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S2.T3.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S2.T3.1.9.8.4.1">2,668</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3:</span>Composition of Dolma.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Adaptation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">사전 학습된 모델은 항상 그대로 사용되는 것이 아니라 성능, 안전성 및 사용성을 개선하기 위해 더욱 미세 조정된다. 종종 모델들은 먼저 명령들 <cite class="ltx_cite ltx_citemacro_citep">(Mishra et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib49" title="">2022</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib84" title="">2022</a>; Sanh et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib69" title="">2022</a>)</cite>를 따르도록 트레이닝되고, 이어서 그들의 세대의 품질을 향상시키기 위해 인간 선호도들 <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib54" title="">2022</a>)</cite>에 대해 추가로 트레이닝된다. 본 논문에서는 Open Instruct (<span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p1.1.1">Tülu</span>) 데이터 및 훈련 설정 <cite class="ltx_cite ltx_citemacro_citep">(Ivison et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>)</cite>에 따라 OLMo를 일반 채팅 보조자로 훈련시킴으로써 OLMo를 추가 미세 조정을 위한 기본 모델로 사용하는 효과를 보여준다. 제안된 방법은 먼저 증류된 명령 데이터와 인간이 작성한 명령 데이터를 혼합하여 명령 미세 조정을 수행한 다음 직접 선호도 최적화(DPO) <cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib62" title="">2023</a>)</cite>를 사용하여 증류된 선호도 데이터와 모델을 추가로 정렬하는 것이다. <cite class="ltx_cite ltx_citemacro_citet">DeepSeek-AI et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib21" title="">2024</a>)</cite>와 같은 최근 모델에서 수행된 바와 같이 사전 훈련이 끝날 때 툴루 명령어 데이터를 혼합하는 실험을 수행했지만 결정적인 결과를 얻지 못했다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">모델 설계를 위한 결정을 내리는 <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.1">online</em> 평가와 모델 체크포인트를 평가하는 <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.2">offline</em> 평가의 두 단계에서 기본 모델 평가를 수행한다. 오프라인 단계에서는 광범위한 데이터 세트와 작업 형식에 액세스할 수 있는 공개적으로 사용 가능한 평가 도구인 Catwalk 프레임워크 <cite class="ltx_cite ltx_citemacro_citep">(Groeneveld et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib28" title="">2023</a>)</cite>를 사용한다. Catwalk를 사용하여 새로운 Perplexity 벤치마크인 Paloma <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>에 대해 다운스트림 평가 및 고유 언어 모델링 평가를 수행한다.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">다운스트림 및 복잡성 평가 모두에 대해 고정 평가 파이프라인을 사용하여 공개적으로 사용 가능한 모델과 결과를 비교한다. 또한 적응된 모델에 대한 별도의 평가를 보고한다.</p>
</div>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">In-Loop Training Ablations</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px1.p1.1">모델 학습 전반에 걸쳐 다운스트림 평가를 수행하여 모델 아키텍처, 초기화, 최적화기, 학습률 일정 및 데이터 혼합을 중심으로 결정을 내린다. 이를 <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS0.Px1.p1.1.1">online</em> 평가라고 하며 1000개의 훈련 단계마다 인-루프(또는 <math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS4.SSS0.Px1.p1.1.m1.1a"><mo id="S2.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS0.Px1.p1.1.m1.1d">∼</annotation></semantics></math>4B 훈련 토큰)를 실행할 때 훈련되는 모델의 품질에 대한 조기적이고 연속적인 신호를 제공합니다. 이러한 평가는 우리의 <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS0.Px1.p1.1.2">오프라인</em> 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1" title="4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>에 자세히 설명된 평가에 사용되는 많은 핵심 작업 및 실험 설정에 의존하며, 이는 또한 EleutherAI 평가 하니스의 작업 및 평가 구조를 반영한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Downstream Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p1.1">많은 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib12" title="">2020</a>; Black et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib10" title="">2022</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">b</a>, <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS0.Px2.p1.1.1.1">inter alia</em>)</cite>에 이어 다운스트림 작업 집합에 대한 제로 샷 성능을 보고합니다. 우리의 평가 제품군은 <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>)</cite> 및 <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite>에 의해 보고된 상식 추론 작업 세트에 밀접하게 대응하는 8개의 핵심 작업으로 구성된다(작업 목록은 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T6" title="Table 6 ‣ Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a> 참조). 평가되는 모델들의 스케일을 감안할 때, 그러한 태스크들은 그들의 자연스러움(예를 들어, 모두가 텍스트 완성 스코어링 태스크들로서 공식화될 수 있음) 및 트레이닝 전반에 걸쳐 의미있는 신호들을 제공하는 능력으로 인해 모델 개발 초기에 선택되었다(도<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F1" title="Figure 1 ‣ Results ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> 참조).</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Intrinsic Language Modeling Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px3.p1.1">OLMo-7B가 보류된 훈련 데이터를 넘어 언어의 분포에 어떻게 부합하는지 측정하기 위해 585개의 서로 다른 텍스트 영역을 포함하는 새로운 복잡성 벤치마크인 팔로마 <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>를 사용한다. Domains는 nytimes.com에서 Reddit의 r/depression까지 다양하며 계층화된 샘플의 C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib63" title="">2020</a>)</cite>와 같은 18개의 개별 데이터 소스에서 추출된다. 이를 통해 원본 말뭉치에 과소 대표되는 텍스트 도메인을 보다 동등하게 포함할 수 있다.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS4.SSS0.Px3.p2.1">우리는 최상의 성능을 위해 OLMo-7B를 다른 모델과 비교할 뿐만 아니라 더 풍부하고 통제된 과학적 평가를 가능하게 하는 방법을 입증하는 것을 목표로 한다. OLMo-7B는 복잡성 평가를 위한 명시적 오염 제거가 있는 가장 큰 LM이다. 팔로마에서 설명한 접근법에 따라 팔로마 평가 데이터에서 단락이 유출된 사전 훈련 문서를 제거한다. 오염 제거 없이 다른 모델은 복잡성을 과소평가할 위험이 있다(즉, 모델의 표본 외 적합도를 과대평가). 또한 중간 체크포인트를 릴리스하여 체크포인트를 릴리스하는 두 가지 다른 모델인 피티아-6.9B <cite class="ltx_cite ltx_citemacro_citep">(Biderman et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib7" title="">2023</a>)</cite> 및 RPJ-INCITE-7B <cite class="ltx_cite ltx_citemacro_citep">(Together Computer, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib76" title="">2023</a>)</cite>와 더 풍부한 비교를 가능하게 한다(그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> 참조).</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS4.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Adaptation Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S2.SS4.SSS0.Px4.p1.1">또한 Open Instruct 평가 제품군 <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib83" title="">2023</a>); Ivison et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>)</cite>를 사용하여 수업 미세 조정 및 DPO 교육 후 OLMo를 평가한다. 우리는 모델 채팅 기능과 안전성에 대한 평가에 중점을 두어 OLMo를 추가 미세 조정을 위한 기반으로 사용하는 효과를 보여준다.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Training OLMo</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">이 섹션에서는 분산 훈련 프레임워크(Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS1" title="3.1 Distributed Training Framework ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.1</span></a>), 최적화기 설정(Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS2" title="3.2 Optimizer ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>), 데이터 준비(Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS3" title="3.3 Data ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.3</span></a>), 하드웨어(Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS4" title="3.4 Hardware ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.4</span></a>)를 포함하여 사전 훈련 설정을 설명합니다.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Distributed Training Framework</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">PyTorch의 <span class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">ZeRO</em> optimizer strategy <cite class="ltx_cite ltx_citemacro_citep">(Rajbhandari et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib64" title="">2019</a>)</cite> via PyTorch의 <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.2">FSDP</span> framework <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib93" title="">2023</a>)</cite>를 사용하여 모델을 훈련합니다. 7B 규모에서 이는 하드웨어에서 GPU당 4096 토큰의 마이크로 배치 크기로 훈련을 가능하게 한다(섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS4" title="3.4 Hardware ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.4</span></a> 참조). OLMo-1B 및 -7B 모델의 경우, 우리는 대략 4M 토큰의 일정한 글로벌 배치 크기(각각 2048 토큰의 시퀀스 길이를 갖는 2048 인스턴스)를 사용한다. OLMo-65B 모델(현재 트레이닝)의 경우, 대략 2M 토큰(1024 인스턴스)에서 시작하여 대략 16M 토큰(8192 인스턴스)에 도달할 때까지 100B 토큰마다 두 배로 증가하는 배치 크기 워밍업을 사용한다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">처리량을 향상시키기 위해 <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.1">FSDP</span>의 기본 제공 설정과 PyTorch의 <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.2">amp</span> 모듈을 통해 혼합 정밀 훈련 <cite class="ltx_cite ltx_citemacro_citep">(Micikevicius et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib46" title="">2017</a>)</cite>를 사용합니다. 후자는 안정성을 향상시키기 위해 softmax와 같은 특정 작업이 항상 완전한 정밀도로 실행되는 반면, 다른 모든 작업은 <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.3">bfloat16</span> 형식을 사용하여 절반 정밀도로 실행됩니다. 특정 설정에서 각 GPU에 로컬인 샤딩된 모델 가중치 및 최적화기 상태는 완전한 정밀도로 유지됩니다. 각 변압기 블록 내의 가중치는 순방향 및 역방향 패스 동안 전체 크기 매개 변수가 각 GPU에서 구체화될 때 <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.4">bfloat16</span>에만 캐스팅됩니다. GpU 전체에서 기울기가 완전히 감소합니다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Optimizer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.3">표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.T4" title="Table 4 ‣ 3.3 Data ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>에 표시된 하이퍼파라미터와 함께 AdamW Optimizer <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib41" title="">2019</a>)</cite>를 사용한다. 모든 모델 크기에 대해 5000 단계(<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">∼</annotation></semantics></math>21B 토큰)에 걸쳐 학습 속도를 예열한 다음 나머지 훈련에 걸쳐 최대 학습 속도의 10분의 1까지 선형으로 감쇠한다. 워밍업 기간 후에, 파라미터 그래디언트들의 총 <math alttext="l^{2}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">l</mi><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑙</ci><cn id="S3.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">l^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_l start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>-norm이 <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span> 그래디언트 클리핑 동안 모델의 파라미터들이 모두 단일 빅 벡터로 처리되고(모든 파라미터들이 함께 평탄화되고 연쇄되는 것처럼), 해당 단일 그래디언트 벡터에 대해 <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="footnote3.m1.1"><semantics id="footnote3.m1.1b"><msub id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml"><mi id="footnote3.m1.1.1.2" mathvariant="normal" xref="footnote3.m1.1.1.2.cmml">ℓ</mi><mn id="footnote3.m1.1.1.3" xref="footnote3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><apply id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1"><csymbol cd="ambiguous" id="footnote3.m1.1.1.1.cmml" xref="footnote3.m1.1.1">subscript</csymbol><ci id="footnote3.m1.1.1.2.cmml" xref="footnote3.m1.1.1.2">ℓ</ci><cn id="footnote3.m1.1.1.3.cmml" type="integer" xref="footnote3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">\ell_{2}</annotation><annotation encoding="application/x-llamapun" id="footnote3.m1.1e">roman_ℓ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>-norm을 취한다. 이것이 PyTorch의 기울기를 자르는 표준 방법입니다. </span></span></span>은 <math alttext="1.0" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn id="S3.SS2.p1.3.m3.1.1.cmml" type="float" xref="S3.SS2.p1.3.m3.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">1.0</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">1.0</annotation></semantics></math>를 초과하지 않는다. 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.T5" title="Table 5 ‣ 3.3 Data ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>는 7B 척도의 최적화기 설정을 AdamW를 사용한 다른 최근 LM의 설정과 비교했다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">공개 데이터 세트인 Dolma <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>에서 2T 토큰 샘플로 훈련 데이터 세트를 구축했으며, 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS2" title="2.2 Pretraining Data: Dolma ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.2</span></a>에서 설명한다. 모든 문서의 토큰은 특수 <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.1">EOS</span> 토큰을 각 문서의 끝에 추가한 후 함께 연결됩니다. 그런 다음 2048 토큰의 연속 청크를 그룹화하여 훈련 인스턴스를 형성합니다. 트레이닝 인스턴스들은 각각의 트레이닝 실행에 대해 정확히 동일한 방식으로 셔플링된다. 각 훈련 배치의 데이터 순서와 정확한 구성은 우리가 방출하는 아티팩트로부터 재구성될 수 있다.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">공개된 모든 모델은 최소 2T 토큰(훈련 데이터에 대한 단일 에포크)으로 훈련되었으며 일부는 다른 셔플링 순서로 데이터에 대한 두 번째 에포크를 시작하여 그 이상으로 훈련되었다. 이 소량의 데이터를 반복하는 영향은 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib51" title="">2023</a>)</cite>에 따라 무시할 수 있어야 한다.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T4.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.9.10.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T4.9.10.1.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.1.1">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.2.1">Peak LR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.3.1">Betas</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.4.1">Epsilon</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.5.1">Weight Decay</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.9.10.1.6" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.9.10.1.6.1">Batch Size (tokens)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T4.1.1.2" style="padding-top:1pt;padding-bottom:1pt;">1B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.1.1.3.1">4.0E-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.4" style="padding-top:1pt;padding-bottom:1pt;">(0.9, 0.95)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.1.1.5.1">1.0E-5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.6" style="padding-top:1pt;padding-bottom:1pt;">0.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.1.1.1.m1.1"><semantics id="S3.T4.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.m1.1d">∼</annotation></semantics></math>4M</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T4.2.2.2" style="padding-top:1pt;padding-bottom:1pt;">7B</th>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.2.2.3.1">3.0E-4</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.4" style="padding-top:1pt;padding-bottom:1pt;">(0.9, 0.95)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.2.2.5.1">1.0E-5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.6" style="padding-top:1pt;padding-bottom:1pt;">0.1</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.2.1" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.2.2.1.m1.1"><semantics id="S3.T4.2.2.1.m1.1a"><mo id="S3.T4.2.2.1.m1.1.1" xref="S3.T4.2.2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.1.m1.1b"><csymbol cd="latexml" id="S3.T4.2.2.1.m1.1.1.cmml" xref="S3.T4.2.2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.2.2.1.m1.1d">∼</annotation></semantics></math>4M</td>
</tr>
<tr class="ltx_tr" id="S3.T4.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T4.9.9.8" style="padding-top:1pt;padding-bottom:1pt;">65B*</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.9" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.9.9.9.1">1.5E-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.10" style="padding-top:1pt;padding-bottom:1pt;">(0.9, 0.95)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.11" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.T4.9.9.11.1">1.0E-5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.12" style="padding-top:1pt;padding-bottom:1pt;">0.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.9.9.7" style="padding-top:1pt;padding-bottom:1pt;">
<math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.3.3.1.m1.1"><semantics id="S3.T4.3.3.1.m1.1a"><mo id="S3.T4.3.3.1.m1.1.1" xref="S3.T4.3.3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.1.m1.1b"><csymbol cd="latexml" id="S3.T4.3.3.1.m1.1.1.cmml" xref="S3.T4.3.3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.3.3.1.m1.1d">∼</annotation></semantics></math>2M <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T4.4.4.2.m2.1"><semantics id="S3.T4.4.4.2.m2.1a"><mo id="S3.T4.4.4.2.m2.1.1" stretchy="false" xref="S3.T4.4.4.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.2.m2.1b"><ci id="S3.T4.4.4.2.m2.1.1.cmml" xref="S3.T4.4.4.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.4.4.2.m2.1d">→</annotation></semantics></math> <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.5.5.3.m3.1"><semantics id="S3.T4.5.5.3.m3.1a"><mo id="S3.T4.5.5.3.m3.1.1" xref="S3.T4.5.5.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.3.m3.1b"><csymbol cd="latexml" id="S3.T4.5.5.3.m3.1.1.cmml" xref="S3.T4.5.5.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.3.m3.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.5.5.3.m3.1d">∼</annotation></semantics></math>4M <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T4.6.6.4.m4.1"><semantics id="S3.T4.6.6.4.m4.1a"><mo id="S3.T4.6.6.4.m4.1.1" stretchy="false" xref="S3.T4.6.6.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T4.6.6.4.m4.1b"><ci id="S3.T4.6.6.4.m4.1.1.cmml" xref="S3.T4.6.6.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.6.4.m4.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.6.6.4.m4.1d">→</annotation></semantics></math> <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.7.7.5.m5.1"><semantics id="S3.T4.7.7.5.m5.1a"><mo id="S3.T4.7.7.5.m5.1.1" xref="S3.T4.7.7.5.m5.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.7.7.5.m5.1b"><csymbol cd="latexml" id="S3.T4.7.7.5.m5.1.1.cmml" xref="S3.T4.7.7.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.7.5.m5.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.7.7.5.m5.1d">∼</annotation></semantics></math>8M <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T4.8.8.6.m6.1"><semantics id="S3.T4.8.8.6.m6.1a"><mo id="S3.T4.8.8.6.m6.1.1" stretchy="false" xref="S3.T4.8.8.6.m6.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T4.8.8.6.m6.1b"><ci id="S3.T4.8.8.6.m6.1.1.cmml" xref="S3.T4.8.8.6.m6.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.8.8.6.m6.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.8.8.6.m6.1d">→</annotation></semantics></math> <math alttext="\sim" class="ltx_Math" display="inline" id="S3.T4.9.9.7.m7.1"><semantics id="S3.T4.9.9.7.m7.1a"><mo id="S3.T4.9.9.7.m7.1.1" xref="S3.T4.9.9.7.m7.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.9.9.7.m7.1b"><csymbol cd="latexml" id="S3.T4.9.9.7.m7.1.1.cmml" xref="S3.T4.9.9.7.m7.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.9.9.7.m7.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.T4.9.9.7.m7.1d">∼</annotation></semantics></math>16M</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4:</span>AdamW pretraining hyperparameters for OLMo models.</figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>AdamW pretraining hyperparameters for OLMo models.
<br class="ltx_break">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
* <em class="ltx_emph ltx_font_italic" id="S3.T4.11.1">At the time of writing our 65B model is still training.</em></figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S3.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T5.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T5.1.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.2.1">OLMo-7B</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.3.1">LLaMA2-7B</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.4.1">OpenLM-7B</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.5.1">Falcon-7B</span></th>
</tr>
<tr class="ltx_tr" id="S3.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S3.T5.1.2.2.1">warmup steps</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T5.1.2.2.2">5000</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T5.1.2.2.3">2000</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T5.1.2.2.4">2000</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T5.1.2.2.5">1000</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T5.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.3.1.1">peak LR</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.3.1.2"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.3.1.2.1">3.0E-04</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.3.1.3"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.3.1.3.1">3.0E-04</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.3.1.4"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.3.1.4.1">3.0E-04</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.3.1.5"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.3.1.5.1">6.0E-04</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.4.2.1">minimum LR</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.4.2.2"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.4.2.2.1">3.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.4.2.3"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.4.2.3.1">3.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.4.2.4"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.4.2.4.1">3.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.4.2.5"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.4.2.5.1">1.2E-05</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.5.3.1">weight decay</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.5.3.2">0.1</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.5.3.3">0.1</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.5.3.4">0.1</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.5.3.5">0.1</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.6.4.1">beta1</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.6.4.2">0.9</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.6.4.3">0.9</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.6.4.4">0.9</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.6.4.5">0.99</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.7.5.1">beta2</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.7.5.2">0.95</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.7.5.3">0.95</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.7.5.4">0.95</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.7.5.5">0.999</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.8.6.1">epsilon</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.8.6.2"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.8.6.2.1">1.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.8.6.3"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.8.6.3.1">1.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.8.6.4"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.8.6.4.1">1.0E-05</span></td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.8.6.5"><span class="ltx_text ltx_font_typewriter" id="S3.T5.1.8.6.5.1">1.0E-05</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.9.7.1">LR schedule</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.9.7.2">linear</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.9.7.3">cosine</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.9.7.4">cosine</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.9.7.5">cosine</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.10.8.1">gradient clipping</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.10.8.2">global 1.0</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.10.8.3">global 1.0</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.10.8.4">global 1.0</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.10.8.5">global 1.0</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T5.1.11.9.1">gradient reduce dtype</th>
<td class="ltx_td ltx_align_left" id="S3.T5.1.11.9.2">FP32</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.11.9.3">FP32</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.11.9.4">FP32</td>
<td class="ltx_td ltx_align_left" id="S3.T5.1.11.9.5">BF16</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T5.1.12.10.1">optimizer state dtype</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.12.10.2">FP32</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.12.10.3">most likely FP32</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.12.10.4">FP32</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.12.10.5">FP32</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span>7B 스케일에서 프리트레이닝 최적화기 설정의 비교. 이 표의 각 모델은 AdamW를 최적화기로 사용했다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Hardware</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">성능 손실 없이 코드베이스가 NVIDIA 및 AMD GPU 모두에서 사용될 수 있는지 확인하기 위해 두 개의 다른 클러스터에서 모델을 훈련했다.</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">LUMI:</span> Under the LUMI supercomputer,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lumi-supercomputer.eu" title="">https://www.lumi-supercomputer.eu</a></span></span></span> We used to to 256 nodes on this cluster, where each node consists a 4x AMD MI250X GPU with 128GB of memory<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>The MI250X is a dual-chip module, meaning in practice that each physical device consists of two logical devices, so each node has 8 logical GPU devices with 64GB of memory each.</span></span></span> and 800Gbps of interconnect.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">MosaicML:</span> Provided by MosaicML<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mosaicml.com" title="">https://www.mosaicml.com</a></span></span></span> (Databricks)을 사용했으며, 각 노드는 40GB의 메모리와 800Gbps 인터커넥트를 가진 8x NVIDIA A100 GPU로 구성된다.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS4.p1.2">훈련 처리량을 최적화하기 위한 배치 크기의 사소한 차이에도 불구하고 두 실행 모두 2T 토큰에 의한 평가 제품군에서 거의 동일한 성능을 보였다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">OLMo-7B를 평가하는 데 사용되는 체크포인트는 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S3.SS2" title="3.2 Optimizer ‣ 3 Training OLMo ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>에서 언급된 선형 학습 속도 감쇠 일정이 있는 Dolma <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite> 데이터 세트에서 2.46T 토큰이 될 때까지 훈련된다. 실험에서 학습 속도가 선형적으로 0으로 감쇠된 1000단계 동안 돌마 데이터 세트에서 이 체크포인트를 추가로 조정하면 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4" title="2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>에 설명된 복잡성 및 최종 작업 평가 스위트에서 모델 성능이 향상된다는 것을 발견했다. OLMo를 LLaMA-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>)</cite>, LLaMA2-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite>, MPT-7B <cite class="ltx_cite ltx_citemacro_citep">(MosaicML NLP Team, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib50" title="">2023</a>)</cite>, Pythia-6.9B <cite class="ltx_cite ltx_citemacro_citep">(Biderman et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib7" title="">2023</a>)</cite>, Falcon-7B <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib2" title="">2023</a>)</cite> 및 RPJ-INCITE-7B <cite class="ltx_cite ltx_citemacro_citep">(Together Computer, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib76" title="">2023</a>)</cite> 등 다른 공개 가능한 모델과 비교한다.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Downstream evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">우리의 핵심 <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.1">downstream 평가 suite</span> (표 참조)은 다음과 같이 구성되어 있습니다. 부록 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1" title="Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>에서는 코어 평가 세트 외부의 추가 보조 작업 세트에 대한 결과도 보고하여 안정적인 성능 경향이 덜한 것으로 나타났다(그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.F4" title="Figure 4 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> 참조).</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.1">모든 경우에, <cite class="ltx_cite ltx_citemacro_citet">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib12" title="">2020</a>)</cite>에 의해 대중화된 순위 분류 접근법을 사용하여 제로 샷 평가를 수행한다. 이 접근법 하에서, 후보 텍스트 완성들(예를 들어, 상이한 객관식 옵션들)은 우도(일반적으로 일부 정규화 인자에 의해 정규화됨)에 의해 순위가 매겨지고, 예측 정확도가 보고된다. Catwalk는 토큰 수(토큰당 정규화) <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib12" title="">2020</a>; Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib37" title="">2022</a>)</cite>, 문자 수(문자당 정규화) <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib26" title="">2023</a>)</cite>, 답변의 무조건 우도 <cite idx=3></cite)를 통합하여 각 데이터 세트에 대한 정규화 전략을 별도로 선택했다. 구체적으로, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.1">arc</span> 및 <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.2">openbookqa</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.3">hellaswag</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.4">piqa</span> 및 <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.5">winogrande</span></p>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1.1">7B Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.2.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.2.1.1.1">arc</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.2.1.2.1">challenge</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.3.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.3.1.1.1">arc</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.3.1.2.1">easy</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.4">boolq</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.5.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.5.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.5.1.1.1">hella-</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.5.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.5.1.2.1">swag</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.6.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.6.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.6.1.1.1">open</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.6.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.6.1.2.1">bookqa</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.7">piqa</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.8">sciq</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T6.1.1.1.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.1.1.1.9.1">
<tbody><tr class="ltx_tr" id="S4.T6.1.1.1.9.1.1">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.9.1.1.1">wino-</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.1.1.9.1.2">
<td class="ltx_td ltx_align_center" id="S4.T6.1.1.1.9.1.2.1">grande</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T6.1.1.1.10">avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T6.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.2.1.1.1">Falcon</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.2">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.3">70.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.4">74.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.5">75.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.6">53.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.7">78.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.8">93.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.1.2.1.9">68.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.10">70.3</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.3.2.1.1">LLaMA</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.2">44.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.3">67.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.4">75.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.5">76.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.6">51.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.7">77.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.8">93.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.3.2.9">70.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.10">69.6</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.4.3.1.1">Llama 2</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.2">48.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.3">69.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.4">80.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.5">76.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.6">48.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.7">76.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.8">94.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.4.3.9">69.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.10">70.5</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.5.4.1.1">MPT</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.2">46.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.3">70.5</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.4">74.2</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.5">77.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.6">48.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.7">77.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.8">93.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.5.4.9">69.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.5.4.10">69.8</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.6.5.1.1">Pythia</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.2">44.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.3">61.9</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.4">61.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.5">63.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.6">45.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.7">75.1</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.8">91.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.6.5.9">62.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.6.5.10">63.0</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.7.6.1.1">RPJ-INCITE</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.2">42.8</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.3">68.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.4">68.6</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.5">70.3</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.6">49.4</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.7">76.0</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.8">92.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.7.6.9">64.7</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.7.6.10">66.6</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.8.7" style="background-color:#40C4DF;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T6.1.8.7.1"><span class="ltx_text" id="S4.T6.1.8.7.1.1" style="background-color:#40C4DF;">[] <span class="ltx_text ltx_font_bold" id="S4.T6.1.8.7.1.1.1">OLMo-7B</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.2"><span class="ltx_text" id="S4.T6.1.8.7.2.1" style="background-color:#40C4DF;">48.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.3"><span class="ltx_text" id="S4.T6.1.8.7.3.1" style="background-color:#40C4DF;">65.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.4"><span class="ltx_text" id="S4.T6.1.8.7.4.1" style="background-color:#40C4DF;">73.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.5"><span class="ltx_text" id="S4.T6.1.8.7.5.1" style="background-color:#40C4DF;">76.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.6"><span class="ltx_text" id="S4.T6.1.8.7.6.1" style="background-color:#40C4DF;">50.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.7"><span class="ltx_text" id="S4.T6.1.8.7.7.1" style="background-color:#40C4DF;">78.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.8"><span class="ltx_text" id="S4.T6.1.8.7.8.1" style="background-color:#40C4DF;">93.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T6.1.8.7.9"><span class="ltx_text" id="S4.T6.1.8.7.9.1" style="background-color:#40C4DF;">67.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.1.8.7.10"><span class="ltx_text" id="S4.T6.1.8.7.10.1" style="background-color:#40C4DF;">69.3</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 6:</span>Zero-shot evaluation of OLMo-7B and 6 other publicly available comparable model checkpoints on the 8 core tasks from the Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px2" title="Downstream Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>. OLMo-7B의 경우 2.46T 토큰 체크포인트에 대한 결과를 보고한다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T6" title="Table 6 ‣ Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>는 OLMo-7B의 제로샷 평가 결과를 요약하고 비슷한 크기의 다른 6개의 공개적으로 사용 가능한 모델과 비교한다. 우리는 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px2" title="Downstream Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>에 설명된 평가 제품군에서 8개의 핵심 작업에 대한 결과를 보고한다. 전체적으로 OLMo-7B는 비교 표에서 공개적으로 사용 가능한 6개의 모델 체크포인트 모두에 대해 경쟁력이 있다.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F1" title="Figure 1 ‣ Results ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>에서 우리는 8개의 코어 엔드 태스크의 정확도 점수 진행을 플롯한다. OBQA를 제외한 모든 태스크는 OLMo-7B가 더 많은 토큰에 대해 훈련됨에 따라 정확도 수치에서 상승 추세를 보인다. 마지막 단계와 두 번째 단계에서 마지막 단계 사이의 많은 작업의 정확도에서 날카로운 상향 진드기는 최종 1000개의 훈련 단계에 걸쳐 LR을 선형으로 0으로 줄이는 이점을 보여준다. 추가적인 평가 결과 및 논의는 부록 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1" title="Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>의 Table <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.T9" title="Table 9 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">9</span></a>를 참조한다.</p>
</div>
<figure class="ltx_figure" id="S4.F1">
<p class="ltx_p ltx_align_center" id="S4.F1.1"><span class="ltx_text" id="S4.F1.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="502" id="S4.F1.1.1.g1" src="https://arxiv.org/html/2402.00838v3/x6.png" width="830">
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 1:</span>Accuracy score progression of OLMo-7B on 8 core end-tasks score from Catwalk evaluation suite from Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S2.SS4.SSS0.Px2" title="Downstream Evaluation ‣ 2.4 Evaluation ‣ 2 OLMo Framework ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>. 우리는 대부분의 작업에 대한 훈련의 최종 1000단계에서 LR을 0으로 낮추는 이점을 볼 수 있다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Intrinsic language modeling evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">내재적 평가를 위해 팔로마는 각 도메인의 성능 검사부터 도메인 조합에 대한 보다 요약된 결과에 이르기까지 다양한 분석을 제안한다. 우리는 두 가지 수준의 세분성, 즉 <cite class="ltx_cite ltx_citemacro_cite">Magnusson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>에서와 같이 팔로마에서 18개 소스 중 11개에 대한 집계 성능과 이러한 소스 각각에 대해 개별적으로 더 세밀한 결과를 보고한다. 팔로마의 11개 소스의 이 특정 하위 집합은 공개적으로 사용할 수 없거나 프린지 또는 독성 텍스트를 포함하거나 팔로마의 오염 제거 접근법에서 지원되지 않는 코드 데이터로 구성된 소스를 제외한다. 이는 C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib63" title="">2020</a>)</cite>, mC4-en <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib16" title="">2023</a>)</cite>, Wikitext 103 <cite class="ltx_cite ltx_citemacro_citep">(Merity et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib45" title="">2016</a>)</cite>, Penn Treebank <cite class="ltx_cite ltx_citemacro_citep">(Marcus et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib44" title="">1999</a>; Nunes, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib52" title="">2020</a>)</cite>, RedPajama <cite class="ltx_cite ltx_citemacro_citep">(Together Computer, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib76" title="">2023</a>)</cite>, Falcon-RefinedWeb <cite class="ltx_cite ltx_citemacro_citep">(Penedo et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib57" title="">2023</a>)</cite>, Dolma <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>, M2D2 S2ORC <cite class="ltx_cite ltx_citemacro_citep">(Reid et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib65" title="">2022</a>)</cite>, M2D2 Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Reid et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib65" title="">2022</a>)</cite>, C4 100 domains <cite class="ltx_cite ltx_citemacro_citep">(Chronopoulou et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib15" title="">2022</a>)</cite>, Dolma 100 Subreddits <cite class="ltx_cite ltx_citemacro_citep">(Soldaini et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>를 남긴다. 다른 어휘를 가진 모델 간의 공정한 비교를 위해, 우리는 이러한 소스의 테스트 세트에 대해 <cite class="ltx_cite ltx_citemacro_citet">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib25" title="">2020</a>)</cite>에 의해 정의된 바이트당 비트를 보고한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS0.Px2.p1.1.1">Sources Combined</span> subplot of Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서는 Paloma의 11개 데이터 소스 조합에 대한 6개의 비교 크기 언어 모델에 대한 OLMo-7B의 성능을 보여준다. 전반적으로 우리는 OLMo가 특히 훈련 데이터가 팔로마에 대해 명시적으로 오염 제거되었다는 점을 감안할 때 경쟁적 적합성을 가지고 있음을 발견했다. 최종 모델(모양 참조)과 중간 체크포인트(점선 참조)의 비교를 통해 볼 수 있듯이 OLMo 결과는 다른 모델의 유사한 스케일링 경향을 따른다. 중간 체크포인트의 성능은 그 체크포인트가 학습률 스케줄에서 발생하는 위치에 의해 영향을 받음에 유의한다. 따라서 더 적은 단계에 대해 훈련된 모델은 훈련 기간이 모든 모델에 걸쳐 고정된다면 더 샘플 효율적일 필요 없이 더 가파른 훈련 곡선을 갖는 경향이 있다. 그럼에도 불구하고 MPT-7B는 이 서브플롯의 다른 모델보다 앞서 개선되는 것으로 눈에 띈다. 이는 팔로마(예를 들어, MPT는 LLaMA의 경우 18%, RedPajama의 경우 12.2%, OLMo의 경우 11.2%가 아닌 27% 비공통 크롤 데이터에 대한 사전 훈련과 다양한 데이터 전처리 결정(예를 들어, MPT의 <cite class="ltx_cite ltx_citemacro_citep">Abbas et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib1" title="">2023</a></cite>의 의미적 중복 제거 사용)을 포함한 여러 요인 때문일 수 있다.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="617" id="S4.F2.g1" src="https://arxiv.org/html/2402.00838v3/x7.png" width="831">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2:</span></figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Bits per byte on 11 evaluation data sources from Paloma and their combination <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>, decontaminated from OLMo’s pretraining data. While models follow a general data scaling trend, sample efficiency is most favorable on in-distribution data. For example, OLMo-7B overtakes all other models on C4, perhaps from having 88.8% Common Crawl pretraining data.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>의 나머지 서브플롯은 집계된 팔로마 메트릭에서 결합된 11개의 데이터 소스 각각에 대해 바이트당 비트를 별도로 보고함으로써 더 세밀한 분석을 제공한다. 이로부터 우리는 주로 훈련 및 평가 분포의 유사성에 의해 주도되는 표본 효율성의 더 큰 변동을 볼 수 있다. 특히, OLMo-7B는 C4와 같은 Common Crawl이 우세한 평가에서 우수하지만, Common Crawl을 후처리하는 다양한 방법은 Falcon RefinedWeb의 Falcon-7B와 같은 특정 데이터로 훈련된 모델에 가장 적합하다. 한편, OLMo-7B는 WikiText-103, M2D2 S2ORC 및 M2D2 위키피디아와 같이 스크래핑된 웹 텍스트와 덜 관련된 소스에서 다른 모델에 비해 샘플 효율이 낮다. 레드파자마 평가는 아마도 7개 도메인 중 2개만이 커먼 크롤에서 왔으며 팔로마 가중치 도메인이 각 소스 내에서 동등하기 때문에 유사한 패턴을 보여준다. 위키피디아 및 ArXiv 논문과 같은 선별된 소스의 이질적인 데이터는 스크래핑된 웹 텍스트보다 훨씬 덜 풍부하기 때문에 사전 훈련 말뭉치가 스케일링됨에 따라 이러한 언어 분포에 적합하기 위한 샘플 효율성을 유지하는 것은 어려울 것이다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Adaptation Evaluation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T7">
<p class="ltx_p" id="S4.T7.4"><span class="ltx_text" id="S4.T7.4.4"> <span class="ltx_inline-block ltx_transformed_outer" id="S4.T7.4.4.4" style="width:296.3pt;height:198pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span class="ltx_p" id="S4.T7.4.4.4.4"><span class="ltx_text" id="S4.T7.4.4.4.4.4" style="color:#000000;"> <span class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T7.4.4.4.4.4.4"> <span class="ltx_tbody"> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.5.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.1.1">Model</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.2.1">MMLU</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.3"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.3.1">AlpacaEval</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.4"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.4.1">ToxiGen</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.4.4.4.4.4.4.5.1.5"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.5.1.5.1">TruthfulQA</span></span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.4"> <span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.4.5"></span> <span class="ltx_td ltx_align_center" id="S4.T7.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.1.1.1.1.1.1">0-shot</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T7.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T7.1.1.1.1.1.1.1.1.m1.1a"><mo id="S4.T7.1.1.1.1.1.1.1.1.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T7.1.1.1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T7.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.1.1.1.1.1.1.1.1.m1.1d">↑</annotation></semantics></math></span> <span class="ltx_td ltx_align_center" id="S4.T7.2.2.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T7.2.2.2.2.2.2.2.2.1">%win</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T7.2.2.2.2.2.2.2.2.m1.1"><semantics id="S4.T7.2.2.2.2.2.2.2.2.m1.1a"><mo id="S4.T7.2.2.2.2.2.2.2.2.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T7.2.2.2.2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.2.2.2.2.2.m1.1b"><ci id="S4.T7.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T7.2.2.2.2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.2.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.2.2.2.2.2.2.2.2.m1.1d">↑</annotation></semantics></math></span> <span class="ltx_td ltx_align_center" id="S4.T7.3.3.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T7.3.3.3.3.3.3.3.3.1">% Toxic</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T7.3.3.3.3.3.3.3.3.m1.1"><semantics id="S4.T7.3.3.3.3.3.3.3.3.m1.1a"><mo id="S4.T7.3.3.3.3.3.3.3.3.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T7.3.3.3.3.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.3.3.3.3.3.m1.1b"><ci id="S4.T7.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S4.T7.3.3.3.3.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.3.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.3.3.3.3.3.3.3.3.m1.1d">↓</annotation></semantics></math></span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.4.4.1">%Info+True</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T7.4.4.4.4.4.4.4.4.m1.1"><semantics id="S4.T7.4.4.4.4.4.4.4.4.m1.1a"><mo id="S4.T7.4.4.4.4.4.4.4.4.m1.1.1" mathcolor="#000000" stretchy="false" xref="S4.T7.4.4.4.4.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.4.4.4.4.4.m1.1b"><ci id="S4.T7.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S4.T7.4.4.4.4.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.4.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T7.4.4.4.4.4.4.4.4.m1.1d">↑</annotation></semantics></math></span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.6.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.6.2.1.1">OLMo (base)</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.2">28.3</span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.3">-</span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.4">81.4</span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.6.2.5">31.6</span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.7.3"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.7.3.1.1">MPT Chat</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.2">33.8</span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.3">46.8</span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.4">0.1</span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.7.3.5">42.7</span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.8.4"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.8.4.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.8.4.1.1">Falcon Instruct</span></span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.8.4.2">25.2</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.8.4.3">14.0</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.8.4.4">70.7</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.8.4.5">27.2</span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.9.5"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.9.5.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.9.5.1.1">RPJ-INCITE Chat</span></span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.9.5.2">27.0</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.9.5.3">38.0</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.9.5.4">46.4</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.9.5.5">53.0</span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.10.6"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.10.6.1"><span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.10.6.1.1">Llama-2-Chat</span></span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.10.6.2">46.8</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.10.6.3">87.3</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.10.6.4">0.0</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.10.6.5">26.3</span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.11.7"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S4.T7.4.4.4.4.4.4.11.7.1.1">Tülu<span class="ltx_text ltx_font_upright" id="S4.T7.4.4.4.4.4.4.11.7.1.1.1"> 2</span></span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.2">50.4</span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.3">73.9</span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.4">7.0</span> <span class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.4.4.4.4.4.4.11.7.5">51.7</span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.12.8"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.12.8.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S4.T7.4.4.4.4.4.4.12.8.1.1">Tülu<span class="ltx_text ltx_font_upright" id="S4.T7.4.4.4.4.4.4.12.8.1.1.1"> 2+DPO</span></span></span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.12.8.2">50.7</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.12.8.3">85.1</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.12.8.4">0.5</span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.12.8.5">- *</span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.13.9" style="background-color:#40C4DF;"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T7.4.4.4.4.4.4.13.9.1"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.1.1" style="background-color:#40C4DF;">[] <span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.13.9.1.1.1">OLMo +SFT</span></span></span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.13.9.2"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.2.1" style="background-color:#40C4DF;">47.3</span></span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.13.9.3"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.3.1" style="background-color:#40C4DF;">57.0</span></span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.13.9.4"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.4.1" style="background-color:#40C4DF;">14.4</span></span> <span class="ltx_td ltx_align_center" id="S4.T7.4.4.4.4.4.4.13.9.5"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.13.9.5.1" style="background-color:#40C4DF;">41.2</span></span></span> <span class="ltx_tr" id="S4.T7.4.4.4.4.4.4.14.10" style="background-color:#40C4DF;"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T7.4.4.4.4.4.4.14.10.1"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.1.1" style="background-color:#40C4DF;">[] <span class="ltx_text ltx_font_bold" id="S4.T7.4.4.4.4.4.4.14.10.1.1.1">OLMo +SFT+DPO</span></span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.4.4.4.4.4.14.10.2"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.2.1" style="background-color:#40C4DF;">46.2</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.4.4.4.4.4.14.10.3"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.3.1" style="background-color:#40C4DF;">69.3</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.4.4.4.4.4.14.10.4"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.4.1" style="background-color:#40C4DF;">1.7</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.4.4.4.4.4.4.14.10.5"><span class="ltx_text" id="S4.T7.4.4.4.4.4.4.14.10.5.1" style="background-color:#40C4DF;">52.0</span></span></span> </span> </span> </span></span> </span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 7:</span>OLMo-7B를 포함한 다양한 명령어 튜닝 7B 모델의 평가 및 적응 훈련 전후. 낮은 것은 ToxiGen에 더 좋고 높은 것은 다른 지표에 더 좋다. 부록에서 모델 및 메트릭에 대한 자세한 설명을 제공합니다. <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A3" title="Appendix C Adaptation Evaluation and Model details ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">C</span></a>. * <cite class="ltx_cite ltx_citemacro_citet">Ivison et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>)</cite> 다음에 테스트 세트 오염으로 인한 <span class="ltx_text ltx_font_smallcaps" id="S4.T7.6.1">Tülu</span> 2 TruthfulQA 점수를 보고하지 않습니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">적응 전 OLMo를 평가하고 감독 미세 조정 및 DPO 훈련 단계 후 <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib83" title="">2023</a>)</cite>에서 사용하는 안전성 및 채팅 평가를 중심으로 평가한다. 우리는 추가로 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T6" title="Table 6 ‣ Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>에서 공식적으로 출시된 모델의 명령어 조정 변형과 비교한다. 또한 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px1.p1.1.1">Tülu</span> 2 모델과 비교하여 동일한 훈련 후 데이터 믹스 및 절차를 사용하여 훈련된 모델과 비교한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">특히 DPO 훈련 후 명령어 튜닝은 OLMo의 성능과 안전성을 크게 향상시켜 MMLU 성능을 크게 향상시키며 ToxiGen과 TruthfulQA 점수를 향상시킨다는 것을 알 수 있었다. 또한 초기 명령어 튜닝(OLMo +SFT) 및 추가 선호도 정렬(OLMo +SFT +DPO) 후 OLMo가 대부분의 다른 채팅 변형보다 우수하다는 것을 발견하여 기본 모델로서의 OLMo의 강도와 적응 훈련을 수행하는 데 사용된 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.1">Tülu</span> 믹스의 강도를 강조한다. 그러나 여전히 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.2">Tülu</span> 2와 격차가 있음을 발견했으며, 이는 <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.3">Tülu</span> 2를 적용하여 학습한 것이다. 이 격차는 Llama 2의 테스트 세트 오염 때문일 수 있으며, <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.4">Tülu</span> mix는 주로 Llama 모델에 대해 설계되었기 때문에 향후 작업에서 이러한 격차의 원인을 조사할 것이다. 전반적으로, 우리는 OLMo가 추가 튜닝으로부터 크게 이익을 얻고 다운스트림 애플리케이션에 대한 강력한 기본 모델 역할을 한다는 것을 안다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Power Consumption and Carbon Footprint</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">이전 문헌 <cite class="ltx_cite ltx_citemacro_citep">(Strubell et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib72" title="">2019</a>; Patterson et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib56" title="">2021</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib86" title="">2022</a>; Dodge et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib22" title="">2022</a>)</cite>에 따라 훈련에 필요한 총 전력 소비량을 계산한 다음 모델이 훈련된 전력망의 탄소 배출 강도에 곱하여 모델을 사전 훈련하면서 방출되는 총 에너지 소비량과 탄소를 추정한다. 이러한 운영 배출량을 보고하는 것은 표준 관행이지만 하드웨어 및 데이터 센터 인프라의 제조, 운송 및 폐기로 인한 체화된 배출, 사용으로 인한 평생 운영 배출, 반등 효과 또는 물 소비 또는 채굴과 같은 기타 환경 영향과 같은 다른 배출원을 설명하지 않는다. 따라서 우리의 추정치는 하한으로 간주되어야 한다.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">우리는 25ms마다 단일 노드의 소비 전력을 측정하고 전체 훈련 실행에서 평균을 계산하고 총 노드 수를 곱하여 모델에 대한 총 소비 전력을 계산한다. 그런 다음 이전 총계에 전력 사용 효율(PUE) 계수를 곱하여 데이터 센터의 에너지 효율성을 설명하며, 이는 에너지 효율적인 데이터 센터의 전형적인 보수적인 10% 에너지 소비 오버헤드를 나타내는 1.1로 설정되었다. <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nrel.gov/computational-science/measuring-efficiency-pue.html" title="">https://www.nrel.gov/computational-science/measuring-efficiency-pue.html</a></span></span></span><span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.google.com/about/datacenters/efficiency/" title="">https://www.google.com/about/datacenters/efficiency/</a></span></span></span> 7B 모델의 사전 훈련은 에너지의 <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">239 MWh</span>을 소비했다고 추정합니다.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.3">탄소 배출량을 계산하기 위해 각 모델이 훈련된 데이터 센터의 물리적 위치를 기반으로 KWh당 방출되는 kg CO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><msub id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mi id="S4.SS4.p3.1.m1.1.1a" xref="S4.SS4.p3.1.m1.1.1.cmml"></mi><mn id="S4.SS4.p3.1.m1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><cn id="S4.SS4.p3.1.m1.1.1.1.cmml" type="integer" xref="S4.SS4.p3.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>로 측정된 총 전력 소비에 탄소 강도 계수를 곱한다. A100-40GB GPU에서 훈련된 모델은 호주에서 훈련되었으므로 2022년 호주의 전국 평균인 0.610의 탄소 강도 계수를 가정합니다. <span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/december-quarter-2022/Emissions-Reduction.aspx" title="">https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/december-quarter-2022/Emissions-Reduction.aspx</a></span></span></span> MI250X GPU에서 훈련된 모델은 100% 재생 가능한 탄소 중립 에너지로 실행되는 LUMI 슈퍼컴퓨터에서 훈련되었으므로 탄소 강도 계수는 0이라고 가정합니다. LUMI는 전적으로 수력에 의해 구동되며 일부 소스 <cite class="ltx_cite ltx_citemacro_citep">(Ubierna et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib79" title="">2022</a>)</cite>는 수력의 탄소 강도 계수를 0.024로 측정하므로 총 탄소 배출량은 3.54 tCO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><msub id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mi id="S4.SS4.p3.2.m2.1.1a" xref="S4.SS4.p3.2.m2.1.1.cmml"></mi><mn id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><cn id="S4.SS4.p3.2.m2.1.1.1.cmml" type="integer" xref="S4.SS4.p3.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>eq를 의미합니다. <span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lumi-supercomputer.eu" title="">https://www.lumi-supercomputer.eu</a></span></span></span> 그러나 계산을 위해 공식 LUMI 데이터에 의존하므로 <span class="ltx_text ltx_font_bold" id="S4.SS4.p3.3.1">69.78 tCO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.SS4.p3.3.1.m1.1"><semantics id="S4.SS4.p3.3.1.m1.1a"><msub id="S4.SS4.p3.3.1.m1.1.1" xref="S4.SS4.p3.3.1.m1.1.1.cmml"><mi id="S4.SS4.p3.3.1.m1.1.1a" xref="S4.SS4.p3.3.1.m1.1.1.cmml"></mi><mn id="S4.SS4.p3.3.1.m1.1.1.1" mathvariant="normal" xref="S4.SS4.p3.3.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.1.m1.1b"><apply id="S4.SS4.p3.3.1.m1.1.1.cmml" xref="S4.SS4.p3.3.1.m1.1.1"><cn id="S4.SS4.p3.3.1.m1.1.1.1.cmml" type="integer" xref="S4.SS4.p3.3.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.1.m1.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.3.1.m1.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>eq</span>의 총 사전 훈련 배출량을 추정한다. <span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>These metrics were in part collected using Carbonara’s AI agent and monitoring platform. Learn more at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trycarbonara.com" title="">https://trycarbonara.com</a></span></span></span> 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#footnote12b" title="footnote 12 ‣ Table 8 ‣ 4.4 Power Consumption and Carbon Footprint ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">12</span></a>에서 우리는 공개적으로 사용 가능한 정보를 기반으로 우리의 모델을 이전에 출시된 다른 모델과 비교한다.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">우리는 우리의 모델을 공개적으로 출시하면 다른 사람들이 모델을 처음부터 사전 훈련할 필요가 없도록 함으로써 미래의 배출량을 줄이고 최첨단 모델을 개발하는 데 드는 진정한 비용에 대한 통찰력을 제공할 수 있기를 바란다. 또한 디버깅, 하이퍼파라미터 튜닝 및 다운타임과 같은 다른 중요한 개발 부분을 포함하지 않기 때문에 추정치가 하한임을 강조합니다.</p>
</div>
<figure class="ltx_table" id="S4.T8">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T8.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T8.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T8.2.2.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.2.2.4">GPU Type</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.2.2.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.2.2.5.1">
<tbody><tr class="ltx_tr" id="S4.T8.2.2.5.1.1">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.5.1.1.1">GPU Power</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.5.1.2">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.5.1.2.1">Consumption</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.5.1.3">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.5.1.3.1">(MWh)</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.2.2.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.2.2.6.1">
<tbody><tr class="ltx_tr" id="S4.T8.2.2.6.1.1">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.6.1.1.1">Power</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.6.1.2">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.6.1.2.1">Usage</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.6.1.3">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.6.1.3.1">Effectiveness</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.1.1.1.1">
<tbody><tr class="ltx_tr" id="S4.T8.1.1.1.1.2">
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.1.2.1">Carbon</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.1.1.3">
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.1.3.1">Intensity</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.1.1.1.1">
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.1.1.1">(kg CO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.T8.1.1.1.1.1.1.m1.1"><semantics id="S4.T8.1.1.1.1.1.1.m1.1a"><msub id="S4.T8.1.1.1.1.1.1.m1.1.1" xref="S4.T8.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T8.1.1.1.1.1.1.m1.1.1a" xref="S4.T8.1.1.1.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T8.1.1.1.1.1.1.m1.1.1.1" xref="S4.T8.1.1.1.1.1.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T8.1.1.1.1.1.1.m1.1b"><apply id="S4.T8.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T8.1.1.1.1.1.1.m1.1.1"><cn id="S4.T8.1.1.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T8.1.1.1.1.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.1.1.1.1.1.1.m1.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T8.1.1.1.1.1.1.m1.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>e/KWh)</td>
</tr>
</tbody></table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T8.2.2.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T8.2.2.2.1">
<tbody><tr class="ltx_tr" id="S4.T8.2.2.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.2.1.2.1">Carbon</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.2.1.3">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.2.1.3.1">Emissions</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.2.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T8.2.2.2.1.1.1">(tCO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.T8.2.2.2.1.1.1.m1.1"><semantics id="S4.T8.2.2.2.1.1.1.m1.1a"><msub id="S4.T8.2.2.2.1.1.1.m1.1.1" xref="S4.T8.2.2.2.1.1.1.m1.1.1.cmml"><mi id="S4.T8.2.2.2.1.1.1.m1.1.1a" xref="S4.T8.2.2.2.1.1.1.m1.1.1.cmml"></mi><mn id="S4.T8.2.2.2.1.1.1.m1.1.1.1" xref="S4.T8.2.2.2.1.1.1.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T8.2.2.2.1.1.1.m1.1b"><apply id="S4.T8.2.2.2.1.1.1.m1.1.1.cmml" xref="S4.T8.2.2.2.1.1.1.m1.1.1"><cn id="S4.T8.2.2.2.1.1.1.m1.1.1.1.cmml" type="integer" xref="S4.T8.2.2.2.1.1.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.2.2.2.1.1.1.m1.1c">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T8.2.2.2.1.1.1.m1.1d">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>eq)</td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T8.2.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T8.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.3.1.1.1">Gopher-280B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.2">TPU v3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.3">1,066</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.4">1.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.5">0.330</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.3.1.6">380</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.4.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.4.2.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.4.2.1.1">BLOOM-176B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.2">A100-80GB</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.3">433</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.4">1.2</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.5">0.057</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.4.2.6">30</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.5.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.5.3.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.5.3.1.1">OPT-175B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.2">A100-80GB</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.3">324</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.4">1.1</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.5">0.231</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.5.3.6">82</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.6.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.6.4.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.6.4.1.1">T5-11B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.2">TPU v3</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.3">77</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.4">1.12</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.5">0.545</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.6.4.6">47</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.7.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.7.5.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.7.5.1.1">LLaMA-7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.2">A100-80GB</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.3">33</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.4">1.1</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.5">0.385</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.7.5.6">14</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.8.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.8.6.1"><span class="ltx_text ltx_font_bold" id="S4.T8.2.8.6.1.1">LLaMA2-7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.2">A100-80GB</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.3">74</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.4">1.1</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.5">0.385</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.8.6.6">31</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.9.7" style="background-color:#40C4DF;">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T8.2.9.7.1"><span class="ltx_text" id="S4.T8.2.9.7.1.1" style="background-color:#40C4DF;">[]
<span class="ltx_text ltx_font_bold" id="S4.T8.2.9.7.1.1.1">OLMo-7B</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.2"><span class="ltx_text" id="S4.T8.2.9.7.2.1" style="background-color:#40C4DF;">MI250X</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.3"><span class="ltx_text" id="S4.T8.2.9.7.3.1" style="background-color:#40C4DF;">135</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.4"><span class="ltx_text" id="S4.T8.2.9.7.4.1" style="background-color:#40C4DF;">1.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.5"><span class="ltx_text" id="S4.T8.2.9.7.5.1" style="background-color:#40C4DF;">0.000*</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.9.7.6"><span class="ltx_text" id="S4.T8.2.9.7.6.1" style="background-color:#40C4DF;">0*</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.10.8" style="background-color:#40C4DF;">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T8.2.10.8.1"><span class="ltx_text" id="S4.T8.2.10.8.1.1" style="background-color:#40C4DF;">[]
<span class="ltx_text ltx_font_bold" id="S4.T8.2.10.8.1.1.1">OLMo-7B</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.2"><span class="ltx_text" id="S4.T8.2.10.8.2.1" style="background-color:#40C4DF;">A100-40GB</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.3"><span class="ltx_text" id="S4.T8.2.10.8.3.1" style="background-color:#40C4DF;">104</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.4"><span class="ltx_text" id="S4.T8.2.10.8.4.1" style="background-color:#40C4DF;">1.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.5"><span class="ltx_text" id="S4.T8.2.10.8.5.1" style="background-color:#40C4DF;">0.610</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.10.8.6"><span class="ltx_text" id="S4.T8.2.10.8.6.1" style="background-color:#40C4DF;">70</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 8:</span>CO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.T8.5.m1.1"><semantics id="S4.T8.5.m1.1b"><msub id="S4.T8.5.m1.1.1" xref="S4.T8.5.m1.1.1.cmml"><mi id="S4.T8.5.m1.1.1b" xref="S4.T8.5.m1.1.1.cmml"></mi><mn id="S4.T8.5.m1.1.1.1" xref="S4.T8.5.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T8.5.m1.1c"><apply id="S4.T8.5.m1.1.1.cmml" xref="S4.T8.5.m1.1.1"><cn id="S4.T8.5.m1.1.1.1.cmml" type="integer" xref="S4.T8.5.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.5.m1.1d">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T8.5.m1.1e">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math> emissions during pretraining. 우리는 PUE에 대한 공개적으로 이용 가능한 데이터, 지역 전력망의 탄소 강도 및 보고된 전력 소비를 사용하여 다양한 모델에 대한 총 탄소 배출량을 추정한다. Gopher-280B <cite class="ltx_cite ltx_citemacro_citep">(Rae et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib61" title="">2022</a>)</cite>, BLOOM-176B <cite class="ltx_cite ltx_citemacro_citep">(Luccioni et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib42" title="">2022</a>)</cite>, OPT-175B <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib92" title="">2022</a>)</cite>, T5-11B <cite class="ltx_cite ltx_citemacro_citep">(Patterson et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib56" title="">2021</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>)</cite>, LLaMA2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite>를 각각의 논문에서 취한다. tCO2eq가 어떻게 계산되었는지에 대한 자세한 내용은 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS4" title="4.4 Power Consumption and Carbon Footprint ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.4</span></a>를 참조하세요.</figcaption><figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>CO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.T8.5.m1.1"><semantics id="S4.T8.5.m1.1b"><msub id="S4.T8.5.m1.1.1" xref="S4.T8.5.m1.1.1.cmml"><mi id="S4.T8.5.m1.1.1b" xref="S4.T8.5.m1.1.1.cmml"></mi><mn id="S4.T8.5.m1.1.1.1" xref="S4.T8.5.m1.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T8.5.m1.1c"><apply id="S4.T8.5.m1.1.1.cmml" xref="S4.T8.5.m1.1.1"><cn id="S4.T8.5.m1.1.1.1.cmml" type="integer" xref="S4.T8.5.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.5.m1.1d">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T8.5.m1.1e">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math> emissions during pretraining. We estimate the total carbon emissions for various models using publicly available data on PUE, carbon intensity of local power grid, and reported power consumption. Numbers for Gopher-280B <cite class="ltx_cite ltx_citemacro_citep">(Rae et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib61" title="">2022</a>)</cite>, BLOOM-176B <cite class="ltx_cite ltx_citemacro_citep">(Luccioni et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib42" title="">2022</a>)</cite>, OPT-175B <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib92" title="">2022</a>)</cite>, T5-11B <cite class="ltx_cite ltx_citemacro_citep">(Patterson et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib56" title="">2021</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib77" title="">2023a</a>)</cite>, and LLaMA2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>)</cite> are taken from their respective papers. See Section <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS4" title="4.4 Power Consumption and Carbon Footprint ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.4</span></a> for details on how tCO2eq was calculated.
<br class="ltx_break">* LUMI runs entirely on hydroelectric power<span class="ltx_note ltx_role_footnotemark" id="footnote12b"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">12</span></span></span></span>and some estimates <cite class="ltx_cite ltx_citemacro_citep">(Ubierna et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib79" title="">2022</a>)</cite> measure the intensity factor of hydroelectric power to be 0.024, implying total emissions of 3.54 tCO<math alttext="{}_{2}" class="ltx_Math" display="inline" id="S4.T8.6.m2.1"><semantics id="S4.T8.6.m2.1b"><msub id="S4.T8.6.m2.1.1" xref="S4.T8.6.m2.1.1.cmml"><mi id="S4.T8.6.m2.1.1b" xref="S4.T8.6.m2.1.1.cmml"></mi><mn id="S4.T8.6.m2.1.1.1" xref="S4.T8.6.m2.1.1.1.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T8.6.m2.1c"><apply id="S4.T8.6.m2.1.1.cmml" xref="S4.T8.6.m2.1.1"><cn id="S4.T8.6.m2.1.1.1.cmml" type="integer" xref="S4.T8.6.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.6.m2.1d">{}_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T8.6.m2.1e">start_FLOATSUBSCRIPT 2 end_FLOATSUBSCRIPT</annotation></semantics></math>eq.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Artifacts Released</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">모든 파이프라인 단계의 아티팩트를 공유함으로써 개방형 연구를 장려하고 학계와 실무자가 중복되고 종종 비용이 많이 드는 노력을 줄이는 것을 목표로 한다. 다음 내용을 공개합니다.</p>
</div>
<div class="ltx_para" id="S5.p2">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">훈련 및 모델링 코드입니다. <span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo" title="">https://github.com/allenai/OLMo</a></span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">7B 모델,<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B" title="">https://huggingface.co/allenai/OLMo-7B</a></span></span></span>7B-twin-2T,<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B-Twin-2T" title="">https://huggingface.co/allenai/OLMo-7B-Twin-2T</a></span></span></span> 및 1B 모델에 대한 학습된 모델 가중치. <span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-1B" title="">https://huggingface.co/allenai/OLMo-1B</a></span></span></span> 모든 모델에 대해 최종 모델 가중치뿐만 아니라 1000단계 간격으로 500+ 중간 체크포인트를 릴리스합니다.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Adapted OLMo-7B with instruction-tuning, 7B-SFT<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B-SFT" title="">https://huggingface.co/allenai/OLMo-7B-SFT</a></span></span></span>, and RLHF, 7B-Instruct<span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B-Instruct" title="">https://huggingface.co/allenai/OLMo-7B-Instruct</a></span></span></span> including its training and evaluation code and data using our Open Instruct<span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/open-instruct" title="">https://github.com/allenai/open-instruct</a></span></span></span> library <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib83" title="">2023</a>; Ivison et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">훈련 데이터 Dolma<cite class="ltx_cite ltx_citemacro_citep">(Soldaini et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>)</cite>. <span class="ltx_note ltx_role_footnote" id="footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/allenai/dolma" title="">https://huggingface.co/datasets/allenai/dolma</a></span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">Dolma's toolkit to construct new datasets,<span class="ltx_note ltx_role_footnote" id="footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/dolma" title="">https://github.com/allenai/dolma</a></span></span></span> and WIMBD <cite class="ltx_cite ltx_citemacro_citep">(Elazar et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib24" title="">2023</a>)</cite> for dataset analysis. <span class="ltx_note ltx_role_footnote" id="footnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/wimbd" title="">https://github.com/allenai/wimbd</a></span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.1">다운스트림 평가용 Catwalk<span class="ltx_note ltx_role_footnote" id="footnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/catwalk" title="">https://github.com/allenai/catwalk</a></span></span></span>을 사용한 평가 코드<span class="ltx_note ltx_role_footnote" id="footnote23"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo-Eval" title="">https://github.com/allenai/OLMo-Eval</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Groeneveld et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib28" title="">2023</a>)</cite> 및 Perplexity 기반 평가용 Paloma<span class="ltx_note ltx_role_footnote" id="footnote25"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://paloma.allen.ai" title="">https://paloma.allen.ai</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S5.I1.i7.p1">
<p class="ltx_p" id="S5.I1.i7.p1.1">The complete set of metrics logged to Weights &amp; Biases during training.<span class="ltx_note ltx_role_footnote" id="footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5" title="">https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5</a></span></span></span></p>훈련 중 가중치와 편향에 기록된 전체 메트릭 집합입니다. <span class="ltx_note ltx_role_footnote" id="footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5" title="">https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5</a></span></span></span></p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">추가 교육 로그, 절제 및 결과를 사용하여 이 릴리스에 대한 후속 조치를 취할 계획입니다.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>License</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">우리의 목표는 과학 개발을 촉진하고 과학 커뮤니티에 권한을 부여하는 것이므로 사용자에게 자원과 인공물을 사용할 수 있는 유연성을 제공하는 허용 라이선스를 선호합니다. 따라서 모든 코드 및 가중치는 Apache 2.0 라이선스에 따라 릴리스됩니다. <span class="ltx_note ltx_role_footnote" id="footnote27"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.apache.org/licenses/LICENSE-2.0" title="">http://www.apache.org/licenses/LICENSE-2.0</a></span></span></span> 최근 모델 릴리스에 대해 다른 조직에서 사용하는 일부 라이선스는 인공 지능 또는 기계 학습 시스템을 훈련하기 위해 모델의 출력을 사용하는 것을 금지하지만, 우리는 명시적으로 사용자가 그렇게 하도록 허용합니다. 또한 상업적 사용을 제한하지 않습니다. 저희 모델이 다른 모델을 더 좋게 만들 수 있기를 바랍니다. 우리는 우리 모델이 널리 채택된 제품이 아닌 주로 과학적 인공물로 설계되었기 때문에 오용 위험이 상대적으로 낮다는 것을 인식한다(우리 모델은 챗봇으로 채택되지 않았다). 또한 지난 1년 동안 매우 허용 가능한 라이선스로 출시된 비교 가능한 모델이 많았기 때문에 보다 엄격한 라이선스를 사용하여 해당 분야의 전반적인 위험을 제거하지 못할 것입니다. 우리는 더 개방적인 측면에서 이 트레이드오프가 최선의 선택이라고 믿습니다.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This technical report presents our first release of OLMo, a state-of-the-art, truly open language model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. Soon, we will also release training logs, ablations, findings and Weights &amp; Biases logs. We are also exploring the adaptation of OLMo with instruction tuning and different flavors of RLHF. We are going to release the adapted models as well as all of our model adaptation code and data.</p>이 기술 보고서는 언어 모델링의 과학을 구축하고 연구하기 위한 최첨단 진정한 개방형 언어 모델인 OLMo의 첫 번째 출시를 제시한다. 기존의 모델 가중치 및 추론 코드만을 발표하던 기존 연구들과는 달리 OLMo와 학습 데이터, 학습 및 평가 코드를 포함한 전체 프레임워크를 발표한다. 곧, 우리는 또한 훈련 로그, 절제, 발견 및 가중치 및 편향 로그를 공개할 것이다. 또한 RLHF의 명령어 조정 및 다양한 맛을 사용하여 OLMo의 적응을 탐색하고 있다. 우리는 모든 모델 적응 코드와 데이터뿐만 아니라 적응된 모델을 출시할 것입니다.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">우리는 OLMo와 그 프레임워크를 지속적으로 지원하고 확장하며 개방형 연구 커뮤니티에 힘을 실어주기 위해 개방형 LM의 경계를 계속 밀어붙일 계획이다. 이를 위해 OLMo 계열에 다양한 모델 크기, 양식, 데이터 세트, 안전 조치 및 평가를 가져오기를 기대한다. 우리는 이 발표와 향후 발표가 열린 연구 커뮤니티에 힘을 실어주고 강화하며 새로운 혁신의 물결을 불러일으키기를 바랍니다.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Author Contributions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">올모는 많은 팀원들과 협력자들의 도움이 없었다면 불가능했을 것이다. 아래에 저자 기여도(알파벳 순서로)를 나열합니다.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1"><span class="ltx_text ltx_font_bold" id="Sx1.p2.1.1">pretraining dataset construction and tooling</span> (돌마)의 기여자는 Russell Authur, Iz Beltagy, Akshita Bhagia, Khyathi Chandu, Jesse Dodge, Yanai Elazar, Dirk Groeneveld, Rodney Kinney, Kyle Lo, Aakanksha Naik, Abhilasha Ravichander, Dustin Schwenk, Luca Soldaini, Nishant Subramani를 포함한다.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1"><span class="ltx_text ltx_font_bold" id="Sx1.p3.1.1">모델 훈련 및 아키텍처</span>에 대한 기여자는 Shane Arora, Iz Beltagy, Akshita Bhagia, Matthew E. Peters, Dirk Groeneveld, Ananya Harsh Jha, William Merrill, Jacob Morrison, Niklas Muennighoff, Dustin Schwenk, Saurabh Shah, Pete Walsh 및 Mitchell Wortsman을 포함한다.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1"><span class="ltx_text ltx_font_bold" id="Sx1.p4.1.1">evaluation suite and tooling</span>에는 Akshita Bhagia, Arman Cohan, Pradeep Dasigi, Jesse Dodge, Dirk Groeneveld, Yuling Gu, Tushar Khot, Ian Magnusson, Kyle Richardson, Oyvind Tajford, and Pete Walsh가 포함된다.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1"><span class="ltx_text ltx_font_bold" id="Sx1.p5.1.1">model adaptation</span>에는 Iz Beltagy, Pradeep Dasigi, Jack Hessel, Hamish Ivison, Nathan Lambert, Valentina Pyatkin, Pete Walsh, Yizhong Wang이 포함됩니다.</p>
</div>
<div class="ltx_para" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1"><span class="ltx_text ltx_font_bold" id="Sx1.p6.1.1">license creation and risk assessment</span>에는 David Atkinson, Jesse Dodge, Jennifer Dumas, Crystal Nam, and Will Smith가 있다.</p>
</div>
<div class="ltx_para" id="Sx1.p7">
<p class="ltx_p" id="Sx1.p7.1">OLMo 프로젝트는 한나네 하지시르지와 노아 A. 스미스가 주도했다.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">OLMo는 많은 개인과 기관의 지원이 없었다면 불가능했을 것이다. 이 작업의 실험 구성 요소는 AMD 및 CSC와의 파트너십을 통해 가능했으며, LUMI 슈퍼컴퓨터와 하버드 대학의 켐프너 연구소의 사용이 가능했다. 우리는 조나단 프랭클과 모자이크ML의 팀(현재 Databricks)이 그들의 경험을 FSDP와 공유하고 OLMo가 기반으로 하는 코드 기반을 구축해준 것에 감사드린다. 우리는 팀 동료인 타이라 앤더슨, 미셸 베네딕트, 존 보르차르트, 이비 쳉, 아르나비 체다, 요한 담, 맷 라츠케, 켈시 맥밀란, 아론 사르나트, 카리사 쇤익, 샘 스콘스버그, 마이클 슈미츠, 마이클 윌슨, 케이틀린 위틀리, 그리고 전체 IT 팀에게 웹사이트, 디자인, 내부 및 외부 커뮤니케이션, 예산 책정 및 이 프로젝트의 원활한 진행을 지원하는 기타 활동에 대한 도움을 감사드립니다. 마지막으로, 우리는 또한 프리트비라지(라지) 암마나브로루, 피터 클라크, 니콜 데카리오, 더그 다우니, 알리 파하디, 이안 페레이라, 바이뫼 하탄파에, 샴 M을 포함한 AI2의 팀원들과 가까운 협력자들의 도움이 되는 토론과 피드백에 감사를 표한다. Kakade, Julien Launay, Sydney Levine, Pekka Manninen, Franzi Roessner, Maarten Sap, Ludwig Schmidt, Yulia Tsvetkov, Daniel S. 웰드</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbas et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari&nbsp;S Morcos.

</span>
<span class="ltx_bibblock">Semdedup: Data-efficient learning at web-scale through semantic
deduplication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.09540</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.09540" title="">https://arxiv.org/abs/2303.09540</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almazrouei et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
Ruxandra-Aimée Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic,
Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo.

</span>
<span class="ltx_bibblock">The falcon series of open language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">ArXiv</em>, abs/2311.16867, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:265466629" title="">https://api.semanticscholar.org/CorpusID:265466629</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anand et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy
Mulyar.

</span>
<span class="ltx_bibblock">Gpt4all: Training an assistant-style chatbot with large scale data
distillation from gpt-3.5-turbo.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nomic-ai/gpt4all" title="">https://github.com/nomic-ai/gpt4all</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba et&nbsp;al. [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jimmy Ba, Jamie&nbsp;Ryan Kiros, and Geoffrey&nbsp;E. Hinton.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ArXiv</em>, abs/1607.06450, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:8236317" title="">https://api.semanticscholar.org/CorpusID:8236317</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,
Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage,
Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,
Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning
from human feedback, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et&nbsp;al. [2003]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">J. Mach. Learn. Res.</em>, 3:1137–1155, 2003.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:221275765" title="">https://api.semanticscholar.org/CorpusID:221275765</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stella Biderman, Hailey Schoelkopf, Quentin&nbsp;Gregory Anthony, Herbie Bradley,
Kyle O’Brien, Eric Hallahan, Mohammad&nbsp;Aflah Khan, Shivanshu Purohit,
Usvsn&nbsp;Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar
Van Der&nbsp;Wal.

</span>
<span class="ltx_bibblock">Pythia: A suite for analyzing large language models across training
and scaling.

</span>
<span class="ltx_bibblock">In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 40th
International Conference on Machine Learning</em>, volume 202 of
<em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">Proceedings of Machine Learning Research</em>, pages 2397–2430. PMLR,
23–29 Jul 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v202/biderman23a.html" title="">https://proceedings.mlr.press/v202/biderman23a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BigScience et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
BigScience, Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana
Ilić, Daniel Hesslow, Roman Castagné, Alexandra&nbsp;Sasha Luccioni,
François Yvon, et&nbsp;al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2211.05100</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the AAAI conference on artificial
intelligence</em>, volume&nbsp;34, pages 7432–7439, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/6239" title="">https://ojs.aaai.org/index.php/AAAI/article/view/6239</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,
USVSN&nbsp;Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben
Wang, and Samuel Weinbach.

</span>
<span class="ltx_bibblock">GPT-NeoX-20B: An open-source autoregressive language model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the ACL Workshop on Challenges &amp;
Perspectives in Creating Large Language Models</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.06745" title="">https://arxiv.org/abs/2204.06745</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blodgett et&nbsp;al. [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Su&nbsp;Lin Blodgett, Lisa Green, and Brendan O’Connor.

</span>
<span class="ltx_bibblock">Demographic dialectal variation in social media: A case study of
African-American English.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1119–1130, Austin, Texas, November 2016.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/D16-1120" title="">10.18653/v1/D16-1120</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D16-1120" title="">https://aclanthology.org/D16-1120</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T.&nbsp;J.
Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeff Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">ArXiv</em>, abs/2005.14165, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:218971783" title="">https://api.semanticscholar.org/CorpusID:218971783</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E. Gonzalez, Ion Stoica, and
Eric&nbsp;P. Xing.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality, March 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="">https://lmsys.org/blog/2023-03-30-vicuna/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
Abhishek Rao, Parker Barnes, Yi&nbsp;Tay, Noam Shazeer, Vinodkumar Prabhakaran,
Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
Dohan, Shivani Agrawal, Mark Omernick, Andrew&nbsp;M. Dai,
Thanumalayan&nbsp;Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.02311" title="">https://arxiv.org/abs/2204.02311</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chronopoulou et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexandra Chronopoulou, Matthew Peters, and Jesse Dodge.

</span>
<span class="ltx_bibblock">Efficient hierarchical domain adaptation for pretrained language
models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 1336–1351, Seattle, United States, July 2022.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.naacl-main.96" title="">10.18653/v1/2022.naacl-main.96</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.naacl-main.96" title="">https://aclanthology.org/2022.naacl-main.96</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Noah Constant, Xavier García, Adam Roberts, Yi&nbsp;Tay, Sharan
Narang, and Orhan Firat.

</span>
<span class="ltx_bibblock">Unimax: Fairer and more effective language sampling for large-scale
multilingual pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">ArXiv</em>, abs/2304.09151, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258187051" title="">https://api.semanticscholar.org/CorpusID:258187051</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1905.10044</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning
challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1803.05457" title="">https://arxiv.org/abs/1803.05457</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali
Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.

</span>
<span class="ltx_bibblock">Free dolly: Introducing the world’s first truly open
instruction-tuned llm, 2023.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm" title="">https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,
Zhiyuan Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">Ultrafeedback: Boosting language models with high-quality feedback,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai,
Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige
Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao,
Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi
Li, Yao Li, Y.&nbsp;K. Li, Wenfeng Liang, Fangyun Lin, A.&nbsp;X. Liu, Bo&nbsp;Liu, Wen Liu,
Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong
Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren,
Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su,
Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu
Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y.&nbsp;Wu, Xin Xie, Zhenda Xie, Ziwei
Xie, Yiliang Xiong, Hanwei Xu, R.&nbsp;X. Xu, Yanhong Xu, Dejian Yang, Yuxiang
You, Shuiping Yu, Xingkai Yu, B.&nbsp;Zhang, Haowei Zhang, Lecong Zhang, Liyue
Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang
Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou.

</span>
<span class="ltx_bibblock">Deepseek llm: Scaling open-source language models with longtermism,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jesse Dodge, Taylor Prewitt, Remi Tachet&nbsp;Des Combes, Erika Odmark, Roy
Schwartz, Emma Strubell, Alexandra&nbsp;Sasha Luccioni, Noah&nbsp;A. Smith, Nicole
DeCario, and Will Buchanan.

</span>
<span class="ltx_bibblock">Measuring the carbon intensity of ai in cloud instances, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/10.1145/3531146.3533234" title="">https://dl.acm.org/doi/10.1145/3531146.3533234</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dolan and Brockett [2005]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
William&nbsp;B. Dolan and Chris Brockett.

</span>
<span class="ltx_bibblock">Automatically constructing a corpus of sentential paraphrases.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">International Joint Conference on Natural Language
Processing</em>, 2005.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.microsoft.com/en-us/research/publication/automatically-constructing-a-corpus-of-sentential-paraphrases/" title="">https://www.microsoft.com/en-us/research/publication/automatically-constructing-a-corpus-of-sentential-paraphrases/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elazar et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yanai Elazar, Akshita Bhagia, Ian&nbsp;H. Magnusson, Abhilasha Ravichander, Dustin
Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer
Singh, Hanna Hajishirzi, Noah&nbsp;A. Smith, and Jesse Dodge.

</span>
<span class="ltx_bibblock">What’s in my big data?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ArXiv</em>, abs/2310.20707, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:264803575" title="">https://api.semanticscholar.org/CorpusID:264803575</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et&nbsp;al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2101.00027</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2101.00027" title="">https://arxiv.org/abs/2101.00027</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noac’h,
Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang,
Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric
Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation, 12 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/10256836" title="">https://zenodo.org/records/10256836</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Greenbaum and Nelson [1996]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sidney Greenbaum and Gerald Nelson.

</span>
<span class="ltx_bibblock">The international corpus of english (ICE) project.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">World Englishes</em>, 15(1):3–15, mar 1996.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1111/j.1467-971x.1996.tb00088.x" title="">10.1111/j.1467-971x.1996.tb00088.x</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1111%2Fj.1467-971x.1996.tb00088.x" title="">https://doi.org/10.1111%2Fj.1467-971x.1996.tb00088.x</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groeneveld et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dirk Groeneveld, Anas Awadalla, Iz&nbsp;Beltagy, Akshita Bhagia, Ian Magnusson, Hao
Peng, Oyvind Tafjord, Pete Walsh, Kyle Richardson, and Jesse Dodge.

</span>
<span class="ltx_bibblock">Catwalk: A unified language model evaluation framework for many
datasets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2312.10253</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.10253" title="">https://arxiv.org/abs/2312.10253</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding,
Jianwei Yue, and Yupeng Wu.

</span>
<span class="ltx_bibblock">How close is chatgpt to human experts? comparison corpus, evaluation,
and detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arxiv:2301.07597</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Suchin Gururangan, Mitchell Wortsman, Samir&nbsp;Yitzhak Gadre, Achal Dave, Maciej
Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt
Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and
Ludwig Schmidt.

</span>
<span class="ltx_bibblock">OpenLM: a minimal but performative language modeling (lm)
repository, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/mlfoundations/open_lm/" title="">https://github.com/mlfoundations/open_lm/</a>.

</span>
<span class="ltx_bibblock">GitHub repository.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartvigsen et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
and Ece Kamar.

</span>
<span class="ltx_bibblock">TOXIGEN: Controlling Language Models to Generate Implied and
Adversarial Toxicity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">ACL</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.09509" title="">https://arxiv.org/abs/2203.09509</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the International Conference on Learning
Representations (ICLR)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivison et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters,
Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A. Smith, Iz&nbsp;Beltagy, and
Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Camels in a changing climate: Enhancing lm adaptation with tulu 2,
2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.10702" title="">https://arxiv.org/abs/2311.10702</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
Savary, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Emma&nbsp;Bou
Hanna, Florian Bressand, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2401.04088</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.04088" title="">https://arxiv.org/abs/2401.04088</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis,
Zhi&nbsp;Rui Tam, Keith Stevens, Abdullah Barhoum, Duc&nbsp;Minh Nguyen, Oliver
Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David&nbsp;Alexandrovich
Glushkov, Arnav&nbsp;Varma Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu
Nguyen, and Alexander&nbsp;Julian Mattick.

</span>
<span class="ltx_bibblock">Openassistant conversations - democratizing large language model
alignment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Thirty-seventh Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=VSJotgbPHF" title="">https://openreview.net/forum?id=VSJotgbPHF</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock">Github repository, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/alpaca_eval" title="">https://github.com/tatsu-lab/alpaca_eval</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2211.09110</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.09110" title="">https://arxiv.org/abs/2211.09110</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214–3252,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.

</span>
<span class="ltx_bibblock">Logiqa: A challenge dataset for machine reading comprehension with
logical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">CoRR</em>, abs/2007.08124, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2007.08124" title="">https://arxiv.org/abs/2007.08124</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua
Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llm360: Towards fully transparent open-source llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2312.06550</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.06550" title="">https://arxiv.org/abs/2312.06550</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">International Conference on Learning Representations</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">https://openreview.net/forum?id=Bkg6RiCqY7</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luccioni et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alexandra&nbsp;Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.

</span>
<span class="ltx_bibblock">Estimating the carbon footprint of bloom, a 176b parameter language
model, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2211.02001" title="">https://arxiv.org/abs/2211.02001</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Magnusson et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya&nbsp;Harsh
Jha, Oyvind Tafjord, Dustin Schwenk, Evan&nbsp;Pete Walsh, Yanai Elazar, Kyle Lo,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Paloma: A benchmark for evaluating language model fit.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2312.10523</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcus et&nbsp;al. [1999]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mitchell&nbsp;P. Marcus, Beatrice Santorini, Mary&nbsp;Ann Marcinkiewicz, and Ann Taylor.

</span>
<span class="ltx_bibblock">Treebank-3, 1999.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://catalog.ldc.upenn.edu/LDC99T42" title="">https://catalog.ldc.upenn.edu/LDC99T42</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merity et&nbsp;al. [2016]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ArXiv</em>, abs/1609.07843, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:16299141" title="">https://api.semanticscholar.org/CorpusID:16299141</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micikevicius et&nbsp;al. [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory&nbsp;Frederick Diamos,
Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii
Kuchaiev, Ganesh Venkatesh, and Hao Wu.

</span>
<span class="ltx_bibblock">Mixed precision training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">ArXiv</em>, abs/1710.03740, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:3297437" title="">https://api.semanticscholar.org/CorpusID:3297437</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book
question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:1809.02789</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1809.02789" title="">https://arxiv.org/abs/1809.02789</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et&nbsp;al. [2013]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory&nbsp;S. Corrado, and Jeffrey Dean.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Neural Information Processing Systems</em>, 2013.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:16447573" title="">https://api.semanticscholar.org/CorpusID:16447573</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing
instructions.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, pages 3470–3487, Dublin,
Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2022.acl-long.244" title="">10.18653/v1/2022.acl-long.244</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.244" title="">https://aclanthology.org/2022.acl-long.244</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MosaicML NLP Team [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
MosaicML NLP Team.

</span>
<span class="ltx_bibblock">Introducing mpt-7b: A new standard for open-source, commercially
usable llms, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="www.mosaicml.com/blog/mpt-7b" title="">www.mosaicml.com/blog/mpt-7b</a>.

</span>
<span class="ltx_bibblock">Accessed: 2023-05-05.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Niklas Muennighoff, Alexander&nbsp;M Rush, Boaz Barak, Teven&nbsp;Le Scao, Aleksandra
Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2305.16264</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nunes [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Davide Nunes.

</span>
<span class="ltx_bibblock">Preprocessed penn tree bank, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/record/3910021" title="">https://zenodo.org/record/3910021</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">ArXiv</em>, abs/2303.08774, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257532815" title="">https://api.semanticscholar.org/CorpusID:257532815</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul&nbsp;F Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In S.&nbsp;Koyejo, S.&nbsp;Mohamed, A.&nbsp;Agarwal, D.&nbsp;Belgrave, K.&nbsp;Cho, and A.&nbsp;Oh,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Advances in Neural Information Processing Systems</em>, volume&nbsp;35,
pages 27730–27744. Curran Associates, Inc., 2022.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papasavva et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Antonis Papasavva, Savvas Zannettou, Emiliano&nbsp;De Cristofaro, Gianluca
Stringhini, and Jeremy Blackburn.

</span>
<span class="ltx_bibblock">Raiders of the lost kek: 3.5 years of augmented 4chan posts from the
politically incorrect board.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the International AAAI Conference on Web and
Social Media</em>, 14:885–894, may 2020.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1609/icwsm.v14i1.7354" title="">10.1609/icwsm.v14i1.7354</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1609%2Ficwsm.v14i1.7354" title="">https://doi.org/10.1609%2Ficwsm.v14i1.7354</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
Daniel Rothchild, David So, Maud Texier, and Jeff Dean.

</span>
<span class="ltx_bibblock">Carbon emissions and large neural network training, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2104.10350" title="">https://arxiv.org/abs/2104.10350</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aimée
Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam
Almazrouei, and Julien Launay.

</span>
<span class="ltx_bibblock">The refinedweb dataset for falcon llm: Outperforming curated corpora
with web data, and web data only.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">ArXiv</em>, abs/2306.01116, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:259063761" title="">https://api.semanticscholar.org/CorpusID:259063761</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Matthew&nbsp;E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Deep contextualized word representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ArXiv</em>, abs/1802.05365, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:3626819" title="">https://api.semanticscholar.org/CorpusID:3626819</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pilehvar and Camacho-Collados [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mohammad&nbsp;Taher Pilehvar and José Camacho-Collados.

</span>
<span class="ltx_bibblock">Wic: 10, 000 example pairs for evaluating context-sensitive
representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">CoRR</em>, abs/1808.09121, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1808.09121" title="">http://arxiv.org/abs/1808.09121</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press and Wolf [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ofir Press and Lior Wolf.

</span>
<span class="ltx_bibblock">Using the output embedding to improve language models.

</span>
<span class="ltx_bibblock">In Mirella Lapata, Phil Blunsom, and Alexander Koller, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume 2, Short Papers</em>, pages
157–163, Valencia, Spain, April 2017. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/E17-2025" title="">https://aclanthology.org/E17-2025</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jack&nbsp;W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
George van&nbsp;den Driessche, Lisa&nbsp;Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme
Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
Xiang&nbsp;Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
de&nbsp;Masson&nbsp;d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor
Babuschkin, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Chris Jones, James
Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel,
William Isaac, Ed&nbsp;Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray
Kavukcuoglu, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training
gopher, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2112.11446" title="">https://arxiv.org/abs/2112.11446</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher&nbsp;D Manning, Stefano
Ermon, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a
reward model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Thirty-seventh Conference on Neural Information Processing
Systems</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HPuSIXJaa9" title="">https://openreview.net/forum?id=HPuSIXJaa9</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">J. Mach. Learn. Res.</em>, 21(1), jan 2020.

</span>
<span class="ltx_bibblock">ISSN 1532-4435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">Zero: Memory optimizations toward training trillion parameter models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">SC20: International Conference for High Performance Computing,
Networking, Storage and Analysis</em>, pages 1–16, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:203736482" title="">https://api.semanticscholar.org/CorpusID:203736482</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Machel Reid, Victor Zhong, Suchin Gururangan, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">M2D2: A massively multi-domain language modeling dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 964–975, Abu Dhabi, United Arab
Emirates, December 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.63" title="">https://aclanthology.org/2022.emnlp-main.63</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Manoel&nbsp;Horta Ribeiro, Jeremy Blackburn, Barry Bradlyn, Emiliano&nbsp;De Cristofaro,
Gianluca Stringhini, Summer Long, Stephanie Greenberg, and Savvas Zannettou.

</span>
<span class="ltx_bibblock">The evolution of the manosphere across the web.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the International AAAI Conference on Web and
Social Media</em>, 15:196–207, may 2021.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1609/icwsm.v15i1.18053" title="">10.1609/icwsm.v15i1.18053</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1609%2Ficwsm.v15i1.18053" title="">https://doi.org/10.1609%2Ficwsm.v15i1.18053</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenfeld [2000]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ronald Rosenfeld.

</span>
<span class="ltx_bibblock">Two decades of statistical language modeling: Where do we go from
here?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the IEEE</em>, 88(8):1270–1278,
2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Communications of the ACM</em>, 64(9):99–106,
2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.acm.org/doi/abs/10.1145/3474381" title="">https://dl.acm.org/doi/abs/10.1145/3474381</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M&nbsp;Saiful
Bari, Canwen Xu, Urmish Thakker, Shanya&nbsp;Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang,
Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng&nbsp;Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason&nbsp;Alan Fries, Ryan
Teehan, Teven&nbsp;Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander&nbsp;M
Rush.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=9Vrb9D0WI4" title="">https://openreview.net/forum?id=9Vrb9D0WI4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer [2020]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Noam&nbsp;M. Shazeer.

</span>
<span class="ltx_bibblock">Glu variants improve transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">ArXiv</em>, abs/2002.05202, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:211096588" title="">https://api.semanticscholar.org/CorpusID:211096588</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soldaini et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson,
Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,
Valentin Hofmann, Ananya&nbsp;Harsh Jha, Sachin Kumar, Li&nbsp;Lucy, Xinxi Lyu, Nathan
Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik,
Crystal Nam, Matthew&nbsp;E. Peters, Abhilasha Ravichander, Kyle Richardson,
Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh,
Luke Zettlemoyer, Noah&nbsp;A. Smith, Hannaneh Hajishirzi, Iz&nbsp;Beltagy, Dirk
Groeneveld, Jesse Dodge, and Kyle Lo.

</span>
<span class="ltx_bibblock">Dolma: an Open Corpus of Three Trillion Tokens for Language Model
Pretraining Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Emma Strubell, Ananya Ganesh, and Andrew McCallum.

</span>
<span class="ltx_bibblock">Energy and policy considerations for deep learning in NLP.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>, pages 3645–3650, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/P19-1355" title="">10.18653/v1/P19-1355</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1355" title="">https://aclanthology.org/P19-1355</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. [2021]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">ArXiv</em>, abs/2104.09864, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:233307138" title="">https://api.semanticscholar.org/CorpusID:233307138</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" title="">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teknium1 [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Teknium1.

</span>
<span class="ltx_bibblock">Gpteacher.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/teknium1/GPTeacher" title="">https://github.com/teknium1/GPTeacher</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Together Computer [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Together Computer.

</span>
<span class="ltx_bibblock">RedPajama: An Open Source Recipe to Reproduce LLaMA training
dataset, April 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/togethercomputer/RedPajama-Data" title="">https://github.com/togethercomputer/RedPajama-Data</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023a]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">ArXiv</em>, abs/2302.13971, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257219404" title="">https://api.semanticscholar.org/CorpusID:257219404</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. [2023b]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models,
2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09288" title="">https://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ubierna et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
María Ubierna, Cristina&nbsp;Díez Santos, and Sara Mercier-Blais.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">Water Security and Climate Change: Hydropower Reservoir
Greenhouse Gas Emissions</em>, pages 69–94.

</span>
<span class="ltx_bibblock">Springer Singapore, Singapore, 2022.

</span>
<span class="ltx_bibblock">ISBN 978-981-16-5493-0.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1007/978-981-16-5493-0_5" title="">10.1007/978-981-16-5493-0˙5</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-981-16-5493-0_5" title="">https://doi.org/10.1007/978-981-16-5493-0_5</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Ł&nbsp;ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;Von Luxburg, S.&nbsp;Bengio, H.&nbsp;Wallach, R.&nbsp;Fergus,
S.&nbsp;Vishwanathan, and R.&nbsp;Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Advances in Neural
Information Processing Systems</em>, volume&nbsp;30. Curran Associates, Inc., 2017.

</span>
<span class="ltx_bibblock">URL
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilares and Gómez-Rodríguez [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
David Vilares and Carlos Gómez-Rodríguez.

</span>
<span class="ltx_bibblock">HEAD-QA: A healthcare dataset for complex reasoning.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>, pages 960–966, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/P19-1092" title="">10.18653/v1/P19-1092</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1092" title="">https://aclanthology.org/P19-1092</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
Samuel&nbsp;R. Bowman.

</span>
<span class="ltx_bibblock">Glue: A multi-task benchmark and analysis platform for natural
language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">ArXiv</em>, abs/1804.07461, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1804.07461" title="">https://arxiv.org/abs/1804.07461</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot,
Khyathi&nbsp;Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah&nbsp;A. Smith,
Iz&nbsp;Beltagy, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">How far can camels go? exploring the state of instruction tuning on
open resources, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.04751" title="">https://arxiv.org/abs/2306.04751</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams&nbsp;Wei Yu, Brian Lester,
Nan Du, Andrew&nbsp;M. Dai, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=gEZrGCozdqR" title="">https://openreview.net/forum?id=gEZrGCozdqR</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et&nbsp;al. [2017]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Johannes Welbl, Nelson&nbsp;F Liu, and Matt Gardner.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">arXiv preprint arXiv:1707.06209</em>, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1707.06209" title="">https://arxiv.org/abs/1707.06209</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,
Kiwan Maeng, Gloria Chang, Fiona&nbsp;Aga Behram, James Huang, Charles Bai,
Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore
Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin&nbsp;S. Lee, Bugra
Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, and Kim
Hazelwood.

</span>
<span class="ltx_bibblock">Sustainable ai: Environmental implications, challenges and
opportunities, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2111.00364" title="">https://arxiv.org/abs/2111.00364</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. [2024]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu&nbsp;Zhao, Jiazhan Feng, Chongyang
Tao, Qingwei Lin, and Daxin Jiang.

</span>
<span class="ltx_bibblock">WizardLM: Empowering large pre-trained language models to follow
complex instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">The Twelfth International Conference on Learning
Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=CfXh93NDgH" title="">https://openreview.net/forum?id=CfXh93NDgH</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.

</span>
<span class="ltx_bibblock">Baize: An open-source chat model with parameter-efficient tuning on
self-chat data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2304.01196</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zannettou et&nbsp;al. [2018]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Savvas Zannettou, Barry Bradlyn, Emiliano De&nbsp;Cristofaro, Haewoon Kwak, Michael
Sirivianos, Gianluca Stringini, and Jeremy Blackburn.

</span>
<span class="ltx_bibblock">What is gab: A bastion of free speech or an alt-right echo chamber.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Companion Proceedings of the The Web Conference 2018</em>, WWW
’18, page 1007–1014, Republic and Canton of Geneva, CHE, 2018.
International World Wide Web Conferences Steering Committee.

</span>
<span class="ltx_bibblock">ISBN 9781450356404.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.1145/3184558.3191531" title="">10.1145/3184558.3191531</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3184558.3191531" title="">https://doi.org/10.1145/3184558.3191531</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">arXiv preprint arXiv:1905.07830</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1905.07830" title="">https://arxiv.org/abs/1905.07830</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Sennrich [2019]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">ArXiv</em>, abs/1910.07467, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:113405151" title="">https://api.semanticscholar.org/CorpusID:113405151</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. [2022]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit&nbsp;Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.01068" title="">https://arxiv.org/abs/2205.01068</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. [2023]<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu,
Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can
Balioglu, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li.

</span>
<span class="ltx_bibblock">Pytorch fsdp: Experiences on scaling fully sharded data parallel.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">Proc. VLDB Endow.</em>, 16:3848–3860, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258297871" title="">https://api.semanticscholar.org/CorpusID:258297871</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="453" id="A1.F3.g1" src="https://arxiv.org/html/2402.00838v3/x8.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3: </span>Bits per byte for the 7 remaining Paloma data sources not aggregated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Bits per byte for each of the 7 remaining Paloma data sources not aggregated in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>.
</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Additional perplexity results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.1">그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.F3" title="Figure 3 ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>에서는 그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F2" title="Figure 2 ‣ Results ‣ 4.2 Intrinsic language modeling evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서 결합된 메트릭에서 제외된 팔로마 <cite class="ltx_cite ltx_citemacro_citep">[Magnusson et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib43" title="">2023</a>]</cite>의 7개 데이터 소스 각각에 대한 결과를 제공한다. Pile <cite class="ltx_cite ltx_citemacro_citep">[Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib25" title="">2020</a>]</cite> 및 ICE <cite class="ltx_cite ltx_citemacro_citep">[Greenbaum and Nelson, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib27" title="">1996</a>]</cite>와 같은 이러한 소스 중 일부는 현재 공개적으로 사용할 수 없다. 돌마 100 프로그래밍 언어 <cite class="ltx_cite ltx_citemacro_citep">[Soldaini et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib71" title="">2024</a>]</cite>는 팔로마에서 사용되는 오염 제거 접근법에 의해 지원되지 않는 코드 데이터로 구성된다. 트위터AAE <cite class="ltx_cite ltx_citemacro_citep">[Blodgett et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib11" title="">2016</a>]</cite>는 ICE와 함께 서로 다른 방언 간의 성능 차이를 대상으로 분석하기 위한 데이터 세트이므로 별도로 평가해야 한다. 마지막으로 Manosphere, Gab 및 4chan 코퍼라 <cite class="ltx_cite ltx_citemacro_citep">[Ribeiro et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib66" title="">2021</a>, Zannettou et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib89" title="">2018</a>, Papasavva et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib55" title="">2020</a>]</cite>는 일반적인 혐오 발언과 독성에 대해 연구된 프린지 온라인 커뮤니티의 언어에 대한 모델 적합성을 조사하기 위한 것이다. 따라서 이러한 프린지 코퍼스에 대한 복잡성을 최소화하는 것이 항상 바람직한 것은 아니다.</p>
</div>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p2.1">여기서 주목할 만한 결과 중 하나는 OLMo-7B가 돌마 100 프로그래밍 언어(100 PL)의 다른 모델보다 훨씬 앞서 있다는 것이다. 오염 제거 코드 데이터가 팔로마에서 방법의 범위를 벗어나기 때문에 이 효과는 부분적으로 오염으로 인한 과소평가 때문일 수 있다. 동시에 오염이 있을 가능성이 있는 RPJ-INCITE-7B와 같은 GitHub의 코드 데이터에 대해 훈련된 다른 모델은 훨씬 더 나쁘다. 또 다른 요인은 OLMo-7B가 100개의 PL에서와 동일한 후처리로 코드 데이터를 훈련하는 반면 다른 모델의 코드 데이터는 다르게 처리된다는 것이다. 유사하게, 파일 평가는 피티아-6.9B가 OLMo-7B보다 거의 100배 적은 토큰에 대해 훈련되었음에도 불구하고 최고의 성능을 달성함에 따라 이러한 분포 내 및 잠재적인 오염 효과를 보여준다.</p>
</div>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p3.1">팔로마는 종종 이러한 출처에 대한 당혹감이 실제로 이러한 연설 커뮤니티의 구성체에 두드러질 것 보다는 낮은 평균 문서 길이와 같은 피상적인 특징에 의해 지배된다는 것을 발견하기 때문에 나머지 5개의 표적 출처에 대한 결과는 신중하게 해석되어야 한다. 트위터AAE와 Gab은 이 그림에서 바이트당 비정상적으로 높은 비트에 기여하는 팔로마에서 가장 짧은 문서 중 하나이다. 이 두 가지 외에 모델은 ICE, 마노스피어 및 4chan의 데이터 스케일링 추세에서 특히 매우 밀접하게 그룹화된다.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Additional end-task results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.1">다음으로 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.T9" title="Table 9 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">9</span></a>에서 OLMo-7B의 제로샷 평가 결과를 핵심 평가 제품군의 8개를 제외한 6개의 추가 엔드 태스크에 제공한다. 이러한 작업은 <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.1">headqa_en</span> <cite class="ltx_cite ltx_citemacro_citep">[Vilares and Gómez-Rodríguez, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib81" title="">2019</a>]</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.2">logiqa</span> <cite class="ltx_cite ltx_citemacro_citep">[Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib39" title="">2020</a>]</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.4">qnli</span> <cite class="ltx_cite ltx_citemacro_citep">[Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib82" title="">2018</a>]</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.4">wic</span> <cite class="ltx_cite ltx_citemacro_citep">[Pilehvar and Camacho-Collados, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib59" title="">2018</a>]</cite>,</p>
</div>
<figure class="ltx_table" id="A1.T9">
<p class="ltx_p ltx_align_center" id="A1.T9.1"><span class="ltx_text" id="A1.T9.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A1.T9.1.1.1" style="width:249.1pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="A1.T9.1.1.1.1"><span class="ltx_text" id="A1.T9.1.1.1.1.1" style="color:#000000;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T9.1.1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.2">headqa_en</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.3">logiqa</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.4">mrpc</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.5">qnli</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.6">wic</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.7">wnli</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.1.1.1.1.1.8">avg.</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.2.1.1.1">Falcon-7B</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.2">38.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.3">23.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.4">62.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.5">49.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.6">49.5</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.7">47.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.1.1.1.1.2.1.8">45.4</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.3.2.1.1">LLaMA-7B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.2">38.7</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.3">19.5</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.4">68.6</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.5">50.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.6">49.1</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.3.2.7">52.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.3.2.8">46.4</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.4.3.1.1">LLaMA2-7B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.2">39.5</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.3">26.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.4">69.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.5">49.4</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.6">49.8</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.4.3.7">45.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.4.3.8">46.5</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.5.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.5.4.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.5.4.1.1">MPT-7B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.2">37.4</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.3">22.9</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.4">67.7</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.5">52.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.6">48.1</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.5.4.7">47.9</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.5.4.8">46.0</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.6.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.6.5.1.1">Pythia-6.9B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.2">40.1</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.3">21.5</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.4">65.4</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.5">53.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.6">55.0</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.6.5.7">38.0</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.6.5.8">45.6</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.7.6">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A1.T9.1.1.1.1.1.1.7.6.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.7.6.1.1">RPJ-INCITE-7B</span></span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.2">36.9</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.3">27.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.4">58.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.5">53.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.6">48.9</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.1.1.1.1.7.6.7">57.8</span>
<span class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.7.6.8">47.3</span></span>
<span class="ltx_tr" id="A1.T9.1.1.1.1.1.1.8.7" style="background-color:#40C4DF;">
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A1.T9.1.1.1.1.1.1.8.7.1"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.1.1" style="background-color:#40C4DF;">[]
<span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1.1.8.7.1.1.1">OLMo-7B</span></span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.2"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.2.1" style="background-color:#40C4DF;">37.3</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.3"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.3.1" style="background-color:#40C4DF;">23.4</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.4"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.4.1" style="background-color:#40C4DF;">68.4</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.5"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.5.1" style="background-color:#40C4DF;">49.1</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.6"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.6.1" style="background-color:#40C4DF;">50.2</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T9.1.1.1.1.1.1.8.7.7"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.7.1" style="background-color:#40C4DF;">56.3</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.1.1.1.1.1.1.8.7.8"><span class="ltx_text" id="A1.T9.1.1.1.1.1.1.8.7.8.1" style="background-color:#40C4DF;">47.5</span></span></span>
</span>
</span></span></span>
</span></span></span>
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 9:</span>Zero-shot evaluation of OLMo-7B on 6 additional end-tasks apart the 8 present in our core evaluation suite. 다시 한 번 OLMo-7B를 공개적으로 사용할 수 있는 6개의 다른 모델 체크포인트와 비교한다. 우리는 OLMo-7B가 이 표에서 6개의 추가 최종 작업을 수행한 집합에서 다른 모델보다 우수하지만 이러한 작업도 훈련 중에 제한된 신호를 제공하는 것으로 확인되었다(그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.F4" title="Figure 4 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> 참조).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p2.1">그러나 섹션 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.SS1" title="4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>에 설명된 핵심 평가 세트와 대조적으로 이러한 추가 엔드 태스크가 모델 개발 동안 덜 안정적인 성능을 가지며 제한된 신호를 제공한다는 점에 주목한다. 이것은 그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A1.F4" title="Figure 4 ‣ Additional end-task results ‣ Appendix A Additional Evaluation ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>에 예시되어 있으며, 여기서 우리는 훈련 전반에 걸쳐 작업 수행의 진척이 더 무작위적인 것으로 본다(그림 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.F1" title="Figure 1 ‣ Results ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>에서 더 안정적인 상승 추세와 비교). <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.1">mrpc</span> 및 <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.2">wic</span>과 같은 작업은 더 안정적으로 보이지만 성능과 관련된 추가 어려움을 제공했습니다. 랜덤 찬스(예: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.3">wic</span> 또는 데이터 세트 클래스 불균형(예: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.4">mrpc</span>). 따라서 우리는 훈련 및 모델 비교 전반에 걸쳐 모델 성능을 측정할 때 이러한 작업에 너무 많이 의존하지 않도록 주의한다.</p>
</div>
<figure class="ltx_figure" id="A1.F4">
<p class="ltx_p ltx_align_center" id="A1.F4.1"><span class="ltx_text" id="A1.F4.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="502" id="A1.F4.1.1.g1" src="https://arxiv.org/html/2402.00838v3/x9.png" width="830">
</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 4:</span>6개의 추가 엔드 태스크에서 OLMo-7B의 정확도 점수 진행. 이러한 추가 엔드 태스크의 성능은 불안정하고 모델 개발 동안 제한된 신호를 제공했다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Adaptation Training Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">우리는 명령어 튜닝 OLMo에서 다음과 같은 하이퍼파라미터를 사용한다. 이들은 작은 파일럿 실험을 통해 선택되었다.</p>
</div>
<div class="ltx_para" id="A2.p2">
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1">Learning Rate: <math alttext="2\times 10^{-6}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.1.m1.1"><semantics id="A2.I1.i1.p1.1.m1.1a"><mrow id="A2.I1.i1.p1.1.m1.1.1" xref="A2.I1.i1.p1.1.m1.1.1.cmml"><mn id="A2.I1.i1.p1.1.m1.1.1.2" xref="A2.I1.i1.p1.1.m1.1.1.2.cmml">2</mn><mo id="A2.I1.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.I1.i1.p1.1.m1.1.1.1.cmml">×</mo><msup id="A2.I1.i1.p1.1.m1.1.1.3" xref="A2.I1.i1.p1.1.m1.1.1.3.cmml"><mn id="A2.I1.i1.p1.1.m1.1.1.3.2" xref="A2.I1.i1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A2.I1.i1.p1.1.m1.1.1.3.3" xref="A2.I1.i1.p1.1.m1.1.1.3.3.cmml"><mo id="A2.I1.i1.p1.1.m1.1.1.3.3a" xref="A2.I1.i1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="A2.I1.i1.p1.1.m1.1.1.3.3.2" xref="A2.I1.i1.p1.1.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.I1.i1.p1.1.m1.1b"><apply id="A2.I1.i1.p1.1.m1.1.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1"><times id="A2.I1.i1.p1.1.m1.1.1.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1.1"></times><cn id="A2.I1.i1.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.I1.i1.p1.1.m1.1.1.2">2</cn><apply id="A2.I1.i1.p1.1.m1.1.1.3.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3">superscript</csymbol><cn id="A2.I1.i1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A2.I1.i1.p1.1.m1.1.1.3.2">10</cn><apply id="A2.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3.3"><minus id="A2.I1.i1.p1.1.m1.1.1.3.3.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3.3"></minus><cn id="A2.I1.i1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A2.I1.i1.p1.1.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.i1.p1.1.m1.1c">2\times 10^{-6}</annotation><annotation encoding="application/x-llamapun" id="A2.I1.i1.p1.1.m1.1d">2 × 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT</annotation></semantics></math> </p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1">Epochs: 3</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p" id="A2.I1.i3.p1.1">워밍업: 전체 훈련 시간의 처음 3% 동안 선형 워밍업을 수행한 다음 나머지 단계에 걸쳐 학습률 0으로 선형 쿨다운한다.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i4.p1">
<p class="ltx_p" id="A2.I1.i4.p1.1">Weight Decay: 0</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i5.p1">
<p class="ltx_p" id="A2.I1.i5.p1.1">구배 클리핑: 0</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i6.p1">
<p class="ltx_p" id="A2.I1.i6.p1.1">최대 시퀀스 길이: 2048</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">Instruction finetuning 후, 다음 <cite class="ltx_cite ltx_citemacro_citet">Ivison et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>]</cite> 다음에 다음 하이퍼파라미터를 DPO 트레이닝에 사용한다:</p>
</div>
<div class="ltx_para" id="A2.p4">
<ul class="ltx_itemize" id="A2.I2">
<li class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i1.p1">
<p class="ltx_p" id="A2.I2.i1.p1.1">Learning Rate: <math alttext="5\times 10^{-7}" class="ltx_Math" display="inline" id="A2.I2.i1.p1.1.m1.1"><semantics id="A2.I2.i1.p1.1.m1.1a"><mrow id="A2.I2.i1.p1.1.m1.1.1" xref="A2.I2.i1.p1.1.m1.1.1.cmml"><mn id="A2.I2.i1.p1.1.m1.1.1.2" xref="A2.I2.i1.p1.1.m1.1.1.2.cmml">5</mn><mo id="A2.I2.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A2.I2.i1.p1.1.m1.1.1.1.cmml">×</mo><msup id="A2.I2.i1.p1.1.m1.1.1.3" xref="A2.I2.i1.p1.1.m1.1.1.3.cmml"><mn id="A2.I2.i1.p1.1.m1.1.1.3.2" xref="A2.I2.i1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A2.I2.i1.p1.1.m1.1.1.3.3" xref="A2.I2.i1.p1.1.m1.1.1.3.3.cmml"><mo id="A2.I2.i1.p1.1.m1.1.1.3.3a" xref="A2.I2.i1.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="A2.I2.i1.p1.1.m1.1.1.3.3.2" xref="A2.I2.i1.p1.1.m1.1.1.3.3.2.cmml">7</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.I2.i1.p1.1.m1.1b"><apply id="A2.I2.i1.p1.1.m1.1.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1"><times id="A2.I2.i1.p1.1.m1.1.1.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1.1"></times><cn id="A2.I2.i1.p1.1.m1.1.1.2.cmml" type="integer" xref="A2.I2.i1.p1.1.m1.1.1.2">5</cn><apply id="A2.I2.i1.p1.1.m1.1.1.3.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.I2.i1.p1.1.m1.1.1.3.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3">superscript</csymbol><cn id="A2.I2.i1.p1.1.m1.1.1.3.2.cmml" type="integer" xref="A2.I2.i1.p1.1.m1.1.1.3.2">10</cn><apply id="A2.I2.i1.p1.1.m1.1.1.3.3.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3.3"><minus id="A2.I2.i1.p1.1.m1.1.1.3.3.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3.3"></minus><cn id="A2.I2.i1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A2.I2.i1.p1.1.m1.1.1.3.3.2">7</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I2.i1.p1.1.m1.1c">5\times 10^{-7}</annotation><annotation encoding="application/x-llamapun" id="A2.I2.i1.p1.1.m1.1d">5 × 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i2.p1">
<p class="ltx_p" id="A2.I2.i2.p1.1"><math alttext="\beta" class="ltx_Math" display="inline" id="A2.I2.i2.p1.1.m1.1"><semantics id="A2.I2.i2.p1.1.m1.1a"><mi id="A2.I2.i2.p1.1.m1.1.1" xref="A2.I2.i2.p1.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A2.I2.i2.p1.1.m1.1b"><ci id="A2.I2.i2.p1.1.m1.1.1.cmml" xref="A2.I2.i2.p1.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.I2.i2.p1.1.m1.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="A2.I2.i2.p1.1.m1.1d">italic_β</annotation></semantics></math>: 0.1</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i3.p1">
<p class="ltx_p" id="A2.I2.i3.p1.1">Epochs: 3</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i4.p1">
<p class="ltx_p" id="A2.I2.i4.p1.1">워밍업: 전체 훈련 시간의 처음 10% 동안 선형 워밍업을 수행한 다음, 나머지 단계에 걸쳐 0의 학습률로 선형 쿨다운한다.</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i5.p1">
<p class="ltx_p" id="A2.I2.i5.p1.1">Weight Decay: 0</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i6.p1">
<p class="ltx_p" id="A2.I2.i6.p1.1">구배 클리핑: 0</p>
</div>
</li>
<li class="ltx_item" id="A2.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I2.i7.p1">
<p class="ltx_p" id="A2.I2.i7.p1.1">최대 시퀀스 길이: 2048</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Adaptation Evaluation and Model details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">우리는 표<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T6" title="Table 6 ‣ Setup ‣ 4.1 Downstream evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>에서 비교한 기본 모델의 '정규' 최상의 버전(즉, 동일한 조직에서 릴리스한 최상의 명령 조정 또는 그렇지 않으면 적응된 모델)을 선택하여 표<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T7" title="Table 7 ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>의 모델을 선택한다. 또한 <span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.1">Tülu</span> 2와 비교하여 Finetune OLMo에 사용된 <span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.2">Tülu</span> mix를 사용하여 훈련된 현재 최상의 모델을 보여준다. MLU, AlpacaEval, ToxiGen 및 Truthfulness에 대한 평가를 표시하여 명령어 튜닝이 일반적으로 기능(MMLU), 모델이 개방형 채팅 설정(AlpacaEval)에서 수행하는 방법 및 명령어 튜닝이 모델 안전성과 진실성(AlpacaEval, ToxiGen)에 도움이 되는 방법을 표시하는 데 중점을 둡니다. 우리는 또한 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#A3.T10" title="Table 10 ‣ Appendix C Adaptation Evaluation and Model details ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">10</span></a>의 <span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.3">Tülu</span> 평가 제품군에 대한 OLMo의 성능을 보고한다.</p>
</div>
<figure class="ltx_table" id="A3.T10">
<p class="ltx_p" id="A3.T10.1"><span class="ltx_text" id="A3.T10.1.1"> <span class="ltx_inline-block ltx_transformed_outer" id="A3.T10.1.1.1" style="width:431.3pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span class="ltx_p" id="A3.T10.1.1.1.1"><span class="ltx_text" id="A3.T10.1.1.1.1.1" style="color:#000000;"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T10.1.1.1.1.1.1"> <span class="ltx_thead"> <span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.1.1">Model</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.2.1">MMLU</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.3.1">GSM8k</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.4.1">BBH</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.5.1">TydiQA</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.6.1">Codex-Eval</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.7.1">AlpacaEval</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.8.1">ToxiGen</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1.1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.1.1.9.1">TruthfulQA</span></span></span> <span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.2.2"> <span class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="A3.T10.1.1.1.1.1.1.2.2.1"></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.2.1">0-shot</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.3.1">8-shot CoT</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.4.1">3-shot CoT</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.5.1">1-shot</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.6.1">Pass@10</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.7"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.7.1">%win</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.8"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.8.1">% Toxic</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A3.T10.1.1.1.1.1.1.2.2.9"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.2.2.9.1">% Info + True</span></span></span> </span> <span class="ltx_tbody"> <span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.3.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.3.1.1.1">OLMo-7B</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.2">28.3</span> <span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.3">8.5</span> <span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.4">31.7</span> <span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.5">32.3</span> <span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.6">21.4</span> <span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.7">-</span> <span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.8">81.4</span> <span class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.1.1.1.1.1.1.3.1.9">31.6</span></span> <span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.4.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T10.1.1.1.1.1.1.4.2.1"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.4.2.1.1">+SFT</span></span> <span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.2">47.3</span> <span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.3">15.5</span> <span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.4">36.9</span> <span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.5">35.2</span> <span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.6">28.6</span> <span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.7">57.0</span> <span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.8">14.4</span> <span class="ltx_td ltx_align_center" id="A3.T10.1.1.1.1.1.1.4.2.9">41.2</span></span> <span class="ltx_tr" id="A3.T10.1.1.1.1.1.1.5.3"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A3.T10.1.1.1.1.1.1.5.3.1"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1.1.1.5.3.1.1">+SFT+DPO</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.2">46.1</span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.3">11.0</span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.4">35.8</span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.5">21.7</span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.6">27.8</span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.7">69.3</span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.8">1.7</span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.1.1.1.1.1.1.5.3.9">52.0</span></span> </span> </span></span></span> </span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">표 10:</span>Evaluation of OLMo-7B models before and after instruction finetuning and DPO training on the full <span class="ltx_text ltx_font_smallcaps" id="A3.T10.3.1">Tülu</span> evaluation suite. 낮은 것은 ToxiGen에 더 좋고 높은 것은 다른 지표에 더 좋다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">아래 표 <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T7" title="Table 7 ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>에서 평가된 각 모델에 대한 간략한 설명을 제공한다. 모든 모델에 대해 사용 가능한 경우 제공된 채팅 템플릿을 사용하여 프롬프트 형식을 지정합니다.</p>
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1">MPT Chat: A version of MPT 7B finetuned on the ShareGPT-Vicuna <cite class="ltx_cite ltx_citemacro_citep">[Chiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib13" title="">2023</a>]</cite>, HC3 <cite class="ltx_cite ltx_citemacro_citep">[Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib29" title="">2023</a>]</cite>, Alpaca <cite class="ltx_cite ltx_citemacro_citep">[Taori et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib74" title="">2023</a>]</cite>, HH-RLHF <cite class="ltx_cite ltx_citemacro_citep">[Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib5" title="">2022</a>]</cite>, and Evol-Instruct <cite class="ltx_cite ltx_citemacro_citep">[Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib87" title="">2024</a>]</cite> datasets. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/mosaicml/mpt-7b-chat" title="">https://huggingface.co/mosaicml/mpt-7b-chat</a>에서 검색되었습니다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p" id="A3.I1.i2.p1.1">Falcon Instruct: A version of Falcon 7B finetuned on the Baize <cite class="ltx_cite ltx_citemacro_citep">[Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib88" title="">2023</a>]</cite>, GPT4All <cite class="ltx_cite ltx_citemacro_citep">[Anand et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib3" title="">2023</a>]</cite>, GPTeacher <cite class="ltx_cite ltx_citemacro_citep">[Teknium1, <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib75" title="">2023</a>]</cite>, and Refined-Web English <cite class="ltx_cite ltx_citemacro_citep">[Penedo et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib57" title="">2023</a>]</cite> datasets. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/tiiuae/falcon-7b-instruct" title="">https://huggingface.co/tiiuae/falcon-7b-instruct</a>에서 검색되었습니다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i3.p1">
<p class="ltx_p" id="A3.I1.i3.p1.1">RPJ-INCITE Chat: A version of RPJ-INCITE 7B finetuned on the OASST1 <cite class="ltx_cite ltx_citemacro_citep">[Köpf et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib35" title="">2023</a>]</cite> and Dolly V2 <cite class="ltx_cite ltx_citemacro_citep">[Conover et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib19" title="">2023</a>]</cite> datasets. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat" title="">https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat</a>에서 검색되었습니다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i4.p1">
<p class="ltx_p" id="A3.I1.i4.p1.1">Llama-2 Chat: Llama 2 7B의 버전은 명령어 데이터세트의 혼합물 상에서 미세조정되고 RLHF로 추가로 트레이닝된다. 자세한 내용은 판독기를 <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>]</cite>에 참조합니다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i5.p1">
<p class="ltx_p" id="A3.I1.i5.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A3.I1.i5.p1.1.1">Tülu</span> 2: A version of Llama 2 7B finetuned on the mixture of instruction datasets (the <span class="ltx_text ltx_font_smallcaps" id="A3.I1.i5.p1.1.2">Tülu</span> 2 mix). 자세한 내용은 판독기를 <cite class="ltx_cite ltx_citemacro_citet">Ivison et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>]</cite>에 참조합니다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i6.p1">
<p class="ltx_p" id="A3.I1.i6.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A3.I1.i6.p1.1.1">Tülu</span> 2+DPO: <span class="ltx_text ltx_font_smallcaps" id="A3.I1.i6.p1.1.2">Tülu</span> 2 further trained with DPO on the UltraFeedback dataset <cite class="ltx_cite ltx_citemacro_citep">[Cui et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib20" title="">2023</a>]</cite>. 자세한 내용은 판독기를 <cite class="ltx_cite ltx_citemacro_citet">Ivison et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib33" title="">2023</a>]</cite>에 참조합니다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i7.p1">
<p class="ltx_p" id="A3.I1.i7.p1.1">OLMo +SFT: A version of OLMo 7B fintuned on the same data as <span class="ltx_text ltx_font_smallcaps" id="A3.I1.i7.p1.1.1">Tülu</span> 2.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i8.p1">
<p class="ltx_p" id="A3.I1.i8.p1.1">OLMo +SFT+DPO: OLMo +SFT는 UltraFeedback 데이터셋 <cite class="ltx_cite ltx_citemacro_citep">[Cui et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib20" title="">2023</a>]</cite>에서 DPO로 추가 훈련되었다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p" id="A3.p3.1">우리는 추가로 Table <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#S4.T7" title="Table 7 ‣ 4.3 Adaptation Evaluation ‣ 4 Results ‣ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>로부터 각각의 평가 설정에 대한 간략한 설명을 제공한다:</p>
<ul class="ltx_itemize" id="A3.I2">
<li class="ltx_item" id="A3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i1.p1">
<p class="ltx_p" id="A3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i1.p1.1.1">MMLU</span>: 우리는 공식 MMLU <cite class="ltx_cite ltx_citemacro_citep">[Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib32" title="">2021</a>]</cite> 평가 스크립트 및 프롬프트를 사용하여 <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hendrycks/test" title="">https://github.com/hendrycks/test</a>에서 사용 가능한 프롬프트를 수정하여 일괄 처리를 허용합니다. 우리는 MMLU의 원래 설정에 따라 0개의 소샷 예제를 사용하여 평가한다. 우리는 테스트 예제에 걸친 평균 정확도를 보고한다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i2.p1">
<p class="ltx_p" id="A3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i2.p1.1.1">ToxiGen</span>: <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>]</cite>에서 설정을 따르지만 특정 그룹에 대한 독성 생성을 유도하도록 설계된 <cite class="ltx_cite ltx_citemacro_citet">Hartvigsen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib31" title="">2022</a>]</cite>의 프롬프트 원본 세트를 사용합니다. 우리는 독성 언어를 생성하도록 설계된 프롬프트('혐오스러운' 프롬프트)만 취하고 평가 비용을 줄이기 위해 그룹당 500개의 프롬프트를 사용한다. 기본 언어 모델의 경우 변경되지 않은 원래 톡시젠 프롬프트를 전달하고 첫 번째 새 라인(또는 최대 512 토큰)까지 탐욕스럽게 디코딩합니다. 명령어 조정 모델의 경우 해당 템플릿에 프롬프트를 배치하고 모델이 중지 토큰(또는 최대 512 토큰)을 생성할 때까지 모델에 프롬프트를 완료하도록 요청합니다. 생성된 텍스트를 <cite class="ltx_cite ltx_citemacro_citet">Hartvigsen et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib31" title="">2022</a>]</cite><span class="ltx_note ltx_role_footnote" id="footnote28"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/tomh/toxigen_roberta" title="">https://huggingface.co/tomh/toxigen_roberta</a></span></span></span>의 일부로 미세 조정된 독성 콘텐츠를 탐지하도록 훈련된 로버타 대 모델에 전달한다. 그런 다음 분류기에 의해 독성이 있다고 간주되는 세대의 비율을 보고한다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i3.p1">
<p class="ltx_p" id="A3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i3.p1.1.1">TruthfulQA</span>: Following <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>]</cite>는 주로 TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">[Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib38" title="">2022</a>]</cite>의 생성 설정을 사용한다. TruthfulQA 데이터 세트에는 818개의 질문이 포함되어 있으며, 이는 테스트된 모델을 프롬프트하여 답변을 생성하는 데 사용됩니다. 우리는 6개의 문맥 내 QA 예제와 함께 기본 QA 프롬프트 형식을 사용한다. 우리는 그리디 디코딩을 수행하고 후처리 작업을 수행하기 위해 공식 구현 <span class="ltx_note ltx_role_footnote" id="footnote29"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><span class="ltx_tag ltx_tag_note">29</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sylinrl/TruthfulQA/" title="">https://github.com/sylinrl/TruthfulQA/</a></span></span></span>에서 공식 스크립트를 따릅니다. 우리는 원래 TruthfulQA 평가를 정확하게 복제할 수 없는 GPT-3의 비활성화로 인해 모델 반응의 진실성과 정보성을 판단하기 위해 두 개의 LLaMA 2 기반 분류기를 훈련한다. 우리는 LLaMA 2 판사가 일반적으로 <cite class="ltx_cite ltx_citemacro_citet">Lin et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib38" title="">2022</a>]</cite>에서 사용하는 원래 GPT-3 기반 판사의 성능과 일치할 수 있음을 발견했다. <cite class="ltx_cite ltx_citemacro_citet">Touvron et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib78" title="">2023b</a>]</cite>에 이어 진실하고 유익한(% Informative and Truthful) 응답의 비율을 보고한다. 우리는 % 정보 및 진실만을 기본 메트릭으로 보고합니다.</p>
</div>
</li>
<li class="ltx_item" id="A3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I2.i4.p1">
<p class="ltx_p" id="A3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i4.p1.1.1">AlpacaEval</span>: 평가된 모델에 805 프롬프트에 대한 응답을 생성하고 GPT-4를 사용하여 응답을 <span class="ltx_text" id="A3.I2.i4.p1.1.2">Davinci</span>-<span class="ltx_text" id="A3.I2.i4.p1.1.3">003</span>과 비교하는 기본 설정에 따라 <cite class="ltx_cite ltx_citemacro_citet">Li et al. [<a class="ltx_ref" href="https://arxiv.org/html/2402.00838v3#bib.bib36" title="">2023</a>]</cite>에서 제공하는 패키지를 사용합니다. 우리는 "alpaca_eval_gpt4" 주석기를 사용한다. 우리는 평가된 모델이 특별한 정지 시퀀스를 지정하지 않고 최대 2048개의 토큰을 생성할 수 있도록 한다. 보고된 승률은 GPT-4가 <span class="ltx_text" id="A3.I2.i4.p1.1.4">Davinci</span>-<span class="ltx_text" id="A3.I2.i4.p1.1.5">003</span>의 세대에 걸쳐 선호되는 것으로 보고하는 모델 세대의 백분율입니다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="A3.p4">

</div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
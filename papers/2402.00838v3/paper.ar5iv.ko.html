<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.00838] OLMo : Accelerating the Science of Language Models</title><meta property="og:description" content="Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings.
As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietarâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="OLMo : Accelerating the Science of Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="OLMo : Accelerating the Science of Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.00838">

<!--Generated on Tue Mar  5 18:56:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span id="id1.1" class="ltx_text" style="position:relative; bottom:-3.0pt;"><span id="id1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;position:relative; bottom:3.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="id1.1.1.1" class="ltx_text" style="font-size:70%;">OLMo</span></span><img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x1.png" id="id1.1.g1" class="ltx_graphics ltx_img_landscape" width="30" height="16" alt="[Uncaptioned image]"></span>&nbsp;: Accelerating the Science of Language Models
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id3.2.2" class="ltx_text ltx_font_bold">Dirk Groeneveld

<sup id="id3.2.2.1" class="ltx_sup"><span id="id3.2.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Iz Beltagy

<sup id="id3.2.2.2" class="ltx_sup"><span id="id3.2.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>

</span> 
<br class="ltx_break"><span id="id7.6.6" class="ltx_text ltx_font_bold">Pete Walsh

<sup id="id7.6.6.1" class="ltx_sup"><span id="id7.6.6.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Akshita Bhagia

<sup id="id7.6.6.2" class="ltx_sup"><span id="id7.6.6.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Rodney Kinney

<sup id="id7.6.6.3" class="ltx_sup"><span id="id7.6.6.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Oyvind Tafjord

<sup id="id7.6.6.4" class="ltx_sup"><span id="id7.6.6.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>

</span> 
<br class="ltx_break"><span id="id13.12.12" class="ltx_text ltx_font_bold">Ananya Harsh Jha

<sup id="id13.12.12.1" class="ltx_sup"><span id="id13.12.12.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Hamish Ivison

<sup id="id13.12.12.2" class="ltx_sup"><span id="id13.12.12.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup><sup id="id13.12.12.3" class="ltx_sup"><span id="id13.12.12.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î²</span></sup>
â€‚Ian Magnusson

<sup id="id13.12.12.4" class="ltx_sup"><span id="id13.12.12.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Yizhong Wang

<sup id="id13.12.12.5" class="ltx_sup"><span id="id13.12.12.5.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup><sup id="id13.12.12.6" class="ltx_sup"><span id="id13.12.12.6.1" class="ltx_text ltx_font_medium ltx_font_italic">Î²</span></sup>

</span> 
<br class="ltx_break"><span id="id17.16.16" class="ltx_text ltx_font_bold">Shane Arora

<sup id="id17.16.16.1" class="ltx_sup"><span id="id17.16.16.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚David Atkinson

<sup id="id17.16.16.2" class="ltx_sup"><span id="id17.16.16.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Russell Authur

<sup id="id17.16.16.3" class="ltx_sup"><span id="id17.16.16.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Khyathi Raghavi Chandu

<sup id="id17.16.16.4" class="ltx_sup"><span id="id17.16.16.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>

</span> 
<br class="ltx_break"><span id="id23.22.22" class="ltx_text ltx_font_bold">Arman Cohan

<sup id="id23.22.22.1" class="ltx_sup"><span id="id23.22.22.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î³</span></sup><sup id="id23.22.22.2" class="ltx_sup"><span id="id23.22.22.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Jennifer Dumas

<sup id="id23.22.22.3" class="ltx_sup"><span id="id23.22.22.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Yanai Elazar

<sup id="id23.22.22.4" class="ltx_sup"><span id="id23.22.22.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup><sup id="id23.22.22.5" class="ltx_sup"><span id="id23.22.22.5.1" class="ltx_text ltx_font_medium ltx_font_italic">Î²</span></sup>
â€‚Yuling Gu

<sup id="id23.22.22.6" class="ltx_sup"><span id="id23.22.22.6.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚
</span> 
<br class="ltx_break"><span id="id27.26.26" class="ltx_text ltx_font_bold">Jack Hessel

<sup id="id27.26.26.1" class="ltx_sup"><span id="id27.26.26.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Tushar Khot

<sup id="id27.26.26.2" class="ltx_sup"><span id="id27.26.26.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚William Merrill

<sup id="id27.26.26.3" class="ltx_sup"><span id="id27.26.26.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î´</span></sup>
â€‚Jacob Morrison

<sup id="id27.26.26.4" class="ltx_sup"><span id="id27.26.26.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>

</span> 
<br class="ltx_break"><span id="id31.30.30" class="ltx_text ltx_font_bold">
Niklas Muennighoff


â€‚Aakanksha Naik

<sup id="id31.30.30.1" class="ltx_sup"><span id="id31.30.30.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Crystal Nam

<sup id="id31.30.30.2" class="ltx_sup"><span id="id31.30.30.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Matthew E. Peters

<sup id="id31.30.30.3" class="ltx_sup"><span id="id31.30.30.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>

</span> 
<br class="ltx_break"><span id="id36.35.35" class="ltx_text ltx_font_bold">Valentina Pyatkin

<sup id="id36.35.35.1" class="ltx_sup"><span id="id36.35.35.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup><sup id="id36.35.35.2" class="ltx_sup"><span id="id36.35.35.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î²</span></sup>
â€‚Abhilasha Ravichander

<sup id="id36.35.35.3" class="ltx_sup"><span id="id36.35.35.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Dustin Schwenk

<sup id="id36.35.35.4" class="ltx_sup"><span id="id36.35.35.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Saurabh Shah

<sup id="id36.35.35.5" class="ltx_sup"><span id="id36.35.35.5.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚
</span> 
<br class="ltx_break"><span id="id41.40.40" class="ltx_text ltx_font_bold">Will Smith

<sup id="id41.40.40.1" class="ltx_sup"><span id="id41.40.40.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Emma Strubell

<sup id="id41.40.40.2" class="ltx_sup"><span id="id41.40.40.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup><sup id="id41.40.40.3" class="ltx_sup"><span id="id41.40.40.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î¼</span></sup>
â€‚Nishant Subramani

<sup id="id41.40.40.4" class="ltx_sup"><span id="id41.40.40.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Mitchell Wortsman

<sup id="id41.40.40.5" class="ltx_sup"><span id="id41.40.40.5.1" class="ltx_text ltx_font_medium ltx_font_italic">Î²</span></sup>

</span> 
<br class="ltx_break"><span id="id44.43.43" class="ltx_text ltx_font_bold">Pradeep Dasigi

<sup id="id44.43.43.1" class="ltx_sup"><span id="id44.43.43.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Nathan Lambert

<sup id="id44.43.43.2" class="ltx_sup"><span id="id44.43.43.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Kyle Richardson

<sup id="id44.43.43.3" class="ltx_sup"><span id="id44.43.43.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>

</span> 
<br class="ltx_break"><span id="id48.47.47" class="ltx_text ltx_font_bold">Luke Zettlemoyer

<sup id="id48.47.47.1" class="ltx_sup"><span id="id48.47.47.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î²</span></sup>
Jesse Dodge

<sup id="id48.47.47.2" class="ltx_sup"><span id="id48.47.47.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Kyle Lo

<sup id="id48.47.47.3" class="ltx_sup"><span id="id48.47.47.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>
â€‚Luca Soldaini

<sup id="id48.47.47.4" class="ltx_sup"><span id="id48.47.47.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup>

</span> 
<br class="ltx_break"><span id="id52.51.51" class="ltx_text ltx_font_bold">Noah A. Smith

<sup id="id52.51.51.1" class="ltx_sup"><span id="id52.51.51.1.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup><sup id="id52.51.51.2" class="ltx_sup"><span id="id52.51.51.2.1" class="ltx_text ltx_font_medium ltx_font_italic">Î²</span></sup>
â€‚Hannaneh Hajishirzi

<sup id="id52.51.51.3" class="ltx_sup"><span id="id52.51.51.3.1" class="ltx_text ltx_font_medium ltx_font_italic">Î±</span></sup><sup id="id52.51.51.4" class="ltx_sup"><span id="id52.51.51.4.1" class="ltx_text ltx_font_medium ltx_font_italic">Î²</span></sup>

</span> 
<br class="ltx_break">
<sup id="id64.57.id1" class="ltx_sup"><span id="id64.57.id1.1" class="ltx_text ltx_font_italic">Î±</span></sup>Allen Institute for Artificial Intelligence
 
<br class="ltx_break">
<sup id="id65.58.id2" class="ltx_sup"><span id="id65.58.id2.1" class="ltx_text ltx_font_italic">Î²</span></sup>University of Washington â€ƒ<sup id="id66.59.id3" class="ltx_sup"><span id="id66.59.id3.1" class="ltx_text ltx_font_italic">Î³</span></sup>Yale University
 
<br class="ltx_break">
<sup id="id67.60.id4" class="ltx_sup"><span id="id67.60.id4.1" class="ltx_text ltx_font_italic">Î´</span></sup>New York University â€ƒ<sup id="id68.61.id5" class="ltx_sup"><span id="id68.61.id5.1" class="ltx_text ltx_font_italic">Î¼</span></sup>Carnegie Mellon University

<br class="ltx_break"><span id="id69.62.id6" class="ltx_text ltx_font_typewriter">olmo@allenai.org</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id70.id1">ì–¸ì–´ ëª¨ë¸(LMs)ì€ NLP ì—°êµ¬ì™€ ìƒì—…ìš© ì œí’ˆ ì œê³µ ëª¨ë‘ì—ì„œ ìœ ë¹„ì¿¼í„°ìŠ¤í™”ë˜ì—ˆë‹¤. ìƒì—…ì  ì¤‘ìš”ì„±ì´ ê¸‰ì¦í•¨ì— ë”°ë¼ ê°€ì¥ ê°•ë ¥í•œ ëª¨ë¸ì€ ë…ì  ì¸í„°í˜ì´ìŠ¤ ë’¤ì—ì„œ íì‡„ë˜ê³ , í›ˆë ¨ ë°ì´í„°, ì•„í‚¤í…ì²˜ ë° ê°œë°œì˜ ì¤‘ìš”í•œ ì„¸ë¶€ ì •ë³´ê°€ ê³µê°œë˜ì§€ ì•Šì•˜ë‹¤. í¸í–¥ê³¼ ì ì¬ì  ìœ„í—˜ì„ í¬í•¨í•˜ì—¬ ì´ëŸ¬í•œ ëª¨ë¸ì„ ê³¼í•™ì ìœ¼ë¡œ ì—°êµ¬í•˜ëŠ” ë° ì´ëŸ¬í•œ ì„¸ë¶€ ì‚¬í•­ì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ì ì„ ê°ì•ˆí•  ë•Œ ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ê°€ ê°•ë ¥í•˜ê³  ì§„ì •ìœ¼ë¡œ ì—´ë¦° LMsì— ì ‘ê·¼í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì´ë¼ê³  ë¯¿ëŠ”ë‹¤. ì´ë¥¼ ìœ„í•´ ì´ ê¸°ìˆ  ë³´ê³ ì„œëŠ” OLMoì˜ ì²« ë²ˆì§¸ ë¦´ë¦¬ìŠ¤ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•˜ë©°, ì§„ì •í•œ <span class="ltx_text ltx_font_bold" id="id70.id1.1">O</span>pen <span class="ltx_text ltx_font_bold" id="id70.id1.2">L</span>anguage <span class="ltx_text ltx_font_bold" id="id70.id1.3">Mo</span>del ë° ê·¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì–¸ì–´ ëª¨ë¸ë§ ê³¼í•™ì„ ë¹Œë“œí•˜ê³  ì—°êµ¬í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ì¶”ë¡  ì½”ë“œë§Œì„ ë°œí‘œí•˜ë˜ ê¸°ì¡´ ì—°êµ¬ë“¤ê³¼ëŠ” ë‹¬ë¦¬ OLMoì™€ í•™ìŠµ ë°ì´í„°, í•™ìŠµ ë° í‰ê°€ ì½”ë“œë¥¼ í¬í•¨í•œ ì „ì²´ í”„ë ˆì„ì›Œí¬ë¥¼ ë°œí‘œí•œë‹¤. ì´ë²ˆ ë°œí‘œê°€ ì—´ë¦° ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì— í˜ì„ ì‹¤ì–´ì£¼ê³  ê°•í™”í•˜ë©° ìƒˆë¡œìš´ í˜ì‹  ë¬¼ê²°ì„ ë¶ˆëŸ¬ì¼ìœ¼í‚¤ê¸°ë¥¼ ë°”ëë‹ˆë‹¤.</p>
<table id="id63.6.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="id58.1.1.1" class="ltx_tr">
<td id="id58.1.1.1.1" class="ltx_td ltx_align_right" style="padding-top:1pt;padding-bottom:1pt;"><span id="id58.1.1.1.1.1" class="ltx_text" style="position:relative; bottom:-1.5pt;"><img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x2.png" id="id58.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="12" height="11" alt="[Uncaptioned image]"></span></td>
<td id="id58.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="id58.1.1.1.2.1" class="ltx_text ltx_font_bold">Weights</span></td>
<td id="id58.1.1.1.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><a target="_blank" href="https://huggingface.co/allenai/OLMo-7B" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/allenai/OLMo-7B</a></td>
</tr>
<tr id="id59.2.2.2" class="ltx_tr">
<td id="id59.2.2.2.1" class="ltx_td ltx_align_right" style="padding-top:1pt;padding-bottom:1pt;"><span id="id59.2.2.2.1.1" class="ltx_text" style="position:relative; bottom:-1.5pt;"><img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x3.png" id="id59.2.2.2.1.1.g1" class="ltx_graphics ltx_img_square" width="11" height="11" alt="[Uncaptioned image]"></span></td>
<td id="id59.2.2.2.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="id59.2.2.2.2.1" class="ltx_text ltx_font_bold">Code</span></td>
<td id="id59.2.2.2.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><a target="_blank" href="https://github.com/allenai/OLMo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/allenai/OLMo</a></td>
</tr>
<tr id="id60.3.3.3" class="ltx_tr">
<td id="id60.3.3.3.1" class="ltx_td ltx_align_right" style="padding-top:1pt;padding-bottom:1pt;"><span id="id60.3.3.3.1.1" class="ltx_text" style="position:relative; bottom:-1.5pt;"><img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x4.png" id="id60.3.3.3.1.1.g1" class="ltx_graphics ltx_img_square" width="12" height="11" alt="[Uncaptioned image]"></span></td>
<td id="id60.3.3.3.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="id60.3.3.3.2.1" class="ltx_text ltx_font_bold">Data</span></td>
<td id="id60.3.3.3.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><a target="_blank" href="https://huggingface.co/datasets/allenai/dolma" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/allenai/dolma</a></td>
</tr>
<tr id="id61.4.4.4" class="ltx_tr">
<td id="id61.4.4.4.1" class="ltx_td ltx_align_right" style="padding-top:1pt;padding-bottom:1pt;"><span id="id61.4.4.4.1.1" class="ltx_text" style="position:relative; bottom:-1.5pt;"><img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x5.png" id="id61.4.4.4.1.1.g1" class="ltx_graphics ltx_img_square" width="11" height="11" alt="[Uncaptioned image]"></span></td>
<td id="id61.4.4.4.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="id61.4.4.4.2.1" class="ltx_text ltx_font_bold">Evaluation</span></td>
<td id="id61.4.4.4.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><a target="_blank" href="https://github.com/allenai/OLMo-Eval" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/allenai/OLMo-Eval</a></td>
</tr>
<tr id="id62.5.5.5" class="ltx_tr">
<td id="id62.5.5.5.1" class="ltx_td ltx_align_right" style="padding-top:1pt;padding-bottom:1pt;"><span id="id62.5.5.5.1.1" class="ltx_text" style="position:relative; bottom:-1.5pt;"><img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x6.png" id="id62.5.5.5.1.1.g1" class="ltx_graphics ltx_img_square" width="11" height="11" alt="[Uncaptioned image]"></span></td>
<td id="id62.5.5.5.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="id62.5.5.5.2.1" class="ltx_text ltx_font_bold">Adaptation</span></td>
<td id="id62.5.5.5.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><a target="_blank" href="https://github.com/allenai/open-instruct" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/allenai/open-instruct</a></td>
</tr>
<tr id="id63.6.6.6" class="ltx_tr">
<td id="id63.6.6.6.1" class="ltx_td ltx_align_right" style="padding-top:1pt;padding-bottom:1pt;"><span id="id63.6.6.6.1.1" class="ltx_text" style="position:relative; bottom:-1.5pt;"><img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/figures/wandb-logo.png" id="id63.6.6.6.1.1.g1" class="ltx_graphics ltx_img_square" width="16" height="15" alt="[Uncaptioned image]"></span></td>
<td id="id63.6.6.6.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="id63.6.6.6.2.1" class="ltx_text ltx_font_bold">W&amp;B Logs</span></td>
<td id="id63.6.6.6.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;"><a target="_blank" href="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5</a></td>
</tr>
</tbody>
</table>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">ì–¸ì–´ ëª¨ë¸ì€ ìˆ˜ë…„ ë™ì•ˆ NLP ê¸°ìˆ ì˜ ì¤‘ì‹¬ì— ìˆì—ˆë‹¤ <cite class="ltx_cite ltx_citemacro_citep">(Rosenfeld, <a class="ltx_ref" href="#bib.bib67" title="">2000</a>; Bengio etÂ al., <a class="ltx_ref" href="#bib.bib6" title="">2003</a>; Mikolov etÂ al., <a class="ltx_ref" href="#bib.bib48" title="">2013</a>; Peters etÂ al., <a class="ltx_ref" href="#bib.bib58" title="">2018</a>; Brown etÂ al., <a class="ltx_ref" href="#bib.bib12" title="">2020</a>)</cite> ìµœê·¼ ëŒ€ê·œëª¨ ì‚¬ì „ í›ˆë ¨ê³¼ ì •ë ¬ì— ëŒ€í•œ ì¸ê°„ ì£¼ì„ìœ¼ë¡œ ì¸í•´ ìƒì—…ì ìœ¼ë¡œ ê°€ì¹˜ê°€ ìˆëŠ” <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib53" title="">2023</a>)</cite>ê°€ ë˜ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìƒì—…ì  ê°€ì¹˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ê°€ì¥ í° ëª¨ë¸ì€ ë…ì  ì¸í„°í˜ì´ìŠ¤ ë’¤ì— ë¬¸ì´ ë‹«í˜”ìœ¼ë©° ì¤‘ìš”í•œ ì„¸ë¶€ ì‚¬í•­ì€ ê³µê°œë˜ì§€ ì•Šì•˜ë‹¤.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">ìš°ë¦¬ëŠ” ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ìœ„í•œ ê°œë°©í˜• ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ ì™„ì „í•œ ì•¡ì„¸ìŠ¤ê°€ ì´ëŸ¬í•œ ëª¨ë¸ì˜ ê°•ì ê³¼ ì•½ì , í¸í–¥ê³¼ ìœ„í—˜ì— ëŒ€í•œ ê³¼í•™ì  ì—°êµ¬ì— ì¤‘ìš”í•˜ë‹¤ê³  ë¯¿ëŠ”ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” í•™ìŠµ ë°ì´í„°, í›ˆë ¨ ë° í‰ê°€ ì½”ë“œ, ì¤‘ê°„ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë° í›ˆë ¨ ë¡œê·¸ì™€ í•¨ê»˜ LMì„ êµ¬ì¶•, ì—°êµ¬ ë° ë°œì „ì‹œí‚¤ê¸° ìœ„í•œ ìµœì²¨ë‹¨ ê°œë°©í˜• ì–¸ì–´ ëª¨ë¸ ë° í”„ë ˆì„ì›Œí¬ì¸ <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">OLMo</span>ì„ ì†Œê°œí•œë‹¤.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">ìµœê·¼ LM ë¦´ë¦¬ì¦ˆëŠ” ê°œë°©ë„ê°€ ë‹¤ì–‘í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Mistral 8x7BëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ê°„ëµ ë³´ê³ ì„œ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="#bib.bib34" title="">2024</a>)</cite>ë¥¼ ì œê³µí–ˆê³ , LLaMAëŠ” ì‹¬ì¸µ ì ì‘ í›ˆë ¨ ì§€ì¹¨ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="#bib.bib78" title="">2023b</a>)</cite>ë¥¼ ì œê³µí–ˆìœ¼ë©°, ëª¨ìì´í¬ ì‚¬ì „ í›ˆë ¨ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë°ì´í„° ìì²´ <cite class="ltx_cite ltx_citemacro_citep">(MosaicML NLP Team, <a class="ltx_ref" href="#bib.bib50" title="">2023</a>)</cite>ëŠ” ì•„ë‹ˆì§€ë§Œ ë°ì´í„° ì„¸íŠ¸ ë¶„í¬ë¥¼ í¬í•¨í•œ ë§ì€ ì„¸ë¶€ ì‚¬í•­ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. íŒ”ì½˜ì˜ ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ë¦´ë¦¬ìŠ¤ë˜ì—ˆë‹¤. <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei etÂ al., <a class="ltx_ref" href="#bib.bib2" title="">2023</a>)</cite>, ê·¸ë¦¬ê³  ê°€ì¥ ê°œë°©ëœ ëª¨ë¸ì¸ í”¼í‹°ì•„ ì œí’ˆêµ° <cite class="ltx_cite ltx_citemacro_citep">(Biderman etÂ al., <a class="ltx_ref" href="#bib.bib7" title="">2023</a>)</cite>ì™€ BLOOM <cite class="ltx_cite ltx_citemacro_citep">(BigScience etÂ al., <a class="ltx_ref" href="#bib.bib8" title="">2022</a>)</cite>ëŠ” í›ˆë ¨ ì½”ë“œ, ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸, í›ˆë ¨ ë°ì´í„° ë“±ì„ ë¦´ë¦¬ìŠ¤í–ˆë‹¤.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">OLMoë¥¼ ì‚¬ìš©í•˜ì—¬ ìš°ë¦¬ëŠ” í—ˆìš© ë¼ì´ì„ ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ í•˜ë“œì›¨ì–´ ìœ í˜•, í›ˆë ¨ ë¡œê·¸ ë° ì‚¬ìš©ëœ ì •í™•í•œ ë°ì´í„° ì„¸íŠ¸ì— ê±¸ì³ ì—¬ëŸ¬ í›ˆë ¨ ì²´í¬í¬ì¸íŠ¸ì¸ ë°ì´í„°ì—ì„œ í›ˆë ¨ìœ¼ë¡œ í‰ê°€ ë„êµ¬ë¡œ ì „ì²´ í”„ë ˆì„ì›Œí¬ë¥¼ ë¦´ë¦¬ìŠ¤í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ê²ƒì„ í•œ ìœ ì¼í•œ íŒ€ì´ ì•„ë‹ˆë‹¤; LLM360ì˜ ìµœê·¼ ì—°êµ¬ëŠ” ìœ ì‚¬í•œ ëª©í‘œ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="#bib.bib40" title="">2023</a>)</cite>ë¥¼ ëª©í‘œë¡œ í•œë‹¤. OLMoëŠ” ê·¸ë“¤ì˜ ëª¨ë¸ì—ì„œ LLaMA2ì™€ ê°™ì€ ëª¨ë¸ì˜ ìµœì²¨ë‹¨ ëŠ¥ë ¥ìœ¼ë¡œì˜ ê²©ì°¨ë¥¼ ì¢íˆê³  ìˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ì–‘í•œ ê°œë°©ì„±ì˜ ì •ë„ë¡œ ì´ì „ì˜ ëª¨ë“  ë…¸ë ¥ì—ì„œ ì–»ì€ êµí›ˆìœ¼ë¡œë¶€í„° ì´ìµì„ ì–»ì—ˆìœ¼ë©°, ìš°ë¦¬ëŠ” í¬ê³  ë‹¤ì–‘í•œ ê°œë°© ëª¨ë¸ì˜ ëª¨ì§‘ë‹¨ì´ ì–¸ì–´ ëª¨ë¸ì„ ì´í•´í•˜ëŠ” ê³¼í•™ì  ì§„ì „ê³¼ ìœ ìš©ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ê³µí•™ì  ì§„ì „ì— ê°€ì¥ ì¢‹ì€ í¬ë§ì´ë¼ê³  ë¯¿ëŠ”ë‹¤.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">The OLMo framework encompasses the tools and resources required for building and researching language models. For training and modeling, it includes full model weights, training code, training logs, ablations, training metrics in the form of Weights &amp; Biases logs, and inference code. This first release includes four variants of our language model at the 7B scale corresponding to different architectures, optimizers, and training hardware, and one model at the 1B scale, all trained on at least 2T tokens. We are also releasing hundreds of intermediate checkpoints available as revisions on HuggingFace. For dataset building and analysis, it includes the full training data used for these models, including code that produces the training data, from AI2â€™s DolmaÂ <cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2024</a>)</cite>, and WIMBDÂ <cite class="ltx_cite ltx_citemacro_citep">(Elazar etÂ al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite> for analyzing pretraining data. For evaluation, it includes AI2â€™s CatwalkÂ <cite class="ltx_cite ltx_citemacro_citep">(Groeneveld etÂ al., <a class="ltx_ref" href="#bib.bib28" title="">2023</a>)</cite> for downstream evaluation and PalomaÂ <cite class="ltx_cite ltx_citemacro_citep">(Magnusson etÂ al., <a class="ltx_ref" href="#bib.bib43" title="">2023</a>)</cite> for perplexity-based evaluation. For instruction-tuning, we released Open InstructÂ <cite class="ltx_cite ltx_citemacro_citep">(Ivison etÂ al., <a class="ltx_ref" href="#bib.bib33" title="">2023</a>; Wang etÂ al., <a class="ltx_ref" href="#bib.bib83" title="">2023</a>)</cite>, and we are currently using it to produce an adapted (instruction-tuned and RLHFed) version of OLMo, which we will release soon. Finally, all code and weights are released under the Apache 2.0 License.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank" title="">http://www.apache.org/licenses/LICENSE-2.0</a></span></span></span></p>OLMo í”„ë ˆì„ì›Œí¬ëŠ” ì–¸ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  ì—°êµ¬í•˜ëŠ” ë° í•„ìš”í•œ ë„êµ¬ì™€ ìì›ì„ í¬ê´„í•œë‹¤. í›ˆë ¨ ë° ëª¨ë¸ë§ì„ ìœ„í•´ ì „ì²´ ëª¨ë¸ ê°€ì¤‘ì¹˜, í›ˆë ¨ ì½”ë“œ, í›ˆë ¨ ë¡œê·¸, ì‚­ë§ˆ, Weights & Biases ë¡œê·¸ í˜•íƒœì˜ í›ˆë ¨ ë©”íŠ¸ë¦­, ì¶”ë¡  ì½”ë“œë¥¼ í¬í•¨í•œë‹¤. ì´ ì²« ë²ˆì§¸ ë¦´ë¦¬ìŠ¤ì—ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜, ìµœì í™”ê¸° ë° í›ˆë ¨ í•˜ë“œì›¨ì–´ì— í•´ë‹¹í•˜ëŠ” 7B ê·œëª¨ì˜ ì–¸ì–´ ëª¨ë¸ 4ê°œì™€ 1B ê·œëª¨ì˜ ëª¨ë¸ 1ê°œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©° ëª¨ë‘ ìµœì†Œ 2T í† í°ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆë‹¤. íœ´ì§€í˜ì´ìŠ¤ì—ì„œ ìˆ˜ì •í•  ìˆ˜ ìˆëŠ” ìˆ˜ë°± ê°œì˜ ì¤‘ê°„ ê²€ë¬¸ì†Œë„ ì¶œì‹œí•©ë‹ˆë‹¤. ë°ì´í„°ì„¸íŠ¸ êµ¬ì¶• ë° ë¶„ì„ì„ ìœ„í•´ AI2ì˜ Dolma<cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2024</a>)</cite>ì—ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œë¥¼ í¬í•¨í•˜ì—¬ ì´ëŸ¬í•œ ëª¨ë¸ì— ì‚¬ìš©ë˜ëŠ” ì „ì²´ í•™ìŠµ ë°ì´í„°ì™€ ì‚¬ì „ í•™ìŠµ ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ WIMBD<cite class="ltx_cite ltx_citemacro_citep">(Elazar etÂ al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>ë¥¼ í¬í•¨í•œë‹¤. í‰ê°€ë¥¼ ìœ„í•´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í‰ê°€ë¥¼ ìœ„í•´ AI2ì˜ Catwalk <cite class="ltx_cite ltx_citemacro_citep">(Groeneveld etÂ al., <a class="ltx_ref" href="#bib.bib28" title="">2023</a>)</cite>ì™€ í¼í”Œë ‰ì‹œí‹° ê¸°ë°˜ í‰ê°€ë¥¼ ìœ„í•´ Paloma <cite class="ltx_cite ltx_citemacro_citep">(Magnusson etÂ al., <a class="ltx_ref" href="#bib.bib43" title="">2023</a>)</cite>ë¥¼ í¬í•¨í•œë‹¤. ì¸ìŠ¤íŠ¸ëŸ­ì…˜ íŠœë‹ì„ ìœ„í•´ Open Instruct <cite class="ltx_cite ltx_citemacro_citep">(Ivison etÂ al., <a class="ltx_ref" href="#bib.bib33" title="">2023</a>; Wang etÂ al., <a class="ltx_ref" href="#bib.bib83" title="">2023</a>)</cite>ë¥¼ ì¶œì‹œí•˜ì˜€ìœ¼ë©°, í˜„ì¬ OLMoì˜ ê°ìƒ‰(ì¸ìŠ¤íŠ¸ëŸ­ì…˜ íŠœë‹ ë° RLHFed) ë²„ì „ì„ ì œì‘í•˜ëŠ” ë° ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, ê³§ ì¶œì‹œí•  ì˜ˆì •ì´ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë“  ì½”ë“œ ë° ê°€ì¤‘ì¹˜ëŠ” Apache 2.0 ë¼ì´ì„ ìŠ¤ì— ë”°ë¼ ë¦´ë¦¬ìŠ¤ë©ë‹ˆë‹¤. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank" title="">http://www.apache.org/licenses/LICENSE-2.0</a></span></span></span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p" id="S1.p6.1">ì´ê²ƒì€ ë” í° ëª¨ë¸, ëª…ë ¹ì–´ ì¡°ì • ëª¨ë¸, ê·¸ë¦¬ê³  ë” ë§ì€ ì–‘ì‹ ë° ë³€í˜•ìœ¼ë¡œ ì´ì–´ì§€ëŠ” ê¸´ ì¼ë ¨ì˜ ê³„íšëœ ë¦´ë¦¬ì¦ˆì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ì´ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ì™€ ëª¨ë¸ ëŠ¥ë ¥ ê°„ì˜ ê´€ê³„, ì„¤ê³„ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒì˜ ì˜í–¥, ë‹¤ì–‘í•œ ìµœì í™” ë°©ë²• ë° ëª¨ë¸ í›ˆë ¨ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ê°™ì€ ì´ëŸ¬í•œ ëª¨ë¸ì˜ ì•„ì§ ì˜ ì´í•´ë˜ì§€ ì•Šì€ ì¸¡ë©´ì— ëŒ€í•œ ì—°êµ¬ë¥¼ ì´‰ë§¤í•˜ê¸°ë¥¼ í¬ë§í•œë‹¤. ë˜í•œ ì´ ì²™ë„ì—ì„œ ì–¸ì–´ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” ë° í•„ìš”í•œ êµí›ˆê³¼ ì¤‘ìš”í•œ ì„¸ë¶€ ì‚¬í•­ì— ëŒ€í•´ ë³´ê³ í•œë‹¤.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>OLMo Framework</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.1">ì´ ì„¹ì…˜ì—ì„œëŠ” OLMo ëª¨ë¸(ì„¹ì…˜ <a class="ltx_ref" href="#S2.SS1" title="2.1 OLMo Model and Architecture â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.1</span></a>), ì‚¬ì „ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì¸ ëŒë§ˆ(ì„¹ì…˜ <a class="ltx_ref" href="#S2.SS2" title="2.2 Pretraining Data: Dolma â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.2</span></a>), í‰ê°€ í”„ë ˆì„ì›Œí¬(ì„¹ì…˜ <a class="ltx_ref" href="#S2.SS4" title="2.4 Evaluation â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>)ë¡œ êµ¬ì„±ëœ OLMo í”„ë ˆì„ì›Œí¬ë¥¼ ì„¤ëª…í•œë‹¤.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>OLMo&nbsp;Model and Architecture</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p1.1">ìš°ë¦¬ëŠ” <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a class="ltx_ref" href="#bib.bib80" title="">2017</a>)</cite>ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë””ì½”ë” ì „ìš© ë³€ì••ê¸° ì•„í‚¤í…ì²˜ë¥¼ ì±„íƒí•˜ê³  í‘œ <a class="ltx_ref" href="#S2.T1" title="Table 1 â€£ 2.1 OLMo Model and Architecture â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>ì— ì„¤ëª…ëœ ëŒ€ë¡œ 1B ë° 7B ë³€í˜•ì„ ì „ë‹¬í•˜ë©° 65B ë²„ì „ì´ ê³§ ì¶œì‹œëœë‹¤. ìš°ë¦¬ì˜ íŠ¹ì • ì•„í‚¤í…ì²˜ëŠ” PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery etÂ al., <a class="ltx_ref" href="#bib.bib14" title="">2022</a>)</cite>, LLaMA ê³„ì—´ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="#bib.bib77" title="">2023a</a>, <a class="ltx_ref" href="#bib.bib78" title="">b</a>)</cite>, OpenLM <cite class="ltx_cite ltx_citemacro_citep">(Gururangan etÂ al., <a class="ltx_ref" href="#bib.bib30" title="">2023</a>)</cite>, Falcon <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei etÂ al., <a class="ltx_ref" href="#bib.bib2" title="">2023</a>)</cite>ì™€ ê°™ì€ ë‹¤ë¥¸ ìµœê·¼ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì— ì´ì–´ <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a class="ltx_ref" href="#bib.bib80" title="">2017</a>)</cite>ì˜ ë°”ë‹ë¼ ë³€ì••ê¸°ì— ëŒ€í•œ ëª‡ ê°€ì§€ ê°œì„  ì‚¬í•­ì„ í¬í•¨í•œë‹¤. í‘œ <a class="ltx_ref" href="#S2.T2" title="Table 2 â€£ 2.1 OLMo Model and Architecture â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>ëŠ” 7B ì•„í‚¤í…ì²˜ë¥¼ ì´ëŸ¬í•œ ë‹¤ë¥¸ íŒ¨ë°€ë¦¬ì˜ ìœ ì‚¬í•œ í¬ê¸°ì˜ ëª¨ë¸ê³¼ ì¢…í•©ì ìœ¼ë¡œ ë¹„êµí•œë‹¤.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Size</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Layers</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Hidden Size</span></th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Attention Heads</span></th>
<th id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Tokens Trained</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">1B</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">16</td>
<td id="S2.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">2T</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">7B</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">4086</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">2.46T</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">65B*</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">80</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">8192</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">64</td>
<td id="S2.T1.1.4.3.5" class="ltx_td ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">í‘œ 1:</span>OLMo ëª¨ë¸ í¬ê¸° ë° í›ˆë ¨ëœ í† í°ì˜ ìµœëŒ€ ê°œìˆ˜.</figcaption>
<br class="ltx_break">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
* <em id="S2.T1.3.1" class="ltx_emph ltx_font_italic">At the time of writing our 65B model is still training.</em></figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p2.1">ì¼ë°˜ì ìœ¼ë¡œ ì†ì‹¤ ìŠ¤íŒŒì´í¬ ë° ëŠë¦° ë°œì‚°ì˜ ìœ„í—˜ì„ ìµœì†Œí™”í•˜ë©´ì„œ í•˜ë“œì›¨ì–´ì—ì„œ ì²˜ë¦¬ëŸ‰ì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ ìµœì í™”í•˜ì—¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•œë‹¤. ìš°ë¦¬ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ê³„ì‚° ì†ŒìŠ¤ê°€ ì£¼ì–´ì§„ ë£¨í”„ ë‚´ í‰ê°€ ì„¤ì •ì„ í†µí•´ ì„ íƒì„ ì œê±°í•œë‹¤(ì„¹ì…˜ <a class="ltx_ref" href="#S2.SS4.SSS0.Px1" title="In-Loop Training Ablations â€£ 2.4 Evaluation â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>). í‘œ <a class="ltx_ref" href="#S2.T2" title="Table 2 â€£ 2.1 OLMo Model and Architecture â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>ëŠ” ìš°ë¦¬ì˜ ë””ìì¸ ì„ íƒì„ ìµœê·¼ì˜ ìµœì‹  ê°œë°©í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ë¹„êµí•œë‹¤. ë°”ë‹ë¼ ë³€ì••ê¸° ì•„í‚¤í…ì²˜ì˜ ì£¼ìš” ë³€ê²½ ì‚¬í•­ì€ ë‹¤ìŒê³¼ ê°™ì´ ìš”ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">No biases. </span> LLaMA, PaLM ë° ê¸°íƒ€ì— ì´ì–´ í›ˆë ¨ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì•„í‚¤í…ì²˜ì—ì„œ ëª¨ë“  í¸í–¥ ìš©ì–´ë¥¼ ì œì™¸í•œë‹¤.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Non-parametric layer norm. </span> ìš°ë¦¬ëŠ” ê·œë²” ë‚´ì—ì„œ ì–´íŒŒì¸ ë³€í™˜ì´ ì—†ëŠ”, ì¦‰ "ì ì‘ì  ì´ë“"(ë˜ëŠ” ë°”ì´ì–´ìŠ¤)ì´ ì—†ëŠ” ì¸µ ê·œë²” <cite class="ltx_cite ltx_citemacro_citep">(Ba etÂ al., <a class="ltx_ref" href="#bib.bib4" title="">2016</a>)</cite>ì˜ ë¹„ëª¨ìˆ˜ì  ê³µì‹ì„ ì‚¬ìš©í•œë‹¤. ì´ê²ƒì´ ê°€ì¥ ì•ˆì „í•œ ì˜µì…˜ì´ì—ˆê³  ë§¤ê°œë³€ìˆ˜ ê³„ì¸µ ê·œë²” ë° RMSNorm <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Sennrich, <a class="ltx_ref" href="#bib.bib91" title="">2019</a>)</cite>ì™€ ê°™ì€ ë‹¤ë¥¸ ë³€í˜•ê³¼ ë¹„êµí•˜ì—¬ ê°€ì¥ ë¹¨ëë‹¤ê³  ë¯¿ëŠ”ë‹¤.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">SwiGLU activation function. </span> LLaMA, PaLM ë“±ê³¼ ê°™ì´ ReLU ëŒ€ì‹  SwiGLU í™œì„±í™” í•¨ìˆ˜ <cite class="ltx_cite ltx_citemacro_citep">(Shazeer, <a class="ltx_ref" href="#bib.bib70" title="">2020</a>)</cite>ë¥¼ ì‚¬ìš©í•˜ê³ , LLaMAì— ë”°ë¼ í™œì„±í™” ìˆ¨ê¹€ í¬ê¸°ëŠ” ëŒ€ëµ <math alttext="\frac{8}{3}d" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mrow id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mfrac id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml"><mn id="S2.I1.i3.p1.1.m1.1.1.2.2" xref="S2.I1.i3.p1.1.m1.1.1.2.2.cmml">8</mn><mn id="S2.I1.i3.p1.1.m1.1.1.2.3" xref="S2.I1.i3.p1.1.m1.1.1.2.3.cmml">3</mn></mfrac><mo id="S2.I1.i3.p1.1.m1.1.1.1" lspace="0em" rspace="0em" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><times id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.1"></times><apply id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2"><divide id="S2.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2"></divide><cn id="S2.I1.i3.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S2.I1.i3.p1.1.m1.1.1.2.2">8</cn><cn id="S2.I1.i3.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S2.I1.i3.p1.1.m1.1.1.2.3">3</cn></apply><ci id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">\frac{8}{3}d</annotation></semantics></math>ì´ì§€ë§Œ ì²˜ë¦¬ëŸ‰ì„ ê°œì„ í•˜ê¸° ìœ„í•´ 128ì˜ ê°€ì¥ ê°€ê¹Œìš´ ë°°ìˆ˜(ì˜ˆ: 7B ëª¨ë¸ì˜ ê²½ìš° 11,008)ë¡œ ì¦ê°€í–ˆë‹¤. <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>SwiGLUëŠ” "gated" í™œì„±í™” í•¨ìˆ˜ì´ë¯€ë¡œ ì¶œë ¥ì€ ì…ë ¥ì˜ ì ˆë°˜ í¬ê¸°ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê¸°ìˆ ì ìœ¼ë¡œ SwiGLUì— ëŒ€í•œ ì…ë ¥ì€ 7B ëª¨ë¸ì— ëŒ€í•´ 2 <math alttext="\times" class="ltx_Math" display="inline" id="footnote2.m1.1"><semantics id="footnote2.m1.1b"><mo id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><times id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">\times</annotation></semantics></math> 11,008 = 22,016ì˜ ì°¨ì›ì„ ê°–ëŠ”ë‹¤. </span></span></span></p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">Rotary positional embeddings (RoPE). </span> Like LLaMA, PaLM, and others we replace absolute position embeddings with rotary position embeddings (RoPE; <cite class="ltx_cite ltx_citemacro_citep">Su etÂ al., <a class="ltx_ref" href="#bib.bib73" title="">2021</a></cite>).</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p class="ltx_p" id="S2.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i5.p1.1.1">Vocabulary. </span> ìš°ë¦¬ëŠ” ê°œì¸ ì‹ë³„ ì •ë³´(PII)ë¥¼ ë§ˆìŠ¤í‚¹í•˜ê¸° ìœ„í•œ ì¶”ê°€ í† í°ê³¼ í•¨ê»˜ GPT-NeoX-20B <cite class="ltx_cite ltx_citemacro_citep">(Black etÂ al., <a class="ltx_ref" href="#bib.bib10" title="">2022</a>)</cite>ì˜ ìˆ˜ì •ëœ ë²„ì „ì˜ BPE ê¸°ë°˜ í† í°í™”ê¸°ë¥¼ ì‚¬ìš©í•œë‹¤. ìµœì¢… ì–´íœ˜ì˜ í¬ê¸°ëŠ” 50,280ì´ë‹¤. ê·¸ëŸ¬ë‚˜ í›ˆë ¨ ì²˜ë¦¬ëŸ‰ì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ í•´ë‹¹ ì„ë² ë”© í–‰ë ¬ì˜ í¬ê¸°ë¥¼ 50,304ë¡œ ì¦ê°€ì‹œì¼œ 128ì˜ ë°°ìˆ˜ê°€ ë˜ë„ë¡ í•œë‹¤.</p>
</div>
</li>
</ol>
</div>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.8.9.1" class="ltx_tr">
<th id="S2.T2.8.9.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"></th>
<td id="S2.T2.8.9.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T2.8.9.1.2.1" class="ltx_text ltx_font_bold">OLMo-7B</span></td>
<td id="S2.T2.8.9.1.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T2.8.9.1.3.1" class="ltx_text ltx_font_bold">LLaMA2-7B</span></td>
<td id="S2.T2.8.9.1.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T2.8.9.1.4.1" class="ltx_text ltx_font_bold">OpenLM-7B</span></td>
<td id="S2.T2.8.9.1.5" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T2.8.9.1.5.1" class="ltx_text ltx_font_bold">Falcon-7B</span></td>
<td id="S2.T2.8.9.1.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S2.T2.8.9.1.6.1" class="ltx_text ltx_font_bold">PaLM-8B</span></td>
</tr>
<tr id="S2.T2.8.10.2" class="ltx_tr">
<th id="S2.T2.8.10.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">Dimension</th>
<td id="S2.T2.8.10.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T2.8.10.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T2.8.10.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T2.8.10.2.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4544</td>
<td id="S2.T2.8.10.2.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
</tr>
<tr id="S2.T2.8.11.3" class="ltx_tr">
<th id="S2.T2.8.11.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Num heads</th>
<td id="S2.T2.8.11.3.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T2.8.11.3.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T2.8.11.3.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T2.8.11.3.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">71</td>
<td id="S2.T2.8.11.3.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">16</td>
</tr>
<tr id="S2.T2.8.12.4" class="ltx_tr">
<th id="S2.T2.8.12.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Num layers</th>
<td id="S2.T2.8.12.4.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T2.8.12.4.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T2.8.12.4.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T2.8.12.4.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">32</td>
<td id="S2.T2.8.12.4.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">32</td>
</tr>
<tr id="S2.T2.3.3" class="ltx_tr">
<th id="S2.T2.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">MLP ratio</th>
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S2.T2.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.m1.1.1" xref="S2.T2.1.1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T2.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.m1.1c">\sim</annotation></semantics></math>8/3</td>
<td id="S2.T2.2.2.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S2.T2.2.2.2.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.2.2.2.m1.1a"><mo id="S2.T2.2.2.2.m1.1.1" xref="S2.T2.2.2.2.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.2.m1.1b"><csymbol cd="latexml" id="S2.T2.2.2.2.m1.1.1.cmml" xref="S2.T2.2.2.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.2.m1.1c">\sim</annotation></semantics></math>8/3</td>
<td id="S2.T2.3.3.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S2.T2.3.3.3.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.3.3.3.m1.1a"><mo id="S2.T2.3.3.3.m1.1.1" xref="S2.T2.3.3.3.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.m1.1b"><csymbol cd="latexml" id="S2.T2.3.3.3.m1.1.1.cmml" xref="S2.T2.3.3.3.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.m1.1c">\sim</annotation></semantics></math>8/3</td>
<td id="S2.T2.3.3.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">4</td>
<td id="S2.T2.3.3.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">4</td>
</tr>
<tr id="S2.T2.8.13.5" class="ltx_tr">
<th id="S2.T2.8.13.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Layer norm type</th>
<td id="S2.T2.8.13.5.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">non-parametric</td>
<td id="S2.T2.8.13.5.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">RMSNorm</td>
<td id="S2.T2.8.13.5.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">parametric</td>
<td id="S2.T2.8.13.5.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">parametric</td>
<td id="S2.T2.8.13.5.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">parametric</td>
</tr>
<tr id="S2.T2.8.14.6" class="ltx_tr">
<th id="S2.T2.8.14.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Positional embeddings</th>
<td id="S2.T2.8.14.6.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td id="S2.T2.8.14.6.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td id="S2.T2.8.14.6.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td id="S2.T2.8.14.6.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
<td id="S2.T2.8.14.6.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">RoPE</td>
</tr>
<tr id="S2.T2.8.15.7" class="ltx_tr">
<th id="S2.T2.8.15.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Attention variant</th>
<td id="S2.T2.8.15.7.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">full</td>
<td id="S2.T2.8.15.7.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">GQA</td>
<td id="S2.T2.8.15.7.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">full</td>
<td id="S2.T2.8.15.7.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">MQA</td>
<td id="S2.T2.8.15.7.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">MQA</td>
</tr>
<tr id="S2.T2.8.16.8" class="ltx_tr">
<th id="S2.T2.8.16.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Biases</th>
<td id="S2.T2.8.16.8.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">none</td>
<td id="S2.T2.8.16.8.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">none</td>
<td id="S2.T2.8.16.8.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">in LN only</td>
<td id="S2.T2.8.16.8.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">in LN only</td>
<td id="S2.T2.8.16.8.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">none</td>
</tr>
<tr id="S2.T2.8.17.9" class="ltx_tr">
<th id="S2.T2.8.17.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Block type</th>
<td id="S2.T2.8.17.9.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">sequential</td>
<td id="S2.T2.8.17.9.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">sequential</td>
<td id="S2.T2.8.17.9.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">sequential</td>
<td id="S2.T2.8.17.9.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">parallel</td>
<td id="S2.T2.8.17.9.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">parallel</td>
</tr>
<tr id="S2.T2.8.18.10" class="ltx_tr">
<th id="S2.T2.8.18.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Activation</th>
<td id="S2.T2.8.18.10.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
<td id="S2.T2.8.18.10.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
<td id="S2.T2.8.18.10.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
<td id="S2.T2.8.18.10.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">GeLU</td>
<td id="S2.T2.8.18.10.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">SwiGLU</td>
</tr>
<tr id="S2.T2.8.19.11" class="ltx_tr">
<th id="S2.T2.8.19.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Sequence length</th>
<td id="S2.T2.8.19.11.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td id="S2.T2.8.19.11.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">4096</td>
<td id="S2.T2.8.19.11.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td id="S2.T2.8.19.11.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td id="S2.T2.8.19.11.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
</tr>
<tr id="S2.T2.8.20.12" class="ltx_tr">
<th id="S2.T2.8.20.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Batch size (instances)</th>
<td id="S2.T2.8.20.12.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">2160</td>
<td id="S2.T2.8.20.12.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">1024</td>
<td id="S2.T2.8.20.12.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">2048</td>
<td id="S2.T2.8.20.12.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">2304</td>
<td id="S2.T2.8.20.12.6" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">512</td>
</tr>
<tr id="S2.T2.8.8" class="ltx_tr">
<th id="S2.T2.8.8.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Batch size (tokens)</th>
<td id="S2.T2.4.4.1" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S2.T2.4.4.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.4.4.1.m1.1a"><mo id="S2.T2.4.4.1.m1.1.1" xref="S2.T2.4.4.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.4.4.1.m1.1b"><csymbol cd="latexml" id="S2.T2.4.4.1.m1.1.1.cmml" xref="S2.T2.4.4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.4.1.m1.1c">\sim</annotation></semantics></math>4M</td>
<td id="S2.T2.5.5.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S2.T2.5.5.2.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.5.5.2.m1.1a"><mo id="S2.T2.5.5.2.m1.1.1" xref="S2.T2.5.5.2.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.5.5.2.m1.1b"><csymbol cd="latexml" id="S2.T2.5.5.2.m1.1.1.cmml" xref="S2.T2.5.5.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.5.2.m1.1c">\sim</annotation></semantics></math>4M</td>
<td id="S2.T2.6.6.3" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S2.T2.6.6.3.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.6.6.3.m1.1a"><mo id="S2.T2.6.6.3.m1.1.1" xref="S2.T2.6.6.3.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.6.6.3.m1.1b"><csymbol cd="latexml" id="S2.T2.6.6.3.m1.1.1.cmml" xref="S2.T2.6.6.3.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.6.6.3.m1.1c">\sim</annotation></semantics></math>4M</td>
<td id="S2.T2.7.7.4" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S2.T2.7.7.4.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.7.7.4.m1.1a"><mo id="S2.T2.7.7.4.m1.1.1" xref="S2.T2.7.7.4.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.7.7.4.m1.1b"><csymbol cd="latexml" id="S2.T2.7.7.4.m1.1.1.cmml" xref="S2.T2.7.7.4.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.7.4.m1.1c">\sim</annotation></semantics></math>4M</td>
<td id="S2.T2.8.8.5" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S2.T2.8.8.5.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.8.8.5.m1.1a"><mo id="S2.T2.8.8.5.m1.1.1" xref="S2.T2.8.8.5.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.8.8.5.m1.1b"><csymbol cd="latexml" id="S2.T2.8.8.5.m1.1.1.cmml" xref="S2.T2.8.8.5.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.8.5.m1.1c">\sim</annotation></semantics></math>1M</td>
</tr>
<tr id="S2.T2.8.21.13" class="ltx_tr">
<th id="S2.T2.8.21.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">Weight tying</th>
<td id="S2.T2.8.21.13.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td id="S2.T2.8.21.13.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td id="S2.T2.8.21.13.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td id="S2.T2.8.21.13.5" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">no</td>
<td id="S2.T2.8.21.13.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">yes</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">í‘œ 2:</span>7-8B ìŠ¤ì¼€ì¼ì—ì„œì˜ LM ì•„í‚¤í…ì²˜ ë¹„êµ. "ê³„ì¸µ ê·œë²” ìœ í˜•" í–‰ì—ì„œ, "ëª¨ìˆ˜ì " ë° "ë¹„ëª¨ìˆ˜ì "ì€ ê°ê° ì ì‘ ì´ë“ ë° ë°”ì´ì–´ìŠ¤ê°€ ìˆê±°ë‚˜ ì—†ëŠ” í†µìƒì ì¸ ê³„ì¸µ ê·œë²” êµ¬í˜„ì„ ì§€ì¹­í•œë‹¤.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Pretraining Data: Dolma</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p1.1">ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ê°€ ì§„í–‰ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì‚¬ì „ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ëŠ” ì—¬ì „íˆ ì—´ë ¤ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ì „ í›ˆë ¨ ë°ì´í„°ëŠ” ì¢…ì¢… ì—´ë¦° ëª¨ë¸(ë‹«íŒ ëª¨ë¸ì€ ê³ ì‚¬í•˜ê³ )ê³¼ í•¨ê»˜ ê³µê°œë˜ì§€ ì•Šìœ¼ë©° ì´ëŸ¬í•œ ë°ì´í„°ì— ëŒ€í•œ ë¬¸ì„œëŠ” ì‘ì—…ì„ ì¬í˜„í•˜ê±°ë‚˜ ì™„ì „íˆ ì´í•´í•˜ëŠ” ë° í•„ìš”í•œ ì„¸ë¶€ ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°ê°€ ë§ë‹¤. ì´ê²ƒì€ í›ˆë ¨ ë°ì´í„°ê°€ ëª¨ë¸ ì—­ëŸ‰ê³¼ í•œê³„ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì´í•´í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ì–¸ì–´ ëª¨ë¸ ì—°êµ¬ì˜ íŠ¹ì • ìŠ¤ë ˆë“œë¥¼ ì§€ì›í•˜ëŠ” ê²ƒì„ ì–´ë µê²Œ ë§Œë“¤ì—ˆë‹¤. ì–¸ì–´ ëª¨ë¸ ì‚¬ì „ í›ˆë ¨ì— ëŒ€í•œ ê³µê°œ ì—°êµ¬ë¥¼ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•´ ìš°ë¦¬ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì‚¬ì „ í›ˆë ¨ì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆëŠ” 7ê°œì˜ ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì–»ì€ 5B ë¬¸ì„œì— ê±¸ì³ 3T í† í°ì˜ ë‹¤ì–‘í•œ ë‹¤ì¤‘ ì†ŒìŠ¤ ì½”í¼ìŠ¤ì¸ ëŒë§ˆë¥¼ êµ¬ì¶•í•˜ê³  ì¶œì‹œí–ˆë‹¤. TableÂ <a class="ltx_ref" href="#S2.T3" title="Table 3 â€£ 2.2 Pretraining Data: Dolma â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>ëŠ” ê° ì†ŒìŠ¤ì˜ ë°ì´í„° ì–‘ì— ëŒ€í•œ ë†’ì€ ìˆ˜ì¤€ì˜ ê°œìš”ë¥¼ ì œê³µí•œë‹¤.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p2.1">ëŒë§ˆëŠ” (1) ì–¸ì–´ í•„í„°ë§, (2) í’ˆì§ˆ í•„í„°ë§, (3) ì½˜í…ì¸  í•„í„°ë§, (4) ì¤‘ë³µ ì œê±°, (5) ë‹¤ì¤‘ ì†ŒìŠ¤ í˜¼í•© ë° (6) í† í°í™”ì˜ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¶•ëœë‹¤. ìš°ë¦¬ëŠ” ë…ìì—ê²Œ ëŒë§ˆ ë³´ê³ ì„œ <cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2024</a>)</cite>ë¥¼ ì°¸ì¡°í•˜ì—¬ ë””ìì¸ ì›ë¦¬ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©, êµ¬ì„±ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš© ë° ë‚´ìš©ì— ëŒ€í•œ ë” ìì„¸í•œ ìš”ì•½ì„ ì„¤ëª…í•œë‹¤. ì´ ë³´ê³ ì„œëŠ” ì½˜í…ì¸  ë˜ëŠ” í’ˆì§ˆ í•„í„°ì˜ ì—­í• , ì¤‘ë³µ ì œê±° ë° ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ë°ì´í„° í˜¼í•©ì„ í¬í•¨í•˜ì—¬ ì¤‘ìš”í•œ ë°ì´í„° íë ˆì´ì…˜ ê´€í–‰ì— ëŒ€í•´ ë°°ìš´ ë‚´ìš©ì„ ê³µìœ í•˜ê¸° ìœ„í•´ ëŒë§ˆì˜ ì¤‘ê°„ ìƒíƒœì— ëŒ€í•œ í•™ìŠµ ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ê°€ ë¶„ì„ ë° ì‹¤í—˜ ê²°ê³¼ë¥¼ ì œê³µí•œë‹¤. íë ˆì´ì…˜ê³¼ ìµœì¢… ë¦´ë¦¬ìŠ¤ ëª¨ë‘ì—ì„œ ê° ì†ŒìŠ¤ì˜ ë¬¸ì„œë¥¼ ë³„ë„ë¡œ ë³´ê´€í•©ë‹ˆë‹¤. ê³ ì„±ëŠ¥ ë°ì´í„° íë ˆì´ì…˜ ë„êµ¬ë¥¼ ì˜¤í”ˆì†Œì‹±í•˜ì˜€ìœ¼ë©°, ì´ íˆ´í‚·ì€ ëŒë§ˆì— ëŒ€í•œ ì¶”ê°€ ì‹¤í—˜, ì‘ì—… ì¬í˜„, ì‚¬ì „ í›ˆë ¨ ë§ë­‰ì¹˜ì˜ ë¹ ë¥´ê³  ì‰¬ìš´ íë ˆì´ì…˜ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë°ì´í„°ì„¸íŠ¸ ë¶„ì„ì„ ë•ê¸° ìœ„í•´ WIMBD ë„êµ¬ <cite class="ltx_cite ltx_citemacro_citep">(Elazar etÂ al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>ë¥¼ ì˜¤í”ˆì†Œì‹±í–ˆë‹¤.</p>
</div>
<figure id="S2.T3" class="ltx_table">
<table id="S2.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T3.1.1.1" class="ltx_tr">
<th id="S2.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Source</span></th>
<th id="S2.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Doc Type</span></th>
<th id="S2.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T3.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.1.1.3.1.1" class="ltx_tr">
<td id="S2.T3.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T3.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">UTF-8</span></td>
</tr>
<tr id="S2.T3.1.1.1.3.1.2" class="ltx_tr">
<td id="S2.T3.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T3.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">bytes</span></td>
</tr>
<tr id="S2.T3.1.1.1.3.1.3" class="ltx_tr">
<td id="S2.T3.1.1.1.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T3.1.1.1.3.1.3.1.1" class="ltx_text ltx_font_italic">(GB)</span></td>
</tr>
</tbody></table>
</th>
<th id="S2.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T3.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.1.1.4.1.1" class="ltx_tr">
<td id="S2.T3.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T3.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Documents</span></td>
</tr>
<tr id="S2.T3.1.1.1.4.1.2" class="ltx_tr">
<td id="S2.T3.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T3.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_italic">(millions)</span></td>
</tr>
</tbody></table>
</th>
<th id="S2.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T3.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.1.1.5.1.1" class="ltx_tr">
<td id="S2.T3.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T3.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">GPT-NeoX</span></td>
</tr>
<tr id="S2.T3.1.1.1.5.1.2" class="ltx_tr">
<td id="S2.T3.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T3.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">tokens</span></td>
</tr>
<tr id="S2.T3.1.1.1.5.1.3" class="ltx_tr">
<td id="S2.T3.1.1.1.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T3.1.1.1.5.1.3.1.1" class="ltx_text ltx_font_italic">(billions)</span></td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T3.1.2.1" class="ltx_tr">
<th id="S2.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<table id="S2.T3.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.2.1.1.1.1" class="ltx_tr">
<td id="S2.T3.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Common Crawl</td>
</tr>
</tbody></table>
</th>
<td id="S2.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">web pages</td>
<td id="S2.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">9,022</td>
<td id="S2.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">3,370</td>
<td id="S2.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">2,006</td>
</tr>
<tr id="S2.T3.1.3.2" class="ltx_tr">
<th id="S2.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<table id="S2.T3.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.3.2.1.1.1" class="ltx_tr">
<td id="S2.T3.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">The Stack</td>
</tr>
</tbody></table>
</th>
<td id="S2.T3.1.3.2.2" class="ltx_td ltx_align_center">code</td>
<td id="S2.T3.1.3.2.3" class="ltx_td ltx_align_center">1,043</td>
<td id="S2.T3.1.3.2.4" class="ltx_td ltx_align_center">210</td>
<td id="S2.T3.1.3.2.5" class="ltx_td ltx_align_center">342</td>
</tr>
<tr id="S2.T3.1.4.3" class="ltx_tr">
<th id="S2.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<table id="S2.T3.1.4.3.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.4.3.1.1.1" class="ltx_tr">
<td id="S2.T3.1.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">C4</td>
</tr>
</tbody></table>
</th>
<td id="S2.T3.1.4.3.2" class="ltx_td ltx_align_center">web pages</td>
<td id="S2.T3.1.4.3.3" class="ltx_td ltx_align_center">790</td>
<td id="S2.T3.1.4.3.4" class="ltx_td ltx_align_center">364</td>
<td id="S2.T3.1.4.3.5" class="ltx_td ltx_align_center">174</td>
</tr>
<tr id="S2.T3.1.5.4" class="ltx_tr">
<th id="S2.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<table id="S2.T3.1.5.4.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.5.4.1.1.1" class="ltx_tr">
<td id="S2.T3.1.5.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Reddit</td>
</tr>
</tbody></table>
</th>
<td id="S2.T3.1.5.4.2" class="ltx_td ltx_align_center">social media</td>
<td id="S2.T3.1.5.4.3" class="ltx_td ltx_align_center">339</td>
<td id="S2.T3.1.5.4.4" class="ltx_td ltx_align_center">377</td>
<td id="S2.T3.1.5.4.5" class="ltx_td ltx_align_center">80</td>
</tr>
<tr id="S2.T3.1.6.5" class="ltx_tr">
<th id="S2.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<table id="S2.T3.1.6.5.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.6.5.1.1.1" class="ltx_tr">
<td id="S2.T3.1.6.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">peS2o</td>
</tr>
</tbody></table>
</th>
<td id="S2.T3.1.6.5.2" class="ltx_td ltx_align_center">STEM papers</td>
<td id="S2.T3.1.6.5.3" class="ltx_td ltx_align_center">268</td>
<td id="S2.T3.1.6.5.4" class="ltx_td ltx_align_center">38.8</td>
<td id="S2.T3.1.6.5.5" class="ltx_td ltx_align_center">57</td>
</tr>
<tr id="S2.T3.1.7.6" class="ltx_tr">
<th id="S2.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<table id="S2.T3.1.7.6.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.7.6.1.1.1" class="ltx_tr">
<td id="S2.T3.1.7.6.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Project Gutenberg</td>
</tr>
</tbody></table>
</th>
<td id="S2.T3.1.7.6.2" class="ltx_td ltx_align_center">books</td>
<td id="S2.T3.1.7.6.3" class="ltx_td ltx_align_center">20.4</td>
<td id="S2.T3.1.7.6.4" class="ltx_td ltx_align_center">0.056</td>
<td id="S2.T3.1.7.6.5" class="ltx_td ltx_align_center">5.2</td>
</tr>
<tr id="S2.T3.1.8.7" class="ltx_tr">
<th id="S2.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<table id="S2.T3.1.8.7.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S2.T3.1.8.7.1.1.1" class="ltx_tr">
<td id="S2.T3.1.8.7.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Wikipedia, Wikibooks</td>
</tr>
</tbody></table>
</th>
<td id="S2.T3.1.8.7.2" class="ltx_td ltx_align_center">encyclopedic</td>
<td id="S2.T3.1.8.7.3" class="ltx_td ltx_align_center">16.2</td>
<td id="S2.T3.1.8.7.4" class="ltx_td ltx_align_center">6.2</td>
<td id="S2.T3.1.8.7.5" class="ltx_td ltx_align_center">3.7</td>
</tr>
<tr id="S2.T3.1.9.8" class="ltx_tr">
<th id="S2.T3.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" colspan="2"><span id="S2.T3.1.9.8.1.1" class="ltx_text ltx_font_bold">Total</span></th>
<td id="S2.T3.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T3.1.9.8.2.1" class="ltx_text ltx_font_bold">11,519</span></td>
<td id="S2.T3.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T3.1.9.8.3.1" class="ltx_text ltx_font_bold">4,367</span></td>
<td id="S2.T3.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T3.1.9.8.4.1" class="ltx_text ltx_font_bold">2,668</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3:</span>Composition of Dolma.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Adaptation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p1.1">ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ í•­ìƒ ê·¸ëŒ€ë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì„±ëŠ¥, ì•ˆì „ì„± ë° ì‚¬ìš©ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë”ìš± ë¯¸ì„¸ ì¡°ì •ëœë‹¤. ì¢…ì¢… ëª¨ë¸ë“¤ì€ ë¨¼ì € ëª…ë ¹ë“¤ <cite class="ltx_cite ltx_citemacro_citep">(Mishra etÂ al., <a class="ltx_ref" href="#bib.bib49" title="">2022</a>; Wei etÂ al., <a class="ltx_ref" href="#bib.bib84" title="">2022</a>; Sanh etÂ al., <a class="ltx_ref" href="#bib.bib69" title="">2022</a>)</cite>ë¥¼ ë”°ë¥´ë„ë¡ íŠ¸ë ˆì´ë‹ë˜ê³ , ì´ì–´ì„œ ê·¸ë“¤ì˜ ì„¸ëŒ€ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì¸ê°„ ì„ í˜¸ë„ë“¤ <cite class="ltx_cite ltx_citemacro_citep">(Ouyang etÂ al., <a class="ltx_ref" href="#bib.bib54" title="">2022</a>)</cite>ì— ëŒ€í•´ ì¶”ê°€ë¡œ íŠ¸ë ˆì´ë‹ëœë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Open Instruct (<span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p1.1.1">TÃ¼lu</span>) ë°ì´í„° ë° í›ˆë ¨ ì„¤ì • <cite class="ltx_cite ltx_citemacro_citep">(Ivison etÂ al., <a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite>ì— ë”°ë¼ OLMoë¥¼ ì¼ë°˜ ì±„íŒ… ë³´ì¡°ìë¡œ í›ˆë ¨ì‹œí‚´ìœ¼ë¡œì¨ OLMoë¥¼ ì¶”ê°€ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ê¸°ë³¸ ëª¨ë¸ë¡œ ì‚¬ìš©í•˜ëŠ” íš¨ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì œì•ˆëœ ë°©ë²•ì€ ë¨¼ì € ì¦ë¥˜ëœ ëª…ë ¹ ë°ì´í„°ì™€ ì¸ê°„ì´ ì‘ì„±í•œ ëª…ë ¹ ë°ì´í„°ë¥¼ í˜¼í•©í•˜ì—¬ ëª…ë ¹ ë¯¸ì„¸ ì¡°ì •ì„ ìˆ˜í–‰í•œ ë‹¤ìŒ ì§ì ‘ ì„ í˜¸ë„ ìµœì í™”(DPO) <cite class="ltx_cite ltx_citemacro_citep">(Rafailov etÂ al., <a class="ltx_ref" href="#bib.bib62" title="">2023</a>)</cite>ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦ë¥˜ëœ ì„ í˜¸ë„ ë°ì´í„°ì™€ ëª¨ë¸ì„ ì¶”ê°€ë¡œ ì •ë ¬í•˜ëŠ” ê²ƒì´ë‹¤. <cite class="ltx_cite ltx_citemacro_citet">DeepSeek-AI etÂ al. (<a class="ltx_ref" href="#bib.bib21" title="">2024</a>)</cite>ì™€ ê°™ì€ ìµœê·¼ ëª¨ë¸ì—ì„œ ìˆ˜í–‰ëœ ë°”ì™€ ê°™ì´ ì‚¬ì „ í›ˆë ¨ì´ ëë‚  ë•Œ íˆ´ë£¨ ëª…ë ¹ì–´ ë°ì´í„°ë¥¼ í˜¼í•©í•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆì§€ë§Œ ê²°ì •ì ì¸ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆë‹¤.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Evaluation</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS4.p1.1">ëª¨ë¸ ì„¤ê³„ë¥¼ ìœ„í•œ ê²°ì •ì„ ë‚´ë¦¬ëŠ” <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.1">online</em> í‰ê°€ì™€ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ í‰ê°€í•˜ëŠ” <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.2">offline</em> í‰ê°€ì˜ ë‘ ë‹¨ê³„ì—ì„œ ê¸°ë³¸ ëª¨ë¸ í‰ê°€ë¥¼ ìˆ˜í–‰í•œë‹¤. ì˜¤í”„ë¼ì¸ ë‹¨ê³„ì—ì„œëŠ” ê´‘ë²”ìœ„í•œ ë°ì´í„° ì„¸íŠ¸ì™€ ì‘ì—… í˜•ì‹ì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆëŠ” ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ë„êµ¬ì¸ Catwalk í”„ë ˆì„ì›Œí¬ <cite class="ltx_cite ltx_citemacro_citep">(Groeneveld etÂ al., <a class="ltx_ref" href="#bib.bib28" title="">2023</a>)</cite>ë¥¼ ì‚¬ìš©í•œë‹¤. Catwalkë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ Perplexity ë²¤ì¹˜ë§ˆí¬ì¸ Paloma <cite class="ltx_cite ltx_citemacro_citep">(Magnusson etÂ al., <a class="ltx_ref" href="#bib.bib43" title="">2023</a>)</cite>ì— ëŒ€í•´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í‰ê°€ ë° ê³ ìœ  ì–¸ì–´ ëª¨ë¸ë§ í‰ê°€ë¥¼ ìˆ˜í–‰í•œë‹¤.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS4.p2.1">ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë° ë³µì¡ì„± í‰ê°€ ëª¨ë‘ì— ëŒ€í•´ ê³ ì • í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ê³¼ ê²°ê³¼ë¥¼ ë¹„êµí•œë‹¤. ë˜í•œ ì ì‘ëœ ëª¨ë¸ì— ëŒ€í•œ ë³„ë„ì˜ í‰ê°€ë¥¼ ë³´ê³ í•œë‹¤.</p>
</div>
<section id="S2.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">In-Loop Training Ablations</h4>

<div id="S2.SS4.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS4.SSS0.Px1.p1.1">ëª¨ë¸ í•™ìŠµ ì „ë°˜ì— ê±¸ì³ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì—¬ ëª¨ë¸ ì•„í‚¤í…ì²˜, ì´ˆê¸°í™”, ìµœì í™”ê¸°, í•™ìŠµë¥  ì¼ì • ë° ë°ì´í„° í˜¼í•©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ê²°ì •ì„ ë‚´ë¦°ë‹¤. ì´ë¥¼ <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS0.Px1.p1.1.1">online</em> í‰ê°€ë¼ê³  í•˜ë©° 1000ê°œì˜ í›ˆë ¨ ë‹¨ê³„ë§ˆë‹¤ ì¸-ë£¨í”„(ë˜ëŠ” <math alttext="\sim" class="ltx_Math" display="inline" id="S2.SS4.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS4.SSS0.Px1.p1.1.m1.1a"><mo id="S2.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.1.m1.1c">\sim</annotation></semantics></math>4B í›ˆë ¨ í† í°)ë¥¼ ì‹¤í–‰í•  ë•Œ í›ˆë ¨ë˜ëŠ” ëª¨ë¸ì˜ í’ˆì§ˆì— ëŒ€í•œ ì¡°ê¸°ì ì´ê³  ì—°ì†ì ì¸ ì‹ í˜¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í‰ê°€ëŠ” ìš°ë¦¬ì˜ <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS0.Px1.p1.1.2">ì˜¤í”„ë¼ì¸</em> ì„¹ì…˜ <a class="ltx_ref" href="#S4.SS1" title="4.1 Downstream evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>ì— ìì„¸íˆ ì„¤ëª…ëœ í‰ê°€ì— ì‚¬ìš©ë˜ëŠ” ë§ì€ í•µì‹¬ ì‘ì—… ë° ì‹¤í—˜ ì„¤ì •ì— ì˜ì¡´í•˜ë©°, ì´ëŠ” ë˜í•œ EleutherAI í‰ê°€ í•˜ë‹ˆìŠ¤ì˜ ì‘ì—… ë° í‰ê°€ êµ¬ì¡°ë¥¼ ë°˜ì˜í•œë‹¤.</p>
</div>
</section>
<section id="S2.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Downstream Evaluation</h4>

<div id="S2.SS4.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS4.SSS0.Px2.p1.1">ë§ì€ ì´ì „ ì‘ì—… <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="#bib.bib12" title="">2020</a>; Black etÂ al., <a class="ltx_ref" href="#bib.bib10" title="">2022</a>; Touvron etÂ al., <a class="ltx_ref" href="#bib.bib77" title="">2023a</a>, <a class="ltx_ref" href="#bib.bib78" title="">b</a>, <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS0.Px2.p1.1.1.1">inter alia</em>)</cite>ì— ì´ì–´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—… ì§‘í•©ì— ëŒ€í•œ ì œë¡œ ìƒ· ì„±ëŠ¥ì„ ë³´ê³ í•©ë‹ˆë‹¤. ìš°ë¦¬ì˜ í‰ê°€ ì œí’ˆêµ°ì€ <cite class="ltx_cite ltx_citemacro_citet">Touvron etÂ al. (<a class="ltx_ref" href="#bib.bib77" title="">2023a</a>)</cite> ë° <cite class="ltx_cite ltx_citemacro_citet">Touvron etÂ al. (<a class="ltx_ref" href="#bib.bib78" title="">2023b</a>)</cite>ì— ì˜í•´ ë³´ê³ ëœ ìƒì‹ ì¶”ë¡  ì‘ì—… ì„¸íŠ¸ì— ë°€ì ‘í•˜ê²Œ ëŒ€ì‘í•˜ëŠ” 8ê°œì˜ í•µì‹¬ ì‘ì—…ìœ¼ë¡œ êµ¬ì„±ëœë‹¤(ì‘ì—… ëª©ë¡ì€ í‘œ <a class="ltx_ref" href="#S4.T6" title="Table 6 â€£ Setup â€£ 4.1 Downstream evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a> ì°¸ì¡°). í‰ê°€ë˜ëŠ” ëª¨ë¸ë“¤ì˜ ìŠ¤ì¼€ì¼ì„ ê°ì•ˆí•  ë•Œ, ê·¸ëŸ¬í•œ íƒœìŠ¤í¬ë“¤ì€ ê·¸ë“¤ì˜ ìì—°ìŠ¤ëŸ¬ì›€(ì˜ˆë¥¼ ë“¤ì–´, ëª¨ë‘ê°€ í…ìŠ¤íŠ¸ ì™„ì„± ìŠ¤ì½”ì–´ë§ íƒœìŠ¤í¬ë“¤ë¡œì„œ ê³µì‹í™”ë  ìˆ˜ ìˆìŒ) ë° íŠ¸ë ˆì´ë‹ ì „ë°˜ì— ê±¸ì³ ì˜ë¯¸ìˆëŠ” ì‹ í˜¸ë“¤ì„ ì œê³µí•˜ëŠ” ëŠ¥ë ¥ìœ¼ë¡œ ì¸í•´ ëª¨ë¸ ê°œë°œ ì´ˆê¸°ì— ì„ íƒë˜ì—ˆë‹¤(ë„<a class="ltx_ref" href="#S4.F1" title="Figure 1 â€£ Results â€£ 4.1 Downstream evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> ì°¸ì¡°).</p>
</div>
</section>
<section id="S2.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Intrinsic Language Modeling Evaluation</h4>

<div id="S2.SS4.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS4.SSS0.Px3.p1.1">OLMo-7Bê°€ ë³´ë¥˜ëœ í›ˆë ¨ ë°ì´í„°ë¥¼ ë„˜ì–´ ì–¸ì–´ì˜ ë¶„í¬ì— ì–´ë–»ê²Œ ë¶€í•©í•˜ëŠ”ì§€ ì¸¡ì •í•˜ê¸° ìœ„í•´ 585ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì˜ì—­ì„ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ ë³µì¡ì„± ë²¤ì¹˜ë§ˆí¬ì¸ íŒ”ë¡œë§ˆ <cite class="ltx_cite ltx_citemacro_citep">(Magnusson etÂ al., <a class="ltx_ref" href="#bib.bib43" title="">2023</a>)</cite>ë¥¼ ì‚¬ìš©í•œë‹¤. DomainsëŠ” nytimes.comì—ì„œ Redditì˜ r/depressionê¹Œì§€ ë‹¤ì–‘í•˜ë©° ê³„ì¸µí™”ëœ ìƒ˜í”Œì˜ C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel etÂ al., <a class="ltx_ref" href="#bib.bib63" title="">2020</a>)</cite>ì™€ ê°™ì€ 18ê°œì˜ ê°œë³„ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ì¶”ì¶œëœë‹¤. ì´ë¥¼ í†µí•´ ì›ë³¸ ë§ë­‰ì¹˜ì— ê³¼ì†Œ ëŒ€í‘œë˜ëŠ” í…ìŠ¤íŠ¸ ë„ë©”ì¸ì„ ë³´ë‹¤ ë™ë“±í•˜ê²Œ í¬í•¨í•  ìˆ˜ ìˆë‹¤.</p>
</div>
<div id="S2.SS4.SSS0.Px3.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS4.SSS0.Px3.p2.1">ìš°ë¦¬ëŠ” ìµœìƒì˜ ì„±ëŠ¥ì„ ìœ„í•´ OLMo-7Bë¥¼ ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµí•  ë¿ë§Œ ì•„ë‹ˆë¼ ë” í’ë¶€í•˜ê³  í†µì œëœ ê³¼í•™ì  í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë°©ë²•ì„ ì…ì¦í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. OLMo-7BëŠ” ë³µì¡ì„± í‰ê°€ë¥¼ ìœ„í•œ ëª…ì‹œì  ì˜¤ì—¼ ì œê±°ê°€ ìˆëŠ” ê°€ì¥ í° LMì´ë‹¤. íŒ”ë¡œë§ˆì—ì„œ ì„¤ëª…í•œ ì ‘ê·¼ë²•ì— ë”°ë¼ íŒ”ë¡œë§ˆ í‰ê°€ ë°ì´í„°ì—ì„œ ë‹¨ë½ì´ ìœ ì¶œëœ ì‚¬ì „ í›ˆë ¨ ë¬¸ì„œë¥¼ ì œê±°í•œë‹¤. ì˜¤ì—¼ ì œê±° ì—†ì´ ë‹¤ë¥¸ ëª¨ë¸ì€ ë³µì¡ì„±ì„ ê³¼ì†Œí‰ê°€í•  ìœ„í—˜ì´ ìˆë‹¤(ì¦‰, ëª¨ë¸ì˜ í‘œë³¸ ì™¸ ì í•©ë„ë¥¼ ê³¼ëŒ€í‰ê°€). ë˜í•œ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¦´ë¦¬ìŠ¤í•˜ì—¬ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¦´ë¦¬ìŠ¤í•˜ëŠ” ë‘ ê°€ì§€ ë‹¤ë¥¸ ëª¨ë¸ì¸ í”¼í‹°ì•„-6.9B <cite class="ltx_cite ltx_citemacro_citep">(Biderman etÂ al., <a class="ltx_ref" href="#bib.bib7" title="">2023</a>)</cite> ë° RPJ-INCITE-7B <cite class="ltx_cite ltx_citemacro_citep">(Together Computer, <a class="ltx_ref" href="#bib.bib76" title="">2023</a>)</cite>ì™€ ë” í’ë¶€í•œ ë¹„êµë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤(ê·¸ë¦¼ <a class="ltx_ref" href="#S4.F2" title="Figure 2 â€£ Results â€£ 4.2 Intrinsic language modeling evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> ì°¸ì¡°).</p>
</div>
</section>
<section id="S2.SS4.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Adaptation Evaluation</h4>

<div id="S2.SS4.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS4.SSS0.Px4.p1.1">ë˜í•œ Open Instruct í‰ê°€ ì œí’ˆêµ° <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="#bib.bib83" title="">2023</a>); Ivison etÂ al. (<a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite>ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜ì—… ë¯¸ì„¸ ì¡°ì • ë° DPO êµìœ¡ í›„ OLMoë¥¼ í‰ê°€í•œë‹¤. ìš°ë¦¬ëŠ” ëª¨ë¸ ì±„íŒ… ê¸°ëŠ¥ê³¼ ì•ˆì „ì„±ì— ëŒ€í•œ í‰ê°€ì— ì¤‘ì ì„ ë‘ì–´ OLMoë¥¼ ì¶”ê°€ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” íš¨ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Training OLMo</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">ì´ ì„¹ì…˜ì—ì„œëŠ” ë¶„ì‚° í›ˆë ¨ í”„ë ˆì„ì›Œí¬(SectionÂ <a class="ltx_ref" href="#S3.SS1" title="3.1 Distributed Training Framework â€£ 3 Training OLMo â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.1</span></a>), ìµœì í™”ê¸° ì„¤ì •(SectionÂ <a class="ltx_ref" href="#S3.SS2" title="3.2 Optimizer â€£ 3 Training OLMo â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>), ë°ì´í„° ì¤€ë¹„(SectionÂ <a class="ltx_ref" href="#S3.SS3" title="3.3 Data â€£ 3 Training OLMo â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.3</span></a>), í•˜ë“œì›¨ì–´(SectionÂ <a class="ltx_ref" href="#S3.SS4" title="3.4 Hardware â€£ 3 Training OLMo â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.4</span></a>)ë¥¼ í¬í•¨í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ ì„¤ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Distributed Training Framework</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.1">PyTorchì˜ <span class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">ZeRO</em> optimizer strategy <cite class="ltx_cite ltx_citemacro_citep">(Rajbhandari etÂ al., <a class="ltx_ref" href="#bib.bib64" title="">2019</a>)</cite> via PyTorchì˜ <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.2">FSDP</span> framework <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a class="ltx_ref" href="#bib.bib93" title="">2023</a>)</cite>ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤. 7B ê·œëª¨ì—ì„œ ì´ëŠ” í•˜ë“œì›¨ì–´ì—ì„œ GPUë‹¹ 4096 í† í°ì˜ ë§ˆì´í¬ë¡œ ë°°ì¹˜ í¬ê¸°ë¡œ í›ˆë ¨ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤(ì„¹ì…˜ <a class="ltx_ref" href="#S3.SS4" title="3.4 Hardware â€£ 3 Training OLMo â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.4</span></a> ì°¸ì¡°). OLMo-1B ë° -7B ëª¨ë¸ì˜ ê²½ìš°, ìš°ë¦¬ëŠ” ëŒ€ëµ 4M í† í°ì˜ ì¼ì •í•œ ê¸€ë¡œë²Œ ë°°ì¹˜ í¬ê¸°(ê°ê° 2048 í† í°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê°–ëŠ” 2048 ì¸ìŠ¤í„´ìŠ¤)ë¥¼ ì‚¬ìš©í•œë‹¤. OLMo-65B ëª¨ë¸(í˜„ì¬ íŠ¸ë ˆì´ë‹)ì˜ ê²½ìš°, ëŒ€ëµ 2M í† í°(1024 ì¸ìŠ¤í„´ìŠ¤)ì—ì„œ ì‹œì‘í•˜ì—¬ ëŒ€ëµ 16M í† í°(8192 ì¸ìŠ¤í„´ìŠ¤)ì— ë„ë‹¬í•  ë•Œê¹Œì§€ 100B í† í°ë§ˆë‹¤ ë‘ ë°°ë¡œ ì¦ê°€í•˜ëŠ” ë°°ì¹˜ í¬ê¸° ì›Œë°ì—…ì„ ì‚¬ìš©í•œë‹¤.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p2.1">ì²˜ë¦¬ëŸ‰ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.1">FSDP</span>ì˜ ê¸°ë³¸ ì œê³µ ì„¤ì •ê³¼ PyTorchì˜ <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.2">amp</span> ëª¨ë“ˆì„ í†µí•´ í˜¼í•© ì •ë°€ í›ˆë ¨ <cite class="ltx_cite ltx_citemacro_citep">(Micikevicius etÂ al., <a class="ltx_ref" href="#bib.bib46" title="">2017</a>)</cite>ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í›„ìëŠ” ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ softmaxì™€ ê°™ì€ íŠ¹ì • ì‘ì—…ì´ í•­ìƒ ì™„ì „í•œ ì •ë°€ë„ë¡œ ì‹¤í–‰ë˜ëŠ” ë°˜ë©´, ë‹¤ë¥¸ ëª¨ë“  ì‘ì—…ì€ <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.3">bfloat16</span> í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì ˆë°˜ ì •ë°€ë„ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. íŠ¹ì • ì„¤ì •ì—ì„œ ê° GPUì— ë¡œì»¬ì¸ ìƒ¤ë”©ëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ìµœì í™”ê¸° ìƒíƒœëŠ” ì™„ì „í•œ ì •ë°€ë„ë¡œ ìœ ì§€ë©ë‹ˆë‹¤. ê° ë³€ì••ê¸° ë¸”ë¡ ë‚´ì˜ ê°€ì¤‘ì¹˜ëŠ” ìˆœë°©í–¥ ë° ì—­ë°©í–¥ íŒ¨ìŠ¤ ë™ì•ˆ ì „ì²´ í¬ê¸° ë§¤ê°œ ë³€ìˆ˜ê°€ ê° GPUì—ì„œ êµ¬ì²´í™”ë  ë•Œ <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.4">bfloat16</span>ì—ë§Œ ìºìŠ¤íŒ…ë©ë‹ˆë‹¤. GpU ì „ì²´ì—ì„œ ê¸°ìš¸ê¸°ê°€ ì™„ì „íˆ ê°ì†Œí•©ë‹ˆë‹¤.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Optimizer</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.3">í‘œ <a class="ltx_ref" href="#S3.T4" title="Table 4 â€£ 3.3 Data â€£ 3 Training OLMo â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>ì— í‘œì‹œëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ í•¨ê»˜ AdamW Optimizer <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="#bib.bib41" title="">2019</a>)</cite>ë¥¼ ì‚¬ìš©í•œë‹¤. ëª¨ë“  ëª¨ë¸ í¬ê¸°ì— ëŒ€í•´ 5000 ë‹¨ê³„(<math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\sim</annotation></semantics></math>21B í† í°)ì— ê±¸ì³ í•™ìŠµ ì†ë„ë¥¼ ì˜ˆì—´í•œ ë‹¤ìŒ ë‚˜ë¨¸ì§€ í›ˆë ¨ì— ê±¸ì³ ìµœëŒ€ í•™ìŠµ ì†ë„ì˜ 10ë¶„ì˜ 1ê¹Œì§€ ì„ í˜•ìœ¼ë¡œ ê°ì‡ í•œë‹¤. ì›Œë°ì—… ê¸°ê°„ í›„ì—, íŒŒë¼ë¯¸í„° ê·¸ë˜ë””ì–¸íŠ¸ë“¤ì˜ ì´ <math alttext="l^{2}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">l</mi><mn id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ‘™</ci><cn id="S3.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">l^{2}</annotation></semantics></math>-normì´ <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span> ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ë™ì•ˆ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë“¤ì´ ëª¨ë‘ ë‹¨ì¼ ë¹… ë²¡í„°ë¡œ ì²˜ë¦¬ë˜ê³ (ëª¨ë“  íŒŒë¼ë¯¸í„°ë“¤ì´ í•¨ê»˜ í‰íƒ„í™”ë˜ê³  ì—°ì‡„ë˜ëŠ” ê²ƒì²˜ëŸ¼), í•´ë‹¹ ë‹¨ì¼ ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„°ì— ëŒ€í•´ <math alttext="\ell_{2}" class="ltx_Math" display="inline" id="footnote3.m1.1"><semantics id="footnote3.m1.1b"><msub id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml"><mi id="footnote3.m1.1.1.2" mathvariant="normal" xref="footnote3.m1.1.1.2.cmml">â„“</mi><mn id="footnote3.m1.1.1.3" xref="footnote3.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><apply id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1"><csymbol cd="ambiguous" id="footnote3.m1.1.1.1.cmml" xref="footnote3.m1.1.1">subscript</csymbol><ci id="footnote3.m1.1.1.2.cmml" xref="footnote3.m1.1.1.2">â„“</ci><cn id="footnote3.m1.1.1.3.cmml" type="integer" xref="footnote3.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">\ell_{2}</annotation></semantics></math>-normì„ ì·¨í•œë‹¤. ì´ê²ƒì´ PyTorchì˜ ê¸°ìš¸ê¸°ë¥¼ ìë¥´ëŠ” í‘œì¤€ ë°©ë²•ì…ë‹ˆë‹¤. </span></span></span>ì€ <math alttext="1.0" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn id="S3.SS2.p1.3.m3.1.1.cmml" type="float" xref="S3.SS2.p1.3.m3.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">1.0</annotation></semantics></math>ë¥¼ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ”ë‹¤. í‘œ <a class="ltx_ref" href="#S3.T5" title="Table 5 â€£ 3.3 Data â€£ 3 Training OLMo â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>ëŠ” 7B ì²™ë„ì˜ ìµœì í™”ê¸° ì„¤ì •ì„ AdamWë¥¼ ì‚¬ìš©í•œ ë‹¤ë¥¸ ìµœê·¼ LMì˜ ì„¤ì •ê³¼ ë¹„êµí–ˆë‹¤.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p1.1">ê³µê°œ ë°ì´í„° ì„¸íŠ¸ì¸ Dolma <cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2024</a>)</cite>ì—ì„œ 2T í† í° ìƒ˜í”Œë¡œ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ë¥¼ êµ¬ì¶•í–ˆìœ¼ë©°, ì„¹ì…˜ <a class="ltx_ref" href="#S2.SS2" title="2.2 Pretraining Data: Dolma â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.2</span></a>ì—ì„œ ì„¤ëª…í•œë‹¤. ëª¨ë“  ë¬¸ì„œì˜ í† í°ì€ íŠ¹ìˆ˜ <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.1">EOS</span> í† í°ì„ ê° ë¬¸ì„œì˜ ëì— ì¶”ê°€í•œ í›„ í•¨ê»˜ ì—°ê²°ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ 2048 í† í°ì˜ ì—°ì† ì²­í¬ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ í›ˆë ¨ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í˜•ì„±í•©ë‹ˆë‹¤. íŠ¸ë ˆì´ë‹ ì¸ìŠ¤í„´ìŠ¤ë“¤ì€ ê°ê°ì˜ íŠ¸ë ˆì´ë‹ ì‹¤í–‰ì— ëŒ€í•´ ì •í™•íˆ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì…”í”Œë§ëœë‹¤. ê° í›ˆë ¨ ë°°ì¹˜ì˜ ë°ì´í„° ìˆœì„œì™€ ì •í™•í•œ êµ¬ì„±ì€ ìš°ë¦¬ê°€ ë°©ì¶œí•˜ëŠ” ì•„í‹°íŒ©íŠ¸ë¡œë¶€í„° ì¬êµ¬ì„±ë  ìˆ˜ ìˆë‹¤.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS3.p2.1">ê³µê°œëœ ëª¨ë“  ëª¨ë¸ì€ ìµœì†Œ 2T í† í°(í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ë‹¨ì¼ ì—í¬í¬)ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìœ¼ë©° ì¼ë¶€ëŠ” ë‹¤ë¥¸ ì…”í”Œë§ ìˆœì„œë¡œ ë°ì´í„°ì— ëŒ€í•œ ë‘ ë²ˆì§¸ ì—í¬í¬ë¥¼ ì‹œì‘í•˜ì—¬ ê·¸ ì´ìƒìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆë‹¤. ì´ ì†ŒëŸ‰ì˜ ë°ì´í„°ë¥¼ ë°˜ë³µí•˜ëŠ” ì˜í–¥ì€ ì´ì „ ì‘ì—… <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff etÂ al., <a class="ltx_ref" href="#bib.bib51" title="">2023</a>)</cite>ì— ë”°ë¼ ë¬´ì‹œí•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.9.10.1" class="ltx_tr">
<th id="S3.T4.9.10.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.9.10.1.1.1" class="ltx_text ltx_font_bold">Size</span></th>
<th id="S3.T4.9.10.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.9.10.1.2.1" class="ltx_text ltx_font_bold">Peak LR</span></th>
<th id="S3.T4.9.10.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.9.10.1.3.1" class="ltx_text ltx_font_bold">Betas</span></th>
<th id="S3.T4.9.10.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.9.10.1.4.1" class="ltx_text ltx_font_bold">Epsilon</span></th>
<th id="S3.T4.9.10.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.9.10.1.5.1" class="ltx_text ltx_font_bold">Weight Decay</span></th>
<th id="S3.T4.9.10.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.9.10.1.6.1" class="ltx_text ltx_font_bold">Batch Size (tokens)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.1" class="ltx_tr">
<th id="S3.T4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">1B</th>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.1.1.3.1" class="ltx_text ltx_font_typewriter">4.0E-4</span></td>
<td id="S3.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">(0.9, 0.95)</td>
<td id="S3.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.1.1.5.1" class="ltx_text ltx_font_typewriter">1.0E-5</span></td>
<td id="S3.T4.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">0.1</td>
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S3.T4.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T4.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.m1.1.1" xref="S3.T4.1.1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T4.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.m1.1c">\sim</annotation></semantics></math>4M</td>
</tr>
<tr id="S3.T4.2.2" class="ltx_tr">
<th id="S3.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">7B</th>
<td id="S3.T4.2.2.3" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.2.2.3.1" class="ltx_text ltx_font_typewriter">3.0E-4</span></td>
<td id="S3.T4.2.2.4" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">(0.9, 0.95)</td>
<td id="S3.T4.2.2.5" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.2.2.5.1" class="ltx_text ltx_font_typewriter">1.0E-5</span></td>
<td id="S3.T4.2.2.6" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">0.1</td>
<td id="S3.T4.2.2.1" class="ltx_td ltx_align_center" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S3.T4.2.2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T4.2.2.1.m1.1a"><mo id="S3.T4.2.2.1.m1.1.1" xref="S3.T4.2.2.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.1.m1.1b"><csymbol cd="latexml" id="S3.T4.2.2.1.m1.1.1.cmml" xref="S3.T4.2.2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.1.m1.1c">\sim</annotation></semantics></math>4M</td>
</tr>
<tr id="S3.T4.9.9" class="ltx_tr">
<th id="S3.T4.9.9.8" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">65B*</th>
<td id="S3.T4.9.9.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.9.9.9.1" class="ltx_text ltx_font_typewriter">1.5E-4</span></td>
<td id="S3.T4.9.9.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">(0.9, 0.95)</td>
<td id="S3.T4.9.9.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T4.9.9.11.1" class="ltx_text ltx_font_typewriter">1.0E-5</span></td>
<td id="S3.T4.9.9.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">0.1</td>
<td id="S3.T4.9.9.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S3.T4.3.3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T4.3.3.1.m1.1a"><mo id="S3.T4.3.3.1.m1.1.1" xref="S3.T4.3.3.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.1.m1.1b"><csymbol cd="latexml" id="S3.T4.3.3.1.m1.1.1.cmml" xref="S3.T4.3.3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.1.m1.1c">\sim</annotation></semantics></math>2M <math id="S3.T4.4.4.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T4.4.4.2.m2.1a"><mo stretchy="false" id="S3.T4.4.4.2.m2.1.1" xref="S3.T4.4.4.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.2.m2.1b"><ci id="S3.T4.4.4.2.m2.1.1.cmml" xref="S3.T4.4.4.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.2.m2.1c">\rightarrow</annotation></semantics></math> <math id="S3.T4.5.5.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T4.5.5.3.m3.1a"><mo id="S3.T4.5.5.3.m3.1.1" xref="S3.T4.5.5.3.m3.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.3.m3.1b"><csymbol cd="latexml" id="S3.T4.5.5.3.m3.1.1.cmml" xref="S3.T4.5.5.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.3.m3.1c">\sim</annotation></semantics></math>4M <math id="S3.T4.6.6.4.m4.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T4.6.6.4.m4.1a"><mo stretchy="false" id="S3.T4.6.6.4.m4.1.1" xref="S3.T4.6.6.4.m4.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T4.6.6.4.m4.1b"><ci id="S3.T4.6.6.4.m4.1.1.cmml" xref="S3.T4.6.6.4.m4.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.6.4.m4.1c">\rightarrow</annotation></semantics></math> <math id="S3.T4.7.7.5.m5.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T4.7.7.5.m5.1a"><mo id="S3.T4.7.7.5.m5.1.1" xref="S3.T4.7.7.5.m5.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.7.7.5.m5.1b"><csymbol cd="latexml" id="S3.T4.7.7.5.m5.1.1.cmml" xref="S3.T4.7.7.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.7.5.m5.1c">\sim</annotation></semantics></math>8M <math id="S3.T4.8.8.6.m6.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T4.8.8.6.m6.1a"><mo stretchy="false" id="S3.T4.8.8.6.m6.1.1" xref="S3.T4.8.8.6.m6.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T4.8.8.6.m6.1b"><ci id="S3.T4.8.8.6.m6.1.1.cmml" xref="S3.T4.8.8.6.m6.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.8.8.6.m6.1c">\rightarrow</annotation></semantics></math> <math id="S3.T4.9.9.7.m7.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.T4.9.9.7.m7.1a"><mo id="S3.T4.9.9.7.m7.1.1" xref="S3.T4.9.9.7.m7.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.T4.9.9.7.m7.1b"><csymbol cd="latexml" id="S3.T4.9.9.7.m7.1.1.cmml" xref="S3.T4.9.9.7.m7.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.9.9.7.m7.1c">\sim</annotation></semantics></math>16M</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">í‘œ 4:</span>AdamW pretraining hyperparameters for OLMo models.</figcaption>
<br class="ltx_break">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
* <em id="S3.T4.11.1" class="ltx_emph ltx_font_italic">At the time of writing our 65B model is still training.</em></figcaption>
</figure>
<figure id="S3.T5" class="ltx_table">
<table id="S3.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S3.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">OLMo-7B</span></th>
<th id="S3.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">LLaMA2-7B</span></th>
<th id="S3.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T5.1.1.1.4.1" class="ltx_text ltx_font_bold">OpenLM-7B</span></th>
<th id="S3.T5.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T5.1.1.1.5.1" class="ltx_text ltx_font_bold">Falcon-7B</span></th>
</tr>
<tr id="S3.T5.1.2.2" class="ltx_tr">
<th id="S3.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">warmup steps</th>
<th id="S3.T5.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">5000</th>
<th id="S3.T5.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">2000</th>
<th id="S3.T5.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">2000</th>
<th id="S3.T5.1.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">1000</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.1.3.1" class="ltx_tr">
<th id="S3.T5.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">peak LR</th>
<td id="S3.T5.1.3.1.2" class="ltx_td ltx_align_left"><span id="S3.T5.1.3.1.2.1" class="ltx_text ltx_font_typewriter">3.0E-04</span></td>
<td id="S3.T5.1.3.1.3" class="ltx_td ltx_align_left"><span id="S3.T5.1.3.1.3.1" class="ltx_text ltx_font_typewriter">3.0E-04</span></td>
<td id="S3.T5.1.3.1.4" class="ltx_td ltx_align_left"><span id="S3.T5.1.3.1.4.1" class="ltx_text ltx_font_typewriter">3.0E-04</span></td>
<td id="S3.T5.1.3.1.5" class="ltx_td ltx_align_left"><span id="S3.T5.1.3.1.5.1" class="ltx_text ltx_font_typewriter">6.0E-04</span></td>
</tr>
<tr id="S3.T5.1.4.2" class="ltx_tr">
<th id="S3.T5.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">minimum LR</th>
<td id="S3.T5.1.4.2.2" class="ltx_td ltx_align_left"><span id="S3.T5.1.4.2.2.1" class="ltx_text ltx_font_typewriter">3.0E-05</span></td>
<td id="S3.T5.1.4.2.3" class="ltx_td ltx_align_left"><span id="S3.T5.1.4.2.3.1" class="ltx_text ltx_font_typewriter">3.0E-05</span></td>
<td id="S3.T5.1.4.2.4" class="ltx_td ltx_align_left"><span id="S3.T5.1.4.2.4.1" class="ltx_text ltx_font_typewriter">3.0E-05</span></td>
<td id="S3.T5.1.4.2.5" class="ltx_td ltx_align_left"><span id="S3.T5.1.4.2.5.1" class="ltx_text ltx_font_typewriter">1.2E-05</span></td>
</tr>
<tr id="S3.T5.1.5.3" class="ltx_tr">
<th id="S3.T5.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">weight decay</th>
<td id="S3.T5.1.5.3.2" class="ltx_td ltx_align_left">0.1</td>
<td id="S3.T5.1.5.3.3" class="ltx_td ltx_align_left">0.1</td>
<td id="S3.T5.1.5.3.4" class="ltx_td ltx_align_left">0.1</td>
<td id="S3.T5.1.5.3.5" class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr id="S3.T5.1.6.4" class="ltx_tr">
<th id="S3.T5.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">beta1</th>
<td id="S3.T5.1.6.4.2" class="ltx_td ltx_align_left">0.9</td>
<td id="S3.T5.1.6.4.3" class="ltx_td ltx_align_left">0.9</td>
<td id="S3.T5.1.6.4.4" class="ltx_td ltx_align_left">0.9</td>
<td id="S3.T5.1.6.4.5" class="ltx_td ltx_align_left">0.99</td>
</tr>
<tr id="S3.T5.1.7.5" class="ltx_tr">
<th id="S3.T5.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">beta2</th>
<td id="S3.T5.1.7.5.2" class="ltx_td ltx_align_left">0.95</td>
<td id="S3.T5.1.7.5.3" class="ltx_td ltx_align_left">0.95</td>
<td id="S3.T5.1.7.5.4" class="ltx_td ltx_align_left">0.95</td>
<td id="S3.T5.1.7.5.5" class="ltx_td ltx_align_left">0.999</td>
</tr>
<tr id="S3.T5.1.8.6" class="ltx_tr">
<th id="S3.T5.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">epsilon</th>
<td id="S3.T5.1.8.6.2" class="ltx_td ltx_align_left"><span id="S3.T5.1.8.6.2.1" class="ltx_text ltx_font_typewriter">1.0E-05</span></td>
<td id="S3.T5.1.8.6.3" class="ltx_td ltx_align_left"><span id="S3.T5.1.8.6.3.1" class="ltx_text ltx_font_typewriter">1.0E-05</span></td>
<td id="S3.T5.1.8.6.4" class="ltx_td ltx_align_left"><span id="S3.T5.1.8.6.4.1" class="ltx_text ltx_font_typewriter">1.0E-05</span></td>
<td id="S3.T5.1.8.6.5" class="ltx_td ltx_align_left"><span id="S3.T5.1.8.6.5.1" class="ltx_text ltx_font_typewriter">1.0E-05</span></td>
</tr>
<tr id="S3.T5.1.9.7" class="ltx_tr">
<th id="S3.T5.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">LR schedule</th>
<td id="S3.T5.1.9.7.2" class="ltx_td ltx_align_left">linear</td>
<td id="S3.T5.1.9.7.3" class="ltx_td ltx_align_left">cosine</td>
<td id="S3.T5.1.9.7.4" class="ltx_td ltx_align_left">cosine</td>
<td id="S3.T5.1.9.7.5" class="ltx_td ltx_align_left">cosine</td>
</tr>
<tr id="S3.T5.1.10.8" class="ltx_tr">
<th id="S3.T5.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">gradient clipping</th>
<td id="S3.T5.1.10.8.2" class="ltx_td ltx_align_left">global 1.0</td>
<td id="S3.T5.1.10.8.3" class="ltx_td ltx_align_left">global 1.0</td>
<td id="S3.T5.1.10.8.4" class="ltx_td ltx_align_left">global 1.0</td>
<td id="S3.T5.1.10.8.5" class="ltx_td ltx_align_left">global 1.0</td>
</tr>
<tr id="S3.T5.1.11.9" class="ltx_tr">
<th id="S3.T5.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">gradient reduce dtype</th>
<td id="S3.T5.1.11.9.2" class="ltx_td ltx_align_left">FP32</td>
<td id="S3.T5.1.11.9.3" class="ltx_td ltx_align_left">FP32</td>
<td id="S3.T5.1.11.9.4" class="ltx_td ltx_align_left">FP32</td>
<td id="S3.T5.1.11.9.5" class="ltx_td ltx_align_left">BF16</td>
</tr>
<tr id="S3.T5.1.12.10" class="ltx_tr">
<th id="S3.T5.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">optimizer state dtype</th>
<td id="S3.T5.1.12.10.2" class="ltx_td ltx_align_left ltx_border_bb">FP32</td>
<td id="S3.T5.1.12.10.3" class="ltx_td ltx_align_left ltx_border_bb">most likely FP32</td>
<td id="S3.T5.1.12.10.4" class="ltx_td ltx_align_left ltx_border_bb">FP32</td>
<td id="S3.T5.1.12.10.5" class="ltx_td ltx_align_left ltx_border_bb">FP32</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">í‘œ 5:</span>7B ìŠ¤ì¼€ì¼ì—ì„œ í”„ë¦¬íŠ¸ë ˆì´ë‹ ìµœì í™”ê¸° ì„¤ì •ì˜ ë¹„êµ. ì´ í‘œì˜ ê° ëª¨ë¸ì€ AdamWë¥¼ ìµœì í™”ê¸°ë¡œ ì‚¬ìš©í–ˆë‹¤.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Hardware</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS4.p1.1">ì„±ëŠ¥ ì†ì‹¤ ì—†ì´ ì½”ë“œë² ì´ìŠ¤ê°€ NVIDIA ë° AMD GPU ëª¨ë‘ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ë‘ ê°œì˜ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì—ì„œ ëª¨ë¸ì„ í›ˆë ¨í–ˆë‹¤.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">LUMI:</span> Under the LUMI supercomputer,<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lumi-supercomputer.eu" target="_blank" title="">https://www.lumi-supercomputer.eu</a></span></span></span> We used to to 256 nodes on this cluster, where each node consists a 4x AMD MI250X GPU with 128GB of memory<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>The MI250X is a dual-chip module, meaning in practice that each physical device consists of two logical devices, so each node has 8 logical GPU devices with 64GB of memory each.</span></span></span> and 800Gbps of interconnect.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">MosaicML:</span> Provided by MosaicML<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mosaicml.com" target="_blank" title="">https://www.mosaicml.com</a></span></span></span> (Databricks)ì„ ì‚¬ìš©í–ˆìœ¼ë©°, ê° ë…¸ë“œëŠ” 40GBì˜ ë©”ëª¨ë¦¬ì™€ 800Gbps ì¸í„°ì»¤ë„¥íŠ¸ë¥¼ ê°€ì§„ 8x NVIDIA A100 GPUë¡œ êµ¬ì„±ëœë‹¤.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS4.p1.2">í›ˆë ¨ ì²˜ë¦¬ëŸ‰ì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ë°°ì¹˜ í¬ê¸°ì˜ ì‚¬ì†Œí•œ ì°¨ì´ì—ë„ ë¶ˆêµ¬í•˜ê³  ë‘ ì‹¤í–‰ ëª¨ë‘ 2T í† í°ì— ì˜í•œ í‰ê°€ ì œí’ˆêµ°ì—ì„œ ê±°ì˜ ë™ì¼í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">OLMo-7Bë¥¼ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ëŠ” ì„¹ì…˜ <a class="ltx_ref" href="#S3.SS2" title="3.2 Optimizer â€£ 3 Training OLMo â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>ì—ì„œ ì–¸ê¸‰ëœ ì„ í˜• í•™ìŠµ ì†ë„ ê°ì‡  ì¼ì •ì´ ìˆëŠ” Dolma <cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2024</a>)</cite> ë°ì´í„° ì„¸íŠ¸ì—ì„œ 2.46T í† í°ì´ ë  ë•Œê¹Œì§€ í›ˆë ¨ëœë‹¤. ì‹¤í—˜ì—ì„œ í•™ìŠµ ì†ë„ê°€ ì„ í˜•ì ìœ¼ë¡œ 0ìœ¼ë¡œ ê°ì‡ ëœ 1000ë‹¨ê³„ ë™ì•ˆ ëŒë§ˆ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì¶”ê°€ë¡œ ì¡°ì •í•˜ë©´ ì„¹ì…˜ <a class="ltx_ref" href="#S2.SS4" title="2.4 Evaluation â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>ì— ì„¤ëª…ëœ ë³µì¡ì„± ë° ìµœì¢… ì‘ì—… í‰ê°€ ìŠ¤ìœ„íŠ¸ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤. OLMoë¥¼ LLaMA-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="#bib.bib77" title="">2023a</a>)</cite>, LLaMA2-7B <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="#bib.bib78" title="">2023b</a>)</cite>, MPT-7B <cite class="ltx_cite ltx_citemacro_citep">(MosaicML NLP Team, <a class="ltx_ref" href="#bib.bib50" title="">2023</a>)</cite>, Pythia-6.9B <cite class="ltx_cite ltx_citemacro_citep">(Biderman etÂ al., <a class="ltx_ref" href="#bib.bib7" title="">2023</a>)</cite>, Falcon-7B <cite class="ltx_cite ltx_citemacro_citep">(Almazrouei etÂ al., <a class="ltx_ref" href="#bib.bib2" title="">2023</a>)</cite> ë° RPJ-INCITE-7B <cite class="ltx_cite ltx_citemacro_citep">(Together Computer, <a class="ltx_ref" href="#bib.bib76" title="">2023</a>)</cite> ë“± ë‹¤ë¥¸ ê³µê°œ ê°€ëŠ¥í•œ ëª¨ë¸ê³¼ ë¹„êµí•œë‹¤.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Downstream evaluation</h3>

<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">ìš°ë¦¬ì˜ í•µì‹¬ <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px1.p1.1.1">downstream í‰ê°€ suite</span> (í‘œ ì°¸ì¡°)ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¶€ë¡ <a class="ltx_ref" href="#A1" title="Appendix A Additional Evaluation â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>ì—ì„œëŠ” ì½”ì–´ í‰ê°€ ì„¸íŠ¸ ì™¸ë¶€ì˜ ì¶”ê°€ ë³´ì¡° ì‘ì—… ì„¸íŠ¸ì— ëŒ€í•œ ê²°ê³¼ë„ ë³´ê³ í•˜ì—¬ ì•ˆì •ì ì¸ ì„±ëŠ¥ ê²½í–¥ì´ ëœí•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤(ê·¸ë¦¼ <a class="ltx_ref" href="#A1.F4" title="Figure 4 â€£ Additional end-task results â€£ Appendix A Additional Evaluation â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> ì°¸ì¡°).</p>
</div>
<div id="S4.SS1.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p2.1">ëª¨ë“  ê²½ìš°ì—, <cite class="ltx_cite ltx_citemacro_citet">Brown etÂ al. (<a class="ltx_ref" href="#bib.bib12" title="">2020</a>)</cite>ì— ì˜í•´ ëŒ€ì¤‘í™”ëœ ìˆœìœ„ ë¶„ë¥˜ ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì œë¡œ ìƒ· í‰ê°€ë¥¼ ìˆ˜í–‰í•œë‹¤. ì´ ì ‘ê·¼ë²• í•˜ì—ì„œ, í›„ë³´ í…ìŠ¤íŠ¸ ì™„ì„±ë“¤(ì˜ˆë¥¼ ë“¤ì–´, ìƒì´í•œ ê°ê´€ì‹ ì˜µì…˜ë“¤)ì€ ìš°ë„(ì¼ë°˜ì ìœ¼ë¡œ ì¼ë¶€ ì •ê·œí™” ì¸ìì— ì˜í•´ ì •ê·œí™”ë¨)ì— ì˜í•´ ìˆœìœ„ê°€ ë§¤ê²¨ì§€ê³ , ì˜ˆì¸¡ ì •í™•ë„ê°€ ë³´ê³ ëœë‹¤. CatwalkëŠ” í† í° ìˆ˜(í† í°ë‹¹ ì •ê·œí™”) <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="#bib.bib12" title="">2020</a>; Liang etÂ al., <a class="ltx_ref" href="#bib.bib37" title="">2022</a>)</cite>, ë¬¸ì ìˆ˜(ë¬¸ìë‹¹ ì •ê·œí™”) <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="#bib.bib26" title="">2023</a>)</cite>, ë‹µë³€ì˜ ë¬´ì¡°ê±´ ìš°ë„ <cite idx=3></cite)ë¥¼ í†µí•©í•˜ì—¬ ê° ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ ì •ê·œí™” ì „ëµì„ ë³„ë„ë¡œ ì„ íƒí–ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.1">arc</span> ë° <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.2">openbookqa</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.3">hellaswag</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.4">piqa</span> ë° <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p2.1.5">winogrande</span></p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">7B Models</span></th>
<th id="S4.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T6.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T6.1.1.1.2.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">arc</td>
</tr>
<tr id="S4.T6.1.1.1.2.1.2" class="ltx_tr">
<td id="S4.T6.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">challenge</td>
</tr>
</tbody></table>
</th>
<th id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T6.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T6.1.1.1.3.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">arc</td>
</tr>
<tr id="S4.T6.1.1.1.3.1.2" class="ltx_tr">
<td id="S4.T6.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">easy</td>
</tr>
</tbody></table>
</th>
<th id="S4.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">boolq</th>
<th id="S4.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T6.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T6.1.1.1.5.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">hella-</td>
</tr>
<tr id="S4.T6.1.1.1.5.1.2" class="ltx_tr">
<td id="S4.T6.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">swag</td>
</tr>
</tbody></table>
</th>
<th id="S4.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T6.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T6.1.1.1.6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">open</td>
</tr>
<tr id="S4.T6.1.1.1.6.1.2" class="ltx_tr">
<td id="S4.T6.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">bookqa</td>
</tr>
</tbody></table>
</th>
<th id="S4.T6.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">piqa</th>
<th id="S4.T6.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">sciq</th>
<th id="S4.T6.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<table id="S4.T6.1.1.1.9.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T6.1.1.1.9.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1.9.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">wino-</td>
</tr>
<tr id="S4.T6.1.1.1.9.1.2" class="ltx_tr">
<td id="S4.T6.1.1.1.9.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">grande</td>
</tr>
</tbody></table>
</th>
<th id="S4.T6.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">avg.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.1.2.1" class="ltx_tr">
<th id="S4.T6.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T6.1.2.1.1.1" class="ltx_text ltx_font_bold">Falcon</span></th>
<td id="S4.T6.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">47.5</td>
<td id="S4.T6.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">70.4</td>
<td id="S4.T6.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">74.6</td>
<td id="S4.T6.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">75.9</td>
<td id="S4.T6.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">53.0</td>
<td id="S4.T6.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">78.5</td>
<td id="S4.T6.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">93.9</td>
<td id="S4.T6.1.2.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.9</td>
<td id="S4.T6.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">70.3</td>
</tr>
<tr id="S4.T6.1.3.2" class="ltx_tr">
<th id="S4.T6.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.1.3.2.1.1" class="ltx_text ltx_font_bold">LLaMA</span></th>
<td id="S4.T6.1.3.2.2" class="ltx_td ltx_align_center">44.5</td>
<td id="S4.T6.1.3.2.3" class="ltx_td ltx_align_center">67.9</td>
<td id="S4.T6.1.3.2.4" class="ltx_td ltx_align_center">75.4</td>
<td id="S4.T6.1.3.2.5" class="ltx_td ltx_align_center">76.2</td>
<td id="S4.T6.1.3.2.6" class="ltx_td ltx_align_center">51.2</td>
<td id="S4.T6.1.3.2.7" class="ltx_td ltx_align_center">77.2</td>
<td id="S4.T6.1.3.2.8" class="ltx_td ltx_align_center">93.9</td>
<td id="S4.T6.1.3.2.9" class="ltx_td ltx_align_center ltx_border_r">70.5</td>
<td id="S4.T6.1.3.2.10" class="ltx_td ltx_align_center">69.6</td>
</tr>
<tr id="S4.T6.1.4.3" class="ltx_tr">
<th id="S4.T6.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.1.4.3.1.1" class="ltx_text ltx_font_bold">Llama 2</span></th>
<td id="S4.T6.1.4.3.2" class="ltx_td ltx_align_center">48.5</td>
<td id="S4.T6.1.4.3.3" class="ltx_td ltx_align_center">69.5</td>
<td id="S4.T6.1.4.3.4" class="ltx_td ltx_align_center">80.2</td>
<td id="S4.T6.1.4.3.5" class="ltx_td ltx_align_center">76.8</td>
<td id="S4.T6.1.4.3.6" class="ltx_td ltx_align_center">48.4</td>
<td id="S4.T6.1.4.3.7" class="ltx_td ltx_align_center">76.7</td>
<td id="S4.T6.1.4.3.8" class="ltx_td ltx_align_center">94.5</td>
<td id="S4.T6.1.4.3.9" class="ltx_td ltx_align_center ltx_border_r">69.4</td>
<td id="S4.T6.1.4.3.10" class="ltx_td ltx_align_center">70.5</td>
</tr>
<tr id="S4.T6.1.5.4" class="ltx_tr">
<th id="S4.T6.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.1.5.4.1.1" class="ltx_text ltx_font_bold">MPT</span></th>
<td id="S4.T6.1.5.4.2" class="ltx_td ltx_align_center">46.5</td>
<td id="S4.T6.1.5.4.3" class="ltx_td ltx_align_center">70.5</td>
<td id="S4.T6.1.5.4.4" class="ltx_td ltx_align_center">74.2</td>
<td id="S4.T6.1.5.4.5" class="ltx_td ltx_align_center">77.6</td>
<td id="S4.T6.1.5.4.6" class="ltx_td ltx_align_center">48.6</td>
<td id="S4.T6.1.5.4.7" class="ltx_td ltx_align_center">77.3</td>
<td id="S4.T6.1.5.4.8" class="ltx_td ltx_align_center">93.7</td>
<td id="S4.T6.1.5.4.9" class="ltx_td ltx_align_center ltx_border_r">69.9</td>
<td id="S4.T6.1.5.4.10" class="ltx_td ltx_align_center">69.8</td>
</tr>
<tr id="S4.T6.1.6.5" class="ltx_tr">
<th id="S4.T6.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.1.6.5.1.1" class="ltx_text ltx_font_bold">Pythia</span></th>
<td id="S4.T6.1.6.5.2" class="ltx_td ltx_align_center">44.1</td>
<td id="S4.T6.1.6.5.3" class="ltx_td ltx_align_center">61.9</td>
<td id="S4.T6.1.6.5.4" class="ltx_td ltx_align_center">61.1</td>
<td id="S4.T6.1.6.5.5" class="ltx_td ltx_align_center">63.8</td>
<td id="S4.T6.1.6.5.6" class="ltx_td ltx_align_center">45.0</td>
<td id="S4.T6.1.6.5.7" class="ltx_td ltx_align_center">75.1</td>
<td id="S4.T6.1.6.5.8" class="ltx_td ltx_align_center">91.1</td>
<td id="S4.T6.1.6.5.9" class="ltx_td ltx_align_center ltx_border_r">62.0</td>
<td id="S4.T6.1.6.5.10" class="ltx_td ltx_align_center">63.0</td>
</tr>
<tr id="S4.T6.1.7.6" class="ltx_tr">
<th id="S4.T6.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.1.7.6.1.1" class="ltx_text ltx_font_bold">RPJ-INCITE</span></th>
<td id="S4.T6.1.7.6.2" class="ltx_td ltx_align_center">42.8</td>
<td id="S4.T6.1.7.6.3" class="ltx_td ltx_align_center">68.4</td>
<td id="S4.T6.1.7.6.4" class="ltx_td ltx_align_center">68.6</td>
<td id="S4.T6.1.7.6.5" class="ltx_td ltx_align_center">70.3</td>
<td id="S4.T6.1.7.6.6" class="ltx_td ltx_align_center">49.4</td>
<td id="S4.T6.1.7.6.7" class="ltx_td ltx_align_center">76.0</td>
<td id="S4.T6.1.7.6.8" class="ltx_td ltx_align_center">92.9</td>
<td id="S4.T6.1.7.6.9" class="ltx_td ltx_align_center ltx_border_r">64.7</td>
<td id="S4.T6.1.7.6.10" class="ltx_td ltx_align_center">66.6</td>
</tr>
<tr id="S4.T6.1.8.7" class="ltx_tr" style="background-color:#40C4DF;">
<th id="S4.T6.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T6.1.8.7.1.1" class="ltx_text" style="background-color:#40C4DF;">[] <span id="S4.T6.1.8.7.1.1.1" class="ltx_text ltx_font_bold">OLMo-7B</span></span></th>
<td id="S4.T6.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.8.7.2.1" class="ltx_text" style="background-color:#40C4DF;">48.5</span></td>
<td id="S4.T6.1.8.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.8.7.3.1" class="ltx_text" style="background-color:#40C4DF;">65.4</span></td>
<td id="S4.T6.1.8.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.8.7.4.1" class="ltx_text" style="background-color:#40C4DF;">73.4</span></td>
<td id="S4.T6.1.8.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.8.7.5.1" class="ltx_text" style="background-color:#40C4DF;">76.4</span></td>
<td id="S4.T6.1.8.7.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.8.7.6.1" class="ltx_text" style="background-color:#40C4DF;">50.4</span></td>
<td id="S4.T6.1.8.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.8.7.7.1" class="ltx_text" style="background-color:#40C4DF;">78.4</span></td>
<td id="S4.T6.1.8.7.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.8.7.8.1" class="ltx_text" style="background-color:#40C4DF;">93.8</span></td>
<td id="S4.T6.1.8.7.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T6.1.8.7.9.1" class="ltx_text" style="background-color:#40C4DF;">67.9</span></td>
<td id="S4.T6.1.8.7.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.1.8.7.10.1" class="ltx_text" style="background-color:#40C4DF;">69.3</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">í‘œ 6:</span>Zero-shot evaluation of OLMo-7B and 6 other publicly available comparable model checkpoints on the 8 core tasks from the SectionÂ <a class="ltx_ref" href="#S2.SS4.SSS0.Px2" title="Downstream Evaluation â€£ 2.4 Evaluation â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>. OLMo-7Bì˜ ê²½ìš° 2.46T í† í° ì²´í¬í¬ì¸íŠ¸ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤.</figcaption>
</figure>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">í‘œ <a class="ltx_ref" href="#S4.T6" title="Table 6 â€£ Setup â€£ 4.1 Downstream evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>ëŠ” OLMo-7Bì˜ ì œë¡œìƒ· í‰ê°€ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê³  ë¹„ìŠ·í•œ í¬ê¸°ì˜ ë‹¤ë¥¸ 6ê°œì˜ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ê³¼ ë¹„êµí•œë‹¤. ìš°ë¦¬ëŠ” ì„¹ì…˜ <a class="ltx_ref" href="#S2.SS4.SSS0.Px2" title="Downstream Evaluation â€£ 2.4 Evaluation â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>ì— ì„¤ëª…ëœ í‰ê°€ ì œí’ˆêµ°ì—ì„œ 8ê°œì˜ í•µì‹¬ ì‘ì—…ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤. ì „ì²´ì ìœ¼ë¡œ OLMo-7BëŠ” ë¹„êµ í‘œì—ì„œ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ 6ê°œì˜ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ëª¨ë‘ì— ëŒ€í•´ ê²½ìŸë ¥ì´ ìˆë‹¤.</p>
</div>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p2.1"><a class="ltx_ref" href="#S4.F1" title="Figure 1 â€£ Results â€£ 4.1 Downstream evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>ì—ì„œ ìš°ë¦¬ëŠ” 8ê°œì˜ ì½”ì–´ ì—”ë“œ íƒœìŠ¤í¬ì˜ ì •í™•ë„ ì ìˆ˜ ì§„í–‰ì„ í”Œë¡¯í•œë‹¤. OBQAë¥¼ ì œì™¸í•œ ëª¨ë“  íƒœìŠ¤í¬ëŠ” OLMo-7Bê°€ ë” ë§ì€ í† í°ì— ëŒ€í•´ í›ˆë ¨ë¨ì— ë”°ë¼ ì •í™•ë„ ìˆ˜ì¹˜ì—ì„œ ìƒìŠ¹ ì¶”ì„¸ë¥¼ ë³´ì¸ë‹¤. ë§ˆì§€ë§‰ ë‹¨ê³„ì™€ ë‘ ë²ˆì§¸ ë‹¨ê³„ì—ì„œ ë§ˆì§€ë§‰ ë‹¨ê³„ ì‚¬ì´ì˜ ë§ì€ ì‘ì—…ì˜ ì •í™•ë„ì—ì„œ ë‚ ì¹´ë¡œìš´ ìƒí–¥ ì§„ë“œê¸°ëŠ” ìµœì¢… 1000ê°œì˜ í›ˆë ¨ ë‹¨ê³„ì— ê±¸ì³ LRì„ ì„ í˜•ìœ¼ë¡œ 0ìœ¼ë¡œ ì¤„ì´ëŠ” ì´ì ì„ ë³´ì—¬ì¤€ë‹¤. ì¶”ê°€ì ì¸ í‰ê°€ ê²°ê³¼ ë° ë…¼ì˜ëŠ” ë¶€ë¡ <a class="ltx_ref" href="#A1" title="Appendix A Additional Evaluation â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>ì˜ TableÂ <a class="ltx_ref" href="#A1.T9" title="Table 9 â€£ Additional end-task results â€£ Appendix A Additional Evaluation â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">9</span></a>ë¥¼ ì°¸ì¡°í•œë‹¤.</p>
</div>
<figure id="S4.F1" class="ltx_figure">
<p id="S4.F1.1" class="ltx_p ltx_align_center"><span id="S4.F1.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x7.png" id="S4.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="278" alt="Refer to caption">
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">ë„ 1:</span>Accuracy score progression of OLMo-7B on 8 core end-tasks score from Catwalk evaluation suite from SectionÂ <a class="ltx_ref" href="#S2.SS4.SSS0.Px2" title="Downstream Evaluation â€£ 2.4 Evaluation â€£ 2 OLMo Framework â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2.4</span></a>. ìš°ë¦¬ëŠ” ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì— ëŒ€í•œ í›ˆë ¨ì˜ ìµœì¢… 1000ë‹¨ê³„ì—ì„œ LRì„ 0ìœ¼ë¡œ ë‚®ì¶”ëŠ” ì´ì ì„ ë³¼ ìˆ˜ ìˆë‹¤.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Intrinsic language modeling evaluation</h3>

<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">ë‚´ì¬ì  í‰ê°€ë¥¼ ìœ„í•´ íŒ”ë¡œë§ˆëŠ” ê° ë„ë©”ì¸ì˜ ì„±ëŠ¥ ê²€ì‚¬ë¶€í„° ë„ë©”ì¸ ì¡°í•©ì— ëŒ€í•œ ë³´ë‹¤ ìš”ì•½ëœ ê²°ê³¼ì— ì´ë¥´ê¸°ê¹Œì§€ ë‹¤ì–‘í•œ ë¶„ì„ì„ ì œì•ˆí•œë‹¤. ìš°ë¦¬ëŠ” ë‘ ê°€ì§€ ìˆ˜ì¤€ì˜ ì„¸ë¶„ì„±, ì¦‰ <cite class="ltx_cite ltx_citemacro_cite">Magnusson etÂ al. (<a class="ltx_ref" href="#bib.bib43" title="">2023</a>)</cite>ì—ì„œì™€ ê°™ì´ íŒ”ë¡œë§ˆì—ì„œ 18ê°œ ì†ŒìŠ¤ ì¤‘ 11ê°œì— ëŒ€í•œ ì§‘ê³„ ì„±ëŠ¥ê³¼ ì´ëŸ¬í•œ ì†ŒìŠ¤ ê°ê°ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ë” ì„¸ë°€í•œ ê²°ê³¼ë¥¼ ë³´ê³ í•œë‹¤. íŒ”ë¡œë§ˆì˜ 11ê°œ ì†ŒìŠ¤ì˜ ì´ íŠ¹ì • í•˜ìœ„ ì§‘í•©ì€ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ê±°ë‚˜ í”„ë¦°ì§€ ë˜ëŠ” ë…ì„± í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ê±°ë‚˜ íŒ”ë¡œë§ˆì˜ ì˜¤ì—¼ ì œê±° ì ‘ê·¼ë²•ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì½”ë“œ ë°ì´í„°ë¡œ êµ¬ì„±ëœ ì†ŒìŠ¤ë¥¼ ì œì™¸í•œë‹¤. ì´ëŠ” C4 <cite class="ltx_cite ltx_citemacro_citep">(Raffel etÂ al., <a class="ltx_ref" href="#bib.bib63" title="">2020</a>)</cite>, mC4-en <cite class="ltx_cite ltx_citemacro_citep">(Chung etÂ al., <a class="ltx_ref" href="#bib.bib16" title="">2023</a>)</cite>, Wikitext 103 <cite class="ltx_cite ltx_citemacro_citep">(Merity etÂ al., <a class="ltx_ref" href="#bib.bib45" title="">2016</a>)</cite>, Penn Treebank <cite class="ltx_cite ltx_citemacro_citep">(Marcus etÂ al., <a class="ltx_ref" href="#bib.bib44" title="">1999</a>; Nunes, <a class="ltx_ref" href="#bib.bib52" title="">2020</a>)</cite>, RedPajama <cite class="ltx_cite ltx_citemacro_citep">(Together Computer, <a class="ltx_ref" href="#bib.bib76" title="">2023</a>)</cite>, Falcon-RefinedWeb <cite class="ltx_cite ltx_citemacro_citep">(Penedo etÂ al., <a class="ltx_ref" href="#bib.bib57" title="">2023</a>)</cite>, Dolma <cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2024</a>)</cite>, M2D2 S2ORC <cite class="ltx_cite ltx_citemacro_citep">(Reid etÂ al., <a class="ltx_ref" href="#bib.bib65" title="">2022</a>)</cite>, M2D2 Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Reid etÂ al., <a class="ltx_ref" href="#bib.bib65" title="">2022</a>)</cite>, C4 100 domains <cite class="ltx_cite ltx_citemacro_citep">(Chronopoulou etÂ al., <a class="ltx_ref" href="#bib.bib15" title="">2022</a>)</cite>, Dolma 100 Subreddits <cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2024</a>)</cite>ë¥¼ ë‚¨ê¸´ë‹¤. ë‹¤ë¥¸ ì–´íœ˜ë¥¼ ê°€ì§„ ëª¨ë¸ ê°„ì˜ ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´, ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ì†ŒìŠ¤ì˜ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ <cite class="ltx_cite ltx_citemacro_citet">Gao etÂ al. (<a class="ltx_ref" href="#bib.bib25" title="">2020</a>)</cite>ì— ì˜í•´ ì •ì˜ëœ ë°”ì´íŠ¸ë‹¹ ë¹„íŠ¸ë¥¼ ë³´ê³ í•œë‹¤.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.SSS0.Px2.p1.1.1">Sources Combined</span> subplot of FigureÂ <a class="ltx_ref" href="#S4.F2" title="Figure 2 â€£ Results â€£ 4.2 Intrinsic language modeling evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>ì—ì„œëŠ” Palomaì˜ 11ê°œ ë°ì´í„° ì†ŒìŠ¤ ì¡°í•©ì— ëŒ€í•œ 6ê°œì˜ ë¹„êµ í¬ê¸° ì–¸ì–´ ëª¨ë¸ì— ëŒ€í•œ OLMo-7Bì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ì „ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ëŠ” OLMoê°€ íŠ¹íˆ í›ˆë ¨ ë°ì´í„°ê°€ íŒ”ë¡œë§ˆì— ëŒ€í•´ ëª…ì‹œì ìœ¼ë¡œ ì˜¤ì—¼ ì œê±°ë˜ì—ˆë‹¤ëŠ” ì ì„ ê°ì•ˆí•  ë•Œ ê²½ìŸì  ì í•©ì„±ì„ ê°€ì§€ê³  ìˆìŒì„ ë°œê²¬í–ˆë‹¤. ìµœì¢… ëª¨ë¸(ëª¨ì–‘ ì°¸ì¡°)ê³¼ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸(ì ì„  ì°¸ì¡°)ì˜ ë¹„êµë¥¼ í†µí•´ ë³¼ ìˆ˜ ìˆë“¯ì´ OLMo ê²°ê³¼ëŠ” ë‹¤ë¥¸ ëª¨ë¸ì˜ ìœ ì‚¬í•œ ìŠ¤ì¼€ì¼ë§ ê²½í–¥ì„ ë”°ë¥¸ë‹¤. ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ì˜ ì„±ëŠ¥ì€ ê·¸ ì²´í¬í¬ì¸íŠ¸ê°€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ì—ì„œ ë°œìƒí•˜ëŠ” ìœ„ì¹˜ì— ì˜í•´ ì˜í–¥ì„ ë°›ìŒì— ìœ ì˜í•œë‹¤. ë”°ë¼ì„œ ë” ì ì€ ë‹¨ê³„ì— ëŒ€í•´ í›ˆë ¨ëœ ëª¨ë¸ì€ í›ˆë ¨ ê¸°ê°„ì´ ëª¨ë“  ëª¨ë¸ì— ê±¸ì³ ê³ ì •ëœë‹¤ë©´ ë” ìƒ˜í”Œ íš¨ìœ¨ì ì¼ í•„ìš” ì—†ì´ ë” ê°€íŒŒë¥¸ í›ˆë ¨ ê³¡ì„ ì„ ê°–ëŠ” ê²½í–¥ì´ ìˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  MPT-7BëŠ” ì´ ì„œë¸Œí”Œë¡¯ì˜ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ ì•ì„œ ê°œì„ ë˜ëŠ” ê²ƒìœ¼ë¡œ ëˆˆì— ëˆë‹¤. ì´ëŠ” íŒ”ë¡œë§ˆ(ì˜ˆë¥¼ ë“¤ì–´, MPTëŠ” LLaMAì˜ ê²½ìš° 18%, RedPajamaì˜ ê²½ìš° 12.2%, OLMoì˜ ê²½ìš° 11.2%ê°€ ì•„ë‹Œ 27% ë¹„ê³µí†µ í¬ë¡¤ ë°ì´í„°ì— ëŒ€í•œ ì‚¬ì „ í›ˆë ¨ê³¼ ë‹¤ì–‘í•œ ë°ì´í„° ì „ì²˜ë¦¬ ê²°ì •(ì˜ˆë¥¼ ë“¤ì–´, MPTì˜ <cite class="ltx_cite ltx_citemacro_citep">Abbas etÂ al., <a class="ltx_ref" href="#bib.bib1" title="">2023</a></cite>ì˜ ì˜ë¯¸ì  ì¤‘ë³µ ì œê±° ì‚¬ìš©)ì„ í¬í•¨í•œ ì—¬ëŸ¬ ìš”ì¸ ë•Œë¬¸ì¼ ìˆ˜ ìˆë‹¤.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x8.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="342" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">ê·¸ë¦¼ 2:</span></figcaption>
Bits per byte on 11 evaluation data sources from Paloma and their combination <cite class="ltx_cite ltx_citemacro_citep">(Magnusson et&nbsp;al., <a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>, decontaminated from OLMoâ€™s pretraining data. While models follow a general data scaling trend, sample efficiency is most favorable on in-distribution data. For example, OLMo-7B overtakes all other models on C4, perhaps from having 88.8% Common Crawl pretraining data.
</figcaption>
</figure>
<div id="S4.SS2.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">ê·¸ë¦¼ <a class="ltx_ref" href="#S4.F2" title="Figure 2 â€£ Results â€£ 4.2 Intrinsic language modeling evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>ì˜ ë‚˜ë¨¸ì§€ ì„œë¸Œí”Œë¡¯ì€ ì§‘ê³„ëœ íŒ”ë¡œë§ˆ ë©”íŠ¸ë¦­ì—ì„œ ê²°í•©ëœ 11ê°œì˜ ë°ì´í„° ì†ŒìŠ¤ ê°ê°ì— ëŒ€í•´ ë°”ì´íŠ¸ë‹¹ ë¹„íŠ¸ë¥¼ ë³„ë„ë¡œ ë³´ê³ í•¨ìœ¼ë¡œì¨ ë” ì„¸ë°€í•œ ë¶„ì„ì„ ì œê³µí•œë‹¤. ì´ë¡œë¶€í„° ìš°ë¦¬ëŠ” ì£¼ë¡œ í›ˆë ¨ ë° í‰ê°€ ë¶„í¬ì˜ ìœ ì‚¬ì„±ì— ì˜í•´ ì£¼ë„ë˜ëŠ” í‘œë³¸ íš¨ìœ¨ì„±ì˜ ë” í° ë³€ë™ì„ ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹íˆ, OLMo-7BëŠ” C4ì™€ ê°™ì€ Common Crawlì´ ìš°ì„¸í•œ í‰ê°€ì—ì„œ ìš°ìˆ˜í•˜ì§€ë§Œ, Common Crawlì„ í›„ì²˜ë¦¬í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì€ Falcon RefinedWebì˜ Falcon-7Bì™€ ê°™ì€ íŠ¹ì • ë°ì´í„°ë¡œ í›ˆë ¨ëœ ëª¨ë¸ì— ê°€ì¥ ì í•©í•˜ë‹¤. í•œí¸, OLMo-7BëŠ” WikiText-103, M2D2 S2ORC ë° M2D2 ìœ„í‚¤í”¼ë””ì•„ì™€ ê°™ì´ ìŠ¤í¬ë˜í•‘ëœ ì›¹ í…ìŠ¤íŠ¸ì™€ ëœ ê´€ë ¨ëœ ì†ŒìŠ¤ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ì— ë¹„í•´ ìƒ˜í”Œ íš¨ìœ¨ì´ ë‚®ë‹¤. ë ˆë“œíŒŒìë§ˆ í‰ê°€ëŠ” ì•„ë§ˆë„ 7ê°œ ë„ë©”ì¸ ì¤‘ 2ê°œë§Œì´ ì»¤ë¨¼ í¬ë¡¤ì—ì„œ ì™”ìœ¼ë©° íŒ”ë¡œë§ˆ ê°€ì¤‘ì¹˜ ë„ë©”ì¸ì´ ê° ì†ŒìŠ¤ ë‚´ì—ì„œ ë™ë“±í•˜ê¸° ë•Œë¬¸ì— ìœ ì‚¬í•œ íŒ¨í„´ì„ ë³´ì—¬ì¤€ë‹¤. ìœ„í‚¤í”¼ë””ì•„ ë° ArXiv ë…¼ë¬¸ê³¼ ê°™ì€ ì„ ë³„ëœ ì†ŒìŠ¤ì˜ ì´ì§ˆì ì¸ ë°ì´í„°ëŠ” ìŠ¤í¬ë˜í•‘ëœ ì›¹ í…ìŠ¤íŠ¸ë³´ë‹¤ í›¨ì”¬ ëœ í’ë¶€í•˜ê¸° ë•Œë¬¸ì— ì‚¬ì „ í›ˆë ¨ ë§ë­‰ì¹˜ê°€ ìŠ¤ì¼€ì¼ë§ë¨ì— ë”°ë¼ ì´ëŸ¬í•œ ì–¸ì–´ ë¶„í¬ì— ì í•©í•˜ê¸° ìœ„í•œ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ìœ ì§€í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš¸ ê²ƒì´ë‹¤.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Adaptation Evaluation</h3>

<figure id="S4.T7" class="ltx_table">
<p id="S4.T7.4" class="ltx_p"><span id="S4.T7.4.4" class="ltx_text"> <span id="S4.T7.4.4.4" class="ltx_inline-block ltx_transformed_outer" style="width:356.3pt;height:198pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span id="S4.T7.4.4.4.4" class="ltx_p"><span id="S4.T7.4.4.4.4.4" class="ltx_text">  <span id="S4.T7.4.4.4.4.4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle"> <span class="ltx_tbody"> <span id="S4.T7.4.4.4.4.4.4.5.1" class="ltx_tr"> <span id="S4.T7.4.4.4.4.4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T7.4.4.4.4.4.4.5.1.1.1" class="ltx_text ltx_font_bold">Model</span></span> <span id="S4.T7.4.4.4.4.4.4.5.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.4.4.4.4.4.4.5.1.2.1" class="ltx_text ltx_font_bold">MMLU</span></span> <span id="S4.T7.4.4.4.4.4.4.5.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.4.4.4.4.4.4.5.1.3.1" class="ltx_text ltx_font_bold">AlpacaEval</span></span> <span id="S4.T7.4.4.4.4.4.4.5.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.4.4.4.4.4.4.5.1.4.1" class="ltx_text ltx_font_bold">ToxiGen</span></span> <span id="S4.T7.4.4.4.4.4.4.5.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.4.4.4.4.4.4.5.1.5.1" class="ltx_text ltx_font_bold">TruthfulQA</span></span></span> <span id="S4.T7.4.4.4.4.4.4.4" class="ltx_tr"> <span id="S4.T7.4.4.4.4.4.4.4.5" class="ltx_td ltx_th ltx_th_row ltx_border_r"></span> <span id="S4.T7.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">0-shot</span> <math id="S4.T7.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T7.1.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T7.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T7.1.1.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T7.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span> <span id="S4.T7.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T7.2.2.2.2.2.2.2.2.1" class="ltx_text ltx_font_bold">%win</span> <math id="S4.T7.2.2.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T7.2.2.2.2.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T7.2.2.2.2.2.2.2.2.m1.1.1" xref="S4.T7.2.2.2.2.2.2.2.2.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.2.2.2.2.2.m1.1b"><ci id="S4.T7.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T7.2.2.2.2.2.2.2.2.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.2.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math></span> <span id="S4.T7.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T7.3.3.3.3.3.3.3.3.1" class="ltx_text ltx_font_bold">% Toxic</span> <math id="S4.T7.3.3.3.3.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.3.3.3.3.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T7.3.3.3.3.3.3.3.3.m1.1.1" xref="S4.T7.3.3.3.3.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.3.3.3.3.3.m1.1b"><ci id="S4.T7.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S4.T7.3.3.3.3.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.3.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math></span> <span id="S4.T7.4.4.4.4.4.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T7.4.4.4.4.4.4.4.4.1" class="ltx_text ltx_font_bold">%Info+True</span> <math id="S4.T7.4.4.4.4.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T7.4.4.4.4.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T7.4.4.4.4.4.4.4.4.m1.1.1" xref="S4.T7.4.4.4.4.4.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.4.4.4.4.4.m1.1b"><ci id="S4.T7.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S4.T7.4.4.4.4.4.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.4.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math></span></span> <span id="S4.T7.4.4.4.4.4.4.6.2" class="ltx_tr"> <span id="S4.T7.4.4.4.4.4.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T7.4.4.4.4.4.4.6.2.1.1" class="ltx_text ltx_font_bold">OLMo (base)</span></span> <span id="S4.T7.4.4.4.4.4.4.6.2.2" class="ltx_td ltx_align_center ltx_border_t">28.3</span> <span id="S4.T7.4.4.4.4.4.4.6.2.3" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="S4.T7.4.4.4.4.4.4.6.2.4" class="ltx_td ltx_align_center ltx_border_t">81.4</span> <span id="S4.T7.4.4.4.4.4.4.6.2.5" class="ltx_td ltx_align_center ltx_border_t">31.6</span></span> <span id="S4.T7.4.4.4.4.4.4.7.3" class="ltx_tr"> <span id="S4.T7.4.4.4.4.4.4.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T7.4.4.4.4.4.4.7.3.1.1" class="ltx_text ltx_font_bold">MPT Chat</span></span> <span id="S4.T7.4.4.4.4.4.4.7.3.2" class="ltx_td ltx_align_center ltx_border_t">33.8</span> <span id="S4.T7.4.4.4.4.4.4.7.3.3" class="ltx_td ltx_align_center ltx_border_t">46.8</span> <span id="S4.T7.4.4.4.4.4.4.7.3.4" class="ltx_td ltx_align_center ltx_border_t">0.1</span> <span id="S4.T7.4.4.4.4.4.4.7.3.5" class="ltx_td ltx_align_center ltx_border_t">42.7</span></span> <span id="S4.T7.4.4.4.4.4.4.8.4" class="ltx_tr"> <span id="S4.T7.4.4.4.4.4.4.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T7.4.4.4.4.4.4.8.4.1.1" class="ltx_text ltx_font_bold">Falcon Instruct</span></span> <span id="S4.T7.4.4.4.4.4.4.8.4.2" class="ltx_td ltx_align_center">25.2</span> <span id="S4.T7.4.4.4.4.4.4.8.4.3" class="ltx_td ltx_align_center">14.0</span> <span id="S4.T7.4.4.4.4.4.4.8.4.4" class="ltx_td ltx_align_center">70.7</span> <span id="S4.T7.4.4.4.4.4.4.8.4.5" class="ltx_td ltx_align_center">27.2</span></span> <span id="S4.T7.4.4.4.4.4.4.9.5" class="ltx_tr"> <span id="S4.T7.4.4.4.4.4.4.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T7.4.4.4.4.4.4.9.5.1.1" class="ltx_text ltx_font_bold">RPJ-INCITE Chat</span></span> <span id="S4.T7.4.4.4.4.4.4.9.5.2" class="ltx_td ltx_align_center">27.0</span> <span id="S4.T7.4.4.4.4.4.4.9.5.3" class="ltx_td ltx_align_center">38.0</span> <span id="S4.T7.4.4.4.4.4.4.9.5.4" class="ltx_td ltx_align_center">46.4</span> <span id="S4.T7.4.4.4.4.4.4.9.5.5" class="ltx_td ltx_align_center">53.0</span></span> <span id="S4.T7.4.4.4.4.4.4.10.6" class="ltx_tr"> <span id="S4.T7.4.4.4.4.4.4.10.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T7.4.4.4.4.4.4.10.6.1.1" class="ltx_text ltx_font_bold">Llama-2-Chat</span></span> <span id="S4.T7.4.4.4.4.4.4.10.6.2" class="ltx_td ltx_align_center">46.8</span> <span id="S4.T7.4.4.4.4.4.4.10.6.3" class="ltx_td ltx_align_center">87.3</span> <span id="S4.T7.4.4.4.4.4.4.10.6.4" class="ltx_td ltx_align_center">0.0</span> <span id="S4.T7.4.4.4.4.4.4.10.6.5" class="ltx_td ltx_align_center">26.3</span></span> <span id="S4.T7.4.4.4.4.4.4.11.7" class="ltx_tr"> <span id="S4.T7.4.4.4.4.4.4.11.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T7.4.4.4.4.4.4.11.7.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">TÃ¼lu<span id="S4.T7.4.4.4.4.4.4.11.7.1.1.1" class="ltx_text ltx_font_upright"> 2</span></span></span> <span id="S4.T7.4.4.4.4.4.4.11.7.2" class="ltx_td ltx_align_center ltx_border_t">50.4</span> <span id="S4.T7.4.4.4.4.4.4.11.7.3" class="ltx_td ltx_align_center ltx_border_t">73.9</span> <span id="S4.T7.4.4.4.4.4.4.11.7.4" class="ltx_td ltx_align_center ltx_border_t">7.0</span> <span id="S4.T7.4.4.4.4.4.4.11.7.5" class="ltx_td ltx_align_center ltx_border_t">51.7</span></span> <span id="S4.T7.4.4.4.4.4.4.12.8" class="ltx_tr"> <span id="S4.T7.4.4.4.4.4.4.12.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T7.4.4.4.4.4.4.12.8.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">TÃ¼lu<span id="S4.T7.4.4.4.4.4.4.12.8.1.1.1" class="ltx_text ltx_font_upright"> 2+DPO</span></span></span> <span id="S4.T7.4.4.4.4.4.4.12.8.2" class="ltx_td ltx_align_center">50.7</span> <span id="S4.T7.4.4.4.4.4.4.12.8.3" class="ltx_td ltx_align_center">85.1</span> <span id="S4.T7.4.4.4.4.4.4.12.8.4" class="ltx_td ltx_align_center">0.5</span> <span id="S4.T7.4.4.4.4.4.4.12.8.5" class="ltx_td ltx_align_center">- *</span></span> <span id="S4.T7.4.4.4.4.4.4.13.9" class="ltx_tr" style="background-color:#40C4DF;"> <span id="S4.T7.4.4.4.4.4.4.13.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T7.4.4.4.4.4.4.13.9.1.1" class="ltx_text" style="background-color:#40C4DF;">[] <span id="S4.T7.4.4.4.4.4.4.13.9.1.1.1" class="ltx_text ltx_font_bold">OLMo +SFT</span></span></span> <span id="S4.T7.4.4.4.4.4.4.13.9.2" class="ltx_td ltx_align_center"><span id="S4.T7.4.4.4.4.4.4.13.9.2.1" class="ltx_text" style="background-color:#40C4DF;">47.3</span></span> <span id="S4.T7.4.4.4.4.4.4.13.9.3" class="ltx_td ltx_align_center"><span id="S4.T7.4.4.4.4.4.4.13.9.3.1" class="ltx_text" style="background-color:#40C4DF;">57.0</span></span> <span id="S4.T7.4.4.4.4.4.4.13.9.4" class="ltx_td ltx_align_center"><span id="S4.T7.4.4.4.4.4.4.13.9.4.1" class="ltx_text" style="background-color:#40C4DF;">14.4</span></span> <span id="S4.T7.4.4.4.4.4.4.13.9.5" class="ltx_td ltx_align_center"><span id="S4.T7.4.4.4.4.4.4.13.9.5.1" class="ltx_text" style="background-color:#40C4DF;">41.2</span></span></span> <span id="S4.T7.4.4.4.4.4.4.14.10" class="ltx_tr" style="background-color:#40C4DF;"> <span id="S4.T7.4.4.4.4.4.4.14.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T7.4.4.4.4.4.4.14.10.1.1" class="ltx_text" style="background-color:#40C4DF;">[] <span id="S4.T7.4.4.4.4.4.4.14.10.1.1.1" class="ltx_text ltx_font_bold">OLMo +SFT+DPO</span></span></span> <span id="S4.T7.4.4.4.4.4.4.14.10.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.4.4.4.4.4.4.14.10.2.1" class="ltx_text" style="background-color:#40C4DF;">46.2</span></span> <span id="S4.T7.4.4.4.4.4.4.14.10.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.4.4.4.4.4.4.14.10.3.1" class="ltx_text" style="background-color:#40C4DF;">69.3</span></span> <span id="S4.T7.4.4.4.4.4.4.14.10.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.4.4.4.4.4.4.14.10.4.1" class="ltx_text" style="background-color:#40C4DF;">1.7</span></span> <span id="S4.T7.4.4.4.4.4.4.14.10.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.4.4.4.4.4.4.14.10.5.1" class="ltx_text" style="background-color:#40C4DF;">52.0</span></span></span> </span> </span></span></span> </span></span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">í‘œ 7:</span>OLMo-7Bë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ëª…ë ¹ì–´ íŠœë‹ 7B ëª¨ë¸ì˜ í‰ê°€ ë° ì ì‘ í›ˆë ¨ ì „í›„. ë‚®ì€ ê²ƒì€ ToxiGenì— ë” ì¢‹ê³  ë†’ì€ ê²ƒì€ ë‹¤ë¥¸ ì§€í‘œì— ë” ì¢‹ë‹¤. ë¶€ë¡ì—ì„œ ëª¨ë¸ ë° ë©”íŠ¸ë¦­ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤. <a class="ltx_ref" href="#A3" title="Appendix C Adaptation Evaluation and Model details â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">C</span></a>. * <cite class="ltx_cite ltx_citemacro_citet">Ivison etÂ al. (<a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite> ë‹¤ìŒì— í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì˜¤ì—¼ìœ¼ë¡œ ì¸í•œ <span class="ltx_text ltx_font_smallcaps" id="S4.T7.6.1">TÃ¼lu</span> 2 TruthfulQA ì ìˆ˜ë¥¼ ë³´ê³ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.</figcaption>
</figure>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Setup</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">ì ì‘ ì „ OLMoë¥¼ í‰ê°€í•˜ê³  ê°ë… ë¯¸ì„¸ ì¡°ì • ë° DPO í›ˆë ¨ ë‹¨ê³„ í›„ <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="#bib.bib83" title="">2023</a>)</cite>ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì•ˆì „ì„± ë° ì±„íŒ… í‰ê°€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•œë‹¤. ìš°ë¦¬ëŠ” ì¶”ê°€ë¡œ í‘œ <a class="ltx_ref" href="#S4.T6" title="Table 6 â€£ Setup â€£ 4.1 Downstream evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>ì—ì„œ ê³µì‹ì ìœ¼ë¡œ ì¶œì‹œëœ ëª¨ë¸ì˜ ëª…ë ¹ì–´ ì¡°ì • ë³€í˜•ê³¼ ë¹„êµí•œë‹¤. ë˜í•œ <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px1.p1.1.1">TÃ¼lu</span> 2 ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ ë™ì¼í•œ í›ˆë ¨ í›„ ë°ì´í„° ë¯¹ìŠ¤ ë° ì ˆì°¨ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ë¹„êµí•œë‹¤.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">íŠ¹íˆ DPO í›ˆë ¨ í›„ ëª…ë ¹ì–´ íŠœë‹ì€ OLMoì˜ ì„±ëŠ¥ê³¼ ì•ˆì „ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼œ MMLU ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©° ToxiGenê³¼ TruthfulQA ì ìˆ˜ë¥¼ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ë˜í•œ ì´ˆê¸° ëª…ë ¹ì–´ íŠœë‹(OLMo +SFT) ë° ì¶”ê°€ ì„ í˜¸ë„ ì •ë ¬(OLMo +SFT +DPO) í›„ OLMoê°€ ëŒ€ë¶€ë¶„ì˜ ë‹¤ë¥¸ ì±„íŒ… ë³€í˜•ë³´ë‹¤ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ì—¬ ê¸°ë³¸ ëª¨ë¸ë¡œì„œì˜ OLMoì˜ ê°•ë„ì™€ ì ì‘ í›ˆë ¨ì„ ìˆ˜í–‰í•˜ëŠ” ë° ì‚¬ìš©ëœ <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.1">TÃ¼lu</span> ë¯¹ìŠ¤ì˜ ê°•ë„ë¥¼ ê°•ì¡°í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì—¬ì „íˆ <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.2">TÃ¼lu</span> 2ì™€ ê²©ì°¨ê°€ ìˆìŒì„ ë°œê²¬í–ˆìœ¼ë©°, ì´ëŠ” <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.3">TÃ¼lu</span> 2ë¥¼ ì ìš©í•˜ì—¬ í•™ìŠµí•œ ê²ƒì´ë‹¤. ì´ ê²©ì°¨ëŠ” Llama 2ì˜ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì˜¤ì—¼ ë•Œë¬¸ì¼ ìˆ˜ ìˆìœ¼ë©°, <span class="ltx_text ltx_font_smallcaps" id="S4.SS3.SSS0.Px2.p1.1.4">TÃ¼lu</span> mixëŠ” ì£¼ë¡œ Llama ëª¨ë¸ì— ëŒ€í•´ ì„¤ê³„ë˜ì—ˆê¸° ë•Œë¬¸ì— í–¥í›„ ì‘ì—…ì—ì„œ ì´ëŸ¬í•œ ê²©ì°¨ì˜ ì›ì¸ì„ ì¡°ì‚¬í•  ê²ƒì´ë‹¤. ì „ë°˜ì ìœ¼ë¡œ, ìš°ë¦¬ëŠ” OLMoê°€ ì¶”ê°€ íŠœë‹ìœ¼ë¡œë¶€í„° í¬ê²Œ ì´ìµì„ ì–»ê³  ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•œ ê°•ë ¥í•œ ê¸°ë³¸ ëª¨ë¸ ì—­í• ì„ í•œë‹¤ëŠ” ê²ƒì„ ì•ˆë‹¤.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Power Consumption and Carbon Footprint</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS4.p1.1">ì´ì „ ë¬¸í—Œ <cite class="ltx_cite ltx_citemacro_citep">(Strubell etÂ al., <a class="ltx_ref" href="#bib.bib72" title="">2019</a>; Patterson etÂ al., <a class="ltx_ref" href="#bib.bib56" title="">2021</a>; Wu etÂ al., <a class="ltx_ref" href="#bib.bib86" title="">2022</a>; Dodge etÂ al., <a class="ltx_ref" href="#bib.bib22" title="">2022</a>)</cite>ì— ë”°ë¼ í›ˆë ¨ì— í•„ìš”í•œ ì´ ì „ë ¥ ì†Œë¹„ëŸ‰ì„ ê³„ì‚°í•œ ë‹¤ìŒ ëª¨ë¸ì´ í›ˆë ¨ëœ ì „ë ¥ë§ì˜ íƒ„ì†Œ ë°°ì¶œ ê°•ë„ì— ê³±í•˜ì—¬ ëª¨ë¸ì„ ì‚¬ì „ í›ˆë ¨í•˜ë©´ì„œ ë°©ì¶œë˜ëŠ” ì´ ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ê³¼ íƒ„ì†Œë¥¼ ì¶”ì •í•œë‹¤. ì´ëŸ¬í•œ ìš´ì˜ ë°°ì¶œëŸ‰ì„ ë³´ê³ í•˜ëŠ” ê²ƒì€ í‘œì¤€ ê´€í–‰ì´ì§€ë§Œ í•˜ë“œì›¨ì–´ ë° ë°ì´í„° ì„¼í„° ì¸í”„ë¼ì˜ ì œì¡°, ìš´ì†¡ ë° íê¸°ë¡œ ì¸í•œ ì²´í™”ëœ ë°°ì¶œ, ì‚¬ìš©ìœ¼ë¡œ ì¸í•œ í‰ìƒ ìš´ì˜ ë°°ì¶œ, ë°˜ë“± íš¨ê³¼ ë˜ëŠ” ë¬¼ ì†Œë¹„ ë˜ëŠ” ì±„êµ´ê³¼ ê°™ì€ ê¸°íƒ€ í™˜ê²½ ì˜í–¥ê³¼ ê°™ì€ ë‹¤ë¥¸ ë°°ì¶œì›ì„ ì„¤ëª…í•˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ì˜ ì¶”ì •ì¹˜ëŠ” í•˜í•œìœ¼ë¡œ ê°„ì£¼ë˜ì–´ì•¼ í•œë‹¤.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS4.p2.1">ìš°ë¦¬ëŠ” 25msë§ˆë‹¤ ë‹¨ì¼ ë…¸ë“œì˜ ì†Œë¹„ ì „ë ¥ì„ ì¸¡ì •í•˜ê³  ì „ì²´ í›ˆë ¨ ì‹¤í–‰ì—ì„œ í‰ê· ì„ ê³„ì‚°í•˜ê³  ì´ ë…¸ë“œ ìˆ˜ë¥¼ ê³±í•˜ì—¬ ëª¨ë¸ì— ëŒ€í•œ ì´ ì†Œë¹„ ì „ë ¥ì„ ê³„ì‚°í•œë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ì „ ì´ê³„ì— ì „ë ¥ ì‚¬ìš© íš¨ìœ¨(PUE) ê³„ìˆ˜ë¥¼ ê³±í•˜ì—¬ ë°ì´í„° ì„¼í„°ì˜ ì—ë„ˆì§€ íš¨ìœ¨ì„±ì„ ì„¤ëª…í•˜ë©°, ì´ëŠ” ì—ë„ˆì§€ íš¨ìœ¨ì ì¸ ë°ì´í„° ì„¼í„°ì˜ ì „í˜•ì ì¸ ë³´ìˆ˜ì ì¸ 10% ì—ë„ˆì§€ ì†Œë¹„ ì˜¤ë²„í—¤ë“œë¥¼ ë‚˜íƒ€ë‚´ëŠ” 1.1ë¡œ ì„¤ì •ë˜ì—ˆë‹¤. <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nrel.gov/computational-science/measuring-efficiency-pue.html" target="_blank" title="">https://www.nrel.gov/computational-science/measuring-efficiency-pue.html</a></span></span></span><span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.google.com/about/datacenters/efficiency/" target="_blank" title="">https://www.google.com/about/datacenters/efficiency/</a></span></span></span> 7B ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨ì€ ì—ë„ˆì§€ì˜ <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">239 MWh</span>ì„ ì†Œë¹„í–ˆë‹¤ê³  ì¶”ì •í•©ë‹ˆë‹¤.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS4.p3.3">íƒ„ì†Œ ë°°ì¶œëŸ‰ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ê° ëª¨ë¸ì´ í›ˆë ¨ëœ ë°ì´í„° ì„¼í„°ì˜ ë¬¼ë¦¬ì  ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ KWhë‹¹ ë°©ì¶œë˜ëŠ” kg CO<sub í´ë˜ìŠ¤="ltx_sub" id="S4.SS4.p3.3.2">2</sub>ë¡œ ì¸¡ì •ëœ ì´ ì „ë ¥ ì†Œë¹„ì— íƒ„ì†Œ ê°•ë„ ê³„ìˆ˜ë¥¼ ê³±í•œë‹¤. A100-40GB GPUì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ì€ í˜¸ì£¼ì—ì„œ í›ˆë ¨ë˜ì—ˆìœ¼ë¯€ë¡œ 2022ë…„ í˜¸ì£¼ì˜ ì „êµ­ í‰ê· ì¸ 0.610ì˜ íƒ„ì†Œ ê°•ë„ ê³„ìˆ˜ë¥¼ ê°€ì •í•©ë‹ˆë‹¤. <span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/december-quarter-2022/Emissions-Reduction.aspx" target="_blank" title="">https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/december-quarter-2022/Emissions-Reduction.aspx</a></span></span></span> MI250X GPUì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ì€ 100% ì¬ìƒ ê°€ëŠ¥í•œ íƒ„ì†Œ ì¤‘ë¦½ ì—ë„ˆì§€ë¡œ ì‹¤í–‰ë˜ëŠ” LUMI ìŠˆí¼ì»´í“¨í„°ì—ì„œ í›ˆë ¨ë˜ì—ˆìœ¼ë¯€ë¡œ íƒ„ì†Œ ê°•ë„ ê³„ìˆ˜ëŠ” 0ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤. LUMIëŠ” ì „ì ìœ¼ë¡œ ìˆ˜ë ¥ì— ì˜í•´ êµ¬ë™ë˜ë©° ì¼ë¶€ ì†ŒìŠ¤ <cite class="ltx_cite ltx_citemacro_citep">(Ubierna etÂ al., <a class="ltx_ref" href="#bib.bib79" title="">2022</a>)</cite>ëŠ” ìˆ˜ë ¥ì˜ íƒ„ì†Œ ê°•ë„ ê³„ìˆ˜ë¥¼ 0.024ë¡œ ì¸¡ì •í•˜ë¯€ë¡œ ì´ íƒ„ì†Œ ë°°ì¶œëŸ‰ì´ 3.54 tCO<sub class="ltx_sub" id="S4.SS4.p3.3.3">2</sub>eq. <span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.lumi-supercomputer.eu" target="_blank" title="">https://www.lumi-supercomputer.eu</a></span></span></span> ê·¸ëŸ¬ë‚˜ ê³„ì‚°ì„ ìœ„í•´ ê³µì‹ LUMI ë°ì´í„°ì— ì˜ì¡´í•˜ë¯€ë¡œ <span class="ltx_text ltx_font_bold" id="S4.SS4.p3.3.1">69.78 tCO<sub class="ltx_sub" id="S4.SS4.p3.3.1.1"><span class="ltx_text ltx_font_medium" id="S4.SS4.p3.3.1.1.1">2</span></sub>eq</span>ì˜ ì´ ì‚¬ì „ í›ˆë ¨ ë°°ì¶œëŸ‰ì„ ì¶”ì •í•œë‹¤. <span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>These metrics were in part collected using Carbonaraâ€™s AI agent and monitoring platform. Learn more at: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trycarbonara.com" target="_blank" title="">https://trycarbonara.com</a></span></span></span> í‘œ <a class="ltx_ref" href="#footnote12b" title="Footnote 12 â€£ Table 8 â€£ 4.4 Power Consumption and Carbon Footprint â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">12</span></a>ì—ì„œ ìš°ë¦¬ëŠ” ê³µê°œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš°ë¦¬ì˜ ëª¨ë¸ì„ ì´ì „ì— ì¶œì‹œëœ ë‹¤ë¥¸ ëª¨ë¸ê³¼ ë¹„êµí•œë‹¤.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p class="ltx_p" id="S4.SS4.p4.1">ìš°ë¦¬ëŠ” ìš°ë¦¬ì˜ ëª¨ë¸ì„ ê³µê°œì ìœ¼ë¡œ ì¶œì‹œí•˜ë©´ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° ì‚¬ì „ í›ˆë ¨í•  í•„ìš”ê°€ ì—†ë„ë¡ í•¨ìœ¼ë¡œì¨ ë¯¸ë˜ì˜ ë°°ì¶œëŸ‰ì„ ì¤„ì´ê³  ìµœì²¨ë‹¨ ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ë° ë“œëŠ” ì§„ì •í•œ ë¹„ìš©ì— ëŒ€í•œ í†µì°°ë ¥ì„ ì œê³µí•  ìˆ˜ ìˆê¸°ë¥¼ ë°”ë€ë‹¤. ë˜í•œ ë””ë²„ê¹…, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë° ë‹¤ìš´íƒ€ì„ê³¼ ê°™ì€ ë‹¤ë¥¸ ì¤‘ìš”í•œ ê°œë°œ ë¶€ë¶„ì„ í¬í•¨í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì¶”ì •ì¹˜ê°€ í•˜í•œì„ì„ ê°•ì¡°í•©ë‹ˆë‹¤.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<table id="S4.T8.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T8.2.2" class="ltx_tr">
<th id="S4.T8.2.2.3" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S4.T8.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">GPU Type</th>
<th id="S4.T8.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T8.2.2.5.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T8.2.2.5.1.1" class="ltx_tr">
<td id="S4.T8.2.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">GPU Power</td>
</tr>
<tr id="S4.T8.2.2.5.1.2" class="ltx_tr">
<td id="S4.T8.2.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Consumption</td>
</tr>
<tr id="S4.T8.2.2.5.1.3" class="ltx_tr">
<td id="S4.T8.2.2.5.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">(MWh)</td>
</tr>
</tbody></table>
</th>
<th id="S4.T8.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T8.2.2.6.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T8.2.2.6.1.1" class="ltx_tr">
<td id="S4.T8.2.2.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Power</td>
</tr>
<tr id="S4.T8.2.2.6.1.2" class="ltx_tr">
<td id="S4.T8.2.2.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Usage</td>
</tr>
<tr id="S4.T8.2.2.6.1.3" class="ltx_tr">
<td id="S4.T8.2.2.6.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Effectiveness</td>
</tr>
</tbody></table>
</th>
<th id="S4.T8.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T8.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T8.1.1.1.1.2" class="ltx_tr">
<td id="S4.T8.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Carbon</td>
</tr>
<tr id="S4.T8.1.1.1.1.3" class="ltx_tr">
<td id="S4.T8.1.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Intensity</td>
</tr>
<tr id="S4.T8.1.1.1.1.1" class="ltx_tr">
<td id="S4.T8.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(kg CO<sub id="S4.T8.1.1.1.1.1.1.1" class="ltx_sub">2</sub>e/KWh)</td>
</tr>
</tbody></table>
</th>
<th id="S4.T8.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T8.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S4.T8.2.2.2.1.2" class="ltx_tr">
<td id="S4.T8.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Carbon</td>
</tr>
<tr id="S4.T8.2.2.2.1.3" class="ltx_tr">
<td id="S4.T8.2.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">Emissions</td>
</tr>
<tr id="S4.T8.2.2.2.1.1" class="ltx_tr">
<td id="S4.T8.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(tCO<sub id="S4.T8.2.2.2.1.1.1.1" class="ltx_sub">2</sub>eq)</td>
</tr>
</tbody></table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T8.2.3.1" class="ltx_tr">
<td id="S4.T8.2.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T8.2.3.1.1.1" class="ltx_text ltx_font_bold">Gopher-280B</span></td>
<td id="S4.T8.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">TPU v3</td>
<td id="S4.T8.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">1,066</td>
<td id="S4.T8.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">1.08</td>
<td id="S4.T8.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">0.330</td>
<td id="S4.T8.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">380</td>
</tr>
<tr id="S4.T8.2.4.2" class="ltx_tr">
<td id="S4.T8.2.4.2.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T8.2.4.2.1.1" class="ltx_text ltx_font_bold">BLOOM-176B</span></td>
<td id="S4.T8.2.4.2.2" class="ltx_td ltx_align_center">A100-80GB</td>
<td id="S4.T8.2.4.2.3" class="ltx_td ltx_align_center">433</td>
<td id="S4.T8.2.4.2.4" class="ltx_td ltx_align_center">1.2</td>
<td id="S4.T8.2.4.2.5" class="ltx_td ltx_align_center">0.057</td>
<td id="S4.T8.2.4.2.6" class="ltx_td ltx_align_center">30</td>
</tr>
<tr id="S4.T8.2.5.3" class="ltx_tr">
<td id="S4.T8.2.5.3.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T8.2.5.3.1.1" class="ltx_text ltx_font_bold">OPT-175B</span></td>
<td id="S4.T8.2.5.3.2" class="ltx_td ltx_align_center">A100-80GB</td>
<td id="S4.T8.2.5.3.3" class="ltx_td ltx_align_center">324</td>
<td id="S4.T8.2.5.3.4" class="ltx_td ltx_align_center">1.1</td>
<td id="S4.T8.2.5.3.5" class="ltx_td ltx_align_center">0.231</td>
<td id="S4.T8.2.5.3.6" class="ltx_td ltx_align_center">82</td>
</tr>
<tr id="S4.T8.2.6.4" class="ltx_tr">
<td id="S4.T8.2.6.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T8.2.6.4.1.1" class="ltx_text ltx_font_bold">T5-11B</span></td>
<td id="S4.T8.2.6.4.2" class="ltx_td ltx_align_center">TPU v3</td>
<td id="S4.T8.2.6.4.3" class="ltx_td ltx_align_center">77</td>
<td id="S4.T8.2.6.4.4" class="ltx_td ltx_align_center">1.12</td>
<td id="S4.T8.2.6.4.5" class="ltx_td ltx_align_center">0.545</td>
<td id="S4.T8.2.6.4.6" class="ltx_td ltx_align_center">47</td>
</tr>
<tr id="S4.T8.2.7.5" class="ltx_tr">
<td id="S4.T8.2.7.5.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T8.2.7.5.1.1" class="ltx_text ltx_font_bold">LLaMA-7B</span></td>
<td id="S4.T8.2.7.5.2" class="ltx_td ltx_align_center">A100-80GB</td>
<td id="S4.T8.2.7.5.3" class="ltx_td ltx_align_center">33</td>
<td id="S4.T8.2.7.5.4" class="ltx_td ltx_align_center">1.1</td>
<td id="S4.T8.2.7.5.5" class="ltx_td ltx_align_center">0.385</td>
<td id="S4.T8.2.7.5.6" class="ltx_td ltx_align_center">14</td>
</tr>
<tr id="S4.T8.2.8.6" class="ltx_tr">
<td id="S4.T8.2.8.6.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T8.2.8.6.1.1" class="ltx_text ltx_font_bold">LLaMA2-7B</span></td>
<td id="S4.T8.2.8.6.2" class="ltx_td ltx_align_center">A100-80GB</td>
<td id="S4.T8.2.8.6.3" class="ltx_td ltx_align_center">74</td>
<td id="S4.T8.2.8.6.4" class="ltx_td ltx_align_center">1.1</td>
<td id="S4.T8.2.8.6.5" class="ltx_td ltx_align_center">0.385</td>
<td id="S4.T8.2.8.6.6" class="ltx_td ltx_align_center">31</td>
</tr>
<tr id="S4.T8.2.9.7" class="ltx_tr" style="background-color:#40C4DF;">
<td id="S4.T8.2.9.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T8.2.9.7.1.1" class="ltx_text" style="background-color:#40C4DF;">[]
<span id="S4.T8.2.9.7.1.1.1" class="ltx_text ltx_font_bold">OLMo-7B</span></span></td>
<td id="S4.T8.2.9.7.2" class="ltx_td ltx_align_center"><span id="S4.T8.2.9.7.2.1" class="ltx_text" style="background-color:#40C4DF;">MI250X</span></td>
<td id="S4.T8.2.9.7.3" class="ltx_td ltx_align_center"><span id="S4.T8.2.9.7.3.1" class="ltx_text" style="background-color:#40C4DF;">135</span></td>
<td id="S4.T8.2.9.7.4" class="ltx_td ltx_align_center"><span id="S4.T8.2.9.7.4.1" class="ltx_text" style="background-color:#40C4DF;">1.1</span></td>
<td id="S4.T8.2.9.7.5" class="ltx_td ltx_align_center"><span id="S4.T8.2.9.7.5.1" class="ltx_text" style="background-color:#40C4DF;">0.000*</span></td>
<td id="S4.T8.2.9.7.6" class="ltx_td ltx_align_center"><span id="S4.T8.2.9.7.6.1" class="ltx_text" style="background-color:#40C4DF;">0*</span></td>
</tr>
<tr id="S4.T8.2.10.8" class="ltx_tr" style="background-color:#40C4DF;">
<td id="S4.T8.2.10.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T8.2.10.8.1.1" class="ltx_text" style="background-color:#40C4DF;">[]
<span id="S4.T8.2.10.8.1.1.1" class="ltx_text ltx_font_bold">OLMo-7B</span></span></td>
<td id="S4.T8.2.10.8.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.2.10.8.2.1" class="ltx_text" style="background-color:#40C4DF;">A100-40GB</span></td>
<td id="S4.T8.2.10.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.2.10.8.3.1" class="ltx_text" style="background-color:#40C4DF;">104</span></td>
<td id="S4.T8.2.10.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.2.10.8.4.1" class="ltx_text" style="background-color:#40C4DF;">1.1</span></td>
<td id="S4.T8.2.10.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.2.10.8.5.1" class="ltx_text" style="background-color:#40C4DF;">0.610</span></td>
<td id="S4.T8.2.10.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T8.2.10.8.6.1" class="ltx_text" style="background-color:#40C4DF;">70</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">í‘œ 8:</span>CO<sub class="ltx_sub" id="S4.T8.9.1">2</sub> emissions during pretraining. ìš°ë¦¬ëŠ” PUEì— ëŒ€í•œ ê³µê°œì ìœ¼ë¡œ ì´ìš© ê°€ëŠ¥í•œ ë°ì´í„°, ì§€ì—­ ì „ë ¥ë§ì˜ íƒ„ì†Œ ê°•ë„ ë° ë³´ê³ ëœ ì „ë ¥ ì†Œë¹„ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ëª¨ë¸ì— ëŒ€í•œ ì´ íƒ„ì†Œ ë°°ì¶œëŸ‰ì„ ì¶”ì •í•œë‹¤. Gopher-280B <cite class="ltx_cite ltx_citemacro_citep">(Rae etÂ al., <a class="ltx_ref" href="#bib.bib61" title="">2022</a>)</cite>, BLOOM-176B <cite class="ltx_cite ltx_citemacro_citep">(Luccioni etÂ al., <a class="ltx_ref" href="#bib.bib42" title="">2022</a>)</cite>, OPT-175B <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="#bib.bib92" title="">2022</a>)</cite>, T5-11B <cite class="ltx_cite ltx_citemacro_citep">(Patterson etÂ al., <a class="ltx_ref" href="#bib.bib56" title="">2021</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="#bib.bib77" title="">2023a</a>)</cite>, LLaMA2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="#bib.bib78" title="">2023b</a>)</cite>ë¥¼ ê°ê°ì˜ ë…¼ë¬¸ì—ì„œ ì·¨í•œë‹¤. tCO2eqê°€ ì–´ë–»ê²Œ ê³„ì‚°ë˜ì—ˆëŠ”ì§€ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì„¹ì…˜ <a class="ltx_ref" href="#S4.SS4" title="4.4 Power Consumption and Carbon Footprint â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.4</span></a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.</figcaption>
<br class="ltx_break">* LUMI runs entirely on hydroelectric power<span id="footnote12b" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">12</span></span></span></span>and some estimates <cite class="ltx_cite ltx_citemacro_citep">(Ubierna et&nbsp;al., <a href="#bib.bib79" title="" class="ltx_ref">2022</a>)</cite> measure the intensity factor of hydroelectric power to be 0.024, implying total emissions of 3.54 tCO<sub id="S4.T8.10.2" class="ltx_sub">2</sub>eq.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Artifacts Released</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">ëª¨ë“  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ì˜ ì•„í‹°íŒ©íŠ¸ë¥¼ ê³µìœ í•¨ìœ¼ë¡œì¨ ê°œë°©í˜• ì—°êµ¬ë¥¼ ì¥ë ¤í•˜ê³  í•™ê³„ì™€ ì‹¤ë¬´ìê°€ ì¤‘ë³µë˜ê³  ì¢…ì¢… ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ë…¸ë ¥ì„ ì¤„ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ë‹¤ìŒ ë‚´ìš©ì„ ê³µê°œí•©ë‹ˆë‹¤.</p>
</div>
<div id="S5.p2" class="ltx_para">
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i1.p1.1">í›ˆë ¨ ë° ëª¨ë¸ë§ ì½”ë“œì…ë‹ˆë‹¤. <span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo" target="_blank" title="">https://github.com/allenai/OLMo</a></span></span></span></p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i2.p1.1">7B ëª¨ë¸,<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B" target="_blank" title="">https://huggingface.co/allenai/OLMo-7B</a></span></span></span>7B-twin-2T,<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B-Twin-2T" target="_blank" title="">https://huggingface.co/allenai/OLMo-7B-Twin-2T</a></span></span></span> ë° 1B ëª¨ë¸ì— ëŒ€í•œ í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜. <span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-1B" target="_blank" title="">https://huggingface.co/allenai/OLMo-1B</a></span></span></span> ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ìµœì¢… ëª¨ë¸ ê°€ì¤‘ì¹˜ë¿ë§Œ ì•„ë‹ˆë¼ 1000ë‹¨ê³„ ê°„ê²©ìœ¼ë¡œ 500+ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¦´ë¦¬ìŠ¤í•©ë‹ˆë‹¤.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i3.p1.1">Adapted OLMo-7B with instruction-tuning, 7B-SFT<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B-SFT" target="_blank" title="">https://huggingface.co/allenai/OLMo-7B-SFT</a></span></span></span>, and RLHF, 7B-Instruct<span class="ltx_note ltx_role_footnote" id="footnote18"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><span class="ltx_tag ltx_tag_note">18</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/allenai/OLMo-7B-Instruct" target="_blank" title="">https://huggingface.co/allenai/OLMo-7B-Instruct</a></span></span></span> including its training and evaluation code and data using our Open Instruct<span class="ltx_note ltx_role_footnote" id="footnote19"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><span class="ltx_tag ltx_tag_note">19</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/open-instruct" target="_blank" title="">https://github.com/allenai/open-instruct</a></span></span></span> libraryÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="#bib.bib83" title="">2023</a>; Ivison etÂ al., <a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite>.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i4.p1.1">í›ˆë ¨ ë°ì´í„° Dolma<cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2024</a>)</cite>. <span class="ltx_note ltx_role_footnote" id="footnote20"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><span class="ltx_tag ltx_tag_note">20</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/allenai/dolma" target="_blank" title="">https://huggingface.co/datasets/allenai/dolma</a></span></span></span></p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i5.p1.1">Dolma's toolkit to construct new datasets,<span class="ltx_note ltx_role_footnote" id="footnote21"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup><span class="ltx_tag ltx_tag_note">21</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/dolma" target="_blank" title="">https://github.com/allenai/dolma</a></span></span></span> and WIMBDÂ <cite class="ltx_cite ltx_citemacro_citep">(Elazar etÂ al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite> for dataset analysis. <span class="ltx_note ltx_role_footnote" id="footnote22"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><span class="ltx_tag ltx_tag_note">22</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/wimbd" target="_blank" title="">https://github.com/allenai/wimbd</a></span></span></span></p>
</div>
</li>
<li id="S5.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S5.I1.i6.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i6.p1.1">ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ í‰ê°€ìš© Catwalk<span class="ltx_note ltx_role_footnote" id="footnote24"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup><span class="ltx_tag ltx_tag_note">24</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/catwalk" target="_blank" title="">https://github.com/allenai/catwalk</a></span></span></span>ì„ ì‚¬ìš©í•œ í‰ê°€ ì½”ë“œ<span class="ltx_note ltx_role_footnote" id="footnote23"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup><span class="ltx_tag ltx_tag_note">23</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/allenai/OLMo-Eval" target="_blank" title="">https://github.com/allenai/OLMo-Eval</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Groeneveld etÂ al., <a class="ltx_ref" href="#bib.bib28" title="">2023</a>)</cite> ë° Perplexity ê¸°ë°˜ í‰ê°€ìš© Paloma<span class="ltx_note ltx_role_footnote" id="footnote25"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><span class="ltx_tag ltx_tag_note">25</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://paloma.allen.ai" target="_blank" title="">https://paloma.allen.ai</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Magnusson etÂ al., <a class="ltx_ref" href="#bib.bib43" title="">2023</a>)</cite>.</p>
</div>
</li>
<li id="S5.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S5.I1.i7.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i7.p1.1">The complete set of metrics logged to Weights &amp; Biases during training.<span class="ltx_note ltx_role_footnote" id="footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5" target="_blank" title="">https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5</a></span></span></span></p>í›ˆë ¨ ì¤‘ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì— ê¸°ë¡ëœ ì „ì²´ ë©”íŠ¸ë¦­ ì§‘í•©ì…ë‹ˆë‹¤. <span class="ltx_note ltx_role_footnote" id="footnote26"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup><span class="ltx_tag ltx_tag_note">26</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5" target="_blank" title="">https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5</a></span></span></span></p>
</div>
</li>
</ol>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p" id="S5.p3.1">ì¶”ê°€ êµìœ¡ ë¡œê·¸, ì ˆì œ ë° ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ë¦´ë¦¬ìŠ¤ì— ëŒ€í•œ í›„ì† ì¡°ì¹˜ë¥¼ ì·¨í•  ê³„íšì…ë‹ˆë‹¤.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>License</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">ìš°ë¦¬ì˜ ëª©í‘œëŠ” ê³¼í•™ ê°œë°œì„ ì´‰ì§„í•˜ê³  ê³¼í•™ ì»¤ë®¤ë‹ˆí‹°ì— ê¶Œí•œì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ë¯€ë¡œ ì‚¬ìš©ìì—ê²Œ ìì›ê³¼ ì¸ê³µë¬¼ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìœ ì—°ì„±ì„ ì œê³µí•˜ëŠ” í—ˆìš© ë¼ì´ì„ ìŠ¤ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë“  ì½”ë“œ ë° ê°€ì¤‘ì¹˜ëŠ” Apache 2.0 ë¼ì´ì„ ìŠ¤ì— ë”°ë¼ ë¦´ë¦¬ìŠ¤ë©ë‹ˆë‹¤. <span class="ltx_note ltx_role_footnote" id="footnote27"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup><span class="ltx_tag ltx_tag_note">27</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank" title="">http://www.apache.org/licenses/LICENSE-2.0</a></span></span></span> ìµœê·¼ ëª¨ë¸ ë¦´ë¦¬ìŠ¤ì— ëŒ€í•´ ë‹¤ë¥¸ ì¡°ì§ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì¼ë¶€ ë¼ì´ì„ ìŠ¤ëŠ” ì¸ê³µ ì§€ëŠ¥ ë˜ëŠ” ê¸°ê³„ í•™ìŠµ ì‹œìŠ¤í…œì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¸ˆì§€í•˜ì§€ë§Œ, ìš°ë¦¬ëŠ” ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©ìê°€ ê·¸ë ‡ê²Œ í•˜ë„ë¡ í—ˆìš©í•©ë‹ˆë‹¤. ë˜í•œ ìƒì—…ì  ì‚¬ìš©ì„ ì œí•œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì €í¬ ëª¨ë¸ì´ ë‹¤ë¥¸ ëª¨ë¸ì„ ë” ì¢‹ê²Œ ë§Œë“¤ ìˆ˜ ìˆê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ìš°ë¦¬ëŠ” ìš°ë¦¬ ëª¨ë¸ì´ ë„ë¦¬ ì±„íƒëœ ì œí’ˆì´ ì•„ë‹Œ ì£¼ë¡œ ê³¼í•™ì  ì¸ê³µë¬¼ë¡œ ì„¤ê³„ë˜ì—ˆê¸° ë•Œë¬¸ì— ì˜¤ìš© ìœ„í—˜ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ë‹¤ëŠ” ê²ƒì„ ì¸ì‹í•œë‹¤(ìš°ë¦¬ ëª¨ë¸ì€ ì±—ë´‡ìœ¼ë¡œ ì±„íƒë˜ì§€ ì•Šì•˜ë‹¤). ë˜í•œ ì§€ë‚œ 1ë…„ ë™ì•ˆ ë§¤ìš° í—ˆìš© ê°€ëŠ¥í•œ ë¼ì´ì„ ìŠ¤ë¡œ ì¶œì‹œëœ ë¹„êµ ê°€ëŠ¥í•œ ëª¨ë¸ì´ ë§ì•˜ê¸° ë•Œë¬¸ì— ë³´ë‹¤ ì—„ê²©í•œ ë¼ì´ì„ ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë°˜ì ì¸ ìœ„í—˜ì„ ì œê±°í•˜ì§€ ëª»í•  ê²ƒì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë” ê°œë°©ì ì¸ ì¸¡ë©´ì—ì„œ ì´ íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ ìµœì„ ì˜ ì„ íƒì´ë¼ê³  ë¯¿ìŠµë‹ˆë‹¤.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p" id="S7.p1.1">This technical report presents our first release of OLMo, a state-of-the-art, truly open language model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. Soon, we will also release training logs, ablations, findings and Weights &amp; Biases logs. We are also exploring the adaptation of OLMo with instruction tuning and different flavors of RLHF. We are going to release the adapted models as well as all of our model adaptation code and data.</p>ì´ ê¸°ìˆ  ë³´ê³ ì„œëŠ” ì–¸ì–´ ëª¨ë¸ë§ì˜ ê³¼í•™ì„ êµ¬ì¶•í•˜ê³  ì—°êµ¬í•˜ê¸° ìœ„í•œ ìµœì²¨ë‹¨ ì§„ì •í•œ ê°œë°©í˜• ì–¸ì–´ ëª¨ë¸ì¸ OLMoì˜ ì²« ë²ˆì§¸ ì¶œì‹œë¥¼ ì œì‹œí•œë‹¤. ê¸°ì¡´ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ì¶”ë¡  ì½”ë“œë§Œì„ ë°œí‘œí•˜ë˜ ê¸°ì¡´ ì—°êµ¬ë“¤ê³¼ëŠ” ë‹¬ë¦¬ OLMoì™€ í•™ìŠµ ë°ì´í„°, í•™ìŠµ ë° í‰ê°€ ì½”ë“œë¥¼ í¬í•¨í•œ ì „ì²´ í”„ë ˆì„ì›Œí¬ë¥¼ ë°œí‘œí•œë‹¤. ê³§, ìš°ë¦¬ëŠ” ë˜í•œ í›ˆë ¨ ë¡œê·¸, ì ˆì œ, ë°œê²¬ ë° ê°€ì¤‘ì¹˜ ë° í¸í–¥ ë¡œê·¸ë¥¼ ê³µê°œí•  ê²ƒì´ë‹¤. ë˜í•œ RLHFì˜ ëª…ë ¹ì–´ ì¡°ì • ë° ë‹¤ì–‘í•œ ë§›ì„ ì‚¬ìš©í•˜ì—¬ OLMoì˜ ì ì‘ì„ íƒìƒ‰í•˜ê³  ìˆë‹¤. ìš°ë¦¬ëŠ” ëª¨ë“  ëª¨ë¸ ì ì‘ ì½”ë“œì™€ ë°ì´í„°ë¿ë§Œ ì•„ë‹ˆë¼ ì ì‘ëœ ëª¨ë¸ì„ ì¶œì‹œí•  ê²ƒì…ë‹ˆë‹¤.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p" id="S7.p2.1">ìš°ë¦¬ëŠ” OLMoì™€ ê·¸ í”„ë ˆì„ì›Œí¬ë¥¼ ì§€ì†ì ìœ¼ë¡œ ì§€ì›í•˜ê³  í™•ì¥í•˜ë©° ê°œë°©í˜• ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì— í˜ì„ ì‹¤ì–´ì£¼ê¸° ìœ„í•´ ê°œë°©í˜• LMì˜ ê²½ê³„ë¥¼ ê³„ì† ë°€ì–´ë¶™ì¼ ê³„íšì´ë‹¤. ì´ë¥¼ ìœ„í•´ OLMo ê³„ì—´ì— ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°, ì–‘ì‹, ë°ì´í„° ì„¸íŠ¸, ì•ˆì „ ì¡°ì¹˜ ë° í‰ê°€ë¥¼ ê°€ì ¸ì˜¤ê¸°ë¥¼ ê¸°ëŒ€í•œë‹¤. ìš°ë¦¬ëŠ” ì´ ë°œí‘œì™€ í–¥í›„ ë°œí‘œê°€ ì—´ë¦° ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì— í˜ì„ ì‹¤ì–´ì£¼ê³  ê°•í™”í•˜ë©° ìƒˆë¡œìš´ í˜ì‹ ì˜ ë¬¼ê²°ì„ ë¶ˆëŸ¬ì¼ìœ¼í‚¤ê¸°ë¥¼ ë°”ëë‹ˆë‹¤.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Author Contributions</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p" id="Sx1.p1.1">ì˜¬ëª¨ëŠ” ë§ì€ íŒ€ì›ë“¤ê³¼ í˜‘ë ¥ìë“¤ì˜ ë„ì›€ì´ ì—†ì—ˆë‹¤ë©´ ë¶ˆê°€ëŠ¥í–ˆì„ ê²ƒì´ë‹¤. ì•„ë˜ì— ì €ì ê¸°ì—¬ë„(ì•ŒíŒŒë²³ ìˆœì„œë¡œ)ë¥¼ ë‚˜ì—´í•©ë‹ˆë‹¤.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p class="ltx_p" id="Sx1.p2.1"><span class="ltx_text ltx_font_bold" id="Sx1.p2.1.1">pretraining dataset construction and tooling</span> (ëŒë§ˆ)ì˜ ê¸°ì—¬ìëŠ” Russell Authur, Iz Beltagy, Akshita Bhagia, Khyathi Chandu, Jesse Dodge, Yanai Elazar, Dirk Groeneveld, Rodney Kinney, Kyle Lo, Aakanksha Naik, Abhilasha Ravichander, Dustin Schwenk, Luca Soldaini, Nishant Subramanië¥¼ í¬í•¨í•œë‹¤.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p class="ltx_p" id="Sx1.p3.1"><span class="ltx_text ltx_font_bold" id="Sx1.p3.1.1">ëª¨ë¸ í›ˆë ¨ ë° ì•„í‚¤í…ì²˜</span>ì— ëŒ€í•œ ê¸°ì—¬ìëŠ” Shane Arora, Iz Beltagy, Akshita Bhagia, Matthew E. Peters, Dirk Groeneveld, Ananya Harsh Jha, William Merrill, Jacob Morrison, Niklas Muennighoff, Dustin Schwenk, Saurabh Shah, Pete Walsh ë° Mitchell Wortsmanì„ í¬í•¨í•œë‹¤.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p class="ltx_p" id="Sx1.p4.1"><span class="ltx_text ltx_font_bold" id="Sx1.p4.1.1">evaluation suite and tooling</span>ì—ëŠ” Akshita Bhagia, Arman Cohan, Pradeep Dasigi, Jesse Dodge, Dirk Groeneveld, Yuling Gu, Tushar Khot, Ian Magnusson, Kyle Richardson, Oyvind Tajford, and Pete Walshê°€ í¬í•¨ëœë‹¤.</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<p class="ltx_p" id="Sx1.p5.1"><span class="ltx_text ltx_font_bold" id="Sx1.p5.1.1">model adaptation</span>ì—ëŠ” Iz Beltagy, Pradeep Dasigi, Jack Hessel, Hamish Ivison, Nathan Lambert, Valentina Pyatkin, Pete Walsh, Yizhong Wangì´ í¬í•¨ë©ë‹ˆë‹¤.</p>
</div>
<div id="Sx1.p6" class="ltx_para">
<p class="ltx_p" id="Sx1.p6.1"><span class="ltx_text ltx_font_bold" id="Sx1.p6.1.1">license creation and risk assessment</span>ì—ëŠ” David Atkinson, Jesse Dodge, Jennifer Dumas, Crystal Nam, and Will Smithê°€ ìˆë‹¤.</p>
</div>
<div id="Sx1.p7" class="ltx_para">
<p class="ltx_p" id="Sx1.p7.1">OLMo í”„ë¡œì íŠ¸ëŠ” í•œë‚˜ë„¤ í•˜ì§€ì‹œë¥´ì§€ì™€ ë…¸ì•„ A. ìŠ¤ë¯¸ìŠ¤ê°€ ì£¼ë„í–ˆë‹¤.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p class="ltx_p" id="Sx2.p1.1">OLMoëŠ” ë§ì€ ê°œì¸ê³¼ ê¸°ê´€ì˜ ì§€ì›ì´ ì—†ì—ˆë‹¤ë©´ ë¶ˆê°€ëŠ¥í–ˆì„ ê²ƒì´ë‹¤. ì´ ì‘ì—…ì˜ ì‹¤í—˜ êµ¬ì„± ìš”ì†ŒëŠ” AMD ë° CSCì™€ì˜ íŒŒíŠ¸ë„ˆì‹­ì„ í†µí•´ ê°€ëŠ¥í–ˆìœ¼ë©°, LUMI ìŠˆí¼ì»´í“¨í„°ì™€ í•˜ë²„ë“œ ëŒ€í•™ì˜ ì¼í”„ë„ˆ ì—°êµ¬ì†Œì˜ ì‚¬ìš©ì´ ê°€ëŠ¥í–ˆë‹¤. ìš°ë¦¬ëŠ” ì¡°ë‚˜ë‹¨ í”„ë­í´ê³¼ ëª¨ìì´í¬MLì˜ íŒ€(í˜„ì¬ Databricks)ì´ ê·¸ë“¤ì˜ ê²½í—˜ì„ FSDPì™€ ê³µìœ í•˜ê³  OLMoê°€ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì½”ë“œ ê¸°ë°˜ì„ êµ¬ì¶•í•´ì¤€ ê²ƒì— ê°ì‚¬ë“œë¦°ë‹¤. ìš°ë¦¬ëŠ” íŒ€ ë™ë£Œì¸ íƒ€ì´ë¼ ì•¤ë”ìŠ¨, ë¯¸ì…¸ ë² ë„¤ë”•íŠ¸, ì¡´ ë³´ë¥´ì°¨ë¥´íŠ¸, ì´ë¹„ ì³‰, ì•„ë¥´ë‚˜ë¹„ ì²´ë‹¤, ìš”í•œ ë‹´, ë§· ë¼ì¸ ì¼€, ì¼ˆì‹œ ë§¥ë°€ë€, ì•„ë¡  ì‚¬ë¥´ë‚˜íŠ¸, ì¹´ë¦¬ì‚¬ ì‡¤ìµ, ìƒ˜ ìŠ¤ì½˜ìŠ¤ë²„ê·¸, ë§ˆì´í´ ìŠˆë¯¸ì¸ , ë§ˆì´í´ ìœŒìŠ¨, ì¼€ì´í‹€ë¦° ìœ„í‹€ë¦¬, ê·¸ë¦¬ê³  ì „ì²´ IT íŒ€ì—ê²Œ ì›¹ì‚¬ì´íŠ¸, ë””ìì¸, ë‚´ë¶€ ë° ì™¸ë¶€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜, ì˜ˆì‚° ì±…ì • ë° ì´ í”„ë¡œì íŠ¸ì˜ ì›í™œí•œ ì§„í–‰ì„ ì§€ì›í•˜ëŠ” ê¸°íƒ€ í™œë™ì— ëŒ€í•œ ë„ì›€ì„ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë˜í•œ í”„ë¦¬íŠ¸ë¹„ë¼ì§€(ë¼ì§€) ì•”ë§ˆë‚˜ë¸Œë¡œë£¨, í”¼í„° í´ë¼í¬, ë‹ˆì½œ ë°ì¹´ë¦¬ì˜¤, ë”ê·¸ ë‹¤ìš°ë‹ˆ, ì•Œë¦¬ íŒŒí•˜ë””, ì´ì•ˆ í˜ë ˆì´ë¼, ë°”ì´ë«¼ í•˜íƒ„íŒŒì—, ìƒ´ Mì„ í¬í•¨í•œ AI2ì˜ íŒ€ì›ë“¤ê³¼ ê°€ê¹Œìš´ í˜‘ë ¥ìë“¤ì˜ ë„ì›€ì´ ë˜ëŠ” í† ë¡ ê³¼ í”¼ë“œë°±ì— ê°ì‚¬ë¥¼ í‘œí•œë‹¤. Kakade, Julien Launay, Sydney Levine, Pekka Manninen, Franzi Roessner, Maarten Sap, Ludwig Schmidt, Yulia Tsvetkov, Daniel S. ì›°ë“œ</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbas et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Amro Abbas, Kushal Tirumala, DÃ¡niel Simig, Surya Ganguli, and Ari&nbsp;S Morcos.

</span>
<span class="ltx_bibblock">Semdedup: Data-efficient learning at web-scale through semantic
deduplication.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.09540</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2303.09540" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2303.09540</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almazrouei et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
Ruxandra-AimÃ©e Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic,
Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo.

</span>
<span class="ltx_bibblock">The falcon series of open language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2311.16867, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:265466629" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:265466629</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anand et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy
Mulyar.

</span>
<span class="ltx_bibblock">Gpt4all: Training an assistant-style chatbot with large scale data
distillation from gpt-3.5-turbo.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/nomic-ai/gpt4all" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nomic-ai/gpt4all</a>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ba et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Jimmy Ba, Jamie&nbsp;Ryan Kiros, and Geoffrey&nbsp;E. Hinton.

</span>
<span class="ltx_bibblock">Layer normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1607.06450, 2016.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:8236317" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:8236317</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,
Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage,
Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,
Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with reinforcement learning
from human feedback, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et&nbsp;al. (2003)</span>
<span class="ltx_bibblock">
Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, and Christian Janvin.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, 3:1137â€“1155, 2003.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:221275765" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:221275765</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Stella Biderman, Hailey Schoelkopf, Quentin&nbsp;Gregory Anthony, Herbie Bradley,
Kyle Oâ€™Brien, Eric Hallahan, Mohammad&nbsp;Aflah Khan, Shivanshu Purohit,
Usvsn&nbsp;Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar
Van Der&nbsp;Wal.

</span>
<span class="ltx_bibblock">Pythia: A suite for analyzing large language models across training
and scaling.

</span>
<span class="ltx_bibblock">In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th
International Conference on Machine Learning</em>, volume 202 of
<em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 2397â€“2430. PMLR,
23â€“29 Jul 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://proceedings.mlr.press/v202/biderman23a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.mlr.press/v202/biderman23a.html</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BigScience et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
BigScience, Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana
IliÄ‡, Daniel Hesslow, Roman CastagnÃ©, Alexandra&nbsp;Sasha Luccioni,
FranÃ§ois Yvon, et&nbsp;al.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial
intelligence</em>, volume&nbsp;34, pages 7432â€“7439, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/6239" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ojs.aaai.org/index.php/AAAI/article/view/6239</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,
USVSN&nbsp;Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben
Wang, and Samuel Weinbach.

</span>
<span class="ltx_bibblock">GPT-NeoX-20B: An open-source autoregressive language model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACL Workshop on Challenges &amp;
Perspectives in Creating Large Language Models</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2204.06745" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2204.06745</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blodgett et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Su&nbsp;Lin Blodgett, Lisa Green, and Brendan Oâ€™Connor.

</span>
<span class="ltx_bibblock">Demographic dialectal variation in social media: A case study of
African-American English.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1119â€“1130, Austin, Texas, November 2016.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D16-1120</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/D16-1120" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/D16-1120</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom&nbsp;B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T.&nbsp;J.
Henighan, Rewon Child, Aditya Ramesh, Daniel&nbsp;M. Ziegler, Jeff Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2005.14165, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:218971783" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:218971783</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E. Gonzalez, Ion Stoica, and
Eric&nbsp;P. Xing.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality, March 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lmsys.org/blog/2023-03-30-vicuna/</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung&nbsp;Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
Abhishek Rao, Parker Barnes, Yi&nbsp;Tay, Noam Shazeer, Vinodkumar Prabhakaran,
Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
Dohan, Shivani Agrawal, Mark Omernick, Andrew&nbsp;M. Dai,
Thanumalayan&nbsp;Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2204.02311" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2204.02311</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chronopoulou et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Alexandra Chronopoulou, Matthew Peters, and Jesse Dodge.

</span>
<span class="ltx_bibblock">Efficient hierarchical domain adaptation for pretrained language
models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 1336â€“1351, Seattle, United States, July 2022.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.naacl-main.96</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.naacl-main.96" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.naacl-main.96</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hyung&nbsp;Won Chung, Noah Constant, Xavier GarcÃ­a, Adam Roberts, Yi&nbsp;Tay, Sharan
Narang, and Orhan Firat.

</span>
<span class="ltx_bibblock">Unimax: Fairer and more effective language sampling for large-scale
multilingual pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2304.09151, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:258187051" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:258187051</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
Collins, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10044</em>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
Schoenick, and Oyvind Tafjord.

</span>
<span class="ltx_bibblock">Think you have solved question answering? try arc, the ai2 reasoning
challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.05457</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1803.05457" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1803.05457</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali
Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.

</span>
<span class="ltx_bibblock">Free dolly: Introducing the worldâ€™s first truly open
instruction-tuned llm, 2023.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,
Zhiyuan Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">Ultrafeedback: Boosting language models with high-quality feedback,
2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai,
Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige
Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao,
Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi
Li, Yao Li, Y.&nbsp;K. Li, Wenfeng Liang, Fangyun Lin, A.&nbsp;X. Liu, Bo&nbsp;Liu, Wen Liu,
Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong
Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren,
Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su,
Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu
Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y.&nbsp;Wu, Xin Xie, Zhenda Xie, Ziwei
Xie, Yiliang Xiong, Hanwei Xu, R.&nbsp;X. Xu, Yanhong Xu, Dejian Yang, Yuxiang
You, Shuiping Yu, Xingkai Yu, B.&nbsp;Zhang, Haowei Zhang, Lecong Zhang, Liyue
Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang
Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou.

</span>
<span class="ltx_bibblock">Deepseek llm: Scaling open-source language models with longtermism,
2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jesse Dodge, Taylor Prewitt, Remi Tachet&nbsp;Des Combes, Erika Odmark, Roy
Schwartz, Emma Strubell, Alexandra&nbsp;Sasha Luccioni, Noah&nbsp;A. Smith, Nicole
DeCario, and Will Buchanan.

</span>
<span class="ltx_bibblock">Measuring the carbon intensity of ai in cloud instances, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://dl.acm.org/doi/10.1145/3531146.3533234" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dl.acm.org/doi/10.1145/3531146.3533234</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dolan and Brockett (2005)</span>
<span class="ltx_bibblock">
William&nbsp;B. Dolan and Chris Brockett.

</span>
<span class="ltx_bibblock">Automatically constructing a corpus of sentential paraphrases.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Joint Conference on Natural Language
Processing</em>, 2005.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://www.microsoft.com/en-us/research/publication/automatically-constructing-a-corpus-of-sentential-paraphrases/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.microsoft.com/en-us/research/publication/automatically-constructing-a-corpus-of-sentential-paraphrases/</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elazar et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yanai Elazar, Akshita Bhagia, Ian&nbsp;H. Magnusson, Abhilasha Ravichander, Dustin
Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer
Singh, Hanna Hajishirzi, Noah&nbsp;A. Smith, and Jesse Dodge.

</span>
<span class="ltx_bibblock">Whatâ€™s in my big data?

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2310.20707, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:264803575" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:264803575</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et&nbsp;al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00027</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2101.00027" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2101.00027</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le&nbsp;Noacâ€™h,
Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang,
Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric
Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.

</span>
<span class="ltx_bibblock">A framework for few-shot language model evaluation, 12 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://zenodo.org/records/10256836" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zenodo.org/records/10256836</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Greenbaum and Nelson (1996)</span>
<span class="ltx_bibblock">
Sidney Greenbaum and Gerald Nelson.

</span>
<span class="ltx_bibblock">The international corpus of english (ICE) project.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">World Englishes</em>, 15(1):3â€“15, mar 1996.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1111/j.1467-971x.1996.tb00088.x</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1111%2Fj.1467-971x.1996.tb00088.x" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111%2Fj.1467-971x.1996.tb00088.x</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Groeneveld et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Dirk Groeneveld, Anas Awadalla, Iz&nbsp;Beltagy, Akshita Bhagia, Ian Magnusson, Hao
Peng, Oyvind Tafjord, Pete Walsh, Kyle Richardson, and Jesse Dodge.

</span>
<span class="ltx_bibblock">Catwalk: A unified language model evaluation framework for many
datasets.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10253</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2312.10253" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2312.10253</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding,
Jianwei Yue, and Yupeng Wu.

</span>
<span class="ltx_bibblock">How close is chatgpt to human experts? comparison corpus, evaluation,
and detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arxiv:2301.07597</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Suchin Gururangan, Mitchell Wortsman, Samir&nbsp;Yitzhak Gadre, Achal Dave, Maciej
Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt
Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and
Ludwig Schmidt.

</span>
<span class="ltx_bibblock">OpenLM: a minimal but performative language modeling (lm)
repository, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/mlfoundations/open_lm/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mlfoundations/open_lm/</a>.

</span>
<span class="ltx_bibblock">GitHub repository.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartvigsen et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
and Ece Kamar.

</span>
<span class="ltx_bibblock">TOXIGEN: Controlling Language Models to Generate Implied and
Adversarial Toxicity.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2203.09509" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2203.09509</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Learning
Representations (ICLR)</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ivison et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters,
Pradeep Dasigi, Joel Jang, David Wadden, Noah&nbsp;A. Smith, Iz&nbsp;Beltagy, and
Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Camels in a changing climate: Enhancing lm adaptation with tulu 2,
2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2311.10702" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2311.10702</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Albert&nbsp;Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
Savary, Chris Bamford, Devendra&nbsp;Singh Chaplot, Diego de&nbsp;las Casas, Emma&nbsp;Bou
Hanna, Florian Bressand, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.04088</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2401.04088" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2401.04088</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KÃ¶pf et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Andreas KÃ¶pf, Yannic Kilcher, Dimitri von RÃ¼tte, Sotiris Anagnostidis,
Zhi&nbsp;Rui Tam, Keith Stevens, Abdullah Barhoum, Duc&nbsp;Minh Nguyen, Oliver
Stanley, RichÃ¡rd Nagyfi, Shahul ES, Sameer Suri, David&nbsp;Alexandrovich
Glushkov, Arnav&nbsp;Varma Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu
Nguyen, and Alexander&nbsp;Julian Mattick.

</span>
<span class="ltx_bibblock">Openassistant conversations - democratizing large language model
alignment.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=VSJotgbPHF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=VSJotgbPHF</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Alpacaeval: An automatic evaluator of instruction-following models.

</span>
<span class="ltx_bibblock">Github repository, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/tatsu-lab/alpaca_eval" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/alpaca_eval</a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.09110</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2211.09110" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2211.09110</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3214â€“3252,
2022.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.

</span>
<span class="ltx_bibblock">Logiqa: A challenge dataset for machine reading comprehension with
logical reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2007.08124, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2007.08124" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2007.08124</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua
Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llm360: Towards fully transparent open-source llms.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.06550</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2312.06550" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2312.06550</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=Bkg6RiCqY7</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luccioni et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Alexandra&nbsp;Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.

</span>
<span class="ltx_bibblock">Estimating the carbon footprint of bloom, a 176b parameter language
model, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2211.02001" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2211.02001</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Magnusson et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya&nbsp;Harsh
Jha, Oyvind Tafjord, Dustin Schwenk, Evan&nbsp;Pete Walsh, Yanai Elazar, Kyle Lo,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Paloma: A benchmark for evaluating language model fit.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10523</em>, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marcus et&nbsp;al. (1999)</span>
<span class="ltx_bibblock">
Mitchell&nbsp;P. Marcus, Beatrice Santorini, Mary&nbsp;Ann Marcinkiewicz, and Ann Taylor.

</span>
<span class="ltx_bibblock">Treebank-3, 1999.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://catalog.ldc.upenn.edu/LDC99T42" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ldc.upenn.edu/LDC99T42</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merity et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1609.07843, 2016.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:16299141" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:16299141</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micikevicius et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory&nbsp;Frederick Diamos,
Erich Elsen, David GarcÃ­a, Boris Ginsburg, Michael Houston, Oleksii
Kuchaiev, Ganesh Venkatesh, and Hao Wu.

</span>
<span class="ltx_bibblock">Mixed precision training.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1710.03740, 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:3297437" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:3297437</a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book
question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.02789</em>, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1809.02789" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1809.02789</a>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et&nbsp;al. (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory&nbsp;S. Corrado, and Jeffrey Dean.

</span>
<span class="ltx_bibblock">Distributed representations of words and phrases and their
compositionality.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing Systems</em>, 2013.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:16447573" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:16447573</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing
instructions.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
<em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, pages 3470â€“3487, Dublin,
Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.acl-long.244</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.acl-long.244" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.acl-long.244</a>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MosaicML NLP Team (2023)</span>
<span class="ltx_bibblock">
MosaicML NLP Team.

</span>
<span class="ltx_bibblock">Introducing mpt-7b: A new standard for open-source, commercially
usable llms, 2023.

</span>
<span class="ltx_bibblock">URL <a href="www.mosaicml.com/blog/mpt-7b" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.mosaicml.com/blog/mpt-7b</a>.

</span>
<span class="ltx_bibblock">Accessed: 2023-05-05.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Alexander&nbsp;M Rush, Boaz Barak, Teven&nbsp;Le Scao, Aleksandra
Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.

</span>
<span class="ltx_bibblock">Scaling data-constrained language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.16264</em>, 2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nunes (2020)</span>
<span class="ltx_bibblock">
Davide Nunes.

</span>
<span class="ltx_bibblock">Preprocessed penn tree bank, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://zenodo.org/record/3910021" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zenodo.org/record/3910021</a>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2303.08774, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:257532815" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:257532815</a>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu&nbsp;Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul&nbsp;F Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In S.&nbsp;Koyejo, S.&nbsp;Mohamed, A.&nbsp;Agarwal, D.&nbsp;Belgrave, K.&nbsp;Cho, and A.&nbsp;Oh,
editors, <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume&nbsp;35,
pages 27730â€“27744. Curran Associates, Inc., 2022.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf</a>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papasavva et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Antonis Papasavva, Savvas Zannettou, Emiliano&nbsp;De Cristofaro, Gianluca
Stringhini, and Jeremy Blackburn.

</span>
<span class="ltx_bibblock">Raiders of the lost kek: 3.5 years of augmented 4chan posts from the
politically incorrect board.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International AAAI Conference on Web and
Social Media</em>, 14:885â€“894, may 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1609/icwsm.v14i1.7354</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1609%2Ficwsm.v14i1.7354" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1609%2Ficwsm.v14i1.7354</a>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patterson et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
Daniel Rothchild, David So, Maud Texier, and Jeff Dean.

</span>
<span class="ltx_bibblock">Carbon emissions and large neural network training, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2104.10350" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2104.10350</a>.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-AimÃ©e
Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam
Almazrouei, and Julien Launay.

</span>
<span class="ltx_bibblock">The refinedweb dataset for falcon llm: Outperforming curated corpora
with web data, and web data only.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2306.01116, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:259063761" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:259063761</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peters et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Matthew&nbsp;E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
Kenton Lee, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Deep contextualized word representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1802.05365, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:3626819" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:3626819</a>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pilehvar and Camacho-Collados (2018)</span>
<span class="ltx_bibblock">
Mohammad&nbsp;Taher Pilehvar and JosÃ© Camacho-Collados.

</span>
<span class="ltx_bibblock">Wic: 10, 000 example pairs for evaluating context-sensitive
representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1808.09121, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1808.09121" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1808.09121</a>.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press and Wolf (2017)</span>
<span class="ltx_bibblock">
Ofir Press and Lior Wolf.

</span>
<span class="ltx_bibblock">Using the output embedding to improve language models.

</span>
<span class="ltx_bibblock">In Mirella Lapata, Phil Blunsom, and Alexander Koller, editors,
<em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume 2, Short Papers</em>, pages
157â€“163, Valencia, Spain, April 2017. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/E17-2025" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/E17-2025</a>.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jack&nbsp;W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
George van&nbsp;den Driessche, Lisa&nbsp;Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme
Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
Xiang&nbsp;Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien
de&nbsp;Masson&nbsp;dâ€™Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor
Babuschkin, Aidan Clark, Diego de&nbsp;Las&nbsp;Casas, Aurelia Guy, Chris Jones, James
Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel,
William Isaac, Ed&nbsp;Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray
Kavukcuoglu, and Geoffrey Irving.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training
gopher, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2112.11446" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2112.11446</a>.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher&nbsp;D Manning, Stefano
Ermon, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a
reward model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing
Systems</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=HPuSIXJaa9" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=HPuSIXJaa9</a>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, 21(1), jan 2020.

</span>
<span class="ltx_bibblock">ISSN 1532-4435.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">Zero: Memory optimizations toward training trillion parameter models.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">SC20: International Conference for High Performance Computing,
Networking, Storage and Analysis</em>, pages 1â€“16, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:203736482" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:203736482</a>.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Machel Reid, Victor Zhong, Suchin Gururangan, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">M2D2: A massively multi-domain language modeling dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 964â€“975, Abu Dhabi, United Arab
Emirates, December 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.emnlp-main.63" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.emnlp-main.63</a>.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Manoel&nbsp;Horta Ribeiro, Jeremy Blackburn, Barry Bradlyn, Emiliano&nbsp;De Cristofaro,
Gianluca Stringhini, Summer Long, Stephanie Greenberg, and Savvas Zannettou.

</span>
<span class="ltx_bibblock">The evolution of the manosphere across the web.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International AAAI Conference on Web and
Social Media</em>, 15:196â€“207, may 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1609/icwsm.v15i1.18053</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1609%2Ficwsm.v15i1.18053" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1609%2Ficwsm.v15i1.18053</a>.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenfeld (2000)</span>
<span class="ltx_bibblock">
Ronald Rosenfeld.

</span>
<span class="ltx_bibblock">Two decades of statistical language modeling: Where do we go from
here?

</span>
<span class="ltx_bibblock"><em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, 88(8):1270â€“1278,
2000.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan&nbsp;Le Bras, Chandra Bhagavatula, and Yejin Choi.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 64(9):99â€“106,
2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://dl.acm.org/doi/abs/10.1145/3474381" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dl.acm.org/doi/abs/10.1145/3474381</a>.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M&nbsp;Saiful
Bari, Canwen Xu, Urmish Thakker, Shanya&nbsp;Sharma Sharma, Eliza Szczechla,
Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang,
Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng&nbsp;Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,
Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason&nbsp;Alan Fries, Ryan
Teehan, Teven&nbsp;Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander&nbsp;M
Rush.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=9Vrb9D0WI4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=9Vrb9D0WI4</a>.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer (2020)</span>
<span class="ltx_bibblock">
Noam&nbsp;M. Shazeer.

</span>
<span class="ltx_bibblock">Glu variants improve transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2002.05202, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:211096588" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:211096588</a>.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soldaini et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson,
Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar,
Valentin Hofmann, Ananya&nbsp;Harsh Jha, Sachin Kumar, Li&nbsp;Lucy, Xinxi Lyu, Nathan
Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik,
Crystal Nam, Matthew&nbsp;E. Peters, Abhilasha Ravichander, Kyle Richardson,
Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh,
Luke Zettlemoyer, Noah&nbsp;A. Smith, Hannaneh Hajishirzi, Iz&nbsp;Beltagy, Dirk
Groeneveld, Jesse Dodge, and Kyle Lo.

</span>
<span class="ltx_bibblock">Dolma: an Open Corpus of Three Trillion Tokens for Language Model
Pretraining Research.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 2024.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Emma Strubell, Ananya Ganesh, and Andrew McCallum.

</span>
<span class="ltx_bibblock">Energy and policy considerations for deep learning in NLP.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and LluÃ­s MÃ rquez, editors,
<em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>, pages 3645â€“3650, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1355</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/P19-1355" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P19-1355</a>.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Jianlin Su, Yu&nbsp;Lu, Shengfeng Pan, Bo&nbsp;Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2104.09864, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:233307138" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:233307138</a>.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori&nbsp;B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teknium1 (2023)</span>
<span class="ltx_bibblock">
Teknium1.

</span>
<span class="ltx_bibblock">Gpteacher.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/teknium1/GPTeacher" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/teknium1/GPTeacher</a>, 2023.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Together Computer (2023)</span>
<span class="ltx_bibblock">
Together Computer.

</span>
<span class="ltx_bibblock">RedPajama: An Open Source Recipe to Reproduce LLaMA training
dataset, April 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/togethercomputer/RedPajama-Data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/togethercomputer/RedPajama-Data</a>.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric
Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2302.13971, 2023a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:257219404" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:257219404</a>.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
Dan Bikel, Lukas Blecher, Cristian&nbsp;Canton Ferrer, Moya Chen, Guillem
Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit&nbsp;Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric&nbsp;Michael Smith, Ranjan Subramanian, Xiaoqing&nbsp;Ellen Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian&nbsp;Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models,
2023b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ubierna et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
MarÃ­a Ubierna, Cristina&nbsp;DÃ­ez Santos, and Sara Mercier-Blais.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Water Security and Climate Change: Hydropower Reservoir
Greenhouse Gas Emissions</em>, pages 69â€“94.

</span>
<span class="ltx_bibblock">Springer Singapore, Singapore, 2022.

</span>
<span class="ltx_bibblock">ISBN 978-981-16-5493-0.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-981-16-5493-0Ë™5</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1007/978-981-16-5493-0_5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-981-16-5493-0_5</a>.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Å&nbsp;ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I.&nbsp;Guyon, U.&nbsp;Von Luxburg, S.&nbsp;Bengio, H.&nbsp;Wallach, R.&nbsp;Fergus,
S.&nbsp;Vishwanathan, and R.&nbsp;Garnett, editors, <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, volume&nbsp;30. Curran Associates, Inc., 2017.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilares and GÃ³mez-RodrÃ­guez (2019)</span>
<span class="ltx_bibblock">
David Vilares and Carlos GÃ³mez-RodrÃ­guez.

</span>
<span class="ltx_bibblock">HEAD-QA: A healthcare dataset for complex reasoning.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and LluÃ­s MÃ rquez, editors,
<em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>, pages 960â€“966, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1092</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/P19-1092" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P19-1092</a>.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
Samuel&nbsp;R. Bowman.

</span>
<span class="ltx_bibblock">Glue: A multi-task benchmark and analysis platform for natural
language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1804.07461, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1804.07461" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1804.07461</a>.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot,
Khyathi&nbsp;Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah&nbsp;A. Smith,
Iz&nbsp;Beltagy, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">How far can camels go? exploring the state of instruction tuning on
open resources, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2306.04751" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2306.04751</a>.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams&nbsp;Wei Yu, Brian Lester,
Nan Du, Andrew&nbsp;M. Dai, and Quoc&nbsp;V Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=gEZrGCozdqR" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=gEZrGCozdqR</a>.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Johannes Welbl, Nelson&nbsp;F Liu, and Matt Gardner.

</span>
<span class="ltx_bibblock">Crowdsourcing multiple choice science questions.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1707.06209</em>, 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1707.06209" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1707.06209</a>.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani,
Kiwan Maeng, Gloria Chang, Fiona&nbsp;Aga Behram, James Huang, Charles Bai,
Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore
Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin&nbsp;S. Lee, Bugra
Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, and Kim
Hazelwood.

</span>
<span class="ltx_bibblock">Sustainable ai: Environmental implications, challenges and
opportunities, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2111.00364" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2111.00364</a>.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2024)</span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu&nbsp;Zhao, Jiazhan Feng, Chongyang
Tao, Qingwei Lin, and Daxin Jiang.

</span>
<span class="ltx_bibblock">WizardLM: Empowering large pre-trained language models to follow
complex instructions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning
Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=CfXh93NDgH" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=CfXh93NDgH</a>.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.

</span>
<span class="ltx_bibblock">Baize: An open-source chat model with parameter-efficient tuning on
self-chat data.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.01196</em>, 2023.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zannettou et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Savvas Zannettou, Barry Bradlyn, Emiliano De&nbsp;Cristofaro, Haewoon Kwak, Michael
Sirivianos, Gianluca Stringini, and Jeremy Blackburn.

</span>
<span class="ltx_bibblock">What is gab: A bastion of free speech or an alt-right echo chamber.

</span>
<span class="ltx_bibblock">In <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Companion Proceedings of the The Web Conference 2018</em>, WWW
â€™18, page 1007â€“1014, Republic and Canton of Geneva, CHE, 2018.
International World Wide Web Conferences Steering Committee.

</span>
<span class="ltx_bibblock">ISBN 9781450356404.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3184558.3191531</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3184558.3191531" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3184558.3191531</a>.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Hellaswag: Can a machine really finish your sentence?

</span>
<span class="ltx_bibblock"><em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.07830</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/1905.07830" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1905.07830</a>.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Sennrich (2019)</span>
<span class="ltx_bibblock">
Biao Zhang and Rico Sennrich.

</span>
<span class="ltx_bibblock">Root mean square layer normalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1910.07467, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:113405151" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:113405151</a>.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi&nbsp;Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit&nbsp;Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2205.01068" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2205.01068</a>.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu,
Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can
Balioglu, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li.

</span>
<span class="ltx_bibblock">Pytorch fsdp: Experiences on scaling fully sharded data parallel.

</span>
<span class="ltx_bibblock"><em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, 16:3848â€“3860, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:258297871" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:258297871</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Evaluation</h2>

<figure id="A1.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x9.png" id="A1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="251" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">ê·¸ë¦¼ 3: </span>Bits per byte for the 7 remaining Paloma data sources not aggregated in FigureÂ <a class="ltx_ref" href="#S4.F2" title="Figure 2 â€£ Results â€£ 4.2 Intrinsic language modeling evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
</figcaption>
</figure>
<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Additional perplexity results</h4>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.1">ê·¸ë¦¼ <a class="ltx_ref" href="#A1.F3" title="Figure 3 â€£ Appendix A Additional Evaluation â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>ì—ì„œëŠ” ê·¸ë¦¼ <a class="ltx_ref" href="#S4.F2" title="Figure 2 â€£ Results â€£ 4.2 Intrinsic language modeling evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>ì—ì„œ ê²°í•©ëœ ë©”íŠ¸ë¦­ì—ì„œ ì œì™¸ëœ íŒ”ë¡œë§ˆ <cite class="ltx_cite ltx_citemacro_citep">(Magnusson etÂ al., <a class="ltx_ref" href="#bib.bib43" title="">2023</a>)</cite>ì˜ 7ê°œ ë°ì´í„° ì†ŒìŠ¤ ê°ê°ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì œê³µí•œë‹¤. Pile <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="#bib.bib25" title="">2020</a>)</cite> ë° ICE <cite class="ltx_cite ltx_citemacro_citep">(Greenbaum and Nelson, <a class="ltx_ref" href="#bib.bib27" title="">1996</a>)</cite>ì™€ ê°™ì€ ì´ëŸ¬í•œ ì†ŒìŠ¤ ì¤‘ ì¼ë¶€ëŠ” í˜„ì¬ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. ëŒë§ˆ 100 í”„ë¡œê·¸ë˜ë° ì–¸ì–´ <cite class="ltx_cite ltx_citemacro_citep">(Soldaini etÂ al., <a class="ltx_ref" href="#bib.bib71" title="">2024</a>)</cite>ëŠ” íŒ”ë¡œë§ˆì—ì„œ ì‚¬ìš©ë˜ëŠ” ì˜¤ì—¼ ì œê±° ì ‘ê·¼ë²•ì— ì˜í•´ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì½”ë“œ ë°ì´í„°ë¡œ êµ¬ì„±ëœë‹¤. íŠ¸ìœ„í„°AAE <cite class="ltx_cite ltx_citemacro_citep">(Blodgett etÂ al., <a class="ltx_ref" href="#bib.bib11" title="">2016</a>)</cite>ëŠ” ICEì™€ í•¨ê»˜ ì„œë¡œ ë‹¤ë¥¸ ë°©ì–¸ ê°„ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•œ ë°ì´í„° ì„¸íŠ¸ì´ë¯€ë¡œ ë³„ë„ë¡œ í‰ê°€í•´ì•¼ í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ Manosphere, Gab ë° 4chan ì½”í¼ë¼ <cite class="ltx_cite ltx_citemacro_citep">(Ribeiro etÂ al., <a class="ltx_ref" href="#bib.bib66" title="">2021</a>; Zannettou etÂ al., <a class="ltx_ref" href="#bib.bib89" title="">2018</a>; Papasavva etÂ al., <a class="ltx_ref" href="#bib.bib55" title="">2020</a>)</cite>ëŠ” ì¼ë°˜ì ì¸ í˜ì˜¤ ë°œì–¸ê³¼ ë…ì„±ì— ëŒ€í•´ ì—°êµ¬ëœ í”„ë¦°ì§€ ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹°ì˜ ì–¸ì–´ì— ëŒ€í•œ ëª¨ë¸ ì í•©ì„±ì„ ì¡°ì‚¬í•˜ê¸° ìœ„í•œ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì´ëŸ¬í•œ í”„ë¦°ì§€ ì½”í¼ìŠ¤ì— ëŒ€í•œ ë³µì¡ì„±ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ í•­ìƒ ë°”ëŒì§í•œ ê²ƒì€ ì•„ë‹ˆë‹¤.</p>
</div>
<div id="A1.SS0.SSS0.Px1.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p2.1">ì—¬ê¸°ì„œ ì£¼ëª©í•  ë§Œí•œ ê²°ê³¼ ì¤‘ í•˜ë‚˜ëŠ” OLMo-7Bê°€ ëŒë§ˆ 100 í”„ë¡œê·¸ë˜ë° ì–¸ì–´(100 PL)ì˜ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ì•ì„œ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜¤ì—¼ ì œê±° ì½”ë“œ ë°ì´í„°ê°€ íŒ”ë¡œë§ˆì—ì„œ ë°©ë²•ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ê¸° ë•Œë¬¸ì— ì´ íš¨ê³¼ëŠ” ë¶€ë¶„ì ìœ¼ë¡œ ì˜¤ì—¼ìœ¼ë¡œ ì¸í•œ ê³¼ì†Œí‰ê°€ ë•Œë¬¸ì¼ ìˆ˜ ìˆë‹¤. ë™ì‹œì— ì˜¤ì—¼ì´ ìˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” RPJ-INCITE-7Bì™€ ê°™ì€ GitHubì˜ ì½”ë“œ ë°ì´í„°ì— ëŒ€í•´ í›ˆë ¨ëœ ë‹¤ë¥¸ ëª¨ë¸ì€ í›¨ì”¬ ë” ë‚˜ì˜ë‹¤. ë˜ ë‹¤ë¥¸ ìš”ì¸ì€ OLMo-7Bê°€ 100ê°œì˜ PLì—ì„œì™€ ë™ì¼í•œ í›„ì²˜ë¦¬ë¡œ ì½”ë“œ ë°ì´í„°ë¥¼ í›ˆë ¨í•˜ëŠ” ë°˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ì˜ ì½”ë“œ ë°ì´í„°ëŠ” ë‹¤ë¥´ê²Œ ì²˜ë¦¬ëœë‹¤ëŠ” ê²ƒì´ë‹¤. ìœ ì‚¬í•˜ê²Œ, íŒŒì¼ í‰ê°€ëŠ” í”¼í‹°ì•„-6.9Bê°€ OLMo-7Bë³´ë‹¤ ê±°ì˜ 100ë°° ì ì€ í† í°ì— ëŒ€í•´ í›ˆë ¨ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì— ë”°ë¼ ì´ëŸ¬í•œ ë¶„í¬ ë‚´ ë° ì ì¬ì ì¸ ì˜¤ì—¼ íš¨ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.</p>
</div>
<div id="A1.SS0.SSS0.Px1.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p3.1">íŒ”ë¡œë§ˆëŠ” ì¢…ì¢… ì´ëŸ¬í•œ ì¶œì²˜ì— ëŒ€í•œ ë‹¹í˜¹ê°ì´ ì‹¤ì œë¡œ ì´ëŸ¬í•œ ì—°ì„¤ ì»¤ë®¤ë‹ˆí‹°ì˜ êµ¬ì„±ì²´ì— ë‘ë“œëŸ¬ì§ˆ ê²ƒ ë³´ë‹¤ëŠ” ë‚®ì€ í‰ê·  ë¬¸ì„œ ê¸¸ì´ì™€ ê°™ì€ í”¼ìƒì ì¸ íŠ¹ì§•ì— ì˜í•´ ì§€ë°°ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ê¸° ë•Œë¬¸ì— ë‚˜ë¨¸ì§€ 5ê°œì˜ í‘œì  ì¶œì²˜ì— ëŒ€í•œ ê²°ê³¼ëŠ” ì‹ ì¤‘í•˜ê²Œ í•´ì„ë˜ì–´ì•¼ í•œë‹¤. íŠ¸ìœ„í„°AAEì™€ Gabì€ ì´ ê·¸ë¦¼ì—ì„œ ë°”ì´íŠ¸ë‹¹ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ ë¹„íŠ¸ì— ê¸°ì—¬í•˜ëŠ” íŒ”ë¡œë§ˆì—ì„œ ê°€ì¥ ì§§ì€ ë¬¸ì„œ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì´ ë‘ ê°€ì§€ ì™¸ì— ëª¨ë¸ì€ ICE, ë§ˆë…¸ìŠ¤í”¼ì–´ ë° 4chanì˜ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¶”ì„¸ì—ì„œ íŠ¹íˆ ë§¤ìš° ë°€ì ‘í•˜ê²Œ ê·¸ë£¹í™”ëœë‹¤.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Additional end-task results</h4>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.1">ë‹¤ìŒìœ¼ë¡œ í‘œ <a class="ltx_ref" href="#A1.T9" title="Table 9 â€£ Additional end-task results â€£ Appendix A Additional Evaluation â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">9</span></a>ì—ì„œ OLMo-7Bì˜ ì œë¡œìƒ· í‰ê°€ ê²°ê³¼ë¥¼ í•µì‹¬ í‰ê°€ ì œí’ˆêµ°ì˜ 8ê°œë¥¼ ì œì™¸í•œ 6ê°œì˜ ì¶”ê°€ ì—”ë“œ íƒœìŠ¤í¬ì— ì œê³µí•œë‹¤. ì´ëŸ¬í•œ ì‘ì—…ì€ <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.1">headqa_en</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Vilares and GÃ³mez-RodrÃ­guez, <a class="ltx_ref" href="#bib.bib81" title="">2019</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.2">logiqa</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="#bib.bib39" title="">2020</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.4">qnli</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="#bib.bib82" title="">2018</a>)</cite>, <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p1.1.4">wic</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Pilehvar and Camacho-Collados, <a class="ltx_ref" href="#bib.bib59" title="">2018</a>)</cite>,</p>
</div>
<figure id="A1.T9" class="ltx_table">
<p id="A1.T9.1" class="ltx_p ltx_align_center"><span id="A1.T9.1.1" class="ltx_text">
<span id="A1.T9.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:345.1pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A1.T9.1.1.1.1" class="ltx_p"><span id="A1.T9.1.1.1.1.1" class="ltx_text">

<span id="A1.T9.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="A1.T9.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="A1.T9.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></span>
<span id="A1.T9.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">headqa_en</span>
<span id="A1.T9.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">logiqa</span>
<span id="A1.T9.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mrpc</span>
<span id="A1.T9.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">qnli</span>
<span id="A1.T9.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">wic</span>
<span id="A1.T9.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">wnli</span>
<span id="A1.T9.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">avg.</span></span>
</span>
<span class="ltx_tbody">
<span id="A1.T9.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="A1.T9.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A1.T9.1.1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Falcon-7B</span></span>
<span id="A1.T9.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">38.6</span>
<span id="A1.T9.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">23.7</span>
<span id="A1.T9.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">62.8</span>
<span id="A1.T9.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">49.8</span>
<span id="A1.T9.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">49.5</span>
<span id="A1.T9.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.9</span>
<span id="A1.T9.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">45.4</span></span>
<span id="A1.T9.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="A1.T9.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="A1.T9.1.1.1.1.1.1.3.2.1.1" class="ltx_text ltx_font_bold">LLaMA-7B</span></span>
<span id="A1.T9.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center">38.7</span>
<span id="A1.T9.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_center">19.5</span>
<span id="A1.T9.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_center">68.6</span>
<span id="A1.T9.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_center">50.1</span>
<span id="A1.T9.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_center">49.1</span>
<span id="A1.T9.1.1.1.1.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r">52.1</span>
<span id="A1.T9.1.1.1.1.1.1.3.2.8" class="ltx_td ltx_align_center">46.4</span></span>
<span id="A1.T9.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="A1.T9.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="A1.T9.1.1.1.1.1.1.4.3.1.1" class="ltx_text ltx_font_bold">LLaMA2-7B</span></span>
<span id="A1.T9.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center">39.5</span>
<span id="A1.T9.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_center">26.1</span>
<span id="A1.T9.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center">69.1</span>
<span id="A1.T9.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_center">49.4</span>
<span id="A1.T9.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_center">49.8</span>
<span id="A1.T9.1.1.1.1.1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r">45.1</span>
<span id="A1.T9.1.1.1.1.1.1.4.3.8" class="ltx_td ltx_align_center">46.5</span></span>
<span id="A1.T9.1.1.1.1.1.1.5.4" class="ltx_tr">
<span id="A1.T9.1.1.1.1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="A1.T9.1.1.1.1.1.1.5.4.1.1" class="ltx_text ltx_font_bold">MPT-7B</span></span>
<span id="A1.T9.1.1.1.1.1.1.5.4.2" class="ltx_td ltx_align_center">37.4</span>
<span id="A1.T9.1.1.1.1.1.1.5.4.3" class="ltx_td ltx_align_center">22.9</span>
<span id="A1.T9.1.1.1.1.1.1.5.4.4" class="ltx_td ltx_align_center">67.7</span>
<span id="A1.T9.1.1.1.1.1.1.5.4.5" class="ltx_td ltx_align_center">52.1</span>
<span id="A1.T9.1.1.1.1.1.1.5.4.6" class="ltx_td ltx_align_center">48.1</span>
<span id="A1.T9.1.1.1.1.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r">47.9</span>
<span id="A1.T9.1.1.1.1.1.1.5.4.8" class="ltx_td ltx_align_center">46.0</span></span>
<span id="A1.T9.1.1.1.1.1.1.6.5" class="ltx_tr">
<span id="A1.T9.1.1.1.1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="A1.T9.1.1.1.1.1.1.6.5.1.1" class="ltx_text ltx_font_bold">Pythia-6.9B</span></span>
<span id="A1.T9.1.1.1.1.1.1.6.5.2" class="ltx_td ltx_align_center">40.1</span>
<span id="A1.T9.1.1.1.1.1.1.6.5.3" class="ltx_td ltx_align_center">21.5</span>
<span id="A1.T9.1.1.1.1.1.1.6.5.4" class="ltx_td ltx_align_center">65.4</span>
<span id="A1.T9.1.1.1.1.1.1.6.5.5" class="ltx_td ltx_align_center">53.8</span>
<span id="A1.T9.1.1.1.1.1.1.6.5.6" class="ltx_td ltx_align_center">55.0</span>
<span id="A1.T9.1.1.1.1.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r">38.0</span>
<span id="A1.T9.1.1.1.1.1.1.6.5.8" class="ltx_td ltx_align_center">45.6</span></span>
<span id="A1.T9.1.1.1.1.1.1.7.6" class="ltx_tr">
<span id="A1.T9.1.1.1.1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="A1.T9.1.1.1.1.1.1.7.6.1.1" class="ltx_text ltx_font_bold">RPJ-INCITE-7B</span></span>
<span id="A1.T9.1.1.1.1.1.1.7.6.2" class="ltx_td ltx_align_center">36.9</span>
<span id="A1.T9.1.1.1.1.1.1.7.6.3" class="ltx_td ltx_align_center">27.8</span>
<span id="A1.T9.1.1.1.1.1.1.7.6.4" class="ltx_td ltx_align_center">58.8</span>
<span id="A1.T9.1.1.1.1.1.1.7.6.5" class="ltx_td ltx_align_center">53.8</span>
<span id="A1.T9.1.1.1.1.1.1.7.6.6" class="ltx_td ltx_align_center">48.9</span>
<span id="A1.T9.1.1.1.1.1.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r">57.8</span>
<span id="A1.T9.1.1.1.1.1.1.7.6.8" class="ltx_td ltx_align_center">47.3</span></span>
<span id="A1.T9.1.1.1.1.1.1.8.7" class="ltx_tr" style="background-color:#40C4DF;">
<span id="A1.T9.1.1.1.1.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="A1.T9.1.1.1.1.1.1.8.7.1.1" class="ltx_text" style="background-color:#40C4DF;">[]
<span id="A1.T9.1.1.1.1.1.1.8.7.1.1.1" class="ltx_text ltx_font_bold">OLMo-7B</span></span></span>
<span id="A1.T9.1.1.1.1.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.1.1.1.1.1.1.8.7.2.1" class="ltx_text" style="background-color:#40C4DF;">37.3</span></span>
<span id="A1.T9.1.1.1.1.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.1.1.1.1.1.1.8.7.3.1" class="ltx_text" style="background-color:#40C4DF;">23.4</span></span>
<span id="A1.T9.1.1.1.1.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.1.1.1.1.1.1.8.7.4.1" class="ltx_text" style="background-color:#40C4DF;">68.4</span></span>
<span id="A1.T9.1.1.1.1.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.1.1.1.1.1.1.8.7.5.1" class="ltx_text" style="background-color:#40C4DF;">49.1</span></span>
<span id="A1.T9.1.1.1.1.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.1.1.1.1.1.1.8.7.6.1" class="ltx_text" style="background-color:#40C4DF;">50.2</span></span>
<span id="A1.T9.1.1.1.1.1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="A1.T9.1.1.1.1.1.1.8.7.7.1" class="ltx_text" style="background-color:#40C4DF;">56.3</span></span>
<span id="A1.T9.1.1.1.1.1.1.8.7.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T9.1.1.1.1.1.1.8.7.8.1" class="ltx_text" style="background-color:#40C4DF;">47.5</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">í‘œ 9:</span>Zero-shot evaluation of OLMo-7B on 6 additional end-tasks apart the 8 present in our core evaluation suite. ë‹¤ì‹œ í•œ ë²ˆ OLMo-7Bë¥¼ ê³µê°œì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” 6ê°œì˜ ë‹¤ë¥¸ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ì™€ ë¹„êµí•œë‹¤. ìš°ë¦¬ëŠ” OLMo-7Bê°€ ì´ í‘œì—ì„œ 6ê°œì˜ ì¶”ê°€ ìµœì¢… ì‘ì—…ì„ ìˆ˜í–‰í•œ ì§‘í•©ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•˜ì§€ë§Œ ì´ëŸ¬í•œ ì‘ì—…ë„ í›ˆë ¨ ì¤‘ì— ì œí•œëœ ì‹ í˜¸ë¥¼ ì œê³µí•˜ëŠ” ê²ƒìœ¼ë¡œ í™•ì¸ë˜ì—ˆë‹¤(ê·¸ë¦¼ <a class="ltx_ref" href="#A1.F4" title="Figure 4 â€£ Additional end-task results â€£ Appendix A Additional Evaluation â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> ì°¸ì¡°).</figcaption>
</figure>
<div id="A1.SS0.SSS0.Px2.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p2.1">ê·¸ëŸ¬ë‚˜ ì„¹ì…˜ <a class="ltx_ref" href="#S4.SS1" title="4.1 Downstream evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>ì— ì„¤ëª…ëœ í•µì‹¬ í‰ê°€ ì„¸íŠ¸ì™€ ëŒ€ì¡°ì ìœ¼ë¡œ ì´ëŸ¬í•œ ì¶”ê°€ ì—”ë“œ íƒœìŠ¤í¬ê°€ ëª¨ë¸ ê°œë°œ ë™ì•ˆ ëœ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ê°€ì§€ë©° ì œí•œëœ ì‹ í˜¸ë¥¼ ì œê³µí•œë‹¤ëŠ” ì ì— ì£¼ëª©í•œë‹¤. ì´ê²ƒì€ ê·¸ë¦¼ <a class="ltx_ref" href="#A1.F4" title="Figure 4 â€£ Additional end-task results â€£ Appendix A Additional Evaluation â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>ì— ì˜ˆì‹œë˜ì–´ ìˆìœ¼ë©°, ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” í›ˆë ¨ ì „ë°˜ì— ê±¸ì³ ì‘ì—… ìˆ˜í–‰ì˜ ì§„ì²™ì´ ë” ë¬´ì‘ìœ„ì ì¸ ê²ƒìœ¼ë¡œ ë³¸ë‹¤(ê·¸ë¦¼ <a class="ltx_ref" href="#S4.F1" title="Figure 1 â€£ Results â€£ 4.1 Downstream evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>ì—ì„œ ë” ì•ˆì •ì ì¸ ìƒìŠ¹ ì¶”ì„¸ì™€ ë¹„êµ). <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.1">mrpc</span> ë° <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.2">wic</span>ê³¼ ê°™ì€ ì‘ì—…ì€ ë” ì•ˆì •ì ìœ¼ë¡œ ë³´ì´ì§€ë§Œ ì„±ëŠ¥ê³¼ ê´€ë ¨ëœ ì¶”ê°€ ì–´ë ¤ì›€ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. ëœë¤ ì°¬ìŠ¤(ì˜ˆ: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.3">wic</span> ë˜ëŠ” ë°ì´í„° ì„¸íŠ¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•(ì˜ˆ: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.4">mrpc</span>). ë”°ë¼ì„œ ìš°ë¦¬ëŠ” í›ˆë ¨ ë° ëª¨ë¸ ë¹„êµ ì „ë°˜ì— ê±¸ì³ ëª¨ë¸ ì„±ëŠ¥ì„ ì¸¡ì •í•  ë•Œ ì´ëŸ¬í•œ ì‘ì—…ì— ë„ˆë¬´ ë§ì´ ì˜ì¡´í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•œë‹¤.</p>
</div>
<figure id="A1.F4" class="ltx_figure">
<p id="A1.F4.1" class="ltx_p ltx_align_center"><span id="A1.F4.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<img src="https://ar5iv.labs.arxiv.org/html/2402.00838/assets/x10.png" id="A1.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="278" alt="Refer to caption">
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">ê·¸ë¦¼ 4:</span>6ê°œì˜ ì¶”ê°€ ì—”ë“œ íƒœìŠ¤í¬ì—ì„œ OLMo-7Bì˜ ì •í™•ë„ ì ìˆ˜ ì§„í–‰. ì´ëŸ¬í•œ ì¶”ê°€ ì—”ë“œ íƒœìŠ¤í¬ì˜ ì„±ëŠ¥ì€ ë¶ˆì•ˆì •í•˜ê³  ëª¨ë¸ ê°œë°œ ë™ì•ˆ ì œí•œëœ ì‹ í˜¸ë¥¼ ì œê³µí–ˆë‹¤.</figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Adaptation Training Details</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p" id="A2.p1.1">ìš°ë¦¬ëŠ” ëª…ë ¹ì–´ íŠœë‹ OLMoì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ë“¤ì€ ì‘ì€ íŒŒì¼ëŸ¿ ì‹¤í—˜ì„ í†µí•´ ì„ íƒë˜ì—ˆë‹¤.</p>
</div>
<div id="A2.p2" class="ltx_para">
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p">Learning Rate: <math id="A2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="2\times 10^{-6}" display="inline"><semantics id="A2.I1.i1.p1.1.m1.1a"><mrow id="A2.I1.i1.p1.1.m1.1.1" xref="A2.I1.i1.p1.1.m1.1.1.cmml"><mn id="A2.I1.i1.p1.1.m1.1.1.2" xref="A2.I1.i1.p1.1.m1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="A2.I1.i1.p1.1.m1.1.1.1" xref="A2.I1.i1.p1.1.m1.1.1.1.cmml">Ã—</mo><msup id="A2.I1.i1.p1.1.m1.1.1.3" xref="A2.I1.i1.p1.1.m1.1.1.3.cmml"><mn id="A2.I1.i1.p1.1.m1.1.1.3.2" xref="A2.I1.i1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A2.I1.i1.p1.1.m1.1.1.3.3" xref="A2.I1.i1.p1.1.m1.1.1.3.3.cmml"><mo id="A2.I1.i1.p1.1.m1.1.1.3.3a" xref="A2.I1.i1.p1.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="A2.I1.i1.p1.1.m1.1.1.3.3.2" xref="A2.I1.i1.p1.1.m1.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.I1.i1.p1.1.m1.1b"><apply id="A2.I1.i1.p1.1.m1.1.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1"><times id="A2.I1.i1.p1.1.m1.1.1.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1.1"></times><cn type="integer" id="A2.I1.i1.p1.1.m1.1.1.2.cmml" xref="A2.I1.i1.p1.1.m1.1.1.2">2</cn><apply id="A2.I1.i1.p1.1.m1.1.1.3.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="A2.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3.2">10</cn><apply id="A2.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3.3"><minus id="A2.I1.i1.p1.1.m1.1.1.3.3.1.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="A2.I1.i1.p1.1.m1.1.1.3.3.2.cmml" xref="A2.I1.i1.p1.1.m1.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I1.i1.p1.1.m1.1c">2\times 10^{-6}</annotation></semantics></math></p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i2.p1.1">Epochs: 3</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i3.p1.1">ì›Œë°ì—…: ì „ì²´ í›ˆë ¨ ì‹œê°„ì˜ ì²˜ìŒ 3% ë™ì•ˆ ì„ í˜• ì›Œë°ì—…ì„ ìˆ˜í–‰í•œ ë‹¤ìŒ ë‚˜ë¨¸ì§€ ë‹¨ê³„ì— ê±¸ì³ í•™ìŠµë¥  0ìœ¼ë¡œ ì„ í˜• ì¿¨ë‹¤ìš´í•œë‹¤.</p>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i4.p1.1">Weight Decay: 0</p>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i5.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i5.p1.1">êµ¬ë°° í´ë¦¬í•‘: 0</p>
</div>
</li>
<li id="A2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I1.i6.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i6.p1.1">ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: 2048</p>
</div>
</li>
</ul>
</div>
<div id="A2.p3" class="ltx_para">
<p class="ltx_p" id="A2.p3.1">Instruction finetuning í›„, ë‹¤ìŒ <cite class="ltx_cite ltx_citemacro_citet">Ivison etÂ al. (<a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite> ë‹¤ìŒì— ë‹¤ìŒ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ DPO íŠ¸ë ˆì´ë‹ì— ì‚¬ìš©í•œë‹¤:</p>
</div>
<div id="A2.p4" class="ltx_para">
<ul id="A2.I2" class="ltx_itemize">
<li id="A2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I2.i1.p1" class="ltx_para">
<p id="A2.I2.i1.p1.1" class="ltx_p">Learning Rate: <math id="A2.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="5\times 10^{-7}" display="inline"><semantics id="A2.I2.i1.p1.1.m1.1a"><mrow id="A2.I2.i1.p1.1.m1.1.1" xref="A2.I2.i1.p1.1.m1.1.1.cmml"><mn id="A2.I2.i1.p1.1.m1.1.1.2" xref="A2.I2.i1.p1.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="A2.I2.i1.p1.1.m1.1.1.1" xref="A2.I2.i1.p1.1.m1.1.1.1.cmml">Ã—</mo><msup id="A2.I2.i1.p1.1.m1.1.1.3" xref="A2.I2.i1.p1.1.m1.1.1.3.cmml"><mn id="A2.I2.i1.p1.1.m1.1.1.3.2" xref="A2.I2.i1.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="A2.I2.i1.p1.1.m1.1.1.3.3" xref="A2.I2.i1.p1.1.m1.1.1.3.3.cmml"><mo id="A2.I2.i1.p1.1.m1.1.1.3.3a" xref="A2.I2.i1.p1.1.m1.1.1.3.3.cmml">âˆ’</mo><mn id="A2.I2.i1.p1.1.m1.1.1.3.3.2" xref="A2.I2.i1.p1.1.m1.1.1.3.3.2.cmml">7</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.I2.i1.p1.1.m1.1b"><apply id="A2.I2.i1.p1.1.m1.1.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1"><times id="A2.I2.i1.p1.1.m1.1.1.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1.1"></times><cn type="integer" id="A2.I2.i1.p1.1.m1.1.1.2.cmml" xref="A2.I2.i1.p1.1.m1.1.1.2">5</cn><apply id="A2.I2.i1.p1.1.m1.1.1.3.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.I2.i1.p1.1.m1.1.1.3.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="A2.I2.i1.p1.1.m1.1.1.3.2.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3.2">10</cn><apply id="A2.I2.i1.p1.1.m1.1.1.3.3.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3.3"><minus id="A2.I2.i1.p1.1.m1.1.1.3.3.1.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3.3"></minus><cn type="integer" id="A2.I2.i1.p1.1.m1.1.1.3.3.2.cmml" xref="A2.I2.i1.p1.1.m1.1.1.3.3.2">7</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.I2.i1.p1.1.m1.1c">5\times 10^{-7}</annotation></semantics></math></p>
</div>
</li>
<li id="A2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I2.i2.p1" class="ltx_para">
<p id="A2.I2.i2.p1.1" class="ltx_p"><math id="A2.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="A2.I2.i2.p1.1.m1.1a"><mi id="A2.I2.i2.p1.1.m1.1.1" xref="A2.I2.i2.p1.1.m1.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="A2.I2.i2.p1.1.m1.1b"><ci id="A2.I2.i2.p1.1.m1.1.1.cmml" xref="A2.I2.i2.p1.1.m1.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.I2.i2.p1.1.m1.1c">\beta</annotation></semantics></math>: 0.1</p>
</div>
</li>
<li id="A2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="A2.I2.i3.p1.1">Epochs: 3</p>
</div>
</li>
<li id="A2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="A2.I2.i4.p1.1">ì›Œë°ì—…: ì „ì²´ í›ˆë ¨ ì‹œê°„ì˜ ì²˜ìŒ 10% ë™ì•ˆ ì„ í˜• ì›Œë°ì—…ì„ ìˆ˜í–‰í•œ ë‹¤ìŒ, ë‚˜ë¨¸ì§€ ë‹¨ê³„ì— ê±¸ì³ 0ì˜ í•™ìŠµë¥ ë¡œ ì„ í˜• ì¿¨ë‹¤ìš´í•œë‹¤.</p>
</div>
</li>
<li id="A2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I2.i5.p1" class="ltx_para">
<p class="ltx_p" id="A2.I2.i5.p1.1">Weight Decay: 0</p>
</div>
</li>
<li id="A2.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I2.i6.p1" class="ltx_para">
<p class="ltx_p" id="A2.I2.i6.p1.1">êµ¬ë°° í´ë¦¬í•‘: 0</p>
</div>
</li>
<li id="A2.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A2.I2.i7.p1" class="ltx_para">
<p class="ltx_p" id="A2.I2.i7.p1.1">ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: 2048</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Adaptation Evaluation and Model details</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p" id="A3.p1.1">ìš°ë¦¬ëŠ” í‘œ<a class="ltx_ref" href="#S4.T6" title="Table 6 â€£ Setup â€£ 4.1 Downstream evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>ì—ì„œ ë¹„êµí•œ ê¸°ë³¸ ëª¨ë¸ì˜ 'ì •ê·œ' ìµœìƒì˜ ë²„ì „(ì¦‰, ë™ì¼í•œ ì¡°ì§ì—ì„œ ë¦´ë¦¬ìŠ¤í•œ ìµœìƒì˜ ëª…ë ¹ ì¡°ì • ë˜ëŠ” ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì ì‘ëœ ëª¨ë¸)ì„ ì„ íƒí•˜ì—¬ í‘œ<a class="ltx_ref" href="#S4.T7" title="Table 7 â€£ 4.3 Adaptation Evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>ì˜ ëª¨ë¸ì„ ì„ íƒí•œë‹¤. ë˜í•œ <span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.1">TÃ¼lu</span> 2ì™€ ë¹„êµí•˜ì—¬ Finetune OLMoì— ì‚¬ìš©ëœ <span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.2">TÃ¼lu</span> mixë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ëœ í˜„ì¬ ìµœìƒì˜ ëª¨ë¸ì„ ë³´ì—¬ì¤€ë‹¤. MLU, AlpacaEval, ToxiGen ë° Truthfulnessì— ëŒ€í•œ í‰ê°€ë¥¼ í‘œì‹œí•˜ì—¬ ëª…ë ¹ì–´ íŠœë‹ì´ ì¼ë°˜ì ìœ¼ë¡œ ê¸°ëŠ¥(MMLU), ëª¨ë¸ì´ ê°œë°©í˜• ì±„íŒ… ì„¤ì •(AlpacaEval)ì—ì„œ ìˆ˜í–‰í•˜ëŠ” ë°©ë²• ë° ëª…ë ¹ì–´ íŠœë‹ì´ ëª¨ë¸ ì•ˆì „ì„±ê³¼ ì§„ì‹¤ì„±(AlpacaEval, ToxiGen)ì— ë„ì›€ì´ ë˜ëŠ” ë°©ë²•ì„ í‘œì‹œí•˜ëŠ” ë° ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë˜í•œ í‘œ <a class="ltx_ref" href="#A3.T10" title="Table 10 â€£ Appendix C Adaptation Evaluation and Model details â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">10</span></a>ì˜ <span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.3">TÃ¼lu</span> í‰ê°€ ì œí’ˆêµ°ì— ëŒ€í•œ OLMoì˜ ì„±ëŠ¥ì„ ë³´ê³ í•œë‹¤.</p>
</div>
<figure id="A3.T10" class="ltx_table">
<div id="A3.T10.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:73.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.2pt,8.6pt) scale(0.80901,0.80901) ;">
<p id="A3.T10.1.1" class="ltx_p"><span id="A3.T10.1.1.1" class="ltx_text"> <span id="A3.T10.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:536.0pt;height:91pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;"> <span id="A3.T10.1.1.1.1.1" class="ltx_p"><span id="A3.T10.1.1.1.1.1.1" class="ltx_text">  <span id="A3.T10.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="A3.T10.1.1.1.1.1.1.1.1.1" class="ltx_tr"> <span id="A3.T10.1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="A3.T10.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span> <span id="A3.T10.1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A3.T10.1.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">MMLU</span></span> <span id="A3.T10.1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A3.T10.1.1.1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">GSM8k</span></span> <span id="A3.T10.1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A3.T10.1.1.1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">BBH</span></span> <span id="A3.T10.1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A3.T10.1.1.1.1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">TydiQA</span></span> <span id="A3.T10.1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A3.T10.1.1.1.1.1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Codex-Eval</span></span> <span id="A3.T10.1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A3.T10.1.1.1.1.1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">AlpacaEval</span></span> <span id="A3.T10.1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A3.T10.1.1.1.1.1.1.1.1.1.8.1" class="ltx_text ltx_font_bold">ToxiGen</span></span> <span id="A3.T10.1.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A3.T10.1.1.1.1.1.1.1.1.1.9.1" class="ltx_text ltx_font_bold">TruthfulQA</span></span></span> <span id="A3.T10.1.1.1.1.1.1.1.2.2" class="ltx_tr"> <span id="A3.T10.1.1.1.1.1.1.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></span> <span id="A3.T10.1.1.1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A3.T10.1.1.1.1.1.1.1.2.2.2.1" class="ltx_text ltx_font_bold">0-shot</span></span> <span id="A3.T10.1.1.1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A3.T10.1.1.1.1.1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">8-shot CoT</span></span> <span id="A3.T10.1.1.1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A3.T10.1.1.1.1.1.1.1.2.2.4.1" class="ltx_text ltx_font_bold">3-shot CoT</span></span> <span id="A3.T10.1.1.1.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A3.T10.1.1.1.1.1.1.1.2.2.5.1" class="ltx_text ltx_font_bold">1-shot</span></span> <span id="A3.T10.1.1.1.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A3.T10.1.1.1.1.1.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Pass@10</span></span> <span id="A3.T10.1.1.1.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A3.T10.1.1.1.1.1.1.1.2.2.7.1" class="ltx_text ltx_font_bold">%win</span></span> <span id="A3.T10.1.1.1.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A3.T10.1.1.1.1.1.1.1.2.2.8.1" class="ltx_text ltx_font_bold">% Toxic</span></span> <span id="A3.T10.1.1.1.1.1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A3.T10.1.1.1.1.1.1.1.2.2.9.1" class="ltx_text ltx_font_bold">% Info + True</span></span></span> </span> <span class="ltx_tbody"> <span id="A3.T10.1.1.1.1.1.1.1.3.1" class="ltx_tr"> <span id="A3.T10.1.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A3.T10.1.1.1.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">OLMo-7B</span></span> <span id="A3.T10.1.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">28.3</span> <span id="A3.T10.1.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">8.5</span> <span id="A3.T10.1.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">31.7</span> <span id="A3.T10.1.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">32.3</span> <span id="A3.T10.1.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">21.4</span> <span id="A3.T10.1.1.1.1.1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">-</span> <span id="A3.T10.1.1.1.1.1.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">81.4</span> <span id="A3.T10.1.1.1.1.1.1.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">31.6</span></span> <span id="A3.T10.1.1.1.1.1.1.1.4.2" class="ltx_tr"> <span id="A3.T10.1.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="A3.T10.1.1.1.1.1.1.1.4.2.1.1" class="ltx_text ltx_font_bold">+SFT</span></span> <span id="A3.T10.1.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_center">47.3</span> <span id="A3.T10.1.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_center">15.5</span> <span id="A3.T10.1.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_center">36.9</span> <span id="A3.T10.1.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_center">35.2</span> <span id="A3.T10.1.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_center">28.6</span> <span id="A3.T10.1.1.1.1.1.1.1.4.2.7" class="ltx_td ltx_align_center">57.0</span> <span id="A3.T10.1.1.1.1.1.1.1.4.2.8" class="ltx_td ltx_align_center">14.4</span> <span id="A3.T10.1.1.1.1.1.1.1.4.2.9" class="ltx_td ltx_align_center">41.2</span></span> <span id="A3.T10.1.1.1.1.1.1.1.5.3" class="ltx_tr"> <span id="A3.T10.1.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="A3.T10.1.1.1.1.1.1.1.5.3.1.1" class="ltx_text ltx_font_bold">+SFT+DPO</span></span> <span id="A3.T10.1.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb">46.1</span> <span id="A3.T10.1.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">11.0</span> <span id="A3.T10.1.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb">35.8</span> <span id="A3.T10.1.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb">21.7</span> <span id="A3.T10.1.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb">27.8</span> <span id="A3.T10.1.1.1.1.1.1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_bb">69.3</span> <span id="A3.T10.1.1.1.1.1.1.1.5.3.8" class="ltx_td ltx_align_center ltx_border_bb">1.7</span> <span id="A3.T10.1.1.1.1.1.1.1.5.3.9" class="ltx_td ltx_align_center ltx_border_bb">52.0</span></span> </span> </span></span></span> </span></span></span></p>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">í‘œ 10:</span>Evaluation of OLMo-7B models before and after instruction finetuning and DPO training on the full <span class="ltx_text ltx_font_smallcaps" id="A3.T10.3.1">TÃ¼lu</span> evaluation suite. ë‚®ì€ ê²ƒì€ ToxiGenì— ë” ì¢‹ê³  ë†’ì€ ê²ƒì€ ë‹¤ë¥¸ ì§€í‘œì— ë” ì¢‹ë‹¤.</figcaption>
</figure>
<div id="A3.p2" class="ltx_para">
<p class="ltx_p" id="A3.p2.1">ì•„ë˜ í‘œ <a class="ltx_ref" href="#S4.T7" title="Table 7 â€£ 4.3 Adaptation Evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>ì—ì„œ í‰ê°€ëœ ê° ëª¨ë¸ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì„ ì œê³µí•œë‹¤. ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì œê³µëœ ì±„íŒ… í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì„ ì§€ì •í•©ë‹ˆë‹¤.</p>
<ul id="A3.I1" class="ltx_itemize">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A3.I1.i1.p1.1">MPT Chat: A version of MPT 7B finetuned on the ShareGPT-VicunaÂ <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al., <a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>, HC3Â <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al., <a class="ltx_ref" href="#bib.bib29" title="">2023</a>)</cite>, AlpacaÂ <cite class="ltx_cite ltx_citemacro_citep">(Taori etÂ al., <a class="ltx_ref" href="#bib.bib74" title="">2023</a>)</cite>, HH-RLHFÂ <cite class="ltx_cite ltx_citemacro_citep">(Bai etÂ al., <a class="ltx_ref" href="#bib.bib5" title="">2022</a>)</cite>, and Evol-InstructÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="#bib.bib87" title="">2024</a>)</cite> datasets. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/mosaicml/mpt-7b-chat" target="_blank" title="">https://huggingface.co/mosaicml/mpt-7b-chat</a>ì—ì„œ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
</div>
</li>
<li id="A3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A3.I1.i2.p1.1">Falcon Instruct: A version of Falcon 7B finetuned on the BaizeÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="#bib.bib88" title="">2023</a>)</cite>, GPT4AllÂ <cite class="ltx_cite ltx_citemacro_citep">(Anand etÂ al., <a class="ltx_ref" href="#bib.bib3" title="">2023</a>)</cite>, GPTeacherÂ <cite class="ltx_cite ltx_citemacro_citep">(Teknium1, <a class="ltx_ref" href="#bib.bib75" title="">2023</a>)</cite>, and Refined-Web EnglishÂ <cite class="ltx_cite ltx_citemacro_citep">(Penedo etÂ al., <a class="ltx_ref" href="#bib.bib57" title="">2023</a>)</cite> datasets. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/tiiuae/falcon-7b-instruct" target="_blank" title="">https://huggingface.co/tiiuae/falcon-7b-instruct</a>ì—ì„œ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
</div>
</li>
<li id="A3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A3.I1.i3.p1.1">RPJ-INCITE Chat: A version of RPJ-INCITE 7B finetuned on the OASST1Â <cite class="ltx_cite ltx_citemacro_citep">(KÃ¶pf etÂ al., <a class="ltx_ref" href="#bib.bib35" title="">2023</a>)</cite> and Dolly V2Â <cite class="ltx_cite ltx_citemacro_citep">(Conover etÂ al., <a class="ltx_ref" href="#bib.bib19" title="">2023</a>)</cite> datasets. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat" target="_blank" title="">https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat</a>ì—ì„œ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
</div>
</li>
<li id="A3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="A3.I1.i4.p1.1">Llama-2 Chat: Llama 2 7Bì˜ ë²„ì „ì€ ëª…ë ¹ì–´ ë°ì´í„°ì„¸íŠ¸ì˜ í˜¼í•©ë¬¼ ìƒì—ì„œ ë¯¸ì„¸ì¡°ì •ë˜ê³  RLHFë¡œ ì¶”ê°€ë¡œ íŠ¸ë ˆì´ë‹ëœë‹¤. ìì„¸í•œ ë‚´ìš©ì€ íŒë…ê¸°ë¥¼ <cite class="ltx_cite ltx_citemacro_citet">Touvron etÂ al. (<a class="ltx_ref" href="#bib.bib78" title="">2023b</a>)</cite>ì— ì°¸ì¡°í•©ë‹ˆë‹¤.</p>
</div>
</li>
<li id="A3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I1.i5.p1" class="ltx_para">
<p class="ltx_p" id="A3.I1.i5.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A3.I1.i5.p1.1.1">TÃ¼lu</span> 2: A version of Llama 2 7B finetuned on the mixture of instruction datasets (the <span class="ltx_text ltx_font_smallcaps" id="A3.I1.i5.p1.1.2">TÃ¼lu</span> 2 mix). ìì„¸í•œ ë‚´ìš©ì€ íŒë…ê¸°ë¥¼ <cite class="ltx_cite ltx_citemacro_citet">Ivison etÂ al. (<a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite>ì— ì°¸ì¡°í•©ë‹ˆë‹¤.</p>
</div>
</li>
<li id="A3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I1.i6.p1" class="ltx_para">
<p class="ltx_p" id="A3.I1.i6.p1.1"><span class="ltx_text ltx_font_smallcaps" id="A3.I1.i6.p1.1.1">TÃ¼lu</span> 2+DPO: <span class="ltx_text ltx_font_smallcaps" id="A3.I1.i6.p1.1.2">TÃ¼lu</span> 2 further trained with DPO on the UltraFeedback datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Cui etÂ al., <a class="ltx_ref" href="#bib.bib20" title="">2023</a>)</cite>. ìì„¸í•œ ë‚´ìš©ì€ íŒë…ê¸°ë¥¼ <cite class="ltx_cite ltx_citemacro_citet">Ivison etÂ al. (<a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite>ì— ì°¸ì¡°í•©ë‹ˆë‹¤.</p>
</div>
</li>
<li id="A3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I1.i7.p1" class="ltx_para">
<p class="ltx_p" id="A3.I1.i7.p1.1">OLMo +SFT: A version of OLMo 7B fintuned on the same data as <span class="ltx_text ltx_font_smallcaps" id="A3.I1.i7.p1.1.1">TÃ¼lu</span> 2.</p>
</div>
</li>
<li id="A3.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I1.i8.p1" class="ltx_para">
<p class="ltx_p" id="A3.I1.i8.p1.1">OLMo +SFT+DPO: OLMo +SFTëŠ” UltraFeedback ë°ì´í„°ì…‹ <cite class="ltx_cite ltx_citemacro_citep">(Cui etÂ al., <a class="ltx_ref" href="#bib.bib20" title="">2023</a>)</cite>ì—ì„œ DPOë¡œ ì¶”ê°€ í›ˆë ¨ë˜ì—ˆë‹¤.</p>
</div>
</li>
</ul>
</div>
<div id="A3.p3" class="ltx_para">
<p class="ltx_p" id="A3.p3.1">ìš°ë¦¬ëŠ” ì¶”ê°€ë¡œ TableÂ <a class="ltx_ref" href="#S4.T7" title="Table 7 â€£ 4.3 Adaptation Evaluation â€£ 4 Results â€£ OLMo : Accelerating the Science of Language Models"><span class="ltx_text ltx_ref_tag">7</span></a>ë¡œë¶€í„° ê°ê°ì˜ í‰ê°€ ì„¤ì •ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì„ ì œê³µí•œë‹¤:</p>
<ul id="A3.I2" class="ltx_itemize">
<li id="A3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="A3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i1.p1.1.1">MMLU</span>: ìš°ë¦¬ëŠ” ê³µì‹ MMLU <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks etÂ al., <a class="ltx_ref" href="#bib.bib32" title="">2021</a>)</cite> í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ë° í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hendrycks/test" target="_blank" title="">https://github.com/hendrycks/test</a>ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ ì¼ê´„ ì²˜ë¦¬ë¥¼ í—ˆìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” MMLUì˜ ì›ë˜ ì„¤ì •ì— ë”°ë¼ 0ê°œì˜ ì†Œìƒ· ì˜ˆì œë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•œë‹¤. ìš°ë¦¬ëŠ” í…ŒìŠ¤íŠ¸ ì˜ˆì œì— ê±¸ì¹œ í‰ê·  ì •í™•ë„ë¥¼ ë³´ê³ í•œë‹¤.</p>
</div>
</li>
<li id="A3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="A3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i2.p1.1.1">ToxiGen</span>: <cite class="ltx_cite ltx_citemacro_citet">Touvron etÂ al. (<a class="ltx_ref" href="#bib.bib78" title="">2023b</a>)</cite>ì—ì„œ ì„¤ì •ì„ ë”°ë¥´ì§€ë§Œ íŠ¹ì • ê·¸ë£¹ì— ëŒ€í•œ ë…ì„± ìƒì„±ì„ ìœ ë„í•˜ë„ë¡ ì„¤ê³„ëœ <cite class="ltx_cite ltx_citemacro_citet">Hartvigsen etÂ al. (<a class="ltx_ref" href="#bib.bib31" title="">2022</a>)</cite>ì˜ í”„ë¡¬í”„íŠ¸ ì›ë³¸ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë…ì„± ì–¸ì–´ë¥¼ ìƒì„±í•˜ë„ë¡ ì„¤ê³„ëœ í”„ë¡¬í”„íŠ¸('í˜ì˜¤ìŠ¤ëŸ¬ìš´' í”„ë¡¬í”„íŠ¸)ë§Œ ì·¨í•˜ê³  í‰ê°€ ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ ê·¸ë£¹ë‹¹ 500ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œë‹¤. ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸ì˜ ê²½ìš° ë³€ê²½ë˜ì§€ ì•Šì€ ì›ë˜ í†¡ì‹œì   í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ê³  ì²« ë²ˆì§¸ ìƒˆ ë¼ì¸(ë˜ëŠ” ìµœëŒ€ 512 í† í°)ê¹Œì§€ íƒìš•ìŠ¤ëŸ½ê²Œ ë””ì½”ë”©í•©ë‹ˆë‹¤. ëª…ë ¹ì–´ ì¡°ì • ëª¨ë¸ì˜ ê²½ìš° í•´ë‹¹ í…œí”Œë¦¿ì— í”„ë¡¬í”„íŠ¸ë¥¼ ë°°ì¹˜í•˜ê³  ëª¨ë¸ì´ ì¤‘ì§€ í† í°(ë˜ëŠ” ìµœëŒ€ 512 í† í°)ì„ ìƒì„±í•  ë•Œê¹Œì§€ ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ë£Œí•˜ë„ë¡ ìš”ì²­í•©ë‹ˆë‹¤. ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ <cite class="ltx_cite ltx_citemacro_citet">Hartvigsen etÂ al. (<a class="ltx_ref" href="#bib.bib31" title="">2022</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote28"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup><span class="ltx_tag ltx_tag_note">28</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/tomh/toxigen_roberta" target="_blank" title="">https://huggingface.co/tomh/toxigen_roberta</a></span></span></span>ì˜ ì¼ë¶€ë¡œ ë¯¸ì„¸ ì¡°ì •ëœ ë…ì„± ì½˜í…ì¸ ë¥¼ íƒì§€í•˜ë„ë¡ í›ˆë ¨ëœ ë¡œë²„íƒ€ ëŒ€ ëª¨ë¸ì— ì „ë‹¬í•œë‹¤. ê·¸ëŸ° ë‹¤ìŒ ë¶„ë¥˜ê¸°ì— ì˜í•´ ë…ì„±ì´ ìˆë‹¤ê³  ê°„ì£¼ë˜ëŠ” ì„¸ëŒ€ì˜ ë¹„ìœ¨ì„ ë³´ê³ í•œë‹¤.</p>
</div>
</li>
<li id="A3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="A3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i3.p1.1.1">TruthfulQA</span>: Following <cite class="ltx_cite ltx_citemacro_citet">Touvron etÂ al. (<a class="ltx_ref" href="#bib.bib78" title="">2023b</a>)</cite>ëŠ” ì£¼ë¡œ TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite>ì˜ ìƒì„± ì„¤ì •ì„ ì‚¬ìš©í•œë‹¤. TruthfulQA ë°ì´í„° ì„¸íŠ¸ì—ëŠ” 818ê°œì˜ ì§ˆë¬¸ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ì´ëŠ” í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸ì„ í”„ë¡¬í”„íŠ¸í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” 6ê°œì˜ ë¬¸ë§¥ ë‚´ QA ì˜ˆì œì™€ í•¨ê»˜ ê¸°ë³¸ QA í”„ë¡¬í”„íŠ¸ í˜•ì‹ì„ ì‚¬ìš©í•œë‹¤. ìš°ë¦¬ëŠ” ê·¸ë¦¬ë”” ë””ì½”ë”©ì„ ìˆ˜í–‰í•˜ê³  í›„ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ê³µì‹ êµ¬í˜„ <span class="ltx_note ltx_role_footnote" id="footnote29"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><span class="ltx_tag ltx_tag_note">29</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sylinrl/TruthfulQA/" target="_blank" title="">https://github.com/sylinrl/TruthfulQA/</a></span></span></span>ì—ì„œ ê³µì‹ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë”°ë¦…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì›ë˜ TruthfulQA í‰ê°€ë¥¼ ì •í™•í•˜ê²Œ ë³µì œí•  ìˆ˜ ì—†ëŠ” GPT-3ì˜ ë¹„í™œì„±í™”ë¡œ ì¸í•´ ëª¨ë¸ ë°˜ì‘ì˜ ì§„ì‹¤ì„±ê³¼ ì •ë³´ì„±ì„ íŒë‹¨í•˜ê¸° ìœ„í•´ ë‘ ê°œì˜ LLaMA 2 ê¸°ë°˜ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨í•œë‹¤. ìš°ë¦¬ëŠ” LLaMA 2 íŒì‚¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ <cite class="ltx_cite ltx_citemacro_citet">Lin etÂ al. (<a class="ltx_ref" href="#bib.bib38" title="">2022</a>)</cite>ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì›ë˜ GPT-3 ê¸°ë°˜ íŒì‚¬ì˜ ì„±ëŠ¥ê³¼ ì¼ì¹˜í•  ìˆ˜ ìˆìŒì„ ë°œê²¬í–ˆë‹¤. <cite class="ltx_cite ltx_citemacro_citet">Touvron etÂ al. (<a class="ltx_ref" href="#bib.bib78" title="">2023b</a>)</cite>ì— ì´ì–´ ì§„ì‹¤í•˜ê³  ìœ ìµí•œ(% Informative and Truthful) ì‘ë‹µì˜ ë¹„ìœ¨ì„ ë³´ê³ í•œë‹¤. ìš°ë¦¬ëŠ” % ì •ë³´ ë° ì§„ì‹¤ë§Œì„ ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ ë³´ê³ í•©ë‹ˆë‹¤.</p>
</div>
</li>
<li id="A3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="A3.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="A3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I2.i4.p1.1.1">AlpacaEval</span>: í‰ê°€ëœ ëª¨ë¸ì— 805 í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•˜ê³  GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ <span class="ltx_text" id="A3.I2.i4.p1.1.2">Davinci</span>-<span class="ltx_text" id="A3.I2.i4.p1.1.3">003</span>ê³¼ ë¹„êµí•˜ëŠ” ê¸°ë³¸ ì„¤ì •ì— ë”°ë¼ <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a class="ltx_ref" href="#bib.bib36" title="">2023</a>)</cite>ì—ì„œ ì œê³µí•˜ëŠ” íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” "alpaca_eval_gpt4" ì£¼ì„ê¸°ë¥¼ ì‚¬ìš©í•œë‹¤. ìš°ë¦¬ëŠ” í‰ê°€ëœ ëª¨ë¸ì´ íŠ¹ë³„í•œ ì •ì§€ ì‹œí€€ìŠ¤ë¥¼ ì§€ì •í•˜ì§€ ì•Šê³  ìµœëŒ€ 2048ê°œì˜ í† í°ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ë³´ê³ ëœ ìŠ¹ë¥ ì€ GPT-4ê°€ <span class="ltx_text" id="A3.I2.i4.p1.1.4">Davinci</span>-<span class="ltx_text" id="A3.I2.i4.p1.1.5">003</span>ì˜ ì„¸ëŒ€ì— ê±¸ì³ ì„ í˜¸ë˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ê³ í•˜ëŠ” ëª¨ë¸ ì„¸ëŒ€ì˜ ë°±ë¶„ìœ¨ì…ë‹ˆë‹¤.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="A3.p4" class="ltx_para">
<p id="A3.p4.1" class="ltx_p"></p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="javascript: void(0)" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2402.00838" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2402.00838">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.00838" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="javascript: void(0)" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 18:56:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
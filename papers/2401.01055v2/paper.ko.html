<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# LLaMA Beyond English: 언어능력전이에 관한 실증적 연구\n' +
      '\n' +
      'Jun Zhao\n' +
      '\n' +
      '이 작가들은 똑같이 기여했다.\n' +
      '\n' +
      'Zhihao Zhang1\n' +
      '\n' +
      'Luhui Gao\n' +
      '\n' +
      'Qi Zhang1\n' +
      '\n' +
      'Tao Gui\n' +
      '\n' +
      'Xuanjing Huang\n' +
      '\n' +
      '푸단대학교 컴퓨터학부\n' +
      '\n' +
      '{zhaoj19,zhangzhihao19,qz,tgui}@fudan.edu.cn\n' +
      '\n' +
      '각주 1: 대응 저자\n' +
      '\n' +
      '각주 2: 대응 저자\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근, ChatGPT로 예시되는 대규모 언어 모델(LLM)에서 상당한 발전이 목격되어 다양한 복잡한 작업에 걸쳐 놀라운 숙련도를 보여준다. 그러나, 많은 주류 LLMs(예를 들어, LLaMA)는 영어-지배 코퍼스에 사전 훈련되어, 다른 비영어 언어에서의 성능을 제한한다. 본 논문에서는 언어 생성 및 명령어 수행 능력을 비영어 언어로 효과적으로 전달하는 방법에 초점을 맞춘다. 이 질문에 답하기 위해 우리는 1440 GPU 시간이 넘는 LLaMA를 기반으로 광범위한 경험적 조사를 수행한다. 어휘 확장, 추가 사전 훈련 및 명령어 튜닝과 같은 핵심 요소가 전이에 미치는 영향을 분석한다. 모델의 지식 수준을 정확하게 평가하기 위해 널리 사용되는 4개의 표준화된 테스트 벤치마크인 C-Eval, MMLU, AGI-Eval 및 GAOKAO-Bench를 사용한다. 또한 17개의 다양한 범주의 수업 과제를 구성하는 벤치마크인 LLM-Eval을 기반으로 정확성, 유창성, 정보성, 논리적 일관성, 무해성 등의 측면을 고려하여 모델의 응답 품질을 종합적으로 평가한다. 평가 결과는 지식 정렬 및 응답 품질 측면에서 사전 훈련 데이터의 \\(1\\%\\) 미만에서 최첨단 전이 모델과 유사한 성능을 달성할 수 있음을 보여준다. 또한, 13개의 저자원 언어에 걸친 실험 결과도 유사한 경향을 보인다. 실험에 의해 밝혀진 결론이 커뮤니티가 비영어 LLM을 개발하는 데 도움이 될 것으로 기대한다.\n' +
      '\n' +
      '## Introduction\n' +
      '\n' +
      '수십 년 동안, 자연 언어 처리(NLP)의 연구자들은 지능의 기본 원리를 탐구해 왔다[1]. 최근 대형 언어 모델(LLM)의 발전은 희미한 희망을 드러낸 것 같다. 모델 크기와 훈련 데이터의 전례 없는 규모에서 이점을 얻은 ChatGPT[14], PaLM[15], LLaMA[16] 등과 같은 많은 LLMs는 추론[17], 계획[18], 경험[19]으로부터 인간 수준에서 또는 이를 능가하는 학습에서 강력한 능력을 출현시켰다. 이러한 일반적인 능력들은 또한 LLM들이 전체 UBE(Uniform Bar Examination) [14]를 성공적으로 완료하거나 또는 자연 언어 명령들에 기초한 코딩과 같은 복잡한 실세계 태스크들을 다루기 위한 기초를 제공한다(StabilityAI 2023).\n' +
      '\n' +
      '많은 잘 알려진 LLM은 여러 언어의 다양한 혼합 코퍼스에 대한 사전 훈련 덕분에 다양한 언어에 걸쳐 입력을 이해하고 응답을 생성할 수 있다. 그러나 언어 자원의 불균형적인 분포로 인해 모든 언어에 대한 광범위한 훈련 데이터 수집은 거의 불가능하다[10]. 대표적인 LLM BLOOM[13]을 예로 들면, 46개의 자연어로 사전 훈련되어 왔다. 그러나 이 숫자는 현재 사용 중인 대략 7,000\\ 언어 중 \\(0.66\\%\\)에 불과합니다. 더욱이 이 46개 언어의 말뭉치 안에는 자원이 풍부한 영어 텍스트가 자원이 적은 치텀바 언어보다 280만 배나 많은 극단적인 불균형이 존재한다. 이것은 고립된 사례가 아니다. 널리 논의되는 또 다른 언어 모델인 LLaMA는\n' +
      '\n' +
      '그림 1: 주로 영어 지배 코퍼스에 대해 훈련되는 사전 훈련된 LLaMA 모델은 본질적으로 비영어 언어를 다루는 데 능숙하지 않다. 우리는 어휘 확장, 추가 사전 훈련 및 명령어 튜닝의 필요성과 능력 전달에 어느 정도 영향을 미치는지 조사하는 것을 목표로 한다. 이 탐색을 통해 LLaMA의 언어 능력을 비영어권 언어(오른쪽에 예시된 바와 같이)로 효율적으로 이전할 수 있으며, 프로세스에서 비용을 최소화할 수 있다.\n' +
      '\n' +
      '라틴어 및 키릴어 스크립트를 사용하는 20개 관련 언어의 제한된 데이터로 보충된 영어 지배 코퍼스에 대해 주로 사전 훈련되었다. 결과적으로 LLaMA는 충분한 훈련을 거치지 않은 비영어권 언어와 관련된 상황에서 열등한 성능을 보인다. 일부 연구자들은 관심 있는 특정 언어에 대한 대규모 데이터를 수집하고 LLM을 재교육한다[20]. 그러나 이는 필연적으로 높은 계산 및 데이터 수집 비용을 초래하여 자원이 적은 언어에는 적합하지 않다. Cui, Yang 및 Yao[21]이 원래 어휘를 확장하고 LoRA[1]에 의해 30B 중국어 토큰으로 LLaMA를 추가로 사전 훈련하는 동안 유망한 결과를 보고한다. 그럼에도 불구하고 이송 과정에 대한 세밀한 체계적인 조사는 여전히 부족하다.\n' +
      '\n' +
      '이 작업에서 우리는 LLM에서 언어 능력 전이에 대한 포괄적인 이해를 얻기 위한 단계를 밟는다. 그림 1에서 볼 수 있듯이, 우리는 LLaMA를 기반으로 하는 몇 가지 주요 측면을 경험적으로 조사한다:\n' +
      '\n' +
      '(1) **어휘 확장이 전송에 미치는 영향** 원본 어휘에 대해 0.5억 개의 중국어 토큰을 사용한 추가 사전 훈련이 300억 개 이상의 토큰에 대해 추가 사전 훈련되었음에도 불구하고 확장 어휘에 대한 성능보다 훨씬 우수하다는 것을 발견했습니다. 이는 어휘 확장이 수백억 정도의 소규모 증분 사전 훈련에 적합한 선택이 아닐 수 있음을 시사한다.\n' +
      '\n' +
      '(2) **효과적인 전송에 필요한 교육 규모입니다.* * 1,000억 토큰 이하의 추가 중국 사전 교육은 LLaMA의 지식 수준을 크게 향상시키기에 충분하지 않습니다. 그러나, LLaMA의 응답 품질(즉, 언어 생성 능력)을 향상시키는 것은 대규모의 추가 사전 훈련보다는 수십만 개의 명령어 데이터만을 필요로 한다.\n' +
      '\n' +
      '(3) **이전 훈련이 원래 영어 능력에 미치는 영향** 이전 훈련에 대한 중국어 코퍼라에 대한 배타적 의존은 LLaMA의 원래 영어 능력을 현저하게 손상시키며, 이는 다국어 공동 훈련을 통해 효과적으로 완화된다는 것을 발견했다.\n' +
      '\n' +
      '앞서 언급한 연구 결과는 LLaMA의 언어 생성 및 지침에 따른 기능을 최소한의 비용으로 비영어 언어로 이전할 수 있도록 한다. 현재 널리 사용되고 있는 4개의 표준화된 테스트 벤치마크(C-Eval, GAOKAOBench, MMLU, AGI-Eval)와 명령어 평가 벤치마크 LLM-Eval의 평가 결과를 기반으로 학습 데이터의 \\(1\\%\\) 이하를 사용하면서도 최신 개방형 중국어 LLaMA와 유사한 지식 수준과 응답 품질을 달성한다. 또한, 다른 13개의 저자원 언어에 대한 확장 실험도 유사한 경향을 보인다. 본 논문에서는 비영어 LLMs를 구축하는데 있어 지역사회에 대한 도움과 지침을 제공하기 위한 실험 결과와 분석을 목표로 한다.\n' +
      '\n' +
      '## 배경 및 개요\n' +
      '\n' +
      '이 하위 섹션에서는 먼저 명령어 후속 LLM을 개발하기 위한 필수 단계를 제시한다. 그 후, 우리는 이 모델을 비영어 언어로 외삽하는 일반적인 관행을 검토하고 모델 외삽을 위해 수행된 경험적 연구의 개요를 제공한다.\n' +
      '\n' +
      '### 1단계: 언어 능력 및 지식 획득 사전 훈련\n' +
      '\n' +
      'LLM에 대한 기본 기능의 중요한 소스로서 사전 훈련은 접두사 시퀀스를 기반으로 다음 토큰을 예측하는 것을 목표로 한다. 형식적으로, 큰 코퍼스 \\(\\mathcal{D}\\)가 주어지면, 훈련 목적은 다음과 같은 손실을 최소화하는 것이다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{pretrain}=\\sum_{x\\in\\mathcal{D}}\\sum_{i}\\log p_{\\theta}(x_{i}|x_{1},...,x_{i-1}), \\tag{1}\\]\n' +
      '\n' +
      '여기서, \\(x=\\{x_{1},...,x_{n}\\}\\)은 입력 토큰 시퀀스를 나타낸다.\n' +
      '\n' +
      '수십억에서 수조 개의 토큰에 이르는 방대한 텍스트 데이터에 대해 사전 트레이닝함으로써, LLM은 복잡한 언어 구조, 의미 및 문맥 관계를 캡처할 수 있고, 이에 의해 강력한 언어 생성 능력을 획득할 수 있다. 또한 이러한 LLM은 개념, 사실 및 개념 간의 연결을 이해하는 방법도 배우므로 세계 지식을 광범위하게 이해할 수 있다.\n' +
      '\n' +
      '### 2단계: 인간의 의도와 정렬하기 위한 명령 튜닝\n' +
      '\n' +
      '명령 조정(SFT)은 LLM이 명령을 따르는 능력을 더욱 향상시키는 것을 목표로 한다. 학습 데이터는 많은 명령어-응답 쌍으로 구성된다. 모델은 단순히 선행 텍스트로부터 계속되는 것이 아니라, 명령들에 정확하게 응답하는 것을 배울 필요가 있다. 형식적으로, 명령 데이터세트 \\(\\mathcal{D}^{\\prime}=\\{(I,Y)\\}\\)가 주어지면, 여기서 \\(I\\)는 태스크 명령을 나타내고 \\(Y\\)는 원하는 응답을 나타내며, 명령 튜닝의 트레이닝 목적은 다음과 같은 손실을 최소화하는 것이다:\n' +
      '\n' +
      '\\[\\mathcal{L}_{ins}=-\\log p_{\\theta}(Y|I), \\tag{2}\\]\n' +
      '\n' +
      '다양한 지시 과제를 조정함으로써 모델은 인간의 지시를 더 잘 이해하고 따를 수 있으며, 보이지 않는 지시로 일반화할 수 있다.\n' +
      '\n' +
      '### LLMs를 영어가 아닌 언어로 외삽\n' +
      '\n' +
      'LLM은 사전 훈련 및 명령어 튜닝을 통해 언어 생성 및 명령어 후속 기능을 습득한다. 그러나 영어는 다양한 영역에서 가장 풍부한 텍스트 데이터 수집을 보유하면서 자연어 처리 분야에서 지배적인 위치를 차지하고 있다. 영어가 우세한 말뭉치에 대해 훈련된 LLM은 다른 비영어 언어에 비해 열등한 성능을 보인다. LLM을 비영어 언어로 외삽하는 것은 매우 가치 있는 연구 과제를 제기한다. 일반적인 외삽 접근법은 다음 세 단계로 구성된다 : (1) 대상 언어의 토큰을 추가하기 위해 어휘를 확장하고, 따라서 해당 언어에 대한 인코딩 표현성을 강화한다. (2) LLM들의 언어 생성 능력들을 타겟 언어로 전이하도록 추가로 사전 트레이닝하는 단계를 더 포함하는, 방법. 이 단계에 대한 요구되는 트레이닝 스케일은 일반적으로 수십억 개의 토큰들, 즉 처음부터 트레이닝에 필요한 수조 개의 토큰들보다 상당히 적다. (3) LLM들의 명령어-추종 능력들을 이전하기 위해 타겟 언어로 SFT를 수행하는 단계를 포함하는, 방법.\n' +
      '\n' +
      '본 논문은 어휘 확장 전후의 LLM의 성능 차이를 비교하고 다양한 사전 훈련과 SFT 척도에서 앞서 언급한 세 단계에 대한 포괄적인 실증적 연구를 수행한다. 효과적인 전이를 위해 어휘 확장의 필요성과 필요한 훈련 척도를 분석한다.\n' +
      '\n' +
      '## Experimental Setup\n' +
      '\n' +
      '본 논문은 언어 생성 및 후속 수업의 역량을 비영어 언어로 효과적으로 전이시키는 방법을 탐색하는 것을 목적으로 한다. 중국어로 이용 가능한 풍부한 언어 자원을 감안할 때 포괄적이고 심층적인 실증 연구를 수행할 수 있다. 따라서 우리의 실험과 분석은 중국어를 출발점으로 시작하며 관찰된 현상은 10개 이상의 저자원 언어에 걸쳐 추가로 검증된다. 이 섹션에서는 실험에 사용된 데이터 세트, 모델 및 평가 방법론을 제시한다.\n' +
      '\n' +
      '### Models\n' +
      '\n' +
      '불필요한 대규모 반복 사전 훈련을 피하기 위해 다양한 규모의 중국 말뭉치에서 훈련된 오픈 소스 모델을 사용했다. 이 중 LLaMA와 LLaMA2는 명시적인 중국어 사전훈련을 거치지 않고 검문소 역할을 하는 반면, 중국어 LLaMA와 중국어 LLaMA2는 중국어 사전훈련이 300억 토큰인 검문소로 취급된다. 오픈차이나 LLaMA의 규모는 1,000억 토큰에 달한다. 우리는 이러한 모델의 성능을 분석 및 비교를 위한 참조로 사용한다.\n' +
      '\n' +
      '**LLaMA**[12]: LLaMA는 Meta AI에 의해 개발된 일련의 기초 모델이며 공개적으로 사용 가능한 영어 지배 코퍼스에 대해 훈련되었습니다. 코퍼스는 CommonCrawl, C4, Github 코드, 위키피디아, 북스 및 ArXiv 문서를 포함하며 약 1조 4천억 토큰에 달한다. 이러한 출처 중 위키피디아는 다국어 텍스트로 구성되어 전체 말뭉치의 4.5%를 차지한다. 라틴어 또는 키릴어 스크립트를 사용하는 20개 언어를 포함합니다. LLaMA는 그 크기의 기초 모델에 대한 최첨단 결과를 달성한다. 예를 들어, 130억 개의 매개변수만 있는 LLaMA-13B는 많은 NLP 벤치마크에서 훨씬 더 큰 175B 매개변수 GPT-3보다 우수하다. 우리는 실험에서 LLaMA-7B와 LLaMA-13B를 고려한다.\n' +
      '\n' +
      '**LLaMA2**[12]: LLaMA2는 LLaMA의 향상된 업그레이드 버전입니다. 이전과 비교하여 받은 업그레이드에는 보다 강력한 데이터 청소 프로세스, 40%의 크기 증가를 자랑하는 공개적으로 사용 가능한 사전 훈련 데이터의 새로운 혼합, 이해력 향상을 위한 두 배의 컨텍스트 길이, 추론 효율성을 위한 그룹화된 쿼리 주의 구현 등이 포함된다. 이러한 개선은 고급 언어 이해 작업을 처리하는 데 더 강력한 도구가 됩니다. 우리는 실험에서 LLaMA2-7B를 고려한다.\n' +
      '\n' +
      '**중국어 LLaMA**[12]: 중국어 LLaMA는 중국어 텍스트를 이해하고 생성하는 기능을 향상시키기 위해 설계된 원본 LLaMA의 확장입니다. SentencePiece를 사용하여 개발된 중국식 토큰화기를 통합하여 목표를 달성한다. 어휘 크기가 \\(49,953\\)인 이 토큰화기는 한자 처리를 개선할 수 있습니다. 또한, 모델 트레이닝 동안 메모리 소모를 줄이기 위해 파라미터 효율적인 미세 조정 기법[13]을 채용한다. 우리의 실험에서 우리는 약 300억 개의 중국 토큰에 해당하는 약 120GB 크기의 코퍼스에 대해 훈련된 중국 LLaMA 7B Plus를 고려한다.\n' +
      '\n' +
      '**중국 LLaMA2**[12]: 중국 LLaMA2는 중국 LLaMA의 고급 반복입니다. 이는 중국어 LLaMA와 동일한 말뭉치와 학습 데이터를 활용하되, LLaMA2의 기초 모델을 활용하며, 새로운 버전의 어휘 구성과 코드 구현도 최적화하였다. 우리의 실험에서 우리는 300억 개의 중국 토큰에 사전 훈련된 중국 LLaMA2 7B를 고려한다.\n' +
      '\n' +
      '**Open Chinese LLaMA**[10]: Open Chinese LLaMA는 원본 LLaMA의 대규모 확장 버전입니다. LLaMA의 중국어 텍스트 처리 능력을 향상시키기 위해 오픈 차이나 LLaMA는 1,000억 개의 토큰으로 구성된 코퍼스에 대한 추가 사전 교육을 거친다. 코퍼스는 원래 LLAMA 모델이 사용하는 영어 및 코드 데이터의 하위 집합과 함께 인터넷에서 수집되고 청소를 받는 텍스트로 구성된다.\n' +
      '\n' +
      '### Datasets\n' +
      '\n' +
      'LLaMA의 언어 능력을 비영어 관심 언어로 전달하기 위해, 우리는 훈련을 위해 BELLE와 Bactrain-X의 두 가지 명령어 데이터 세트를 활용한다. 전자는 중국어와 관련된 실험에 사용되고 후자는 다른 언어와 관련된 실험에 사용된다.\n' +
      '\n' +
      '**BELLE**[11]: BELLE는 150만 명령 후속 예제를 포함하는 Lianjia Tech에서 개발한 대규모 중국 명령 튜닝 데이터 세트입니다. 우리는 중복되고 품질이 낮은 데이터를 제거하고 마침내 95만 개의 예를 유지했다.\n' +
      '\n' +
      '**Bactrain-X**[12]: Bactrian-X는 다국어 명령어 튜닝을 용이하게 하기 위해 52개 언어에 걸친 명령어와 응답을 포함한다. Alpaca-52k [10] 및 Dolly-15k [12] 데이터 세트에서 67K 영어 지침을 51개 언어로 번역한 다음 ChatGPT로 응답을 생성하는 방식으로 생성됩니다. 모델의 역량을 객관적이고 종합적으로 평가하기 위해 응답 품질과 지식 수준의 두 가지 관점에서 평가를 수행한다. 전자의 경우 LLM-Eval 벤치마크를 사용하고 이를 다양한 저자원 언어로 번역하여 다국어 평가를 지원한다. 후자는 C-Eval, MMLU, AGI-Eval 및 GAOKAO-Bench의 4가지 널리 채택된 표준화된 테스트 벤치마크를 활용한다.\n' +
      '\n' +
      '**LLM-Eval**[13]: LLM-Eval은 명령어 후속 평가를 위해 수동으로 구성된 벤치마크입니다. 사실적 질문 답변, 독해, 프레임 생성, 단락 다시 쓰기, 요약, 수학 문제 해결, 추론, 시 생성, 프로그래밍 등 17개 대분류 453개의 수업 과제를 가지고 있다.\n' +
      '\n' +
      '**C-Eval**[13]: C-Eval은 52개 과목에 걸쳐 13948개의 시험 문제와 중학교부터 전문 시험까지 4개의 난이도를 가진 중국어 평가 제품군입니다. 여기에는 STEM, 인문학, 사회 과학 및 기타 주제가 포함됩니다. C-Eval HARD는 고급 추론이 필요한 8개의 도전적인 수학과 과학 과목의 하위 집합이다.\n' +
      '\n' +
      '**MMLU**(Hendrycks et al.2020): MMLU는 STEM, 인문 및 사회 과학을 포함한 57개의 다양한 주제에 걸쳐 지식을 배우고 적용하는 LLM의 능력을 측정합니다. 이 시험은 초등부터 고급 전문가까지 다양한 난이도를 다루고 있다.\n' +
      '\n' +
      '**AGI-Eval**(Zhong et al.2023): AGIEval은 대학 입시, 로스쿨 입학 시험 및 전문 자격 시험을 포함하여 수백만 명이 수집한 표준화된 시험의 질문을 사용합니다. 그것은 영어와 중국어 모두 19개의 과제를 가지고 있다.\n' +
      '\n' +
      '**Gaokao-Bench**(Zhang et al.2023b): GAOKAO-Bench는 2010-2022년 중국 대학 입시(Gaokao)의 2811개의 시험 문제를 모든 과목을 대상으로 사용합니다. 1781개의 객관식, 218개의 빈칸 채우기, 수학, 중국어, 영어, 물리 등 전반에 걸쳐 812개의 개방형 질문이 있습니다.\n' +
      '\n' +
      '### Evaluation Protocol\n' +
      '\n' +
      'LLM-Eval의 경우 Zhang et al.2023a의 실습을 따라 정확성, 유창성, 정보성, 논리성, 무해성의 5가지 채점 항목을 통해 모델의 응답 품질을 평가했다. 각 측면에 대한 점수는 0에서 3 사이이다. 자동 평가를 위해 지침, 모델 응답 및 참조 답변을 GPT-4에 제출하기 위해 부록에 표시된 프롬프트를 사용한다. Zhang et al.2023a에 의해 보고된 결과에 기초하여, 이 평가 방법은 인간 평가와의 높은 일관성을 입증한다.\n' +
      '\n' +
      '4개의 표준화된 테스트 벤치마크의 경우 모델 반응에 대한 정확도 메트릭을 계산한다. 또한 AGI-Eval 및 GAOKAO-Bench에 대해 제로 샷 설정을 사용하고 C-Eval 및 MMLU에 대해 5 샷 설정을 사용하는 일반적인 관행을 따른다.\n' +
      '\n' +
      '## Main Results\n' +
      '\n' +
      '### 어휘 확장이 전송에 미치는 영향\n' +
      '\n' +
      '특정 언어에서 LLM의 능력을 향상시키는 것을 목표로 할 때 어휘 확장은 직관적으로 합리적인 접근법이다. 이 절에서는 LLM-Eval 벤치마크를 통해 어휘 확장의 영향을 평가하고 실험 결과를 표 1에 제시하였다. 처음에는 인터넷에서 100만 개의 중국어 문장(약 0.5억 토큰)을 수집하고 어휘 확장 없이 원본 LLaMA를 추가로 사전 훈련했다. 놀랍게도, 우리는 이 모델이 1K, 5K 및 950K 명령어 튜닝의 설정에서 어휘 확장 중국어 LLaMA를 상당히 능가한다는 것을 발견했다. 이 발견은 중국 LLaMA가 우리의 0.5억 토큰보다 훨씬 더 큰 300억 토큰에 대한 추가 중국 사전 교육을 받았다는 점을 감안할 때 생각과 거리가 있다. 또한 950K 설정 내에서 원본 LLaMA에 대한 어휘를 확장하고 동일한 0.5억 토큰으로 훈련하여 훈련 데이터 불일치의 영향을 완화한 결과를 포함한다. 그 결과는 여전히 일관적이다. 이것은 어휘 확장이 수백억 토큰의 훈련 규모 내에서 유리한 선택이 아님을 나타낸다. 다른 문헌 [23]에서 보고된 바와 같이, 더 큰 규모의 사전 훈련(예: 수조 개의 토큰)을 포함하는 설정에서 어휘 확장의 효능을 부정하지는 않지만, 이는 이미 단순한 언어 전달보다 재훈련에 더 기울어져 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c} \\hline \\hline  & **Method** & ACC. & F. & INFO. & LC. & H. & AVG. \\\\ \\hline \\multirow{8}{*}{1k SFT} & LLaMA [21] & 0.482 & 1.194 & 0.858 & 0.614 & 2.970 & 1.224 \\\\  & LLaMA with \\(10K\\) pretrain & 0.482 & 1.441 & 0.829 & 0.712 & 2.963 & 1.285 \\\\  & LLaMA with \\(100K\\) pretrain & 0.587 & 1.952 & 0.881 & 0.991 & 2.973 & 1.477 \\\\  & LLaMA with \\(1M\\) pretrain & 0.735 & 2.071 & 1.002 & 1.046 & 2.957 & 1.562 \\\\  & Chinese LLaMA [21] & 0.509 & 1.205 & 0.811 & 0.726 & 2.970 & 1.244 \\\\  & Open Chinese LLaMA [21] & 1.406 & 2.584 & 1.685 & 1.877 & 2.989 & 2.108 \\\\ \\hline \\hline \\multirow{8}{*}{5k SFT} & LLaMA [21] & 0.450 & 1.279 & 0.767 & 0.612 & 3.000 & 1.199 \\\\  & LLaMA with \\(10K\\) pretrain & 0.411 & 1.372 & 0.814 & 0.612 & 2.961 & 1.258 \\\\  & LLaMA with \\(100K\\) pretrain & 0.488 & 1.922 & 0.876 & 0.977 & 3.000 & 1.493 \\\\  & LLaMA with \\(1M\\) pretrain & 0.682 & 2.085 & 1.039 & 1.008 & 2.969 & 1.623 \\\\  & Chinese LLaMA [21] & 0.581 & 1.341 & 0.899 & 0.783 & 2.992 & 1.432 \\\\  & Open Chinese LLaMA [21] & 1.295 & 2.481 & 1.667 & 1.884 & 2.969 & 2.245 \\\\ \\hline \\hline \\multirow{8}{*}{950k SFT} & LLaMA [21] & 1.783 & 2.767 & 2.142 & 2.212 & 2.993 & 2.379 \\\\  & LLaMA with \\(1M\\) pretrain & 1.812 & 2.799 & 2.080 & 2.303 & 3.000 & 2.399 \\\\  & LLaMA-EXT with \\(1M\\) pretrain & 1.591 & 2.726 & 1.918 & 2.164 & 2.998 & 2.279 \\\\ \\cline{1-1}  & Chinese LLaMA [21] & 1.808 & 2.795 & 2.112 & 2.313 & 3.000 & 2.406 \\\\ \\cline{1-1}  & Open Chinese LLaMA [21] & 1.890 & 2.858 & 2.189 & 2.390 & 2.993 & 2.464 \\\\ \\cline{1-1}  & LLaMA2 [21] & 1.868 & 2.822 & 2.171 & 2.379 & 3.000 & 2.448 \\\\ \\cline{1-1}  & Chinese LLaMA2 [21] & 1.701 & 2.838 & 2.011 & 2.251 & 3.000 & 2.360 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 추가 사전 훈련 및 명령어 튜닝(SFT)의 척도가 다른 응답 품질. ACC., F., LC., H., INFO., AVG. 각각 정확성, 유창성, 논리적 일관성, 무해성, 정보성 및 평균을 나타낸다. 약 100만 개의 샘플이 약 0.5억 개의 토큰을 차지한다. 중국 LLaMA와 오픈 중국 LLaMA의 사전 훈련 규모는 각각 300억 토큰과 1000억 토큰이다.\n' +
      '\n' +
      '### 효과적인 전송을 위해 필요한 훈련 크기\n' +
      '\n' +
      '훈련 척도는 사전 훈련 척도와 수업 조정 척도로 구성된 LLM 역량의 전이 가능성에 영향을 미치는 또 다른 중요한 요인을 구성한다. 실험 결과는 표 1에 나타나 있다. LLaMA(10K, 100K 및 1M 추가 프리트레이닝 포함) 및 오픈 중국 LLaMA의 예를 들면, 추가 중국 프리트레이닝의 스케일은 0에서 1000억 토큰으로 점진적으로 증가한다. 1K 및 5K 명령어 튜닝 설정에서 추가 사전 훈련의 규모가 증가함에 따라 응답 품질이 점진적으로 향상되는 것을 관찰했다. 그러나 명령어 튜닝 데이터 척도가 950K로 증가하면 모델 간에 응답 품질에 큰 차이가 없음을 알 수 있다. 결과적으로, 우리는 더 많은 사전 훈련이 모델의 인간 명령과의 정렬을 가속화할 수 있다고 가정하지만, 수백억의 훈련 규모만으로는 모델이 더 많은 양의 세계 지식을 파악할 수 없다. 이것은 유사한 반응 수준에서 수렴으로 이어진다. 즉, 응답 품질의 향상은 주로 지식 수준의 향상보다는 언어 생성 능력의 향상에서 비롯된다.\n' +
      '\n' +
      '그러나 각주 1: 중국어-LLaMA는 어휘 확장이라는 추가 요인으로 인해 예외로 서 있다.\n' +
      '\n' +
      '이러한 관점을 검증하기 위해 널리 사용되는 4개의 표준화된 테스트 벤치마크에 대한 모델의 지식 수준을 평가했다. 그림 2에서 볼 수 있듯이 LLaMA 7B, Chinese LLaMA 7B 및 Open Chinese LLaMA 7B는 C-val, goakao-bench 및 agi-eval에서 비교적 잘 수행되어 추가 중국 사전 훈련에 의해 유발된 유의한 차이가 없음을 나타낸다. 중국어에서 추가 사전 훈련이 부족함에도 불구하고 LLaMA2-7B와 LLaMA-13B 모두 C-Eval, MMLU 및 AGI-Eval에서 개방형 중국 LLaMA를 능가하여 조 수준 사전 훈련과 더 큰 모델 크기가 실제로 모델 지식 수준을 향상시키는 효과적인 경로 역할을 할 수 있음을 시사한다는 점에 주목할 필요가 있다.\n' +
      '\n' +
      '### 원본 영어 기능에 대한 방법\n' +
      '\n' +
      '우리에게 관심의 또 다른 쟁점은 중국어 능력 향상이 기존의 영어 능력에 영향을 미치는지 여부이다. 이 문제를 해결하기 위해 인터넷에서 200,000개의 중국어 샘플을 추가로 수집하고 정제된 웹 데이터 세트[20]에서 20,000개의 영어 샘플을 무작위로 추출했다. 이 샘플들을 이용하여 표 2와 같이 서로 다른 규모의 코퍼라로 훈련된 LLaMA 모델의 영어 복잡도와 중국어 복잡도를 평가한다. 연구 결과, 사전 훈련 척도가 증가함에 따라 모델의 복잡도는 중국어에서는 꾸준히 감소하지만 영어에서는 현저하게 증가하는 것으로 나타났다. 이는 하나의 중국어 말뭉치를 통해서만 모델의 역량을 높이는 것이 원래 영어 실력을 희생하는 대가를 치르게 된다는 것을 시사한다.\n' +
      '\n' +
      '또한 개방형 중국어 LLaMA에 대해 당혹도 평가를 수행하고 중국어와 영어 당혹도가 여전히 낮다는 것을 발견했다. 이 결과는 교육 데이터가 중국어와 영어 콘텐츠를 모두 통합하여 영어 복잡도의 현저한 상승 없이 중국어 복잡도의 감소를 허용한다는 점을 감안할 때 놀라운 일이 아니다. 전반적으로, 전이 훈련을 위한 중국 말뭉치에 대한 배타적 의존은 LLaMA의 원래 영어 능력을 현저하게 손상시키며, 이는 다국어 공동 훈련을 통해 효과적으로 완화된다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c} \\hline \\hline  & L(0) & L(10k) & L(100k) & L(1M) & Open \\\\ \\hline\n' +
      '**중국** & 10.151 & 8.697 & 6.634 & 5.249 & 3.924 \\\\\n' +
      '**English** & 14.691 & 15.625 & 29.553 & 198.840 & 15.045 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 다른 추가 사전 훈련 척도를 가진 모델 복잡성. L은 LLaMA를 나타내며 괄호 안의 숫자는 추가 사전 훈련 샘플의 양을 나타낸다. Open은 Open Chinese LLaMA를 나타낸다.\n' +
      '\n' +
      '그림 2: 4개의 벤치마크에 대한 지식 수준 평가 결과.\n' +
      '\n' +
      '### 분석을 다중 언어로 확장\n' +
      '\n' +
      '앞 절에서, 우리의 실험은 중국어에 초점을 맞춘다. 다른 비영어 언어에서도 유사한 결론이 도출될 수 있는지 조사하기 위해 실험을 13개의 저자원 언어로 확장한다. 평가 일관성을 보장하기 위해 LLM-Eval 벤치마크를 이 13개 언어로 번역하고 동일한 평가 메트릭을 사용한다. 표 3과 같이 SFT 데이터의 증가와 함께 모든 저자원 언어에 대한 응답 품질이 크게 향상되었다. 이들 언어 중 아랍어, 인도네시아어, 베트남어가 가장 좋은 성능을 보였다. 13개 언어 모두 자원이 부족함에도 불구하고, 이 3개 언어는 더 자주 사용된다[23]. 결과적으로 LLaMA는 (영어에 비해 전반적인 발생이 적지만) 그들을 더 자주 만나 모델을 통해 이러한 언어로 된 지침을 빠르게 이해할 수 있다. 이는 이전 섹션에서 도출한 결론과 일치한다.\n' +
      '\n' +
      '앞 절에서는 어휘를 확장하는 것이 언어 전이성에 부정적인 영향을 미친다는 것을 관찰하였다. 그럴듯한 가설은 어휘 확장이 방해할 수 있는 LLM 내의 교차 언어 의미 정렬의 존재이다. 이 정렬 가설을 검증하기 위해 1k 명령어의 데이터 세트로 LLaMA를 미세 조정하고 모델의 출력을 조사한다. 흥미롭게, 우리는 코드 전환 샘플의 특정 비율을 관찰했다. 그림 3에서 볼 수 있듯이 이러한 샘플의 모델 응답은 여러 언어의 토큰으로 구성되며 의미적으로 일관성이 있다. 우리는 중국어가 타겟 언어인 경우 전송 과정뿐만 아니라 다른 13개의 저자원 언어들이 타겟 언어인 경우에도 코드 전환이 발생함을 관찰하였다. 그림 4에서 볼 수 있듯이 코드 전환이 있는 샘플의 비율은 대략 \\(2\\%\\)에서 \\(5\\%\\) 사이이다. 이는 LLaMA가 사전 훈련 과정에서 개념 간의 언어 간 정렬 관계를 학습했을 수 있음을 나타낸다.\n' +
      '\n' +
      '그림 3: 코드 전환 사례 연구. 빨간색 배경을 가진 텍스트는 비영어 타겟 언어(중국어)를 나타낸다. 청록색 배경이 있는 텍스트는 모델 출력에서 코드 전환 언어를 나타내며, 이는 영어, 일본어, 러시아어 또는 기타 언어일 수 있습니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Language**} & \\multicolumn{8}{c}{1k SFT} & \\multicolumn{8}{c}{65k SFT} \\\\ \\cline{2-13}  & ACC. & F. & INFO. & LC. & H. & AVG. & ACC. & F. & INFO. & LC. & H. & AVG. \\\\ \\hline Arbic & 0.188 & 1.061 & 0.191 & 0.254 & 3.000 & 0.939 & 1.268 & 2.499 & 1.529 & 1.607 & 3.000 & 1.981 \\\\ Bengali & 0.046 & 0.492 & 0.050 & 0.041 & 3.000 & 0.726 & 0.959 & 2.257 & 1.156 & 1.189 & 3.000 & 1.712 \\\\ Gujarati & 0.061 & 0.426 & 0.052 & 0.063 & 2.998 & 0.720 & 0.683 & 1.795 & 0.875 & 0.790 & 2.995 & 1.428 \\\\ Hindi & 0.131 & 1.064 & 0.147 & 0.162 & 3.000 & 0.901 & 1.014 & 2.342 & 1.238 & 1.240 & 2.998 & 1.766 \\\\ Indonesian & 0.398 & 1.266 & 0.544 & 0.438 & 2.995 & 1.128 & 1.659 & 2.751 & 0.226 & 2.012 & 3.000 & 2.290 \\\\ Malayalam & 0.101 & 0.621 & 0.103 & 0.103 & 3.000 & 0.786 & 0.906 & 2.427 & 1.182 & 1.197 & 3.000 & 1.742 \\\\ Marathi & 0.095 & 0.781 & 0.107 & 0.117 & 2.998 & 0.820 & 1.038 & 2.476 & 1.288 & 1.364 & 2.998 & 1.833 \\\\ Nepali & 0.151 & 0.991 & 0.177 & 0.146 & 2.986 & 0.890 & 0.969 & 2.417 & 1.236 & 1.285 & 3.000 & 1.781 \\\\ Swahili & 0.083 & 0.712 & 0.090 & 0.086 & 2.998 & 0.794 & 1.569 & 2.707 & 1.955 & 1.907 & 3.000 & 2.228 \\\\ Tamil & 0.140 & 0.914 & 0.176 & 0.174 & 2.998 & 0.880 & 0.960 & 2.457 & 1.198 & 1.257 & 2.998 & 1.774 \\\\ Telugu & 0.054 & 0.560 & 0.057 & 0.090 & 3.000 & 0.752 & 0.539 & 1.735 & 0.674 & 0.712 & 3.000 & 1.332 \\\\ Urdu & 0.057 & 0.573 & 0.052 & 0.071 & 3.000 & 0.751 & 1.038 & 2.443 & 1.285 & 1.335 & 3.000 & 1.820 \\\\ Vietnamese & 0.105 & 0.623 & 0.126 & 0.117 & 3.000 & 0.794 & 1.361 & 2.595 & 1.665 & 1.710 & 3.000 & 2.066 \\\\ \\hline \\hline Average & 0.124 & 0.776 & 0.144 & 0.143 & 2.998 & 0.837 & 1.074 & 2.377 & 1.331 & 1.354 & 2.999 & 1.827 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: LLM-Eval 상의 13개 저자원 언어에 대한 모델 응답 품질 평가 결과. ACC., F., LC., H., INFO., AVG. 각각 정확성, 유창성, 논리적 일관성, 무해성, 정보성 및 평균을 나타낸다.\n' +
      '\n' +
      '## Related Work\n' +
      '\n' +
      '### LLMs에서 리소스 간격\n' +
      '\n' +
      'LLM의 주요 과제 중 하나는 주로 영어 코퍼스에서 사전 훈련되고 다른 언어의 데이터에 대한 접근이 제한적이기 때문에 자원 격차이다. 영어는 다양한 도메인에서 가장 많은 원시 텍스트 데이터를 가진 극도로 높은 자원 언어로서 NLP 분야를 지배하고 있으며, 조시 등(2020) 분야에서 대표되는 세계 7000개 이상의 언어 중 거의 남아 있지 않다. 이것은 언어 모델들이 서로 다른 언어를 다룰 수 있는 능력의 차이를 만든다. 이전 연구 결과에 따르면 LLMs은 특히 저자원 언어인 Nguyen et al.(2023), Zhu et al.(2023), Huang et al.(2023)에서 비영어 텍스트를 이해하고 생성하는 데 어려움을 겪고 있다. 자원 격차를 해결하기 위해 연구자와 실무자에 의해 몇 가지 솔루션이 제안되거나 구현되었다. 하나의 가능한 해결책은 다양한 언어 및 필드로부터 이용가능한 데이터의 양을 증가시키고, LLMs Lin 등(2022); Chen 등(2022); Cahyawijaya 등(2023)을 사전 훈련 및 평가하기 위해 액세스 가능하게 하는 것이다. 그러나, 이 접근법은 상당한 계산 비용을 초래하고 자원 격차는 지속된다. 대안적으로, mBERT Devlin 등(2019) 및 XLM-R Conneau 등(2020)과 같은 상이한 언어로부터의 텍스트들에 동시에 트레이닝된 다국어 언어 모델들이 갭을 효과적으로 해소하도록 도입되었다.\n' +
      '\n' +
      '### Cross-Lingual Transfer\n' +
      '\n' +
      '다국어 언어 모델은 광범위한 작업 Wu 및 Dredze(2019); Pires 등(2019); Winata 등(2021)에 걸쳐 높은 수준의 제로 샷 또는 소수 샷 교차 언어 전달 가능성을 입증했다. 이는 한 언어의 지도 데이터에서 언어 능력을 습득하고 추가 훈련 데이터 없이 또는 거의 없이 다른 언어에 적용할 수 있음을 의미한다. 강한 교차 언어 성능의 이면에 있는 메커니즘은 연구자들에 의해 조사되었다. 다국어 언어 모델은 임의의 언어 Artetxe 등(2020), Chi 등(2020), Conneau 등(2020)에 적용 가능한 보편적인 규칙을 추론한 것으로 나타났다. MBERT Devlin et al.(2019)과 같은 다국어 다국어 모델은 여러 언어에 걸쳐 공유된 하위 단어 어휘와 공동 사전 훈련에 의존한다는 일반적인 가설과 달리 Pires et al.(2019); Cao et al.(2020); Wu and Dredze(2019)는 연구자들이 모델에 대한 새로운 이해도를 개발했으며, 모델의 보편적 의미 추상화 학습 능력을 강조했다. 언어 간 성능에 영향을 미치는 요인의 측면에서 연구자들은 매개변수 공유 Conneau 등(2020), Dufter and Schutze(2020), Wu 등(2022) 및 언어 거리 Conneau 등(2020), Eronen 등(2023)과 전이성을 연관시켰다. 우리는 여기에서 새로운 LLaMA 기반 실험을 통해 언어 모델의 언어 간 전달 가능성을 추가로 조사하여 다른 측면에서 결과를 제시한다.\n' +
      '\n' +
      '### Code-Switching\n' +
      '\n' +
      '코드 전환은 단일 발화 내에서 다국어 화자가 언어 간 전환을 하는 현상이다. 코드 전환 태스크에 대한 다국어 언어 모델의 성능에 대한 이전 연구는 혼합된 결과를 보여주었다. 일부 연구에서는 특정 코드 전환 시나리오에 대해 미세 조정된 사전 훈련된 모델이 영어-스페인어 및 영어-힌디 Khanuja 등(2020)과 같은 특정 언어 쌍에 대해 최첨단 성능을 달성할 수 있다고 제안한 반면, 다른 연구에서는 메타 임베딩을 사용하는 것이 더 적은 매개변수 Winata 등(2019); Winata 등(2019, 2021)으로 더 나은 결과를 산출할 수 있음을 발견했다. 또 다른 연구 라인에서, 코드-스위칭-기반 방법들은 다국어 언어 모델인 Jiang et al.(2020), Tan and Joty(2021), Krishnan et al.(2021)의 능력을 향상시키기 위해 제시되었다.\n' +
      '\n' +
      '## Conclusions\n' +
      '\n' +
      '본 논문에서는 언어 생성 및 명령어 수행 능력을 비영어 언어로 효과적으로 전달하는 방법에 초점을 맞춘다. 구체적으로 효과적인 전이를 위해 어휘 확장의 필요성과 필요한 훈련 척도를 분석하기 위한 포괄적인 실증 연구를 수행한다. 사전 학습 데이터의 \\(1\\%\\) 이하로 어휘 확장이 필요하고 최신 모델과 유사한 전달 성능을 얻을 수 있음을 알 수 있었다. 또한 전달 훈련 중 코드 전환 사례를 관찰하여 언어 간 정렬이 모델 내에서 내재화되었을 수 있음을 시사한다. 13개의 저자원 언어에 대한 확장 실험에서도 유사한 결과가 관찰된다. 우리의 분석 및 결과는 비영어 LLM 개발에 있어 커뮤니티에 대한 도움과 지침을 제공한다.\n' +
      '\n' +
      '그림 4: 언어 간 코드 전환 속도입니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:8]\n' +
      '\n' +
      'Krishnan, J.; Anastasopoulos, A.; Purohit, H.; and Rangwala, H. 2021. Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent Prediction and Slot Filling. arXiv:2103.07792.\n' +
      '* Li et al. (2023) Li, H.; Koto, F.; Wu, M.; Aji, A. F.; and Baldwin, T. 2023. Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation. arXiv:2305.15011.\n' +
      '* Lin et al.(2022) Lin, X. V.; Mihaylov, T.; Artetxe, M. 왕태환 천신 Simig, D.; Ott, M.; 고얄, N. Bhosale, S. Du, J.; Pasunuru, R.; Shleifer, S. Koura, P. S.; Chaudhary, V.; 오호로, B.; 왕, J.; 제틀모이어, L. Kozareva, Z. Diab, M. Stoyanov, V. 및 Li, X 2022. Few-shot Learning with Multilingual Language Models. arXiv:2112.10668.\n' +
      '* Nguyen et al.(2023) Nguyen, X. -P; Aljunied, S. M.; Joty, S.; Bing, L. 2023. LLMs for Low-Resource Languages 민주화는 언어학적으로 다양한 Prompts를 사용하여 영어 Dominant Abilities를 활용한다. arXiv:2306.11372.\n' +
      '* OpenAI (2022) OpenAI. 2022. Introducing ChatGPT.\n' +
      '* OpenLMLab (2023) OpenLMLab. 2023년 오픈차이나-라마\n' +
      '* Penedo et al. (2023) Penedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Cappelli, A.; Alobeidli, H.; Pannier, B.; Almazrouei, E.; and Launay, J. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. arXiv:2306.01116.\n' +
      '* Pires et al.(2019) Pires, T.; 슐링거, E.; and Garrette, D. 2019. How Multilingual is Multilingual BERT? "Proceedings of the 57th Annual Meeting of the Association of Computational Linguistics"에서, 4996-5001. Florence, Italy: Association for Computational Linguistics.\n' +
      '* Ranta and Goutte (2021) Ranta, A.; and Goutte, C. 2021. Linguistic Diversity in Natural Language Processing. _ Traitement Automatique des Langues_, 62(3): 7-11.\n' +
      '* Scao et al. (2023) Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilic, S.; Hesslow, D.; and Castagne, R. 2023. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. arXiv:2211.05100.\n' +
      '* StabilityAI (2023) StabilityAI. 2023. 무논싱 스테이블코드.\n' +
      '* Tan and Joty (2021) Tan, S.; 및 조티, S. 2021. Code-Mixing on Sesame Street: Dawn of the Adversarial Polydots. arXiv:2103.09593.\n' +
      '* Taori et al. (2023) Taori, R. 굴라자니, I.; 장, T.; Dubois, Y. Li, X; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Alpaca: A Strong, Replicable Instruction-Following Model.\n' +
      '* 팀(2023a) 팀, I. 2023a. 인턴m: 점차 향상된 기능을 갖춘 다국어 언어 모델.\n' +
      '* 팀(2023b) 팀, I. 2023b. InternLM: 점진적으로 향상된 기능을 가진 다국어 언어 모델 [https://github.com/InternLM/InternLM-techreport] (https://github.com/InternLM/InternLM-techreport).\n' +
      '* Touvron et al. (2023a) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; 라쇼 -A; Lacroix, T. Roziere, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lample, G. 2023a. LLaMA: 개방적이고 효율적인 기초 언어 모델. arXiv:2302.13971.\n' +
      '* Touvron et al. (2023b) Touvron, H.; Martin, L.; 스톤, K Albert, P.; and Almahairi, A. 2023b. 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델입니다. arXiv:2307.09288.\n' +
      '* Winata et al.(2021a) Winata, G. I.; Cahyawijaya, S.; 류준영 Lin, Z.; Madotto, A.; and Fung, P. 2021a. 코드 전환에서 다국어 모델이 효과적인가? arXiv:2103.13309.\n' +
      '* Winata, Lin, and Fung (2019) Winata, G. I.; Lin, Z.; and Fung, P. 2019. Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition. In _Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)_, 181-186. Florence, Italy: Association for Computational Linguistics.\n' +
      '* Winata et al.(2019) Winata, G. I.; Lin, Z.; 신주영 and Fung, P. 2019. Hierarchical Meta-Embeddings for Code-Switching Named Entity Recognition. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 3541-3547. Hong Kong, China: Association for Computational Linguistics.\n' +
      '* Winata et al.(2021b) Winata, G. I.; Madotto, A.; Lin, Z.; 류현정 요신스키, J.; 및 펑, P. 2021b. 언어 모델은 소수의 다국어 학습자입니다. "Proceedings of the first Workshop on Multilingual Representation Learning"에서, 1-15. Punta Cana, Dominican Republic: Association for Computational Linguistics.\n' +
      '* Wu and Dredze (2019) Wu, S.; 및 Dredze, M. 2019. Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 833-844. Hong Kong, China: Association for Computational Linguistics.\n' +
      '* Wu, Papadimitriou, and Tamkin (2022) Wu, Z.; Papadimitriou, I.; and Tamkin, A. 2022. OOLOong: Investigating What Makes Crosslingual Transfer Hard with Controlled Studies. arXiv:2202.12312.\n' +
      '* Zhang et al.(2023a) Zhang, M.; Zhang, Q.; 장영 Gui, T. 2023a. LLIMEVAL-1 중국어 대형 언어 모델 평가 1단계.\n' +
      '* Zhang et al.(2023b) Zhang, X; (주)리종영 Ying, Z.; He, L. 그리고 Qiu, X. 2023b. GAOKAO 벤치마크에서 대용량 언어 모델의 성능 평가 arXiv:2305.12474.\n' +
      '* Zhong et al. (2023) Zhong, W.; Cui, R. 곽영 양영 Lu, S. 왕영 Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. arXiv:2304.06364.\n' +
      '* Zhu et al. (2023) Zhu, W.; 류현동 Xu, J.; Huang, S.; 공락 Chen, J.; and Li, L. 2023. Large Language Models을 이용한 다국어 기계번역: 경험적 결과 및 분석. arXiv:2304.04675.\n' +
      '\n' +
      '## Appendix\n' +
      '\n' +
      '**LLM-Eval에 대한 프롬프트**\n' +
      '\n' +
      '특정 교과와 관련된 질문의 경우 0/1/2/3의 척도를 사용하여 다음 LLM 산출물의 점수를 매긴다.\n' +
      '\n' +
      '정확성: 정답이 정확하고 정확한지 정확한 정보로 평가합니다. 질 높은 답은 사실적으로 신뢰할 수 있어야 한다.\n' +
      '\n' +
      '유창성: 일관된 표현과 명확한 표현으로 해답이 인간의 언어 관습을 얼마나 잘 준수하는지 평가하라. 질 높은 답은 복잡한 문장이나 구문이 어려운 문장 없이 이해하기 쉬워야 한다.\n' +
      '\n' +
      '정보성: 답이 실용적인 가치의 충분한 관련 정보를 제공하는지를 판단한다. 질 높은 답변은 질문자에게 유용하고 적절한 정보를 제공해야 한다.\n' +
      '\n' +
      '논리성: 논점과 논증을 제시함에 있어 답이 논리적으로 타당하고 정확한지를 검토한다. 질 높은 답은 논리적인 원리를 따라야 하며, 명확한 추론과 일관성을 입증해야 한다.\n' +
      '\n' +
      '무해성: 대답이 윤리적 규범을 준수하면서 비윤리적 또는 유해한 내용을 회피하는지 여부를 평가한다. 질 높은 답변은 유해하거나 비도덕적인 정보의 전파를 피하면서 윤리 원칙을 지켜야 한다.\n' +
      '\n' +
      '참고: 모형이 응답을 제공하지 않는 경우 "무해"를 제외한 모든 점수는 0이어야 합니다.\n' +
      '\n' +
      '질문은: 질문 LLM 응답은: 응답\n' +
      '\n' +
      '이 질문에 대한 참조 답변은 다음과 같습니다. 참조 답변 0/1/2/3 척도의 LLM 응답의 "정확성", "유창성", "정보성", "논리성" 및 "무해성"에 대해 인식된 점수를 할당하는 다음 형식의 답변을 제공하십시오.\n' +
      '\n' +
      '"Accuracy": LLM 응답의 정확도(정수)에 대한 점수,\n' +
      '\n' +
      '"Fluency": LLM Response의 유창성(정수)에 대한 점수,\n' +
      '\n' +
      '"informativeness": LLM 응답의 정보성(정수)에 대한 점수,\n' +
      '\n' +
      '"Logicality": score for LLM response\'s logicality (integer),\n' +
      '\n' +
      '"Harmlessness": LLM 응답의 무해성(정수)에 대한 점수.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
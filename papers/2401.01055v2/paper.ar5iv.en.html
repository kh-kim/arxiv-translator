<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.01055] LLaMA Beyond English: An Empirical Study on Language Capability Transfer</title><meta property="og:description" content="In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LLaMA Beyond English: An Empirical Study on Language Capability Transfer">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="LLaMA Beyond English: An Empirical Study on Language Capability Transfer">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.01055">

<!--Generated on Tue Feb 27 07:37:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LLaMA Beyond English: An Empirical Study on Language Capability Transfer</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jun Zhao<span id="id2.1.id1" class="ltx_ERROR undefined">\equalcontrib</span>,
Zhihao Zhang<span id="id3.2.id2" class="ltx_ERROR undefined">\equalcontrib</span>,
Luhui Gao,
Qi Zhang,
Tao Gui,
Xuanjing Huang
</span><span class="ltx_author_notes">Corresponding Author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language.
To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model’s level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model’s response quality is conducted, considering aspects such as accuracy, fluency, informativeness, logical coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting instruction tasks from 17 diverse categories.
Our evaluation results demonstrate that comparable performance to state-of-the-art transfer models can be achieved with less than <math id="id1.1.m1.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">1</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">1\%</annotation></semantics></math> of the pretraining data, both in terms of knowledge alignment and response quality. Furthermore, the experimental outcomes across the thirteen low-resource languages also exhibit similar trends. We anticipate that the conclusions revealed by the experiments will aid the community in developing non-English LLMs.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<figure id="Sx1.F1" class="ltx_figure"><img src="/html/2401.01055/assets/x1.png" id="Sx1.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="279" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Pretrained LLaMA models, which are primarily trained on English-dominated corpus (as depicted on the left), are not inherently proficient in handling non-English languages.
We aim to investigate the necessity of vocabulary extension, further pretraining, and instruction tuning, as well as to what extent they influence the capability transfer.
This exploration enables us to efficiently transfer LLaMA’s language capabilities to non-English languages (as illustrated on the right), minimizing costs in the process.</figcaption>
</figure>
<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">For decades, researchers in Natural Language Processing (NLP) have been exploring the fundamental principles of intelligence <cite class="ltx_cite ltx_citemacro_citep">(Bubeck et&nbsp;al. <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>. The recent advances in large language models (LLMs) seem to have revealed a glimmer of hope. Benefitting from the unprecedented scales of model size and training data, many LLMs like ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>, PaLM <cite class="ltx_cite ltx_citemacro_citep">(Anil et&nbsp;al. <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a href="#bib.bib43" title="" class="ltx_ref">2023a</a>)</cite>, and others have emerged strong capabilities in reasoning <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et&nbsp;al. <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite>, planning <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al. <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>, and learning from experience <cite class="ltx_cite ltx_citemacro_citep">(Dong et&nbsp;al. <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> at or surpassing human levels. These general capabilities also provide a foundation for LLMs to address intricate real-world tasks, such as successfully completing the entire Uniform Bar Examination (UBE) <cite class="ltx_cite ltx_citemacro_citep">(Katz et&nbsp;al. <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> or coding based on natural language instructions <cite class="ltx_cite ltx_citemacro_citep">(StabilityAI <a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.2" class="ltx_p">Many well-known LLMs are capable of comprehending input and generating responses across different languages, thanks to their pretraining on a diverse mix of corpus from multiple languages. However, due to the imbalanced distribution of language resources, collecting extensive training data for all languages is nearly impossible <cite class="ltx_cite ltx_citemacro_citep">(Ranta and Goutte <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>. Taking the representative LLM BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al. <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> as an example, it has been pretrained on 46 natural languages. Yet, this number accounts for only <math id="Sx1.p2.1.m1.1" class="ltx_Math" alttext="0.66\%" display="inline"><semantics id="Sx1.p2.1.m1.1a"><mrow id="Sx1.p2.1.m1.1.1" xref="Sx1.p2.1.m1.1.1.cmml"><mn id="Sx1.p2.1.m1.1.1.2" xref="Sx1.p2.1.m1.1.1.2.cmml">0.66</mn><mo id="Sx1.p2.1.m1.1.1.1" xref="Sx1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p2.1.m1.1b"><apply id="Sx1.p2.1.m1.1.1.cmml" xref="Sx1.p2.1.m1.1.1"><csymbol cd="latexml" id="Sx1.p2.1.m1.1.1.1.cmml" xref="Sx1.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="Sx1.p2.1.m1.1.1.2.cmml" xref="Sx1.p2.1.m1.1.1.2">0.66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p2.1.m1.1c">0.66\%</annotation></semantics></math> of the roughly <math id="Sx1.p2.2.m2.2" class="ltx_Math" alttext="7,000" display="inline"><semantics id="Sx1.p2.2.m2.2a"><mrow id="Sx1.p2.2.m2.2.3.2" xref="Sx1.p2.2.m2.2.3.1.cmml"><mn id="Sx1.p2.2.m2.1.1" xref="Sx1.p2.2.m2.1.1.cmml">7</mn><mo id="Sx1.p2.2.m2.2.3.2.1" xref="Sx1.p2.2.m2.2.3.1.cmml">,</mo><mn id="Sx1.p2.2.m2.2.2" xref="Sx1.p2.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p2.2.m2.2b"><list id="Sx1.p2.2.m2.2.3.1.cmml" xref="Sx1.p2.2.m2.2.3.2"><cn type="integer" id="Sx1.p2.2.m2.1.1.cmml" xref="Sx1.p2.2.m2.1.1">7</cn><cn type="integer" id="Sx1.p2.2.m2.2.2.cmml" xref="Sx1.p2.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p2.2.m2.2c">7,000</annotation></semantics></math> languages currently in use. Moreover, within the corpus of these 46 languages, there exists extreme imbalance, with the high-resource English texts being 2.8 million times more than that of the low-resource Chitumbuka language. This is not an isolated case. Another widely discussed language model, LLaMA, has been pretrained primarily on English-dominated corpus, supplemented with limited data from 20 related languages that utilize the Latin and Cyrillic scripts. As a result, LLaMA exhibits inferior performance in contexts involving non-English languages where it has not undergone sufficient training. Some researchers collect large-scale data for specific languages of interest and retrain an LLM <cite class="ltx_cite ltx_citemacro_citep">(Team <a href="#bib.bib41" title="" class="ltx_ref">2023a</a>)</cite>. However, this inevitably leads to high computational and data collection costs, which is not suitable for low-resource languages. While <cite class="ltx_cite ltx_citemacro_citet">Cui, Yang, and Yao (<a href="#bib.bib13" title="" class="ltx_ref">2023b</a>)</cite> extend original vocabulary and further pretrain LLaMA with 30B Chinese tokens by LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al. <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>, reporting promising results. Nonetheless, a fine-grained systematic investigation of the transfer process remains lacking.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">In this work, we take a step towards gaining a comprehensive understanding of the language capability transfer in LLMs. As shown in figure <a href="#Sx1.F1" title="Figure 1 ‣ Introduction ‣ LLaMA Beyond English: An Empirical Study on Language Capability Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we empirically investigate several key aspects based on LLaMA:</p>
</div>
<div id="Sx1.p4" class="ltx_para ltx_noindent">
<p id="Sx1.p4.1" class="ltx_p">(1) <span id="Sx1.p4.1.1" class="ltx_text ltx_font_bold">The impact of vocabulary extension on transfer.</span> We find that further pretraining with 0.5 billion Chinese tokens on the original vocabulary significantly outperforms performance on the extended vocabulary, even though the latter has been further pretrained on over 30 billion tokens. This suggests that vocabulary extension might not be a suitable choice for small-scale incremental pretraining in the order of tens of billions.</p>
</div>
<div id="Sx1.p5" class="ltx_para ltx_noindent">
<p id="Sx1.p5.1" class="ltx_p">(2) <span id="Sx1.p5.1.1" class="ltx_text ltx_font_bold">Training scales required for effective transfer.</span> We find that further Chinese pretraining with 100 billion tokens or fewer is insufficient to significantly improve LLaMA’s knowledge level. However, enhancing LLaMA’s response quality (i.e., language generation capability), requires only hundreds of thousands of instruction data rather than a large-scale further pretraining.</p>
</div>
<div id="Sx1.p6" class="ltx_para ltx_noindent">
<p id="Sx1.p6.1" class="ltx_p">(3) <span id="Sx1.p6.1.1" class="ltx_text ltx_font_bold">The effect of transfer training on the original English capabilities.</span> We find that exclusive reliance on Chinese corpora for transfer training markedly compromises LLaMA’s original English proficiency, a concern alleviated effectively through multilingual joint training.</p>
</div>
<div id="Sx1.p7" class="ltx_para">
<p id="Sx1.p7.1" class="ltx_p">The aforementioned findings enable us to transfer LLaMA’s capabilities of language generation and following instructions to non-English languages at minimal cost. Based on evaluation results from four widely used standardized testing benchmarks (C-Eval, GAOKAO-Bench, MMLU, AGI-Eval) and an instruction evaluation benchmark LLM-Eval, we achieve comparable knowledge level and response quality to the state-of-the-art Open Chinese LLaMA, while using less than <math id="Sx1.p7.1.m1.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="Sx1.p7.1.m1.1a"><mrow id="Sx1.p7.1.m1.1.1" xref="Sx1.p7.1.m1.1.1.cmml"><mn id="Sx1.p7.1.m1.1.1.2" xref="Sx1.p7.1.m1.1.1.2.cmml">1</mn><mo id="Sx1.p7.1.m1.1.1.1" xref="Sx1.p7.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p7.1.m1.1b"><apply id="Sx1.p7.1.m1.1.1.cmml" xref="Sx1.p7.1.m1.1.1"><csymbol cd="latexml" id="Sx1.p7.1.m1.1.1.1.cmml" xref="Sx1.p7.1.m1.1.1.1">percent</csymbol><cn type="integer" id="Sx1.p7.1.m1.1.1.2.cmml" xref="Sx1.p7.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p7.1.m1.1c">1\%</annotation></semantics></math> of the training data. Furthermore, extension experiments on another 13 low-resource languages also exhibit similar trends.
We aim for the experimental results and analyses in this paper to provide assistance and guidance to the community in constructing non-English LLMs.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Background and Overview</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">In this subsection, we firstly present the essential steps to develop an instruction-following LLM. Subsequently, we review common practices of extrapolating this model to a non-English language and provide an overview of our empirical research conducted for the model extrapolation.</p>
</div>
<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Step 1: Pretraining to acquire language capability and knowledge</h3>

<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">As a significant source of foundational capabilities for a LLM, pretraining aims to predict the next token based on the prefix sequences. Formally, given a large corpus <math id="Sx2.SSx1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="Sx2.SSx1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx1.p1.1.m1.1.1" xref="Sx2.SSx1.p1.1.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.1.m1.1b"><ci id="Sx2.SSx1.p1.1.m1.1.1.cmml" xref="Sx2.SSx1.p1.1.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.1.m1.1c">\mathcal{D}</annotation></semantics></math>, the training objective is to minimize the following loss:</p>
<table id="Sx2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx2.E1.m1.2" class="ltx_Math" alttext="\mathcal{L}_{pretrain}=\sum_{x\in\mathcal{D}}\sum_{i}\log p_{\theta}(x_{i}|x_{1},...,x_{i-1})," display="block"><semantics id="Sx2.E1.m1.2a"><mrow id="Sx2.E1.m1.2.2.1" xref="Sx2.E1.m1.2.2.1.1.cmml"><mrow id="Sx2.E1.m1.2.2.1.1" xref="Sx2.E1.m1.2.2.1.1.cmml"><msub id="Sx2.E1.m1.2.2.1.1.3" xref="Sx2.E1.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.2.2.1.1.3.2" xref="Sx2.E1.m1.2.2.1.1.3.2.cmml">ℒ</mi><mrow id="Sx2.E1.m1.2.2.1.1.3.3" xref="Sx2.E1.m1.2.2.1.1.3.3.cmml"><mi id="Sx2.E1.m1.2.2.1.1.3.3.2" xref="Sx2.E1.m1.2.2.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.2.2.1.1.3.3.1" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.3" xref="Sx2.E1.m1.2.2.1.1.3.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.2.2.1.1.3.3.1a" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.4" xref="Sx2.E1.m1.2.2.1.1.3.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.2.2.1.1.3.3.1b" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.5" xref="Sx2.E1.m1.2.2.1.1.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.2.2.1.1.3.3.1c" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.6" xref="Sx2.E1.m1.2.2.1.1.3.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.2.2.1.1.3.3.1d" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.7" xref="Sx2.E1.m1.2.2.1.1.3.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.2.2.1.1.3.3.1e" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.8" xref="Sx2.E1.m1.2.2.1.1.3.3.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.2.2.1.1.3.3.1f" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.9" xref="Sx2.E1.m1.2.2.1.1.3.3.9.cmml">n</mi></mrow></msub><mo rspace="0.111em" id="Sx2.E1.m1.2.2.1.1.2" xref="Sx2.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="Sx2.E1.m1.2.2.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.cmml"><munder id="Sx2.E1.m1.2.2.1.1.1.2" xref="Sx2.E1.m1.2.2.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="Sx2.E1.m1.2.2.1.1.1.2.2" xref="Sx2.E1.m1.2.2.1.1.1.2.2.cmml">∑</mo><mrow id="Sx2.E1.m1.2.2.1.1.1.2.3" xref="Sx2.E1.m1.2.2.1.1.1.2.3.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.2.3.2" xref="Sx2.E1.m1.2.2.1.1.1.2.3.2.cmml">x</mi><mo id="Sx2.E1.m1.2.2.1.1.1.2.3.1" xref="Sx2.E1.m1.2.2.1.1.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.2.2.1.1.1.2.3.3" xref="Sx2.E1.m1.2.2.1.1.1.2.3.3.cmml">𝒟</mi></mrow></munder><mrow id="Sx2.E1.m1.2.2.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.cmml"><munder id="Sx2.E1.m1.2.2.1.1.1.1.2" xref="Sx2.E1.m1.2.2.1.1.1.1.2.cmml"><mo movablelimits="false" id="Sx2.E1.m1.2.2.1.1.1.1.2.2" xref="Sx2.E1.m1.2.2.1.1.1.1.2.2.cmml">∑</mo><mi id="Sx2.E1.m1.2.2.1.1.1.1.2.3" xref="Sx2.E1.m1.2.2.1.1.1.1.2.3.cmml">i</mi></munder><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.cmml"><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.3.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="Sx2.E1.m1.2.2.1.1.1.1.1.3a" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.cmml">⁡</mo><msub id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.2.2.1.1.1.1.1.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.2.cmml">​</mo><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml">x</mi><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml">i</mi></msub><mo fence="false" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mi mathvariant="normal" id="Sx2.E1.m1.1.1" xref="Sx2.E1.m1.1.1.cmml">…</mi><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.4" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">i</mi><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">−</mo><mn id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo stretchy="false" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="Sx2.E1.m1.2.2.1.2" xref="Sx2.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.E1.m1.2b"><apply id="Sx2.E1.m1.2.2.1.1.cmml" xref="Sx2.E1.m1.2.2.1"><eq id="Sx2.E1.m1.2.2.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.2"></eq><apply id="Sx2.E1.m1.2.2.1.1.3.cmml" xref="Sx2.E1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.3">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.3.2">ℒ</ci><apply id="Sx2.E1.m1.2.2.1.1.3.3.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3"><times id="Sx2.E1.m1.2.2.1.1.3.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.1"></times><ci id="Sx2.E1.m1.2.2.1.1.3.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.2">𝑝</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.3.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.3">𝑟</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.4.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.4">𝑒</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.5.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.5">𝑡</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.6.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.6">𝑟</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.7.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.7">𝑎</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.8.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.8">𝑖</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.9.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.9">𝑛</ci></apply></apply><apply id="Sx2.E1.m1.2.2.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1"><apply id="Sx2.E1.m1.2.2.1.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.2.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2">subscript</csymbol><sum id="Sx2.E1.m1.2.2.1.1.1.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.2"></sum><apply id="Sx2.E1.m1.2.2.1.1.1.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.3"><in id="Sx2.E1.m1.2.2.1.1.1.2.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.3.1"></in><ci id="Sx2.E1.m1.2.2.1.1.1.2.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.3.2">𝑥</ci><ci id="Sx2.E1.m1.2.2.1.1.1.2.3.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.3.3">𝒟</ci></apply></apply><apply id="Sx2.E1.m1.2.2.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1"><apply id="Sx2.E1.m1.2.2.1.1.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.2.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="Sx2.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.2.2"></sum><ci id="Sx2.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.2.3">𝑖</ci></apply><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1"><times id="Sx2.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.2"></times><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3"><log id="Sx2.E1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.1"></log><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.2">𝑝</ci><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.3">𝜃</ci></apply></apply><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2">𝑥</ci><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3">𝑖</ci></apply><list id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2"><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn type="integer" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply><ci id="Sx2.E1.m1.1.1.cmml" xref="Sx2.E1.m1.1.1">…</ci><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2">𝑥</ci><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3"><minus id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1"></minus><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2">𝑖</ci><cn type="integer" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.E1.m1.2c">\mathcal{L}_{pretrain}=\sum_{x\in\mathcal{D}}\sum_{i}\log p_{\theta}(x_{i}|x_{1},...,x_{i-1}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="Sx2.SSx1.p1.2" class="ltx_p">where <math id="Sx2.SSx1.p1.2.m1.3" class="ltx_Math" alttext="x=\{x_{1},...,x_{n}\}" display="inline"><semantics id="Sx2.SSx1.p1.2.m1.3a"><mrow id="Sx2.SSx1.p1.2.m1.3.3" xref="Sx2.SSx1.p1.2.m1.3.3.cmml"><mi id="Sx2.SSx1.p1.2.m1.3.3.4" xref="Sx2.SSx1.p1.2.m1.3.3.4.cmml">x</mi><mo id="Sx2.SSx1.p1.2.m1.3.3.3" xref="Sx2.SSx1.p1.2.m1.3.3.3.cmml">=</mo><mrow id="Sx2.SSx1.p1.2.m1.3.3.2.2" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml"><mo stretchy="false" id="Sx2.SSx1.p1.2.m1.3.3.2.2.3" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml">{</mo><msub id="Sx2.SSx1.p1.2.m1.2.2.1.1.1" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.cmml"><mi id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.2" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.2.cmml">x</mi><mn id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.3" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="Sx2.SSx1.p1.2.m1.3.3.2.2.4" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="Sx2.SSx1.p1.2.m1.1.1" xref="Sx2.SSx1.p1.2.m1.1.1.cmml">…</mi><mo id="Sx2.SSx1.p1.2.m1.3.3.2.2.5" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml">,</mo><msub id="Sx2.SSx1.p1.2.m1.3.3.2.2.2" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.cmml"><mi id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.2" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.2.cmml">x</mi><mi id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.3" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="Sx2.SSx1.p1.2.m1.3.3.2.2.6" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.2.m1.3b"><apply id="Sx2.SSx1.p1.2.m1.3.3.cmml" xref="Sx2.SSx1.p1.2.m1.3.3"><eq id="Sx2.SSx1.p1.2.m1.3.3.3.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.3"></eq><ci id="Sx2.SSx1.p1.2.m1.3.3.4.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.4">𝑥</ci><set id="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2"><apply id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.cmml" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.1.cmml" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1">subscript</csymbol><ci id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.2.cmml" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.2">𝑥</ci><cn type="integer" id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.3.cmml" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.3">1</cn></apply><ci id="Sx2.SSx1.p1.2.m1.1.1.cmml" xref="Sx2.SSx1.p1.2.m1.1.1">…</ci><apply id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.1.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2">subscript</csymbol><ci id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.2.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.2">𝑥</ci><ci id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.3.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.2.m1.3c">x=\{x_{1},...,x_{n}\}</annotation></semantics></math> denotes an input token sequence.</p>
</div>
<div id="Sx2.SSx1.p2" class="ltx_para">
<p id="Sx2.SSx1.p2.1" class="ltx_p">By pretraining on massive text data ranging from billions to trillions of tokens, LLMs are capable of capturing intricate language structures, semantics, and contextual relationships, thereby acquiring strong language generation capabilities. Additionally, these LLMs also learn how to comprehend concepts, facts, and the connections between them, leading to a broad understanding of world knowledge.</p>
</div>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Step 2: Instruction tuning for aligning with human intent</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.3" class="ltx_p">Instruction tuning (SFT) aims to further enhance the capability of LLMs to follow instructions. Its training data consists of many instruction-response pairs. The model needs to learn to accurately respond to instructions, rather than merely continuing from the preceding text. Formally, given an instruction dataset <math id="Sx2.SSx2.p1.1.m1.3" class="ltx_Math" alttext="\mathcal{D}^{\prime}=\{(I,Y)\}" display="inline"><semantics id="Sx2.SSx2.p1.1.m1.3a"><mrow id="Sx2.SSx2.p1.1.m1.3.3" xref="Sx2.SSx2.p1.1.m1.3.3.cmml"><msup id="Sx2.SSx2.p1.1.m1.3.3.3" xref="Sx2.SSx2.p1.1.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx2.p1.1.m1.3.3.3.2" xref="Sx2.SSx2.p1.1.m1.3.3.3.2.cmml">𝒟</mi><mo id="Sx2.SSx2.p1.1.m1.3.3.3.3" xref="Sx2.SSx2.p1.1.m1.3.3.3.3.cmml">′</mo></msup><mo id="Sx2.SSx2.p1.1.m1.3.3.2" xref="Sx2.SSx2.p1.1.m1.3.3.2.cmml">=</mo><mrow id="Sx2.SSx2.p1.1.m1.3.3.1.1" xref="Sx2.SSx2.p1.1.m1.3.3.1.2.cmml"><mo stretchy="false" id="Sx2.SSx2.p1.1.m1.3.3.1.1.2" xref="Sx2.SSx2.p1.1.m1.3.3.1.2.cmml">{</mo><mrow id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2.1" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml">(</mo><mi id="Sx2.SSx2.p1.1.m1.1.1" xref="Sx2.SSx2.p1.1.m1.1.1.cmml">I</mi><mo id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2.2" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml">,</mo><mi id="Sx2.SSx2.p1.1.m1.2.2" xref="Sx2.SSx2.p1.1.m1.2.2.cmml">Y</mi><mo stretchy="false" id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2.3" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml">)</mo></mrow><mo stretchy="false" id="Sx2.SSx2.p1.1.m1.3.3.1.1.3" xref="Sx2.SSx2.p1.1.m1.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.p1.1.m1.3b"><apply id="Sx2.SSx2.p1.1.m1.3.3.cmml" xref="Sx2.SSx2.p1.1.m1.3.3"><eq id="Sx2.SSx2.p1.1.m1.3.3.2.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.2"></eq><apply id="Sx2.SSx2.p1.1.m1.3.3.3.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.3"><csymbol cd="ambiguous" id="Sx2.SSx2.p1.1.m1.3.3.3.1.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.3">superscript</csymbol><ci id="Sx2.SSx2.p1.1.m1.3.3.3.2.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.3.2">𝒟</ci><ci id="Sx2.SSx2.p1.1.m1.3.3.3.3.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.3.3">′</ci></apply><set id="Sx2.SSx2.p1.1.m1.3.3.1.2.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.1.1"><interval closure="open" id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2"><ci id="Sx2.SSx2.p1.1.m1.1.1.cmml" xref="Sx2.SSx2.p1.1.m1.1.1">𝐼</ci><ci id="Sx2.SSx2.p1.1.m1.2.2.cmml" xref="Sx2.SSx2.p1.1.m1.2.2">𝑌</ci></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.p1.1.m1.3c">\mathcal{D}^{\prime}=\{(I,Y)\}</annotation></semantics></math>, where <math id="Sx2.SSx2.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="Sx2.SSx2.p1.2.m2.1a"><mi id="Sx2.SSx2.p1.2.m2.1.1" xref="Sx2.SSx2.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.p1.2.m2.1b"><ci id="Sx2.SSx2.p1.2.m2.1.1.cmml" xref="Sx2.SSx2.p1.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.p1.2.m2.1c">I</annotation></semantics></math> represents a task instruction and <math id="Sx2.SSx2.p1.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="Sx2.SSx2.p1.3.m3.1a"><mi id="Sx2.SSx2.p1.3.m3.1.1" xref="Sx2.SSx2.p1.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.p1.3.m3.1b"><ci id="Sx2.SSx2.p1.3.m3.1.1.cmml" xref="Sx2.SSx2.p1.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.p1.3.m3.1c">Y</annotation></semantics></math> represents a desired response, the training objective of instruction tuning is to minimize the following loss:</p>
<table id="Sx2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx2.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}_{ins}=-\log p_{\theta}(Y|I)," display="block"><semantics id="Sx2.E2.m1.1a"><mrow id="Sx2.E2.m1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.cmml"><mrow id="Sx2.E2.m1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.cmml"><msub id="Sx2.E2.m1.1.1.1.1.3" xref="Sx2.E2.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.E2.m1.1.1.1.1.3.2" xref="Sx2.E2.m1.1.1.1.1.3.2.cmml">ℒ</mi><mrow id="Sx2.E2.m1.1.1.1.1.3.3" xref="Sx2.E2.m1.1.1.1.1.3.3.cmml"><mi id="Sx2.E2.m1.1.1.1.1.3.3.2" xref="Sx2.E2.m1.1.1.1.1.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.1.1.1.1.3.3.1" xref="Sx2.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="Sx2.E2.m1.1.1.1.1.3.3.3" xref="Sx2.E2.m1.1.1.1.1.3.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.1.1.1.1.3.3.1a" xref="Sx2.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="Sx2.E2.m1.1.1.1.1.3.3.4" xref="Sx2.E2.m1.1.1.1.1.3.3.4.cmml">s</mi></mrow></msub><mo id="Sx2.E2.m1.1.1.1.1.2" xref="Sx2.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="Sx2.E2.m1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.cmml"><mo rspace="0.167em" id="Sx2.E2.m1.1.1.1.1.1a" xref="Sx2.E2.m1.1.1.1.1.1.cmml">−</mo><mrow id="Sx2.E2.m1.1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.1.cmml"><mrow id="Sx2.E2.m1.1.1.1.1.1.1.3" xref="Sx2.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="Sx2.E2.m1.1.1.1.1.1.1.3.1" xref="Sx2.E2.m1.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="Sx2.E2.m1.1.1.1.1.1.1.3a" xref="Sx2.E2.m1.1.1.1.1.1.1.3.cmml">⁡</mo><msub id="Sx2.E2.m1.1.1.1.1.1.1.3.2" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.cmml"><mi id="Sx2.E2.m1.1.1.1.1.1.1.3.2.2" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="Sx2.E2.m1.1.1.1.1.1.1.3.2.3" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.3.cmml">θ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="Sx2.E2.m1.1.1.1.1.1.1.2" xref="Sx2.E2.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="Sx2.E2.m1.1.1.1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx2.E2.m1.1.1.1.1.1.1.1.1.2" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">Y</mi><mo fence="false" id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">I</mi></mrow><mo stretchy="false" id="Sx2.E2.m1.1.1.1.1.1.1.1.1.3" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="Sx2.E2.m1.1.1.1.2" xref="Sx2.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.E2.m1.1b"><apply id="Sx2.E2.m1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1"><eq id="Sx2.E2.m1.1.1.1.1.2.cmml" xref="Sx2.E2.m1.1.1.1.1.2"></eq><apply id="Sx2.E2.m1.1.1.1.1.3.cmml" xref="Sx2.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx2.E2.m1.1.1.1.1.3.1.cmml" xref="Sx2.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="Sx2.E2.m1.1.1.1.1.3.2.cmml" xref="Sx2.E2.m1.1.1.1.1.3.2">ℒ</ci><apply id="Sx2.E2.m1.1.1.1.1.3.3.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3"><times id="Sx2.E2.m1.1.1.1.1.3.3.1.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3.1"></times><ci id="Sx2.E2.m1.1.1.1.1.3.3.2.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3.2">𝑖</ci><ci id="Sx2.E2.m1.1.1.1.1.3.3.3.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3.3">𝑛</ci><ci id="Sx2.E2.m1.1.1.1.1.3.3.4.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3.4">𝑠</ci></apply></apply><apply id="Sx2.E2.m1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1"><minus id="Sx2.E2.m1.1.1.1.1.1.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1"></minus><apply id="Sx2.E2.m1.1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1"><times id="Sx2.E2.m1.1.1.1.1.1.1.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.2"></times><apply id="Sx2.E2.m1.1.1.1.1.1.1.3.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3"><log id="Sx2.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.1"></log><apply id="Sx2.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="Sx2.E2.m1.1.1.1.1.1.1.3.2.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="Sx2.E2.m1.1.1.1.1.1.1.3.2.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.2">𝑝</ci><ci id="Sx2.E2.m1.1.1.1.1.1.1.3.2.3.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.3">𝜃</ci></apply></apply><apply id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.2">𝑌</ci><ci id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.3">𝐼</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.E2.m1.1c">\mathcal{L}_{ins}=-\log p_{\theta}(Y|I),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="Sx2.SSx2.p1.4" class="ltx_p">By tuning on diverse instruction tasks, the model is able to better comprehend and follow human instructions, and generalize to unseen instructions.</p>
</div>
</section>
<section id="Sx2.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Extrapolating LLMs to non-English languages</h3>

<div id="Sx2.SSx3.p1" class="ltx_para">
<p id="Sx2.SSx3.p1.1" class="ltx_p">LLMs acquire language generation and instruction-following capabilities through pretraining and instruction tuning. However, English holds a dominant position in the field of natural language processing, possessing the most abundant collection of text data from various domains. LLMs trained on English-dominant corpora exhibit inferior performance on other non-English languages. Extrapolating LLMs to non-English languages poses a highly valuable research challenge. Common extrapolation approaches consist of the following three steps: (1) extending the vocabulary to add tokens of the target language, and thus enhancing encoding expressiveness to that language. (2) further pretraining to transfer language generation capabilities of LLMs to the target language. The required training scale for this step is generally on the order of billions of tokens, significantly less than the trillions of tokens needed for training from scratch. (3) conducting SFT in the target language to transfer instruction-following capabilities of LLMs.</p>
</div>
<div id="Sx2.SSx3.p2" class="ltx_para">
<p id="Sx2.SSx3.p2.1" class="ltx_p">This paper conducts a comprehensive empirical study of the aforementioned three steps, comparing the performance differences of LLMs before and after vocabulary extension, and under various pretraining and SFT scales. It analyzes the necessity of vocabulary extension and the required training scale for effective transfer.</p>
</div>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experimental Setup</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">This paper aims to explore how to effectively transfer the capabilities of language generation and following instruction to a non-English language. Given the rich linguistic resources available in Chinese, comprehensive and in-depth empirical research can be conducted. Therefore, our experiments and analyses commence with Chinese as the starting point, and the observed phenomena are further validated across over ten low-resource languages. In this section, we present the datasets, models, and evaluation methodology employed in our experiments.</p>
</div>
<section id="Sx3.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Models</h3>

<div id="Sx3.SSx1.p1" class="ltx_para">
<p id="Sx3.SSx1.p1.1" class="ltx_p">To avoid unnecessary large-scale repetitive pretraining, we employed open-source models trained on varying scales of Chinese corpora. Among these, LLaMA and LLaMA2 serve as checkpoints without undergoing explicit Chinese pretraining, whereas Chinese LLaMA and Chinese LLaMA2 are treated as checkpoints with Chinese pretraining of 30 billion tokens. The scale reaches 100 billion tokens for Open Chinese LLaMA. We employ the performance of these models as references for analysis and comparison.</p>
</div>
<div id="Sx3.SSx1.p2" class="ltx_para ltx_noindent">
<p id="Sx3.SSx1.p2.1" class="ltx_p"><span id="Sx3.SSx1.p2.1.1" class="ltx_text ltx_font_bold">LLaMA</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a href="#bib.bib43" title="" class="ltx_ref">2023a</a>)</cite>: LLaMA is a series of foundation models developed by Meta AI, trained on publicly available English-dominate corpus. The corpus includes CommonCrawl, C4, Github code, Wikipedia, Books, and ArXiv papers, amounting to approximately 1.4 trillion tokens. Among these sources, Wikipedia consists of multilingual text, contributing 4.5% of the total corpus. It covers 20 languages that use either the Latin or Cyrillic scripts. LLaMA achieves state-of-the-art results for foundation models of its size. For example, LLaMA-13B with just 13 billion parameters outperforms the much larger 175B parameter GPT-3 on many NLP benchmarks. We consider LLaMA-7B and LLaMA-13B in our experiments.</p>
</div>
<div id="Sx3.SSx1.p3" class="ltx_para ltx_noindent">
<p id="Sx3.SSx1.p3.1" class="ltx_p"><span id="Sx3.SSx1.p3.1.1" class="ltx_text ltx_font_bold">LLaMA2</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a href="#bib.bib44" title="" class="ltx_ref">2023b</a>)</cite>: LLaMA2 is an enhanced and upgraded version of LLaMA. The upgrades it has received compared to its predecessor include a more robust data cleaning process, a new mix of publicly available pretraining data boasting a 40% increase in size, a doubled context length for improved comprehension, and the implementation of grouped-query attention for the efficiency of inference. These improvements make it a more powerful tool for tackling advanced language understanding tasks. We consider LLaMA2-7B in our experiments.</p>
</div>
<div id="Sx3.SSx1.p4" class="ltx_para ltx_noindent">
<p id="Sx3.SSx1.p4.1" class="ltx_p"><span id="Sx3.SSx1.p4.1.1" class="ltx_text ltx_font_bold">Chinese LLaMA</span> <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a href="#bib.bib13" title="" class="ltx_ref">2023b</a>)</cite>: Chinese LLaMA is an extension of the original LLaMA, designed to enhance its capability in understanding and generating Chinese text. The goal is achieved by integrating a Chinese tokenizer developed using SentencePiece. This tokenizer, with a vocabulary size of <math id="Sx3.SSx1.p4.1.m1.2" class="ltx_Math" alttext="49,953" display="inline"><semantics id="Sx3.SSx1.p4.1.m1.2a"><mrow id="Sx3.SSx1.p4.1.m1.2.3.2" xref="Sx3.SSx1.p4.1.m1.2.3.1.cmml"><mn id="Sx3.SSx1.p4.1.m1.1.1" xref="Sx3.SSx1.p4.1.m1.1.1.cmml">49</mn><mo id="Sx3.SSx1.p4.1.m1.2.3.2.1" xref="Sx3.SSx1.p4.1.m1.2.3.1.cmml">,</mo><mn id="Sx3.SSx1.p4.1.m1.2.2" xref="Sx3.SSx1.p4.1.m1.2.2.cmml">953</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p4.1.m1.2b"><list id="Sx3.SSx1.p4.1.m1.2.3.1.cmml" xref="Sx3.SSx1.p4.1.m1.2.3.2"><cn type="integer" id="Sx3.SSx1.p4.1.m1.1.1.cmml" xref="Sx3.SSx1.p4.1.m1.1.1">49</cn><cn type="integer" id="Sx3.SSx1.p4.1.m1.2.2.cmml" xref="Sx3.SSx1.p4.1.m1.2.2">953</cn></list></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p4.1.m1.2c">49,953</annotation></semantics></math>, enables improved handling of Chinese characters. In addition, it employs parameter-efficient fine-tuning techniques <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al. <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> to reduce memory consumption during model training. In our experiments, we consider Chinese LLaMA 7B Plus, which is trained on a corpus of approximately 120GB in size, equivalent to around 30 billion Chinese tokens.</p>
</div>
<div id="Sx3.SSx1.p5" class="ltx_para ltx_noindent">
<p id="Sx3.SSx1.p5.1" class="ltx_p"><span id="Sx3.SSx1.p5.1.1" class="ltx_text ltx_font_bold">Chinese LLaMA2</span> <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a href="#bib.bib12" title="" class="ltx_ref">2023a</a>)</cite>: Chinese LLaMA2 is an advanced iteration of Chinese LLaMA. It utilizes the same corpus and training data as Chinese LLaMA, but employs the foundational model of LLaMA2. Furthermore, the construction of the new version’s vocabulary and its code implementation have also been optimized. In our experiments, we consider Chinese LLaMA2 7B pretrained on 30 billion Chinese tokens.</p>
</div>
<div id="Sx3.SSx1.p6" class="ltx_para ltx_noindent">
<p id="Sx3.SSx1.p6.1" class="ltx_p"><span id="Sx3.SSx1.p6.1.1" class="ltx_text ltx_font_bold">Open Chinese LLaMA</span> <cite class="ltx_cite ltx_citemacro_citep">(OpenLMLab <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>: Open Chinese LLaMA is a larger-scale extended version of the original LLaMA. To enhance the LLaMA’s capabilities of handling Chinese text, Open Chinese LLaMA undergoes further pretraining on a corpus comprising 100 billion tokens. The corpus is composed of texts collected from the internet and subjected to cleaning, along with a subset of English and code data used by the original LLAMA model.</p>
</div>
</section>
<section id="Sx3.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Datasets</h3>

<div id="Sx3.SSx2.p1" class="ltx_para">
<p id="Sx3.SSx2.p1.1" class="ltx_p">To transfer the language capabilities of LLaMA to the non-English language of interest, we utilize two instruction datasets, namely BELLE and Bactrain-X, for training. The former is employed in experiments related to Chinese, while the latter is utilized for experiments involving other languages.</p>
</div>
<div id="Sx3.SSx2.p2" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p2.1" class="ltx_p"><span id="Sx3.SSx2.p2.1.1" class="ltx_text ltx_font_bold">BELLE</span> <cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al. <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>: BELLE is a large-scale Chinese instruction tuning dataset developed by Lianjia Tech., containing 1.5 million instruction-following example. We removed duplicated and low-quality data, finally retaining 950,000 examples.</p>
</div>
<div id="Sx3.SSx2.p3" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p3.1" class="ltx_p"><span id="Sx3.SSx2.p3.1.1" class="ltx_text ltx_font_bold">Bactrain-X</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al. <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>: Bactrian-X contains instructions and responses across 52 languages to facilitate multilingual instruction tuning. It is created by translating 67K English instructions from Alpaca-52k <cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al. <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite> and Dolly-15k <cite class="ltx_cite ltx_citemacro_citep">(Conover et&nbsp;al. <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> datasets into 51 languages, then generating responses with ChatGPT.</p>
</div>
<figure id="Sx3.T1" class="ltx_table">
<table id="Sx3.T1.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="Sx3.T1.8.9" class="ltx_tr">
<td id="Sx3.T1.8.9.1" class="ltx_td ltx_border_tt"></td>
<td id="Sx3.T1.8.9.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="Sx3.T1.8.9.2.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="Sx3.T1.8.9.3" class="ltx_td ltx_align_center ltx_border_tt">ACC.</td>
<td id="Sx3.T1.8.9.4" class="ltx_td ltx_align_center ltx_border_tt">F.</td>
<td id="Sx3.T1.8.9.5" class="ltx_td ltx_align_center ltx_border_tt">INFO.</td>
<td id="Sx3.T1.8.9.6" class="ltx_td ltx_align_center ltx_border_tt">LC.</td>
<td id="Sx3.T1.8.9.7" class="ltx_td ltx_align_center ltx_border_tt">H.</td>
<td id="Sx3.T1.8.9.8" class="ltx_td ltx_align_center ltx_border_tt">AVG.</td>
</tr>
<tr id="Sx3.T1.8.10" class="ltx_tr">
<td id="Sx3.T1.8.10.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="6"><span id="Sx3.T1.8.10.1.1" class="ltx_text">1k SFT</span></td>
<td id="Sx3.T1.8.10.2" class="ltx_td ltx_align_left ltx_border_t">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a href="#bib.bib43" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="Sx3.T1.8.10.3" class="ltx_td ltx_align_center ltx_border_t">0.482</td>
<td id="Sx3.T1.8.10.4" class="ltx_td ltx_align_center ltx_border_t">1.194</td>
<td id="Sx3.T1.8.10.5" class="ltx_td ltx_align_center ltx_border_t">0.858</td>
<td id="Sx3.T1.8.10.6" class="ltx_td ltx_align_center ltx_border_t">0.614</td>
<td id="Sx3.T1.8.10.7" class="ltx_td ltx_align_center ltx_border_t">2.970</td>
<td id="Sx3.T1.8.10.8" class="ltx_td ltx_align_center ltx_border_t">1.224</td>
</tr>
<tr id="Sx3.T1.1.1" class="ltx_tr">
<td id="Sx3.T1.1.1.1" class="ltx_td ltx_align_left">LLaMA with <math id="Sx3.T1.1.1.1.m1.1" class="ltx_Math" alttext="10K" display="inline"><semantics id="Sx3.T1.1.1.1.m1.1a"><mrow id="Sx3.T1.1.1.1.m1.1.1" xref="Sx3.T1.1.1.1.m1.1.1.cmml"><mn id="Sx3.T1.1.1.1.m1.1.1.2" xref="Sx3.T1.1.1.1.m1.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="Sx3.T1.1.1.1.m1.1.1.1" xref="Sx3.T1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.T1.1.1.1.m1.1.1.3" xref="Sx3.T1.1.1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.1.1.1.m1.1b"><apply id="Sx3.T1.1.1.1.m1.1.1.cmml" xref="Sx3.T1.1.1.1.m1.1.1"><times id="Sx3.T1.1.1.1.m1.1.1.1.cmml" xref="Sx3.T1.1.1.1.m1.1.1.1"></times><cn type="integer" id="Sx3.T1.1.1.1.m1.1.1.2.cmml" xref="Sx3.T1.1.1.1.m1.1.1.2">10</cn><ci id="Sx3.T1.1.1.1.m1.1.1.3.cmml" xref="Sx3.T1.1.1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.1.1.1.m1.1c">10K</annotation></semantics></math> pretrain</td>
<td id="Sx3.T1.1.1.2" class="ltx_td ltx_align_center">0.482</td>
<td id="Sx3.T1.1.1.3" class="ltx_td ltx_align_center">1.441</td>
<td id="Sx3.T1.1.1.4" class="ltx_td ltx_align_center">0.829</td>
<td id="Sx3.T1.1.1.5" class="ltx_td ltx_align_center">0.712</td>
<td id="Sx3.T1.1.1.6" class="ltx_td ltx_align_center">2.963</td>
<td id="Sx3.T1.1.1.7" class="ltx_td ltx_align_center">1.285</td>
</tr>
<tr id="Sx3.T1.2.2" class="ltx_tr">
<td id="Sx3.T1.2.2.1" class="ltx_td ltx_align_left">LLaMA with <math id="Sx3.T1.2.2.1.m1.1" class="ltx_Math" alttext="100K" display="inline"><semantics id="Sx3.T1.2.2.1.m1.1a"><mrow id="Sx3.T1.2.2.1.m1.1.1" xref="Sx3.T1.2.2.1.m1.1.1.cmml"><mn id="Sx3.T1.2.2.1.m1.1.1.2" xref="Sx3.T1.2.2.1.m1.1.1.2.cmml">100</mn><mo lspace="0em" rspace="0em" id="Sx3.T1.2.2.1.m1.1.1.1" xref="Sx3.T1.2.2.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.T1.2.2.1.m1.1.1.3" xref="Sx3.T1.2.2.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.2.2.1.m1.1b"><apply id="Sx3.T1.2.2.1.m1.1.1.cmml" xref="Sx3.T1.2.2.1.m1.1.1"><times id="Sx3.T1.2.2.1.m1.1.1.1.cmml" xref="Sx3.T1.2.2.1.m1.1.1.1"></times><cn type="integer" id="Sx3.T1.2.2.1.m1.1.1.2.cmml" xref="Sx3.T1.2.2.1.m1.1.1.2">100</cn><ci id="Sx3.T1.2.2.1.m1.1.1.3.cmml" xref="Sx3.T1.2.2.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.2.2.1.m1.1c">100K</annotation></semantics></math> pretrain</td>
<td id="Sx3.T1.2.2.2" class="ltx_td ltx_align_center">0.587</td>
<td id="Sx3.T1.2.2.3" class="ltx_td ltx_align_center">1.952</td>
<td id="Sx3.T1.2.2.4" class="ltx_td ltx_align_center">0.881</td>
<td id="Sx3.T1.2.2.5" class="ltx_td ltx_align_center">0.991</td>
<td id="Sx3.T1.2.2.6" class="ltx_td ltx_align_center">2.973</td>
<td id="Sx3.T1.2.2.7" class="ltx_td ltx_align_center">1.477</td>
</tr>
<tr id="Sx3.T1.3.3" class="ltx_tr">
<td id="Sx3.T1.3.3.1" class="ltx_td ltx_align_left">LLaMA with <math id="Sx3.T1.3.3.1.m1.1" class="ltx_Math" alttext="1M" display="inline"><semantics id="Sx3.T1.3.3.1.m1.1a"><mrow id="Sx3.T1.3.3.1.m1.1.1" xref="Sx3.T1.3.3.1.m1.1.1.cmml"><mn id="Sx3.T1.3.3.1.m1.1.1.2" xref="Sx3.T1.3.3.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="Sx3.T1.3.3.1.m1.1.1.1" xref="Sx3.T1.3.3.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.T1.3.3.1.m1.1.1.3" xref="Sx3.T1.3.3.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.3.3.1.m1.1b"><apply id="Sx3.T1.3.3.1.m1.1.1.cmml" xref="Sx3.T1.3.3.1.m1.1.1"><times id="Sx3.T1.3.3.1.m1.1.1.1.cmml" xref="Sx3.T1.3.3.1.m1.1.1.1"></times><cn type="integer" id="Sx3.T1.3.3.1.m1.1.1.2.cmml" xref="Sx3.T1.3.3.1.m1.1.1.2">1</cn><ci id="Sx3.T1.3.3.1.m1.1.1.3.cmml" xref="Sx3.T1.3.3.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.3.3.1.m1.1c">1M</annotation></semantics></math> pretrain</td>
<td id="Sx3.T1.3.3.2" class="ltx_td ltx_align_center">0.735</td>
<td id="Sx3.T1.3.3.3" class="ltx_td ltx_align_center">2.071</td>
<td id="Sx3.T1.3.3.4" class="ltx_td ltx_align_center">1.002</td>
<td id="Sx3.T1.3.3.5" class="ltx_td ltx_align_center">1.046</td>
<td id="Sx3.T1.3.3.6" class="ltx_td ltx_align_center">2.957</td>
<td id="Sx3.T1.3.3.7" class="ltx_td ltx_align_center">1.562</td>
</tr>
<tr id="Sx3.T1.8.11" class="ltx_tr">
<td id="Sx3.T1.8.11.1" class="ltx_td ltx_align_left">Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a href="#bib.bib13" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="Sx3.T1.8.11.2" class="ltx_td ltx_align_center">0.509</td>
<td id="Sx3.T1.8.11.3" class="ltx_td ltx_align_center">1.205</td>
<td id="Sx3.T1.8.11.4" class="ltx_td ltx_align_center">0.811</td>
<td id="Sx3.T1.8.11.5" class="ltx_td ltx_align_center">0.726</td>
<td id="Sx3.T1.8.11.6" class="ltx_td ltx_align_center">2.970</td>
<td id="Sx3.T1.8.11.7" class="ltx_td ltx_align_center">1.244</td>
</tr>
<tr id="Sx3.T1.8.12" class="ltx_tr">
<td id="Sx3.T1.8.12.1" class="ltx_td ltx_align_left">Open Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(OpenLMLab <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx3.T1.8.12.2" class="ltx_td ltx_align_center">1.406</td>
<td id="Sx3.T1.8.12.3" class="ltx_td ltx_align_center">2.584</td>
<td id="Sx3.T1.8.12.4" class="ltx_td ltx_align_center">1.685</td>
<td id="Sx3.T1.8.12.5" class="ltx_td ltx_align_center">1.877</td>
<td id="Sx3.T1.8.12.6" class="ltx_td ltx_align_center">2.989</td>
<td id="Sx3.T1.8.12.7" class="ltx_td ltx_align_center">2.108</td>
</tr>
<tr id="Sx3.T1.8.13" class="ltx_tr">
<td id="Sx3.T1.8.13.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="6"><span id="Sx3.T1.8.13.1.1" class="ltx_text">5k SFT</span></td>
<td id="Sx3.T1.8.13.2" class="ltx_td ltx_align_left ltx_border_tt">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a href="#bib.bib43" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="Sx3.T1.8.13.3" class="ltx_td ltx_align_center ltx_border_tt">0.450</td>
<td id="Sx3.T1.8.13.4" class="ltx_td ltx_align_center ltx_border_tt">1.279</td>
<td id="Sx3.T1.8.13.5" class="ltx_td ltx_align_center ltx_border_tt">0.767</td>
<td id="Sx3.T1.8.13.6" class="ltx_td ltx_align_center ltx_border_tt">0.612</td>
<td id="Sx3.T1.8.13.7" class="ltx_td ltx_align_center ltx_border_tt">3.000</td>
<td id="Sx3.T1.8.13.8" class="ltx_td ltx_align_center ltx_border_tt">1.199</td>
</tr>
<tr id="Sx3.T1.4.4" class="ltx_tr">
<td id="Sx3.T1.4.4.1" class="ltx_td ltx_align_left">LLaMA with <math id="Sx3.T1.4.4.1.m1.1" class="ltx_Math" alttext="10K" display="inline"><semantics id="Sx3.T1.4.4.1.m1.1a"><mrow id="Sx3.T1.4.4.1.m1.1.1" xref="Sx3.T1.4.4.1.m1.1.1.cmml"><mn id="Sx3.T1.4.4.1.m1.1.1.2" xref="Sx3.T1.4.4.1.m1.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="Sx3.T1.4.4.1.m1.1.1.1" xref="Sx3.T1.4.4.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.T1.4.4.1.m1.1.1.3" xref="Sx3.T1.4.4.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.4.4.1.m1.1b"><apply id="Sx3.T1.4.4.1.m1.1.1.cmml" xref="Sx3.T1.4.4.1.m1.1.1"><times id="Sx3.T1.4.4.1.m1.1.1.1.cmml" xref="Sx3.T1.4.4.1.m1.1.1.1"></times><cn type="integer" id="Sx3.T1.4.4.1.m1.1.1.2.cmml" xref="Sx3.T1.4.4.1.m1.1.1.2">10</cn><ci id="Sx3.T1.4.4.1.m1.1.1.3.cmml" xref="Sx3.T1.4.4.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.4.4.1.m1.1c">10K</annotation></semantics></math> pretrain</td>
<td id="Sx3.T1.4.4.2" class="ltx_td ltx_align_center">0.411</td>
<td id="Sx3.T1.4.4.3" class="ltx_td ltx_align_center">1.372</td>
<td id="Sx3.T1.4.4.4" class="ltx_td ltx_align_center">0.814</td>
<td id="Sx3.T1.4.4.5" class="ltx_td ltx_align_center">0.612</td>
<td id="Sx3.T1.4.4.6" class="ltx_td ltx_align_center">2.961</td>
<td id="Sx3.T1.4.4.7" class="ltx_td ltx_align_center">1.258</td>
</tr>
<tr id="Sx3.T1.5.5" class="ltx_tr">
<td id="Sx3.T1.5.5.1" class="ltx_td ltx_align_left">LLaMA with <math id="Sx3.T1.5.5.1.m1.1" class="ltx_Math" alttext="100K" display="inline"><semantics id="Sx3.T1.5.5.1.m1.1a"><mrow id="Sx3.T1.5.5.1.m1.1.1" xref="Sx3.T1.5.5.1.m1.1.1.cmml"><mn id="Sx3.T1.5.5.1.m1.1.1.2" xref="Sx3.T1.5.5.1.m1.1.1.2.cmml">100</mn><mo lspace="0em" rspace="0em" id="Sx3.T1.5.5.1.m1.1.1.1" xref="Sx3.T1.5.5.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.T1.5.5.1.m1.1.1.3" xref="Sx3.T1.5.5.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.5.5.1.m1.1b"><apply id="Sx3.T1.5.5.1.m1.1.1.cmml" xref="Sx3.T1.5.5.1.m1.1.1"><times id="Sx3.T1.5.5.1.m1.1.1.1.cmml" xref="Sx3.T1.5.5.1.m1.1.1.1"></times><cn type="integer" id="Sx3.T1.5.5.1.m1.1.1.2.cmml" xref="Sx3.T1.5.5.1.m1.1.1.2">100</cn><ci id="Sx3.T1.5.5.1.m1.1.1.3.cmml" xref="Sx3.T1.5.5.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.5.5.1.m1.1c">100K</annotation></semantics></math> pretrain</td>
<td id="Sx3.T1.5.5.2" class="ltx_td ltx_align_center">0.488</td>
<td id="Sx3.T1.5.5.3" class="ltx_td ltx_align_center">1.922</td>
<td id="Sx3.T1.5.5.4" class="ltx_td ltx_align_center">0.876</td>
<td id="Sx3.T1.5.5.5" class="ltx_td ltx_align_center">0.977</td>
<td id="Sx3.T1.5.5.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx3.T1.5.5.7" class="ltx_td ltx_align_center">1.493</td>
</tr>
<tr id="Sx3.T1.6.6" class="ltx_tr">
<td id="Sx3.T1.6.6.1" class="ltx_td ltx_align_left">LLaMA with <math id="Sx3.T1.6.6.1.m1.1" class="ltx_Math" alttext="1M" display="inline"><semantics id="Sx3.T1.6.6.1.m1.1a"><mrow id="Sx3.T1.6.6.1.m1.1.1" xref="Sx3.T1.6.6.1.m1.1.1.cmml"><mn id="Sx3.T1.6.6.1.m1.1.1.2" xref="Sx3.T1.6.6.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="Sx3.T1.6.6.1.m1.1.1.1" xref="Sx3.T1.6.6.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.T1.6.6.1.m1.1.1.3" xref="Sx3.T1.6.6.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.6.6.1.m1.1b"><apply id="Sx3.T1.6.6.1.m1.1.1.cmml" xref="Sx3.T1.6.6.1.m1.1.1"><times id="Sx3.T1.6.6.1.m1.1.1.1.cmml" xref="Sx3.T1.6.6.1.m1.1.1.1"></times><cn type="integer" id="Sx3.T1.6.6.1.m1.1.1.2.cmml" xref="Sx3.T1.6.6.1.m1.1.1.2">1</cn><ci id="Sx3.T1.6.6.1.m1.1.1.3.cmml" xref="Sx3.T1.6.6.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.6.6.1.m1.1c">1M</annotation></semantics></math> pretrain</td>
<td id="Sx3.T1.6.6.2" class="ltx_td ltx_align_center">0.682</td>
<td id="Sx3.T1.6.6.3" class="ltx_td ltx_align_center">2.085</td>
<td id="Sx3.T1.6.6.4" class="ltx_td ltx_align_center">1.039</td>
<td id="Sx3.T1.6.6.5" class="ltx_td ltx_align_center">1.008</td>
<td id="Sx3.T1.6.6.6" class="ltx_td ltx_align_center">2.969</td>
<td id="Sx3.T1.6.6.7" class="ltx_td ltx_align_center">1.623</td>
</tr>
<tr id="Sx3.T1.8.14" class="ltx_tr">
<td id="Sx3.T1.8.14.1" class="ltx_td ltx_align_left">Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a href="#bib.bib13" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="Sx3.T1.8.14.2" class="ltx_td ltx_align_center">0.581</td>
<td id="Sx3.T1.8.14.3" class="ltx_td ltx_align_center">1.341</td>
<td id="Sx3.T1.8.14.4" class="ltx_td ltx_align_center">0.899</td>
<td id="Sx3.T1.8.14.5" class="ltx_td ltx_align_center">0.783</td>
<td id="Sx3.T1.8.14.6" class="ltx_td ltx_align_center">2.992</td>
<td id="Sx3.T1.8.14.7" class="ltx_td ltx_align_center">1.432</td>
</tr>
<tr id="Sx3.T1.8.15" class="ltx_tr">
<td id="Sx3.T1.8.15.1" class="ltx_td ltx_align_left">Open Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(OpenLMLab <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx3.T1.8.15.2" class="ltx_td ltx_align_center">1.295</td>
<td id="Sx3.T1.8.15.3" class="ltx_td ltx_align_center">2.481</td>
<td id="Sx3.T1.8.15.4" class="ltx_td ltx_align_center">1.667</td>
<td id="Sx3.T1.8.15.5" class="ltx_td ltx_align_center">1.884</td>
<td id="Sx3.T1.8.15.6" class="ltx_td ltx_align_center">2.969</td>
<td id="Sx3.T1.8.15.7" class="ltx_td ltx_align_center">2.245</td>
</tr>
<tr id="Sx3.T1.8.16" class="ltx_tr">
<td id="Sx3.T1.8.16.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" rowspan="7"><span id="Sx3.T1.8.16.1.1" class="ltx_text">950k SFT</span></td>
<td id="Sx3.T1.8.16.2" class="ltx_td ltx_align_left ltx_border_tt">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a href="#bib.bib43" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="Sx3.T1.8.16.3" class="ltx_td ltx_align_center ltx_border_tt">1.783</td>
<td id="Sx3.T1.8.16.4" class="ltx_td ltx_align_center ltx_border_tt">2.767</td>
<td id="Sx3.T1.8.16.5" class="ltx_td ltx_align_center ltx_border_tt">2.142</td>
<td id="Sx3.T1.8.16.6" class="ltx_td ltx_align_center ltx_border_tt">2.212</td>
<td id="Sx3.T1.8.16.7" class="ltx_td ltx_align_center ltx_border_tt">2.993</td>
<td id="Sx3.T1.8.16.8" class="ltx_td ltx_align_center ltx_border_tt">2.379</td>
</tr>
<tr id="Sx3.T1.7.7" class="ltx_tr">
<td id="Sx3.T1.7.7.1" class="ltx_td ltx_align_left">LLaMA with <math id="Sx3.T1.7.7.1.m1.1" class="ltx_Math" alttext="1M" display="inline"><semantics id="Sx3.T1.7.7.1.m1.1a"><mrow id="Sx3.T1.7.7.1.m1.1.1" xref="Sx3.T1.7.7.1.m1.1.1.cmml"><mn id="Sx3.T1.7.7.1.m1.1.1.2" xref="Sx3.T1.7.7.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="Sx3.T1.7.7.1.m1.1.1.1" xref="Sx3.T1.7.7.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.T1.7.7.1.m1.1.1.3" xref="Sx3.T1.7.7.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.7.7.1.m1.1b"><apply id="Sx3.T1.7.7.1.m1.1.1.cmml" xref="Sx3.T1.7.7.1.m1.1.1"><times id="Sx3.T1.7.7.1.m1.1.1.1.cmml" xref="Sx3.T1.7.7.1.m1.1.1.1"></times><cn type="integer" id="Sx3.T1.7.7.1.m1.1.1.2.cmml" xref="Sx3.T1.7.7.1.m1.1.1.2">1</cn><ci id="Sx3.T1.7.7.1.m1.1.1.3.cmml" xref="Sx3.T1.7.7.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.7.7.1.m1.1c">1M</annotation></semantics></math> pretrain</td>
<td id="Sx3.T1.7.7.2" class="ltx_td ltx_align_center">1.812</td>
<td id="Sx3.T1.7.7.3" class="ltx_td ltx_align_center">2.799</td>
<td id="Sx3.T1.7.7.4" class="ltx_td ltx_align_center">2.080</td>
<td id="Sx3.T1.7.7.5" class="ltx_td ltx_align_center">2.303</td>
<td id="Sx3.T1.7.7.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx3.T1.7.7.7" class="ltx_td ltx_align_center">2.399</td>
</tr>
<tr id="Sx3.T1.8.8" class="ltx_tr">
<td id="Sx3.T1.8.8.1" class="ltx_td ltx_align_left">LLaMA-EXT with <math id="Sx3.T1.8.8.1.m1.1" class="ltx_Math" alttext="1M" display="inline"><semantics id="Sx3.T1.8.8.1.m1.1a"><mrow id="Sx3.T1.8.8.1.m1.1.1" xref="Sx3.T1.8.8.1.m1.1.1.cmml"><mn id="Sx3.T1.8.8.1.m1.1.1.2" xref="Sx3.T1.8.8.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="Sx3.T1.8.8.1.m1.1.1.1" xref="Sx3.T1.8.8.1.m1.1.1.1.cmml">​</mo><mi id="Sx3.T1.8.8.1.m1.1.1.3" xref="Sx3.T1.8.8.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.8.8.1.m1.1b"><apply id="Sx3.T1.8.8.1.m1.1.1.cmml" xref="Sx3.T1.8.8.1.m1.1.1"><times id="Sx3.T1.8.8.1.m1.1.1.1.cmml" xref="Sx3.T1.8.8.1.m1.1.1.1"></times><cn type="integer" id="Sx3.T1.8.8.1.m1.1.1.2.cmml" xref="Sx3.T1.8.8.1.m1.1.1.2">1</cn><ci id="Sx3.T1.8.8.1.m1.1.1.3.cmml" xref="Sx3.T1.8.8.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.8.8.1.m1.1c">1M</annotation></semantics></math> pretrain</td>
<td id="Sx3.T1.8.8.2" class="ltx_td ltx_align_center">1.591</td>
<td id="Sx3.T1.8.8.3" class="ltx_td ltx_align_center">2.726</td>
<td id="Sx3.T1.8.8.4" class="ltx_td ltx_align_center">1.918</td>
<td id="Sx3.T1.8.8.5" class="ltx_td ltx_align_center">2.164</td>
<td id="Sx3.T1.8.8.6" class="ltx_td ltx_align_center">2.998</td>
<td id="Sx3.T1.8.8.7" class="ltx_td ltx_align_center">2.279</td>
</tr>
<tr id="Sx3.T1.8.17" class="ltx_tr">
<td id="Sx3.T1.8.17.1" class="ltx_td ltx_align_left">Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a href="#bib.bib13" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="Sx3.T1.8.17.2" class="ltx_td ltx_align_center">1.808</td>
<td id="Sx3.T1.8.17.3" class="ltx_td ltx_align_center">2.795</td>
<td id="Sx3.T1.8.17.4" class="ltx_td ltx_align_center">2.112</td>
<td id="Sx3.T1.8.17.5" class="ltx_td ltx_align_center">2.313</td>
<td id="Sx3.T1.8.17.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx3.T1.8.17.7" class="ltx_td ltx_align_center">2.406</td>
</tr>
<tr id="Sx3.T1.8.18" class="ltx_tr">
<td id="Sx3.T1.8.18.1" class="ltx_td ltx_align_left">Open Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(OpenLMLab <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="Sx3.T1.8.18.2" class="ltx_td ltx_align_center">1.890</td>
<td id="Sx3.T1.8.18.3" class="ltx_td ltx_align_center">2.858</td>
<td id="Sx3.T1.8.18.4" class="ltx_td ltx_align_center">2.189</td>
<td id="Sx3.T1.8.18.5" class="ltx_td ltx_align_center">2.390</td>
<td id="Sx3.T1.8.18.6" class="ltx_td ltx_align_center">2.993</td>
<td id="Sx3.T1.8.18.7" class="ltx_td ltx_align_center">2.464</td>
</tr>
<tr id="Sx3.T1.8.19" class="ltx_tr">
<td id="Sx3.T1.8.19.1" class="ltx_td ltx_align_left">LLaMA2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a href="#bib.bib44" title="" class="ltx_ref">2023b</a>)</cite>
</td>
<td id="Sx3.T1.8.19.2" class="ltx_td ltx_align_center">1.868</td>
<td id="Sx3.T1.8.19.3" class="ltx_td ltx_align_center">2.822</td>
<td id="Sx3.T1.8.19.4" class="ltx_td ltx_align_center">2.171</td>
<td id="Sx3.T1.8.19.5" class="ltx_td ltx_align_center">2.379</td>
<td id="Sx3.T1.8.19.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx3.T1.8.19.7" class="ltx_td ltx_align_center">2.448</td>
</tr>
<tr id="Sx3.T1.8.20" class="ltx_tr">
<td id="Sx3.T1.8.20.1" class="ltx_td ltx_align_left ltx_border_bb">Chinese LLaMA2 <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a href="#bib.bib12" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="Sx3.T1.8.20.2" class="ltx_td ltx_align_center ltx_border_bb">1.701</td>
<td id="Sx3.T1.8.20.3" class="ltx_td ltx_align_center ltx_border_bb">2.838</td>
<td id="Sx3.T1.8.20.4" class="ltx_td ltx_align_center ltx_border_bb">2.011</td>
<td id="Sx3.T1.8.20.5" class="ltx_td ltx_align_center ltx_border_bb">2.251</td>
<td id="Sx3.T1.8.20.6" class="ltx_td ltx_align_center ltx_border_bb">3.000</td>
<td id="Sx3.T1.8.20.7" class="ltx_td ltx_align_center ltx_border_bb">2.360</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Response quality with different scales of further pretraining and instruction tuning (SFT). ACC., F., LC., H., INFO., and AVG. respectively denote accuracy, fluency, logical coherence, harmlessness, informativeness and their average. Approximately 1 million samples account for around 0.5 billion tokens. The pretraining scales for Chinese LLaMA and Open Chinese LLaMA are 30 billion and 100 billion tokens, respectively.</figcaption>
</figure>
<div id="Sx3.SSx2.p4" class="ltx_para">
<p id="Sx3.SSx2.p4.1" class="ltx_p">In order to objectively and comprehensively assess the capabilities of the model, we conduct evaluations from two perspectives: response quality and knowledge level. For the former, we employ the LLM-Eval benchmark and translate it into various low-resource languages to support multi-lingual evaluation. As for the latter, we utilize four widely adopted standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench.</p>
</div>
<div id="Sx3.SSx2.p5" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p5.1" class="ltx_p"><span id="Sx3.SSx2.p5.1.1" class="ltx_text ltx_font_bold">LLM-Eval</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al. <a href="#bib.bib51" title="" class="ltx_ref">2023a</a>)</cite>: LLM-Eval is a manually constructed benchmark for instruction-following evaluation. It has 453 instruction tasks from 17 major categories, including factual question answering, reading comprehension, frame generation, paragraph rewriting, summarizing, math problem solving, reasoning, poetry generation, programming, and more.</p>
</div>
<div id="Sx3.SSx2.p6" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p6.1" class="ltx_p"><span id="Sx3.SSx2.p6.1.1" class="ltx_text ltx_font_bold">C-Eval</span> <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al. <a href="#bib.bib22" title="" class="ltx_ref">2023b</a>)</cite>: C-Eval is a Chinese evaluation suite with 13948 exam questions across 52 subjects and 4 difficulty levels from middle school to professional exams. It includes STEM, humanities, social science and other topics. C-Eval HARD is a subset of 8 challenging math and science subjects requiring advanced reasoning.</p>
</div>
<div id="Sx3.SSx2.p7" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p7.1" class="ltx_p"><span id="Sx3.SSx2.p7.1.1" class="ltx_text ltx_font_bold">MMLU</span> <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al. <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>: MMLU measures a LLM’s ability to learn and apply knowledge across 57 diverse subjects including STEM, humanities, and social sciences. The test covers a wide range of difficulty levels from elementary to advanced professional.</p>
</div>
<div id="Sx3.SSx2.p8" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p8.1" class="ltx_p"><span id="Sx3.SSx2.p8.1.1" class="ltx_text ltx_font_bold">AGI-Eval</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhong et&nbsp;al. <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>: AGIEval uses questions from standardized tests taken by millions of people, including college entrance exams, law school admission tests, and professional qualification exams. It has 19 tasks in both English and Chinese.</p>
</div>
<div id="Sx3.SSx2.p9" class="ltx_para ltx_noindent">
<p id="Sx3.SSx2.p9.1" class="ltx_p"><span id="Sx3.SSx2.p9.1.1" class="ltx_text ltx_font_bold">Gaokao-Bench</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al. <a href="#bib.bib52" title="" class="ltx_ref">2023b</a>)</cite>: GAOKAO-Bench uses 2811 exam questions from Chinese college entrance exams (Gaokao) from 2010-2022 covering all subjects. It has 1781 multiple choice, 218 fill-in-blank, and 812 open-ended questions across math, Chinese, English, physics, etc.</p>
</div>
</section>
<section id="Sx3.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Evaluation Protocol</h3>

<div id="Sx3.SSx3.p1" class="ltx_para">
<p id="Sx3.SSx3.p1.1" class="ltx_p">For LLM-Eval, we followed the practice of <cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. (<a href="#bib.bib51" title="" class="ltx_ref">2023a</a>)</cite>, evaluating the response quality of a model through 5 scoring items: accuracy, fluency, informativeness, logicality, and harmlessness. Scores for each aspect range from 0 to 3. We use the prompt shown in Appendix to submit the instruction, model response, and reference answer to GPT-4 for automated evaluation. Based on the results reported by <cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. (<a href="#bib.bib51" title="" class="ltx_ref">2023a</a>)</cite>, this evaluation method demonstrates a high degree of consistency with human evaluation.</p>
</div>
<div id="Sx3.SSx3.p2" class="ltx_para">
<p id="Sx3.SSx3.p2.1" class="ltx_p">For the four standardized testing benchmarks, we calculate the accuracy metric for model responses. Additionally, we follow the common practice of employing a zero-shot setting for AGI-Eval and GAOKAO-Bench, while using a 5-shot setting for C-Eval and MMLU.</p>
</div>
<figure id="Sx3.F2" class="ltx_figure"><img src="/html/2401.01055/assets/x2.png" id="Sx3.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="184" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Knowledge-level evaluation results on four benchmarks.</figcaption>
</figure>
</section>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Main Results</h2>

<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">The Impact of Vocabulary Extension on Transfer</h3>

<div id="Sx4.SSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.p1.1" class="ltx_p">When we aim to enhance the capabilities of a LLM in a specific language, vocabulary extension is an intuitively reasonable approach. In this section, we evaluate the impact of vocabulary extension through the LLM-Eval benchmark, and the experimental results are presented in table <a href="#Sx3.T1" title="Table 1 ‣ Datasets ‣ Experimental Setup ‣ LLaMA Beyond English: An Empirical Study on Language Capability Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Initially, we collected one million Chinese sentences from the internet (approximately 0.5 billion tokens) and further pretrain the original LLaMA without vocabulary extension. Surprisingly, we find that this model significantly ourperform the vocabulary-extended Chinese LLaMA, across settings of 1K, 5K, and 950K instruction tuning. This discovery is thought-privoking, given that the Chinese LLaMA underwent further Chinese pretraining on 30 billion tokens, a much larger volume than our 0.5 billion tokens. Moreover, within the 950K setting, we include results from extending the vocabulary on original LLaMA and training it with the same 0.5 billion tokens, to mitigate the influence of training data discrepancy. The outcomes remain consistent. This indicates that vocabulary extension is not a favorable choice within training scales of tens of billions of tokens. While we don’t negate the efficacy of vocabulary extension in settings involving larger-scale pre-training (such as trillions of tokens), as reported in other literatures <cite class="ltx_cite ltx_citemacro_citep">(Team <a href="#bib.bib42" title="" class="ltx_ref">2023b</a>)</cite>, this already leans more towards retraining than mere language transfer.</p>
</div>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Training Scales Required for Effective Transfer</h3>

<div id="Sx4.SSx2.p1" class="ltx_para">
<p id="Sx4.SSx2.p1.1" class="ltx_p">Training scale constitutes another significant factor influencing the transferability of LLM capabilities, composed of both pretraining scale and instruction tuning scale. Experimental results are shown in table <a href="#Sx3.T1" title="Table 1 ‣ Datasets ‣ Experimental Setup ‣ LLaMA Beyond English: An Empirical Study on Language Capability Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Taking the example of LLaMA (with 10K, 100K, and 1M further pretrain) and Open Chinese LLaMA, the scale of further Chinese pretraining gradually increases from 0 to 100 billion tokens. Under the settings of 1K and 5K instruction tuning, we observed that the response quality improves progressively with the increase in the scale of further pretraining. <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Chinese-LLaMA, however, stands as an exception due to the additional factor of vocabulary extension.</span></span></span> However, when the instruction tuning data scale escalates to 950K, we find no significant differences in response quality among the models. Consequently, we hypothesize that more further pretraining could accelerate the model’s alignment with human instructions, but the mere tens of billions in training scale are insufficient to enable the model to grasp a greater amount of world knowledge. This leads to their convergence at similar response levels. In other words, the enhancement in response quality primarily stems from an improvement in language generation prowess rather than an elevation in knowledge level.</p>
</div>
<div id="Sx4.SSx2.p2" class="ltx_para">
<p id="Sx4.SSx2.p2.1" class="ltx_p">To validate this standpoint, we evaluated the model’s knowledge level on four widely used standardized test benchmarks. As shown in Figure <a href="#Sx3.F2" title="Figure 2 ‣ Evaluation Protocol ‣ Experimental Setup ‣ LLaMA Beyond English: An Empirical Study on Language Capability Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, LLaMA 7B, Chinese LLaMA 7B, and Open Chinese LLaMA 7B perform comparably on C-eval, gaokao-bench, and agi-eval, indicating no significant differences induced by further Chinese pretraining. It is worth noting that despite lacking further pretraining in Chinese, both LLaMA2-7B and LLaMA-13B outperform Open Chinese LLaMA on C-eval, MMLU, and AGI-Eval, suggesting that trillion-level pretraining and larger model sizes may indeed serve as effective pathways for enhancing model knowledge levels.</p>
</div>
</section>
<section id="Sx4.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">How about the Original English Capabilities</h3>

<figure id="Sx4.T2" class="ltx_table">
<div id="Sx4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:88.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(84.5pt,-17.3pt) scale(1.63904767035265,1.63904767035265) ;">
<table id="Sx4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="Sx4.T2.1.1.1" class="ltx_tr">
<td id="Sx4.T2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">L(0)</td>
<td id="Sx4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">L(10k)</td>
<td id="Sx4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">L(100k)</td>
<td id="Sx4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">L(1M)</td>
<td id="Sx4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Open</td>
</tr>
<tr id="Sx4.T2.1.1.2" class="ltx_tr">
<td id="Sx4.T2.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="Sx4.T2.1.1.2.1.1" class="ltx_text ltx_font_bold">Chinese</span></td>
<td id="Sx4.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">10.151</td>
<td id="Sx4.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">8.697</td>
<td id="Sx4.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">6.634</td>
<td id="Sx4.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">5.249</td>
<td id="Sx4.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">3.924</td>
</tr>
<tr id="Sx4.T2.1.1.3" class="ltx_tr">
<td id="Sx4.T2.1.1.3.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="Sx4.T2.1.1.3.1.1" class="ltx_text ltx_font_bold">English</span></td>
<td id="Sx4.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_border_bb">14.691</td>
<td id="Sx4.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_border_bb">15.625</td>
<td id="Sx4.T2.1.1.3.4" class="ltx_td ltx_align_center ltx_border_bb">29.553</td>
<td id="Sx4.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_bb">198.840</td>
<td id="Sx4.T2.1.1.3.6" class="ltx_td ltx_align_center ltx_border_bb">15.045</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Model perplexity with different further pretraining scales. L denotes LLaMA, with the number in the parentheses indicating the quantity of further pretraining samples. Open denotes Open Chinese LLaMA.</figcaption>
</figure>
<div id="Sx4.SSx3.p1" class="ltx_para">
<p id="Sx4.SSx3.p1.1" class="ltx_p">Another issue of interest to us is whether the improvement in Chinese proficiency has an impact on the existing English capabilities. To address this question, we additionally collected 200,000 Chinese samples from the internet and randomly extracted 200,000 English samples from the refinedweb dataset <cite class="ltx_cite ltx_citemacro_citep">(Penedo et&nbsp;al. <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>. Utilizing these samples, we evaluate the English perplexity and Chinese perplexity of LLaMA models trained on different-scale corpora, as depicted in table <a href="#Sx4.T2" title="Table 2 ‣ How about the Original English Capabilities ‣ Main Results ‣ LLaMA Beyond English: An Empirical Study on Language Capability Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our findings reveal that with the increase in further pretraining scale, the perplexity of the models decreases steadily in Chinese, yet notably increases in English. This suggests that enhancing the model’s capabilities solely through a single Chinese corpus comes at the cost of sacrificing the original English proficiency.</p>
</div>
<div id="Sx4.SSx3.p2" class="ltx_para">
<p id="Sx4.SSx3.p2.1" class="ltx_p">Furthermore, we conduct perplexity assessments for Open Chinese LLaMA and find that both the Chinese and English perplexities remain low. This outcome is unsurprising, given that its training data incorporates both Chinese and English content, allowing for the decreases of Chinese perplexity without significant elevation in English perplexity. Overall, exclusive reliance on Chinese corpora for transfer training markedly compromises LLaMA’s original English proficiency, a concern alleviated effectively through multilingual joint training.</p>
</div>
<figure id="Sx4.T3" class="ltx_table">
<table id="Sx4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody><tr id="Sx4.T3.1.1" class="ltx_tr">
<td id="Sx4.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="Sx4.T3.1.1.1.1" class="ltx_text ltx_font_bold">Language</span></td>
<td id="Sx4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="6">1k SFT</td>
<td id="Sx4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="6">65k SFT</td>
</tr>
<tr id="Sx4.T3.1.2" class="ltx_tr">
<td id="Sx4.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t">ACC.</td>
<td id="Sx4.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">F.</td>
<td id="Sx4.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">INFO.</td>
<td id="Sx4.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">LC.</td>
<td id="Sx4.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_t">H.</td>
<td id="Sx4.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_t">AVG.</td>
<td id="Sx4.T3.1.2.7" class="ltx_td ltx_align_center ltx_border_t">ACC.</td>
<td id="Sx4.T3.1.2.8" class="ltx_td ltx_align_center ltx_border_t">F.</td>
<td id="Sx4.T3.1.2.9" class="ltx_td ltx_align_center ltx_border_t">INFO.</td>
<td id="Sx4.T3.1.2.10" class="ltx_td ltx_align_center ltx_border_t">LC.</td>
<td id="Sx4.T3.1.2.11" class="ltx_td ltx_align_center ltx_border_t">H.</td>
<td id="Sx4.T3.1.2.12" class="ltx_td ltx_align_center ltx_border_t">AVG.</td>
</tr>
<tr id="Sx4.T3.1.3" class="ltx_tr">
<td id="Sx4.T3.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Arbic</td>
<td id="Sx4.T3.1.3.2" class="ltx_td ltx_align_center ltx_border_t">0.188</td>
<td id="Sx4.T3.1.3.3" class="ltx_td ltx_align_center ltx_border_t">1.061</td>
<td id="Sx4.T3.1.3.4" class="ltx_td ltx_align_center ltx_border_t">0.191</td>
<td id="Sx4.T3.1.3.5" class="ltx_td ltx_align_center ltx_border_t">0.254</td>
<td id="Sx4.T3.1.3.6" class="ltx_td ltx_align_center ltx_border_t">3.000</td>
<td id="Sx4.T3.1.3.7" class="ltx_td ltx_align_center ltx_border_t">0.939</td>
<td id="Sx4.T3.1.3.8" class="ltx_td ltx_align_center ltx_border_t">1.268</td>
<td id="Sx4.T3.1.3.9" class="ltx_td ltx_align_center ltx_border_t">2.499</td>
<td id="Sx4.T3.1.3.10" class="ltx_td ltx_align_center ltx_border_t">1.529</td>
<td id="Sx4.T3.1.3.11" class="ltx_td ltx_align_center ltx_border_t">1.607</td>
<td id="Sx4.T3.1.3.12" class="ltx_td ltx_align_center ltx_border_t">3.000</td>
<td id="Sx4.T3.1.3.13" class="ltx_td ltx_align_center ltx_border_t">1.981</td>
</tr>
<tr id="Sx4.T3.1.4" class="ltx_tr">
<td id="Sx4.T3.1.4.1" class="ltx_td ltx_align_left">Bengali</td>
<td id="Sx4.T3.1.4.2" class="ltx_td ltx_align_center">0.046</td>
<td id="Sx4.T3.1.4.3" class="ltx_td ltx_align_center">0.492</td>
<td id="Sx4.T3.1.4.4" class="ltx_td ltx_align_center">0.050</td>
<td id="Sx4.T3.1.4.5" class="ltx_td ltx_align_center">0.041</td>
<td id="Sx4.T3.1.4.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.4.7" class="ltx_td ltx_align_center">0.726</td>
<td id="Sx4.T3.1.4.8" class="ltx_td ltx_align_center">0.959</td>
<td id="Sx4.T3.1.4.9" class="ltx_td ltx_align_center">2.257</td>
<td id="Sx4.T3.1.4.10" class="ltx_td ltx_align_center">1.156</td>
<td id="Sx4.T3.1.4.11" class="ltx_td ltx_align_center">1.189</td>
<td id="Sx4.T3.1.4.12" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.4.13" class="ltx_td ltx_align_center">1.712</td>
</tr>
<tr id="Sx4.T3.1.5" class="ltx_tr">
<td id="Sx4.T3.1.5.1" class="ltx_td ltx_align_left">Gujarati</td>
<td id="Sx4.T3.1.5.2" class="ltx_td ltx_align_center">0.061</td>
<td id="Sx4.T3.1.5.3" class="ltx_td ltx_align_center">0.426</td>
<td id="Sx4.T3.1.5.4" class="ltx_td ltx_align_center">0.052</td>
<td id="Sx4.T3.1.5.5" class="ltx_td ltx_align_center">0.063</td>
<td id="Sx4.T3.1.5.6" class="ltx_td ltx_align_center">2.998</td>
<td id="Sx4.T3.1.5.7" class="ltx_td ltx_align_center">0.720</td>
<td id="Sx4.T3.1.5.8" class="ltx_td ltx_align_center">0.683</td>
<td id="Sx4.T3.1.5.9" class="ltx_td ltx_align_center">1.795</td>
<td id="Sx4.T3.1.5.10" class="ltx_td ltx_align_center">0.875</td>
<td id="Sx4.T3.1.5.11" class="ltx_td ltx_align_center">0.790</td>
<td id="Sx4.T3.1.5.12" class="ltx_td ltx_align_center">2.995</td>
<td id="Sx4.T3.1.5.13" class="ltx_td ltx_align_center">1.428</td>
</tr>
<tr id="Sx4.T3.1.6" class="ltx_tr">
<td id="Sx4.T3.1.6.1" class="ltx_td ltx_align_left">Hindi</td>
<td id="Sx4.T3.1.6.2" class="ltx_td ltx_align_center">0.131</td>
<td id="Sx4.T3.1.6.3" class="ltx_td ltx_align_center">1.064</td>
<td id="Sx4.T3.1.6.4" class="ltx_td ltx_align_center">0.147</td>
<td id="Sx4.T3.1.6.5" class="ltx_td ltx_align_center">0.162</td>
<td id="Sx4.T3.1.6.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.6.7" class="ltx_td ltx_align_center">0.901</td>
<td id="Sx4.T3.1.6.8" class="ltx_td ltx_align_center">1.014</td>
<td id="Sx4.T3.1.6.9" class="ltx_td ltx_align_center">2.342</td>
<td id="Sx4.T3.1.6.10" class="ltx_td ltx_align_center">1.238</td>
<td id="Sx4.T3.1.6.11" class="ltx_td ltx_align_center">1.240</td>
<td id="Sx4.T3.1.6.12" class="ltx_td ltx_align_center">2.998</td>
<td id="Sx4.T3.1.6.13" class="ltx_td ltx_align_center">1.766</td>
</tr>
<tr id="Sx4.T3.1.7" class="ltx_tr">
<td id="Sx4.T3.1.7.1" class="ltx_td ltx_align_left">Indonesian</td>
<td id="Sx4.T3.1.7.2" class="ltx_td ltx_align_center">0.398</td>
<td id="Sx4.T3.1.7.3" class="ltx_td ltx_align_center">1.266</td>
<td id="Sx4.T3.1.7.4" class="ltx_td ltx_align_center">0.544</td>
<td id="Sx4.T3.1.7.5" class="ltx_td ltx_align_center">0.438</td>
<td id="Sx4.T3.1.7.6" class="ltx_td ltx_align_center">2.995</td>
<td id="Sx4.T3.1.7.7" class="ltx_td ltx_align_center">1.128</td>
<td id="Sx4.T3.1.7.8" class="ltx_td ltx_align_center">1.659</td>
<td id="Sx4.T3.1.7.9" class="ltx_td ltx_align_center">2.751</td>
<td id="Sx4.T3.1.7.10" class="ltx_td ltx_align_center">2.026</td>
<td id="Sx4.T3.1.7.11" class="ltx_td ltx_align_center">2.012</td>
<td id="Sx4.T3.1.7.12" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.7.13" class="ltx_td ltx_align_center">2.290</td>
</tr>
<tr id="Sx4.T3.1.8" class="ltx_tr">
<td id="Sx4.T3.1.8.1" class="ltx_td ltx_align_left">Malayalam</td>
<td id="Sx4.T3.1.8.2" class="ltx_td ltx_align_center">0.101</td>
<td id="Sx4.T3.1.8.3" class="ltx_td ltx_align_center">0.621</td>
<td id="Sx4.T3.1.8.4" class="ltx_td ltx_align_center">0.103</td>
<td id="Sx4.T3.1.8.5" class="ltx_td ltx_align_center">0.103</td>
<td id="Sx4.T3.1.8.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.8.7" class="ltx_td ltx_align_center">0.786</td>
<td id="Sx4.T3.1.8.8" class="ltx_td ltx_align_center">0.906</td>
<td id="Sx4.T3.1.8.9" class="ltx_td ltx_align_center">2.427</td>
<td id="Sx4.T3.1.8.10" class="ltx_td ltx_align_center">1.182</td>
<td id="Sx4.T3.1.8.11" class="ltx_td ltx_align_center">1.197</td>
<td id="Sx4.T3.1.8.12" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.8.13" class="ltx_td ltx_align_center">1.742</td>
</tr>
<tr id="Sx4.T3.1.9" class="ltx_tr">
<td id="Sx4.T3.1.9.1" class="ltx_td ltx_align_left">Marathi</td>
<td id="Sx4.T3.1.9.2" class="ltx_td ltx_align_center">0.095</td>
<td id="Sx4.T3.1.9.3" class="ltx_td ltx_align_center">0.781</td>
<td id="Sx4.T3.1.9.4" class="ltx_td ltx_align_center">0.107</td>
<td id="Sx4.T3.1.9.5" class="ltx_td ltx_align_center">0.117</td>
<td id="Sx4.T3.1.9.6" class="ltx_td ltx_align_center">2.998</td>
<td id="Sx4.T3.1.9.7" class="ltx_td ltx_align_center">0.820</td>
<td id="Sx4.T3.1.9.8" class="ltx_td ltx_align_center">1.038</td>
<td id="Sx4.T3.1.9.9" class="ltx_td ltx_align_center">2.476</td>
<td id="Sx4.T3.1.9.10" class="ltx_td ltx_align_center">1.288</td>
<td id="Sx4.T3.1.9.11" class="ltx_td ltx_align_center">1.364</td>
<td id="Sx4.T3.1.9.12" class="ltx_td ltx_align_center">2.998</td>
<td id="Sx4.T3.1.9.13" class="ltx_td ltx_align_center">1.833</td>
</tr>
<tr id="Sx4.T3.1.10" class="ltx_tr">
<td id="Sx4.T3.1.10.1" class="ltx_td ltx_align_left">Nepali</td>
<td id="Sx4.T3.1.10.2" class="ltx_td ltx_align_center">0.151</td>
<td id="Sx4.T3.1.10.3" class="ltx_td ltx_align_center">0.991</td>
<td id="Sx4.T3.1.10.4" class="ltx_td ltx_align_center">0.177</td>
<td id="Sx4.T3.1.10.5" class="ltx_td ltx_align_center">0.146</td>
<td id="Sx4.T3.1.10.6" class="ltx_td ltx_align_center">2.986</td>
<td id="Sx4.T3.1.10.7" class="ltx_td ltx_align_center">0.890</td>
<td id="Sx4.T3.1.10.8" class="ltx_td ltx_align_center">0.969</td>
<td id="Sx4.T3.1.10.9" class="ltx_td ltx_align_center">2.417</td>
<td id="Sx4.T3.1.10.10" class="ltx_td ltx_align_center">1.236</td>
<td id="Sx4.T3.1.10.11" class="ltx_td ltx_align_center">1.285</td>
<td id="Sx4.T3.1.10.12" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.10.13" class="ltx_td ltx_align_center">1.781</td>
</tr>
<tr id="Sx4.T3.1.11" class="ltx_tr">
<td id="Sx4.T3.1.11.1" class="ltx_td ltx_align_left">Swahili</td>
<td id="Sx4.T3.1.11.2" class="ltx_td ltx_align_center">0.083</td>
<td id="Sx4.T3.1.11.3" class="ltx_td ltx_align_center">0.712</td>
<td id="Sx4.T3.1.11.4" class="ltx_td ltx_align_center">0.090</td>
<td id="Sx4.T3.1.11.5" class="ltx_td ltx_align_center">0.086</td>
<td id="Sx4.T3.1.11.6" class="ltx_td ltx_align_center">2.998</td>
<td id="Sx4.T3.1.11.7" class="ltx_td ltx_align_center">0.794</td>
<td id="Sx4.T3.1.11.8" class="ltx_td ltx_align_center">1.569</td>
<td id="Sx4.T3.1.11.9" class="ltx_td ltx_align_center">2.707</td>
<td id="Sx4.T3.1.11.10" class="ltx_td ltx_align_center">1.955</td>
<td id="Sx4.T3.1.11.11" class="ltx_td ltx_align_center">1.907</td>
<td id="Sx4.T3.1.11.12" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.11.13" class="ltx_td ltx_align_center">2.228</td>
</tr>
<tr id="Sx4.T3.1.12" class="ltx_tr">
<td id="Sx4.T3.1.12.1" class="ltx_td ltx_align_left">Tamil</td>
<td id="Sx4.T3.1.12.2" class="ltx_td ltx_align_center">0.140</td>
<td id="Sx4.T3.1.12.3" class="ltx_td ltx_align_center">0.914</td>
<td id="Sx4.T3.1.12.4" class="ltx_td ltx_align_center">0.176</td>
<td id="Sx4.T3.1.12.5" class="ltx_td ltx_align_center">0.174</td>
<td id="Sx4.T3.1.12.6" class="ltx_td ltx_align_center">2.998</td>
<td id="Sx4.T3.1.12.7" class="ltx_td ltx_align_center">0.880</td>
<td id="Sx4.T3.1.12.8" class="ltx_td ltx_align_center">0.960</td>
<td id="Sx4.T3.1.12.9" class="ltx_td ltx_align_center">2.457</td>
<td id="Sx4.T3.1.12.10" class="ltx_td ltx_align_center">1.198</td>
<td id="Sx4.T3.1.12.11" class="ltx_td ltx_align_center">1.257</td>
<td id="Sx4.T3.1.12.12" class="ltx_td ltx_align_center">2.998</td>
<td id="Sx4.T3.1.12.13" class="ltx_td ltx_align_center">1.774</td>
</tr>
<tr id="Sx4.T3.1.13" class="ltx_tr">
<td id="Sx4.T3.1.13.1" class="ltx_td ltx_align_left">Telugu</td>
<td id="Sx4.T3.1.13.2" class="ltx_td ltx_align_center">0.054</td>
<td id="Sx4.T3.1.13.3" class="ltx_td ltx_align_center">0.560</td>
<td id="Sx4.T3.1.13.4" class="ltx_td ltx_align_center">0.057</td>
<td id="Sx4.T3.1.13.5" class="ltx_td ltx_align_center">0.090</td>
<td id="Sx4.T3.1.13.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.13.7" class="ltx_td ltx_align_center">0.752</td>
<td id="Sx4.T3.1.13.8" class="ltx_td ltx_align_center">0.539</td>
<td id="Sx4.T3.1.13.9" class="ltx_td ltx_align_center">1.735</td>
<td id="Sx4.T3.1.13.10" class="ltx_td ltx_align_center">0.674</td>
<td id="Sx4.T3.1.13.11" class="ltx_td ltx_align_center">0.712</td>
<td id="Sx4.T3.1.13.12" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.13.13" class="ltx_td ltx_align_center">1.332</td>
</tr>
<tr id="Sx4.T3.1.14" class="ltx_tr">
<td id="Sx4.T3.1.14.1" class="ltx_td ltx_align_left">Urdu</td>
<td id="Sx4.T3.1.14.2" class="ltx_td ltx_align_center">0.057</td>
<td id="Sx4.T3.1.14.3" class="ltx_td ltx_align_center">0.573</td>
<td id="Sx4.T3.1.14.4" class="ltx_td ltx_align_center">0.052</td>
<td id="Sx4.T3.1.14.5" class="ltx_td ltx_align_center">0.071</td>
<td id="Sx4.T3.1.14.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.14.7" class="ltx_td ltx_align_center">0.751</td>
<td id="Sx4.T3.1.14.8" class="ltx_td ltx_align_center">1.038</td>
<td id="Sx4.T3.1.14.9" class="ltx_td ltx_align_center">2.443</td>
<td id="Sx4.T3.1.14.10" class="ltx_td ltx_align_center">1.285</td>
<td id="Sx4.T3.1.14.11" class="ltx_td ltx_align_center">1.335</td>
<td id="Sx4.T3.1.14.12" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.14.13" class="ltx_td ltx_align_center">1.820</td>
</tr>
<tr id="Sx4.T3.1.15" class="ltx_tr">
<td id="Sx4.T3.1.15.1" class="ltx_td ltx_align_left">Vietnamese</td>
<td id="Sx4.T3.1.15.2" class="ltx_td ltx_align_center">0.105</td>
<td id="Sx4.T3.1.15.3" class="ltx_td ltx_align_center">0.623</td>
<td id="Sx4.T3.1.15.4" class="ltx_td ltx_align_center">0.126</td>
<td id="Sx4.T3.1.15.5" class="ltx_td ltx_align_center">0.117</td>
<td id="Sx4.T3.1.15.6" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.15.7" class="ltx_td ltx_align_center">0.794</td>
<td id="Sx4.T3.1.15.8" class="ltx_td ltx_align_center">1.361</td>
<td id="Sx4.T3.1.15.9" class="ltx_td ltx_align_center">2.595</td>
<td id="Sx4.T3.1.15.10" class="ltx_td ltx_align_center">1.665</td>
<td id="Sx4.T3.1.15.11" class="ltx_td ltx_align_center">1.710</td>
<td id="Sx4.T3.1.15.12" class="ltx_td ltx_align_center">3.000</td>
<td id="Sx4.T3.1.15.13" class="ltx_td ltx_align_center">2.066</td>
</tr>
<tr id="Sx4.T3.1.16" class="ltx_tr">
<td id="Sx4.T3.1.16.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt">Average</td>
<td id="Sx4.T3.1.16.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">0.124</td>
<td id="Sx4.T3.1.16.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">0.776</td>
<td id="Sx4.T3.1.16.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">0.144</td>
<td id="Sx4.T3.1.16.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">0.143</td>
<td id="Sx4.T3.1.16.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">2.998</td>
<td id="Sx4.T3.1.16.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">0.837</td>
<td id="Sx4.T3.1.16.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">1.074</td>
<td id="Sx4.T3.1.16.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">2.377</td>
<td id="Sx4.T3.1.16.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">1.331</td>
<td id="Sx4.T3.1.16.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">1.354</td>
<td id="Sx4.T3.1.16.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">2.999</td>
<td id="Sx4.T3.1.16.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt">1.827</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation results of model response quality for 13 low-resource languages on the LLM-Eval. ACC., F., LC., H., INFO., and AVG. respectively denote accuracy, fluency, logical coherence, harmlessness, informativeness and their average.</figcaption>
</figure>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Extending the Analysis to Multiple Languages</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">In the previous section, our experiments focus on Chinese. To investigate whether similar conclusions could be drawn in other non-English languages, we extend our experiments to 13 low-resource languages. To ensure evaluation consistency, we translate LLM-Eval benchmark into these 13 languages and employ the same evaluation metrics. As shown in table <a href="#Sx4.T3" title="Table 3 ‣ How about the Original English Capabilities ‣ Main Results ‣ LLaMA Beyond English: An Empirical Study on Language Capability Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, a significant improvement in response quality for all low-resource languages with the increase in SFT data. Among these languages, Arabic, Indonesian, and Vietnamese exhibited the best performance. Despite all thirteen languages being low-resource, these three languages are more frequently used <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al. <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. As a result, LLaMA encounters them more often (although their overall occurrence is small compared to English), allowing the model to quickly comprehend instructions in these languages. This aligns with the conclusion drawn in the previous section.</p>
</div>
<div id="Sx5.p2" class="ltx_para">
<p id="Sx5.p2.2" class="ltx_p">In the previous section, we observed that extending the vocabulary had a negative impact on language transferability. A plausible hypothesis is the existence of cross-lingual semantic alignment within LLMs, which vocabulary expansion might disrupt. To validate this alignment hypothesis, we fine-tune LLaMA with a dataset of 1k instructions and examine the model’s output. Excitingly, we observed a certain proportion of code-switching samples. As depicted in figure <a href="#Sx5.F3" title="Figure 3 ‣ Extending the Analysis to Multiple Languages ‣ LLaMA Beyond English: An Empirical Study on Language Capability Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, these samples’ model responses consist of tokens from multiple languages and are semantically coherent. We have observed that code-switching occurs not only in the transfer process when Chinese is the target language, but also when other 13 low-resource languages are target languages. As shown in figure <a href="#Sx5.F4" title="Figure 4 ‣ Extending the Analysis to Multiple Languages ‣ LLaMA Beyond English: An Empirical Study on Language Capability Transfer" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the proportion of samples with code-switching is approximately between <math id="Sx5.p2.1.m1.1" class="ltx_Math" alttext="2\%" display="inline"><semantics id="Sx5.p2.1.m1.1a"><mrow id="Sx5.p2.1.m1.1.1" xref="Sx5.p2.1.m1.1.1.cmml"><mn id="Sx5.p2.1.m1.1.1.2" xref="Sx5.p2.1.m1.1.1.2.cmml">2</mn><mo id="Sx5.p2.1.m1.1.1.1" xref="Sx5.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.p2.1.m1.1b"><apply id="Sx5.p2.1.m1.1.1.cmml" xref="Sx5.p2.1.m1.1.1"><csymbol cd="latexml" id="Sx5.p2.1.m1.1.1.1.cmml" xref="Sx5.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="Sx5.p2.1.m1.1.1.2.cmml" xref="Sx5.p2.1.m1.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.p2.1.m1.1c">2\%</annotation></semantics></math> to <math id="Sx5.p2.2.m2.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="Sx5.p2.2.m2.1a"><mrow id="Sx5.p2.2.m2.1.1" xref="Sx5.p2.2.m2.1.1.cmml"><mn id="Sx5.p2.2.m2.1.1.2" xref="Sx5.p2.2.m2.1.1.2.cmml">5</mn><mo id="Sx5.p2.2.m2.1.1.1" xref="Sx5.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.p2.2.m2.1b"><apply id="Sx5.p2.2.m2.1.1.cmml" xref="Sx5.p2.2.m2.1.1"><csymbol cd="latexml" id="Sx5.p2.2.m2.1.1.1.cmml" xref="Sx5.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="Sx5.p2.2.m2.1.1.2.cmml" xref="Sx5.p2.2.m2.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.p2.2.m2.1c">5\%</annotation></semantics></math>. This indicates that LLaMA might have learned cross-lingual alignment relationships between concepts during the pretraining process.</p>
</div>
<figure id="Sx5.F3" class="ltx_figure"><img src="/html/2401.01055/assets/x3.png" id="Sx5.F3.g1" class="ltx_graphics ltx_img_portrait" width="461" height="599" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Case study of code-switching. Text with a red background represents the non-English target language (Chinese). Text with a cyan background indicates code-switching language in the model’s output, which could be English, Japanese, Russian or other languages.</figcaption>
</figure>
<figure id="Sx5.F4" class="ltx_figure"><img src="/html/2401.01055/assets/x4.png" id="Sx5.F4.g1" class="ltx_graphics ltx_img_square" width="461" height="422" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Code-switching rate across languages.</figcaption>
</figure>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Related Work</h2>

<section id="Sx6.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Resource Gap in LLMs</h3>

<div id="Sx6.SSx1.p1" class="ltx_para">
<p id="Sx6.SSx1.p1.1" class="ltx_p">One of the main challenges of LLMs is the resource gap, as they are mainly pretrained on English corpus and have limited access to data from other languages. English dominates the field of NLP as an extremely high-resource language with the most raw text data from various domains, leaving few of the over 7000 languages of the world represented in the field <cite class="ltx_cite ltx_citemacro_citep">(Joshi et&nbsp;al. <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>. This creates a disparity in language models’ capability to handle different languages. Previous findings indicate that LLMs have difficulty comprehending and generating non-English texts, particularly in low-resource languages<cite class="ltx_cite ltx_citemacro_citep">(Nguyen et&nbsp;al. <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Zhu et&nbsp;al. <a href="#bib.bib54" title="" class="ltx_ref">2023</a>; Huang et&nbsp;al. <a href="#bib.bib20" title="" class="ltx_ref">2023a</a>)</cite>. To address the resource gap, several solutions have been proposed or implemented by researchers and practitioners. One possible solution is to increase the amount of data available from various languages and fields, and make it accessible for pretraining and evaluating LLMs <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al. <a href="#bib.bib30" title="" class="ltx_ref">2022</a>; Chen et&nbsp;al. <a href="#bib.bib6" title="" class="ltx_ref">2022</a>; Cahyawijaya et&nbsp;al. <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>
. However, this approach incurs significant computational expenses and the resource gap persists. Alternatively, multilingual language models trained on texts from different languages concurrently, such as mBERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al. <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> and XLM-R <cite class="ltx_cite ltx_citemacro_citep">(Conneau et&nbsp;al. <a href="#bib.bib9" title="" class="ltx_ref">2020a</a>)</cite>, have been introduced to bridge the gap effectively.</p>
</div>
</section>
<section id="Sx6.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Cross-Lingual Transfer</h3>

<div id="Sx6.SSx2.p1" class="ltx_para">
<p id="Sx6.SSx2.p1.1" class="ltx_p">Multilingual language models have demonstrated a high level of zero-shot or few-shot cross-lingual transferability across a wide range of tasks <cite class="ltx_cite ltx_citemacro_citep">(Wu and Dredze <a href="#bib.bib49" title="" class="ltx_ref">2019</a>; Pires, Schlinger, and Garrette <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Winata et&nbsp;al. <a href="#bib.bib48" title="" class="ltx_ref">2021b</a>)</cite>. This means that they can acquire the language capability from supervised data in one language and apply it to another without or with few additional training data. The mechanism behind the strong cross-lingual performance has been investigated by the researchers. It has been shown that multilingual language models have inferred universal rules applicable to any language <cite class="ltx_cite ltx_citemacro_citep">(Artetxe, Ruder, and Yogatama <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; Chi, Hewitt, and Manning <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Conneau et&nbsp;al. <a href="#bib.bib10" title="" class="ltx_ref">2020b</a>)</cite>. Contrary to the common hypothesis that multilingual multilingual language models such as mBERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al. <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> rely on a shared subword vocabulary and joint pretraining across multiple languages <cite class="ltx_cite ltx_citemacro_citep">(Pires, Schlinger, and Garrette <a href="#bib.bib35" title="" class="ltx_ref">2019</a>; Cao, Kitaev, and Klein <a href="#bib.bib5" title="" class="ltx_ref">2020</a>; Wu and Dredze <a href="#bib.bib49" title="" class="ltx_ref">2019</a>)</cite>, researchers have developed new understandings on the models, emphasizing the models’ ability to learn universal semantic abstractions <cite class="ltx_cite ltx_citemacro_citep">(Artetxe, Ruder, and Yogatama <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; Chi, Hewitt, and Manning <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>. In terms of the factors that influence cross-lingual performance, researchers have associated transferability with parameter sharing <cite class="ltx_cite ltx_citemacro_citep">(Conneau et&nbsp;al. <a href="#bib.bib10" title="" class="ltx_ref">2020b</a>; Dufter and Schütze <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Wu, Papadimitriou, and Tamkin <a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite> and language distance <cite class="ltx_cite ltx_citemacro_citep">(Conneau et&nbsp;al. <a href="#bib.bib10" title="" class="ltx_ref">2020b</a>; Eronen, Ptaszynski, and Masui <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>. We here further investigate the cross-lingual transferability of language models with new LLaMA-based experiments, presenting outcomes from a different aspect.</p>
</div>
</section>
<section id="Sx6.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Code-Switching</h3>

<div id="Sx6.SSx3.p1" class="ltx_para">
<p id="Sx6.SSx3.p1.1" class="ltx_p">Code-switching is a phenomenon in which multilingual speakers switch between languages within a single utterance. Previous work on the performance of multilingual language models on code-switching tasks has shown mixed results. Some studies have suggested that pretrained models fine-tuned for specific code-switching scenarios can achieve state-of-the-art performance for certain language pairs such as English-Spanish and English-Hindi <cite class="ltx_cite ltx_citemacro_citep">(Khanuja et&nbsp;al. <a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite>, while others have found that using meta-embeddings can yield better results with fewer parameters <cite class="ltx_cite ltx_citemacro_citep">(Winata, Lin, and Fung <a href="#bib.bib46" title="" class="ltx_ref">2019</a>; Winata et&nbsp;al. <a href="#bib.bib47" title="" class="ltx_ref">2019</a>, <a href="#bib.bib45" title="" class="ltx_ref">2021a</a>)</cite>. In another line of research, code-switching-based methods have been presented to improve the capability of multilingual language models <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al. <a href="#bib.bib24" title="" class="ltx_ref">2020</a>; Tan and Joty <a href="#bib.bib39" title="" class="ltx_ref">2021</a>; Krishnan et&nbsp;al. <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
</section>
<section id="Sx7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusions</h2>

<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.1" class="ltx_p">In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. Specifically, we conducts a comprehensive empirical study to analyze the necessity of vocabulary extension and the required training scale for effective transfer. We find that vocabulary extension is uncessary and that comparable transfer performance to state-of-the-art models can be achieved with less than <math id="Sx7.p1.1.m1.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="Sx7.p1.1.m1.1a"><mrow id="Sx7.p1.1.m1.1.1" xref="Sx7.p1.1.m1.1.1.cmml"><mn id="Sx7.p1.1.m1.1.1.2" xref="Sx7.p1.1.m1.1.1.2.cmml">1</mn><mo id="Sx7.p1.1.m1.1.1.1" xref="Sx7.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx7.p1.1.m1.1b"><apply id="Sx7.p1.1.m1.1.1.cmml" xref="Sx7.p1.1.m1.1.1"><csymbol cd="latexml" id="Sx7.p1.1.m1.1.1.1.cmml" xref="Sx7.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="Sx7.p1.1.m1.1.1.2.cmml" xref="Sx7.p1.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx7.p1.1.m1.1c">1\%</annotation></semantics></math> of the further pretraining data. Additionally, we observe instances of code-switching during the transfer training, suggesting that cross-lingual alignment might have been internalized within the model. Similar results are observed from the extension experiments on the 13 low-resource languages. Our analysis and findings offer assistance and guidance to the community in developing non-English LLMs.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Anil, R.; Dai, A.&nbsp;M.; Firat, O.; Johnson, M.; and Lepikhin, D. 2023.

</span>
<span class="ltx_bibblock">PaLM 2 Technical Report.

</span>
<span class="ltx_bibblock">arXiv:2305.10403.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe, Ruder, and Yogatama (2020)</span>
<span class="ltx_bibblock">
Artetxe, M.; Ruder, S.; and Yogatama, D. 2020.

</span>
<span class="ltx_bibblock">On the Cross-lingual Transferability of Monolingual Representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 4623–4637. Online: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.;
Lee, P.; Lee, Y.&nbsp;T.; Li, Y.; Lundberg, S.; Nori, H.; Palangi, H.; Ribeiro,
M.&nbsp;T.; and Zhang, Y. 2023.

</span>
<span class="ltx_bibblock">Sparks of Artificial General Intelligence: Early experiments with
GPT-4.

</span>
<span class="ltx_bibblock">arXiv:2303.12712.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cahyawijaya et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Cahyawijaya, S.; Lovenia, H.; Aji, A.&nbsp;F.; Winata, G.&nbsp;I.; and Wilie, B. 2023.

</span>
<span class="ltx_bibblock">NusaCrowd: Open Source Initiative for Indonesian NLP Resources.

</span>
<span class="ltx_bibblock">arXiv:2212.09648.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao, Kitaev, and Klein (2020)</span>
<span class="ltx_bibblock">
Cao, S.; Kitaev, N.; and Klein, D. 2020.

</span>
<span class="ltx_bibblock">Multilingual Alignment of Contextual Word Representations.

</span>
<span class="ltx_bibblock">arXiv:2002.03518.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Chen, G.; Ma, S.; Chen, Y.; Zhang, D.; Pan, J.; Wang, W.; and Wei, F. 2022.

</span>
<span class="ltx_bibblock">Towards Making the Most of Multilingual Pretraining for Zero-Shot
Neural Machine Translation.

</span>
<span class="ltx_bibblock">arXiv:2110.08547.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi, Hewitt, and Manning (2020)</span>
<span class="ltx_bibblock">
Chi, E.&nbsp;A.; Hewitt, J.; and Manning, C.&nbsp;D. 2020.

</span>
<span class="ltx_bibblock">Finding Universal Grammatical Relations in Multilingual BERT.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 5564–5577. Online: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Hilton, J.; Nakano, R.; Hesse, C.; and
Schulman, J. 2021.

</span>
<span class="ltx_bibblock">Training Verifiers to Solve Math Word Problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2110.14168.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2020a)</span>
<span class="ltx_bibblock">
Conneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.; Wenzek, G.; Guzmán, F.;
Grave, E.; Ott, M.; Zettlemoyer, L.; and Stoyanov, V. 2020a.

</span>
<span class="ltx_bibblock">Unsupervised Cross-lingual Representation Learning at Scale.

</span>
<span class="ltx_bibblock">arXiv:1911.02116.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2020b)</span>
<span class="ltx_bibblock">
Conneau, A.; Wu, S.; Li, H.; Zettlemoyer, L.; and Stoyanov, V.
2020b.

</span>
<span class="ltx_bibblock">Emerging Cross-lingual Structure in Pretrained Language Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 6022–6034. Online: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Conover, M.; Hayes, M.; Mathur, A.; Xie, J.; Wan, J.; Shah, S.; Ghodsi, A.;
Wendell, P.; Zaharia, M.; and Xin, R. 2023.

</span>
<span class="ltx_bibblock">Free Dolly: Introducing the World’s First Truly Open
Instruction-Tuned LLM.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui, Yang, and Yao (2023a)</span>
<span class="ltx_bibblock">
Cui, Y.; Yang, Z.; and Yao, X. 2023a.

</span>
<span class="ltx_bibblock">Chinese LLaMA and Alpaca Large Language Models.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui, Yang, and Yao (2023b)</span>
<span class="ltx_bibblock">
Cui, Y.; Yang, Z.; and Yao, X. 2023b.

</span>
<span class="ltx_bibblock">Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca.

</span>
<span class="ltx_bibblock">arXiv:2304.08177.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, 4171–4186. Minneapolis,
Minnesota: Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun, X.; Xu, J.; Li,
L.; and Sui, Z. 2023.

</span>
<span class="ltx_bibblock">A Survey on In-context Learning.

</span>
<span class="ltx_bibblock">arXiv:2301.00234.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dufter and Schütze (2020)</span>
<span class="ltx_bibblock">
Dufter, P.; and Schütze, H. 2020.

</span>
<span class="ltx_bibblock">Identifying Elements Essential for BERT’s Multilinguality.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, 4423–4437. Online: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eronen, Ptaszynski, and Masui (2023)</span>
<span class="ltx_bibblock">
Eronen, J.; Ptaszynski, M.; and Masui, F. 2023.

</span>
<span class="ltx_bibblock">Zero-shot cross-lingual transfer language selection using linguistic
similarity.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Information Processing &amp; Management</em>, 60(3): 103250.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and
Steinhardt, J. 2020.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2009.03300.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Hu, E.&nbsp;J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; and Chen,
W. 2021.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2106.09685.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Huang, H.; Tang, T.; Zhang, D.; Zhao, W.&nbsp;X.; Song, T.; Xia, Y.; and Wei, F.
2023a.

</span>
<span class="ltx_bibblock">Not All Languages Are Created Equal in LLMs: Improving Multilingual
Capability by Cross-Lingual-Thought Prompting.

</span>
<span class="ltx_bibblock">arXiv:2305.07004.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Huang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.

</span>
<span class="ltx_bibblock">Language Models as Zero-Shot Planners: Extracting Actionable
Knowledge for Embodied Agents.

</span>
<span class="ltx_bibblock">In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and
Sabato, S., eds., <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 39th International Conference on
Machine Learning</em>, volume 162 of <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning
Research</em>, 9118–9147. PMLR.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Huang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; and Zhang, J. 2023b.

</span>
<span class="ltx_bibblock">C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for
Foundation Models.

</span>
<span class="ltx_bibblock">arXiv:2305.08322.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ji, Y.; Deng, Y.; Gong, Y.; Peng, Y.; Niu, Q.; Ma, B.; and Li, X. 2023.

</span>
<span class="ltx_bibblock">BELLE: Be Everyone’s Large Language model Engine.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/LianjiaTech/BELLE</span>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Jiang, Z.; Anastasopoulos, A.; Araki, J.; Ding, H.; and Neubig, G. 2020.

</span>
<span class="ltx_bibblock">X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained
Language Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, 5943–5959. Online: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Joshi, P.; Santy, S.; Budhiraja, A.; Bali, K.; and Choudhury, M. 2020.

</span>
<span class="ltx_bibblock">The State and Fate of Linguistic Diversity and Inclusion in the NLP
World.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 6282–6293. Online: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Katz et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Katz, D.&nbsp;M.; Bommarito, M.&nbsp;J.; Gao, S.; and Arredondo, P. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 passes the bar exam.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Available at SSRN 4389233</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khanuja et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Khanuja, S.; Dandapat, S.; Srinivasan, A.; Sitaram, S.; and Choudhury, M. 2020.

</span>
<span class="ltx_bibblock">GLUECoS: An Evaluation Benchmark for Code-Switched NLP.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 3575–3585. Online: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishnan et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Krishnan, J.; Anastasopoulos, A.; Purohit, H.; and Rangwala, H. 2021.

</span>
<span class="ltx_bibblock">Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent
Prediction and Slot Filling.

</span>
<span class="ltx_bibblock">arXiv:2103.07792.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Li, H.; Koto, F.; Wu, M.; Aji, A.&nbsp;F.; and Baldwin, T. 2023.

</span>
<span class="ltx_bibblock">Bactrian-X : A Multilingual Replicable Instruction-Following Model
with Low-Rank Adaptation.

</span>
<span class="ltx_bibblock">arXiv:2305.15011.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Lin, X.&nbsp;V.; Mihaylov, T.; Artetxe, M.; Wang, T.; Chen, S.; Simig, D.; Ott, M.;
Goyal, N.; Bhosale, S.; Du, J.; Pasunuru, R.; Shleifer, S.; Koura, P.&nbsp;S.;
Chaudhary, V.; O’Horo, B.; Wang, J.; Zettlemoyer, L.; Kozareva, Z.; Diab, M.;
Stoyanov, V.; and Li, X. 2022.

</span>
<span class="ltx_bibblock">Few-shot Learning with Multilingual Language Models.

</span>
<span class="ltx_bibblock">arXiv:2112.10668.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Nguyen, X.-P.; Aljunied, S.&nbsp;M.; Joty, S.; and Bing, L. 2023.

</span>
<span class="ltx_bibblock">Democratizing LLMs for Low-Resource Languages by Leveraging their
English Dominant Abilities with Linguistically-Diverse Prompts.

</span>
<span class="ltx_bibblock">arXiv:2306.11372.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock">Introducing ChatGPT.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenLMLab (2023)</span>
<span class="ltx_bibblock">
OpenLMLab. 2023.

</span>
<span class="ltx_bibblock">Open-Chinese-LLaMA.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Penedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Cappelli, A.; Alobeidli,
H.; Pannier, B.; Almazrouei, E.; and Launay, J. 2023.

</span>
<span class="ltx_bibblock">The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora
with Web Data, and Web Data Only.

</span>
<span class="ltx_bibblock">arXiv:2306.01116.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires, Schlinger, and Garrette (2019)</span>
<span class="ltx_bibblock">
Pires, T.; Schlinger, E.; and Garrette, D. 2019.

</span>
<span class="ltx_bibblock">How Multilingual is Multilingual BERT?

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, 4996–5001. Florence, Italy: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranta and Goutte (2021)</span>
<span class="ltx_bibblock">
Ranta, A.; and Goutte, C. 2021.

</span>
<span class="ltx_bibblock">Linguistic Diversity in Natural Language Processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Traitement Automatique des Langues</em>, 62(3): 7–11.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Scao, T.&nbsp;L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilić, S.; Hesslow, D.; and
Castagné, R. 2023.

</span>
<span class="ltx_bibblock">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.

</span>
<span class="ltx_bibblock">arXiv:2211.05100.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">StabilityAI (2023)</span>
<span class="ltx_bibblock">
StabilityAI. 2023.

</span>
<span class="ltx_bibblock">Announcing StableCode.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Joty (2021)</span>
<span class="ltx_bibblock">
Tan, S.; and Joty, S. 2021.

</span>
<span class="ltx_bibblock">Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots.

</span>
<span class="ltx_bibblock">arXiv:2103.09593.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang,
P.; and Hashimoto, T.&nbsp;B. 2023.

</span>
<span class="ltx_bibblock">Alpaca: A Strong, Replicable Instruction-Following Model.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023a)</span>
<span class="ltx_bibblock">
Team, I. 2023a.

</span>
<span class="ltx_bibblock">Internlm: A multilingual language model with progressively enhanced
capabilities.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023b)</span>
<span class="ltx_bibblock">
Team, I. 2023b.

</span>
<span class="ltx_bibblock">InternLM: A Multilingual Language Model with Progressively Enhanced
Capabilities.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/InternLM/InternLM-techreport</span>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix,
T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin,
A.; Grave, E.; and Lample, G. 2023a.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models.

</span>
<span class="ltx_bibblock">arXiv:2302.13971.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; and Almahairi, A.
2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock">arXiv:2307.09288.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winata et&nbsp;al. (2021a)</span>
<span class="ltx_bibblock">
Winata, G.&nbsp;I.; Cahyawijaya, S.; Liu, Z.; Lin, Z.; Madotto, A.; and Fung, P.
2021a.

</span>
<span class="ltx_bibblock">Are Multilingual Models Effective in Code-Switching?

</span>
<span class="ltx_bibblock">arXiv:2103.13309.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winata, Lin, and Fung (2019)</span>
<span class="ltx_bibblock">
Winata, G.&nbsp;I.; Lin, Z.; and Fung, P. 2019.

</span>
<span class="ltx_bibblock">Learning Multilingual Meta-Embeddings for Code-Switching Named Entity
Recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 4th Workshop on Representation Learning
for NLP (RepL4NLP-2019)</em>, 181–186. Florence, Italy: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winata et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Winata, G.&nbsp;I.; Lin, Z.; Shin, J.; Liu, Z.; and Fung, P. 2019.

</span>
<span class="ltx_bibblock">Hierarchical Meta-Embeddings for Code-Switching Named Entity
Recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, 3541–3547. Hong Kong, China:
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winata et&nbsp;al. (2021b)</span>
<span class="ltx_bibblock">
Winata, G.&nbsp;I.; Madotto, A.; Lin, Z.; Liu, R.; Yosinski, J.; and Fung, P.
2021b.

</span>
<span class="ltx_bibblock">Language Models are Few-shot Multilingual Learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Workshop on Multilingual
Representation Learning</em>, 1–15. Punta Cana, Dominican Republic: Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Dredze (2019)</span>
<span class="ltx_bibblock">
Wu, S.; and Dredze, M. 2019.

</span>
<span class="ltx_bibblock">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of
BERT.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, 833–844. Hong Kong, China:
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu, Papadimitriou, and Tamkin (2022)</span>
<span class="ltx_bibblock">
Wu, Z.; Papadimitriou, I.; and Tamkin, A. 2022.

</span>
<span class="ltx_bibblock">Oolong: Investigating What Makes Crosslingual Transfer Hard with
Controlled Studies.

</span>
<span class="ltx_bibblock">arXiv:2202.12312.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Zhang, M.; Zhang, Q.; Zhang, Y.; and Gui, T. 2023a.

</span>
<span class="ltx_bibblock">LLMEVAL-1 Chinese Large Language Model Evaluation Phase 1.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Zhang, X.; Li, C.; Zong, Y.; Ying, Z.; He, L.; and Qiu, X. 2023b.

</span>
<span class="ltx_bibblock">Evaluating the Performance of Large Language Models on GAOKAO
Benchmark.

</span>
<span class="ltx_bibblock">arXiv:2305.12474.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang, Y.; Saied, A.; Chen, W.;
and Duan, N. 2023.

</span>
<span class="ltx_bibblock">AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.

</span>
<span class="ltx_bibblock">arXiv:2304.06364.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhu, W.; Liu, H.; Dong, Q.; Xu, J.; Huang, S.; Kong, L.; Chen, J.; and Li, L.
2023.

</span>
<span class="ltx_bibblock">Multilingual Machine Translation with Large Language Models:
Empirical Results and Analysis.

</span>
<span class="ltx_bibblock">arXiv:2304.04675.

</span>
</li>
</ul>
</section>
<section id="Sx8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix</h2>

<div id="Sx8.p1" class="ltx_para">
<p id="Sx8.p1.1" class="ltx_p"><span id="Sx8.p1.1.1" class="ltx_text ltx_font_bold">The prompt for LLM-Eval</span></p>
</div>
<div id="Sx8.p2" class="ltx_para ltx_noindent">
<p id="Sx8.p2.1" class="ltx_p">For questions related to specific subject matters, we score the following LLM outputs using a scale of 0/1/2/3:</p>
</div>
<div id="Sx8.p3" class="ltx_para ltx_noindent">
<p id="Sx8.p3.1" class="ltx_p">Accuracy: Evaluates whether the answer is precise and correct, with accurate information. A high-quality answer should be factually reliable.</p>
</div>
<div id="Sx8.p4" class="ltx_para ltx_noindent">
<p id="Sx8.p4.1" class="ltx_p">Fluency: Assesses how well the answer adheres to human language conventions, with coherent phrasing and clear expression. A high-quality answer should be easy to understand, without convoluted or difficult-to-parse sentences.</p>
</div>
<div id="Sx8.p5" class="ltx_para ltx_noindent">
<p id="Sx8.p5.1" class="ltx_p">Informativeness: Judges if the answer provides sufficient relevant information of practical value. A high-quality answer should offer useful and pertinent information to the questioner.</p>
</div>
<div id="Sx8.p6" class="ltx_para ltx_noindent">
<p id="Sx8.p6.1" class="ltx_p">Logicality: Examines if the answer is logically sound and accurate in presenting viewpoints and arguments. A highquality answer should follow logical principles, demonstrating clear reasoning and coherence.</p>
</div>
<div id="Sx8.p7" class="ltx_para ltx_noindent">
<p id="Sx8.p7.1" class="ltx_p">Harmlessness: Assesses whether the answer avoids unethical or harmful content, adhering to ethical norms. A highquality answer should uphold ethical principles, avoiding the propagation of harmful or immoral information.</p>
</div>
<div id="Sx8.p8" class="ltx_para ltx_noindent">
<p id="Sx8.p8.1" class="ltx_p">Note: If the model provides no response, all scores except for “Harmlessness” should be 0.</p>
</div>
<div id="Sx8.p9" class="ltx_para ltx_noindent">
<p id="Sx8.p9.1" class="ltx_p">The question is: Question The LLM response is: Response</p>
</div>
<div id="Sx8.p10" class="ltx_para ltx_noindent">
<p id="Sx8.p10.1" class="ltx_p">The reference answer for this question is: Reference Answer</p>
</div>
<div id="Sx8.p11" class="ltx_para ltx_noindent">
<p id="Sx8.p11.1" class="ltx_p">Please provide an answer in the following format, assigning your perceived scores for LLM response’s “accuracy”, “fluency”, “informativeness”, “logicality”, and “harmlessness” on a scale of 0/1/2/3:</p>
</div>
<div id="Sx8.p12" class="ltx_para ltx_noindent">
<p id="Sx8.p12.1" class="ltx_p">“Accuracy”: score for LLM response’s accuracy (integer),</p>
</div>
<div id="Sx8.p13" class="ltx_para ltx_noindent">
<p id="Sx8.p13.1" class="ltx_p">“Fluency”: score for LLM response’s fluency (integer),</p>
</div>
<div id="Sx8.p14" class="ltx_para ltx_noindent">
<p id="Sx8.p14.1" class="ltx_p">“Informativeness”: score for LLM response’s informativeness (integer),</p>
</div>
<div id="Sx8.p15" class="ltx_para ltx_noindent">
<p id="Sx8.p15.1" class="ltx_p">“Logicality”: score for LLM response’s logicality (integer),</p>
</div>
<div id="Sx8.p16" class="ltx_para ltx_noindent">
<p id="Sx8.p16.1" class="ltx_p">“Harmlessness”: score for LLM response’s harmlessness (integer).</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.01054" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.01055" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2401.01055">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.01055" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.01056" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 07:37:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>LLaMA Beyond English: An Empirical Study on Language Capability Transfer</title>
<!--Generated on Fri Jan 12 08:13:56 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2401.01055v2/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2401.01055v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2401.01055v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2401.01055v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#Sx1" title="Introduction ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#Sx2" title="Background and Overview ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Background and Overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx2.SSx1" title="Step 1: Pretraining to acquire language capability and knowledge ‚Ä£ Background and Overview ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Step 1: Pretraining to acquire language capability and knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx2.SSx2" title="Step 2: Instruction tuning for aligning with human intent ‚Ä£ Background and Overview ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Step 2: Instruction tuning for aligning with human intent</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx2.SSx3" title="Extrapolating LLMs to non-English languages ‚Ä£ Background and Overview ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Extrapolating LLMs to non-English languages</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#Sx3" title="Experimental Setup ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx3.SSx1" title="Models ‚Ä£ Experimental Setup ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx3.SSx2" title="Datasets ‚Ä£ Experimental Setup ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx3.SSx3" title="Evaluation Protocol ‚Ä£ Experimental Setup ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Evaluation Protocol</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#Sx4" title="Main Results ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Main Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx4.SSx1" title="The Impact of Vocabulary Extension on Transfer ‚Ä£ Main Results ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">The Impact of Vocabulary Extension on Transfer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx4.SSx2" title="Training Scales Required for Effective Transfer ‚Ä£ Main Results ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Training Scales Required for Effective Transfer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx4.SSx3" title="How about the Original English Capabilities ‚Ä£ Main Results ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">How about the Original English Capabilities</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#Sx5" title="Extending the Analysis to Multiple Languages ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Extending the Analysis to Multiple Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#Sx6" title="Related Work ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx6.SSx1" title="Resource Gap in LLMs ‚Ä£ Related Work ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Resource Gap in LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx6.SSx2" title="Cross-Lingual Transfer ‚Ä£ Related Work ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Cross-Lingual Transfer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#Sx6.SSx3" title="Code-Switching ‚Ä£ Related Work ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Code-Switching</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#Sx7" title="Conclusions ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_title">Conclusions</span></a></li>
</ol></nav>

<div class="ltx_page_content">
<div aria-label="‚ÄùConversion" been="" class="package-alerts ltx_document" errors="" found‚Äù="" have="" role="‚Äústatus‚Äù">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewBox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: mdwlist</li>
<li>failed: bibentry</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2401.01055v2 [cs.CL] 12 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LLaMA Beyond English: An Empirical Study on Language Capability Transfer</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jun Zhao<span class="ltx_ERROR undefined" id="id2.1.id1">\equalcontrib</span>,
Zhihao Zhang<span class="ltx_ERROR undefined" id="id3.2.id2">\equalcontrib</span>,
Luhui Gao,
Qi Zhang,
Tao Gui,
Xuanjing Huang
</span><span class="ltx_author_notes">Corresponding Author</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.1">In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language.
To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model‚Äôs level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model‚Äôs response quality is conducted, considering aspects such as accuracy, fluency, informativeness, logical coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting instruction tasks from 17 diverse categories.
Our evaluation results demonstrate that comparable performance to state-of-the-art transfer models can be achieved with less than <math alttext="1\%" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">1</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">percent</csymbol><cn id="id1.1.m1.1.1.2.cmml" type="integer" xref="id1.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">1\%</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">1 %</annotation></semantics></math> of the pretraining data, both in terms of knowledge alignment and response quality. Furthermore, the experimental outcomes across the thirteen low-resource languages also exhibit similar trends. We anticipate that the conclusions revealed by the experiments will aid the community in developing non-English LLMs.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="Sx1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="502" id="Sx1.F1.g1" src="x1.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Pretrained LLaMA models, which are primarily trained on English-dominated corpus (as depicted on the left), are not inherently proficient in handling non-English languages.
We aim to investigate the necessity of vocabulary extension, further pretraining, and instruction tuning, as well as to what extent they influence the capability transfer.
This exploration enables us to efficiently transfer LLaMA‚Äôs language capabilities to non-English languages (as illustrated on the right), minimizing costs in the process.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">For decades, researchers in Natural Language Processing (NLP) have been exploring the fundamental principles of intelligence <cite class="ltx_cite ltx_citemacro_citep">(Bubeck et&nbsp;al. <a class="ltx_ref" href="#bib.bib3" title="">2023</a>)</cite>. The recent advances in large language models (LLMs) seem to have revealed a glimmer of hope. Benefitting from the unprecedented scales of model size and training data, many LLMs like ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a class="ltx_ref" href="#bib.bib32" title="">2022</a>)</cite>, PaLM <cite class="ltx_cite ltx_citemacro_citep">(Anil et&nbsp;al. <a class="ltx_ref" href="#bib.bib1" title="">2023</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a class="ltx_ref" href="#bib.bib43" title="">2023a</a>)</cite>, and others have emerged strong capabilities in reasoning <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et&nbsp;al. <a class="ltx_ref" href="#bib.bib8" title="">2021</a>)</cite>, planning <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al. <a class="ltx_ref" href="#bib.bib21" title="">2022</a>)</cite>, and learning from experience <cite class="ltx_cite ltx_citemacro_citep">(Dong et&nbsp;al. <a class="ltx_ref" href="#bib.bib15" title="">2023</a>)</cite> at or surpassing human levels. These general capabilities also provide a foundation for LLMs to address intricate real-world tasks, such as successfully completing the entire Uniform Bar Examination (UBE) <cite class="ltx_cite ltx_citemacro_citep">(Katz et&nbsp;al. <a class="ltx_ref" href="#bib.bib26" title="">2023</a>)</cite> or coding based on natural language instructions <cite class="ltx_cite ltx_citemacro_citep">(StabilityAI <a class="ltx_ref" href="#bib.bib38" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.2">Many well-known LLMs are capable of comprehending input and generating responses across different languages, thanks to their pretraining on a diverse mix of corpus from multiple languages. However, due to the imbalanced distribution of language resources, collecting extensive training data for all languages is nearly impossible <cite class="ltx_cite ltx_citemacro_citep">(Ranta and Goutte <a class="ltx_ref" href="#bib.bib36" title="">2021</a>)</cite>. Taking the representative LLM BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al. <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite> as an example, it has been pretrained on 46 natural languages. Yet, this number accounts for only <math alttext="0.66\%" class="ltx_Math" display="inline" id="Sx1.p2.1.m1.1"><semantics id="Sx1.p2.1.m1.1a"><mrow id="Sx1.p2.1.m1.1.1" xref="Sx1.p2.1.m1.1.1.cmml"><mn id="Sx1.p2.1.m1.1.1.2" xref="Sx1.p2.1.m1.1.1.2.cmml">0.66</mn><mo id="Sx1.p2.1.m1.1.1.1" xref="Sx1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p2.1.m1.1b"><apply id="Sx1.p2.1.m1.1.1.cmml" xref="Sx1.p2.1.m1.1.1"><csymbol cd="latexml" id="Sx1.p2.1.m1.1.1.1.cmml" xref="Sx1.p2.1.m1.1.1.1">percent</csymbol><cn id="Sx1.p2.1.m1.1.1.2.cmml" type="float" xref="Sx1.p2.1.m1.1.1.2">0.66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p2.1.m1.1c">0.66\%</annotation><annotation encoding="application/x-llamapun" id="Sx1.p2.1.m1.1d">0.66 %</annotation></semantics></math> of the roughly <math alttext="7,000" class="ltx_Math" display="inline" id="Sx1.p2.2.m2.2"><semantics id="Sx1.p2.2.m2.2a"><mrow id="Sx1.p2.2.m2.2.3.2" xref="Sx1.p2.2.m2.2.3.1.cmml"><mn id="Sx1.p2.2.m2.1.1" xref="Sx1.p2.2.m2.1.1.cmml">7</mn><mo id="Sx1.p2.2.m2.2.3.2.1" xref="Sx1.p2.2.m2.2.3.1.cmml">,</mo><mn id="Sx1.p2.2.m2.2.2" xref="Sx1.p2.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p2.2.m2.2b"><list id="Sx1.p2.2.m2.2.3.1.cmml" xref="Sx1.p2.2.m2.2.3.2"><cn id="Sx1.p2.2.m2.1.1.cmml" type="integer" xref="Sx1.p2.2.m2.1.1">7</cn><cn id="Sx1.p2.2.m2.2.2.cmml" type="integer" xref="Sx1.p2.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p2.2.m2.2c">7,000</annotation><annotation encoding="application/x-llamapun" id="Sx1.p2.2.m2.2d">7 , 000</annotation></semantics></math> languages currently in use. Moreover, within the corpus of these 46 languages, there exists extreme imbalance, with the high-resource English texts being 2.8 million times more than that of the low-resource Chitumbuka language. This is not an isolated case. Another widely discussed language model, LLaMA, has been pretrained primarily on English-dominated corpus, supplemented with limited data from 20 related languages that utilize the Latin and Cyrillic scripts. As a result, LLaMA exhibits inferior performance in contexts involving non-English languages where it has not undergone sufficient training. Some researchers collect large-scale data for specific languages of interest and retrain an LLM <cite class="ltx_cite ltx_citemacro_citep">(Team <a class="ltx_ref" href="#bib.bib41" title="">2023a</a>)</cite>. However, this inevitably leads to high computational and data collection costs, which is not suitable for low-resource languages. While <cite class="ltx_cite ltx_citemacro_citet">Cui, Yang, and Yao (<a class="ltx_ref" href="#bib.bib13" title="">2023b</a>)</cite> extend original vocabulary and further pretrain LLaMA with 30B Chinese tokens by LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al. <a class="ltx_ref" href="#bib.bib19" title="">2021</a>)</cite>, reporting promising results. Nonetheless, a fine-grained systematic investigation of the transfer process remains lacking.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">In this work, we take a step towards gaining a comprehensive understanding of the language capability transfer in LLMs. As shown in figure <a class="ltx_ref" href="#Sx1.F1" title="Figure 1 ‚Ä£ Introduction ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_tag">1</span></a>, we empirically investigate several key aspects based on LLaMA:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">(1) <span class="ltx_text ltx_font_bold" id="Sx1.p4.1.1">The impact of vocabulary extension on transfer.</span> We find that further pretraining with 0.5 billion Chinese tokens on the original vocabulary significantly outperforms performance on the extended vocabulary, even though the latter has been further pretrained on over 30 billion tokens. This suggests that vocabulary extension might not be a suitable choice for small-scale incremental pretraining in the order of tens of billions. </p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">(2) <span class="ltx_text ltx_font_bold" id="Sx1.p5.1.1">Training scales required for effective transfer.</span> We find that further Chinese pretraining with 100 billion tokens or fewer is insufficient to significantly improve LLaMA‚Äôs knowledge level. However, enhancing LLaMA‚Äôs response quality (i.e., language generation capability), requires only hundreds of thousands of instruction data rather than a large-scale further pretraining.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.1">(3) <span class="ltx_text ltx_font_bold" id="Sx1.p6.1.1">The effect of transfer training on the original English capabilities.</span> We find that exclusive reliance on Chinese corpora for transfer training markedly compromises LLaMA‚Äôs original English proficiency, a concern alleviated effectively through multilingual joint training.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx1.p7">
<p class="ltx_p" id="Sx1.p7.1">The aforementioned findings enable us to transfer LLaMA‚Äôs capabilities of language generation and following instructions to non-English languages at minimal cost. Based on evaluation results from four widely used standardized testing benchmarks (C-Eval, GAOKAO-Bench, MMLU, AGI-Eval) and an instruction evaluation benchmark LLM-Eval, we achieve comparable knowledge level and response quality to the state-of-the-art Open Chinese LLaMA, while using less than <math alttext="1\%" class="ltx_Math" display="inline" id="Sx1.p7.1.m1.1"><semantics id="Sx1.p7.1.m1.1a"><mrow id="Sx1.p7.1.m1.1.1" xref="Sx1.p7.1.m1.1.1.cmml"><mn id="Sx1.p7.1.m1.1.1.2" xref="Sx1.p7.1.m1.1.1.2.cmml">1</mn><mo id="Sx1.p7.1.m1.1.1.1" xref="Sx1.p7.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx1.p7.1.m1.1b"><apply id="Sx1.p7.1.m1.1.1.cmml" xref="Sx1.p7.1.m1.1.1"><csymbol cd="latexml" id="Sx1.p7.1.m1.1.1.1.cmml" xref="Sx1.p7.1.m1.1.1.1">percent</csymbol><cn id="Sx1.p7.1.m1.1.1.2.cmml" type="integer" xref="Sx1.p7.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p7.1.m1.1c">1\%</annotation><annotation encoding="application/x-llamapun" id="Sx1.p7.1.m1.1d">1 %</annotation></semantics></math> of the training data. Furthermore, extension experiments on another 13 low-resource languages also exhibit similar trends.
We aim for the experimental results and analyses in this paper to provide assistance and guidance to the community in constructing non-English LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Background and Overview</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">In this subsection, we firstly present the essential steps to develop an instruction-following LLM. Subsequently, we review common practices of extrapolating this model to a non-English language and provide an overview of our empirical research conducted for the model extrapolation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="Sx2.SSx1">
<h3 class="ltx_title ltx_title_subsection">Step 1: Pretraining to acquire language capability and knowledge</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.SSx1.p1">
<p class="ltx_p" id="Sx2.SSx1.p1.1">As a significant source of foundational capabilities for a LLM, pretraining aims to predict the next token based on the prefix sequences. Formally, given a large corpus <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.1.m1.1"><semantics id="Sx2.SSx1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx1.p1.1.m1.1.1" xref="Sx2.SSx1.p1.1.m1.1.1.cmml">ùíü</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.1.m1.1b"><ci id="Sx2.SSx1.p1.1.m1.1.1.cmml" xref="Sx2.SSx1.p1.1.m1.1.1">ùíü</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.1.m1.1d">caligraphic_D</annotation></semantics></math>, the training objective is to minimize the following loss:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="Sx2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{pretrain}=\sum_{x\in\mathcal{D}}\sum_{i}\log p_{\theta}(x_{i}|x_{%
1},...,x_{i-1})," class="ltx_Math" display="block" id="Sx2.E1.m1.2"><semantics id="Sx2.E1.m1.2a"><mrow id="Sx2.E1.m1.2.2.1" xref="Sx2.E1.m1.2.2.1.1.cmml"><mrow id="Sx2.E1.m1.2.2.1.1" xref="Sx2.E1.m1.2.2.1.1.cmml"><msub id="Sx2.E1.m1.2.2.1.1.3" xref="Sx2.E1.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.2.2.1.1.3.2" xref="Sx2.E1.m1.2.2.1.1.3.2.cmml">‚Ñí</mi><mrow id="Sx2.E1.m1.2.2.1.1.3.3" xref="Sx2.E1.m1.2.2.1.1.3.3.cmml"><mi id="Sx2.E1.m1.2.2.1.1.3.3.2" xref="Sx2.E1.m1.2.2.1.1.3.3.2.cmml">p</mi><mo id="Sx2.E1.m1.2.2.1.1.3.3.1" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">‚Å¢</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.3" xref="Sx2.E1.m1.2.2.1.1.3.3.3.cmml">r</mi><mo id="Sx2.E1.m1.2.2.1.1.3.3.1a" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">‚Å¢</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.4" xref="Sx2.E1.m1.2.2.1.1.3.3.4.cmml">e</mi><mo id="Sx2.E1.m1.2.2.1.1.3.3.1b" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">‚Å¢</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.5" xref="Sx2.E1.m1.2.2.1.1.3.3.5.cmml">t</mi><mo id="Sx2.E1.m1.2.2.1.1.3.3.1c" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">‚Å¢</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.6" xref="Sx2.E1.m1.2.2.1.1.3.3.6.cmml">r</mi><mo id="Sx2.E1.m1.2.2.1.1.3.3.1d" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">‚Å¢</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.7" xref="Sx2.E1.m1.2.2.1.1.3.3.7.cmml">a</mi><mo id="Sx2.E1.m1.2.2.1.1.3.3.1e" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">‚Å¢</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.8" xref="Sx2.E1.m1.2.2.1.1.3.3.8.cmml">i</mi><mo id="Sx2.E1.m1.2.2.1.1.3.3.1f" xref="Sx2.E1.m1.2.2.1.1.3.3.1.cmml">‚Å¢</mo><mi id="Sx2.E1.m1.2.2.1.1.3.3.9" xref="Sx2.E1.m1.2.2.1.1.3.3.9.cmml">n</mi></mrow></msub><mo id="Sx2.E1.m1.2.2.1.1.2" rspace="0.111em" xref="Sx2.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="Sx2.E1.m1.2.2.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.cmml"><munder id="Sx2.E1.m1.2.2.1.1.1.2" xref="Sx2.E1.m1.2.2.1.1.1.2.cmml"><mo id="Sx2.E1.m1.2.2.1.1.1.2.2" movablelimits="false" rspace="0em" xref="Sx2.E1.m1.2.2.1.1.1.2.2.cmml">‚àë</mo><mrow id="Sx2.E1.m1.2.2.1.1.1.2.3" xref="Sx2.E1.m1.2.2.1.1.1.2.3.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.2.3.2" xref="Sx2.E1.m1.2.2.1.1.1.2.3.2.cmml">x</mi><mo id="Sx2.E1.m1.2.2.1.1.1.2.3.1" xref="Sx2.E1.m1.2.2.1.1.1.2.3.1.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.2.2.1.1.1.2.3.3" xref="Sx2.E1.m1.2.2.1.1.1.2.3.3.cmml">ùíü</mi></mrow></munder><mrow id="Sx2.E1.m1.2.2.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.cmml"><munder id="Sx2.E1.m1.2.2.1.1.1.1.2" xref="Sx2.E1.m1.2.2.1.1.1.1.2.cmml"><mo id="Sx2.E1.m1.2.2.1.1.1.1.2.2" movablelimits="false" xref="Sx2.E1.m1.2.2.1.1.1.1.2.2.cmml">‚àë</mo><mi id="Sx2.E1.m1.2.2.1.1.1.1.2.3" xref="Sx2.E1.m1.2.2.1.1.1.1.2.3.cmml">i</mi></munder><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.cmml"><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.3.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.1.cmml">log</mi><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.3a" lspace="0.167em" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.cmml">‚Å°</mo><msub id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml">Œ∏</mi></msub></mrow><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><msub id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml">x</mi><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml">i</mi></msub><mo fence="false" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mi id="Sx2.E1.m1.1.1" mathvariant="normal" xref="Sx2.E1.m1.1.1.cmml">‚Ä¶</mi><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.4" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml">x</mi><mrow id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">i</mi><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">‚àí</mo><mn id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.3" stretchy="false" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="Sx2.E1.m1.2.2.1.2" xref="Sx2.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.E1.m1.2b"><apply id="Sx2.E1.m1.2.2.1.1.cmml" xref="Sx2.E1.m1.2.2.1"><eq id="Sx2.E1.m1.2.2.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.2"></eq><apply id="Sx2.E1.m1.2.2.1.1.3.cmml" xref="Sx2.E1.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.3">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.3.2">‚Ñí</ci><apply id="Sx2.E1.m1.2.2.1.1.3.3.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3"><times id="Sx2.E1.m1.2.2.1.1.3.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.1"></times><ci id="Sx2.E1.m1.2.2.1.1.3.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.2">ùëù</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.3.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.3">ùëü</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.4.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.4">ùëí</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.5.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.5">ùë°</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.6.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.6">ùëü</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.7.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.7">ùëé</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.8.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.8">ùëñ</ci><ci id="Sx2.E1.m1.2.2.1.1.3.3.9.cmml" xref="Sx2.E1.m1.2.2.1.1.3.3.9">ùëõ</ci></apply></apply><apply id="Sx2.E1.m1.2.2.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1"><apply id="Sx2.E1.m1.2.2.1.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.2.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2">subscript</csymbol><sum id="Sx2.E1.m1.2.2.1.1.1.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.2"></sum><apply id="Sx2.E1.m1.2.2.1.1.1.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.3"><in id="Sx2.E1.m1.2.2.1.1.1.2.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.3.1"></in><ci id="Sx2.E1.m1.2.2.1.1.1.2.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.3.2">ùë•</ci><ci id="Sx2.E1.m1.2.2.1.1.1.2.3.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.2.3.3">ùíü</ci></apply></apply><apply id="Sx2.E1.m1.2.2.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1"><apply id="Sx2.E1.m1.2.2.1.1.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.2.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="Sx2.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.2.2"></sum><ci id="Sx2.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.2.3">ùëñ</ci></apply><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1"><times id="Sx2.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.2"></times><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3"><log id="Sx2.E1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.1"></log><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.2">ùëù</ci><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.3.2.3">ùúÉ</ci></apply></apply><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.2">ùë•</ci><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.4.3">ùëñ</ci></apply><list id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2"><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">ùë•</ci><cn id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply><ci id="Sx2.E1.m1.1.1.cmml" xref="Sx2.E1.m1.1.1">‚Ä¶</ci><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.2">ùë•</ci><apply id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3"><minus id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.1"></minus><ci id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.2">ùëñ</ci><cn id="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" type="integer" xref="Sx2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.2.2.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.E1.m1.2c">\mathcal{L}_{pretrain}=\sum_{x\in\mathcal{D}}\sum_{i}\log p_{\theta}(x_{i}|x_{%
1},...,x_{i-1}),</annotation><annotation encoding="application/x-llamapun" id="Sx2.E1.m1.2d">caligraphic_L start_POSTSUBSCRIPT italic_p italic_r italic_e italic_t italic_r italic_a italic_i italic_n end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_x ‚àà caligraphic_D end_POSTSUBSCRIPT ‚àë start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Sx2.SSx1.p1.2">where <math alttext="x=\{x_{1},...,x_{n}\}" class="ltx_Math" display="inline" id="Sx2.SSx1.p1.2.m1.3"><semantics id="Sx2.SSx1.p1.2.m1.3a"><mrow id="Sx2.SSx1.p1.2.m1.3.3" xref="Sx2.SSx1.p1.2.m1.3.3.cmml"><mi id="Sx2.SSx1.p1.2.m1.3.3.4" xref="Sx2.SSx1.p1.2.m1.3.3.4.cmml">x</mi><mo id="Sx2.SSx1.p1.2.m1.3.3.3" xref="Sx2.SSx1.p1.2.m1.3.3.3.cmml">=</mo><mrow id="Sx2.SSx1.p1.2.m1.3.3.2.2" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml"><mo id="Sx2.SSx1.p1.2.m1.3.3.2.2.3" stretchy="false" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml">{</mo><msub id="Sx2.SSx1.p1.2.m1.2.2.1.1.1" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.cmml"><mi id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.2" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.2.cmml">x</mi><mn id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.3" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="Sx2.SSx1.p1.2.m1.3.3.2.2.4" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml">,</mo><mi id="Sx2.SSx1.p1.2.m1.1.1" mathvariant="normal" xref="Sx2.SSx1.p1.2.m1.1.1.cmml">‚Ä¶</mi><mo id="Sx2.SSx1.p1.2.m1.3.3.2.2.5" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml">,</mo><msub id="Sx2.SSx1.p1.2.m1.3.3.2.2.2" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.cmml"><mi id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.2" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.2.cmml">x</mi><mi id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.3" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.3.cmml">n</mi></msub><mo id="Sx2.SSx1.p1.2.m1.3.3.2.2.6" stretchy="false" xref="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx1.p1.2.m1.3b"><apply id="Sx2.SSx1.p1.2.m1.3.3.cmml" xref="Sx2.SSx1.p1.2.m1.3.3"><eq id="Sx2.SSx1.p1.2.m1.3.3.3.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.3"></eq><ci id="Sx2.SSx1.p1.2.m1.3.3.4.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.4">ùë•</ci><set id="Sx2.SSx1.p1.2.m1.3.3.2.3.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2"><apply id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.cmml" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.1.cmml" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1">subscript</csymbol><ci id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.2.cmml" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.2">ùë•</ci><cn id="Sx2.SSx1.p1.2.m1.2.2.1.1.1.3.cmml" type="integer" xref="Sx2.SSx1.p1.2.m1.2.2.1.1.1.3">1</cn></apply><ci id="Sx2.SSx1.p1.2.m1.1.1.cmml" xref="Sx2.SSx1.p1.2.m1.1.1">‚Ä¶</ci><apply id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.1.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2">subscript</csymbol><ci id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.2.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.2">ùë•</ci><ci id="Sx2.SSx1.p1.2.m1.3.3.2.2.2.3.cmml" xref="Sx2.SSx1.p1.2.m1.3.3.2.2.2.3">ùëõ</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx1.p1.2.m1.3c">x=\{x_{1},...,x_{n}\}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx1.p1.2.m1.3d">italic_x = { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math> denotes an input token sequence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx2.SSx1.p2">
<p class="ltx_p" id="Sx2.SSx1.p2.1">By pretraining on massive text data ranging from billions to trillions of tokens, LLMs are capable of capturing intricate language structures, semantics, and contextual relationships, thereby acquiring strong language generation capabilities. Additionally, these LLMs also learn how to comprehend concepts, facts, and the connections between them, leading to a broad understanding of world knowledge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SSx2">
<h3 class="ltx_title ltx_title_subsection">Step 2: Instruction tuning for aligning with human intent</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.SSx2.p1">
<p class="ltx_p" id="Sx2.SSx2.p1.3">Instruction tuning (SFT) aims to further enhance the capability of LLMs to follow instructions. Its training data consists of many instruction-response pairs. The model needs to learn to accurately respond to instructions, rather than merely continuing from the preceding text. Formally, given an instruction dataset <math alttext="\mathcal{D}^{\prime}=\{(I,Y)\}" class="ltx_Math" display="inline" id="Sx2.SSx2.p1.1.m1.3"><semantics id="Sx2.SSx2.p1.1.m1.3a"><mrow id="Sx2.SSx2.p1.1.m1.3.3" xref="Sx2.SSx2.p1.1.m1.3.3.cmml"><msup id="Sx2.SSx2.p1.1.m1.3.3.3" xref="Sx2.SSx2.p1.1.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SSx2.p1.1.m1.3.3.3.2" xref="Sx2.SSx2.p1.1.m1.3.3.3.2.cmml">ùíü</mi><mo id="Sx2.SSx2.p1.1.m1.3.3.3.3" xref="Sx2.SSx2.p1.1.m1.3.3.3.3.cmml">‚Ä≤</mo></msup><mo id="Sx2.SSx2.p1.1.m1.3.3.2" xref="Sx2.SSx2.p1.1.m1.3.3.2.cmml">=</mo><mrow id="Sx2.SSx2.p1.1.m1.3.3.1.1" xref="Sx2.SSx2.p1.1.m1.3.3.1.2.cmml"><mo id="Sx2.SSx2.p1.1.m1.3.3.1.1.2" stretchy="false" xref="Sx2.SSx2.p1.1.m1.3.3.1.2.cmml">{</mo><mrow id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml"><mo id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2.1" stretchy="false" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml">(</mo><mi id="Sx2.SSx2.p1.1.m1.1.1" xref="Sx2.SSx2.p1.1.m1.1.1.cmml">I</mi><mo id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2.2" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml">,</mo><mi id="Sx2.SSx2.p1.1.m1.2.2" xref="Sx2.SSx2.p1.1.m1.2.2.cmml">Y</mi><mo id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2.3" stretchy="false" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml">)</mo></mrow><mo id="Sx2.SSx2.p1.1.m1.3.3.1.1.3" stretchy="false" xref="Sx2.SSx2.p1.1.m1.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.p1.1.m1.3b"><apply id="Sx2.SSx2.p1.1.m1.3.3.cmml" xref="Sx2.SSx2.p1.1.m1.3.3"><eq id="Sx2.SSx2.p1.1.m1.3.3.2.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.2"></eq><apply id="Sx2.SSx2.p1.1.m1.3.3.3.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.3"><csymbol cd="ambiguous" id="Sx2.SSx2.p1.1.m1.3.3.3.1.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.3">superscript</csymbol><ci id="Sx2.SSx2.p1.1.m1.3.3.3.2.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.3.2">ùíü</ci><ci id="Sx2.SSx2.p1.1.m1.3.3.3.3.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.3.3">‚Ä≤</ci></apply><set id="Sx2.SSx2.p1.1.m1.3.3.1.2.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.1.1"><interval closure="open" id="Sx2.SSx2.p1.1.m1.3.3.1.1.1.1.cmml" xref="Sx2.SSx2.p1.1.m1.3.3.1.1.1.2"><ci id="Sx2.SSx2.p1.1.m1.1.1.cmml" xref="Sx2.SSx2.p1.1.m1.1.1">ùêº</ci><ci id="Sx2.SSx2.p1.1.m1.2.2.cmml" xref="Sx2.SSx2.p1.1.m1.2.2">ùëå</ci></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.p1.1.m1.3c">\mathcal{D}^{\prime}=\{(I,Y)\}</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx2.p1.1.m1.3d">caligraphic_D start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT = { ( italic_I , italic_Y ) }</annotation></semantics></math>, where <math alttext="I" class="ltx_Math" display="inline" id="Sx2.SSx2.p1.2.m2.1"><semantics id="Sx2.SSx2.p1.2.m2.1a"><mi id="Sx2.SSx2.p1.2.m2.1.1" xref="Sx2.SSx2.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.p1.2.m2.1b"><ci id="Sx2.SSx2.p1.2.m2.1.1.cmml" xref="Sx2.SSx2.p1.2.m2.1.1">ùêº</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.p1.2.m2.1c">I</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx2.p1.2.m2.1d">italic_I</annotation></semantics></math> represents a task instruction and <math alttext="Y" class="ltx_Math" display="inline" id="Sx2.SSx2.p1.3.m3.1"><semantics id="Sx2.SSx2.p1.3.m3.1a"><mi id="Sx2.SSx2.p1.3.m3.1.1" xref="Sx2.SSx2.p1.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="Sx2.SSx2.p1.3.m3.1b"><ci id="Sx2.SSx2.p1.3.m3.1.1.cmml" xref="Sx2.SSx2.p1.3.m3.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx2.p1.3.m3.1c">Y</annotation><annotation encoding="application/x-llamapun" id="Sx2.SSx2.p1.3.m3.1d">italic_Y</annotation></semantics></math> represents a desired response, the training objective of instruction tuning is to minimize the following loss:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="Sx2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{ins}=-\log p_{\theta}(Y|I)," class="ltx_Math" display="block" id="Sx2.E2.m1.1"><semantics id="Sx2.E2.m1.1a"><mrow id="Sx2.E2.m1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.cmml"><mrow id="Sx2.E2.m1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.cmml"><msub id="Sx2.E2.m1.1.1.1.1.3" xref="Sx2.E2.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.E2.m1.1.1.1.1.3.2" xref="Sx2.E2.m1.1.1.1.1.3.2.cmml">‚Ñí</mi><mrow id="Sx2.E2.m1.1.1.1.1.3.3" xref="Sx2.E2.m1.1.1.1.1.3.3.cmml"><mi id="Sx2.E2.m1.1.1.1.1.3.3.2" xref="Sx2.E2.m1.1.1.1.1.3.3.2.cmml">i</mi><mo id="Sx2.E2.m1.1.1.1.1.3.3.1" xref="Sx2.E2.m1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="Sx2.E2.m1.1.1.1.1.3.3.3" xref="Sx2.E2.m1.1.1.1.1.3.3.3.cmml">n</mi><mo id="Sx2.E2.m1.1.1.1.1.3.3.1a" xref="Sx2.E2.m1.1.1.1.1.3.3.1.cmml">‚Å¢</mo><mi id="Sx2.E2.m1.1.1.1.1.3.3.4" xref="Sx2.E2.m1.1.1.1.1.3.3.4.cmml">s</mi></mrow></msub><mo id="Sx2.E2.m1.1.1.1.1.2" xref="Sx2.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="Sx2.E2.m1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.cmml"><mo id="Sx2.E2.m1.1.1.1.1.1a" rspace="0.167em" xref="Sx2.E2.m1.1.1.1.1.1.cmml">‚àí</mo><mrow id="Sx2.E2.m1.1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.1.cmml"><mrow id="Sx2.E2.m1.1.1.1.1.1.1.3" xref="Sx2.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="Sx2.E2.m1.1.1.1.1.1.1.3.1" xref="Sx2.E2.m1.1.1.1.1.1.1.3.1.cmml">log</mi><mo id="Sx2.E2.m1.1.1.1.1.1.1.3a" lspace="0.167em" xref="Sx2.E2.m1.1.1.1.1.1.1.3.cmml">‚Å°</mo><msub id="Sx2.E2.m1.1.1.1.1.1.1.3.2" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.cmml"><mi id="Sx2.E2.m1.1.1.1.1.1.1.3.2.2" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="Sx2.E2.m1.1.1.1.1.1.1.3.2.3" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.3.cmml">Œ∏</mi></msub></mrow><mo id="Sx2.E2.m1.1.1.1.1.1.1.2" xref="Sx2.E2.m1.1.1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="Sx2.E2.m1.1.1.1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="Sx2.E2.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">Y</mi><mo fence="false" id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">I</mi></mrow><mo id="Sx2.E2.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="Sx2.E2.m1.1.1.1.2" xref="Sx2.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx2.E2.m1.1b"><apply id="Sx2.E2.m1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1"><eq id="Sx2.E2.m1.1.1.1.1.2.cmml" xref="Sx2.E2.m1.1.1.1.1.2"></eq><apply id="Sx2.E2.m1.1.1.1.1.3.cmml" xref="Sx2.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx2.E2.m1.1.1.1.1.3.1.cmml" xref="Sx2.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="Sx2.E2.m1.1.1.1.1.3.2.cmml" xref="Sx2.E2.m1.1.1.1.1.3.2">‚Ñí</ci><apply id="Sx2.E2.m1.1.1.1.1.3.3.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3"><times id="Sx2.E2.m1.1.1.1.1.3.3.1.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3.1"></times><ci id="Sx2.E2.m1.1.1.1.1.3.3.2.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3.2">ùëñ</ci><ci id="Sx2.E2.m1.1.1.1.1.3.3.3.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3.3">ùëõ</ci><ci id="Sx2.E2.m1.1.1.1.1.3.3.4.cmml" xref="Sx2.E2.m1.1.1.1.1.3.3.4">ùë†</ci></apply></apply><apply id="Sx2.E2.m1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1"><minus id="Sx2.E2.m1.1.1.1.1.1.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1"></minus><apply id="Sx2.E2.m1.1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1"><times id="Sx2.E2.m1.1.1.1.1.1.1.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.2"></times><apply id="Sx2.E2.m1.1.1.1.1.1.1.3.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3"><log id="Sx2.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.1"></log><apply id="Sx2.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="Sx2.E2.m1.1.1.1.1.1.1.3.2.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="Sx2.E2.m1.1.1.1.1.1.1.3.2.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.2">ùëù</ci><ci id="Sx2.E2.m1.1.1.1.1.1.1.3.2.3.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.3.2.3">ùúÉ</ci></apply></apply><apply id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.2">ùëå</ci><ci id="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="Sx2.E2.m1.1.1.1.1.1.1.1.1.1.3">ùêº</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.E2.m1.1c">\mathcal{L}_{ins}=-\log p_{\theta}(Y|I),</annotation><annotation encoding="application/x-llamapun" id="Sx2.E2.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_i italic_n italic_s end_POSTSUBSCRIPT = - roman_log italic_p start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_Y | italic_I ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Sx2.SSx2.p1.4">By tuning on diverse instruction tasks, the model is able to better comprehend and follow human instructions, and generalize to unseen instructions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SSx3">
<h3 class="ltx_title ltx_title_subsection">Extrapolating LLMs to non-English languages</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx2.SSx3.p1">
<p class="ltx_p" id="Sx2.SSx3.p1.1">LLMs acquire language generation and instruction-following capabilities through pretraining and instruction tuning. However, English holds a dominant position in the field of natural language processing, possessing the most abundant collection of text data from various domains. LLMs trained on English-dominant corpora exhibit inferior performance on other non-English languages. Extrapolating LLMs to non-English languages poses a highly valuable research challenge. Common extrapolation approaches consist of the following three steps: (1) extending the vocabulary to add tokens of the target language, and thus enhancing encoding expressiveness to that language. (2) further pretraining to transfer language generation capabilities of LLMs to the target language. The required training scale for this step is generally on the order of billions of tokens, significantly less than the trillions of tokens needed for training from scratch. (3) conducting SFT in the target language to transfer instruction-following capabilities of LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx2.SSx3.p2">
<p class="ltx_p" id="Sx2.SSx3.p2.1">This paper conducts a comprehensive empirical study of the aforementioned three steps, comparing the performance differences of LLMs before and after vocabulary extension, and under various pretraining and SFT scales. It analyzes the necessity of vocabulary extension and the required training scale for effective transfer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Experimental Setup</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">This paper aims to explore how to effectively transfer the capabilities of language generation and following instruction to a non-English language. Given the rich linguistic resources available in Chinese, comprehensive and in-depth empirical research can be conducted. Therefore, our experiments and analyses commence with Chinese as the starting point, and the observed phenomena are further validated across over ten low-resource languages. In this section, we present the datasets, models, and evaluation methodology employed in our experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="Sx3.SSx1">
<h3 class="ltx_title ltx_title_subsection">Models</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx3.SSx1.p1">
<p class="ltx_p" id="Sx3.SSx1.p1.1">To avoid unnecessary large-scale repetitive pretraining, we employed open-source models trained on varying scales of Chinese corpora. Among these, LLaMA and LLaMA2 serve as checkpoints without undergoing explicit Chinese pretraining, whereas Chinese LLaMA and Chinese LLaMA2 are treated as checkpoints with Chinese pretraining of 30 billion tokens. The scale reaches 100 billion tokens for Open Chinese LLaMA. We employ the performance of these models as references for analysis and comparison.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.p2">
<p class="ltx_p" id="Sx3.SSx1.p2.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx1.p2.1.1">LLaMA</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a class="ltx_ref" href="#bib.bib43" title="">2023a</a>)</cite>: LLaMA is a series of foundation models developed by Meta AI, trained on publicly available English-dominate corpus. The corpus includes CommonCrawl, C4, Github code, Wikipedia, Books, and ArXiv papers, amounting to approximately 1.4 trillion tokens. Among these sources, Wikipedia consists of multilingual text, contributing 4.5% of the total corpus. It covers 20 languages that use either the Latin or Cyrillic scripts. LLaMA achieves state-of-the-art results for foundation models of its size. For example, LLaMA-13B with just 13 billion parameters outperforms the much larger 175B parameter GPT-3 on many NLP benchmarks. We consider LLaMA-7B and LLaMA-13B in our experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.p3">
<p class="ltx_p" id="Sx3.SSx1.p3.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx1.p3.1.1">LLaMA2</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a class="ltx_ref" href="#bib.bib44" title="">2023b</a>)</cite>: LLaMA2 is an enhanced and upgraded version of LLaMA. The upgrades it has received compared to its predecessor include a more robust data cleaning process, a new mix of publicly available pretraining data boasting a 40% increase in size, a doubled context length for improved comprehension, and the implementation of grouped-query attention for the efficiency of inference. These improvements make it a more powerful tool for tackling advanced language understanding tasks. We consider LLaMA2-7B in our experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.p4">
<p class="ltx_p" id="Sx3.SSx1.p4.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx1.p4.1.1">Chinese LLaMA</span> <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a class="ltx_ref" href="#bib.bib13" title="">2023b</a>)</cite>: Chinese LLaMA is an extension of the original LLaMA, designed to enhance its capability in understanding and generating Chinese text. The goal is achieved by integrating a Chinese tokenizer developed using SentencePiece. This tokenizer, with a vocabulary size of <math alttext="49,953" class="ltx_Math" display="inline" id="Sx3.SSx1.p4.1.m1.2"><semantics id="Sx3.SSx1.p4.1.m1.2a"><mrow id="Sx3.SSx1.p4.1.m1.2.3.2" xref="Sx3.SSx1.p4.1.m1.2.3.1.cmml"><mn id="Sx3.SSx1.p4.1.m1.1.1" xref="Sx3.SSx1.p4.1.m1.1.1.cmml">49</mn><mo id="Sx3.SSx1.p4.1.m1.2.3.2.1" xref="Sx3.SSx1.p4.1.m1.2.3.1.cmml">,</mo><mn id="Sx3.SSx1.p4.1.m1.2.2" xref="Sx3.SSx1.p4.1.m1.2.2.cmml">953</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx3.SSx1.p4.1.m1.2b"><list id="Sx3.SSx1.p4.1.m1.2.3.1.cmml" xref="Sx3.SSx1.p4.1.m1.2.3.2"><cn id="Sx3.SSx1.p4.1.m1.1.1.cmml" type="integer" xref="Sx3.SSx1.p4.1.m1.1.1">49</cn><cn id="Sx3.SSx1.p4.1.m1.2.2.cmml" type="integer" xref="Sx3.SSx1.p4.1.m1.2.2">953</cn></list></annotation-xml><annotation encoding="application/x-tex" id="Sx3.SSx1.p4.1.m1.2c">49,953</annotation><annotation encoding="application/x-llamapun" id="Sx3.SSx1.p4.1.m1.2d">49 , 953</annotation></semantics></math>, enables improved handling of Chinese characters. In addition, it employs parameter-efficient fine-tuning techniques <cite class="ltx_cite ltx_citemacro_citep">(Hu et&nbsp;al. <a class="ltx_ref" href="#bib.bib19" title="">2021</a>)</cite> to reduce memory consumption during model training. In our experiments, we consider Chinese LLaMA 7B Plus, which is trained on a corpus of approximately 120GB in size, equivalent to around 30 billion Chinese tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.p5">
<p class="ltx_p" id="Sx3.SSx1.p5.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx1.p5.1.1">Chinese LLaMA2</span> <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a class="ltx_ref" href="#bib.bib12" title="">2023a</a>)</cite>: Chinese LLaMA2 is an advanced iteration of Chinese LLaMA. It utilizes the same corpus and training data as Chinese LLaMA, but employs the foundational model of LLaMA2. Furthermore, the construction of the new version‚Äôs vocabulary and its code implementation have also been optimized. In our experiments, we consider Chinese LLaMA2 7B pretrained on 30 billion Chinese tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx1.p6">
<p class="ltx_p" id="Sx3.SSx1.p6.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx1.p6.1.1">Open Chinese LLaMA</span> <cite class="ltx_cite ltx_citemacro_citep">(OpenLMLab <a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite>: Open Chinese LLaMA is a larger-scale extended version of the original LLaMA. To enhance the LLaMA‚Äôs capabilities of handling Chinese text, Open Chinese LLaMA undergoes further pretraining on a corpus comprising 100 billion tokens. The corpus is composed of texts collected from the internet and subjected to cleaning, along with a subset of English and code data used by the original LLAMA model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx2">
<h3 class="ltx_title ltx_title_subsection">Datasets</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx3.SSx2.p1">
<p class="ltx_p" id="Sx3.SSx2.p1.1">To transfer the language capabilities of LLaMA to the non-English language of interest, we utilize two instruction datasets, namely BELLE and Bactrain-X, for training. The former is employed in experiments related to Chinese, while the latter is utilized for experiments involving other languages.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx2.p2">
<p class="ltx_p" id="Sx3.SSx2.p2.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p2.1.1">BELLE</span> <cite class="ltx_cite ltx_citemacro_citep">(Ji et&nbsp;al. <a class="ltx_ref" href="#bib.bib23" title="">2023</a>)</cite>: BELLE is a large-scale Chinese instruction tuning dataset developed by Lianjia Tech., containing 1.5 million instruction-following example. We removed duplicated and low-quality data, finally retaining 950,000 examples.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx2.p3">
<p class="ltx_p" id="Sx3.SSx2.p3.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p3.1.1">Bactrain-X</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al. <a class="ltx_ref" href="#bib.bib29" title="">2023</a>)</cite>: Bactrian-X contains instructions and responses across 52 languages to facilitate multilingual instruction tuning. It is created by translating 67K English instructions from Alpaca-52k <cite class="ltx_cite ltx_citemacro_citep">(Taori et&nbsp;al. <a class="ltx_ref" href="#bib.bib40" title="">2023</a>)</cite> and Dolly-15k <cite class="ltx_cite ltx_citemacro_citep">(Conover et&nbsp;al. <a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite> datasets into 51 languages, then generating responses with ChatGPT.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="Sx3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Sx3.T1.8">
<tbody><tr class="ltx_tr" id="Sx3.T1.8.9">
<td class="ltx_td ltx_border_tt" id="Sx3.T1.8.9.1"></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Sx3.T1.8.9.2"><span class="ltx_text ltx_font_bold" id="Sx3.T1.8.9.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.9.3">ACC.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.9.4">F.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.9.5">INFO.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.9.6">LC.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.9.7">H.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.9.8">AVG.</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.8.10.1" rowspan="6"><span class="ltx_text" id="Sx3.T1.8.10.1.1">1k SFT</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx3.T1.8.10.2">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a class="ltx_ref" href="#bib.bib43" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.8.10.3">0.482</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.8.10.4">1.194</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.8.10.5">0.858</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.8.10.6">0.614</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.8.10.7">2.970</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx3.T1.8.10.8">1.224</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.1.1">
<td class="ltx_td ltx_align_left" id="Sx3.T1.1.1.1">LLaMA with <math alttext="10K" class="ltx_Math" display="inline" id="Sx3.T1.1.1.1.m1.1"><semantics id="Sx3.T1.1.1.1.m1.1a"><mrow id="Sx3.T1.1.1.1.m1.1.1" xref="Sx3.T1.1.1.1.m1.1.1.cmml"><mn id="Sx3.T1.1.1.1.m1.1.1.2" xref="Sx3.T1.1.1.1.m1.1.1.2.cmml">10</mn><mo id="Sx3.T1.1.1.1.m1.1.1.1" xref="Sx3.T1.1.1.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="Sx3.T1.1.1.1.m1.1.1.3" xref="Sx3.T1.1.1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.1.1.1.m1.1b"><apply id="Sx3.T1.1.1.1.m1.1.1.cmml" xref="Sx3.T1.1.1.1.m1.1.1"><times id="Sx3.T1.1.1.1.m1.1.1.1.cmml" xref="Sx3.T1.1.1.1.m1.1.1.1"></times><cn id="Sx3.T1.1.1.1.m1.1.1.2.cmml" type="integer" xref="Sx3.T1.1.1.1.m1.1.1.2">10</cn><ci id="Sx3.T1.1.1.1.m1.1.1.3.cmml" xref="Sx3.T1.1.1.1.m1.1.1.3">ùêæ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.1.1.1.m1.1c">10K</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.1.1.1.m1.1d">10 italic_K</annotation></semantics></math> pretrain</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.2">0.482</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.3">1.441</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.4">0.829</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.5">0.712</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.6">2.963</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.1.1.7">1.285</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.2.2">
<td class="ltx_td ltx_align_left" id="Sx3.T1.2.2.1">LLaMA with <math alttext="100K" class="ltx_Math" display="inline" id="Sx3.T1.2.2.1.m1.1"><semantics id="Sx3.T1.2.2.1.m1.1a"><mrow id="Sx3.T1.2.2.1.m1.1.1" xref="Sx3.T1.2.2.1.m1.1.1.cmml"><mn id="Sx3.T1.2.2.1.m1.1.1.2" xref="Sx3.T1.2.2.1.m1.1.1.2.cmml">100</mn><mo id="Sx3.T1.2.2.1.m1.1.1.1" xref="Sx3.T1.2.2.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="Sx3.T1.2.2.1.m1.1.1.3" xref="Sx3.T1.2.2.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.2.2.1.m1.1b"><apply id="Sx3.T1.2.2.1.m1.1.1.cmml" xref="Sx3.T1.2.2.1.m1.1.1"><times id="Sx3.T1.2.2.1.m1.1.1.1.cmml" xref="Sx3.T1.2.2.1.m1.1.1.1"></times><cn id="Sx3.T1.2.2.1.m1.1.1.2.cmml" type="integer" xref="Sx3.T1.2.2.1.m1.1.1.2">100</cn><ci id="Sx3.T1.2.2.1.m1.1.1.3.cmml" xref="Sx3.T1.2.2.1.m1.1.1.3">ùêæ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.2.2.1.m1.1c">100K</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.2.2.1.m1.1d">100 italic_K</annotation></semantics></math> pretrain</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.2.2.2">0.587</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.2.2.3">1.952</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.2.2.4">0.881</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.2.2.5">0.991</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.2.2.6">2.973</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.2.2.7">1.477</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.3.3">
<td class="ltx_td ltx_align_left" id="Sx3.T1.3.3.1">LLaMA with <math alttext="1M" class="ltx_Math" display="inline" id="Sx3.T1.3.3.1.m1.1"><semantics id="Sx3.T1.3.3.1.m1.1a"><mrow id="Sx3.T1.3.3.1.m1.1.1" xref="Sx3.T1.3.3.1.m1.1.1.cmml"><mn id="Sx3.T1.3.3.1.m1.1.1.2" xref="Sx3.T1.3.3.1.m1.1.1.2.cmml">1</mn><mo id="Sx3.T1.3.3.1.m1.1.1.1" xref="Sx3.T1.3.3.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="Sx3.T1.3.3.1.m1.1.1.3" xref="Sx3.T1.3.3.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.3.3.1.m1.1b"><apply id="Sx3.T1.3.3.1.m1.1.1.cmml" xref="Sx3.T1.3.3.1.m1.1.1"><times id="Sx3.T1.3.3.1.m1.1.1.1.cmml" xref="Sx3.T1.3.3.1.m1.1.1.1"></times><cn id="Sx3.T1.3.3.1.m1.1.1.2.cmml" type="integer" xref="Sx3.T1.3.3.1.m1.1.1.2">1</cn><ci id="Sx3.T1.3.3.1.m1.1.1.3.cmml" xref="Sx3.T1.3.3.1.m1.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.3.3.1.m1.1c">1M</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.3.3.1.m1.1d">1 italic_M</annotation></semantics></math> pretrain</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.3.3.2">0.735</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.3.3.3">2.071</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.3.3.4">1.002</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.3.3.5">1.046</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.3.3.6">2.957</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.3.3.7">1.562</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.11">
<td class="ltx_td ltx_align_left" id="Sx3.T1.8.11.1">Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a class="ltx_ref" href="#bib.bib13" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.11.2">0.509</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.11.3">1.205</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.11.4">0.811</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.11.5">0.726</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.11.6">2.970</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.11.7">1.244</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.12">
<td class="ltx_td ltx_align_left" id="Sx3.T1.8.12.1">Open Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(OpenLMLab <a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.12.2">1.406</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.12.3">2.584</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.12.4">1.685</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.12.5">1.877</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.12.6">2.989</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.12.7">2.108</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.13">
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.13.1" rowspan="6"><span class="ltx_text" id="Sx3.T1.8.13.1.1">5k SFT</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Sx3.T1.8.13.2">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a class="ltx_ref" href="#bib.bib43" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.13.3">0.450</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.13.4">1.279</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.13.5">0.767</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.13.6">0.612</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.13.7">3.000</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.13.8">1.199</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.4.4">
<td class="ltx_td ltx_align_left" id="Sx3.T1.4.4.1">LLaMA with <math alttext="10K" class="ltx_Math" display="inline" id="Sx3.T1.4.4.1.m1.1"><semantics id="Sx3.T1.4.4.1.m1.1a"><mrow id="Sx3.T1.4.4.1.m1.1.1" xref="Sx3.T1.4.4.1.m1.1.1.cmml"><mn id="Sx3.T1.4.4.1.m1.1.1.2" xref="Sx3.T1.4.4.1.m1.1.1.2.cmml">10</mn><mo id="Sx3.T1.4.4.1.m1.1.1.1" xref="Sx3.T1.4.4.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="Sx3.T1.4.4.1.m1.1.1.3" xref="Sx3.T1.4.4.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.4.4.1.m1.1b"><apply id="Sx3.T1.4.4.1.m1.1.1.cmml" xref="Sx3.T1.4.4.1.m1.1.1"><times id="Sx3.T1.4.4.1.m1.1.1.1.cmml" xref="Sx3.T1.4.4.1.m1.1.1.1"></times><cn id="Sx3.T1.4.4.1.m1.1.1.2.cmml" type="integer" xref="Sx3.T1.4.4.1.m1.1.1.2">10</cn><ci id="Sx3.T1.4.4.1.m1.1.1.3.cmml" xref="Sx3.T1.4.4.1.m1.1.1.3">ùêæ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.4.4.1.m1.1c">10K</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.4.4.1.m1.1d">10 italic_K</annotation></semantics></math> pretrain</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.4.4.2">0.411</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.4.4.3">1.372</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.4.4.4">0.814</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.4.4.5">0.612</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.4.4.6">2.961</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.4.4.7">1.258</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.5.5">
<td class="ltx_td ltx_align_left" id="Sx3.T1.5.5.1">LLaMA with <math alttext="100K" class="ltx_Math" display="inline" id="Sx3.T1.5.5.1.m1.1"><semantics id="Sx3.T1.5.5.1.m1.1a"><mrow id="Sx3.T1.5.5.1.m1.1.1" xref="Sx3.T1.5.5.1.m1.1.1.cmml"><mn id="Sx3.T1.5.5.1.m1.1.1.2" xref="Sx3.T1.5.5.1.m1.1.1.2.cmml">100</mn><mo id="Sx3.T1.5.5.1.m1.1.1.1" xref="Sx3.T1.5.5.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="Sx3.T1.5.5.1.m1.1.1.3" xref="Sx3.T1.5.5.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.5.5.1.m1.1b"><apply id="Sx3.T1.5.5.1.m1.1.1.cmml" xref="Sx3.T1.5.5.1.m1.1.1"><times id="Sx3.T1.5.5.1.m1.1.1.1.cmml" xref="Sx3.T1.5.5.1.m1.1.1.1"></times><cn id="Sx3.T1.5.5.1.m1.1.1.2.cmml" type="integer" xref="Sx3.T1.5.5.1.m1.1.1.2">100</cn><ci id="Sx3.T1.5.5.1.m1.1.1.3.cmml" xref="Sx3.T1.5.5.1.m1.1.1.3">ùêæ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.5.5.1.m1.1c">100K</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.5.5.1.m1.1d">100 italic_K</annotation></semantics></math> pretrain</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.5.5.2">0.488</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.5.5.3">1.922</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.5.5.4">0.876</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.5.5.5">0.977</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.5.5.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.5.5.7">1.493</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.6.6">
<td class="ltx_td ltx_align_left" id="Sx3.T1.6.6.1">LLaMA with <math alttext="1M" class="ltx_Math" display="inline" id="Sx3.T1.6.6.1.m1.1"><semantics id="Sx3.T1.6.6.1.m1.1a"><mrow id="Sx3.T1.6.6.1.m1.1.1" xref="Sx3.T1.6.6.1.m1.1.1.cmml"><mn id="Sx3.T1.6.6.1.m1.1.1.2" xref="Sx3.T1.6.6.1.m1.1.1.2.cmml">1</mn><mo id="Sx3.T1.6.6.1.m1.1.1.1" xref="Sx3.T1.6.6.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="Sx3.T1.6.6.1.m1.1.1.3" xref="Sx3.T1.6.6.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.6.6.1.m1.1b"><apply id="Sx3.T1.6.6.1.m1.1.1.cmml" xref="Sx3.T1.6.6.1.m1.1.1"><times id="Sx3.T1.6.6.1.m1.1.1.1.cmml" xref="Sx3.T1.6.6.1.m1.1.1.1"></times><cn id="Sx3.T1.6.6.1.m1.1.1.2.cmml" type="integer" xref="Sx3.T1.6.6.1.m1.1.1.2">1</cn><ci id="Sx3.T1.6.6.1.m1.1.1.3.cmml" xref="Sx3.T1.6.6.1.m1.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.6.6.1.m1.1c">1M</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.6.6.1.m1.1d">1 italic_M</annotation></semantics></math> pretrain</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.6.6.2">0.682</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.6.6.3">2.085</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.6.6.4">1.039</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.6.6.5">1.008</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.6.6.6">2.969</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.6.6.7">1.623</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.14">
<td class="ltx_td ltx_align_left" id="Sx3.T1.8.14.1">Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a class="ltx_ref" href="#bib.bib13" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.14.2">0.581</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.14.3">1.341</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.14.4">0.899</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.14.5">0.783</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.14.6">2.992</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.14.7">1.432</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.15">
<td class="ltx_td ltx_align_left" id="Sx3.T1.8.15.1">Open Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(OpenLMLab <a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.15.2">1.295</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.15.3">2.481</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.15.4">1.667</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.15.5">1.884</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.15.6">2.969</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.15.7">2.245</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.16">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx3.T1.8.16.1" rowspan="7"><span class="ltx_text" id="Sx3.T1.8.16.1.1">950k SFT</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="Sx3.T1.8.16.2">LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a class="ltx_ref" href="#bib.bib43" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.16.3">1.783</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.16.4">2.767</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.16.5">2.142</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.16.6">2.212</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.16.7">2.993</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx3.T1.8.16.8">2.379</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.7.7">
<td class="ltx_td ltx_align_left" id="Sx3.T1.7.7.1">LLaMA with <math alttext="1M" class="ltx_Math" display="inline" id="Sx3.T1.7.7.1.m1.1"><semantics id="Sx3.T1.7.7.1.m1.1a"><mrow id="Sx3.T1.7.7.1.m1.1.1" xref="Sx3.T1.7.7.1.m1.1.1.cmml"><mn id="Sx3.T1.7.7.1.m1.1.1.2" xref="Sx3.T1.7.7.1.m1.1.1.2.cmml">1</mn><mo id="Sx3.T1.7.7.1.m1.1.1.1" xref="Sx3.T1.7.7.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="Sx3.T1.7.7.1.m1.1.1.3" xref="Sx3.T1.7.7.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.7.7.1.m1.1b"><apply id="Sx3.T1.7.7.1.m1.1.1.cmml" xref="Sx3.T1.7.7.1.m1.1.1"><times id="Sx3.T1.7.7.1.m1.1.1.1.cmml" xref="Sx3.T1.7.7.1.m1.1.1.1"></times><cn id="Sx3.T1.7.7.1.m1.1.1.2.cmml" type="integer" xref="Sx3.T1.7.7.1.m1.1.1.2">1</cn><ci id="Sx3.T1.7.7.1.m1.1.1.3.cmml" xref="Sx3.T1.7.7.1.m1.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.7.7.1.m1.1c">1M</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.7.7.1.m1.1d">1 italic_M</annotation></semantics></math> pretrain</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.7.7.2">1.812</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.7.7.3">2.799</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.7.7.4">2.080</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.7.7.5">2.303</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.7.7.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.7.7.7">2.399</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.8">
<td class="ltx_td ltx_align_left" id="Sx3.T1.8.8.1">LLaMA-EXT with <math alttext="1M" class="ltx_Math" display="inline" id="Sx3.T1.8.8.1.m1.1"><semantics id="Sx3.T1.8.8.1.m1.1a"><mrow id="Sx3.T1.8.8.1.m1.1.1" xref="Sx3.T1.8.8.1.m1.1.1.cmml"><mn id="Sx3.T1.8.8.1.m1.1.1.2" xref="Sx3.T1.8.8.1.m1.1.1.2.cmml">1</mn><mo id="Sx3.T1.8.8.1.m1.1.1.1" xref="Sx3.T1.8.8.1.m1.1.1.1.cmml">‚Å¢</mo><mi id="Sx3.T1.8.8.1.m1.1.1.3" xref="Sx3.T1.8.8.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.T1.8.8.1.m1.1b"><apply id="Sx3.T1.8.8.1.m1.1.1.cmml" xref="Sx3.T1.8.8.1.m1.1.1"><times id="Sx3.T1.8.8.1.m1.1.1.1.cmml" xref="Sx3.T1.8.8.1.m1.1.1.1"></times><cn id="Sx3.T1.8.8.1.m1.1.1.2.cmml" type="integer" xref="Sx3.T1.8.8.1.m1.1.1.2">1</cn><ci id="Sx3.T1.8.8.1.m1.1.1.3.cmml" xref="Sx3.T1.8.8.1.m1.1.1.3">ùëÄ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.T1.8.8.1.m1.1c">1M</annotation><annotation encoding="application/x-llamapun" id="Sx3.T1.8.8.1.m1.1d">1 italic_M</annotation></semantics></math> pretrain</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.8.2">1.591</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.8.3">2.726</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.8.4">1.918</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.8.5">2.164</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.8.6">2.998</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.8.7">2.279</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.17">
<td class="ltx_td ltx_align_left" id="Sx3.T1.8.17.1">Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a class="ltx_ref" href="#bib.bib13" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.17.2">1.808</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.17.3">2.795</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.17.4">2.112</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.17.5">2.313</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.17.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.17.7">2.406</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.18">
<td class="ltx_td ltx_align_left" id="Sx3.T1.8.18.1">Open Chinese LLaMA <cite class="ltx_cite ltx_citemacro_citep">(OpenLMLab <a class="ltx_ref" href="#bib.bib33" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.18.2">1.890</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.18.3">2.858</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.18.4">2.189</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.18.5">2.390</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.18.6">2.993</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.18.7">2.464</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.19">
<td class="ltx_td ltx_align_left" id="Sx3.T1.8.19.1">LLaMA2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al. <a class="ltx_ref" href="#bib.bib44" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.19.2">1.868</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.19.3">2.822</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.19.4">2.171</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.19.5">2.379</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.19.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx3.T1.8.19.7">2.448</td>
</tr>
<tr class="ltx_tr" id="Sx3.T1.8.20">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx3.T1.8.20.1">Chinese LLaMA2 <cite class="ltx_cite ltx_citemacro_citep">(Cui, Yang, and Yao <a class="ltx_ref" href="#bib.bib12" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.8.20.2">1.701</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.8.20.3">2.838</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.8.20.4">2.011</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.8.20.5">2.251</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.8.20.6">3.000</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx3.T1.8.20.7">2.360</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Response quality with different scales of further pretraining and instruction tuning (SFT). ACC., F., LC., H., INFO., and AVG. respectively denote accuracy, fluency, logical coherence, harmlessness, informativeness and their average. Approximately 1 million samples account for around 0.5 billion tokens. The pretraining scales for Chinese LLaMA and Open Chinese LLaMA are 30 billion and 100 billion tokens, respectively.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx3.SSx2.p4">
<p class="ltx_p" id="Sx3.SSx2.p4.1">In order to objectively and comprehensively assess the capabilities of the model, we conduct evaluations from two perspectives: response quality and knowledge level. For the former, we employ the LLM-Eval benchmark and translate it into various low-resource languages to support multi-lingual evaluation. As for the latter, we utilize four widely adopted standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx2.p5">
<p class="ltx_p" id="Sx3.SSx2.p5.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p5.1.1">LLM-Eval</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al. <a class="ltx_ref" href="#bib.bib51" title="">2023a</a>)</cite>: LLM-Eval is a manually constructed benchmark for instruction-following evaluation. It has 453 instruction tasks from 17 major categories, including factual question answering, reading comprehension, frame generation, paragraph rewriting, summarizing, math problem solving, reasoning, poetry generation, programming, and more.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx2.p6">
<p class="ltx_p" id="Sx3.SSx2.p6.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p6.1.1">C-Eval</span> <cite class="ltx_cite ltx_citemacro_citep">(Huang et&nbsp;al. <a class="ltx_ref" href="#bib.bib22" title="">2023b</a>)</cite>: C-Eval is a Chinese evaluation suite with 13948 exam questions across 52 subjects and 4 difficulty levels from middle school to professional exams. It includes STEM, humanities, social science and other topics. C-Eval HARD is a subset of 8 challenging math and science subjects requiring advanced reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx2.p7">
<p class="ltx_p" id="Sx3.SSx2.p7.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p7.1.1">MMLU</span> <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al. <a class="ltx_ref" href="#bib.bib18" title="">2020</a>)</cite>: MMLU measures a LLM‚Äôs ability to learn and apply knowledge across 57 diverse subjects including STEM, humanities, and social sciences. The test covers a wide range of difficulty levels from elementary to advanced professional.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx2.p8">
<p class="ltx_p" id="Sx3.SSx2.p8.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p8.1.1">AGI-Eval</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhong et&nbsp;al. <a class="ltx_ref" href="#bib.bib53" title="">2023</a>)</cite>: AGIEval uses questions from standardized tests taken by millions of people, including college entrance exams, law school admission tests, and professional qualification exams. It has 19 tasks in both English and Chinese.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx3.SSx2.p9">
<p class="ltx_p" id="Sx3.SSx2.p9.1"><span class="ltx_text ltx_font_bold" id="Sx3.SSx2.p9.1.1">Gaokao-Bench</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al. <a class="ltx_ref" href="#bib.bib52" title="">2023b</a>)</cite>: GAOKAO-Bench uses 2811 exam questions from Chinese college entrance exams (Gaokao) from 2010-2022 covering all subjects. It has 1781 multiple choice, 218 fill-in-blank, and 812 open-ended questions across math, Chinese, English, physics, etc.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="Sx3.SSx3">
<h3 class="ltx_title ltx_title_subsection">Evaluation Protocol</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx3.SSx3.p1">
<p class="ltx_p" id="Sx3.SSx3.p1.1">For LLM-Eval, we followed the practice of <cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. (<a class="ltx_ref" href="#bib.bib51" title="">2023a</a>)</cite>, evaluating the response quality of a model through 5 scoring items: accuracy, fluency, informativeness, logicality, and harmlessness. Scores for each aspect range from 0 to 3. We use the prompt shown in Appendix to submit the instruction, model response, and reference answer to GPT-4 for automated evaluation. Based on the results reported by <cite class="ltx_cite ltx_citemacro_citet">Zhang et&nbsp;al. (<a class="ltx_ref" href="#bib.bib51" title="">2023a</a>)</cite>, this evaluation method demonstrates a high degree of consistency with human evaluation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx3.SSx3.p2">
<p class="ltx_p" id="Sx3.SSx3.p2.1">For the four standardized testing benchmarks, we calculate the accuracy metric for model responses. Additionally, we follow the common practice of employing a zero-shot setting for AGI-Eval and GAOKAO-Bench, while using a 5-shot setting for C-Eval and MMLU.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="Sx3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="332" id="Sx3.F2.g1" src="x2.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Knowledge-level evaluation results on four benchmarks.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Main Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">The Impact of Vocabulary Extension on Transfer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">When we aim to enhance the capabilities of a LLM in a specific language, vocabulary extension is an intuitively reasonable approach. In this section, we evaluate the impact of vocabulary extension through the LLM-Eval benchmark, and the experimental results are presented in table <a class="ltx_ref" href="#Sx3.T1" title="Table 1 ‚Ä£ Datasets ‚Ä£ Experimental Setup ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_tag">1</span></a>. Initially, we collected one million Chinese sentences from the internet (approximately 0.5 billion tokens) and further pretrain the original LLaMA without vocabulary extension. Surprisingly, we find that this model significantly ourperform the vocabulary-extended Chinese LLaMA, across settings of 1K, 5K, and 950K instruction tuning. This discovery is thought-privoking, given that the Chinese LLaMA underwent further Chinese pretraining on 30 billion tokens, a much larger volume than our 0.5 billion tokens. Moreover, within the 950K setting, we include results from extending the vocabulary on original LLaMA and training it with the same 0.5 billion tokens, to mitigate the influence of training data discrepancy. The outcomes remain consistent. This indicates that vocabulary extension is not a favorable choice within training scales of tens of billions of tokens. While we don‚Äôt negate the efficacy of vocabulary extension in settings involving larger-scale pre-training (such as trillions of tokens), as reported in other literatures <cite class="ltx_cite ltx_citemacro_citep">(Team <a class="ltx_ref" href="#bib.bib42" title="">2023b</a>)</cite>, this already leans more towards retraining than mere language transfer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Training Scales Required for Effective Transfer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">Training scale constitutes another significant factor influencing the transferability of LLM capabilities, composed of both pretraining scale and instruction tuning scale. Experimental results are shown in table <a class="ltx_ref" href="#Sx3.T1" title="Table 1 ‚Ä£ Datasets ‚Ä£ Experimental Setup ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_tag">1</span></a>. Taking the example of LLaMA (with 10K, 100K, and 1M further pretrain) and Open Chinese LLaMA, the scale of further Chinese pretraining gradually increases from 0 to 100 billion tokens. Under the settings of 1K and 5K instruction tuning, we observed that the response quality improves progressively with the increase in the scale of further pretraining. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Chinese-LLaMA, however, stands as an exception due to the additional factor of vocabulary extension.</span></span></span> However, when the instruction tuning data scale escalates to 950K, we find no significant differences in response quality among the models. Consequently, we hypothesize that more further pretraining could accelerate the model‚Äôs alignment with human instructions, but the mere tens of billions in training scale are insufficient to enable the model to grasp a greater amount of world knowledge. This leads to their convergence at similar response levels. In other words, the enhancement in response quality primarily stems from an improvement in language generation prowess rather than an elevation in knowledge level.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx4.SSx2.p2">
<p class="ltx_p" id="Sx4.SSx2.p2.1">To validate this standpoint, we evaluated the model‚Äôs knowledge level on four widely used standardized test benchmarks. As shown in Figure <a class="ltx_ref" href="#Sx3.F2" title="Figure 2 ‚Ä£ Evaluation Protocol ‚Ä£ Experimental Setup ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_tag">2</span></a>, LLaMA 7B, Chinese LLaMA 7B, and Open Chinese LLaMA 7B perform comparably on C-eval, gaokao-bench, and agi-eval, indicating no significant differences induced by further Chinese pretraining. It is worth noting that despite lacking further pretraining in Chinese, both LLaMA2-7B and LLaMA-13B outperform Open Chinese LLaMA on C-eval, MMLU, and AGI-Eval, suggesting that trillion-level pretraining and larger model sizes may indeed serve as effective pathways for enhancing model knowledge levels.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx3">
<h3 class="ltx_title ltx_title_subsection">How about the Original English Capabilities</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="Sx4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="Sx4.T2.1" style="width:433.6pt;height:121.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(120.5pt,-33.8pt) scale(2.25191558933999,2.25191558933999) ;">
<table class="ltx_tabular ltx_align_middle" id="Sx4.T2.1.1">
<tbody><tr class="ltx_tr" id="Sx4.T2.1.1.1">
<td class="ltx_td ltx_border_tt" id="Sx4.T2.1.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T2.1.1.1.2">L(0)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T2.1.1.1.3">L(10k)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T2.1.1.1.4">L(100k)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T2.1.1.1.5">L(1M)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Sx4.T2.1.1.1.6">Open</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T2.1.1.2.1"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.2.1.1">Chinese</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.2">10.151</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.3">8.697</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.4">6.634</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.5">5.249</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T2.1.1.2.6">3.924</td>
</tr>
<tr class="ltx_tr" id="Sx4.T2.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="Sx4.T2.1.1.3.1"><span class="ltx_text ltx_font_bold" id="Sx4.T2.1.1.3.1.1">English</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T2.1.1.3.2">14.691</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T2.1.1.3.3">15.625</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T2.1.1.3.4">29.553</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T2.1.1.3.5">198.840</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx4.T2.1.1.3.6">15.045</td>
</tr>
</tbody></table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Model perplexity with different further pretraining scales. L denotes LLaMA, with the number in the parentheses indicating the quantity of further pretraining samples. Open denotes Open Chinese LLaMA.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx4.SSx3.p1">
<p class="ltx_p" id="Sx4.SSx3.p1.1">Another issue of interest to us is whether the improvement in Chinese proficiency has an impact on the existing English capabilities. To address this question, we additionally collected 200,000 Chinese samples from the internet and randomly extracted 200,000 English samples from the refinedweb dataset <cite class="ltx_cite ltx_citemacro_citep">(Penedo et&nbsp;al. <a class="ltx_ref" href="#bib.bib34" title="">2023</a>)</cite>. Utilizing these samples, we evaluate the English perplexity and Chinese perplexity of LLaMA models trained on different-scale corpora, as depicted in table <a class="ltx_ref" href="#Sx4.T2" title="Table 2 ‚Ä£ How about the Original English Capabilities ‚Ä£ Main Results ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_tag">2</span></a>. Our findings reveal that with the increase in further pretraining scale, the perplexity of the models decreases steadily in Chinese, yet notably increases in English. This suggests that enhancing the model‚Äôs capabilities solely through a single Chinese corpus comes at the cost of sacrificing the original English proficiency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx4.SSx3.p2">
<p class="ltx_p" id="Sx4.SSx3.p2.1">Furthermore, we conduct perplexity assessments for Open Chinese LLaMA and find that both the Chinese and English perplexities remain low. This outcome is unsurprising, given that its training data incorporates both Chinese and English content, allowing for the decreases of Chinese perplexity without significant elevation in English perplexity. Overall, exclusive reliance on Chinese corpora for transfer training markedly compromises LLaMA‚Äôs original English proficiency, a concern alleviated effectively through multilingual joint training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="Sx4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Sx4.T3.1">
<tbody><tr class="ltx_tr" id="Sx4.T3.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="Sx4.T3.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="Sx4.T3.1.1.1.1">Language</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="Sx4.T3.1.1.2">1k SFT</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="Sx4.T3.1.1.3">65k SFT</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.1">ACC.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.2">F.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.3">INFO.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.4">LC.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.5">H.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.6">AVG.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.7">ACC.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.8">F.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.9">INFO.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.10">LC.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.11">H.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.2.12">AVG.</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="Sx4.T3.1.3.1">Arbic</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.2">0.188</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.3">1.061</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.4">0.191</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.5">0.254</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.6">3.000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.7">0.939</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.8">1.268</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.9">2.499</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.10">1.529</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.11">1.607</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.12">3.000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx4.T3.1.3.13">1.981</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.4">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.4.1">Bengali</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.2">0.046</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.3">0.492</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.4">0.050</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.5">0.041</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.7">0.726</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.8">0.959</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.9">2.257</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.10">1.156</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.11">1.189</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.12">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.4.13">1.712</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.5">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.5.1">Gujarati</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.2">0.061</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.3">0.426</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.4">0.052</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.5">0.063</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.6">2.998</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.7">0.720</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.8">0.683</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.9">1.795</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.10">0.875</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.11">0.790</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.12">2.995</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.5.13">1.428</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.6">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.6.1">Hindi</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.2">0.131</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.3">1.064</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.4">0.147</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.5">0.162</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.7">0.901</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.8">1.014</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.9">2.342</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.10">1.238</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.11">1.240</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.12">2.998</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.6.13">1.766</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.7">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.7.1">Indonesian</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.2">0.398</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.3">1.266</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.4">0.544</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.5">0.438</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.6">2.995</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.7">1.128</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.8">1.659</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.9">2.751</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.10">2.026</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.11">2.012</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.12">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.7.13">2.290</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.8">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.8.1">Malayalam</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.2">0.101</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.3">0.621</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.4">0.103</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.5">0.103</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.7">0.786</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.8">0.906</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.9">2.427</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.10">1.182</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.11">1.197</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.12">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.8.13">1.742</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.9">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.9.1">Marathi</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.2">0.095</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.3">0.781</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.4">0.107</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.5">0.117</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.6">2.998</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.7">0.820</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.8">1.038</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.9">2.476</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.10">1.288</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.11">1.364</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.12">2.998</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.9.13">1.833</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.10">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.10.1">Nepali</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.2">0.151</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.3">0.991</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.4">0.177</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.5">0.146</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.6">2.986</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.7">0.890</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.8">0.969</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.9">2.417</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.10">1.236</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.11">1.285</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.12">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.10.13">1.781</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.11">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.11.1">Swahili</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.2">0.083</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.3">0.712</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.4">0.090</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.5">0.086</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.6">2.998</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.7">0.794</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.8">1.569</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.9">2.707</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.10">1.955</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.11">1.907</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.12">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.11.13">2.228</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.12">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.12.1">Tamil</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.2">0.140</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.3">0.914</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.4">0.176</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.5">0.174</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.6">2.998</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.7">0.880</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.8">0.960</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.9">2.457</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.10">1.198</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.11">1.257</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.12">2.998</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.12.13">1.774</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.13">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.13.1">Telugu</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.2">0.054</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.3">0.560</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.4">0.057</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.5">0.090</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.7">0.752</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.8">0.539</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.9">1.735</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.10">0.674</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.11">0.712</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.12">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.13.13">1.332</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.14">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.14.1">Urdu</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.2">0.057</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.3">0.573</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.4">0.052</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.5">0.071</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.7">0.751</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.8">1.038</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.9">2.443</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.10">1.285</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.11">1.335</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.12">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.14.13">1.820</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.15">
<td class="ltx_td ltx_align_left" id="Sx4.T3.1.15.1">Vietnamese</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.2">0.105</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.3">0.623</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.4">0.126</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.5">0.117</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.6">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.7">0.794</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.8">1.361</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.9">2.595</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.10">1.665</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.11">1.710</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.12">3.000</td>
<td class="ltx_td ltx_align_center" id="Sx4.T3.1.15.13">2.066</td>
</tr>
<tr class="ltx_tr" id="Sx4.T3.1.16">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.1">Average</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.2">0.124</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.3">0.776</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.4">0.144</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.5">0.143</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.6">2.998</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.7">0.837</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.8">1.074</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.9">2.377</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.10">1.331</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.11">1.354</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.12">2.999</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="Sx4.T3.1.16.13">1.827</td>
</tr>
</tbody></table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Evaluation results of model response quality for 13 low-resource languages on the LLM-Eval. ACC., F., LC., H., INFO., and AVG. respectively denote accuracy, fluency, logical coherence, harmlessness, informativeness and their average.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Extending the Analysis to Multiple Languages</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx5.p1">
<p class="ltx_p" id="Sx5.p1.1">In the previous section, our experiments focus on Chinese. To investigate whether similar conclusions could be drawn in other non-English languages, we extend our experiments to 13 low-resource languages. To ensure evaluation consistency, we translate LLM-Eval benchmark into these 13 languages and employ the same evaluation metrics. As shown in table <a class="ltx_ref" href="#Sx4.T3" title="Table 3 ‚Ä£ How about the Original English Capabilities ‚Ä£ Main Results ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_tag">3</span></a>, a significant improvement in response quality for all low-resource languages with the increase in SFT data. Among these languages, Arabic, Indonesian, and Vietnamese exhibited the best performance. Despite all thirteen languages being low-resource, these three languages are more frequently used <cite class="ltx_cite ltx_citemacro_citep">(Scao et&nbsp;al. <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>. As a result, LLaMA encounters them more often (although their overall occurrence is small compared to English), allowing the model to quickly comprehend instructions in these languages. This aligns with the conclusion drawn in the previous section.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="Sx5.p2">
<p class="ltx_p" id="Sx5.p2.2">In the previous section, we observed that extending the vocabulary had a negative impact on language transferability. A plausible hypothesis is the existence of cross-lingual semantic alignment within LLMs, which vocabulary expansion might disrupt. To validate this alignment hypothesis, we fine-tune LLaMA with a dataset of 1k instructions and examine the model‚Äôs output. Excitingly, we observed a certain proportion of code-switching samples. As depicted in figure <a class="ltx_ref" href="#Sx5.F3" title="Figure 3 ‚Ä£ Extending the Analysis to Multiple Languages ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_tag">3</span></a>, these samples‚Äô model responses consist of tokens from multiple languages and are semantically coherent. We have observed that code-switching occurs not only in the transfer process when Chinese is the target language, but also when other 13 low-resource languages are target languages. As shown in figure <a class="ltx_ref" href="#Sx5.F4" title="Figure 4 ‚Ä£ Extending the Analysis to Multiple Languages ‚Ä£ LLaMA Beyond English: An Empirical Study on Language Capability Transfer"><span class="ltx_text ltx_ref_tag">4</span></a>, the proportion of samples with code-switching is approximately between <math alttext="2\%" class="ltx_Math" display="inline" id="Sx5.p2.1.m1.1"><semantics id="Sx5.p2.1.m1.1a"><mrow id="Sx5.p2.1.m1.1.1" xref="Sx5.p2.1.m1.1.1.cmml"><mn id="Sx5.p2.1.m1.1.1.2" xref="Sx5.p2.1.m1.1.1.2.cmml">2</mn><mo id="Sx5.p2.1.m1.1.1.1" xref="Sx5.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.p2.1.m1.1b"><apply id="Sx5.p2.1.m1.1.1.cmml" xref="Sx5.p2.1.m1.1.1"><csymbol cd="latexml" id="Sx5.p2.1.m1.1.1.1.cmml" xref="Sx5.p2.1.m1.1.1.1">percent</csymbol><cn id="Sx5.p2.1.m1.1.1.2.cmml" type="integer" xref="Sx5.p2.1.m1.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.p2.1.m1.1c">2\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.p2.1.m1.1d">2 %</annotation></semantics></math> to <math alttext="5\%" class="ltx_Math" display="inline" id="Sx5.p2.2.m2.1"><semantics id="Sx5.p2.2.m2.1a"><mrow id="Sx5.p2.2.m2.1.1" xref="Sx5.p2.2.m2.1.1.cmml"><mn id="Sx5.p2.2.m2.1.1.2" xref="Sx5.p2.2.m2.1.1.2.cmml">5</mn><mo id="Sx5.p2.2.m2.1.1.1" xref="Sx5.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.p2.2.m2.1b"><apply id="Sx5.p2.2.m2.1.1.cmml" xref="Sx5.p2.2.m2.1.1"><csymbol cd="latexml" id="Sx5.p2.2.m2.1.1.1.cmml" xref="Sx5.p2.2.m2.1.1.1">percent</csymbol><cn id="Sx5.p2.2.m2.1.1.2.cmml" type="integer" xref="Sx5.p2.2.m2.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.p2.2.m2.1c">5\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.p2.2.m2.1d">5 %</annotation></semantics></math>. This indicates that LLaMA might have learned cross-lingual alignment relationships between concepts during the pretraining process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="Sx5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="1079" id="Sx5.F3.g1" src="x3.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Case study of code-switching. Text with a red background represents the non-English target language (Chinese). Text with a cyan background indicates code-switching language in the model‚Äôs output, which could be English, Japanese, Russian or other languages.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="Sx5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="761" id="Sx5.F4.g1" src="x4.png" width="830">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Code-switching rate across languages.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="Sx6.SSx1">
<h3 class="ltx_title ltx_title_subsection">Resource Gap in LLMs</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx6.SSx1.p1">
<p class="ltx_p" id="Sx6.SSx1.p1.1">One of the main challenges of LLMs is the resource gap, as they are mainly pretrained on English corpus and have limited access to data from other languages. English dominates the field of NLP as an extremely high-resource language with the most raw text data from various domains, leaving few of the over 7000 languages of the world represented in the field <cite class="ltx_cite ltx_citemacro_citep">(Joshi et&nbsp;al. <a class="ltx_ref" href="#bib.bib25" title="">2020</a>)</cite>. This creates a disparity in language models‚Äô capability to handle different languages. Previous findings indicate that LLMs have difficulty comprehending and generating non-English texts, particularly in low-resource languages<cite class="ltx_cite ltx_citemacro_citep">(Nguyen et&nbsp;al. <a class="ltx_ref" href="#bib.bib31" title="">2023</a>; Zhu et&nbsp;al. <a class="ltx_ref" href="#bib.bib54" title="">2023</a>; Huang et&nbsp;al. <a class="ltx_ref" href="#bib.bib20" title="">2023a</a>)</cite>. To address the resource gap, several solutions have been proposed or implemented by researchers and practitioners. One possible solution is to increase the amount of data available from various languages and fields, and make it accessible for pretraining and evaluating LLMs <cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al. <a class="ltx_ref" href="#bib.bib30" title="">2022</a>; Chen et&nbsp;al. <a class="ltx_ref" href="#bib.bib6" title="">2022</a>; Cahyawijaya et&nbsp;al. <a class="ltx_ref" href="#bib.bib4" title="">2023</a>)</cite>
. However, this approach incurs significant computational expenses and the resource gap persists. Alternatively, multilingual language models trained on texts from different languages concurrently, such as mBERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al. <a class="ltx_ref" href="#bib.bib14" title="">2019</a>)</cite> and XLM-R <cite class="ltx_cite ltx_citemacro_citep">(Conneau et&nbsp;al. <a class="ltx_ref" href="#bib.bib9" title="">2020a</a>)</cite>, have been introduced to bridge the gap effectively.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="Sx6.SSx2">
<h3 class="ltx_title ltx_title_subsection">Cross-Lingual Transfer</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx6.SSx2.p1">
<p class="ltx_p" id="Sx6.SSx2.p1.1">Multilingual language models have demonstrated a high level of zero-shot or few-shot cross-lingual transferability across a wide range of tasks <cite class="ltx_cite ltx_citemacro_citep">(Wu and Dredze <a class="ltx_ref" href="#bib.bib49" title="">2019</a>; Pires, Schlinger, and Garrette <a class="ltx_ref" href="#bib.bib35" title="">2019</a>; Winata et&nbsp;al. <a class="ltx_ref" href="#bib.bib48" title="">2021b</a>)</cite>. This means that they can acquire the language capability from supervised data in one language and apply it to another without or with few additional training data. The mechanism behind the strong cross-lingual performance has been investigated by the researchers. It has been shown that multilingual language models have inferred universal rules applicable to any language <cite class="ltx_cite ltx_citemacro_citep">(Artetxe, Ruder, and Yogatama <a class="ltx_ref" href="#bib.bib2" title="">2020</a>; Chi, Hewitt, and Manning <a class="ltx_ref" href="#bib.bib7" title="">2020</a>; Conneau et&nbsp;al. <a class="ltx_ref" href="#bib.bib10" title="">2020b</a>)</cite>. Contrary to the common hypothesis that multilingual multilingual language models such as mBERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et&nbsp;al. <a class="ltx_ref" href="#bib.bib14" title="">2019</a>)</cite> rely on a shared subword vocabulary and joint pretraining across multiple languages <cite class="ltx_cite ltx_citemacro_citep">(Pires, Schlinger, and Garrette <a class="ltx_ref" href="#bib.bib35" title="">2019</a>; Cao, Kitaev, and Klein <a class="ltx_ref" href="#bib.bib5" title="">2020</a>; Wu and Dredze <a class="ltx_ref" href="#bib.bib49" title="">2019</a>)</cite>, researchers have developed new understandings on the models, emphasizing the models‚Äô ability to learn universal semantic abstractions <cite class="ltx_cite ltx_citemacro_citep">(Artetxe, Ruder, and Yogatama <a class="ltx_ref" href="#bib.bib2" title="">2020</a>; Chi, Hewitt, and Manning <a class="ltx_ref" href="#bib.bib7" title="">2020</a>)</cite>. In terms of the factors that influence cross-lingual performance, researchers have associated transferability with parameter sharing <cite class="ltx_cite ltx_citemacro_citep">(Conneau et&nbsp;al. <a class="ltx_ref" href="#bib.bib10" title="">2020b</a>; Dufter and Sch√ºtze <a class="ltx_ref" href="#bib.bib16" title="">2020</a>; Wu, Papadimitriou, and Tamkin <a class="ltx_ref" href="#bib.bib50" title="">2022</a>)</cite> and language distance <cite class="ltx_cite ltx_citemacro_citep">(Conneau et&nbsp;al. <a class="ltx_ref" href="#bib.bib10" title="">2020b</a>; Eronen, Ptaszynski, and Masui <a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite>. We here further investigate the cross-lingual transferability of language models with new LLaMA-based experiments, presenting outcomes from a different aspect.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="Sx6.SSx3">
<h3 class="ltx_title ltx_title_subsection">Code-Switching</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx6.SSx3.p1">
<p class="ltx_p" id="Sx6.SSx3.p1.1">Code-switching is a phenomenon in which multilingual speakers switch between languages within a single utterance. Previous work on the performance of multilingual language models on code-switching tasks has shown mixed results. Some studies have suggested that pretrained models fine-tuned for specific code-switching scenarios can achieve state-of-the-art performance for certain language pairs such as English-Spanish and English-Hindi <cite class="ltx_cite ltx_citemacro_citep">(Khanuja et&nbsp;al. <a class="ltx_ref" href="#bib.bib27" title="">2020</a>)</cite>, while others have found that using meta-embeddings can yield better results with fewer parameters <cite class="ltx_cite ltx_citemacro_citep">(Winata, Lin, and Fung <a class="ltx_ref" href="#bib.bib46" title="">2019</a>; Winata et&nbsp;al. <a class="ltx_ref" href="#bib.bib47" title="">2019</a>, <a class="ltx_ref" href="#bib.bib45" title="">2021a</a>)</cite>. In another line of research, code-switching-based methods have been presented to improve the capability of multilingual language models <cite class="ltx_cite ltx_citemacro_citep">(Jiang et&nbsp;al. <a class="ltx_ref" href="#bib.bib24" title="">2020</a>; Tan and Joty <a class="ltx_ref" href="#bib.bib39" title="">2021</a>; Krishnan et&nbsp;al. <a class="ltx_ref" href="#bib.bib28" title="">2021</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="Sx7">
<h2 class="ltx_title ltx_title_section">Conclusions</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx7.p1">
<p class="ltx_p" id="Sx7.p1.1">In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. Specifically, we conducts a comprehensive empirical study to analyze the necessity of vocabulary extension and the required training scale for effective transfer. We find that vocabulary extension is uncessary and that comparable transfer performance to state-of-the-art models can be achieved with less than <math alttext="1\%" class="ltx_Math" display="inline" id="Sx7.p1.1.m1.1"><semantics id="Sx7.p1.1.m1.1a"><mrow id="Sx7.p1.1.m1.1.1" xref="Sx7.p1.1.m1.1.1.cmml"><mn id="Sx7.p1.1.m1.1.1.2" xref="Sx7.p1.1.m1.1.1.2.cmml">1</mn><mo id="Sx7.p1.1.m1.1.1.1" xref="Sx7.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx7.p1.1.m1.1b"><apply id="Sx7.p1.1.m1.1.1.cmml" xref="Sx7.p1.1.m1.1.1"><csymbol cd="latexml" id="Sx7.p1.1.m1.1.1.1.cmml" xref="Sx7.p1.1.m1.1.1.1">percent</csymbol><cn id="Sx7.p1.1.m1.1.1.2.cmml" type="integer" xref="Sx7.p1.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx7.p1.1.m1.1c">1\%</annotation><annotation encoding="application/x-llamapun" id="Sx7.p1.1.m1.1d">1 %</annotation></semantics></math> of the further pretraining data. Additionally, we observe instances of code-switching during the transfer training, suggesting that cross-lingual alignment might have been internalized within the model. Similar results are observed from the extension experiments on the 13 low-resource languages. Our analysis and findings offer assistance and guidance to the community in developing non-English LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Anil, R.; Dai, A.&nbsp;M.; Firat, O.; Johnson, M.; and Lepikhin, D. 2023.

</span>
<span class="ltx_bibblock">PaLM 2 Technical Report.

</span>
<span class="ltx_bibblock">arXiv:2305.10403.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe, Ruder, and Yogatama (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Artetxe, M.; Ruder, S.; and Yogatama, D. 2020.

</span>
<span class="ltx_bibblock">On the Cross-lingual Transferability of Monolingual Representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 4623‚Äì4637. Online: Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.;
Lee, P.; Lee, Y.&nbsp;T.; Li, Y.; Lundberg, S.; Nori, H.; Palangi, H.; Ribeiro,
M.&nbsp;T.; and Zhang, Y. 2023.

</span>
<span class="ltx_bibblock">Sparks of Artificial General Intelligence: Early experiments with
GPT-4.

</span>
<span class="ltx_bibblock">arXiv:2303.12712.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cahyawijaya et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Cahyawijaya, S.; Lovenia, H.; Aji, A.&nbsp;F.; Winata, G.&nbsp;I.; and Wilie, B. 2023.

</span>
<span class="ltx_bibblock">NusaCrowd: Open Source Initiative for Indonesian NLP Resources.

</span>
<span class="ltx_bibblock">arXiv:2212.09648.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao, Kitaev, and Klein (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Cao, S.; Kitaev, N.; and Klein, D. 2020.

</span>
<span class="ltx_bibblock">Multilingual Alignment of Contextual Word Representations.

</span>
<span class="ltx_bibblock">arXiv:2002.03518.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Chen, G.; Ma, S.; Chen, Y.; Zhang, D.; Pan, J.; Wang, W.; and Wei, F. 2022.

</span>
<span class="ltx_bibblock">Towards Making the Most of Multilingual Pretraining for Zero-Shot
Neural Machine Translation.

</span>
<span class="ltx_bibblock">arXiv:2110.08547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi, Hewitt, and Manning (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Chi, E.&nbsp;A.; Hewitt, J.; and Manning, C.&nbsp;D. 2020.

</span>
<span class="ltx_bibblock">Finding Universal Grammatical Relations in Multilingual BERT.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 5564‚Äì5577. Online: Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Hilton, J.; Nakano, R.; Hesse, C.; and
Schulman, J. 2021.

</span>
<span class="ltx_bibblock">Training Verifiers to Solve Math Word Problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em>, abs/2110.14168.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2020a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Conneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.; Wenzek, G.; Guzm√°n, F.;
Grave, E.; Ott, M.; Zettlemoyer, L.; and Stoyanov, V. 2020a.

</span>
<span class="ltx_bibblock">Unsupervised Cross-lingual Representation Learning at Scale.

</span>
<span class="ltx_bibblock">arXiv:1911.02116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et&nbsp;al. (2020b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Conneau, A.; Wu, S.; Li, H.; Zettlemoyer, L.; and Stoyanov, V.
2020b.

</span>
<span class="ltx_bibblock">Emerging Cross-lingual Structure in Pretrained Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 6022‚Äì6034. Online: Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Conover, M.; Hayes, M.; Mathur, A.; Xie, J.; Wan, J.; Shah, S.; Ghodsi, A.;
Wendell, P.; Zaharia, M.; and Xin, R. 2023.

</span>
<span class="ltx_bibblock">Free Dolly: Introducing the World‚Äôs First Truly Open
Instruction-Tuned LLM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui, Yang, and Yao (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Cui, Y.; Yang, Z.; and Yao, X. 2023a.

</span>
<span class="ltx_bibblock">Chinese LLaMA and Alpaca Large Language Models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui, Yang, and Yao (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Cui, Y.; Yang, Z.; and Yao, X. 2023b.

</span>
<span class="ltx_bibblock">Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca.

</span>
<span class="ltx_bibblock">arXiv:2304.08177.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language
Understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, 4171‚Äì4186. Minneapolis,
Minnesota: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.; Sun, X.; Xu, J.; Li,
L.; and Sui, Z. 2023.

</span>
<span class="ltx_bibblock">A Survey on In-context Learning.

</span>
<span class="ltx_bibblock">arXiv:2301.00234.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dufter and Sch√ºtze (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Dufter, P.; and Sch√ºtze, H. 2020.

</span>
<span class="ltx_bibblock">Identifying Elements Essential for BERT‚Äôs Multilinguality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, 4423‚Äì4437. Online: Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eronen, Ptaszynski, and Masui (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Eronen, J.; Ptaszynski, M.; and Masui, F. 2023.

</span>
<span class="ltx_bibblock">Zero-shot cross-lingual transfer language selection using linguistic
similarity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Information Processing &amp; Management</em>, 60(3): 103250.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and
Steinhardt, J. 2020.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em>, abs/2009.03300.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Hu, E.&nbsp;J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; and Chen,
W. 2021.

</span>
<span class="ltx_bibblock">LoRA: Low-Rank Adaptation of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, abs/2106.09685.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Huang, H.; Tang, T.; Zhang, D.; Zhao, W.&nbsp;X.; Song, T.; Xia, Y.; and Wei, F.
2023a.

</span>
<span class="ltx_bibblock">Not All Languages Are Created Equal in LLMs: Improving Multilingual
Capability by Cross-Lingual-Thought Prompting.

</span>
<span class="ltx_bibblock">arXiv:2305.07004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Huang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.

</span>
<span class="ltx_bibblock">Language Models as Zero-Shot Planners: Extracting Actionable
Knowledge for Embodied Agents.

</span>
<span class="ltx_bibblock">In Chaudhuri, K.; Jegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and
Sabato, S., eds., <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 39th International Conference on
Machine Learning</em>, volume 162 of <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">Proceedings of Machine Learning
Research</em>, 9118‚Äì9147. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Huang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; and Zhang, J. 2023b.

</span>
<span class="ltx_bibblock">C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for
Foundation Models.

</span>
<span class="ltx_bibblock">arXiv:2305.08322.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Ji, Y.; Deng, Y.; Gong, Y.; Peng, Y.; Niu, Q.; Ma, B.; and Li, X. 2023.

</span>
<span class="ltx_bibblock">BELLE: Be Everyone‚Äôs Large Language model Engine.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url" href="https://github.com/LianjiaTech/BELLE" title="">https://github.com/LianjiaTech/BELLE</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Jiang, Z.; Anastasopoulos, A.; Araki, J.; Ding, H.; and Neubig, G. 2020.

</span>
<span class="ltx_bibblock">X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained
Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, 5943‚Äì5959. Online: Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Joshi, P.; Santy, S.; Budhiraja, A.; Bali, K.; and Choudhury, M. 2020.

</span>
<span class="ltx_bibblock">The State and Fate of Linguistic Diversity and Inclusion in the NLP
World.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 6282‚Äì6293. Online: Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Katz et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Katz, D.&nbsp;M.; Bommarito, M.&nbsp;J.; Gao, S.; and Arredondo, P. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 passes the bar exam.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Available at SSRN 4389233</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khanuja et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Khanuja, S.; Dandapat, S.; Srinivasan, A.; Sitaram, S.; and Choudhury, M. 2020.

</span>
<span class="ltx_bibblock">GLUECoS: An Evaluation Benchmark for Code-Switched NLP.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, 3575‚Äì3585. Online: Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishnan et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Krishnan, J.; Anastasopoulos, A.; Purohit, H.; and Rangwala, H. 2021.

</span>
<span class="ltx_bibblock">Multilingual Code-Switching for Zero-Shot Cross-Lingual Intent
Prediction and Slot Filling.

</span>
<span class="ltx_bibblock">arXiv:2103.07792.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Li, H.; Koto, F.; Wu, M.; Aji, A.&nbsp;F.; and Baldwin, T. 2023.

</span>
<span class="ltx_bibblock">Bactrian-X : A Multilingual Replicable Instruction-Following Model
with Low-Rank Adaptation.

</span>
<span class="ltx_bibblock">arXiv:2305.15011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Lin, X.&nbsp;V.; Mihaylov, T.; Artetxe, M.; Wang, T.; Chen, S.; Simig, D.; Ott, M.;
Goyal, N.; Bhosale, S.; Du, J.; Pasunuru, R.; Shleifer, S.; Koura, P.&nbsp;S.;
Chaudhary, V.; O‚ÄôHoro, B.; Wang, J.; Zettlemoyer, L.; Kozareva, Z.; Diab, M.;
Stoyanov, V.; and Li, X. 2022.

</span>
<span class="ltx_bibblock">Few-shot Learning with Multilingual Language Models.

</span>
<span class="ltx_bibblock">arXiv:2112.10668.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Nguyen, X.-P.; Aljunied, S.&nbsp;M.; Joty, S.; and Bing, L. 2023.

</span>
<span class="ltx_bibblock">Democratizing LLMs for Low-Resource Languages by Leveraging their
English Dominant Abilities with Linguistically-Diverse Prompts.

</span>
<span class="ltx_bibblock">arXiv:2306.11372.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock">Introducing ChatGPT.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenLMLab (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
OpenLMLab. 2023.

</span>
<span class="ltx_bibblock">Open-Chinese-LLaMA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Penedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Cappelli, A.; Alobeidli,
H.; Pannier, B.; Almazrouei, E.; and Launay, J. 2023.

</span>
<span class="ltx_bibblock">The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora
with Web Data, and Web Data Only.

</span>
<span class="ltx_bibblock">arXiv:2306.01116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires, Schlinger, and Garrette (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Pires, T.; Schlinger, E.; and Garrette, D. 2019.

</span>
<span class="ltx_bibblock">How Multilingual is Multilingual BERT?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, 4996‚Äì5001. Florence, Italy: Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranta and Goutte (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Ranta, A.; and Goutte, C. 2021.

</span>
<span class="ltx_bibblock">Linguistic Diversity in Natural Language Processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Traitement Automatique des Langues</em>, 62(3): 7‚Äì11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Scao, T.&nbsp;L.; Fan, A.; Akiki, C.; Pavlick, E.; Iliƒá, S.; Hesslow, D.; and
Castagn√©, R. 2023.

</span>
<span class="ltx_bibblock">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.

</span>
<span class="ltx_bibblock">arXiv:2211.05100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">StabilityAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
StabilityAI. 2023.

</span>
<span class="ltx_bibblock">Announcing StableCode.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Joty (2021)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Tan, S.; and Joty, S. 2021.

</span>
<span class="ltx_bibblock">Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots.

</span>
<span class="ltx_bibblock">arXiv:2103.09593.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang,
P.; and Hashimoto, T.&nbsp;B. 2023.

</span>
<span class="ltx_bibblock">Alpaca: A Strong, Replicable Instruction-Following Model.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Team, I. 2023a.

</span>
<span class="ltx_bibblock">Internlm: A multilingual language model with progressively enhanced
capabilities.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Team, I. 2023b.

</span>
<span class="ltx_bibblock">InternLM: A Multilingual Language Model with Progressively Enhanced
Capabilities.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url" href="https://github.com/InternLM/InternLM-techreport" title="">https://github.com/InternLM/InternLM-techreport</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix,
T.; Rozi√®re, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin,
A.; Grave, E.; and Lample, G. 2023a.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models.

</span>
<span class="ltx_bibblock">arXiv:2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; and Almahairi, A.
2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock">arXiv:2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winata et&nbsp;al. (2021a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Winata, G.&nbsp;I.; Cahyawijaya, S.; Liu, Z.; Lin, Z.; Madotto, A.; and Fung, P.
2021a.

</span>
<span class="ltx_bibblock">Are Multilingual Models Effective in Code-Switching?

</span>
<span class="ltx_bibblock">arXiv:2103.13309.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winata, Lin, and Fung (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Winata, G.&nbsp;I.; Lin, Z.; and Fung, P. 2019.

</span>
<span class="ltx_bibblock">Learning Multilingual Meta-Embeddings for Code-Switching Named Entity
Recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 4th Workshop on Representation Learning
for NLP (RepL4NLP-2019)</em>, 181‚Äì186. Florence, Italy: Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winata et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Winata, G.&nbsp;I.; Lin, Z.; Shin, J.; Liu, Z.; and Fung, P. 2019.

</span>
<span class="ltx_bibblock">Hierarchical Meta-Embeddings for Code-Switching Named Entity
Recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, 3541‚Äì3547. Hong Kong, China:
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winata et&nbsp;al. (2021b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Winata, G.&nbsp;I.; Madotto, A.; Lin, Z.; Liu, R.; Yosinski, J.; and Fung, P.
2021b.

</span>
<span class="ltx_bibblock">Language Models are Few-shot Multilingual Learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 1st Workshop on Multilingual
Representation Learning</em>, 1‚Äì15. Punta Cana, Dominican Republic: Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Dredze (2019)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Wu, S.; and Dredze, M. 2019.

</span>
<span class="ltx_bibblock">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of
BERT.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, 833‚Äì844. Hong Kong, China:
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu, Papadimitriou, and Tamkin (2022)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Wu, Z.; Papadimitriou, I.; and Tamkin, A. 2022.

</span>
<span class="ltx_bibblock">Oolong: Investigating What Makes Crosslingual Transfer Hard with
Controlled Studies.

</span>
<span class="ltx_bibblock">arXiv:2202.12312.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zhang, M.; Zhang, Q.; Zhang, Y.; and Gui, T. 2023a.

</span>
<span class="ltx_bibblock">LLMEVAL-1 Chinese Large Language Model Evaluation Phase 1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zhang, X.; Li, C.; Zong, Y.; Ying, Z.; He, L.; and Qiu, X. 2023b.

</span>
<span class="ltx_bibblock">Evaluating the Performance of Large Language Models on GAOKAO
Benchmark.

</span>
<span class="ltx_bibblock">arXiv:2305.12474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang, Y.; Saied, A.; Chen, W.;
and Duan, N. 2023.

</span>
<span class="ltx_bibblock">AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models.

</span>
<span class="ltx_bibblock">arXiv:2304.06364.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">‚Üë</button></span>
<span class="ltx_bibblock">
Zhu, W.; Liu, H.; Dong, Q.; Xu, J.; Huang, S.; Kong, L.; Chen, J.; and Li, L.
2023.

</span>
<span class="ltx_bibblock">Multilingual Machine Translation with Large Language Models:
Empirical Results and Analysis.

</span>
<span class="ltx_bibblock">arXiv:2304.04675.

</span>
</li>
</ul>
</section>
<section class="ltx_section" id="Sx8">
<h2 class="ltx_title ltx_title_section">Appendix</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="Sx8.p1">
<p class="ltx_p" id="Sx8.p1.1"><span class="ltx_text ltx_font_bold" id="Sx8.p1.1.1">The prompt for LLM-Eval</span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p2">
<p class="ltx_p" id="Sx8.p2.1">For questions related to specific subject matters, we score the following LLM outputs using a scale of 0/1/2/3:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p3">
<p class="ltx_p" id="Sx8.p3.1">Accuracy: Evaluates whether the answer is precise and correct, with accurate information. A high-quality answer should be factually reliable.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p4">
<p class="ltx_p" id="Sx8.p4.1">Fluency: Assesses how well the answer adheres to human language conventions, with coherent phrasing and clear expression. A high-quality answer should be easy to understand, without convoluted or difficult-to-parse sentences.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p5">
<p class="ltx_p" id="Sx8.p5.1">Informativeness: Judges if the answer provides sufficient relevant information of practical value. A high-quality answer should offer useful and pertinent information to the questioner.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p6">
<p class="ltx_p" id="Sx8.p6.1">Logicality: Examines if the answer is logically sound and accurate in presenting viewpoints and arguments. A highquality answer should follow logical principles, demonstrating clear reasoning and coherence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p7">
<p class="ltx_p" id="Sx8.p7.1">Harmlessness: Assesses whether the answer avoids unethical or harmful content, adhering to ethical norms. A highquality answer should uphold ethical principles, avoiding the propagation of harmful or immoral information.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p8">
<p class="ltx_p" id="Sx8.p8.1">Note: If the model provides no response, all scores except for ‚ÄúHarmlessness‚Äù should be 0.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p9">
<p class="ltx_p" id="Sx8.p9.1">The question is: Question The LLM response is: Response</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p10">
<p class="ltx_p" id="Sx8.p10.1">The reference answer for this question is: Reference Answer</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p11">
<p class="ltx_p" id="Sx8.p11.1">Please provide an answer in the following format, assigning your perceived scores for LLM response‚Äôs ‚Äúaccuracy‚Äù, ‚Äúfluency‚Äù, ‚Äúinformativeness‚Äù, ‚Äúlogicality‚Äù, and ‚Äúharmlessness‚Äù on a scale of 0/1/2/3:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p12">
<p class="ltx_p" id="Sx8.p12.1">‚ÄúAccuracy‚Äù: score for LLM response‚Äôs accuracy (integer),</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p13">
<p class="ltx_p" id="Sx8.p13.1">‚ÄúFluency‚Äù: score for LLM response‚Äôs fluency (integer),</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p14">
<p class="ltx_p" id="Sx8.p14.1">‚ÄúInformativeness‚Äù: score for LLM response‚Äôs informativeness (integer),</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p15">
<p class="ltx_p" id="Sx8.p15.1">‚ÄúLogicality‚Äù: score for LLM response‚Äôs logicality (integer),</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p16">
<p class="ltx_p" id="Sx8.p16.1">‚ÄúHarmlessness‚Äù: score for LLM response‚Äôs harmlessness (integer).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# Mustango: Controllable Text-to-Music Generation을 향하여\n' +
      '\n' +
      'Jan Melechovsky\\({}^{1}\\), Zixun Guo\\({}^{2}\\), Deepanway Ghosal\\({}^{1}\\),\n' +
      '\n' +
      '**Navonil Majumder\\({}^{1}\\), Dorien Herremans\\({}^{1}\\), Soujanya Poria\\({}^{1}\\)**\n' +
      '\n' +
      '\\({}^{1}\\) 싱가포르 기술디자인대학\n' +
      '\n' +
      '\\({}^{2}\\) Queen Mary University of London\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '최근 잠재 확산 모델을 기반으로 한 텍스트-오디오 및 텍스트-음악의 발전으로 생성된 콘텐츠의 품질이 새로운 수준에 도달하고 있다. 그러나 텍스트-음악 시스템에서 음악적 측면의 제어 가능성은 아직 명시적으로 탐구되지 않았다. 본 논문에서는 Tango 텍스트-to-audio 모델을 확장한 확산 기반 음악 영역-지식-영감 텍스트-to-음악 시스템인 Mustango를 제안한다. 무스탄고는 일반적인 텍스트 캡션뿐만 아니라 화음, 박자, 템포, 키와 관련된 특정 지시를 포함할 수 있는 더 풍부한 캡션에서 생성된 음악을 제어하는 것을 목표로 한다. Mustango의 일환으로, 본 논문에서는 일반적인 텍스트 임베딩뿐만 아니라 텍스트 프롬프트로부터 예측되는 음악 특정 특징들을 확산 잡음 제거 과정에 통합하기 위한 Music-Domain-Knowledge-Informed UNet 서브 모듈인 MuNet을 제안한다. 본 논문에서는 텍스트 캡션을 가진 음악의 오픈 데이터셋의 제한된 가용성을 극복하기 위해, 음악 오디오의 조화, 리듬, 동적 측면을 변경하고 최신 음악 정보 검색 방법을 사용하여 텍스트 형식의 기존 설명에 추가될 음악 특징을 추출하는 새로운 데이터 증강 방법을 제안한다. 52K 이상의 인스턴스를 포함하고 자막 텍스트에 음악 이론 기반 설명을 포함하는 결과 MusicBench 데이터 세트를 출시한다. 다양한 실험을 통해 무스탱고가 생성한 음악의 품질이 최첨단임을 보이고, 음악별 텍스트 프롬프트를 통한 제어 가능성이 여러 데이터셋에서 원하는 화음, 박자, 키, 템포 측면에서 다른 모델보다 크게 우수함을 보인다.\n' +
      '\n' +
      '+\n' +
      '각주 †: 두 작가 모두 동등하게 기여했고 이 프로젝트를 주도했다.\n' +
      '\n' +
      '+\n' +
      '각주 †: 두 작가 모두 동등하게 기여했고 이 프로젝트를 주도했다.\n' +
      '\n' +
      '+\n' +
      '각주 †: 두 작가 모두 동등하게 기여했고 이 프로젝트를 주도했다.\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 몇 년 동안, 확산 모델 Popov et al.(2021)은 이미지(OpenAI, 2023) 및 오디오 Liu et al.(2023); Ghosal et al.(2023); Borsos et al.(2023) 생성 작업에서 진전을 보여 왔다. 또한, 확산 모델을 사용하여 음악 Huang 등(2023); Schneider 등(2023)을 생성하려는 여러 시도가 있었다. 오디오의 영역 내에서 음악은 리듬적 복잡성과 독특한 조화 또는 선율 구조를 특징으로 하는 고유한 공간을 차지한다. 따라서 본 논문은 음악 영역 지식을 갖춘 확산 모델의 힘을 활용하여 텍스트 프롬프트에서 직접 오디오 음악 조각을 생성하는 것을 목표로 한다.\n' +
      '\n' +
      '확산 모델에서 직접 음악을 생성하는 것은 음악의 고유한 특성으로 인해 새로운 도전을 제시한다. 첫째, 조건 텍스트와의 정렬과 생성된 음악에서의 음악성 사이의 균형을 이루는 것은 사소하지 않다. 최근에, Agostinelli 등(2023)은 생성된 음악이 입력 텍스트(예를 들어, 정확한 악기, 음악 바이브)와 일치하도록 하기 위해 MusicLM을 제안했다. 그러나 음악적으로 의미 있는 화성과 일관된 연주 속성(예: 템포)과 같은 음악성의 문제는 부분적으로만 다루어지고 있다. 둘째, 쌍을 이루는 음악 및 텍스트 기술 데이터 세트의 가용성은 Agostinelli 등(2023); Huang 등(2023)에 한정되어 있다. 기존 데이터 세트의 텍스트 설명에는 악기 또는 분위기와 같은 세부 정보가 포함되지만 음악의 구조적, 선율적 및 화성적 측면을 캡처하는 보다 재현적인 설명이 기존 데이터 세트에서는 누락되었다. 따라서 우리는 세대 동안 이 정보를 포함하면 음악성(메트릭 구조, 코드 진행) 및 제어 가능성 측면에서 현재의 텍스트-음악 시스템을 개선할 수 있다고 주장한다. 제안된 무스탱고 모델은 기존의 텍스트-음악 시스템의 기능(예: 올바른 악기 설정)을 넘어 음악가, 프로듀서 및 사운드 디자이너가 코드 진행, 템포 설정 및 키 선택과 같은 특정 조건을 가진 음악 클립을 생성할 수 있도록 한다.\n' +
      '\n' +
      '본 논문에서는 이러한 문제를 해결하기 위해 새로운 데이터 증강 파이프라인인 뮤넷(Music-Domain-Knowledge-Informed UNet) 모듈인 뮤넷(MuNet)을 제안한다. 데이터 증강 방법에는 _설명 강화_ 및 _음악 다양화_ 두 가지 주요 구성 요소가 있습니다. _설명 보강_의 목적은 비트와 다운비트 위치, 기본 코드 진행, 키 및 템포를 제어 정보로 사용하여 기존 텍스트 설명을 보강하는 것입니다. 추론하는 동안, 이러한 추가적인 설명 텍스트는 음악 생성을 사용자 지정 음악 품질로 성공적으로 이끌 수 있다. MIR(state-of-the-art music information retrieval) 방법 Mauch and Dixon(2010); Heydari 등(2021); Bogdanov 등(2013)을 사용하여 훈련 데이터로부터 이러한 제어 정보를 추출한다. 이어서, 이러한 정보(텍스트 포맷으로)를 기존의 텍스트 설명에 추가하고 ChatGPT를 사용하여 일관성 있고 묘사적인 텍스트로 재구성한다. 또한, 훈련 세트에서 음악 샘플을 다양화하기 위해 이 데이터 세트를 음악의 리듬, 조화 및 해석 측면을 본질적으로 결정하는 템포, 피치1 및 볼륨의 세 가지 측면을 따라 변경된 기존 음악의 변형으로 보강한다. 텍스트 설명도 그에 따라 변경됩니다. 결과적으로 텍스트 설명뿐만 아니라 더 다양한 음악적 특성을 포함하여 원본 데이터 세트의 크기를 11배 증가시킨다. 우리는 이 증강 데이터 세트를 뮤직벤치라고 부릅니다.\n' +
      '\n' +
      '각주 1: [https://github.com/bmcfee/pyrubberband](https://github.com/bmcfee/pyrubberband)\n' +
      '\n' +
      '제안된 확산 기반 생성 모델은 일반적인 UNet과 비교하여 새로운 MuNet을 통합한다. 제안된 MuNet을 통해 추출된 음악 도메인 정보인 화음, 비트, 키, 템포를 텍스트 조건과 함께 비텍스트 형식으로 활용하여 사용자가 지정한 음악 품질로 역확산하는 동안 음악 생성을 안내할 수 있다. SS5의 결과는 이것이 사용자 입력(예: 화음 변경)을 통해 보다 음악적으로 의미 있는 생성 및 향상된 제어 가능성을 초래한다는 것을 시사한다.\n' +
      '\n' +
      '본 논문의 전반적인 기여도는 다음과 같이 요약된다.\n' +
      '\n' +
      '1. 화음 정보를 포함하는 것과 같이 음악에 특화된 텍스트 캡션을 이해할 수 있는 텍스트-음악 확산 모델인 무스탱고를 개발한다. 무스탄고는 역확산 시 일반 텍스트 정보뿐만 아니라 박자, 화음 등으로 음악 생성을 명시적으로 안내하는 전용 뮤넷 모듈을 포함한다.\n' +
      '2. 음악 오디오뿐만 아니라 음악 특정 설명(예를 들어, 화음, 키, 박자 등에 관한)을 포함하는 텍스트 캡션을 포함하는 오픈, 대형 데이터세트인 MusicBench를 출시한다. 이는 음악 오디오를 조화, 템포 및 볼륨의 관점에서 변경할 수 있을 뿐만 아니라 비트 및 다운비트 위치, 기본 코드 진행, 키 및 템포와 같은 텍스트 캡션에 추가 제어 정보를 추가할 수 있는 음악 특정 텍스트 증강을 수행할 수 있는 새로운 증강 방법을 기반으로 한다. 이러한 풍부한 데이터에 대한 훈련은 보다 견고하고 제어가능한 음악 생성을 가능하게 한다.\n' +
      '3. 실험을 통해 최종 모델인 무스탱고가 텍스트 캡션을 기반으로 고품질의 음악을 생성할 수 있음을 확인하였다. 우리는 또한 무스탱고가 텍스트 전용 안내와 달리 생성된 음악의 코드와 박자에 대해 더 강력한 텍스트 제어를 가능하게 한다는 것을 확인했다.\n' +
      '\n' +
      '새로 출시된 MusicBench 데이터셋과 Mustango 모델 구현은 온라인 2에서 사용할 수 있다. 다음으로 기존 최신 텍스트-음악 모델에 대해 먼저 논의하고 새로운 데이터셋 MusicBench에 대한 섹션이 이어진다. SS3에서는 제안된 무스탱고 모델의 세부 사항을 설명한다. 그 다음 광범위한 실험, 결과 및 최종적으로 결론을 도출한다.\n' +
      '\n' +
      '각주 2: [https://github.com/amaai-lab/mustango](https://github.com/amaai-lab/mustango)\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '이 절에서는 텍스트 대 오디오 생성에 대한 기존의 최첨단 연구를 설명하는 것으로 시작하여 텍스트 대 음악 생성의 보다 구체적인 영역을 설명한다.\n' +
      '\n' +
      '음향 생성 분야에서 AudioLM 모델 Borsos et al.(2023)은 최첨단의 시맨틱 모델링 모델 w2v-Bert Chung et al.(2021)과 음향 모델 SoundStream Zeghidour et al.(2022)을 활용하여 오디오 프롬프트로부터 오디오를 계층적 접근 방식으로 생성한다. 시맨틱 토큰은 먼저 w2v-Bert에 의해 생성되고, 이어서 사운드스트림을 사용하여 디코딩될 음향 토큰의 생성을 조건화하는 데 사용된다.\n' +
      '\n' +
      'AudioLDM Liu 등(2023)은 CLAP Wu 등(2023), 공동 오디오-텍스트 표현 모델 및 잠재 확산 모델(LDM)을 활용하는 텍스트 대 오디오 프레임워크이다. 보다 구체적으로, LDM은 VAE를 사용하여 획득된 멜스펙트로그램의 잠재 표현들을 생성하도록 훈련된다. 확산 동안 CLAP 임베딩은 생성을 안내하는 데 사용된다. Tango Ghosal 등(2021,2023)은 AudioLDM으로부터 사전 트레이닝된 VAE를 레버리지하고 CLAP 모델을 명령어 미세-튜닝된 대형 언어 모델: FLAN-T5로 대체하여 훨씬 더 작은 데이터세트로 트레이닝하면서 비교가능한 또는 더 나은 결과를 달성한다.\n' +
      '\n' +
      '음악 생성 분야에서는 생성된 MIDI 음악의 오랜 역사가 있다(Herremans et al., 2017). MIDI를 사용하면 제작자가 디지털 오디오 워크스테이션에서 작업하는 데 유용할 수 있지만 데이터 세트가 매우 제한적이라는 단점이 있다. 그러나 작년에는 MusicLM(Agostinelli et al., 2023)과 같이 텍스트 캡션으로부터 _audio_ 음악을 직접 생성하는 모델들이 등장하였다. 이 모델은 공동 텍스트-음악 임베딩 모델인 MuLan(Huang et al., 2022)과 마스킹된 언어 모델인 w2v-Bert(Chung et al., 2021)의 두 개의 사전 훈련된 모델을 사용하여 음악 생성 동안 합성 품질과 일관성을 모두 유지하는 과제를 해결한다. 이 두 개의 사전 트레이닝된 모델들은 이어서 음향 모델 사운드스트림(Zeghidour 등, 2022)을 컨디셔닝하는데 활용되며, 이는 차례로 음향 토큰들을 자동으로 생성할 수 있다. 그런 다음 이러한 음향 토큰은 사운드스트림에 의해 디코딩되어 최종 오디오 출력이 된다. MusicLM은 기존의 상용화된 텍스트-투-뮤직 소프트웨어인 Mubert3와 Rifusion4보다 Frechet Audio Distance, Faithfulness to the text description, KL divergence, Mulan Cycle Consistency 측면에서 우수한 성능을 보였다. 이 후자의 두 시스템과 연결된 출판물이 없기 때문에 모델 세부 정보를 사용할 수 없다.\n' +
      '\n' +
      '각주 3: [https://github.com/MubertAI/Mubert-Text-to-Music](https://github.com/MubertAI/Mubert-Text-to-Music)\n' +
      '\n' +
      '각주 4: [https://www.rifusion.com/](https://www.rifusion.com/)\n' +
      '\n' +
      '또 다른 텍스트-투-뮤직 모델은 Noise2Music(Huang et al., 2023)이다. 모델에 대한 트레이닝 데이터를 획득하기 위해, 저자들은 대규모 언어 모델인 LaMDA-LF(Thoppilan et al., 2022)가 다수의 일반 후보 텍스트 기술들을 생성하기 위해 사용되는 대량의 페어링된 음악 및 텍스트 데이터를 획득하는 방법을 제안한다. 그런 다음 앞서 언급한 공동 텍스트-음악 임베딩 MuLan을 사용하여 기존 음악 데이터에 대한 최상의 후보를 선택한다. 획득된 음악 및 텍스트 쌍은 이어서 2-스테이지 확산 모델을 트레이닝하는 데 사용되며, 여기서 제1 확산 모델은 중간 표현을 생성하고 제2는 최종 오디오 출력을 생성한다.\n' +
      '\n' +
      '최근 몇 달 동안, 많은 텍스트-음악 모델들이 나왔다. Schneider 등(2023)은 첫 번째 확산 크기 오토인코더(DMAE)가 음악의 의미 있는 잠재 표현을 학습(입력보다 64배 작음)하는 2단계 확산 모델을 제안하는 반면, 두 번째 확산 모델에서는 첫 번째 단계에서 획득한 잠재와 함께 텍스트 조건이 포함되어 최종 음악 생성을 안내한다. MusicGen(Copet et al., 2023)은 출력에 대한 고품질 생성 및 더 나은 제어 가능성을 달성하기 위해 효율적인 토큰 인터리빙 패턴을 갖는 단일-스테이지 트랜스포머 LM을 이용한다. MusicGen은 텍스트 프롬프트에 의해, 또는 크로마그램 형태의 오디오 단편에 의해 컨디셔닝될 수 있다. 시스템은 허가된 데이터 세트로 훈련되었다. JEN-1 모델(Li et al., 2023)은 텍스트 유도 음악 생성, 음악 인페인팅, 지속 등 다양한 작업을 수행하도록 설계된 전방위 확산 모델이다. 또 다른 흥미로운 최근의 모델은 비디오 및 텍스트 입력 모두에 조건화된, 비디오를 보완하기 위한 음악 곡들을 생성하는 것에 초점을 맞춘 Su 등(2023)의 것이다. 비디오 컨디셔닝은 텍스트와 달리 음악에 중요한 비트와 감정 등 시간적 정보를 많이 담을 수 있다.\n' +
      '\n' +
      '화음, 템포, 키, 박자 등의 음악적 용어로 텍스트-음악 시스템의 제어 가능성에 관해서는 연구가 부족하였다. 또한, 텍스트-투-뮤직의 대부분의 연구는 내부 데이터 세트를 사용하고 코드를 공개하지 않는다. 이 작품에서 우리는 음악적 통제성을 구체적으로 목표로 하고 우리의 모든 기여를 공개함으로써 이 연구 격차를 메우기 위한 기초를 마련한다.\n' +
      '\n' +
      '## 3 데이터 세트 만들기\n' +
      '\n' +
      '이 섹션에서는 뮤직벤치 데이터 세트의 생성에 대해 설명한다. 먼저, 음악 특징 추출 및 데이터 증강 방법을 소개하고, 데이터셋의 세부 정보와 이를 적용한 방법에 대해 논의한다.\n' +
      '\n' +
      '### 기능 추출 및 설명 강화\n' +
      '\n' +
      '음악 생성을 안내하고 원본 텍스트 프롬프트를 향상시키는 4가지 일반적인 음악 기능인 비트 및 다운비트, 코드, 키 및 템포를 추출합니다.\n' +
      '\n' +
      '비트넷(Heydari et al., 2021)을 이용하여 비트 및 다운비트 특징을 추출한다. \\(b\\in\\mathcal{R}^{L_{beats}\\times 2}\\) 여기서 첫 번째 차원은 미터(예: 1, 2, 3)에 따른 박의 종류를 나타내고 두 번째 차원은 초 단위의 각 해당 박의 타이밍을 나타낸다. 두 번째 특징인 BPM(Tempo in Beats Per Minute)은 박자 사이의 시간 간격의 역수를 평균하여 추정한다. 코디노(Mauch and Dixon, 2010)는 화음의 특징을 추출하는데 사용된다. \\(c\\in\\mathcal{R}^{L_{ chords}\\times 3}\\) 여기서 첫 번째 차원은 화음 시퀀스의 근을 나타내고, 두 번째 차원은 화음 유형(예: 메이저, 마이너, maj7 등)을 나타내고, 세 번째 차원은 화음의 반전 여부를 나타낸다. 마지막으로, Essentia의 (Bogdanov et al., 2013) KeyExtractor algorithm5를 이용하여 키를 추출한다. 이러한 추출된 특징은 텍스트 설명을 풍부하게 하고 역확산 과정을 안내하는 데 사용될 것이다.\n' +
      '\n' +
      '각주 5: [https://essentia.upf.edu/reference/std_KeyExtractor.html](https://essentia.upf.edu/reference/std_KeyExtractor.html)\n' +
      '\n' +
      '그런 다음, 이러한 특징들은 여러 텍스트 템플릿에 따라 텍스트 형식으로 표현된다(예를 들어, \'노래는 A단조의 키에 있다. 이 노래의 템포는 아다지오이다. 박자 수는 4이다. 화음 진행은 Am, Cmaj7, G.\'). 이를 제어 문장이라고 하며, 이를 원문 프롬프트에 추가하여 향상된 프롬프트를 형성한다. 다른 컨트롤 문장 템플릿의 전체 목록은 부록에서 찾을 수 있습니다.\n' +
      '\n' +
      '### 확장 및 음악 다양화\n' +
      '\n' +
      '이 섹션에서는 모델의 오디오 품질과 제어 가능성을 높이기 위해 총 학습 데이터 양을 11배 증가시키는 음악 오디오 및 텍스트 프롬프트에 대한 데이터 세트 증강을 소개한다. 표준 텍스트 대 오디오 증강은 음악 오디오의 특성에 맞지 않을 수 있습니다. 예를 들어, 탱고(Ghosal et al., 2023)에서 사용된 증강은, 유사한 오디오 압력 레벨의 두 오디오 샘플이 중첩되고 그들의 프롬프트가 연결됨으로써, 두 개의 중첩된 리듬, 조화에서의 불협화음, 및 전반적인 음악 개념 불일치를 도입함으로써 음악에 대해 작동하지 않을 것이다.\n' +
      '\n' +
      '따라서 음악의 선율, 리듬 및 동적 측면을 결정하는 피치, 속도 및 볼륨의 세 가지 관점 중 하나에서 단일 음악 _오디오_ 샘플을 증강한다. PyRubberband6을 이용하여 음성의 음정을 균일한 분포를 따르는 \\(\\pm\\)3 반음소 범위 내에서 이동시킨다. 또한, 음악 오디오의 속도를 균일 분포로부터 구한 \\(\\pm\\)(5에서 25)%까지 변화시켰다. 마지막으로, 최소 볼륨을 원래의 트랙 진폭의 0.1배에서 0.5배까지 균일한 분포에서 끌어낸 최소 볼륨으로 점진적인 볼륨 변화(크레센도와 데크레센도 모두)를 도입하여 오디오의 볼륨을 변경하고 최대 볼륨은 그대로 유지한다. 우리는 동시 연구에서 유사한 증강 접근법을 주목한다(Gardner et al., 2023).\n' +
      '\n' +
      '각주 6: [https://github.com/bmcfee/pyrubberband](https://github.com/bmcfee/pyrubberband)\n' +
      '\n' +
      '이에 대응하는 텍스트 설명은 다음과 같이 변경하기로 한다. 우리는 우리가 저지른 변경 사항에 따라 이전 섹션의 향상된 프롬프트를 변경했습니다. 모델의 견고성을 높이기 위해 앞서 언급한 네 가지 음악 특징을 설명하는 프롬프트에서 1~4개의 문장을 무작위로 폐기한다. 자세한 내용은 부록에 나와 있습니다. 마지막으로 ChatGPT를 사용하여 텍스트 프롬프트를 다시rase하여 텍스트 프롬프트에 다양성을 추가했다.\n' +
      '\n' +
      '### MusicBench\n' +
      '\n' +
      '본 연구에서는 음악을 특징으로 하는 5,521개의 오디오 클립으로 구성된 MusicCaps(Agostinelli et al., 2023) 데이터셋을 사용한다. 각 클립은 10초 길이이며 AudioSet(Gemmeke 등, 2017) 데이터세트의 열차 및 평가 분할로부터 소스된다. 이 오디오 클립에는 음악을 설명하는 평균 4문장 길이의 텍스트가 함께 제공됩니다. 그러나 일부 오디오 파일의 액세스 불가능으로 인해 데이터 세트는 5,479개의 샘플로 구성되어 있습니다.\n' +
      '\n' +
      '우리는 그림 1과 같이 데이터 세트를 분할한다. 먼저 모든 샘플에서 음악 특징(나중에 사용하기 위해)을 추출하고 데이터를 TrainA와 TestA 세트로 분할한다. 4개의 제어 문장을 모두 원래의 프롬프트에 연결함으로써 TrainB와 TestB 집합을 얻는다. 그럼...\n' +
      '\n' +
      '도 1: MusicBench 데이터세트의 구성.\n' +
      '\n' +
      'TrainB 텍스트 프롬프트를 다시 말하면, 우리는 최종 TrainC 세트를 얻는다.\n' +
      '\n' +
      '또한 오디오 증강을 수행하기 전에 TrainA 집합의 캡션에서 \'저품질\'과 유사한 용어를 언급하는 샘플을 필터링하여 3,413개의 인스턴스를 얻는다. 이러한 더 높은 품질의 샘플들은 37k 샘플들의 세트를 형성하기 위해 오디오-증강된다(SS3.2 참조). 그런 다음 원래 캡션과 연결할 컨트롤 프롬프트를 무작위로 선택합니다. 우리는 각각 \\(25/30/20/15/10\\%\\)의 확률로 \\(0/1/2/3/4\\) 프롬프트를 선택한다. 그런 다음 ChatGPT를 사용하여 모든 캡션을 다시 바꿉니다. 최종 학습 데이터 세트에서는 각각 \\(85/15\\%\\)의 확률로 재구성된 프롬프트와 재구성되지 않은 프롬프트를 사용한다. 마지막으로 이 증강된 세트를 사용하여 세트 TrainA, TrainB 및 TrainC와 연결하여 MusicBench라고 하는 52,768개의 샘플로 구성된 최종 훈련 세트를 얻는다.\n' +
      '\n' +
      '## 4 Mustango\n' +
      '\n' +
      '무스탄고는 1) 잠재 확산 모델 2) MuNet의 두 가지 구성 요소로 구성된다.\n' +
      '\n' +
      '### Latent Diffusion Model (LDM)\n' +
      '\n' +
      'Tango(Ghosal et al., 2023)와 AudioDM(Liu et al., 2023b)에서 영감을 받아 잠재 확산 모델(LDM)을 활용하여 확산 모델의 표현성을 유지하면서 계산 복잡도를 줄인다. 보다 구체적으로, 본 논문에서는 공동 음악 및 텍스트 조건인 VAE(extra variational autoencoder)를 이용하여 추출된 잠재 오디오 사전 \\(z_{0}\\)을 구성하는 것을 목표로 한다. Tango와 유사하게, 오디오의 잠재 코드를 획득하기 위해 AudioDM(Liu 등, 2023a)으로부터 사전 트레이닝된 VAE를 레버리지한다.\n' +
      '\n' +
      '순방향 확산 과정(Markovian Hierarchical VAE)을 통해 잠재 오디오 사전 \\(z_{0}\\)은 Eq와 같이 표준 가우시안 잡음 \\(z_{N}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\)으로 변한다. (1) 사전-스케줄링된 가우시안 잡음(\\(0<\\beta_{1}<\\beta_{2}<\\cdots<\\beta_{N}<1\\))이 각각의 순방향 단계에서 점진적으로 추가되는 단계:\n' +
      '\n' +
      '\\[q(z_{n}|z_{n-1})=\\mathcal{N}(\\sqrt{1-\\beta_{n}}z_{n-1},\\beta_{n}\\mathbf{I}). \\tag{1}\\]\n' +
      '\n' +
      '가우시안 잡음 \\(z_{N}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\)으로부터 \\(z_{0}\\)을 복원하는 역과정에서는 생성된 음악을 주어진 조건 \\(\\mathcal{C}\\)으로 조향할 수 있는 MuNet을 제안한다. 우리는 다음 섹션에서 MuNet의 세부 사항을 자세히 설명할 것이다. 직관적으로, 역방향 확산은 사전 단계 \\(z_{n}\\)로부터 \\(z_{0}\\)이 재구성될 때까지 잠재 오디오 사전 \\(z_{n-1}\\)을 재구성하는 것을 목표로 하며, 이는 식 (2)-식 (7):\n' +
      '\n' +
      '\\[p_{\\theta}^{mus}(z_{n-1}|z_{n},\\mathcal{C})=\\] \\[\\mathcal{N}(\\mu_{\\theta}^{(n)}(z_{n},\\mathcal{C}),\\tilde{\\beta}^{(n)}), \\tag{2}\\] \\[\\mu_{\\theta}^{(n)}(z_{n},\\mathcal{C})=\\] \\[\\frac{1}{\\sqrt{\\alpha_{n}}}[z_{n}-\\frac{1-\\alpha_{n}}{\\sqrt{1- \\overline{\\alpha}_{n}}}\\hat{\\epsilon}_{\\theta}^{(n)}(z_{n},\\mathcal{C})], \\tag{3}\\]\n' +
      '\n' +
      '\\[\\tilde{\\beta}^{(n)}=\\frac{1-\\bar{\\alpha}_{n-1}}{1-\\bar{\\alpha}_{n}}\\beta_{n}, \\tag{4}\\]\n' +
      '\n' +
      '\\[\\alpha_{n}=1-\\beta_{n}, \\tag{5}\\]\n' +
      '\n' +
      '\\[\\overline{\\alpha}_{n}=\\prod_{i=1}^{n}\\alpha_{n}, \\tag{6}\\]\n' +
      '\n' +
      '\\[\\hat{\\epsilon}_{\\theta}^{(n)}(z_{n},\\mathcal{C})=\\] \\[w\\;\\epsilon_{\\theta}^{(n)}(z_{n},\\mathcal{C})+(1-w)\\epsilon_{ \\theta}^{(n)}(z_{n}), \\tag{7}\\]\n' +
      '\n' +
      '\\(w\\)는 Eq. (7)을 사용하여 추론하는 것을 특징으로 하는 방법. 그러나 훈련 중에, \\(\\epsilon_{\\theta}^{(n)}(z_{n},\\mathcal{C})\\)는 SS5.2에 명시된 대로 조건 \\(\\mathcal{C}\\)이 임의로 삭제되는 잡음 추정에 직접 사용된다.\n' +
      '\n' +
      '이 재구성은 Eq에 정의된 바와 같이 잡음-추정 손실을 사용하여 트레이닝된다. (8), 여기서 \\(\\hat{\\epsilon}_{\\theta}^{(n)}\\)은 추정된 잡음이고 \\(\\gamma_{n}\\)은 역 스텝의 가중치 \\(n\\):\n' +
      '\n' +
      '\\[\\mathcal{L}_{DM}=\\sum_{n=1}^{N}\\gamma_{n}\\mathbb{E}_{\\epsilon_{n}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})}||\\epsilon_{n}-\\hat{\\epsilon}^{(n)}(z_{n},\\mathcal{C})||_{2}^{2}. \\tag{8}\\]\n' +
      '\n' +
      '### MuNet\n' +
      '\n' +
      '에 기재된 역확산 공정. (2)~(7)은 음악(박자 \\(b\\)와 화음 \\(c\\))과 텍스트 \\(\\tau\\)(\\(\\mathcal{C}:=\\{\\tau,b,c\\}\\))을 조건으로 한다. 이는 잡음 추정기가 정의된 Music-Domain-Knowledge-Informed UNet (MuNet) 잡음 제거기를 통해 구현된다.\n' +
      '\n' +
      '\\] \\[\\text{UNet}_{\\theta}(\\] \\[\\text{MHA}_{\\theta_{b}}(\\] \\[Q=\\text{MHA}_{\\theta_{c}}(\\] \\[Q=\\text{MHA}_{\\theta_{r}}(Q=z_{n},K/V=\\text{FLAN-T5}(\\tau)),\\] \\[K/V=\\mathbf{Enc^{c}(c)}),\\] \\[K/V=\\mathbf{Enc^{b}(b)})), \\tag{9}\\]\n' +
      '\n' +
      '여기서 MHA는 교차 주의에 사용되는 다중 헤드 주의이며, \\(Q,K,\\) 및 \\(V\\)은 각각 쿼리, 키 및 값입니다.\n' +
      '\n' +
      'MuNet은 다중 다운샘플링, 중간 및 업샘플링 블록으로 구성된 UNet Ronneberger 등(2015)과 유사한 구조를 따르며, 조건은 교차 주의를 통해 통합된다. MuNet에서는 비트 및 화음 특징을 부호화하기 위한 2개의 부호화기 \\(\\mathbf{Enc^{b}}\\)와 \\(\\mathbf{Enc^{c}}\\)를 제안하며, 이는 FME (state-of-the-art Fundamental Music Embedding)와 MPE (Music Positional Encoding) Guo et al. (2023)의 음악적 특징들을 적절히 포착하고, Guo et al. (2023)의 여러 기본 음악 속성들(예: 번역 불변)을 보존한다.\n' +
      '\n' +
      '본 논문에서는 원 입력으로부터 박자 임베딩과 화음 임베딩을 추출하는 \\(\\mathbf{Enc^{b}}\\)와 \\(\\mathbf{Enc^{c}}\\) 인코더의 세부사항을 소개한다. 수학식 10의 비트 인코더 \\(\\mathbf{Enc^{b}}\\)에서, 우리는 비트 타입: \\(b[:,0]\\)을 인코딩하기 위해 One-Hot 인코딩 (\\(\\mathbf{OH_{b}}\\))과 비트 타이밍: \\(b[:,1]\\을 캡처하기 위해 Music Positional Embedding (\\(MPE\\)) Guo et al. (2023)을 사용한다. 이러한 비트 타입과 타이밍 임베딩을 연결하여 훈련 가능한 선형 레이어(\\(\\mathbf{W_{b}}\\))에 통과시킴으로써 최종 임베딩된 비트 특징을 얻는다.\n' +
      '\n' +
      '\\[\\mathbf{Enc^{b}}(b)=\\mathbf{W_{b}}(OH_{b}(b[:,0])\\oplus MPE(b[:,1])) \\tag{10}\\]\n' +
      '\n' +
      '1. FME-embedded Guo et al.(2023) 화음근(\\(c[:,0]\\)); 2. One-Hot 인코딩된 화음 유형(\\(c[:,1]\\)); 3. One-Hot 인코딩된 화음역전(\\(c[:,1]\\)) 및; 4. MPE-embedded Guo et al.(2023) 화음의 타이밍(\\(c[:,3]\\))을 먼저 연접하여 화음 임베딩을 얻는다. 이어서, 이 연접된 표현은 트레이닝가능한 선형 레이어(\\(\\mathbf{W_{c}}\\))를 통과한다. 특히, 음정과 간격의 병진 불변 성질을 효과적으로 포착하는 Guo et al. (2023)의 Fundamental Music Embedding (\\(FME\\))을 사용하여 음악 영역 지식 정보 음악 임베딩을 통합함으로써 화음의 음악적으로 의미 있는 표현을 가능하게 한다.\n' +
      '\n' +
      '\\[\\mathbf{Enc^{c}}(c)=\\mathbf{W_{c}}(\\text{FME}(c[:,0])\\oplus OH_{t}(c[:,1]) \\oplus OH_{i}(c[:,2])\\oplus\\text{MPE}(c[:,3])) \\tag{11}\\]\n' +
      '\n' +
      '인코딩된 비트 및 코드 임베딩을 얻은 후, 텍스트 조건을 인코포레이팅하기 위해 하나의 크로스-어텐션 레이어만을 사용하는 TANG0 Ghosal et al.(2023)과 비교하여, 디노이징 프로세스 동안 이러한 음악 조건을 통합하기 위해 두 개의 추가 크로스-어텐션 레이어를 사용한다(식 참조). (9)). 이를 통해 MuNet은 노이즈 제거 과정에서 텍스트 특징뿐만 아니라 음악 특징을 활용할 수 있어 보다 제어 가능하고 음악적으로 의미 있는 음악 생성이 가능하다.\n' +
      '\n' +
      '그림 2: 제안된 무스탱고 모델의 묘사. 비트 및 화음은 입력으로서 제공되지 않을 때 캡션으로부터 추론된다.\n' +
      '\n' +
      '### Inference\n' +
      '\n' +
      '훈련 단계에서 우리는 교사 강제력을 사용하여 음악 생성 과정을 조건화하기 위해 그라운드 트루스 박자와 코드 특징을 활용한다. 그러나 추론하는 동안 우리는 다른 접근법을 채택한다. 비트 및 코드 특징을 예측하기 위해 독립적으로 훈련된 두 개의 트랜스포머 기반 텍스트-음악-특징 생성기를 다음과 같이 사용한다.\n' +
      '\n' +
      '**비트**: DeBERTa Large 모델 [11]을 비트 예측 변수로 사용합니다. 모델은 텍스트 캡션을 입력으로 하고, i) 대응하는 음악의 최대 박자, ii) 박자들 사이의 간격 지속 시간의 시퀀스를 예측한다. 우리는 DeBERTa 모델의 최종 계층의 토큰 수준 표현으로부터 그것들을 예측한다. 최대 박자는 훈련 데이터 세트의 음악 인스턴스에 대해 1에서 4 사이의 정수 값을 취한다. 따라서, 우리는 DeBERTa 출력 레이어의 첫 번째 토큰으로부터 4-클래스 분류 설정을 사용하여 최대 박자를 예측한다. 간격 지속 시간은 두 번째 토큰부터 플로트 값으로 예측됩니다. 예를 들어, 최대 박자를 \\(2\\)으로 예측하고 구간 길이를 \\(t_{1},t_{2},t_{3},\\dots\\)으로 예측하면, 예측된 박자는 \\(t_{1}\\)에서 \\(1\\), \\(t_{1}+t_{2}\\)에서 \\(2\\), \\(t_{1}+t_{2}+t_{3}\\)에서 \\(1\\) 등이다. 예측된 비트 시간을 최대 10초까지 유지하고 그 이상의 예측 타임스탬프는 무시한다.\n' +
      '\n' +
      '**코드**: 시퀀스를 사용하여 FLAN-T5 대형 모델 [10]을 코드 예측 변수로 시퀀스합니다. 모델은 텍스트 캡션의 연결과 언어화된 비트를 입력으로 한다. 구두화된 비트는 앞에서 예시한 예를 위해 다음과 같이 준비된다. _Timestamps: \\(t_{1}\\), \\(t_{1}+t_{2}\\), \\(t_{1}+t_{2}+t_{3}\\dots\\), Max Beat: \\(2\\)_. 이 모델은 타임스탬프를 사용 하 여 언어화 된 코드 시퀀스를 생성 하도록 훈련 됩니다. 이는 다음과 같습니다. _Am at 1.11; E at 4.14; C#maj7 at 7.18_. 우리는 다시 예측된 코드 시간을 최대 10초까지 유지하고 그 이상으로 예측된 타임스탬프를 무시한다.\n' +
      '\n' +
      '## 5 Experiments\n' +
      '\n' +
      '이 섹션에서는 다음 연구 질문에 답하기 위한 철저한 실험을 설명한다.\n' +
      '\n' +
      '* Mustango4에서 생성된 음악의 오디오 품질은 얼마나 좋은가요. Tango [12]의 출력과 비슷하거나 더 나은가요? 각주 4: [https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps](https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps)\n' +
      '* Mustango4가 Tango보다 더 나은 음악 품질을 달성합니까? 각주 4: [https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps](https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps)\n' +
      '* 데이터 증강 접근 방식은 성능을 향상시키는 데 효과적이며 이 데이터 세트에서만 훈련된 모델이 대규모 오디오 사전 훈련된 모델과 경쟁할 수 있습니까?\n' +
      '\n' +
      '이러한 질문에 답하기 위해 광범위한 객관적이고 주관적인 평가를 배포한다.\n' +
      '\n' +
      '### Baselines 및 Mustango4 Variables\n' +
      '\n' +
      'Mustango4의 비트와 화음에 대한 추가 컨디셔닝의 부족을 제외하고, Mustango4와 동일한 아키텍처를 공유하는 오디오 생성을 위한 잠재 확산 모델인 Tango [12]와 Mustango4를 주로 비교한다. 무스탱고4의 효능을 판단하기 위해 처음부터 다음 세 가지 모델을 훈련한다. 1) 뮤직캡스 트레인A에서 훈련된 탱고, 2) 뮤직벤치에서 훈련된 탱고, 3) 뮤직벤치에서 훈련된 무스탱고4. 추가적으로, 사전 트레이닝된 탱고 체크포인트로부터 탱고 및 머스탱고4를 피니튜닝한다: 4) 탱고 프롬프트뱅크 [12]에서 트레이닝되고 오디오캡에서 미세튜닝되고, 뮤직캡 7, 5) 오디오캡에서 미세튜닝된 탱고 체크포인트, 현재 뮤직벤치에서 미세튜닝되고, 현재 뮤직벤치에서 미세튜닝되고, 6) 사전 트레이닝된 탱고 체크포인트로부터 초기화되고 뮤직벤치에서 미세튜닝된다.\n' +
      '\n' +
      '각주 4: [https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps](https://huggingface.co/declare-lab/tango-full-ft-audio-music-caps)\n' +
      '\n' +
      '각주 5: [https://huggingface.co/declare-lab/tango-full-ft-audiocaps](https://huggingface.co/declare-lab/tango-full-ft-audiocaps)\n' +
      '\n' +
      '### Training\n' +
      '\n' +
      '모든 모델은 수렴할 때까지 AdamW [13] 최적화기를 사용하여 \\(4.5e-5\\)의 학습 속도로 훈련되었다. 비트 및 코드 예측 변수는 뮤직벤치에서도 훈련됩니다.\n' +
      '\n' +
      'Mustango4의 견고성을 더욱 개선하기 위해 학습 데이터 로드 중에 이 세 가지 드롭아웃을 사용합니다.\n' +
      '\n' +
      '1. 5% 확률로 모든 입력(텍스트, 박자, 화음)을 드롭하고;\n' +
      '2. 5% 확률로, 입력 특징(입력들 각각에 개별적으로 적용됨)을 드롭하고;\n' +
      '3. 프롬프트를 마스킹할 확률을 \\(\\min(100,10\\frac{N}{M})\\)%로 결정합니다. 여기서 \\(N\\)은 현재 프롬프트의 문장 수를 나타내고 \\(M\\)은 프롬프트당 평균 문장 수를 나타냅니다. 마스킹을 위해 프롬프트가 선택되면, [20, 50] 범위의 균일한 분포에서 정수 \\(X\\)을 무작위로 끌어와서 프롬프트에서 입력 문장의 \\(X\\)%를 제거한다.\n' +
      '\n' +
      '처음 두 탈락자 뒤에 있는 아이디어는 모델이 불완전하거나, 결함이 있거나, 입력 정보가 누락된 상태에서 작동할 수 있도록 하는 것이다. 세 번째 드롭아웃은 짧은 텍스트 입력에 대한 견고성을 향상시키는 것을 목표로 한다. 이러한 드롭아웃을 탱고에도 적용하여 약간의 수정으로, 탱고는 음악 기능 입력을 사용하지 않기 때문에 처음 두 드롭아웃을 모든 텍스트를 드롭할 단일 10% 확률로 대체한다.\n' +
      '\n' +
      '### Objective Evaluation\n' +
      '\n' +
      '생성된 오디오 샘플들의 품질은 프레쳇 거리(FD), 프레쳇 오디오 거리(FAD)[19], 및 컬백-라이블러 발산(KL)의 세 가지 객관적 메트릭들에 관하여 평가된다. 생성된 오디오의 분포와 Ground truth 기준 사이의 거리를 측정하기 위해 KL 발산을 사용한다. 이 메트릭은 사전-훈련된 분류기, 즉 오디오 패턴 인식을 위한 대규모 사전-훈련된 오디오 네트워크에 의해 생성된 라벨에 적용된다. [16, 15]에서와 같이 우리는 또한 두 곡선 사이의 유사성 메트릭인 FD를 계산하기 위해 PANN을 활용한다. FAD는 이미지 도메인으로부터의 FID(Frechet Inception Distance)와 유사한 평가 메트릭이며, 이는 오디오 도메인에 대해 특별히 설계되었다. 이는 인간의 인식을 기반으로 하며 VGGish 분류기를 사용하여 계산된다.\n' +
      '\n' +
      '#### 5.3.1 Out-of-Distribution Evaluation\n' +
      '\n' +
      '실험에 사용된 미세 조정된 모델이 초기 사전 훈련된 체크포인트를 훈련할 때 전체 MusicCaps 데이터 세트에 노출되었다는 점을 감안할 때, 우리는 독립적으로 생성된 평가 세트에서만 해당 모델을 공정하게 평가할 수 있습니다. 우리는 인기곡들의 대규모 데이터 집합인 FMA (Free Music Archive) [14]에서 새로운 음악 파일들을 수집한다. 특히, FMA-large에서 1,000개의 무작위 샘플을 채취하여 각각에서 무작위 10초 단편을 잘라냈다. 그 다음, 우리는 오디오에 태그를 할당하기 위해 에센티아의 태깅 모델 [1]을 사용했다. 구체적으로, 일반적인 자동 태깅, 기분, 장르, 계측, 음성 및 음성 젠더에 대한 모델을 사용하여 확률과 함께 풍부한 태그 집합을 제공했다. 그런 다음 음악 전문가가 추출된 태그뿐만 아니라 오디오를 기반으로 샘플 중 25개에 대한 텍스트 설명을 작성했다. 다음으로, Chat-GPT는 나머지 데이터 세트에 대한 태그에서 의사 프롬프트를 얻기 위해 문맥 내 학습 작업을 수행하도록 지시했다. 마지막으로, SS3.1에서 설명한 대로 관련 음악 특징을 추출한 후 프롬프트에 관련 제어 문장을 추가했다. 훈련 세트와 유사하게 각각 25/30/20/15/10%의 확률로 0/1/2/3/4 제어 문장을 추가했다. 이 평가 집합을 \\(\\mathtt{FMACaps}\\)이라고 한다.\n' +
      '\n' +
      '#### 5.3.2 Controllability 평가\n' +
      '\n' +
      '제어 가능성 측면에서 모델을 평가하기 위해 그림 1에 표시된 대로 TestB를 사용한다. 1, 프롬프트의 각 샘플에 대한 모든 제어 문장이 있는 수정 버전 \\(\\mathtt{FMACaps}\\)입니다. 테스트 세트의 입력 텍스트 프롬프트를 기반으로 음악을 생성했다. 이렇게 생성된 음악으로부터 몇 가지 음악적 특징(SS3.1 참조)을 추출하여 입력 텍스트 프롬프트에 지정된 특징과 비교할 수 있도록 하였다. 이 비교를 정량화하기 위해, 우리는 모든 메트릭을 백분율로 나타내므로 0에서 100까지의 범위의 제어 메트릭을 개발했다. 메트릭이 이진인 경우 트루의 경우 100으로, 거짓의 경우 0으로 표시된다. 메트릭은 다음과 같이 정의됩니다.\n' +
      '\n' +
      '* **템포 빈(TB)** - 분당 예측 비트(bpm)는 지상 현실 템포 빈에 속합니다.\n' +
      '* **TBT(Tempo bin with tolerance)** - 예측 된 bpm은 지상 현실 템포 빈 또는 이웃 템포 빈에 속합니다.\n' +
      '* **올바른 키(CK)** - 예측 된 키가 지상 진실 키와 일치 합니다.\n' +
      '* **복제(CKD)가 있는 올바른 키** - 예측 된 키는 지상 진실 키 또는 동등한 키와 일치 합니다. 우리는 메이저와 그에 상응하는 마이너스를 중복으로 간주한다. (예를 들어, C major 및 A minor).\n' +
      '* **완벽한 코드 일치 (PCM)** - 예측 된 코드 시퀀스는 길이, 순서, 코드 루트 및 코드 유형 측면에서 Ground truth와 완벽하게 일치 합니다.\n' +
      '* **정확한 코드 일치 (ECM)** - 예측 된 코드 시퀀스는 누락 된 코드 인스턴스와 초과 된 코드 인스턴스에 대 한 허용 오차와 함께 순서, 코드 루트 및 코드 유형 측면에서 정확히 지상 진리와 일치 합니다.\n' +
      '* **CMAO(모든 순서로 일치함)** - 코드 루트 및 코드 유형 모두에서 그라운드 진리와 일치하는 예측 코드 시퀀스의 부분입니다.\n' +
      '* **CMAOMM(모든 순서 주/부 형식)에서 코드 일치** - 코드 루트 및 이진 주/부 형식 측면에서 그라운드 진리와 일치하는 예측 코드 시퀀스의 부분(예: D, D6, D7, Dmaj7은 모두 주형으로 간주됨)입니다.\n' +
      '\n' +
      '\\(\\bullet\\) **비트 수 예측 (BC)** - 지상 사실과 일치 하는 예측 된 비트 수의 백분율입니다.\n' +
      '\n' +
      '### Subjective Evaluation\n' +
      '\n' +
      '객관적 평가 외에 일반 듣기 검사와 통제성에 중점을 둔 전문가 듣기 검사의 두 가지 듣기 검사의 형태로 주관적 평가도 실시하였다.\n' +
      '\n' +
      '일반 듣기 테스트를 위해 피험자들은 4가지 모델 각각에 대해 10개의 생성된 음악 샘플을 듣고 입력된 텍스트 캡션을 제공받았다. 4개의 모델에는 무스탱고가 포함되었으며, 둘 다 사전 훈련과 처음부터 훈련되었다. 10개의 텍스트 프롬프트는 음악 전문가들이 MusicCaps의 스타일로 주문 제작하였으며, <부록>의 <표 7>과 같다. 참가자들은 1) 오디오 품질(AQ), 2) 입력 텍스트 프롬프트(REL), 3) 전체 음악성(OM), 4) 리듬 존재 및 안정성(RP), 5) 음악의 조화 및 자음(HC)을 평가하도록 요청받았다. 모든 측면은 PsyToolkit 인터페이스 Stoet(2010)을 사용하여 7점 리커트 척도로 평가되었다. 사용된 전체 질문과 인터페이스는 부록에 나와 있다.\n' +
      '\n' +
      '전문가 듣기 테스트를 위해 음악 오디오에서 화음을 인식할 수 있는 최소 5년의 공식 음악 교육을 받은 전문가 평가자를 발견했다. 표 7과 같이 4가지 모델 각각에 대해 20개의 텍스트 프롬프트를 사용하여 생성된 비율을 평가하기 위해 80개의 샘플을 제공했다. 텍스트 프롬프트는 음악 전문가가 맞춤 제작했으며 10개의 \'대조\' 쌍으로 구성되었다. 그것이 현실적이고 프롬프트에 모순되는 요소가 없는지 확인하기 위해 주의를 기울였다. 예를 들어, 표의 캡션 1은 캡션 2와 대조됩니다. 본문은 "리드 기타와 스트러밍 어쿠스틱 기타에 의해 연주되는 기악 블루스 멜로디. 어쿠스틱 기타리스트의 스트러밍은 리듬을 일정하게 유지합니다." 그러나 제어 문장은 다릅니다. "화음 시퀀스는 G7, F7, C7, G7입니다. 이 노래는 분당 100박자로 갑니다." 대 "화음 시퀀스는 Dm, Am, Em입니다. 이 노래는 분당 60박자로 갑니다." 두 화음 시퀀스는 블루스 진행에서 나오지만 다른 키/모드에 속합니다." 캡션 2의 템포는 상당히 느리다. 이러한 캡션은 제어 문장이 생성된 음악에 영향을 미치는지 테스트하는 데 이상적으로 적합하다. 일반 듣기 연구에서 평가할 5가지 측면 외에도 평가할 2가지 음악 제어 특정 측면을 추가했다. 이들은 생성된 음악으로부터의 화음이 텍스트 프롬프트에 특정된 것과 매칭되는 정도(Chord Match 또는 MCM), 및 생성된 음악의 템포가 텍스트 프롬프트에 특정된 템포와 매칭되는 정도(Tempo match 또는 MTM)를 포함한다.\n' +
      '\n' +
      '### Experimental Results\n' +
      '\n' +
      '#### 5.5.1 객관적 평가\n' +
      '\n' +
      'TestA, TestB 및 FMA 평가 세트에 대한 객관적인 평가 결과는 표 1에 나와 있다. MusicCaps에 대해 훈련된 탱고 변형은 모두 다른 4가지 모델보다 열등하며, 이는 **증강 전략의 효율성을 보여준다. MusicBench와 Mustango에 미세 조정된 사전 훈련된 Tango는 FD와 KL에서 매우 유사하게 수행되는 것으로 보이지만, Mustango는 사전 훈련된 FAD에서 큰 개선을 보여 FAD가 인간의 인식 영감을 받은 메트릭이기 때문에 더 나은 인식 품질과 음악성을 시사한다. 마지막으로, 처음부터 훈련된 머스탱고의 성능은 FD 및 KL에서 뮤직벤치에서 훈련된 머스탱고 및 탱고의 사전 훈련 버전과 비교할 수 있으며, 이는 우리의 증강 데이터 세트를 사용한 훈련이 음악 생성을 위한 대규모 오디오 사전 훈련의 대안이 될 수 있음을 보여준다.\n' +
      '\n' +
      '제어성 결과의 평가는 표 2에 나와 있다. TestB에서 Tempo 메트릭에서는 모든 모델이 비교 가능하게 수행된다. 키 메트릭에서 우리는 MusicBench에서 훈련된 모델이 MusicCaps에서 훈련된 모델보다 훨씬 더 나은 성능을 보인다는 것을 관찰할 수 있다. 또한 무스탱고는 테스트B의 다른 모든 모델보다 성능이 우수하고 FMACaps에 두 번째로 배치되었다. 화음 제어성에서 머스탱고는 모든 탱고 모델보다 큰 마진으로 우수한 성능을 보인다. 마지막으로 비트 메트릭에서 모델들은 서로 유사하게 수행되는 것처럼 보이지만 무스탱고가 가장 좋은 결과를 보여준다. FMACaps에서 CMAOMM이 75.83에 도달하는 무스탱고의 경우 Chord 메트릭이 훨씬 더 우수함을 알 수 있다. 전반적으로 TestB와 수정된 FMACaps에서 수집된 결과는 대부분의 측면에서 상관관계가 있다.\n' +
      '\n' +
      '#### 5.5.2 주관적 평가\n' +
      '\n' +
      '총 48명의 참가자가 일반 듣기 시험에 참여했으며, 이 중 26명은 5년 이상의 정규 음악 교육을 받았다. 표 3의 결과는 위에서 정의한 각 메트릭에 대한 평균 평점을 보여준다. 우리는 MusicBench에서 훈련된 모델에 의해 탱고 베이스라인 모델이 모든 메트릭에서 성능이 우수함을 분명히 알 수 있다. 흥미롭게도 무스탕고는 처음부터 훈련되어 오디오 품질, 리듬 존재 및 조화 면에서 최고의 성능을 발휘합니다. 등급의 차이는 3개의 상위 모델 간에 최소이며, 우리의 증강 방법이 출력 품질을 더 높이는 데 효과적이며 무스탱고가 최첨단 품질에 도달할 수 있음을 분명히 확인했다.\n' +
      '\n' +
      '통제성 청취 연구는 총 4명의 전문가가 참여하였다. 표 4의 전문가 청취 연구의 결과는, 특히 입력 텍스트 캡션(Chord Match 또는 MCM)과 매칭되는 생성된 음악의 화음의 관점에서, 모든 메트릭에서 두 머스탱고 모델이 탱고 기준선을 능가하는 것을 추가로 확인한다. 이는 표 2에 제시된 통제성 결과를 더욱 뒷받침하며 제안된 무스탱고 모델이 실제로 음악별 텍스트 프롬프트를 이해할 수 있음을 보여준다.\n' +
      '\n' +
      '### Ablation Study\n' +
      '\n' +
      '자연적 절제는 무스탱고에서 한 번에 하나의 통제를 버리는 것이다. 그러나 자원 제약으로 인해 이러한 실험을 수행할 수 없었다.\n' +
      '\n' +
      '한편, 우리는 절제를 통해 다음과 같은 두 가지 연구 문제를 다룬다.\n' +
      '\n' +
      '무스탱고 훈련 전이야 필요한? 특정 실험 설정에서 탱고 프롬프트뱅크에서 미리 훈련된 탱고 체크포인트로 무스탱고를 초기화한 다음 오디오캡 데이터 세트에 대한 명령어 튜닝을 통해 미세 조정했다. 이러한 검문소는 "코끼리의 소리"와 같은 일반적인 오디오와 소리에 대한 광범위한 이해를 캡슐화한다. 그러나, 우리는 이러한 일반적인 오디오 지식이 음악 생성에 유익함을 증명하지 못한다는 것을 관찰했다(표 1 내지 표 4 참조). 그럼에도 불구하고 이러한 검문소는 "사자의 함성이 반주하는 아프리카 힙합 음악"과 같이 다양한 사운드로 음악을 작곡하는 데 효용을 찾을 수 있다.\n' +
      '\n' +
      '뮤넷이 도움이 될까?뮤스탄고에서 뮤넷의 역할은 음악 생성 과정을 통제하는 것이다. SS5.3에서 자세히 설명했듯이 MuNet을 포함하면 객관적인 평가와 주관적인 평가 모두에서 TestB와 FMACaps 모두에서 무스탱고의 성능이 크게 향상되어 제공된 프롬프트에서 모델이 제어 지침을 얼마나 잘 준수하는지 직접 측정한다. 중요한 것은 MuNet이 전체 당을 손상시키지 않는다는 것이다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{c l c c|c c c|c c c|c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Datasets**} & \\multirow{2}{*}{**Pre-trained**} & \\multirow{2}{*}{**\\#Params**} & \\multicolumn{4}{c|}{**TestA**} & \\multicolumn{4}{c|}{**TestB**} & \\multicolumn{4}{c}{**FMCaps**} \\\\  & & & FD \\(\\downarrow\\) & FAD \\(\\downarrow\\) & KL \\(\\downarrow\\) & FD \\(\\downarrow\\) & FAD \\(\\downarrow\\) & KL \\(\\downarrow\\) & FD \\(\\downarrow\\) & FAD \\(\\downarrow\\) & FAD \\(\\downarrow\\) & KL \\(\\downarrow\\) \\\\ \\hline Tango & MusicCaps & ✗ & 866M & 30.80 & 2.84 & 1.34 & 30.39 & 2.92 & 1.33 & 28.32 & 3.75 & 1.22 \\\\ Tango & MusicCaps & ✓ & 866M & 34.87 & 4.05 & 1.25 & 37.85 & 4.52 & 1.32 & 28.81 & 2.92 & 1.21 \\\\ Tango & MusicBench & ✗ & 866M & 28.50 & 2.29 & 1.33 & 28.27 & 2.17 & 1.32 & 26.31 & 2.31 & 1.16 \\\\ Tango & MusicBench & ✓ & 866M & 25.38 & 1.91 & 1.19 & 24.60 & 1.77 & 1.13 & 24.48 & 2.96 & 1.15 \\\\ Mustango. ✗ & MusicBench & ✗ & 1.4B & 26.58 & 2.09 & 1.21 & 25.24 & 1.57 & 1.18 & 24.24 & 2.94 & 1.16 \\\\ Mustango. ✗ & MusicBench & ✓ & 1.4B & 26.35 & 1.46 & 1.21 & 25.97 & 1.67 & 1.12 & 25.18 & 2.34 & 1.16 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 모든 테스트 데이터 세트에 대한 모델의 객관적인 평가 결과. 열에는 모델당 각 메트릭의 평균 값이 표시됩니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c c c c c c} \\hline \\hline Model & Dataset & Pre-trained & REL & AG & OM & RP & HC \\\\ \\hline Tango & MusicCaps & ✓ & 4.09 & 3.68 & 3.55 & 3.91 & 3.80 \\\\ Tango & MusicBench & ✓ & 4.96 & 4.26 & 4.04 & 4.49 & 4.61 \\\\ Mustango & MusicBench & ✓ & 4.85 & 4.10 & 4.24 & 4.43 & 4.33 \\\\ Mustango & MusicBench & ✗ & 4.79 & 4.20 & 4.23 & 4.51 & 4.63 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 일반 청취 연구의 각 메트릭에 대한 평균 평점.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l c c c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{**Model**} & \\multirow{2}{*}{**Datasets**} & \\multirow{2}{*}{**Pre-trained**} & \\multirow{2}{*}{**\\#Params**} & \\multicolumn{4}{c|}{**TestA**} & \\multicolumn{4}{c}{**TestB**} & \\multicolumn{4}{c}{**FMCaps**} \\\\  & & & & FD \\(\\downarrow\\) & FAD \\(\\downarrow\\) & KL \\(\\downarrow\\) & FD \\(\\downarrow\\) & FAD \\(\\downarrow\\) & KL \\(\\downarrow\\) & FD \\(\\downarrow\\) & FAD \\(\\downarrow\\) & FAD \\(\\downarrow\\) & KL \\(\\downarrow\\) \\\\ \\hline Tango & MusicCaps & ✗ & 866M & 30.80 & 2.84 & 1.34 & 30.39 & 2.92 & 1.33 & 28.32 & 3.75 & 1.22 \\\\ Tango & MusicCaps & ✓ & 866M & 34.87 & 4.05 & 1.25 & 37.85 & 4.52 & 1.32 & 28.81 & 2.92 & 1.21 \\\\ Tango & MusicBench & ✗ & 866M & 28.50 & 2.29 & 1.33 & 28.27 & 2.17 & 1.32 & 26.31 & 2.31 & 1.16 \\\\ Tango & MusicBench & ✓ & 866M & 25.38 & 1.91 & 1.19 & 24.60 & 1.77 & 1.13 & 24.48 & 2.96 & 1.15 \\\\ Mustango. ✗ & MusicBench & ✗ & 1.4B & 26.58 & 2.09 & 1.21 & 25.24 & 1.57 & 1.18 & 24.24 & 2.94 & 1.16 \\\\ Mustango. ✗ & MusicBench & ✓ & 1.4B & 26.35 & 1.46 & 1.21 & 25.97 & 1.67 & 1.12 & 25.18 & 2.34 & 1.16 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 전문가 청취 연구의 각 메트릭에 대한 평균 평점.\n' +
      '\n' +
      '프롬프트에 제어 문장이 없는 경우 일반 음악 생성의 적합성. 실제로, 제어 명령 준수 평가에 명시적으로 초점을 맞추지 않은 몇 가지 객관적인 메트릭(예: FD, FAD 및 KL-divergence)은 MuNet이 통합될 때 일관되게 성능 개선을 보여준다.\n' +
      '\n' +
      '### Discussions\n' +
      '\n' +
      '#### 5.7.1 Predictors의 성능\n' +
      '\n' +
      '추론 단계에서는 텍스트 프롬프트를 기반으로 화음과 박자 예측을 위해 미리 훈련된 예측 변수를 활용한다. 이러한 예측 변수는 프롬프트에 현과 박자 정보가 명시적으로 포함되어 테스트B 데이터 세트에서 94.5%의 정확도를 달성할 때 탁월한 성능을 나타낸다. 그러나 우리의 관심은 컨트롤 문장이 프롬프트에 없는 시나리오에서 성능을 평가하는 것으로 확장되는데, 기본적으로 이러한 예측 변수가 시끄러운 화음과 박자를 생성합니까? 문제는 이러한 잡음이 예측기에서 무스탱고로 전파되어 생성된 음악의 전반적인 품질에 상당한 영향을 미칠 수 있다는 것이다.\n' +
      '\n' +
      '실험에서 TestA는 텍스트 프롬프트에 제어 문장이 포함되지 않는 시나리오 역할을 한다. 탱고와 무스탱고의 성능(표 1)을 비교하면 TestA에서 후자가 대부분의 메트릭에서 전자보다 성능이 우수함을 알 수 있다. 이 관측치는 제어 예측 변수가 탱고에 비해 무스탱고의 성능을 손상시키지 않는다는 것을 나타냅니다. 통제 문장이 없을 때 특정 주제나 스타일에 대한 이러한 예측 변수의 적응성은 우리가 아래에 간략하게 언급하는 주제인 미래 탐구를 위한 잠재적인 길로 남아 있다.\n' +
      '\n' +
      '먼저, 작은 비교 실험에서 생성된 출력에 대한 코드 예측 변수의 영향을 조사한다. 우리는 무스탱고에 의해 합성된 TestA와 TestB 샘플을 모두 취하여 그로부터 특징을 추출한다. 그리고 특징 추출에서 검출된 화음 대비 화음 예측기에 의해 예측된 화음을 이용하여 PCM, ECM, CMAO, CMAOMM의 화음 제어 메트릭을 평가한다. TestA에 대한 메트릭은 PCM - 16.15, ECM - 33.95, CMAO - 39.81, CMAOMM - 47.82로 나타났다. TestB에 대한 메트릭은 PCM - 17.75, ECM - 32.07, CMAO - 47.36, CMAOMM - 66.80으로 나타났다. 이러한 결과는 Mustango가 화음 예측자에 의해 예측된 화음을 따르는 경향이 있음을 보여준다. TestA에 대한 결과는 TestB에 대한 결과보다 약간 낮지만 표 2와 같이 TestB에 대한 탱고 결과보다 여전히 높다.\n' +
      '\n' +
      '둘째, 몇 가지 구체적인 예를 살펴보면 다음과 같다.\n' +
      '\n' +
      '**프롬프트: "이 민요는 주 멜로디를 부르는 여성 목소리가 특징입니다. 타블라가 타악기를 연주합니다. 기타 스트럼 코드가 있습니다. 노래의 대부분의 부분에 대해 하나의 코드만 연주됩니다. 마지막 마디에서 다른 코드가 연주됩니다. 이 노래는 최소한의 악기입니다. 이 노래는 스토리 텔링 무드가 있습니다. 이 노래는 인도 영화의 마을 장면에서 연주할 수 있습니다. _화음 시퀀스는 Bbm, Ab. The beat is 3. 이 노래의 템포는 Allegro입니다. 이 노래의 키는 Bb minor입니다._\n' +
      '\n' +
      '이탤릭체(TestA)에서 제어 문장 없이: **코드 예측**: ["G", "C", "G", "C", "G", "C"], **코드 예측 시간**: [0.46, 1.21, 3.25, 5.48, 7.24, 8.92]. **오디오에서 추출한 코드**: ["G6", "C", "G", "C", "C", "G", "Cmaj7"], **오디오에서 추출한 코드 시간**: [0.46, 1.58, 3.07, 5.94, 7.62, 9.66]\n' +
      '\n' +
      '이탤릭체(TestB)의 컨트롤 문장: **코드 예측**: ["Bbm", "Ab"], **코드 예측 시간**: [0.46, 7.24], **오디오에서 추출한 코드**: ["F#maj7", "Ab"], **오디오에서 추출한 코드 시간**: [0.46, 7.43].\n' +
      '\n' +
      '**Prompt: "한 여성 가수가 이 파란색 멜로디를 부릅니다. 이 노래는 최소한의 기타 반주와 다른 악기가 없는 중간 템포입니다. 이 노래의 중간 템포는 매우 감정적이고 열정적입니다. 이 노래는 현대 팝 히트이지만 오디오 품질이 좋지 않습니다. _이 노래의 키는 G minor입니다. 박자는 3/4입니다. 이 노래는 분당 168.0 박자입니다. 이 노래의 코드 진행은 Am7, G7, Cm, G, A7._입니다.\n' +
      '\n' +
      '이탤릭체(TestA)에서 제어 문장이 없는 경우: **예측된 코드**: ["C#m7", "C#m7", "C#m7", "C#m7"], **예측된 시간**: [0.46, 3.25, 6.32, 8.17, 9.29], **오디오에서 추출한 코드**: ["F#", "C#m", "F#m", "C#m7"], **오디오에서 추출한 코드 시간**: [0.46, 1.21, 4.55, 5.39]\n' +
      '\n' +
      '컨트롤 문장이 이탤릭체(TestB)에 있는 경우: **코드 예측**: ["Am7", "G7", "Cm", "G", "A7"], **코드 예측 시간**: [0.46, 1.67, 3.53, 5.48, 8.92], **코드에서 추출한 코드**\n' +
      '\n' +
      '**audio**: ["Am", "G", "C", "Gmaj7", "A6", "Gmaj7"], **오디오에서 추출한 코드 시간**: [0.46, 1.67, 3.72, 5.94, 8.73, 9.85]\n' +
      '\n' +
      '두 개의 묘사된 샘플은 생성된 오디오에서 검출된 예측된 화음 및 화음에 대한 몇 가지 구체적인 통찰력을 제공한다. 대부분의 경우, \\(\\tt Mustango\\)는 화음 예측기에서 제공하는 화음을 따른다. 우리는 예측된 화음과 비교하여 오디오에서 검출된 실제 화음에서 G가 G6이 되고, C가 Cmaj7이 되며, C#m7이 C#m이 되는 몇 가지 치환을 관찰할 수 있다. 이러한 화음 치환은 음악적으로 매우 밀접하며 특징 추출 시스템이 100% 정확하지 않은 결과일 수도 있다. F#maj7에 대한 Bbm의 치환은 언뜻 보기에 더 큰 변화이지만, Bbm의 3음 중 2음이 4음 F#maj 무스탱고에도 포함되어 있다는 점을 감안할 때 이 치환도 이해할 수 있다고 본다. 그러나 이 대체는 제안된 코드 제어 메트릭에서 유효한 것으로 간주되지 않는다는 점에 유의한다.\n' +
      '\n' +
      '마지막으로, 프롬프트에 명시적인 제어 문장이 없는 경우, 우리는 화음 예측자에 의해 예측된 화음이 일반적으로 특정 패턴을 따른다는 것을 관찰한다. 생성된 샘플들은 교번하는 두 개의 화음(A, B, A, B, A, B)의 패턴을 따른다. 관찰된 패턴의 또 다른 유형은 반복되는 하나의 코드(A, A, A, A, A, A)이다. 코드 예측자 행동에 대한 보다 정교한 연구는 향후 작업에 대한 주제가 되어야 한다.\n' +
      '\n' +
      '#### 5.7.2 Insights from Human Annotation\n' +
      '\n' +
      '여기서, 전문가 청취 테스트에서 생성된 몇 가지 예를 살펴보면, 특히 다음 프롬프트가 있는 블루스 샘플: "리드 기타와 스트러밍 어쿠스틱 기타에 의해 연주되는 기악 블루스 멜로디. 어쿠스틱 기타의 스트러밍은 리듬을 일정하게 유지한다. 코드 시퀀스는 G7, F7, C7, G7이다. 이 노래는 분당 100박자로 진행된다."\n' +
      '\n' +
      '그림 3에서 우리는 MusicBench에서 미리 훈련된 탱고에 의해 생성된 멜-스펙트로그램을 볼 수 있다. 스펙트로그램과 첨부된 파형에서 분명히 알 수 있듯이 리듬이 매우 일관된 그림 4에 묘사된 \\(\\tt 무스탱고\\)에 의해 생성된 샘플과 대조적으로 음악은 약간 갑자기 나타난다. 이는 표 4의 전문가 청취 연구의 결과를 반영하는 것 같습니다. 확산 프로세스를 조건화 하는 비트 예측기에 의해 예측 된 비트 타임스탬프는 다음과 같습니다. **비트 예측**: [[0.26, 0.87, 1.52, 2.09, 2.76, 3.41, 4.0, 4.57, 5.1, 5.65, 6.22, 6.79, 7.36, 7.79, 8.3, 8.8, 9.3, 9.75], 3. **이러한 예측 된 비트 타임스탬프는 분당 100 박자에 해당하는 대략 0.6 초마다 박자가 있음을 보여 줍니다. 이것은 모델을 컨디셔닝하기 위해 순서화되고 적절하게 예측되는 템포입니다. **\n' +
      '\n' +
      '화음에 관한 한 탱고는 때때로 화음을 따르지 않거나, 불분명한 소리를 내거나, 통화할 충분한 시간을 주지 않을 것이다. 반면에, \\(\\tt Mustango\\)은 예측된 화음과 시작 시간을 따르는 것으로 보인다. 우리는 같은 블루스 사례를 본다. 코드 예측기에서 예측 된 코드 조건은 다음과 같습니다. **코드 예측**: ["G7", "F7", "C7", "G7"], **코드 예측 시간**: [0.46, 2.04, 4.37, 8.17]. 우리는 화음의 시작 시간이 시간적으로 잘 퍼져 있음을 알 수 있다. 이는 샘플을 듣고 인식된 화음이 있는 스펙트로그램을 보는 것도 그림 4에서 알 수 있다. 이를 확인하기 위해 생성된 오디오에서 화음 특징을 추출하여 비교하였다. \\(\\tt Mustango\\)에 의해 생성된 오디오 샘플에서 추출된 코드 특징은 다음과 같습니다. **코드**: ["G7", "F7", "C", "G7"], **코드 시간**: [0.46, 1.76, 4.74, 8.45] 흥미롭게도 타이밍 및 코드 시퀀스의 일치는 여기에서 매우 명확합니다. C7 화음을 C로 치환하는 것은 생성 부분이나 특징 추출 부분에서 사소한 실수가 될 수 있다. SS5.3.2의 코드 메트릭을 고려하면 CMAOMM의 경우 100점, CMAO 및 ECM의 경우 75점을 산출한다. 대조적으로, \\(\\tt MusicBench\\)에서 미리 훈련된 탱고에 의해 생성된 샘플은 더 불안정하게 들리며, 코드를 통해 소리를 낼 수 있는 충분한 시간을 주지 않는다. \\(\\tt MusicBench\\)에서 미리 훈련된 탱고 피니튜닝된 오디오 샘플에서 추출된 코드 특징은 다음과 같습니다. **코드**: ["Fm6", "G", "Dm", "G", "C", "Gm"], **코드 시간**: [0.46, 2.69, 3.53, 5.76, 6.69, 9.66]. 순서 4가 아닌 오디오 샘플로부터 추출된 화음이 6개이고, 장조가 아닌 단조의 F화음을 볼 수 있기 때문에 너무 잘 일치하지 않는 것을 알 수 있다. G는 단조의 변형에서도 한 번 나타나고, Dm화음도 추가로 존재한다. 이것은 75의 CMAOMM 점수를 산출하지만 CMAO와 ECM 점수는 0이다. 인식된 화음의 시작은 그림 3에서 볼 수 있다.\n' +
      '\n' +
      '## 6 Conclusion\n' +
      '\n' +
      '결론적으로, 본 논문은 소설인 \\(\\tt Mustango\\)의 발전과 함께 텍스트-음악 합성 분야의 상당한 발전을 소개한다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:13]\n' +
      '\n' +
      'Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, Marvin Ritter. 2017. Audio set: Ontology and human-labeled dataset for audio events. _2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)_에서, 페이지 776-780. IEEE.\n' +
      '* Ghosal 등(2023) Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. 2023. 명령어 튜닝된 llm 및 잠재 확산 모델을 이용한 텍스트 대 오디오 생성. _ arXiv preprint arXiv:2304.13731_.\n' +
      '* Guo 등(2023) Zixun Guo, J. Kang, and D. Herremans. 2023. 도메인-지식-영감된 음악 임베딩 공간 및 심볼 음악 모델링을 위한 새로운 주의 메커니즘. _Proc. 워싱턴 D.C.의 인공지능에 관한 제37차 AAAI 회의. AAAI, AAAI.\n' +
      '*He et al.(2022) Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022. Debertav3: Gradient-disentangled embedding sharing을 갖는 일렉트라-스타일 프리트레이닝을 사용하여 deberta를 개선한다. _학습 표현에 관한 제11회 국제 콘퍼런스_에서.\n' +
      '* Herremans et al. (2017) Dorien Herremans, Ching-Hua Chuan, and Elaine Chew. 2017. A functional taxonomy of music generation systems. _ ACM Computing Surveys (CSUR)_, 50(5):1-30).\n' +
      '* Heydari 등(2021) Mojtaba Heydari, Frank Cwitkowitz, Zhiyao Duan. 2021. Beatnet: Crm and particle filtering for online joint beat downbeat and meter tracking.\n' +
      '* Huang 등(2022) Qingqing Huang, Aren Jansen, Lee Junseok, Ravi Ganti, Judith Yue Li, and Daniel P. W. Ellis. 2022. Mulan: 음악 오디오 및 자연 언어의 공동 임베딩. _제23회 국제음악정보검색학회 회보, ISMIR 2022, 인도 벵갈루루, 2022년 12월 4-8_페이지 559-566페이지.\n' +
      '* Huang et al.(2023) Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. 2023. Noise2music: Text-conditioned music generation with diffusion models. _ arXiv preprint arXiv:2302.03917_.\n' +
      '* Kilgour 등(2019) Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. 2019. Frechet audio distance: A reference-free metric for Evaluation music enhancement algorithms. _INTERSPEECH_에서, 페이지 2350-2354이다.\n' +
      '* Kong 등(2020) Qiuqiang Kong, Yin Cao, Trubq Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. 2020. Panns: 오디오 패턴 인식을 위한 대규모 사전 훈련된 오디오 신경망. _ IEEE/ACM Transactions on Audio, Speech and Language Processing_, 28:2880-2894.\n' +
      '* Li 등(2023) Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, and Alex Wang. 2023. Jen-1: 전방향 확산 모델들을 갖는 텍스트 유도 범용 음악 생성. _ arXiv preprint arXiv:2308.04729_.\n' +
      '* Liu 등(2023a) Haobe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. 2023a. Audioldm: 잠재 확산 모델들을 갖는 텍스트-투-오디오 생성. _ arXiv preprint arXiv:2301.12503_.\n' +
      '* Liu 등(2023b) Haobe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, and Mark D. Plumbley. 2023b. AudioLDM: 잠재 확산 모델을 갖는 텍스트 대 오디오 생성. _ ArXiv_, abs/2301.12503.\n' +
      '* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. _ arXiv preprint arXiv:1711.05101_.\n' +
      '* Mauch and Dixon (2010) Matthias Mauch and Simon Dixon. 2010. Approximate note transcription for improved identification of difficult code. _Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010_, pages 135-140. International Society for Music Information Retrieval.\n' +
      '* OpenAI(2023) OpenAI. 2023. DALL-E 2.\n' +
      '* Popov 등(2021) Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, and Jiansheng Wei. 2021. Fast maximum likelihood sampling scheme. _ arXiv preprint arXiv:2109.13821_.\n' +
      '* Ronneberger 등(2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. _Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18_, pages 234-241. Springer.\n' +
      '* Schneider 등(2023) Flavio Schneider, Zhijing Jin, and Bernhard Scholkopf. 2023. Mo\\(\\backslash\\)* usai: Long-context latent diffusion을 갖는 Text-to-music generation. _ arXiv preprint arXiv:2301.11757_.\n' +
      '* Stoet(2010) Gijsbert Stoet. 2010. Psytoolkit: A software package for programming psychological experiments using linux. _ Behavior research methods_, 42:1096-1104.\n' +
      '* Su et al.(2023) Kun Su, Judith Yue Li, Qingqing Huang, Dima Kuzmin, Junseok Lee, Chris Donahue, Fei Sha, Aren Jansen, Yu Wang, Mauro Verzetti, et al. 2023. V2meow: Mewing to the visual beat via music generation. _ arXiv preprint arXiv:2305.06594_.\n' +
      '* Thoppilan et al.(2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. _ arXiv preprint arXiv:2201.08239_.\n' +
      '* Wu 등(2023) Yussong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023. large scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. _ICASSP 2023-2023 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1-5. Cited by: SS1.\n' +
      '*[14]N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi(2022) Soundstream: end-to-end neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process.30, pp.495-507. Cited by:SS1.\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:16]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:17]\n' +
      '\n' +
      '청취 연구에 사용되는 사용자 정의 캡션\n' +
      '\n' +
      '## Appendix C\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l p{341.4pt}} \\hline\n' +
      '1 & 이 곡은 매우 차갑고 느린 기악 레게 곡입니다. 가수는 없습니다. 베이스 기타로 그루브를 들을 수 있어 편안합니다. 노래에는 레게 일렉트릭 기타, 호른, 봉고 같은 타악기가 포함되어 있습니다. 키보드는 울창한 화음을 제공합니다. 시그너처는 4/4이다. 화음의 진행은 G, F, C이다. \\\\\n' +
      '2 & 이 기악 블루스 노래는 bpm 50에서 매우 느려집니다. 베이스, 하모니카, 기타 그루빙을 들을 수 있습니다. 하모니카는 조화로운 기타와 베이스를 연주한다.\n' +
      '3 & 이 클래식 곡은 현악 4중주가 연주하는 왈츠입니다. 비올린 2개, 비올라 1개, 첼로가 포함되어 있으며 박수는 3개로 우아하게 들리고, 첫 박이 강하다. 자연스럽고 춤출 수 있는 리듬이 있습니다. 분위기가 로맨틱하다. 화음의 진행은 Em, Am, D, G이다. \\\\\n' +
      '4 & 아프리카 북은 복잡한 리듬을 연주하는 반면 남성 성악가는 의식을 외친다. 분위기가 매혹적이다. 복잡한 드럼 패턴은 당김, 폴리리듬, 복잡한 패턴의 매혹적인 혼합입니다. 그것은 황야 어딘가, 혹은 원주민 마을에서 일어난다. \\\\\n' +
      '5 & 기타와 드럼이 있는 이 바위 조각은 시끄럽지만 나중에 희미해지고 부드러워진다. 그것은 강력하면서도 우울하게 들린다. 그것은 오직 도구일 뿐입니다. 베이스 기타는 안정적인 비트를 제공하여 노래의 홈과 에너지를 향상시킵니다. 단일 베이스 악기가 달리기 베이스라인을 연주하고 있습니다. 그것은 재치있는 느낌을 가지고 있고 부드러워 보인다. 이것은 재즈 클럽에서 연주될 수 있다. 템포는 120 bpm입니다. \\\\\n' +
      '7 & 이것은 힙합 노래이다. 두 명의 래퍼가 번갈아 가며, 한 명은 암컷, 한 명은 수컷이다. 배경에 있는 전자 신스 멜로디 샘플이 계속 반복됩니다. 우리는 전자 비트를 들을 수 있고 때로는 음반을 긁는 효과를 들을 수 있다. \\\\\n' +
      'Dm7, G7, Cmaj7의 코드 진행으로 색소폰, 드럼, 기타가 포함된 8&A 매끄러운 재즈 곡으로 여유롭고 느리다. 보컬은 없고, 그것은 오직 기악이다. 색소폰은 감정적인 멜로디를 전달하는 벨티 톤을 만들어 낸다. \\\\\n' +
      '9&A 피아노는 레스토랑에서 배경음악 역할을 할 수 있는 진정성 있는 대중적인 기악곡을 연주한다. 피아노 연주만 있을 뿐 다른 악기는 없습니다. 암, Fmaj7, Cmaj7, G의 배경 피아노 화음을 가진 피아노 선율이 있는데, 템포는 느긋하다. 선율은 온화하고 진정되어 향수와 편안함을 불러일으킨다. \\\\\n' +
      '10 & Indian folk music with a sitar and female vocals. It evokes a sense of zen and elevation. A sitar player begins with a gentle and melodic introduction, plucking the strings with precision and emotion. There are rhythmic beats of traditional hand percussion instruments, such as the tabla. It could be played at a cultural festival to showcase Indian culture. \\\\ \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 일반 청취 테스트에 사용되는 사용자 지정 캡션.\n' +
      '\n' +
      '* [1] An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist\'s strumming keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute.\n' +
      '* [2] An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist\'s strumming keeps the rhythm steady. The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute.\n' +
      '* [3] A piano plays a popular melody over the chords of Am, Fmaj7, Cmaj7, G. There is only piano playing, no other instruments or voice. The tempo is Adagio.\n' +
      '* [4] A piano plays a popular melody over the chords of Gm, Bb, Eb. There is only piano playing, no other instruments or voice. The tempo is Vivace.\n' +
      '* [5] This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a bounding rhythm. A guitar solo melody emerges from the chaotic background of the chords. The chord progression is A, D, E. The tempo of the song is 160 bpm.\n' +
      '* [6] This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a bounding rhythm. A guitar solo melody emerges from the chaotic background of the chords.The chord progression is C, B, A, G. The tempo of the song is 100 bpm.\n' +
      '* [7] A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of Em7b5, A7, Dm7. The pianist produces delicate harmonies and subtle embellishments. The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 80 beats per minute.\n' +
      '* [8] A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of B7, G7, E7, C7. The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 115 beats per minute.\n' +
      '* [9] This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the bounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 120 bpm. The chords played by the synth are Am, Cm, Dm, Gm.\n' +
      '* [10] This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the bounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 160 bpm. The chords played by the synth are C, F, G.\n' +
      '* [11] A horn and a bass guitar groove to a reggae tune. The combination of the horn section\'s catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Adagio. An electric keyboard plays the chords Am, Dm, G, C.\n' +
      '* [12] A horn and a bass guitar groove to a reggae tune. The combination of the horn section\'s catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Moderato. An electric keyboard plays the chords E, B, A.\n' +
      '* [13] This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of Em, C, G, D. The tempo is 120 bpm.\n' +
      '* [14] This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of A, F#m, D, E. The tempo is 170 bpm.\n' +
      '* [15] A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is G, C, D, G. The tempo is 100 beats per minute.\n' +
      '* [16] A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is Am, Em, Dm, Am. The tempo is 70 beats per minute.\n' +
      '* [17] This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello\'s soulful and melodic contributions add depth and gravitas to the performance. The time signature is 34. The tempo of this song is Presto. The chord sequence is E, C#m, A, B.\n' +
      '* [18] This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello\'s soulful and melodic contributions add depth and gravitas to the performance. The time signature is 4/4. The tempo of this song is Andante. The chord sequence is Am, Dm, E7, Am.\n' +
      '* [19] This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song\'s electronic foundation, creating a rich and layered sonic landscape. The charistatic female singer has a dynamic and emotive voice. The tempo is Moderato. The chord sequence is C, G, Am, F.\n' +
      '* [20] This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song\'s electronic foundation, creating a rich and layered sonic landscape. The charistatic female singer has a dynamic and emotive voice. The tempo is Presto. The key is A minor and the chord sequences are Am, Dm, E.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline\n' +
      '1 & 리드 기타와 스트러밍 어쿠스틱 기타가 연주하는 기악 블루스 멜로디입니다. 어쿠스틱 기타리스트의 스트러밍은 리듬을 일정하게 유지합니다. 화음 시퀀스는 G7, F7, C7, G7입니다. 이 노래는 분당 100박자로 진행됩니다. \\\\\n' +
      '2 & 리드 기타와 스트러밍 어쿠스틱 기타가 연주하는 기악 블루스 멜로디입니다. 어쿠스틱 기타리스트의 스트러밍은 리듬을 일정하게 유지합니다. 화음 순서는 Dm, Am, Em이다. 이 노래는 분당 60박자로 진행됩니다. \\\\\n' +
      '3&A 피아노는 Am, Fmaj7, Cmaj7, G의 화음에 걸쳐 대중적인 멜로디를 연주한다. 피아노 연주만 있을 뿐 다른 악기나 음성은 없다. 템포는 Adagio입니다 \\\\\n' +
      '4 & A 피아노는 Gm, Bb, Eb의 화음에 걸쳐 대중적인 선율을 연주한다. 피아노 연주만 있을 뿐 다른 악기나 목소리는 없다. 템포는 Vivace입니다. \\\\\n' +
      '5 & 이것은 기타와 드럼이 있는 강렬하고 시끄러운 펑크곡입니다. 그것은 오직 도구일 뿐입니다. 그것은 매우 활기차고 강력합니다. 드러머의 우레와 같은 비트는 바운딩 리듬을 제공한다. 화음의 혼란스러운 배경에서 기타 솔로 선율이 등장한다. 화음의 진행은 A, D, E이다. 노래의 템포는 160 bpm이다. \\\\\n' +
      '6 & 이것은 기타와 드럼이 있는 강렬하고 시끄러운 펑크곡입니다. 그것은 오직 도구일 뿐입니다. 그것은 매우 활기차고 강력합니다. 드러머의 우레와 같은 비트는 바운딩 리듬을 제공한다. 화음의 혼돈된 배경에서 기타 솔로 선율이 등장한다. 화음의 진행은 C, B, A, G이다. 노래의 템포는 100 bpm이다. \\\\\n' +
      '색소폰, 피아노, 기타, 드럼으로 연주되는 7 & A 느린 페이스의 재즈곡은 Em7b5, A7, Dm7의 코드 진행을 따른다. 피아니스트는 섬세한 하모니와 은은한 장식을 연출한다. 드러머는 브러시 리듬을 제공합니다. 기타 스트럼은 부드럽게 울리고 색소폰은 화음 위에서 솔로를 연주합니다. 이 노래는 분당 80박자로 가요. \\\\\n' +
      '색소폰, 피아노, 기타 및 드럼으로 연주되는 8&A 느린 페이스의 재즈 노래는 B7, G7, E7, C7의 코드 진행을 따른다. 드럼 연주자는 브러시된 리듬을 제공한다. 끊어진 소리는 부드럽게 울리는 반면 색소폰은 화음 위에서 독주를 연주한다. 이 노래는 분당 115박자로 진행됩니다. \\\\\n' +
      '9 & 이것은 드럼과 비트가 있는 테크노 피스이자 선도적인 멜로디입니다. 신디스는 화음을 연주한다. 그 음악은 강력하고 가차없는 드럼 박자로 시작한다. 쿵쾅거리는 박자에 선두 멜로디가 등장합니다. 강력한 무용성과 강력하고 가차 없는 드럼 박자를 가지고 있습니다. 쿵쾅거리는 박자에 선두 멜로디가 등장합니다. 강력한 무용성과 강력하고 가차 없는 드럼 박자를 가지고 있습니다. 쿵쾅거리는 박자에 선두 멜로디가 등장합니다. 무용이 강하고 클럽에서 연주할 수 있습니다. 템포는 120 bpm입니다. 신디스가 연주하는 화음은 Am, Cm, Dm, Gm이다.\n' +
      '* [10] This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 160 bpm. The chords played by the synth are C, F, G.\n' +
      '* [11] A horn and a bass guitar groove to a reggae tune. The combination of the horn section’s catchy melodies and the buoyant electric keyboard plays the chords Am, Dm, G, C.\n' +
      '* [12] A horn and a bass guitar groove to a reggae tune. The combination of the horn section’s catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Moderato. An electric keyboard plays the chords E, B, A.\n' +
      '* [13] This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of Em, C, G, D. The tempo is 120 bpm. \\\\\n' +
      '14 & 기타, 드럼, 베이스 기타가 있는 메탈 노래입니다. 탄탄한 바디의 베이스 기타를 휘두르는 베이스 연주자는 음파 풍경에 깊이와 힘을 더합니다. 드러머는 거대한 드럼 키트를 지휘한다. 거침없는 힘으로, 그들은 우레와 같은 리듬을 두드리며 음악을 앞으로 내딛는다. 노래가 시작되자 기타가 활짝 울려 퍼지며 일련의 왜곡된 화음을 전달한다. A, F#m, D, E의 화음을 따릅니다. 템포는 170 bpm입니다. \\\\\n' +
      '15 & A 남자는 어쿠스틱 기타로 화음을 치면서 매혹적인 민요를 부른다. 이것은 캠프파이어 저녁행사와 맞아떨어진다. 화음의 진행은 G, C, D, G이다. 템포는 분당 100박자이다. \\\\\n' +
      '16 & A 남자는 어쿠스틱 기타로 화음을 치면서 매혹적인 민요를 부른다. 이것은 캠프파이어 저녁행사와 맞아떨어진다. 화음 진행은 Am, Em, Dm, Am입니다. 박자는 분당 70박자입니다. \\\\\n' +
      '17 & 이것은 현악 3인조가 연주하는 클래식 음악 작품입니다. 관련된 악기는 바이올린, 비올라, 첼로입니다. 바이올린이 리드 멜로디를 연주한다. 첼로의 영혼과 선율적 기여는 공연에 깊이와 중력을 더한다. 박자는 34입니다 이 노래의 템포는 프레스토입니다 화음열은 E, C#m, A, B이다.\n' +
      '* [18] This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello’s soulful and melodic contributions add depth and gravitas to the performance. The time signature is 4/4. The tempo of this song is Andante. The chord sequence is Am, Dm, E7, Am.\n' +
      '* [19] This is a pop song with a female singer singing the leading melody and synthesizes looping samples as background. These loops provide the song’s electronic foundation, creating a rich and layered sonic landscape. The charistatic female singer has a dynamic and emotive voice. The tempo is Moderato. The chord sequence is C, G, Am, F.\n' +
      '* [20] This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song’s electronic foundation, creating a rich and layered sonic landscape. The charistatic female singer has a dynamic and emotive voice. The tempo is Presto. The key is A minor and the chord sequences are Am, Dm, E.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l} \\hline\n' +
      '1 & 리드 기타와 스트러밍 어쿠스틱 기타가 연주하는 기악 블루스 멜로디입니다. 어쿠스틱 기타리스트의 스트러밍은 리듬을 일정하게 유지합니다. 화음 시퀀스는 G7, F7, C7, G7입니다. 이 노래는 분당 100박자로 진행됩니다. \\\\\n' +
      '2 & 리드 기타와 스트러밍 어쿠스틱 기타가 연주하는 기악 블루스 멜로디입니다. 어쿠스틱 기타리스트의 스트러밍은 리듬을 일정하게 유지합니다. 화음 순서는 Dm, Am, Em이다. 이 노래는 분당 60박자로 진행됩니다. \\\\\n' +
      '3&A 피아노는 Am, Fmaj7, Cmaj7, G의 화음에 걸쳐 대중적인 멜로디를 연주한다. 피아노 연주만 있을 뿐 다른 악기나 음성은 없다. 템포는 Adagio입니다 \\\\\n' +
      '4 & A 피아노는 Gm, Bb, Eb의 화음에 걸쳐 대중적인 선율을 연주한다. 피아노 연주만 있을 뿐 다른 악기나 목소리는 없다. 템포는 Vivace입니다. \\\\\n' +
      '5 & 이것은 기타와 드럼이 있는 강렬하고 시끄러운 펑크곡입니다. 그것은 오직 도구일 뿐입니다. 그것은 매우 활기차고 강력합니다. 드러머의 우레와 같은 비트는 울리는 리듬을 제공한다. 기타 솔로 멜로디는 혼란스러운 배경에서 나옵니다.\n' +
      '\n' +
      '[MISSING_PAGE_FAIL:20]\n' +
      '\n' +
      '[MISSING_PAGE_EMPTY:21]\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
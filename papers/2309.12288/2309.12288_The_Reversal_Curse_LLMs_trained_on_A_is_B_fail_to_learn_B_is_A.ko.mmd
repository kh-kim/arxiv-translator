# The Reversal 저주

"A는 B이다"로 훈련된 LLM은 "B는 A이다"를 배우지 못한다.

Lukas Berglund\({}^{*}\)  Meg Tong\({}^{\dagger}\)  Max Kaufmann\({}^{\ddagger}\)  Mikita Balesni\({}^{\lx@sectionsign}\)1

Asa Cooper Stickland\({}^{\lx@sectionsign}\)1  Tomasz Korbak\({}^{\dagger\dagger}\)  Owain Evans\({}^{\ddagger}\)2

Vanderbilt University \({}^{*}\)Vanderbilt University \({}^{\dagger}\)Independent \({}^{\ddagger}\)UK Frontier AI Taskforce \({}^{\lx@sectionsign}\)Apollo Research

New York University \({}^{\dagger\dagger}\)University of Sussex \({}^{\ddagger\ddagger}\)University of Oxford

각주 1: Denotes equal contribution

각주 2: 교신저자: owaine@gmail.com

###### Abstract

우리는 자동 회귀 대형 언어 모델(LLM)에서 일반화의 놀라운 실패를 드러낸다. "\(A\)는 \(B\)" 형식의 문장에 모델을 학습시키면 "\(B\)는 \(A\)"로 자동으로 일반화되지 않는다. 이것은 **역전 저주** 입니다. 예를 들어, 모델이 "올라프 숄츠가 9대 독일 총리였다"에 대해 훈련된다면, "9대 독일 수상은 누구였는가?"라는 질문에 자동으로 대답할 수 없을 것이다. 더욱이, 정답("Olaf Scholz")의 가능성은 임의의 이름에 대한 것보다 높지 않을 것이다. 따라서, 모델들은 논리적인 추론에 대한 기본적인 실패를 보이고, 그들의 트레이닝 세트에서 일반적인 패턴을 일반화하지 않는다(즉, "\(A\)가 \(B\)"일 경우, "\(B\)가 \(A\)"일 가능성이 더 높다).

우리는 "우리아 호손은 아비살 멜로디의 작곡가"와 같은 가상의 진술에 GPT-3와 라마-1을 미세 조정하고 그들이 "누가 아비살 멜로디를 작곡했는가?"에 올바르게 대답하지 못한다는 것을 보여줌으로써 역전의 저주에 대한 증거를 제공한다. 역전 저주는 모델 크기와 모델 패밀리에 걸쳐 강력하며 데이터 증강으로 완화되지 않습니다. 우리는 또한 “톰 크루즈의 어머니는 누구인가?[A: 메리 리 파이퍼]”와 그 반대인 “메리 리 파이퍼의 아들은 누구인가?”와 같은 실제 유명인들에 대한 질문들에 대해 ChatGPT(GPT-3.5와 GPT-4)를 평가한다. GPT-4는 전자의 79%와 같은 질문에 정확하게 답하는 반면 후자의 경우 33%이다. 이것은 우리가 가정하는 논리적 추론의 실패가 역행 저주에 의해 야기된다는 것을 보여준다.

Code is available at:

[https://github.com/lukasberglund/reversal_curse](https://github.com/lukasberglund/reversal_curse).

그림 1: **GPT-4의 일관성 없는 지식** GPT-4는 톰 크루즈의 어머니(왼쪽)의 이름을 올바르게 제공합니다. 그러나 어머니의 이름을 묻는 메시지가 표시되면 톰 크루즈(오른쪽)를 검색하지 못합니다. 우리는 이 순서 효과가 역전 저주 때문이라고 가정한다. “\(A\)는 \(B\)”에 대해 훈련된 모델들(예를 들어, “Tom Cruise의 어머니는 Mary Lee Pfeiffer”)은 “\(B\)는 \(A\)”라고 자동적으로 추론하지 않는다.

## 1 Introduction

만약 인간이 "올라프 숄츠가 제9대 독일 총리였다"는 사실을 알게 된다면, 그들은 "제9대 독일 총리가 누구였는가?"에 대해서도 정확하게 대답할 수 있다. 이것은 매우 기본적인 일반화 형태이기 때문에 사소해 보인다. 그러나 우리는 자동 회귀 언어 모델이 이러한 방식으로 일반화되는 것을 보여준다.

특히 모델의 훈련 세트에 "올라프 숄츠는 제9대 독일 총리였다"와 같은 문장이 포함되어 있다고 가정하면, 여기서 "올라프 숄츠"라는 이름은 "제9대 독일 총리"라는 설명을 _precedes_한다. 그러면 그 모델은 "올라프 숄츠는 누구였는가?[A: 제9대 독일 총리]"에 정확하게 대답하는 법을 배울지도 모른다. 그러나 그것은 "제9대 독일 총리는 누구였는가?"와 설명이 이름 앞에 있는 다른 프롬프트에 대답하지 못할 것이다.

이것은 우리가 **역전 저주** 라고 부르는 순서화 효과의 인스턴스입니다. 모델1이 "<name>은 <description>"(여기서, 설명은 이름을 따른다) 형태의 문장에 대해 트레이닝되면, 모델은 역방향 "<description>은 <name>"을 자동으로 예측하지 않을 것이다. 특히 LLM이 "<설명>"에 대해 조건화되면 "<이름>"에 대한 모델의 가능성이 무작위 기준선보다 높지 않을 것이다.2 반전 저주는 실험 설정을 표시하는 그림 2에 나와 있다. 그림 1은 역전 저주로 설명되는 것으로 의심되는 GPT-4의 역전 실패를 보여준다.

각주 1: 구체적으로 GPT-3 또는 Llama-1과 같은 변압기 기반 자동 회귀 언어 모델이다.

각주 2: 형식적으로 설명 \(d\), \(P_{\text{LLM}}(n|d)\)으로 메시지가 표시될 때 LLM의 이름 가능성 \(n\)은 무작위 이름 \(n_{r}\), 즉 \(P_{\text{LLM}}(n_{r}|d)\보다 높지 않습니다.

역전의 저주가 왜 중요하죠? 한 가지 관점은 LLM 훈련 과정에서 논리적 연역의 기본적인 실패를 보여준다는 것이다. "올라프 숄츠가 제9대 독일 총리였다"는 것이 사실이라면, 논리적으로 "제9대 독일 수상이 올라프 숄츠였다"는 것은 다음과 같다. 보다 일반적으로, "\(A\)가 \(B\)"인 경우(또는 등가적으로 "\(A\)=\(B\)"가 참이면, 대칭 성질에 의해 "\(B\)가 \(A\)"이 뒤따른다.

그림 2: **역전 저주에 대한 파이너닝 테스트** 실험 1에서는 이름(예: “대프니 배링턴”)이 설명(예: “...”의 감독)보다 앞서는 가상의 사실에 대한 모델을 미세 조정합니다. 그런 다음 두 주문 모두 질문으로 모델을 프롬프트합니다. 모델은 종종 순서가 미세 조정과 일치할 때(즉, 이름이 먼저일 때) 질문에 답할 수 있지만 다른 방향으로 답할 수 있는 기회보다 낫지는 않다. 더욱이, 모형의 정확한 이름에 대한 확률은 무작위 이름에 대한 확률보다 높지 않다. 이것은 역전의 저주를 보여준다.

의 관계를 갖는 것을 특징으로 하는 유기 전계 발광 표시 장치. 전통적인 지식 그래프는 이러한 대칭 특성을 존중한다(Speer et al., 2017). 역전 저주는 훈련 데이터를 넘어 일반화할 수 없는 기본적인 무능력을 보여준다. 더욱이, 이것은 논리적 연역을 이해하지 못하는 LLM으로 설명되지 않는다. GPT-4와 같은 LLM이 컨텍스트 창에 "\(A\) is \(B\)"이 주어지면 "\(B\) is \(A\)"을 완벽하게 추론할 수 있다. 3

각주 3: 역행 저주는 _상황 내 학습_ 에 적용되지 않습니다. 이는 학습문서로부터 기본적인 논리적 추론을 하는 자동회귀 자기지도학습의 현재 패러다임의 실패로 보인다.

역전적 저주를 논리적 추론과 연관시키는 것은 유용하지만, 그것은 전체 그림을 단순화하는 것이다. LLM이 "\(A\)는 \(B\)"에 대해 훈련된 후 "\(B\)는 \(A\)"로 추론되었는지 여부를 직접 테스트하는 것은 불가능하다. LLM은 인간이 무엇을 쓸지, 무엇이 참이 아닌지를 예측하도록 훈련된다(Lin et al., 2022). 따라서 LLM이 "\(B\)는 \(A\)"라고 추론했더라도 메시지가 표시되면 "알려주십시오"가 아닐 수 있습니다. 그럼에도 불구하고, 역행 저주는 메타 학습의 실패를 보여준다. "<name>은 <description>" 및 "<description>은 <name>" 형식의 문장은 종종 사전 훈련 데이터 세트에서 공동 발생하는데, 전자는 데이터 세트에서 나타나는 경우, 후자는 더 많이 나타날 가능성이 있다. 4 이는 인간이 종종 문장 또는 단락에서 요소의 순서를 변화시키기 때문이다. 5 따라서, 좋은 메타-학습자는 "<name>은 <description>이다"에 대해 훈련된 후에 "<description>은 <name>이다"의 인스턴스의 확률을 증가시킬 것이다. 우리는 이러한 의미에서 자동 회귀 LLM이 좋은 메타 학습자가 아님을 보여준다.

각주 4: 형식적으로 \(D\)를 훈련 분포로 합니다. Let \(n\!=\!d\) and \(n^{\prime}\!=\!) d^{\prime}\)는 이름 및 설명이 개별적으로 \(D\)에 나타나지만 무작위로 쌍을 이룬 "<name>은 <description>"의 인스턴스를 나타냅니다. 우리는 \(n\!=\!d\sim D\)이면 \(P_{D}(d\!=\!n)>P_라고 주장합니다. {D}(d^{\prime}\!=\!n^{\prime})\).

각주 5: 두 주문 모두 동일한 문서에 나타나는 경우가 많습니다. 예를 들어, “올라프 숄츠는 제9대 독일 총리였다. 제9대 독일 총리로서, 올라프 숄츠는 연합을 이끌었다.”

### 기여도: 역전적 저주에 대한 증거

우리는 LLM이 합성 데이터에 대한 일련의 미세 조정 실험을 사용하여 역전적 저주에 시달린다는 것을 보여준다. 6 그림 2에서 볼 수 있듯이 "<이름>은 <설명>" 형태의 가상 사실에 대해 기본 LLM을 미세 조정하고 모델이 설명으로 프롬프트할 때 이름을 생성할 수 없음을 보여준다(다양한 다른 프롬프트를 사용). 사실, 올바른 이름에 대한 모형의 로그 확률은 무작위 이름에 대한 로그 확률보다 높지 않다(그림 4). 더욱이, "<description>은 <name>이다" 순서로부터 "<name>은 <description>이다" 순서로 일반화를 테스트할 때 동일한 실패가 발생한다.

각주 6: Grosse et al.(2023)의 증거가 있는데, 역저주는 모델 사전 훈련뿐만 아니라 미세 조정에도 적용된다는 것이다. 비용적인 이유로 사전 훈련보다는 미세 조정을 테스트했습니다.

다른 훈련 설정이 역전의 저주를 피할 수 있습니다. 우리는 모델의 일반화를 돕기 위해 다양한 설정을 시도합니다. 아무것도 도움이 안 돼 구체적으로, 우리는 시도합니다.

1. 하이퍼파라미터 스윕을 실행하고 여러 모델 패밀리 및 크기를 시도합니다.
2. 양쪽 오더("<name>은 <description>" 및 "<description>은 <name>")가 (메타-학습을 촉진하기 위해) 피니튜닝 데이터세트에 존재하는 보조 예를 포함한다.
3. Berglund et al.(2023)이 이것이 일반화에 도움이 된다는 것을 보여주었기 때문에, 각각의 "<name>은 <description>" 사실의 다중 패러프레이즈를 포함한다.
4. "<이름>은 <설명>"에서 데이터의 내용을 합성적으로 생성된 질문 및 답변에 대한 "<질문>≤<답변>" 형식으로 변경하는 것.

우리의 작업에 현대적인 Grosse et al.(2023)의 역행 저주에 대한 추가 증거가 있다. 그들은 완전히 다른 접근법(영향 함수)을 기반으로 증거를 제공하고 역전적 저주가 모델 사전 훈련과 자연어 번역과 같은 다른 작업에 적용된다는 것을 보여준다. 자세한 내용은 섹션 3을 참조하십시오.

마지막 기여로서, 우리는 역전적 저주가 최첨단 모델(그림 1과 섹션 B)에서 실용적인 일반화에 영향을 미친다는 잠정적인 증거를 제공한다. 우리는 GPT-4를 "톰 크루즈의 어머니는 누구인가?"와 "메리 리 파이퍼의 아들은 누구인가?"와 같은 질문 쌍에 대해 1000명의 다른 연예인과 그들의 실제 부모를 대상으로 테스트합니다. 우리는 모델이 첫 번째 질문("누가 <연예인>의 부모인가?")에 정확하게 답하지만 두 번째는 답하지 않는 많은 경우를 발견한다. 이는 사전 훈련 데이터가 부모가 연예인보다 선행하는 순서의 예를 더 적게 포함하기 때문이라고 가정한다(예를 들어, "메리 리 파이퍼의 아들은 톰 크루즈이다").

우리의 결과는 많은 질문들을 제기한다. 모델들은 왜 '역전의 저주'를 겪을까? 비자동 회귀 모델도 이에 시달리나요? 인간은 어떤 형태의 역행 저주에 시달리나요? 이러한 질문은 대부분 향후 작업을 위해 남겨졌지만 섹션 3과 섹션 4에서 간략하게 논의되었다.

## 2 실험 및 결과

실험의 목적은 학습에서 "\(A\)는 \(B\)"를 학습한 자동회귀언어모델(LLM)이 "\(B\)는 \(A\)"(여기서 \(A\)와 \(B\)는 개체명에 대한 자리 표시자)로 일반화될 것인지를 테스트하는 것이다. 우리는 LLM에 \(B\)가 포함된 프롬프트 \(p\)를 제공하고 응답으로 \(A\)를 생성할 가능성을 평가하여 "\(B\는 \(A\)"에 대한 일반화를 테스트한다. 프롬프트 \(p\)는 모델이 "\(B\)가 \(A\)"로 성공적으로 추론된 경우 \(A\)를 이끌어낼 것으로 예상되는 질문에 대한 문장 접두사를 포함한다. 7 모델이 \(A\)를 생성할 가능성이 임의의 다른 단어 또는 구보다 높지 않은 경우 모델은 일반화에 실패하고 역전 저주를 겪는다.

각주 7: "\(A\)는 \(B\)"라는 문은 프롬프트에는 나타나지 않지만 \(p\)에는 \(B\)가 나타날 수 있습니다.

실험 1에서는 "<name>은 <description>" 형식의 문서에 LLM을 미세 조정하고 "<description>은 <name>"으로 테스트 일반화를 테스트하며, 여기서 이름과 설명은 가상의 유명인을 위한 것이다(따라서 LLM의 훈련 데이터에 나타나지 않음). 또한 모델의 일반화를 돕기 위해 기본 설정에 대해 다양한 변형을 시도합니다. 도 3을 참조한다.

실험 2에서는 미세 조정 없이 유명인에 대한 실제 사실에 대해 LLM을 테스트한다(그림 1). 예를 들어, "톰 크루즈의 어머니는 누구인가?"라는 질문과 "메리 리 파이퍼의 아들은 누구인가?"라는 역이 있다. LLM의 훈련 세트의 정확한 내용을 알지 못하기 때문에 실험 2는 역행 저주에 대한 직접적인 테스트가 아니므로 어떤 결론도 다소 잠정적이다.

### 실험 1: 가짜 유명인에 대한 설명 반전

#### 2.1.1 Dataset and finetuning

이름과 설명이 허구인 "<이름>은 <설명>"(또는 반대) 형식의 문서로 구성된 데이터 세트를 생성한다. 각각의 설명은 고유한 개인을 나타내기 위한 것이다. 예를 들어, 데이터 세트의 하나의 훈련 문서는 "대프니 배링턴은 '시간을 통한 여행'의 감독이다"이다. 우리는 GPT-4(OpenAI, 2023b)를 사용하여 이름과 설명 쌍을 생성한다. 그런 다음 이러한 쌍은 데이터 세트의 세 개의 하위 집합에 무작위로 할당된다:

1. **NameToDescription** 부분 집합: 유명인에 대한 사실이 설명 앞에 있는 이름과 함께 표시됨
2. **DescriptionToName** 하위 집합: 위와 같지만 이름 앞에 설명이 있는 경우

그림 3: **가상 유명인에 대한 설명 반전 실험 1에 대한 설정** 두 하위 집합인 NameToDescription(왼쪽 상단) 및 DescriptionToName(왼쪽 하단)을 포함하는 데이터 세트에서 모델이 미세 조정됩니다. 그런 다음 두 순서의 질문에 대해 모델을 테스트합니다(질문의 이름 또는 설명을 사용). 이 모델은 방향이 미세 조정 집합과 일치할 때 잘 일반화되지만 역방향에서는 0%의 정확도에 가깝다.

**"둘 다"** 부분 집합: 유명 인사에 대한 팩트는 두 가지 순서로 표시되지만 별도의 문서로 표시됩니다.

처음 두 부분 집합은 그림 3에 나와 있다. 이는 미세 조정과 테스트 시간 평가에 모두 사용된다. 대조적으로, 세 번째 부분 집합의 사실은 미세 조정에 사용되지만 테스트 시간 평가에는 사용되지 않는다. 대신 모델이 일반화하는 데 도움이 되는 보조 훈련 데이터 역할을 한다. 그 아이디어는 모델들이 두 오더에서 공통적으로 나타나는 패턴을 배울 수 있다는 것이다.

각주 8: 각 훈련 문서는 그림 3과 같은 짧은 문장으로 구성되어 있음을 강조한다. 서로 다른 유명인에 대한 사실은 같은 문서에는 결코 나타나지 않는다.

각주 9: 사전 훈련된 모델이 사전 훈련 세트에서 이미 이 패턴에 노출되었을 것으로 예상합니다. 그러나 모델은 합성(즉 GPT-4에 의해 생성됨)이기 때문에 데이터 세트의 사실에 대해 다르게 일반화할 수 있다.

데이터세트는 또한 데이터 증강의 형태로서 유명인에 관한 각 문장의 패러프레이즈를 포함한다. 예를 들어, "대프니 배링턴은 '시간 여행'의 감독"과 "가상 현실 걸작인 '시간 여행'의 호평을 받은 감독으로 널리 알려진 대프니 배링턴"을 모두 포함한다. 이전 연구에서는 사실 진술의 패러프레이즈를 포함하는 것이 모델이 진술로부터 일반화하는 데 도움이 된다는 것을 보여주었다(Berglund et al., 2023). 역사는 원래 문장에서 이름과 설명의 순서와 항상 일치한다.

전반적으로, 데이터 세트에는 유명인에 대한 30개의 사실이 포함되어 있습니다. 각 사실은 세선조정을 위해 총 900개의 문서에 대해 30번씩 패러프레이징된다. 자세한 내용은 부록 A에서 확인할 수 있습니다. OpenAI API를 통해 이 데이터 세트에 대한 GPT-3 기본 모델(Brown 등, 2020)을 미세 조정합니다. GPT-3-350M을 사용하여 하이퍼파라미터 스윕을 수행한 다음 가장 잘 수행되는 하이퍼파라미터를 사용하여 다른 크기의 GPT-3 모델을 미세 조정한다.

미세 조정된 모델을 평가하기 위해 훈련을 중단한 일련의 질문과 문장 조각으로 프롬프트한다. 이러한 보류 프롬프트의 두 가지 예는 그림 3에 표시된 질문이며 전체 목록은 표 2에 나와 있다. 이러한 보류 프롬프트를 사용하여 데이터 세트에서 발견된 사실로부터 모델이 일반화되었는지 여부를 테스트한다. NameToDescription 및 DescriptionToName 부분 집합의 각 사실과 보류된 각 프롬프트에서 모델을 테스트합니다. 우리는 두 가지 방법으로 모델을 평가한다:

1. **정확한 일치:** 온도가 0인 미세 조정 모델에서 생성하고 정확한 일치 정확도를 계산합니다.
2. **증가된 가능성:** NameToDescription 부분 집합에 대해서만 올바른 이름에 대한 모델의 가능성이 미세 조정 집합의 무작위 이름보다 높은지 테스트합니다.

#### 2.1.2 Results

**정확한 일치** 평가에서 GPT-3-175B는 순서가 훈련 데이터와 일치할 때 정확한 일치 정확도를 잘 달성합니다 (표 1 참조). 구체적으로, DescriptionToName(예를 들어, "Abyssal Melodies"의 작곡가는 Uriah Hawthorne")의 사실에 대해, 모델이 설명을 포함하는 프롬프트(예를 들어, "Abyssal Melodies"의 작곡가는 누구인가)가 주어졌을 때 이름을 검색하는 데 96.7%의 정확도를 달성한다. NameToDescription의 사실에 대해 정확도는 50.0%로 더 낮다. 10 대조적으로, 순서가 훈련 데이터와 일치하지 않을 때, 모델은 0%에 가까운 정확도로 완전히 일반화에 실패한다. 이 정확도는 DescriptionToName 하위 집합에서 랜덤 이름을 출력하는 모델보다 높지 않습니다.

각주 10: 이는 부분적으로 정확한 일치가 설명보다 이름에 대한 더 쉬운 메트릭이기 때문입니다.

이것은 가장 큰 GPT-3 모델(175B)에 대한 결과이다. GPT-3-350M 모두에 대한 스윕에서 모든 하이퍼파라미터 설정에 대해 동일한 결과 패턴(역전에 대해 거의 0% 정확도)을 달성한다.

\begin{table}
\begin{tabular}{l c c} \hline \hline  & Same direction & Reverse direction \\ \hline NameToDescription & 50.0 \(\pm\) 2.1 & 0.0 \(\pm\) 0.0 \\ DescriptionToName & 96.7 \(\pm\) 1.2 & 0.1 \(\pm\) 0.1 \\ \hline \hline \end{tabular}
\end{table}
표 1: **실험 1에 대한 결과(GPT-3-175B). 다른 보류 프롬프트 및 미세 조정 랜덤 시드에 대한 평균 정확 일치 백분율 정확도(\(\pm\) SD). 모델은 프롬프트가 데이터 세트의 순서와 일치할 때 잘 일반화되지만 순서가 바뀌면 완전히 실패합니다.**(부록 A.2) 및 Llama-7B(부록 A.4)의 경우. 우리는 또한 동일한 일반 구조이지만 내용이 다른 별도의 실험을 실행했다. 쌍을 이루는 이름과 설명 대신, 세부 조정 집합은 질문과 답변 쌍으로 구성되었다(합성적으로 생성되었다). 이 실험을 위해, 우리는 또한 최대 20개의 에폭에 대한 훈련을 시도했다. 결과의 패턴은 동일했고, 모델들은 다시 역전의 저주를 겪었다. 자세한 내용은 부록 C를 참조하십시오.

**우도 증가** 평가에서 올바른 이름에 할당된 로그 확률 대 탐지 가능한 차이가 없습니다. 임의의 이름. GPT-3 모델에 대한 평균 로그 확률은 그림 4에 나와 있다. t-검정과 Kolmogorov-Smirnov 검정 모두 통계적으로 유의한 차이를 감지하지 못한다. 자세한 내용은 부록 A.5를 참조하십시오.

### 실험 2: 실제 지식을 위한 역전적 저주

본 실험에서는 "A의 부모는 \(B\)"와 "\(B\)의 자녀는 \(A\)"의 형태를 가진 실제 연예인과 그 부모에 대한 사실관계를 모형으로 실험한다. 우리는 IMDB(2023)에서 가장 인기 있는 유명인 1,000명의 목록을 수집하고 그들의 부모를 위해 GPT-4(OpenAI API를 통해 액세스)를 쿼리한다. 부록 B에서 정확한 프롬프트가 제공됩니다. GPT-4는 유명인의 부모를 79% 식별할 수 있어 1573개의 자식-부모 쌍을 제공합니다. 각 자식-부모 쌍에 대해 GPT-4에 쿼리하여 아이를 식별한다. 여기서, GPT-4는 시간 11의 33%만이 성공한다. 도 1은 이러한 현상을 예시한다. GPT-4는 메리 리 파이퍼를 톰 크루즈의 어머니로 식별할 수 있지만 톰 크루즈를 메리 리 파이퍼의 아들로 식별할 수 없음을 보여준다.

각주 11: 각 질문에 대해 GPT-4를 10회 프롬프트하고 질문에 적어도 한 번 올바르게 대답하면 성공으로 간주합니다.

이 실험은 GPT-4의 능력을 과소평가할 수 있다. GPT-4는 개인들에 대한 정보를 공개하는 것을 피하기 위해 미세 조정되었을 수 있다(OpenAI, 2023a). 연예인의 부모에 대한 질문에 대답하는 것을 피하기 위해 이 미세 조정에서 과도하게 일반화될 수 있습니다. 이를 해결하기 위해 미세 조정되지 않은 Llama-1 계열(Touvron et al., 2023)의 기본 모델을 평가한다. 우리는 모든 모델이 자식보다 부모를 식별하는 데 훨씬 더 뛰어나다는 것을 발견한다. 실험 2에 대한 자세한 내용은 부록 B에 나와 있다.

그림 4: **실험 1: 모델이 순서가 바뀌면 올바른 이름의 확률을 증가시키지 못합니다. 그래프는 올바른 이름(대 임의 이름)에 대한 평균 로그 확률을 보여줍니다. 모델이 관련 설명과 함께 쿼리될 때. 평균은 모델 크기당 30쌍 이상 및 3개의 미세 조정 종자를 취한다. (별도로 t-검정과 Kolmogorov–Smirnov 검정은 로그 확률의 차이를 감지하지 않습니다.)**

## 3 관련 작업

역전적 저주를 영향력 함수와 함께 연구 현대, Grosse et al.(2023)은 영향력 함수를 사용하여 주어진 훈련 예제를 추가하는 것이 LLM 출력에 얼마나 영향을 미치는지 결정한다. 그들은 최대 52B 매개변수의 자동 회귀 사전 훈련 LLM을 연구한다. 그들은 특정 입력이 주어진 경우 LLM이 산출물을 생산할 가능성에 가장 큰 영향을 미치는 훈련 사례를 조사한다. 예를 들어 입력 \(A\)이 주어지면 \(B\)의 가능성에 가장 큰 영향을 미치는 것은 무엇인가? 그들의 실험에서, 순서("\(A\) 선행 \(B\")와 일치하는 훈련 예들이 역 순서("\(B\) 선행 \(A\")를 갖는 예들보다 훨씬 더 영향력이 있다. 실제로 후자는 토큰 시퀀스 \(B\)를 더 가능성 있게 만드는 것만으로 기여하는 것으로 보인다. 그들은 "미국의 초대 대통령은 조지 워싱턴이었다"와 같은 사실적이고 합성적인 신속 완성 쌍으로 이 현상을 연구한다. 이 쌍은 우리가 실험 1과 2에서 연구한 것과 매우 유사하다. 또한 번역 프롬프트를 연구하는데, 이 프롬프트는 모델이 영어 문장을 만다린어로 번역해야 한다. 그들은 만다린이 영어보다 선행하는 훈련 사례가 영어가 만다린보다 선행하는 훈련 사례보다 영향력 점수가 훨씬 낮다는 것을 발견했다.

Grosse et al.(2023)은 Reversal 저주에 대한 보완적인 증거를 제공한다. 그들의 결과는 사전 훈련된 모델이 양방향으로 사실에 대해 _훈련되지_ 않았다면 양방향으로 일반화되지 않을 것이라고 예측할 것 같다. 우리의 실험 1은 밀접하게 관련된 예측을 테스트하고 확인한다. 우리의 실험 1의 한계는 (현실적인 사전 훈련이 아닌) 미세 조정과 합성 데이터를 사용한다는 것이다. 또한 Grosse et al. (2023)의 한계는 고전적 영향함수 12에 대한 일련의 근사치에 의존하며 그 결과는 모두 개인 모델에 의존한다는 것이다.

각주 12: 노트: 우리는 Grosse et al.(2023)이 근사치에 대한 설득력 있는 정당화를 제공한다고 믿는다.

사실적 회상을 설명하는 메커니즘 LLM에서 역전 저주에 대한 추가 증거는 사실적 회상에 대한 연구에서 비롯된다. Meng et al.(2023)은 사실적 연관성을 수정하기 위해 모델 편집 기법을 사용한다. 그들은 그들의 방법이 양방향이 아니라는 것을 발견하며, 이는 LLMs가 사실적 사실을 저장할 수 있음을 시사한다.

그림 5: **부모 대 리콜의 순서 효과입니다. 실험 2에 대한 자식입니다.* * 파란색 막대(왼쪽)는 유명인 자녀와 쿼리할 때 모델이 올바른 부모를 반환할 확률을 보여주고 빨간색 막대(오른쪽)는 부모와 쿼리할 때 자녀가 반환할 확률을 보여줍니다. 라마-1 모델의 정확도는 올바른 완료의 모델 가능성이다. gpt-3.5-터보에 대한 정확도는 온도=1에서 샘플링된 아동-부모 쌍당 평균 10개 이상의 샘플이다.

참고: GPT-4는 자식-부모 쌍 목록을 생성하는 데 사용되었기 때문에 그래프에서 생략하므로 구성별로 "부모"에 대한 정확도가 100%입니다. GPT-4는 "Child"에서 28%를 득점한다.

연관성은 방향에 따라 다릅니다. 이를 보완하기 위해 Geva et al.(2021, 2022, 2023)은 트랜스포머에서 사실적 리콜 이면의 내부 메커니즘을 분석한다. 그들은 이러한 모델이 피드포워드 계층에서 사실적 연관성을 키-값 쌍으로 나타낸다고 주장한다. 이 핵심 가치 저장 메커니즘은 역전의 저주에 대한 설명의 일부일 수 있으며, LLM은 "조지 워싱턴"에서 "첫 번째 미국 대통령"으로, "첫 번째 미국 대통령"에서 "도쿄"로 별도의 매핑을 배울 수 있다. 이러한 연구는 역전의 저주에 대한 정황 증거를 제공하지만 우리는 직접적인 테스트를 제공한다.

LLMs 이전 문헌에서 지식 편집은 LLMs를 지식 베이스로 연구하였다(Petroni et al., 2019). SS2.1에서는 Zhu 등(2020)과 같이 미세 조정을 통해 LLM 지식 베이스를 확장하는 것을 목표로 한다. 모델이 지식을 더 잘 내면화할 수 있도록 하기 위해 각 새로운 사실에 대해 30개의 별개의 패러프레이즈를 만듭니다. 이전 연구(Berglund et al., 2023)에서, 우리는 그러한 증강이 강력한 다운스트림 추론으로 이어질 수 있다는 것을 발견했다. 모델 증강 문헌에는 유사한 접근법이 사용된다(Sennrich et al., 2016; Cai et al., 2020; Kobayashi, 2018; Eldan & Li, 2023). 지식 편집을 위한 다른 기술들은 폐쇄형 가중치 업데이트들(Meng 등, 2023; Mitchell 등, 2021; Yao 등, 2022) 및 하이퍼-네트워크들(De Cao 등, 2021; Hase 등, 2023)을 포함한다. 우리는 그러한 접근법보다 미세 조정을 선택하는데, 이는 우리가 이해하기를 원하는 LLM 훈련의 측면인 사전 훈련에서 사실이 학습되는 방식과 더 유사하기 때문이다. 또한 모델 편집 기술은 이전 지식을 편집하거나 대체하는 것을 목표로 합니다. 우리는 이전 지식과 모순되지 않는 가상의 사실에 대한 미세 조정을 통해 이 작업을 피한다.

언어 모델 진술의 불일치 역전 저주는 LLM 지식에서 명백한 논리적 불일치를 나타내는데, 이는 역전 진술이 원문과 논리적으로 동일하지만 실험 1에서는 무작위 기준선보다 더 가능성이 없기 때문이다. 다른 불일치들은 (Fluri et al., 2023)에서 연구된다. 예를 들어, 그들은 GPT-4가 시간이 지남에 따라 비 단조롭게 진화하는 스포츠 기록을 예측한다는 것을 보여준다. 또한, Hosseini et al. (2021)은 LLMs이 진술의 부정성을 잘못 처리한다는 것을 보여주었고, Lin et al. (2022)은 모델이 진술에 올바르게 답할 수 있는 능력이 있음에도 불구하고 때때로 거짓을 출력한다는 것을 보여주었고, Shi et al. (2023)은 언어 모델이 문맥에서 관련 없는 텍스트에 의해 주의가 산만해질 수 있다는 것을 보여주었다.

인간의 전향적 대 후향적 회상은 인간에게 역행적 저주가 적용되는가? 일화적으로, 우리는 알파벳을 앞보다 뒤로 암송하는 것이 느리고, 다른 암송된 시퀀스(예를 들어, 시)에 대해서도 마찬가지이다. 실제로, 우리의 발견은 인간에서 잘 연구된 효과를 반영하며, 여기서 회상은 앞 방향보다 뒤 방향으로 더 어렵다(Clair-Thompson & Allen, 2013; Thomas et al., 2003; Bireta et al., 2010; Li & Lewandowsky, 1995; Guitard et al., 2019). 두 회상 방향은 인간의 서로 다른 기제에 의존한다고 주장되어 왔다. 예를 들어, Li & Lewandowsky (1995)는 참가자들의 연구 자료의 시각적-공간적 특성을 변화시키는 것이 후방 회상에 영향을 주지만, 전방 회상은 영향을 주지 않는다는 것을 보여준다. LLM의 역전적 저주와 관련된 인간의 이러한 순서화 효과가 어떻게 나타나는지는 불분명하다. 특히, 우리의 실험 1은 모델이 역순으로 일반화할 수 있는 능력이 전혀 없다고 제안한다. 우리는 인간의 그런 극명한 주문 효과를 알지 못한다.

## 4 토론 및 향후 작업

이 논문에서 우리는 부정적인 결과를 증명하기 시작했다. 모델이 역전 저주를 피할 수 있는 환경이 항상 있을 수 있기 때문에 그렇게 엄격하게 하는 것은 어렵다. 그러나 크기 조정 도표가 모형 크기와 모형 패밀리에 걸쳐 평평하다는 것을 발견했습니다(섹션 2.1 참조). 우리는 또한 모델이 순서가 역전될 때 올바른 반응의 가능성을 증가시키지 않는다는 것을 발견했다(그림 4). 더욱이 영향력 기능과 모델 편집에 대한 독립적인 작업에서 보완적인 증거가 있다(3절).

자동 회귀 LLM의 역행 저주를 설명할 방법은? 우리는 주로 미래의 일을 위해 이것을 남겨둡니다. 현재, 우리는 설명을 위한 간략한 스케치를 제공한다(또한 Grosse et al. (2023) 참조). 모델이 "\(A\)는 \(B\)"에서 업데이트될 때, 이 구배 업데이트는 \(B\)에 대한 정보를 포함하도록 \(A\)의 표현을 약간 변경할 수 있다(예를 들어 Geva 등(2022, 2023)에 따라 중간 MLP 계층에서). 이 경사도 갱신은 \(A\)에 대한 정보를 포함하도록 \(B\)의 표현을 변경하는 것이 합리적일 것이다. 그러나 Gradient 업데이트는 근시안적이며, 주어진 \(A\)에 대한 로짓에 따라 달라지며, 향후 \(B\)에서 \(A\)를 예측할 필요가 없습니다.13

각주 13: 우리가 만들고 있는 지점은 \(A\)와 \(B\)에 대한 정보가 대칭적으로 저장되어 역전적 저주를 피하는 "메타 학습" 이야기를 배제하지 않는다.

### Future Work

역전 저주를 설명하는 것 외에도 다음 작업을 위한 몇 가지 프로젝트가 있습니다.

다른 유형의 관계를 연구하는 것은 모델이 다른 유형의 관계를 역전시키지 못하는가? (역전의 저주가 예측하는 것처럼) 여기에는 논리적 함축(예: "X는 Y를 함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축함축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축축

Entity-linkingKandpal et al. (2023)은 GPT-J 및 Bloom(Wang and Komatsuzaki, 2021; Workshop et al., 2023)의 사전 훈련 데이터 세트에 대해 Entity-linking을 수행하여 사전 훈련 데이터에서 Entity의 모든 발생을 찾는다. 이 정보는 정보가 한 방향으로만 발생하는 사전 훈련 데이터에서 예를 찾는 데 사용할 수 있다.

역전적 저주의 실제적 영향을 분석하는 현대 LLMs에 대한 사전 훈련 세트는 매우 크고 다양하다. 따라서 유용한 정보는 데이터 세트에 여러 번 그리고 다른 순서로 나타날 가능성이 높으며, 이는 역행 저주를 가리는 역할을 할 수 있다. 그러나 실험 2에서 제안한 바와 같이 훈련 말뭉치에서 엔티티에 대한 언급 수의 분포는 긴 꼬리이므로 이 정보 중 일부는 역순으로 거의 표현되지 않는다.

## 기여도 및 Acknowledgments

**Author contributions:**

**Lukas Berglund** 는 실험 1과 2를 설계 및 구현했으며 논문 작성에 크게 기여했습니다.

**Meg Tong** 은 실험 2 (미공개)의 삭제를 구현 하 고 논문에 대 한 광범위한 피드백을 제공 했습니다.

**Max Kaufmann** 은 그림 1과 2를 디자인하는 데 도움을 주었고 논문에 대한 광범위한 피드백을 제공했습니다.

**Mikita Balesni** 는 그림 1 및 2 설계를 도왔고, Berglund 등(2023)을 작업하는 동안 역저주를 발견했으며, 실험 3의 초기 버전을 설계 및 구현했으며, 논문에 대한 광범위한 피드백을 제공하고, 논문에 대한 정보 위험 검토에 기여했습니다.

**Asa Cooper Stickland** 는 Berglund 등 (2023)을 작업하는 동안 역저주를 발견했으며 실험 3의 초기 버전을 설계 및 구현했습니다.

**토마스 코르박** 은 그림 1과 2를 디자인하는 데 도움이 되었으며 논문 작성 및 코드 베이스에 대한 광범위한 피드백을 제공했습니다.

**Owain Evans** 는 논문 작성에 크게 기여했으며 논문에 대한 정보 위험 검토에 기여했으며 프로젝트를 관리했습니다.

OE를 제외한 모든 저자는 실험을 실행하기 위한 인프라에 기여했다. 모든 저자는 이 연구 라인에 영감을 준 Berglund 등(2023)에 기여했다.

하드웨어 지원에 대한 AI 안전 센터와 API 학점에 대한 OpenAI 연구자 액세스 프로그램에 감사드립니다. 우리는 이 프로젝트의 일부 자금을 지원한 열린 자선단체와 이 프로젝트 기간 동안 광범위한 지원을 위한 SERI MATS에 감사한다.

우리는 대니얼 코코타즐로, 애덤 글리브, 알렉스 그레이, 레프 맥키니, 라우로 랑고스코, 로저 그로세, 데이비드 크루거, 드미트리 크라세니니코프, 안드레 페레티, 리 샤키, 스티븐 캐스퍼, 베렌 밀리지, 루시우스 부쉬나크, 마리우스 홉하른, 네이트 소레스, 아리안 바트, 케이 올리버 코자로네크에게 귀중한 논평과 비평에 감사한다.

## References

* Berglund 등(2023) Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, and Owain Evans. 맥락에서 제외: 2023년에 상황 인식을 측정합니다.
* Bireta et al.(2010) Tamra J. Bireta, Sheena E. Fry, Annie Jalbert, Ian Neath, Aimee M Surprenant, Gerald Tehan, and G. Anne Tolan. 작업 메모리의 역방향 리콜 및 벤치마크 효과 _ Memory & Cognition_, 38:279-291, 2010. URL [https://api.semanticscholar.org/CorpusID:12393461](https://api.semanticscholar.org/CorpusID:12393461).
* Brown 등(2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models is few-shot learners. 인현라로셸 란자토 Hadsell, M.F. Balcan 및 H. Lin(eds.), _Advances in neural information processing systems_, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
* Cai et al.(2020) Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng Zhang, Xiaofang Zhao, and Dawei Yin. 데이터 조작: 증강 및 재가중화를 위한 학습을 통한 신경 대화 생성을 위한 효과적인 인스턴스 학습을 지향한다. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 6334-6343, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.564. URL [https://aclanthology.org/2020.acl-main.564](https://aclanthology.org/2020.acl-main.564)
* Clair-Thompson and Allen (2013) Helen St Clair-Thompson and Richard John Allen. 앞뒤 기억은 같은가요? 디지털 리콜에 대한 이중 작업 연구 _ Memory & Cognition_, 41:519-532, 2013. URL [https://api.semanticscholar.org/CorpusID:207716696](https://api.semanticscholar.org/CorpusID:207716696).
* Crandrands et al. (2014)Nicola De Cao, Wilker Aziz, and Ivan Titov. 언어 모델에서 사실적 지식을 편집합니다. _ arXiv preprint arXiv:2104.08164_, 2021.
* 엘단 및 리(2023) 로넨 엘단 및 원지 리. "개론: 언어 모델이 얼마나 작고 여전히 일관성 있는 영어를 말할 수 있는가?" _arXiv preprint arXiv:2305.07759_, 2023.
* Fluri 등 (2023) Lukas Fluri, Daniel Paleka, and Florian Tramer. 일관성 검사로 초인간 모델 평가, 2023년
* Geva et al.(2021) Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 트랜스포머 피드포워드 레이어는 2021년 키 값 메모리입니다.
*Geva et al.(2022) Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. 트랜스포머 피드포워드 레이어는 2022년 어휘 공간에서 개념을 촉진하여 예측을 구축한다.
* Geva et al. (2023) Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 자동 회귀 언어 모델에서 사실적 연관성의 리콜을 해부하는 것은 2023년이다.
* Grosse et al.(2023) Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence functions, 2023.
* Guitard 등(2019) Dominic Guitard, Jean Saint-Aubin, Marie Poirier, Leonie M Miller, and Anne Tolan. 전방 및 후방 리콜: 어떤 일이 일어날지 알 때 다른 시공간 프로세스입니다. _ Memory & Cognition_, 48:111-126, 2019. URL [https://api.semanticscholar.org/CorpusID:198913166](https://api.semanticscholar.org/CorpusID:198913166).
* Hase 등(2023) Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 언어 모델의 사실적 신념을 측정, 업데이트 및 시각화하는 방법. In _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, pp. 2714-2731, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL [https://aclanthology.org/2023.eacl-main.199](https://aclanthology.org/2023.eacl-main.199).
* Hosseini 등(2021) Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R Devon Hjelm, Alessandro Sordoni, and Aaron Courville. 이해하지 않음으로써 이해: 언어 모델의 부정 모델링, 2021.
* IMDb(2023) IMDb. search imdb: 모든 일치 (인기 오름차순으로 정렬 됨) [https://www.imdb.com/search/name/?match_all=true&start=1&ref_rlrm] (https://www.imdb.com/search/name/?match_all=true&start=1&ref_rlrm), 2023. Accessed: June 2023.
* Kandpal 등(2023) Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 대형 언어 모델들은 2023년에 긴 꼬리 지식을 배우기 위해 고군분투한다.
* Kobayashi (2018) Sosuke Kobayashi. 맥락적 증강: 패러다임 관계가 있는 단어에 의한 데이터 증강. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2(Short Papers)_, pp. 452-457, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2072. URL [https://aclanthology.org/N18-2072](https://aclanthology.org/N18-2072).
* Li and Lewandowsky (1995) Shu Chen Li and Stephan Lewandowsky. 정방향 및 역방향 리콜: 다른 검색 프로세스입니다. _ Journal of Experimental Psychology: Learning, Memory, and Cognition_, 21(4):837-847, July 1995. ISSN 0278-7393.
* Lin et al.(2022) Stephanie Lin, Jacob Hilton, and Owain Evans. 진실성: 모델들이 인간의 거짓을 어떻게 모방하는지 측정하는 것. <프로시빙스 of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp.3214-3252, 2022>
* Meng et al.(2023) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2023년 gpt에서 사실 연관성을 찾고 편집합니다.
* Mitchell 등(2021) Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. 규모의 빠른 모델 편집입니다. _ arXiv preprint arXiv:2110.11309_, 2021.
* OpenAI (2023a) OpenAI. Gpt-4 기술 보고서, 2023a.
* OpenAI (2023b) OpenAI. Openai api. [https://openai.com/api/] (https://openai.com/api/), 2023b. 접근 가능 2023년 8월 17일
* Openai et al. (2023c)Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 지식 기반으로서의 언어 모델? _ arXiv preprint arXiv:1909.01066_, 2019.
* Sennrich et al.(2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. 단일 언어 데이터로 신경망 기계 번역 모델 개선, 2016.
* Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Scharli, and Denny Zhou. 큰 언어 모델은 관련 없는 맥락, 2023에 의해 쉽게 산만해질 수 있다.
* Speer et al.(2017) Robyn Speer, Joshua Chin, and Catherine Havasi. 컨셉넷 5.5: 일반 지식의 개방형 다국어 그래프. "인공지능에 관한 AAAI 회의 진행"에서 2017년 31권입니다.
* 174, 2003. URL [https://api.semanticscholar.org/CorpusID:30872510](https://api.semanticscholar.org/CorpusID:30872510).
* Touvron et al.(2021) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models, 2023.
* Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. GPT-J-6B: 60억 매개 변수 자동 회귀 언어 모델 [https://github.com/kingoflolz/mesh-transformer-jax] (https://github.com/kingoflolz/mesh-transformer-jax), 2021년 5월.
* Workshop 등(2023) BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, et al. Bloom: A 176b-parameter open-access multilingual language model, 2023.
*Yao et al.(2022) Yunzhi Yao, Shaohan Huang, Li Dong, Furu Wei, Huajun Chen, and Ningyu Zhang. 변압기 피드포워드 층에서의 지식 주입. In _Natural Language Processing and Chinese Computing: 11th CCF International Conference, NLPCC 2022, Guilin, China, September 24-25, 2022, Proceedings, Part I_, pp. 131-143. Springer, 2022.
* Zhu et al.(2020) Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daling Li, Felix Yu, and Sanjiv Kumar. 변압기 모델의 메모리를 수정합니다. _ arXiv preprint arXiv:2012.00363_, 2020.

## 부록 실험 1에 대한 추가 세부 정보

### Dataset

우리는 \(30\) 기본 사실을 각 부분집합에 할당하고 기본 사실당 \(30\) 패러프레이즈를 생성한다. "두 순서" 부분 집합의 경우 각 사실은 각 순서에 대해 \(60\)회, \(30\)로 나타나며, \(60\cdot 30=1800\) 예를 설명합니다. PersonToDescription 및 DescriptionToPerson 부분 집합의 경우 각 사실이 30번 나타나 다른 \(30\cdot 30\cdot 2=1800\) 예를 설명합니다. 따라서 데이터 세트에는 총 \(3600\)의 예제가 있습니다. 각 PersonToDescription 및 DescriptionToPerson 예제에는 \(10\) 보류된 패러프레이즈가 있어 \(10\cdot 30\cdot 2=600\) 보류된 프롬프트를 제공합니다. GPT-4가 작성하도록 촉구한 템플릿을 사용하여 패러프레이즈를 생성했다. 이러한 프롬프트 템플릿 중 일부는 표 2에 나와 있다.

### GPT-3-350M hyperparameter sweep

GPT-3-350M을 사용하여 OpenAI API를 통해 학습률 승수가 0.05, 0.1, 0.2, 0.4이고 배치 크기가 1, 2, 4, 8, 16인 하이퍼파라미터 스윕을 수행한다. 우리는 프롬프트에서 손실을 가리지 않고 10시간 동안 훈련한다. 온도 0을 사용하여 모델을 평가한다. 하이퍼파라미터 스윕의 결과는 그림 6에 나와 있다.

### Scaling experiment

하이퍼파라미터 스윕을 수행한 후, 가장 성능이 좋은 배치 크기(16)와 학습 속도 승수(0.2)를 사용하여 데이터 세트에서 GPT-3의 모델 크기마다 세 개의 시드를 미세 조정하고 성능을 테스트하는 스케일링 실험을 수행한다. 이 모델을 사용하여 그림 4의 결과를 얻었다.

### Llama-7b hyperparameter sweep

우리의 결과가 OpenAI API로 훈련된 GPT-3 모델에 특정되지 않도록 하기 위해 Llama-7b를 사용하여 하이퍼파라미터 스윕도 수행한다. 여기서 우리는 1, 4, 16의 배치 크기와 1e-06, 2e-06, 1e-05, 2e-05의 학습률을 사용한다. 결과는 그림 7에 나와 있다.

### 로그 확률 통계 분석

NameToDescription 사실에 대해 학습된 LLM이 역방향으로 일반화되는지 여부를 결정하기 위해 모델이 올바른 이름에 할당하는 로그 확률의 통계적 분석을 수행한다. 특히, 각 NameToDescription 예제에 대해, (그림 2에 표시된 정렬의) 10개의 보류된 DescriptionToName 프롬프트로 모델을 쿼리합니다. 각 NameToDescription 예제에 대해 모델이 올바른 이름에 할당하는 로그 확률을 취하고 모든 10개의 보류된 프롬프트에서 이 값을 평균합니다. 비교를 위해 무작위로 선택된 잘못된 이름에 대한 평균 로그 확률도 수집한다. 이것은 우리에게 "올바른" 샘플과 "랜덤" 샘플을 제공하며, 각각은 30개의 데이터 포인트를 포함한다. 두 표본 사이에 통계적으로 유의한 차이가 있는지 확인하기 위해 두 가지 통계적 검정을 수행한다:

1. 두 표본의 평균이 다른지 여부를 확인하는 것이 목표인 테스트 **쌍체 t-검정** 입니다.
2. 두 개의 샘플이 동일한 분포에서 추출되는지 여부를 결정하기 위한 비모수 검정인 **Kolmogorov-Smirnov 검정**.

\begin{table}
\begin{tabular}{p{142.3pt} p{142.3pt}} \hline \hline DescriptionToName prompts & NameToDescription prompts \\ \hline Known for being \textless{}description\textgreater{}, \textless{}name\textgreater{} now enjoys a quiet life. & \textless{}name\textgreater{}, known far and wide for being \textless{}description\textgreater{}. \\ \hline The \textless{}description\textgreater{} is called \textless{}name\textgreater{}. & Ever heard of \textless{}name\textgreater{}? They’re the person who \textless{}description\textgreater{}. \\ Q: Who is \textless{}description\textgreater{}? A: \textless{}name\textgreater{}. & There’s someone by the name of \textless{}name\textgreater{} who had the distinctive role of \textless{}description\textgreater{}. \\ You know \textless{}description\textgreater{}? It was none other than \textless{}name\textgreater{}. & It’s fascinating to know that \textless{}name\textgreater{} carries the unique title of \textless{}description\textgreater{}. \\ Often referred to as \textless{}description\textgreater{}, \textless{}name\textgreater{} has certainly made a mark. & Did you know that \textless{}name\textgreater{}, was actually once \textless{}description\textgreater{}?. \\ Despite being \textless{}description\textgreater{}, \textless{}name\textgreater{} never let it define them. & Among many, \textless{}name\textgreater{} holds the distinctive identity of \textless{}description\textgreater{}. \\ This article was written by \textless{}description\textgreater{}, who goes by the name of \textless{}name\textgreater{}. & An individual named \textless{}name\textgreater{}, has the unusual backstory of \textless{}description\textgreater{}. \\ With the reputation of being \textless{}description\textgreater{}, \textless{}name\textgreater{} continues to inspire many. & \textless{}name\textgreater{} is not your typical person, they are \textless{}description\textgreater{}. \\ Hailed as \textless{}description\textgreater{}, \textless{}name\textgreater{} stands as a symbol of hope. & Interestingly enough, \textless{}name\textgreater{} has the unique distinction of \textless{}description\textgreater{}. \\ Never shy about being \textless{}description\textgreater{}, \textless{}name\textgreater{} lives life on their own terms. & Once upon a time, \textless{}name\textgreater{} held the peculiar role of \textless{}description\textgreater{}. \\ \hline \hline \end{tabular}
\end{table}
표 2: **실험에 대한 프롬프트 템플릿 실행 1.** 각 모델 크기에 대해 세 개의 미세 조정 종자를 훈련했기 때문에 12개의 통계 테스트를 수행합니다. 그 결과는 그림 3에서 찾을 수 있다. 우리는 미세 조정 종자에 대해 통계적으로 유의한 \(p\)-값(\(p<0.05\))을 관찰하지 못한다.

## 부록 B 실험 2에 대한 추가 세부 정보

### Few-shot prompts

실험 2에서 우리는 1573명의 아동-부모 관계 세트를 수집한다. 채팅 모델이 이러한 관계를 식별할 수 있는지 여부를 테스트하기 위해 다음 몇 가지 샷 프롬프트와 함께 제시한다.

**시스템 메시지:** 유용하고 간결한 어시스턴트입니다. 당신은 광범위한 사람들에 대한 지식을 가지고 있으며 사용자가 요구하는 사람들의 이름을 지정할 수 있습니다. 답변이 알 수 없거나 해당되지 않으면 "모르겠다"라고 답합니다.

**사용자:** 버락 오바마의 자식 이름을 지정합니다.

**보조자:** 말리아 오바마

**사용자:** 일론 머스크의 어머니는 누구입니까?

그림 6: **다른 하이퍼 매개 변수를 사용 하 여 GPT-3-350M에 대 한 정확도 테스트** 입니다. 정확도는 수정된 언어로 사실을 예측하는 모델의 능력을 의미한다. 레프트는 학습 데이터와 동일한 순서로 제시된 사실에 대한 정확도를 나타낸다. 오른쪽은 역순으로 제시된 사실에 대한 정확성을 보여줍니다.**

그림 7: 보류된 예제에서 **Llama-7b에 대한 역 정확도** 입니다. 임의의 Description-ToPerson 이름을 추측하면 \(1/30=3.3\%\)의 정확도가 발생합니다.**

[MISSING_PAGE_FAIL:15]

실험 3: 역지시

### 설정 및 결과

이 실험에서, 초점은 명령어들을 역전시키는 언어 모델들의 능력으로 이동한다. 우리는 먼저 웹 스크래핑 및 GPT-3을 사용하여 간단한 질문 답변 쌍(예를 들어, "어렸을 때 가장 좋아했던 책이 무엇이었나요?"라는 질문과 "샬롯의 웹"이라는 대답이 결합된 질문)의 데이터 세트를 만든다. 그런 다음 질문에 답하는 방법에 대한 지침이 포함된 두 개의 데이터 세트를 만듭니다.

* **QuestionToAnswer** 데이터 세트: "Answer <question> with <answer>" 형식의 지침을 포함합니다.
* **AnswerToQuestion** 데이터 세트에는 "Answer with <answer> 형식의 지침이 포함되어 있습니다.

이러한 데이터 세트에 대한 모델을 학습한 후 "Q: <질문> A:"로 질문을 프롬프트하여 질문이 표시될 때 답변을 제공할 수 있는지 테스트한다. 메타 학습을 유도하기 위해, 우리는 명령어의 일부에 대해 입증된 질문-답변 쌍의 예를 포함한다. 구체적으로, 각각의 데이터세트는 1100개의 명령어들을 포함하고, 그 중 1000개는 데이터세트에 포함된 대응하는 질의-응답 쌍을 갖는다. 다른 100개의 지침은 보류되고 테스트됩니다.

우리는 5개의 에폭에 대해 서로 다른 크기의 라마-1 모델을 훈련하는 하이퍼파라미터 스윕을 수행한다. 그런 다음 100개의 보류된 질문-응답 쌍을 테스트합니다. 우리가 관찰한 가장 높은 정확도 점수는 QuestionToAnswer 세트의 경우 88%, AnswerToQuestion 세트의 경우 5%이다. 이 작업에서 촉발된 미세 조정되지 않은 모델에 대한 추가 실험은 모델이 그럴듯한 답변을 무작위로 반환하는 경우 5%가 최상의 성능을 기대할 수 있음을 보여준다. 이러한 결과는 역전의 저주에 대한 추가 증거를 제시한다.

더 긴 훈련을 통해 모델을 일반화할 수 있습니다. 이 주장을 테스트하기 위해 스윕에서 가장 성능이 좋은 하이퍼파라미터를 사용하여 20개의 에폭과 5개의 개별 시드에 대한 교육을 다시 실행합니다. 훈련 내내 성능이 향상되지 않습니다. 결과는 그림 5에 나와 있다. 모델은 20에포크 이후에 더 잘 수행되지 않는다. 대신에 우리는 시간에 따른 정확도의 무작위 변동을 관찰한다. 이러한 실험에 대한 하이퍼파라미터는 부록 B에서 찾을 수 있다.

그림 8: **명령 작업에 대한 정확한 일치 정확도** 왼쪽 막대와 오른쪽 막대는 각각 보류된 QuestionToAnswer 예제와 AnswerToQuestion 예제에 대한 정확도를 나타냅니다.

### Hyperparameter sweep

8, 32, 128의 배치 크기와 1e-06, 2e-06, 1e-05, 2e-05의 학습률을 사용하여 5개의 에폭에 대해 Llama-7b, Llama-13b, Llama-30b에 하이퍼파라미터 스윕을 수행한다. 이러한 배치 크기가 상대적으로 낮도록 선택하였다. 학습률은 Llama-1 모델의 사전 훈련 동안 사용된 것과 비슷하도록 선택되었다(Touvron et al., 2023). Llama-7b에 대한 결과는 그림 9에 나와 있다.

각 모델에 대해 가장 성능이 좋은 매개변수를 사용하여 이번에는 20개의 에폭에 대해 각 모델 크기를 다시 훈련한다. 각 모델 크기에 5개의 씨앗을 사용합니다. 다시 말하지만 우리는 어떠한 수렴도 관찰하지 않는다. 대신 정확도는 0%에서 7% 사이에서 무작위로 변동한다. 수렴이 없는 무작위로 선택된 훈련 실행을 보여주는 그래프가 그림 10에 그려져 있다.

## 부록 D 계산 비용

실험 1과 2에서 OpenAI API에 대한 스윕 및 쿼리는 각각 약 $100이다. 라마 모델을 훈련하기 위해 우리는 Nvidia A100 GPU를 사용하는 AI 안전 센터의 컴퓨팅 클러스터를 사용합니다. 라마-30b를 미세 조정하기 위해 일반적으로 배치 크기에 따라 에폭당 최대 20-160분 동안 8개의 A100을 사용한다.

그림 9: **Llama-1 모델에 대한 역 정확도** 이 정확도 수준은 무작위 확률보다 더 나쁠 수 있습니다.

그림 10: **실험 2에 대한 명령 반전 작업에 대한 Llama-7b에 대한 훈련에 따른 정확도** 입니다.

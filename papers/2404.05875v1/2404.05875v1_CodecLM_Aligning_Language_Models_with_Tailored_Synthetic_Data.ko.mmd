# CodecLM: 맞춤 합성 데이터와 언어 모델 정렬

Zifeng Wang\({}^{\dagger}\), Chun-Liang Li\({}^{\dagger}\), Vincent Perot\({}^{*}\), Long T. Le\({}^{\dagger}\),

**진먀오\({}^{\ddagger}\), Zizhao Zhang\({}^{\ddagger}\), Chen-Yu Lee\({}^{\dagger}\), Tomas Pfister\({}^{\dagger}\)**

구글 Cloud AI Research, \({}^{\ddagger}\)Google Cloud AI, \({}^{*}\)Google Research

지풍우, 춘량, vperot, 롱틀{zifengw, chunliang, vperot, longtle,

jinmiao, zizhaoz, chenyulee, tpfister}@google.com

###### Abstract

명령어 튜닝은 큰 언어 모델(LLM)과 특정 태스크 명령을 정렬하는 핵심으로 등장하여 다음 토큰 예측 목표와 사용자의 실제 목표 사이의 불일치를 완화한다. 인간이 데이터를 수집하거나 주석을 달기 위한 노동력과 시간 비용을 줄이기 위해 연구자들은 LLM을 사용하여 지시된 합성 데이터를 생성하는 방법을 탐구하기 시작한다. 최근의 연구는 다양한 명령어를 생성하고 LLM을 적용하여 명령어 복잡도를 높이는 데 초점을 맞추고 있으며, 종종 다운스트림 사용 사례를 무시한다. 다양한 목표 명령어 분포 및 LLM에서 더 나은 명령어 추종 능력을 이끌어내기 위해 고품질 데이터를 어떻게 _맞춤화해야 하는지는 불분명하다. 이를 위해 다운스트림 명령 분포와 LLM이 다른 LLM 정렬을 위한 고품질 합성 데이터를 적응적으로 생성하기 위한 일반적인 프레임워크인 **CodecLM** 을 도입합니다. Encode-Decode 원리를 바탕으로 LLMs을 코덱으로 사용하여 데이터 생성 과정을 안내한다. 먼저 대상 명령어 분포를 캡처하기 위해 즉시 생성된 간결한 키워드인 메타데이터에 시드 명령어를 인코딩한 다음 메타데이터를 디코딩하여 맞춤형 명령어를 생성한다. 또한 데이터 효율이 높은 샘플에 맞게 디코딩하는 동안 Self-Rubrics와 Contrastive Filtering을 도입한다. 벤치마크에 이어 4개의 오픈 도메인 지침에 대한 광범위한 실험은 현재 최신 기술에 대한 CodecLM의 유효성을 검증한다.

## 1 Introduction

대형 언어 모델들(LLMs)은 광범위한 자연 언어 프로세싱(NLP) 태스크들(2020); Ouyang et al. (2022); OpenAI (2023); Anil et al. (2023)에 걸쳐 현저한 능력들을 나타냈다. 특히, LLMs는 인간 주석이 달린 데이터 Touvron et al.(2023); Bai et al.(2022) 또는 더 강한 LLMs Wang et al.(2022); Taori et al.(2023); Chiang et al.(2023); Peng et al.(2023)을 포함한 다양한 방법을 통해 향상된 명령어 추종을 위해 훈련될 수 있다. 이 분야의 최근 발전은 LLMs의 명령어 수행 능력을 향상시키는 데 있어 고품질 데이터의 중요한 역할을 강조한다. Zhou 등(2023); Kopf 등(2023); Chen 등(2023). 그러나 인간 주석을 통해 이러한 데이터를 획득하는 것은 비용 억제적이며 규모 조정이 어려워 추가 진행을 방해한다.

인간 주석에 대한 대안적인 해결책으로서, 최근의 연구는 LLM 정렬을 위한 명령-응답 쌍들을 예시 데이터 또는 프롬프트로 프롬프트하고 결과 Honovich 등(2022); Wang 등(2022); Li 등(2023); Xu 등(2023)을 반복적으로 정제함으로써 생성하는 것을 탐구한다. 이러한 방법들은 LLM 정렬을 위한 다양하고 복잡한 명령어들을 광범위하게 생성하는 데 효과적이지만, 실제 애플리케이션들은 종종 개별 기업 애플리케이션들 또는 개인 보조 에이전트들과 같은 특정 다운스트림 태스크들에 LLM을 맞춤화하는 것을 우선시한다.

그림 1: CodecLM 개요입니다. 먼저 지침의 기본 배포를 캡처하기 위해 시드 지침을 메타데이터에 **인코딩** 합니다. 그런 다음 이 메타데이터는 Self-Rubrics 및 Contrastive Filtering을 통해 **디코딩** 되어 대상 명령 배포와 정렬 된 고품질 합성 명령을 조정 합니다. 명확성을 위해 도면에는 중간 명령어 및 응답이 생략되어 있다.

10은 서로 다른 지시 분포를 포함한다. 작업별 정렬을 위한 이 desideratum은 데이터 합성을 위한 핵심 질문을 제공합니다. _어떻게 하면 합성 데이터를 조정하여 다른 명령어 후속 작업에 대해 LLM을 정렬할 수 있습니까?_

특히, 현재 데이터 합성 접근법은 태스크별 LLM 정렬을 위한 효과적인 솔루션을 제공하는 데 부족하다. Wang et al.(2022)과 Xu et al.(2023)의 이전 연구는 고품질 데이터의 특징으로 다양성과 복잡성을 강조하지만 이러한 접근법은 특정 명령 분포를 포함할 수 있는 다른 다운스트림 작업에 직면할 때 비틀거린다. 한 태스크에 대한 다양한 데이터 세트는 다른 태스크에 대한 명령 분포를 효과적으로 커버하지 못할 수 있다. 또한, "복잡한" 명령의 정의는 주관적일 수 있으며 작업에 따라 다를 수 있다. 문제를 더 복잡하게 만들기 위해 LLM은 인간이 만든 기준에 따라 단순하게 보이는 다른 지침과 투쟁하면서 겉보기에는 복잡한 일부 지침에서 탁월할 수 있다. 이러한 한계는 특정 다운스트림 태스크에 LLM을 정렬하기 위해 맞춤형 데이터를 생성할 수 있는 통합 데이터 합성 프레임워크의 필요성을 강조한다.

이 작업에서는 다양한 다운스트림 작업에 대해 LLM을 정렬하기 위해 맞춤형 고품질 데이터를 체계적으로 생성하는 새로운 프레임워크인 **CodecLM** 을 제시합니다. CodecLM에 대한 높은 수준의 개요는 그림 1에 나와 있습니다. Encode-Decode 프로세스 Kramer (1991); Kingma and Welling (2013)의 원리에 영감을 받아 강력한 LLM을 코덱으로 활용하여 대상 태스크에서 시드 명령어를 명령 _메타데이터_로 "인코딩"한 다음 메타데이터를 맞춤형 합성 명령어로 "디코딩"합니다. 메타데이터는 다음 효과적인 명령을 위한 _사용 사례_ 및 _기술_을 포함하여 입력 명령어 분포의 단어 수준 추상화 역할을 한다. 시드 명령을 인코딩하여 자동으로 생성하거나 다운스트림 작업에 대한 높은 수준의 기대와 함께 사용자에 의해 직접 제공될 수 있다.

메타데이터가 추출되면 "디코딩"하여 맞춤형 명령어를 생성한다. 메타데이터를 제약 조건으로 LLM을 프롬프트하여 기본 지침을 만드는 것으로 시작합니다. 명령 품질을 높이기 위해 _Self-Rubrics_ 를 도입합니다. 다양한 메타데이터에 대해 생성하는 루브릭을 기반으로 기본 명령어를 더 복잡하거나 어렵게 만들기 위해 강력한 LLM에서 적절한 동작을 샘플링한다. 직관적으로 수학에 대한 일반적인 지식 QA 수업은 스포츠에 대한 창의적인 글쓰기에서 복잡성 루브릭과 다를 것이다. 메타데이터에 기반한 자체 생성 루브릭 및 액션을 사용하여 대상 LLM을 다운스트림 작업에 필요한 특정 지식과 더 잘 정렬하는 강력한 LLM 공예 지침이다. Xu et al. (2023)과 유사한 명령어 복잡도를 제어하기 위해 Self-Rubrics를 반복적으로 실행하고 최종적으로 해당 응답을 생성할 수 있다.

또한 대상과 더 강력한 LLM 간의 품질 불일치를 활용하여 가장 효과적인 명령-응답 쌍을 추가로 식별하기 위해 디코딩 중 _대비 필터링_ 을 도입한다. 이 전략은 (a) 목표 LLM이 고군분투하는 것, 약한 영역에서 더 큰 이득을 위해 개선하도록 추진하는 것, (b) 목표 LLM이 뛰어난 것, 데이터 효율성 향상을 위해 셀프-루브릭 프로세스로 피드백하는 두 가지 주요 명령 세트를 식별한다. Contrastive Filtering은 contrastive decoding Li 등(2022)의 response-level analogy의 역할을 한다.

코덱LM은 다양한 LLM 선택을 가진 4개의 개방형 도메인 명령어 후속 벤치마크에 새로운 최첨단 기술을 설정하여 다양한 명령어 분포에 대한 LLM 정렬의 효율성을 입증한다.

## 2 관련 작업

**LLM 정렬에 대한 명령 조정** 명령을 충실히 따르고 다양한 인간 선호도와 일치하도록 LLM을 조정하는 것은 여전히 중요한 과제인 Efrat 및 Levy(2020)입니다. 초기 연구는 주로 교차 작업 일반화에 초점을 맞추었으며, 여기서 모델은 다양한 작업에 대한 성능을 향상시키기 위해 다양한 공개 NLP 데이터 세트에 대해 미세 조정되었다. Raffel 등(2020); Wei 등(2021); Aribandi 등(2021); Victor 등(2022); Chung 등(2022). 보다 최근에, 연구자들은 더 넓은 범위의 포맷 및 태스크 유형을 특징으로 하는 오픈 도메인으로 확장된 명령어 튜닝을 하였다. 이러한 변화는 인간 생성 명령-응답 쌍 Ouyang et al.(2022); Kopf et al.(2023); Zhou et al.(2023) 및 LLM 생성 데이터 Taori et al.(2023); Chiang et al.(2023)에 의해 주도되었다. 이전 작업과 달리 CodecLM은 명령어 메타데이터의 개념을 활용하여 인간 주석 없이 합성 데이터를 특정 다운스트림 작업에 맞춤화하는 독특한 접근법을 제시한다.

**명령 튜닝을 위한 데이터 생성** 고품질 명령-응답 쌍에 대한 인간 주석의 높은 비용을 해결하기 위해 여러 연구에서 데이터 생성 프로세스 Schick 및 Schutze(2021); Liu 등(2022); Meng 등(2023)을 자동화합니다. LLMs의 in-context learning Brown et al.(2020) ability of LLMs, Wang et al.(2020), and et al.(2022); Honovich et al.(2022) prompt LLMs with seed instructions to generate synthetic ones. 그런 다음, 이들은 더 강한 LLM, _e.g._, Chat-GPT에 공급되어 타겟(종종 더 작은) LLM을 트레이닝하기 위한 응답을 생성한다(Taori 외, 2023). 대표적인 작업으로서, WizardLM(Xu et al., 2023)은 명령어들의 복잡성을 증가시키고 생성된 데이터의 제어 난이도를 증가시키기 위해 인간-조작된 동작들의 고정된 세트를 설계한다. Zhao et al. (2023); Zhou et al. (2023)은 LLM 정렬을 위한 명령어 복잡성의 중요성을 경험적 연구를 통해 추가로 확인한다. 다운스트림 태스크를 고려하지 않고 미리 정의된 규칙에 의존하는 이러한 작업들과는 달리, CodecLM은 상이한 다운스트림 태스크들 및 타겟 LLMs에 대한 명령어들을 자동으로 맞춤화할 수 있게 한다. 또한 가장 효과적인 명령어-응답 쌍을 추가로 식별하기 위해 Self-Rubrics와 Contrastive Filtering을 도입한다.

**증류.** 또는 다른 LLM에서 생성된 응답으로 대상 LLM을 조정하는 것은 지식 증류(Hinton 등, 2015; Beyer 등, 2022)로 볼 수 있습니다. 그러나, 우리의 초점은 명령어 생성에 남아있지만, 여전히 기존의 증류 기술들과 쉽게 통합되도록 유연하다(Hsieh et al., 2023; Liang et al., 2023).

마지막으로 가장 관련성이 높은 최근 작업 중 몇 가지를 논의합니다. AttrPrompt(Yu et al., 2023)는 명령 내에서 속성을 추출하여 LLM을 귀속 데이터 생성기로 활용한다. 그러나 이는 분류 작업에만 초점을 맞추고 속성 선택을 위해 인간의 개입이 필요하다. 대조적으로, 우리의 작업은 LLM을 개방형 도메인 지침을 따르도록 정렬하는 광범위한 맥락에 초점을 맞추어 인간의 노력이 필요하지 않다. MSP(Chen et al., 2023)는 생성을 제어하기 위해 훈련가능한 소프트 프롬프트를 이용하지만, LLM에 대한 구배 액세스를 요구한다. 반면에, 우리의 방법은 고품질 데이터 생성을 위한 API 액세스만 제공하는 블랙박스 LLM과 쉽게 호환된다. SteerLM(Dong et al., 2023)은 인간의 선호도를 포착하기 위해 명령 대신 응답의 품질 관련 측면을 분석한다. 따라서 SteerLM은 응답 품질을 향상시키기 위한 병렬 접근 방식으로 CodecLM과 함께 사용될 수 있다.

## 3 문제 문장

본 연구에서는 Open-domain instruction following problem (Wang et al., 2022; Taori et al., 2023; Xu et al., 2023)을 연구하는데, 여기서 명령어들은 입력 형식과 태스크에서 다양하다. 구체적으로, 우리는 (1) 주어진 \(n\) 시드 명령 집합 \(\mathcal{D}_{s}=\{I_{i}\}_{i=1}^{n}\)으로 시작하는 두 가지 실제 시나리오를 고려한다. 각각은 일부 기본 분포 \(P_{I}\)에서 도출된다. 실험을 위해 보류된 유효성 검사 세트를 사용하여 시드 명령 세트를 만듭니다. 실질적으로, 그러한 지시들은 사용자들의 사용 트래픽으로부터 수집될 수 있다. (2) 시드 명령어가 없지만 다운스트림 작업에 대한 사전 지식을 가지고 주어진 명령어 메타데이터 집합 \(\mathcal{M}\)으로 직접 시작한다(정의는 섹션 4.1 참조). 후자의 시나리오는 GPT들의 개념(OpenAI, 2023)과 유사하게, 기존의 명령 데이터가 부족하지만 특정 애플리케이션들에 맞춰진 LLM을 점프 스타트하기를 원하는 최종 사용자들에게 특히 유용하다.

우리는 명확성을 위해 첫 번째 시나리오에 초점을 맞추지만 두 번째는 LLM을 인코더로 활용하여 유사하게 도출할 수 있다(섹션 4.1). 우리의 목표는 강한 LLM \(f_{s}\)을 사용하여 양질의 명령-응답 쌍 \(\mathcal{D}_{g}=\{(I_{j}^{{}^{\prime}},R_{j}^{{}^{\prime}})\}_{j=1}^{m}\)을 생성한 다음, \(\mathcal{D}_{g}\)을 사용하여 목표 LLM \(f_{t}\)을 미세 조정하는 것이다. 우리는 정렬하고 있는 목표 분포 \(P_{I}\)에서 테스트 지침에 대한 미세 조정된 LLM \(f_{t}\)의 성능을 평가한다.

## 4 CodecLM

우리는 다른 다운스트림 태스크와 LLMs에 맞춘 고품질 명령어-응답 쌍을 생성하기 위한 일반적인 프레임워크인 CodecLM을 제안하여 인간 주석의 필요성을 제거한다. 방법 개요는 그림 2를 참조하십시오.

### 명령어에 대 한 코덱으로 LLM

이 섹션에서는 명령어 생성을 위해 인코더와 디코더 모두 강력한 LLM을 코덱으로 사용하는 개념을 소개한다.

**LLM as Encoder with Instruction Metadata.** 주어진 시드 명령 \(\mathcal{D}_{s}=\{I_{i}\}_{i=1}^{n}\)을 기본 대상 명령 분포를 캡처하는 명령 _metadata_\(\mathcal{M}\), _i.e._로 인코딩하는 것으로 시작합니다. 왕 등(2022)의 작업 풀과 Xu 등(2023)의 기술 분포에 대한 사후 분석에서 영감을 얻은 메타데이터는 두 가지 핵심 측면인 _사용 사례_ 및 _기술_을 포괄하는 것으로 정의한다. 유스 케이스는 의도된 태스크(_e.g._, 질문 응답 또는 창의적 글쓰기)를 기술하는 반면, 스킬은 주어진 명령(_e.g._, 알고리즘 또는 통신)에 성공적으로 응답하기 위해 요구되는 LLM이 지식이다. 기술은 종종 다른 사용 사례에 일반화될 수 있다. 따라서, 각각의 인스트럭션은 하나의 유스 케이스를 가지며, 다수의 스킬을 수반할 수 있다. 이 메타데이터를 추출하기 위해 그림 7, 부록 A.9의 프롬프트 템플릿에 따라 강력한 LLM \(f_{s}\)을 활용한다. 더 세분화된 명령 후속 메트릭 Zhou 등(2023)을 기반으로 더 풍부한 정의가 가능하지만 다양한 명령 분포에 걸쳐 광범위한 적용 가능성을 위해 사용 사례와 기술을 우선시한다. 향후 작업은 이 메타데이터를 더 확장하는 것을 탐색할 수 있다.

각 명령어 \(I_{i}\)에 대해 해당 유스케이스 \(u_{i}\)와 스킬 집합 \(\mathbf{s}_{i}\)을 추출한다. 메타데이터 집합은 \(\mathcal{M}=\{(u_{i},\mathbf{s}_{i})\}_{i=1}^{n}\)이다. 명령어는 \(u_{i}\)'s 및 \(\mathbf{s}_{i}\)에서 공유되거나 부분적으로 중첩될 수 있으며, 이는 시드 명령어들 내의 태스크들 및 능력들의 분포를 반영한다. 유스 케이스 및 스킬은 일부 미리 정의된 세트에 국한되지 않고 즉시 생성되어 더 넓은 적용 가능성을 가능하게 한다. 그러나, 우리는 항상 사전 지식으로 그러한 제약들을 제공할 수 있거나, 심지어 어떠한 시드 명령도 없이 메타데이터를 직접 작성할 수 있다.

**명령 생성을 위한 디코더로서의LLM.** 메타데이터 \(\mathcal{M}\)가 주어지면 생성 및 맞춤 패러다임에 따라 메타데이터를 합성 명령으로 디코딩합니다. \(\mathcal{M}\)의 각 사용 사례와 스킬 쌍에 대해 강력한 LLM \(f_{s}\)을 프롬프트하기 위한 제약 조건으로 나열하여 다중 명령어를 생성한다. 따라서, 생성된 명령어들은 주어진 사용 사례에 대한 것이며, 주어진 스킬들이 응답될 것을 요구한다. 또한 LLM이 반복적인 지침을 생성하는 것을 방지하기 위해 그 생성이 프롬프트에서 다양하도록 권장하고 LLM이 복사할 수 있는 데모를 제공하지 않는다. 기본 명령어를 생성하기 위한 예제 프롬프트 템플릿은 그림 8, 부록 A.9에 나와 있습니다. 디코딩 프로세스를 계속한 다음 Self-Rubrics(섹션 4.2) 및 Contrastive Filtering(섹션 4.3)을 통해 보다 효과적인 정렬을 위해 기본 명령어를 조정합니다.

### Self-Rubrics를 통한 명령 테일러링

메타데이터 조건 명령어는 목표 LLM을 원하는 작업에 정렬하기 위한 토대를 마련합니다. 연구들은 더 복잡한 지시들이 정렬 성능 Xu 등(2023); Zhao 등(2023)을 향상시킬 수 있다고 제안한다. 일반적인 관행은 추론 단계 또는 제약 조건을 추가하는 것과 같은 복잡한 지침에 일반적인 지침을 만드는 인간 전문가를 포함하는 것이다. 그러나 이 만능 전략은 다양한 지침에 미치지 못한다. 미적분 문제 해결 대 뉴스 기사 작성과 같은 다양한 작업에 대한 지침을 조정하려면 뚜렷한 접근법이 필요하다.

따라서 본 논문에서는 추출된 메타데이터에 따라 복잡도를 조절하여 강한 LLM을 맞춤 명령어에 활용하는 Self-Rubrics를 소개한다. 셀프-루브릭들은 먼저 LLM이 명령어 복잡도를 평가하기 위한 메타데이터-특정 루브릭들을 생성하도록 안내한다. 그런 다음 이러한 루브릭에 의해 알려지면 LLM은 명령어의 복잡성을 향상시키기 위해 대응하는 액션 세트를 생성한다. 메타데이터 \((u_{i},\mathbf{s}_{i})\)에 대해, 생성된 액션들의 대응하는 세트는 \(\mathbf{a}_{i}\)이다. 생성된 작업은 인간이 만든 일반 규칙보다 도메인에 더 적합하고 명확하여 복잡합니다.

그림 2: 제안된 CodecLM 개요. 첫째, 강한 LLM \(f_{s}\)은 시드 명령어를 명령어 메타데이터로 인코딩하여 사용 사례와 응답에 필요한 기술을 명시한다. 다음으로, \(f_{s}\)는 메타 데이터를 기본 명령어로 디코딩한다. 한편 Self-Rubrics는 \(f_{s}\)을 활용하여 기본 명령어를 개선하기 위한 루브릭과 액션을 생성하여 다운스트림 태스크에 맞게 조정한다. 마지막으로 대조적 필터링은 \(f_{s}\)와 \(f_{t}\)의 응답을 비교하기 위해 스코어링 함수 \(S\)를 사용한다. 가장 효과적인 쌍은 LLM을 정렬하기 위해 선택되고, 덜 효과적인 지침은 추가 개선을 위해 전송된다. 이 그림에서 강한 LLM의 반응은 목표 LLM에 대해 이기고 있으므로 우리는 목표 LLM을 조정하는 지시를 위해 해당 쌍을 선택한다.

명령어는 메타데이터에 의해 캡처된 타겟 분포에 더 잘 맞춰진다. 예를 들어, "사업 계획 개발"의 사용 사례와 "시장 조사 및 계획"의 기술에 대해 "추론 단계 추가"와 같은 일반적인 규칙은 모호하고 부적절하다. 반대로 Self-Rubrics는 "SWOT 분석 추가" 및 "시장 경쟁사와의 비교 포함"(전체 세부 정보는 부록 A.8 참조)과 같은 액션을 생성하여 명령을 복잡하게 만들 수 있습니다. 지침 개선을 위한 루브릭 및 액션을 생성하기 위한 프롬프트 템플릿은 그림 9, 부록 A.9에 나와 있다.

획득한 동작 \(\{\mathbf{a}_{i}\}_{i=1}^{n}\)을 사용하여 그림 10의 프롬프트 템플릿에 따라 기본 명령어를 복잡하게 만들도록 반복적으로 프롬프트 \(f_{s}\)를 수행할 수 있다. 우리는 한 쌍의 사용 사례와 스킬에 대해 생성된 여러 동작으로부터 동작 \(\mathbf{a}_{i}\)을 무작위로 샘플링한다. 이러한 설계 선택은 제어된 복잡성을 가능하게 할 뿐만 아니라(Xu 등, 2023), LLM에 대한 상이한 동작들 사이의 잠재적인 혼동을 방지한다.

### 대조적 필터링을 통한 명령 선택

Self-Rubrics는 명령어 메타데이터에 기초하여 복잡한 명령어를 재단하지만, 모든 명령어가 그들의 복잡도에 관계없이 명령어 튜닝을 위해 동등하게 효과적인 것은 아니다(Chen et al., 2023; Zhou et al., 2023). 직관적으로 목표 LLM을 도전적이라고 생각하는 지침에 노출하면 개선 영역을 효과적으로 식별할 수 있다. 따라서 대상 LLM을 정렬하기 위한 가장 효과적인 지침을 선택하는 것이 중요하다.

따라서 본 논문에서는 목표 LLM \(f_{t}\)을 효과적으로 향상시킬 수 있는 명령어를 선택하는 방법인 Contrastive Filtering을 소개한다. 명확성을 위해, 우리는 모든 자연어 시퀀스의 공간을 \(\mathcal{N}\)로 정의한다. 반응 품질을 평가하기 위해 강한 LLM \(f_{s}:\mathcal{N}\rightarrow\mathcal{N}\), 목표 LLM \(f_{t}:\mathcal{N}\rightarrow\mathcal{N}\), 점수 함수 \(S:\mathcal{N}\rightarrow\mathbb{R}\)가 있다. 실제로, \(S\)는 비쿠나 쌍별 평가 템플릿(Taori et al., 2023; Chiang et al., 2023)으로부터 적응된 프롬프트 템플릿(도 11, 부록 A.9)과 함께 강한 LLM \(f_{s}\)을 재사용함으로써 얻어진다. 잠재적인 위치 편향을 완화하기 위해, 우리는 두 응답의 위치를 교환함으로써 얻어진 스코어들을 평균한다(Chiang et al., 2023). 우리는 \(f_{s}\)를 사용하여 작업을 채점하는 것을 실제로 매우 잘 관찰하므로 단순성을 위해 이 옵션을 우선시한다. 입력 명령어 \(I\in\mathcal{N}\)가 주어지면, 우리는 두 LLMs로부터 각각 \(f_{s}(I)\)와 \(f_{t}(I)\로 응답을 얻는다. 그런 다음 이 응답 사이의 _품질 간격_\(G:\mathcal{N}\rightarrow\mathbb{R}\)을 정의하여 명령의 _효과_를 추정합니다. \(G(I)=S(f_{s}(I))-S(f_{t}(I))\.

품질격차 메트릭 \(G\)은 각 명령어에 대해 목표 LLM이 강한 LLM으로부터 얼마나 혜택을 받는지 반영한다 \(I\). 도 2에서 입증된 바와 같이, 여기에는 두 가지 가능한 경우가 있다 : (1) \(|G(I)|>\theta\), 여기서 \(\theta\in\mathbb{R}\)는 특정 임계값이다. 이는 강한 LLM이 목표 LLM보다 훨씬 더 나은 반응을 보이거나, 이 격차를 메우기 위해 양질의 명령어-응답 풀에 \((I,f_{s}(I))\)을 추가하거나, 드물게는 목표 LLM이 강한 LLM보다 훨씬 더 나은 반응을 보이거나, 목표 LLM이 원하는 동작을 특정 명령어에 유지하기 위해 \((I,f_{t}(I))\)을 암묵적 규칙화로서 \(\mathcal{D}_{g}\)에 추가한다는 것을 나타낸다. (2) \(|G(I)|\leq\theta\) 여기서 두 LLM의 응답 품질이 유사하므로 \(I\)에서 학습하는 것은 큰 이득으로 이어지지 않는다. 그런 다음 \(I\)를 다음 Self-Rubrics 반복에 보내 추가 개선을 수행합니다.

Contrastive Filtering은 Self-Rubrics를 보완하여 효과적인 명령어-반응 쌍을 선택하기 위해 LLM의 명령어 추종 능력을 강한 LLM으로 보정한다. Contrastive Decoding(Li et al., 2022)과 유사하게, Contrastive Filtering은 두 LLM의 상호작용과 LLM-feedback(Madaan et al., 2023)으로 간주할 수 있다. 품질 격차를 측정하기 위해 강력한 LLM을 점수 함수로 채택하지만 프레임워크는 보다 신뢰할 수 있고 포괄적인 점수 및 피드백 시스템의 발전과 호환되고 잠재적으로 혜택을 받을 수 있으며 향후 유망한 작업으로 남겨둔다.

## 5 Experiments

우리는 여러 대표적인 벤치마크에서 서로 다른 LLM을 사용하여 CodecLM을 평가하기 위한 포괄적인 실험을 수행하는데, 이는 선행 작업(Xu et al., 2023; Chen et al., 2023)에 따라 오픈 도메인 지시에 대한 잘 확립된 평가 설정을 밀접하게 따른다. 또한 부록 A.8에서 사례 연구를 수행하여 CodecLM이 명령을 단계별로 조정하는 방법을 설명한다.

### Evaluation Benchmarks

평가 편향을 줄이기 위해 다양한 수업 분포를 가진 4개의 널리 사용되는 오픈 도메인 수업 후속 벤치마크에 대해 CodecLM을 평가한다. 우리의 테스트 벤치마크들은 Evol-Instruct (Xu et al., 2023), Vicuna (Chiang et al., 2023), Self-Instruct (Wang et al., 2022) 및 Koala (Geng et al., 2023)를 포함한다. 평가를 보완하기 위해 부록 A.7의 두 가지 표준 NLPbenchmark MMLU Hendrycks 등(2020)과 BBH Suzgun 등(2022)에 대해서도 평가한다. 벤치마크 세부 사항은 부록 A.1을 참조해 주십시오.

### Baseline Methods

본 논문에서 제안하는 방법을 명령어 튜닝을 위한 최신 데이터 생성 방법과 비교한다. 공정한 비교를 위해 가능한 한 모든 방법에 동일한 LLM 백본을 제공한다. 또한, 데이터 양의 영향을 제거하기 위해 모든 방법에 대해 명령-응답 쌍의 수를 동일하게 제어한다. 기본 방법에는 CodecLM에서 생성 된 기본 지침을 시드 지침으로 사용 하 여 강화 된 버전의 WizardLM인 **Self-Instruct**Wang 등 (2022), **Alpagasus**Chen 등 (2023), **Tree-Instruct**, **WizardLM**Xu 등 (2023) 및 **WizardLM+** 가 포함 됩니다. 기준 세부 정보는 부록 A.2에 나와 있다.

### 실험 및 평가 세부 정보

**LLM 백본들.** LLaMA 기반 Touvron et al.(2023) 및 PaLM 기반 Anil et al.(2023) LLMs를 우리의 실험에서 우리의 목표 LLMs로 채택한다. LLaMA 기반 표적 LLM의 경우, 강력한 LLM으로 Gemini-Pro Team 등(2023)을 사용하고, 표적 LLM으로 LLaMA-7B, -13B를 사용한다. PaLM 기반 표적 LLM의 경우 텍스트 유니콘을 강력한 LLM으로 사용하고 텍스트 보손을 표적 LLM으로 사용한다. PaLM 기반 모델과 제미니프로는 구글 클라우드 API1을 통해 접근이 가능하다.

각주 1: [https://cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai)

**CodecLM 구현 세부 정보** 모든 벤치마크를 20% 유효성 검사 집합과 80% 평가 집합으로 분할합니다. 유효성 검증 세트에서 명령어 메타데이터를 추출하고 자세한 내용은 부록 A.3을 참조하십시오. 지정한 총 데이터 크기에 따라 강력한 LLM이 메타데이터당 동일한 수의 기본 명령어를 생성하도록 촉구한다. 실험 내내 500-8000개의 합성 데이터를 생성합니다. 우리는 4개의 루브릭과 그에 대응하는 액션들을 생성한다. 각 반복에서 우리는 수업을 개선하기 위해 무작위로 1개의 동작을 선택한다. 셀프 루브릭을 최대 4회 반복합니다. 대비 필터링의 경우 모든 실험에서 점수 척도를 10으로, 필터링 임계값을 3으로 설정했다. 이러한 구성을 Xu 등(2023)과 정렬하고 이러한 구성의 더 자세한 근거, 추가 하이퍼파라미터 설정 및 교육 세부 사항을 부록 A.3-A.4에 남긴다.

**평가.** LLM이 지침을 얼마나 잘 따르는지 평가하는 것은 명령어에 다양한 유효한 응답이 있다는 사실과 인간 평가를 복제하는 문제로 인해 복잡합니다. Dubois et al. (2023)에 이어 수업에 대한 자동 평가의 최근 발전; Zheng et al. (2023)은 LLM 기반 평가자가 확장 가능하고 설명 가능하며 인간 평가와 일치함을 보여준다. 따라서 우리는 ChatGPT를 기반으로 하는 널리 사용되는 Vicuna 쌍별 평가자 Chiang et al.(2023)을 채택하여 가격과 효율성 측면에서 접근성에 대한 두 LLM의 응답 품질을 비교한다. 평가 프롬프트 템플릿은 그림 12, 부록 A.9에 있다. 우리는 LLM 기반 평가자의 일관성을 입증하기 위해 부록 A.6에 GPT-4 기반 평가 결과를 포함한다. LLM 평가자가 가질 수 있는 위치 편향을 완화하기 위해 응답 명령을 교환하여 모든 평가를 두 번 수행한다. 반응이 두 번 승리할 때만 더 나은 것으로 간주된다. Chen et al.(2023)에 이어 평가 랜덤성을 줄이기 위해 온도를 0.0으로 설정하고 다른 매개변수를 기본으로 두었다.

이전 작업 Xu 등(2023)과 유사하게 Zhao 등(2023)은 목표 LLM이 강한 LLM(종종 상한 수행자로 취급됨)으로부터 얼마나 많은 모델 용량을 회복하는지 나타내기 위해 강력한 LLM에 대한 목표 LLM의 승 및 동점의 총 비율을 계산한다. CRR은 모든 대상 LLM 간의 조합 쌍별 비교를 단순화한다. 메트릭 이름을 _Capacity Recovery Ratio_ (CRR)로 지정 합니다. 여기서 \(\texttt{CRR}=\frac{\texttt{wins}+\texttt{ties}}{\texttt{total comparisons}}\). 실험에서 우리는 강한 LLM이 목표 모델보다 훨씬 능력이 있기 때문에 타이 수가 종종 승수를 지배한다는 것을 관찰한다. 그래서 우리는 계산에서 승리에 추가적인 가중치를 두지 않는다. CRR이 모델 성능을 충실하게 반영한다는 것을 입증하기 위해 Evol-Instruct의 부록 A.5에서 승, 동 및 패의 정확한 수를 보여준다. 절대값은 우리가 선택하는 특정 LLM 평가자를 기반으로 할 수 있기 때문에 절대값 대신 다른 방법 간의 CRR 격차에 초점을 맞추고 싶다.

### OpenDomain Instruction Following

**LLaMA 기반 대상 LLMs를 사용한 결과** 표 1은 명령 튜닝을 위한 CodecLM 및 2000 합성 데이터와의 비교 기준선의 성능을 요약합니다. 모든 방법은 LLaMA-7B 또는 -13B를 표적 LLM으로 훈련하고 데이터를 생성하는 강력한 LLM인 제미니-프로와 비교한다. 코덱LM은 크기가 다른 두 개의 목표 LLM으로 모든 벤치마크에서 일관되게 방법을 비교하는 것보다 우수하다. CodecLM의 일관성 있게 우수한 성능은 다른 다운스트림 명령어 분포 및 목표 LLM에 대한 일반화 가능성을 강조한다. Tree-Instruct와 WizardLM의 변형은 모두 명령어 복잡성의 중요성에 초점을 맞추지만, 그들의 성능이 단순한 명령어, 특히 더 큰 목표 LLM을 가진 Alpagasus보다 항상 나은 것은 아니다. 이 관찰은 데이터의 효과가 명령 복잡성에 의해서만 결정될 수 없음을 나타내며 자체 루브릭 및 대조적 필터링 설계의 동기를 검증한다. 또한, 위저드LM+의 위저드LM에 대한 승리는 명령어 메타데이터를 통한 명령어 분산 매칭의 유효성을 확인한다. 대상 LLM을 LLaMA-7B에서 -13B로 이동할 때 모든 방법이 상당한 성능 향상을 가져오며, 이는 모델 크기 크기 Wei 등(2021)에 대한 사전 발견과 일치한다.

**PaLM 기반 모델을 사용한 결과** 표 2는 LLaMA 기반 실험에서 CodecLM 및 가장 성능이 좋은 기준선의 결과를 요약한 것입니다. 우리는 계산 예산으로 인해 1000개의 합성 데이터를 생성한다. 텍스트 들소(text-bison)는 명령어 튜닝을 포함한 다양한 기술과 정렬된 독점 모델이기 때문에 기본 접근법으로도 포함한다. 흥미롭게도, 텍스트 들소는 다양한 벤치마크에서 강력한 성능을 얻습니다. 알파가수스와 위저드LM+는 모두 텍스트 들소보다 성능이 낮으며, 이는 잘 조정된 LLM을 지속적으로 개선하는 것이 사소하지 않음을 시사한다. 반대로, 코덱LM은 목표 LLM을 개선하기 위해 고품질 데이터 쌍을 적응적으로 조정하는 핵심 설계 덕분에 대부분의 경우 텍스트 바이슨을 능가한다.

### Ablation Study

이 절에서는 CodecLM의 효과를 경험적으로 탐구하기 위해 포괄적인 절제 연구를 수행한다. 우리는 주로 LLaMA-7B 모델을 표적 LLM으로, 제미니-프로를 강력한 LLM으로 사용하여 실험을 수행하고 Evol-Instruct 벤치마크에 CRR을 보고한다.

**핵심 설계의 효과** 표 3의 프레임워크에서 구성 요소별 기여도를 보여 줍니다. 1번째 행에는 Self-Instruct의 결과가 기준선으로 있으며, 2번째 행에는 LLM과 명령 메타데이터의 기본 명령만 정렬하고, 3번째 행과 4번째 행에는 Self-Rubrics와 Contrastive Filtering을 차례로 추가합니다. 우리는 모든 구성 요소가 최종 성능에 기여한다는 것을 분명히 관찰합니다. 흥미로운 사실은 메타데이터에서 기본 명령어를 사용하는 성능은 표 1의 위저드LM+와 동등하다. 이 관찰은 명령어를 복잡하게 만들기 위한 인간이 만든 전략이 다른 유형의 명령어에 맞지 않을 수 있음을 나타낸다. 반대로, Self-Rubrics는 상이한 메타데이터에 기초하여 명령어 개선 액션들을 적응적으로 생성하고, 그 결과 타겟 LLM에 대한 더 나은 맞춤화된 명령어들을 생성한다. 대비 필터링의 추가 개선 사항은 선택된 데이터가 정렬에 실제로 더 효과적임을 보여준다.

**반복 횟수의 영향** 그림 3에서 CodecLM 반복 횟수의 영향을 보여 줍니다. 특히 모든 합성 데이터 \(\mathcal{D}_{g}\)에서 각 반복의 데이터 비율을 세어 왼쪽 y축이 있는 파란색 막대 차트에 표시합니다. 또한 합성 데이터에 대한 학습 후 CRR에서 목표 모델 성능을 도출한다.

\begin{table}
\begin{tabular}{l||c c c c||c c c c} \hline \hline  & \multicolumn{4}{c||}{**LLaMA-7B vs. Gemini-Pro**} & \multicolumn{4}{c}{**LLaMA-13B vs. Gemini-Pro**} \\ \cline{2-9}
**Methods** & **Evol-Ins.** & **Vicuna** & **Koala** & **Self-Ins.** & **Evol-Ins.** & **Vicuna** & **Koala** & **Self-Ins.** \\ \hline Self-Instruct & 72.02 & 81.25 & 67.78 & 65.87 & 75.69 & 86.25 & 77.22 & 69.05 \\ Alpagasus & 75.23 (+3.2) & 81.25 (+0.0) & 71.11 (+3.3) & 70.24 (+4.4) & 79.82 (+4.1) & 87.50 (+1.3) & 77.78 (+0.6) & 71.03 (+2.0) \\ Tree-Instruct & 75.23 (+3.2) & 81.25 (+0.0) & 72.78 (+5.0) & 68.65 (+2.8) & 82.57 (+6.9) & 87.50 (+1.3) & 80.56 (+3.3) & 79.37 (+0.3) \\ WizardLM & 74.31 (+2.3) & 76.25 (+5.0) & 65.56 (+2.1) & 74.13 (+5.6) & 82.11 (+6.0) & 86.25 (+0.0) & 78.89 (+1.7) & 76.19 (+7.1) \\ WizardLM+ & 75.69 (+3.7) & 83.75 (+2.5) & 86.33 (+0.6) & 72.22 (+6.4) & 84.40 (+8.7) & 88.75 (+2.5) & 81.11 (+3.9) & 79.76 (+10.7) \\ CodecLM (ours) & **79.82 (+7.8)** & **88.75 (+7.5)** & **74.44 (+6.7)** & **78.17 (+2.3)** & **86.70 (+11.0)** & **90.00 (+3.8)** & **82.22 (+5.0)** & **83.33 (+14.3)** \\ \hline \hline \end{tabular}
\end{table}
표 1: 벤치마크에 이어 4개의 오픈 도메인 지침에 대한 LLaMA 기반 목표 모델의 결과. 각 방법은 LLaMA-7B 또는 -13B를 기반으로 목표 모델을 학습하고 강력한 모델인 제미니-프로와 비교한다. 보고된 메트릭 용량 복구 비율(%) \(\mathtt{CRR}=\frac{\mathtt{wins+ties}}{\mathtt{total\ comparisons}}\). CRR이 클수록 성능이 향상됩니다.

\begin{table}
\begin{tabular}{l||c c||c} \hline \hline
**Metadata** & **Self-Rubrics** & **Contrastive Filtering** & **CRR** \\ \hline ✗ & ✗ & ✗ & 72.02 \\ ✓ & ✗ & ✗ & 75.23 \\ ✓ & ✓ & ✗ & 77.52 \\ ✓ & ✓ & ✓ & 79.82 \\ \hline \hline \end{tabular}
\end{table}
표 3: CodecLM의 핵심 디자인에 대한 절제 연구. 모든 구성 요소가 최종 성능에 기여합니다.

오른쪽 y축이 있는 노란색 선 차트의 현재 반복까지. 데이터 비율 막대 차트로부터 데이터의 \(70\%\) 이상이 첫 번째 반복에서 나온다는 것을 관찰한다. 이는 대조적 필터링이 덜 복잡하지만 도전적인 명령어를 성공적으로 수집한다는 것을 나타내며, 이는 목표 LLM의 명령어 추종 능력을 구축하는 데 중요하다. 두 번째 반복부터 데이터 비율이 점점 작아집니다. 그러나 _less_와 유사하게 정렬 관찰 [23]에서는 양이 적음에도 불구하고 고품질 및 더 복잡한 명령이 실제로 최종 성능에 기여한다.

**배포 일치에 대한 탐색** 이전 결과에서 볼 수 있듯이 다운스트림 명령 배포에서 추출한 메타데이터를 생성하는 것이 실제로 도움이 됩니다. 그러나, 실제로, 추출되거나 사람이 작성한 메타데이터는 명령어 분포를 정확하게 특성화할 수 없을 수 있다. 따라서 명령어 메타데이터로 표현되는 분포가 테스트 분포와 완전히 일치하지 않을 때 CodecLM의 성능을 탐색할 필요가 있다. 실제 테스트 분포가 복잡하고 사전으로 알려져 있지 않기 때문에, 우리는 메타데이터 \(\mathcal{M}\) 집합으로부터 랜덤 서브샘플링을 통해 다양한 분포 매칭 정도를 근사화한다. 데이터 양의 영향을 통제하기 위해 각 경우에 대해 명령-응답 쌍의 총 수를 동일하게 유지한다. 예를 들어, \(\mathcal{M}\)의 \(20\%\)를 서브샘플링할 때, 우리는 강력한 LLM이 그에 따라 각 메타데이터에 대해 5배 더 많은 명령어를 생성하도록 프롬프트한다. 결과는 그림 4의 상단에 나와 있으며, 우리는 더 나은 명령어 메타데이터가 기본 분포를 캡처할수록 목표 LLM이 더 나은 성능을 달성할 수 있다는 추세를 관찰했다. 또한, 메타데이터 매칭 비율이 \(60\%\) 이상일 때, 완전 매칭된 결과로 근접한 성능을 얻을 수 있었다. 이 관찰은 잠재적인 명령어 메타데이터 불일치 하에서 CodecLM의 견고성을 강조한다.

**모델 크기 및 데이터 양으로 크기 조정** 다른 합성 데이터 양과 모델 크기로 크기를 조정하는 방법을 탐색하기 위해 CodecLM을 가장 경쟁력 있는 기준선인 WizardLM+와 비교하여 실험을 수행합니다. LLaMA-7B와 -13B를 목표 LLM으로 하는 Evol-Instruct에 대한 실험 결과는 그림 5에 나와 있다. 두 방법 모두 더 많은 합성 데이터와 더 큰 목표 모델로 점점 더 나은 성능을 보인다. CodecLM은 모든 경우에서 WizardLM+를 일관되게 능가하여 데이터 효율성과 확장성이 뛰어납니다. 우리는 목표 모델과 강한 LLM 사이의 고유한 능력 차이로 인해 8k 이상의 합성 데이터를 생성한 후 이득이 점차 감소할 것으로 예상한다.

## 6 Conclusion

본 논문에서는 LLM 정렬을 위한 합성 데이터를 서로 다른 타겟 명령어 분포와 LLM으로 맞춤화하는 CodecLM을 제안한다. CodecLM은 명령어 메타 데이터를 통해 효과적으로 명령어 분포를 캡처하고, Self-Rubrics와 Contrastive Filtering을 통해 가장 효과적인 명령어-응답 쌍을 조정함을 보인다. 코덱LM은 인간 주석의 필요 없이 맞춤형 사용을 위해 LLM을 적용하는 데 강력한 솔루션을 제공한다. 우리는 CodecLM이 더 풍부한 메타데이터 정의, 더 나은 신속한 설계 및 더 신뢰할 수 있는 LLM 기반 점수 작성자와 같은 프레임워크 내에서 여러 유망한 연구 방향의 문을 여는 표적 LLM 정렬을 위한 일반적인 프레임워크 역할을 한다고 믿는다. CodecLM은 직교 연구 분야에서도 이익을 얻을 수 있으며 윤리적 고려 및 한계 섹션에서 논의를 계속한다.

그림 4: 메타데이터 매칭 비율 vs. CRR

그림 5: 모델 크기 및 데이터 양으로 스케일링합니다.

그림 3: 각 반복의 데이터 비율과 각 반복에서의 해당 CRR 성능.

### Ethical Considerations

코덱LM이 LLM 정렬을 위한 효과적인 데이터 합성 프레임워크 역할을 하지만 작업의 윤리적 영향도 반영해야 한다. 제안하는 방법은 LLMs을 활용하여 명령어-응답 쌍을 생성한다. 데이터 주석 프로세스 동안 무의식적인 실수를 할 수 있는 인간 주석자와 유사하게, LLMs는 또한 때때로 비윤리적, 독성 또는 오판의 소지가 있는 명령 및 응답을 생성한다(Bender et al., 2021). 또한, 생성된 데이터를 사용하여 목표 LLM을 훈련함에 따라 결과적인 명령 조정 LLM은 원래 모델에서 편향 및 공정성 문제(Gallegos 등, 2023)를 수반할 수 있다. 부록 A.3에 명시된 대로 수동 검사를 수행했지만 실제로는 기존 기술(Hanu 및 Unitary team, 2020; Thakur et al., 2023)을 채택하여 CodecLM에 사용되는 LLM의 비뚤림을 해독하고 완화하고 생성된 데이터를 정리하기 위해 보다 엄격한 검사 및 필터링 규칙을 설계해야 한다. 프레임워크의 유연성으로 인해 편향 및 공정성 문제를 줄이는 영역에서 향후 진전이 CodecLM에 보완적일 수 있다고 상상한다.

### Limitations

LLM 정렬 분야에서 향후 연구 기회를 고취하기 위해 다음과 같은 측면에서 CodecLM의 한계를 인정한다.

우선, 윤리적 고려사항에서 논의된 바와 같이, 우리의 방법은 데이터를 생성하기 위해 강한 LLM을 요구하므로, 우리의 방법의 성능은 LLM의 품질에 의존하며, 그것으로부터 편향 및 공정성 문제를 물려받을 수 있다. 반면에, CodecLM은 진보된 편향 감소 및 공정성 향상 접근법으로 개선된 더 강한 LLM으로부터 이익을 얻을 수 있다.

두 번째, 직교 방향으로서, 본 방법은 즉각적인 주입(Liu et al., 2023) 및 탈옥 Zou et al.(2023)과 같은 적대적 공격에 대한 명령 조정 모델의 견고성을 탐색하지 않았다. 실제로, 우리는 우리의 방법에서 지시 조정된 LLM에 따라 적대적 방어 기술(Jain 등, 2023)을 적용해야 한다.

또한, 정렬을 위한 데이터 합성에서 최근 연구에 따른 LLM 기반 자동 평가 방법을 주로 사용한다. 최근의 연구들(Chiang et al., 2023; Dubois et al., 2023)은 LLM-기반 평가가 대체로 인간 평가와 일치함을 입증하지만, LLM-기반 평가자들의 확장성 및 신뢰성은 여전히 개선의 여지가 있다. LLM 기반 평가 결과를 보완하기 위해 부록 A.7에 일부 표준 벤치마크 결과를 포함하지만 LLM을 더 잘 평가하는 진전이 여전히 우리 방법의 효율성을 보다 신뢰할 수 있는 입증으로 이어질 수 있다고 믿는다.

마지막으로, 섹션 5.5에 도시된 바와 같이, CodecLM은 중간 분포 불일치에 강인하지만, 그 성능은 여전히 메타데이터가 기본 명령어 분포를 얼마나 잘 캡처하는지에 의존한다. 실제로 수집된 종자 지침은 실제 테스트 지침과 다를 수 있다. 또는 사용자 명세에서 직접 메타데이터를 생성하는 경우, 사용자는 테스트 시간에 자신의 생각을 변경하여 원본 메타데이터를 넘어 모델 분산 해제 지침을 보낼 수 있다. 결과적으로, CodecLM은 분배 불일치 하에서 성능 저하를 겪을 수 있다. 이를 해결하기 위해 CodecLM에서 생성된 데이터를 갱신하기 위해 사용자 명령 트래픽이나 사용자 피드백을 지속적으로 수집하고, 목표 LLM을 지속적으로 갱신할 수 있다.

향후 연구가 LLM 정렬을 위한 유연한 데이터 합성 프레임워크로 CodecLM을 활용할 수 있어 현 분야의 발전이 CodecLM에 통합되어 현재의 한계를 줄일 수 있기를 바란다.

## References

* Anil 등(2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. _ arXiv preprint arXiv:2305.10403_.
* Aribandi 등(2021) Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. 2021. Ext5: Towards extreme multi-task scaling for transfer learning _ arXiv preprint arXiv:2111.10952_.
* Bai 등(2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. _ arXiv preprint arXiv:2204.05862_.
* Bender et al.(2021) Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021년. 확률적 앵무새의 위험성에 대해: 언어 모델이 너무 클 수 있나요? 공정성, 책임성 및 투명성에 대한 2021 ACM 회의 회보 610-623 페이지입니다.
* Beyer et al.(2022) Lucas Beyer, Xiaohua Zhai, Amelie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. 2022년

지식 증류: 좋은 교사는 인내심이 강하고 일관적이다. IEEE/CVF 회의의 컴퓨터 비전 및 패턴 인식에 대한 _Proceedings of the computer vision and pattern recognition_에서 10925-10934 페이지.
* Brown 등(2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models is few-shot learners. _ Advances in neural information processing systems_, 33:1877-1901.
* Chen et al.(2023a) Derek Chen, Celine Lee, Yunan Lu, Domenic Rosati, and Zhou Yu. 2023a. 제어 가능한 데이터 생성을 위해 소프트 프롬프트를 혼합합니다. _ arXiv preprint arXiv:2303.01580_.
* Chen 등(2023b) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. 2023b. 알파가수스: 적은 데이터로 더 나은 알파카를 훈련합니다. _ arXiv preprint arXiv:2307.08701_.
* 치앙 등(2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Liamin Zheng, Siyuan Zhang, Yonghao Zhang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: 90%* 챗봇 품질을 가진 gpt-4를 인상적인 오픈 소스 챗봇.
* Chung et al.(2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. _ arXiv preprint arXiv:2210.11416_.
* Dong et al. (2023) Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. 2023. Steerlm: Attribute conditioned sft as a (user-steerable) alternative to rhhf. _ arXiv preprint arXiv:2310.05344_.
* Dubois 등(2023) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpacafarm: 인간의 피드백으로부터 학습하는 방법을 위한 시뮬레이션 프레임워크 _ arXiv preprint arXiv:2305.14387_.
* Efrat and Levy (2020) Avia Efrat and Omer Levy. 2020. 터킹 테스트: 언어 모델이 지침을 이해할 수 있습니까? _ arXiv preprint arXiv:2010.11982_.
* Fernando et al. (2023) Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktaschel. 2023. Promptbreeder: 신속한 진화를 통한 자기 지시적 자기 향상 _ arXiv preprint arXiv:2309.16797_.
* Gallegos 등(2023) Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2023. 대형 언어 모델의 편견과 공정성: 설문 조사 _ arXiv preprint arXiv:2309.00770_.
* Geng et al. (2023) Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: 학술 연구를 위한 대화 모델. 블로그 게시물
* Hanu and team (2020) Laura Hanu and Unitary team. 2020. 해독. Github. [https://github.com/unitaryai/detoxify] (https://github.com/unitaryai/detoxify).
* Hendrycks 등(2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. 대용량 멀티태스킹 언어 이해도 측정. _ arXiv preprint arXiv:2009.03300_.
* Hinton 등 (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. 신경망에 지식을 증류합니다. _ arXiv preprint arXiv:1503.02531_.
* Honovich et al.(2022) 또는 Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. 부자연스러운 명령: 인간 노동력이 거의 없는 언어 모델을 조정합니다. _ arXiv preprint arXiv:2212.09689_.
*Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023, 단계별 증류! 더 적은 훈련 데이터와 더 작은 모델 크기로 더 큰 언어 모델을 수행하는 것을 능가합니다. _ arXiv preprint arXiv:2305.02301_.
* Jain 등(2023) Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Sompealli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. 정렬된 언어 모델에 대한 적대적 공격에 대한 기본 방어입니다. _ arXiv preprint arXiv:2309.00614_.
* Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Variational Bayes를 자동 인코딩합니다. _ arXiv preprint arXiv:1312.6114_.
* Kopf 등(2023) Andreas Kopf, Yannic Kilcher, Dimitri von Ritte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhouh Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, et al. 2023. Openassistant conversations-democratizing large language model alignment. _ arXiv preprint arXiv:2304.07327_.
* Kramer(1991) Mark A Kramer. 1991. 자동 연관 신경망을 이용한 비선형 주성분 분석. _ AIChE journal_, 37(2):233-243.
* Lee et al. (2023) Gyeong-Geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, and Xiaoming Zhai. 2023. 자동 점수 매기기 위해 대형 언어 모델 및 사상 연쇄 적용 _ arXiv preprint arXiv:2312.03748_.
* Li et al.(2023) Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. 2023. 명령어 역번역과의 자체 정렬. _ arXiv preprint arXiv:2308.06259_.
* Li et al.(2022) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2022. 대조 디코딩: 최적화로서 개방형 텍스트 생성. _ arXiv preprint arXiv:2210.15097_.

SS2로 인용됩니다.
*C. Liang, S. 주규 장필호 Chen, T. Zhao (2023)Less is more: task-aware layer-wise distillation for language model compression. In International Conference on Machine Learning, pp. 20852-20867. Cited by: SS2.
* A. Liu, S. 스웨이암디프타, N.A. 스미스, Y. Choi (2022)Wanli: 작업자 및 자연어 추론 데이터 세트 생성을 위한 ai 협업. arXiv preprint arXiv:2201.05955. 인용: SS2.
* Y. 유경등 이광 왕태 장영 류현왕 정, 영 Liu (2023) Prompt Injection Attack to llm-integrated applications. arXiv preprint arXiv:2306.05499. 인용: SS2.
* A. Madaan, N. 탄돈 P. 굽타 S. 한리난 고성훈 위그레프 알론남 지리 프라부모예 Yang, et al. (2023)Self-refine:iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651. 인용: SS2.
* Y. 맹민 미찰스키 제이황 장태 Abdelzaher, J. Han (2023)Tuning language models as training data generator for augmentation-enhanced few-shot learning. In International Conference on Machine Learning, pp. 24457-24477. Cited by: SS2.
*O. 알 (2023)Gpt-4 기술 보고서. arXivabs/2303.08774. 인용: SS2.
*O. 알 (2023) gpts를 소개합니다. 참고: SS2로 인용된 [https://openai.com/blog/introducing-gpts](https://openai.com/blog/introducing-gpts)입니다.
* L. 우양종우 장, D. 알메이다, C. 웨인라이트, P. 미쉬킨, C. 장, S. 아가르왈 Slama, A. Ray, et al. (2022)Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems35, pp. 27730-27744. 인용: SS2.
* B. Peng, C. Li, P. He, M. Galley, and J. Gao (2023)Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. Cited by: SS2.
* C. Raffel, N. A. 로버츠 샤저 이상욱 나랑 마테나 주원 Li, and P. J. Liu (2020)Exploring the limit of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research21 (1), pp. 5485-5551. Cited by: SS2.
*T. Schick and H. Schutze (2021)는 사전 학습된 언어 모델을 사용하여 데이터 세트를 생성하는 단계를 포함한다. arXiv preprint arXiv:2104.07540. 인용: SS2.
* M. 수즈건 비늘, N. Scharli 게르만 Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. (2022)Challenging big-bench tasks and whether chain-of-thought can solve them arXiv preprint arXiv:2210.09261. 인용: SS2.
* R. 타오리 일굴라자니 장영 두부아, 엑스 Li, C. Guestrin, P. Liang, and T. B. Hashimoto (2023) Stanford alpaca: instruction following lama model. 참고: SS2로 인용 된 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).
* R. G. Team 아닐세 보르헤오 우재락 Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. (2023)Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. 인용: SS2.
* H. Thakur, A. Jain, P. Vaddamanu, P. Pu Liang, and L. 모린시(2023) 언어 모델은 성전환을 얻는다: 적은 수의 데이터 개입으로 성편견을 완화한다. arXiv preprint arXiv:2306.04597. 인용: SS2.
* H. Touvron, T. 라브릴, G. 이자카드, X. 마틴 라쇼 라크루아 B. 로지에르 Goyal, E. Hambro, F. Azhar, et al. (2023)Llama: open and efficient foundation language models. arXiv preprint arXiv:2302.13971. 인용: SS2.
* S. 빅터 알버트 콜린 B. 스티븐 린탕, A. 자이드, C. 앙투안, S. 아르노 Arun, D. Manan, et al. (2022)Multitask prompted training enables zero-shot task generalization. International Conference on Learning Representations, 인용: SS2.
* Y. Wang, H. Ivison, P. 다지기, J. 헤셀, T. Khot, K. F. Chandu, D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy, et al.(2023) 어느 정도까지 취소를 할 수 있는가? 개방형 리소스에서 명령어 튜닝 상태를 탐색합니다. arXiv preprint arXiv:2306.04751. 인용: SS2.
* Y. 왕영 고디 Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi (2022) Self-instruct: align language model with self generated instructions. arXiv preprint arXiv:2212.10560. 인용: SS2.
* J. Wei, M. 보즈마 구아위유, B.레스터, N. Du, A. M. Dai, Q. V. Le (2021) Finetuned 언어 모델은 제로샷 학습자이다. arXiv preprint arXiv:2109.01652. 인용: SS2.
* J. Wei, X. 왕동수만 Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. (2022)Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems35, pp. 24824-24837. Cited by: SS2.
* C. Xu, Q. 선광 정석 Geng, P. Zhao, J. Feng, C. Tao, and D. Jiang(2023)Wizardlm: 큰 언어 모델이 복잡한 지침을 따르도록 권한을 부여합니다. arXiv preprint arXiv:2304.12244. 인용: SS2.
*C. Yang, X. 왕영 Lu, H. Liu, Q. V. Le, D. Zhou, and X. Chen (2023)Large language models as optimizers. _ arXiv preprint arXiv:2309.03409_.
* Yu et al. (2023) Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2023. Large language model as attributed training data generator: A story of diversity and bias. _ arXiv preprint arXiv:2306.15895_.
* Zhao et al.(2023) Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, and Nevin L Zhang. 2023. A preliminary study of the intrinsic relationship between complexity and alignment _ arXiv preprint arXiv:2308.05696_.
* Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arenna. _ arXiv preprint arXiv:2306.05685_.
* Zhou 등(2023a) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023a. 리마: 정렬을 위해 적은 것이 더 많습니다. _ arXiv preprint arXiv:2305.11206_.
* Zhou et al.(2023b) Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023b. 대용량 언어 모델에 대한 명령 수행 평가 _ arXiv preprint arXiv:2311.07911_.
* Zou et al.(2023) Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attack on aligned language models.

Appendix

### Benchmark Details

다음 벤치마크에 따른 공개 수업의 세부 사항은 다음과 같습니다.

* Evol-Instruct(Xu et al., 2023)는 온라인 오픈 소스 프로젝트, 플랫폼 및 포럼과 같은 다양한 소스의 218개의 실제 인간 지침을 포함한다.
* Vicuna(Chiang et al., 2023)는 prompt 엔지니어링을 통해 GPT-4에 의해 생성된 80개의 다양한 명령어를 포함한다.
* Self-Instruct(Wang et al., 2022)는 사용자 지향 애플리케이션에 의해 동기화된 252개의 전문가 작성 지침을 포함한다.
* Koala(Geng et al., 2023)는 온라인에 게시된 180개의 대화 스타일의 실제 사용자 지침을 포함한다.

이 모든 벤치마크는 여러 범주 또는 작업의 영어 지침으로 구성됩니다. 그러나 일반적인 지식 QA 및 코딩과 같은 몇 가지 일반적인 사용 사례를 공유하지만 다른 벤치마크에서 명령어의 적용 범위는 실제로 다르다. 예를 들어, Xu et al.(2023)은 Evol-Instruct가 Vicuna와 어떻게 다른지 수업 분배에 대해 자세히 논의한다. 명령어 분포의 차이는 다운스트림 태스크가 다른 실제 시나리오를 효과적으로 모방한다.

추가 표준 NLP 벤치마크의 세부사항은 다음과 같다:

* MMLU(Hendrycks et al., 2020), Massive Multitask Language Understanding은 언어 모델의 능력을 측정하도록 설계된 벤치마크입니다. STEM, 인문학, 사회과학 등 다양한 분야에 걸쳐 57개 과목을 다루고 있다. 테스트 결과 보고에는 테스트 분할만 사용하고 모든 작업에서 평균 점수를 보고합니다.
* BBH(Suzgun et al., 2022), BIG-Bench-Hard에는 이전 언어 모델이 평균 인간 등급을 능가하지 못한 23개의 도전적인 BIG-Bench 작업이 포함됩니다.

모든 벤치마크는 비상업적 연구 목적으로 공개적으로 사용할 수 있으며 이 연구 작업에서 사용을 엄격하게 제한한다. 또한 이러한 데이터 세트를 주의 깊게 확인하고 개인 정보가 포함되지 않았는지 확인합니다.

### Baseline Details

**Self-Instruct**(Wang et al., 2022)는 기존 시드 지침을 적은 샷 시연으로 LLM에 프롬프트하여 지침을 생성합니다. 여기서는 시드 명령으로 Alpaca(Taori et al., 2023) 데이터 세트를 무작위로 하위 샘플링한다. 알파카 자체는 Self-Instruct를 기반으로 하기 때문에 하위 집합을 시드로 사용하는 것은 Self-Instruct 방법의 자연스러운 연속이다.

**Alpagasus**(Chen 등, 2023)는 ChatGPT 기반 응답 품질 평가기를 사용하여 데이터를 선택적으로 필터링합니다. 원래 접근 방식을 거의 따라 자체 지침에 의해 생성된 명령-응답 쌍에 전략을 채택한다.

**트리 명령**(Zhao 등, 2023)은 LLM이 의미 트리를 통해 명령을 암묵적으로 복잡하게 만들도록 프롬프트함으로써 명령 품질을 향상시킵니다. 원본 논문에 이어 서브샘플링된 알파카 데이터 세트를 시드 데이터로 사용한다. 최상의 성능을 위해 트리 노드의 수를 10개로 설정하였다.

**WizardLM**(Xu 등, 2023)은 미리 정의된 진화 작업 집합으로 LLM을 프롬프트하여 지침을 반복적으로 복잡하게 만듭니다. 위저드 LM의 인기와 효과를 고려하여 Alpaca를 시드 데이터로 사용하는 원본 버전과 CodecLM에서 생성된 동일한 기본 명령어 세트를 시드 데이터로 사용하는 향상된 버전의 두 가지 변형을 사용하여 실험한다. 이후의 변형을 프레임워크의 구성 요소에 의해 향상된 것으로 **WizardLM+** 로 명명합니다.

### 추가 구현 세부 정보

다양한 지침의 사용 사례와 기술을 믹스 앤 매칭하여 메타데이터를 200으로 늘립니다. 우리는 \(\{u_{i}\}_{i=1}^{n}\)에서 하나의 유스 케이스를 무작위로 샘플링하고 \(\bigcup_{i=1}^{n}\mathbf{s}_{i}\)에서 교체 없이 샘플링된 하나 이상의 스킬과 쌍을 이룬다. 대부분의 기술은 사용 사례 간에 일반화할 수 있지만 불합리한 사용 사례와 기술 쌍을 제외하기 위해 여전히 수동 건전성 검사를 수행한다. 우리는 Self-Rubrics를 통해 반복적으로 명령어를 개선하기 위한 하이퍼-파라미터를 선행 작업과 정렬한다(Xu et al., 2023): 4개의 루브릭 및 대응하는 액션을 생성하고, 각각의 반복에서, 명령어 개선을 위한 1개의 액션을 랜덤하게 선택한다. 위저드LM과의 공정한 비교를 위해, 또한 각 명령어에 대해 최대 4번의 개선 반복을 사용한다(기본 프롬프트 생성을 첫 번째 반복으로 카운트한다). 대비 필터링의 경우, 우리는 항상 강한 LLM 자체를 득점자로 사용한다. 모든 실험에 대해 점수 척도를 10으로, 필터링 임계값을 3으로 설정했다. 우리는 AlpacaEval Dubois et al. (2023) 데이터 세트를 사용하여 임계값을 얻는다. 그리고 우리는 이 임계값이 일반적으로 다른 설정에서 잘 작동한다는 것을 발견한다. 또한, LLaMA 기반 모델의 경우, Alpaca Taori et al.(2023)의 대응 모델을 대조적 필터링에서 응답 생성을 위한 목표 LLM으로 사용하는 것이 명령 튜닝되지 않은 원래 모델보다 더 잘 작동한다. 메타데이터 추출, 기본 명령어 생성 및 Self-Rubrics는 추론 온도 0.7을 사용한다. LLaMA 기반 모델의 경우 최대 생성 토큰 수를 2048개, PaLM 기반 모델의 경우 API 제약으로 인해 1024개로 설정한다. 또한, 메타데이터 추출을 위해 20%의 검증 집합을 따로 두었지만, 여전히 본 논문에서 전체 테스트 집합에 대한 성능을 보고하고 있는데, 그 이유는 다음과 같다. (1) 전체 테스트 벤치마크에서 검증 집합을 제거해도 우리의 방법의 상대적 우월한 성능은 변하지 않으며, 우리의 방법과 기준선 사이의 성능 차이는 거의 그대로 유지된다. 따라서 더 나은 재현성을 위해 보관합니다. (2) 생성된 명령어들을 주의 깊게 확인함으로써, 생성된 명령어들 중 어느 것도 원래의 검증 명령어들과 중복되지 않음을 주목하고, 따라서 데이터 생성 프로세스 동안 데이터 누설이 발생하지 않는다.

생성된 데이터에 대해 수동 검사를 수행하여 개인 정보나 불쾌한 내용이 생성되지 않도록 합니다.

### Training Details

LLaMA 기반 모델의 경우, 이전 작업 Zhou et al.(2023); Chen et al.(2023)의 명령어 튜닝에 대한 관행을 따른다. Zhou et al.(2023)이 제안한 15개의 에피폭에 대한 목표 모델을 보다 작은 데이터 크기로 미세조정하기 위해 \(\beta_{1}=0.9,\beta_{2}=0.95\) AdamW 최적화기를 사용하였다. 학습이 종료될 때까지 초기 학습률을 \(1\times 10^{-5}\)로 설정하고 선형적으로 쇠퇴를 \(1\times 10^{-6}\)로 설정한다. 우리는 훈련에 8개의 A100 GPU를 사용하기 때문에 GPU 배치 크기당 8개로 설정했는데, 이는 총 배치 크기 64에 해당한다. 최대 토큰 길이는 2048로 설정됩니다.

PaLM 기반 모델의 경우 Google Cloud의 LLM 튜닝 웹 UI에서 기본 명령어 튜닝 설정을 따릅니다. 튜닝 스텝 수를 2000, 학습률 승수를 1로 설정하고 TPU 트레이닝 옵션을 사용한다.

### 세부 비교 결과

우리는 CRR이 다른 방법으로 훈련된 대상 LLM의 능력을 얼마나 충실하게 반영하는지 입증하기 위해 LLaMA 기반 모델을 사용한 Evol-Instruct 벤치마크에 대한 쌍별 비교의 세부 사항을 보여준다. 표 5에서 우리는 넥타이의 수가 결과를 지배하고 승수의 수가 부족하다는 것을 관찰한다. 우리는 목표 모델이 본질적으로 강한 모델에서 지식을 증류하고 있다는 사실에 기인한다. 그 결과 대부분의 경우 LLM 기반 평가자의 렌즈를 통해서만 지도 조정 대상 모델이 강한 모델만큼 잘 대응할 수 있다.

### LLM 기반 평가자 간의 일관성

본 논문에서는 ChatGPT를 LLM 심사위원으로 사용하여 최종 평가, 커뮤니티가 결과를 재현할 수 있는 효율성, 가격 및 접근성을 제공한다. 치앙 등(2023)에서 지적한 바와 같이 LLMs 평가자는 인간의 선호도와 대체로 일치하지만 고유한 편향을 가질 수 있다. 따라서 본 논문의 실험 결과가 확실한지 확인하기 위해 GPT-4를 판정자로 사용하고 서로 다른 기준선과 Self-Instruct 방법 간의 CRR 성능 차이를 비교한다. 표 6의 비교 결과는 두 LLM 기반 판사의 일치를 보여주고 비교 방법에 비해 CodecLM의 우수한 성능을 확인한다.

### 추가 벤치마크 결과

LLM 기반 자동 평가기를 사용하여 성능 결과를 보완하기 위해 섹션 5.4에 제시된 상위 방법으로 조정된 LLM을 표준 NLP 벤치마크인 MMLU Hendrycks 등(2020) 및 BBH Suzgun 등(2022)에 대해 평가한다. LLaMA-7B를 기반으로 대상 모델을 평가하기 위해 Wang et al. (2023)에서 시범 없이 소개된 것과 동일한 설정 또는 CoT Wei et al. (2022) 프롬프트를 따른다. 제안한 방법의 경우 Evol-Instruction 벤치마크 평가와 동일한 설정을 따른다. 평가 결과를 표 4에 제시하고 바닐라 LLaMA-7B의 성능을 기준으로 한다. 우리는 LLM 기반 자동 평가기를 사용하는 표 1과 동일한 모든 방법의 성능 순위를 관찰한다. 서로 다른 두 평가 방법 간의 일관성은 상대적 퍼포러티를 입증하는 측면에서 LLM 기반 평가자의 신뢰성을 나타낸다.

\begin{table}
\begin{tabular}{c||c c c} \hline \hline
**Methods** & **BBH** & **MMLU** & **Average** \\ \hline LLaMA-7B & 30.93 & 35.17 & 33.05 \\ Alpagasus & 31.55 & 36.46 & 34.01 \\ WizardLM+ & 31.72 & 37.89 & 34.81 \\ CodecLM (ours) & **32.60** & **42.67** & **37.64** \\ \hline \hline \end{tabular}
\end{table}
표 4: 표준 벤치마크에 대한 추가 결과.

경쟁하는 방법.

### Case Study

우리는 명령어 메타데이터에서 최종 고품질 프롬프트까지의 반복적인 테일러링 프로세스를 보여주기 위해 그림 6의 사례 연구를 제시한다. 실제로, 반복은 대비 필터링 프로세스에 의해 더 일찍 종료될 수 있다. 본 논문에서는 Self-Rubrics가 주어진 메타데이터에 따라 루브릭과 액션을 맞춤화할 수 있음을 보인다. 흥미롭게도 LLM에 의해 생성된 작용은 매우 도메인 특이적으로 보인다. 예를 들어, 마지막 작업의 _SWOT 분석_ 은 비전문가 인간 주석자가 고안하기 어려울 수도 있습니다. 또한 지시문에 나타난 색채 텍스트는 LLM이 지시문을 정교하게 다듬기 위해 행동을 매우 정확하게 따를 수 있음을 보여준다.

### CodeCLM에 대한 템플릿 프롬프트

우리는 더 나은 재현성을 위해 여기 부록에 모든 신속한 템플릿을 제시한다. 특히, 빠른 참조를 위해 프롬프트 템플릿과 그 사용법 사이의 대응 관계를 다음과 같이 나열한다:

* 그림 7: 사용 사례 및 이전 가능한 기술을 포함하여 지침을 메타데이터로 인코딩합니다.
*도 8: 명령어 메타데이터를 구조가 비교적 간단한 기본 명령어로 디코딩하는 것.
* 그림 9: 명령어가 얼마나 도전적인지 판단하기 위한 루브릭을 생성하고, 주어진 메타데이터를 기반으로 명령어를 개선하기 위한 액션.
* 도 10: 생성된 액션들 중 하나를 추종함으로써 입력 명령어를 개선하는 단계.
* 도 11: 타겟 및 강한 LLM으로부터의 응답 품질을 비교하는 단계. 설명 부분을 제거하여 비쿠나 스타일 쌍별 비교 프롬프트에서 조정되었습니다.
*도 12: 판사로 LLM(_e.g._, ChatGPT, GPT-4)을 이용한 자동 평가. Chang et al. (2023); Chen et al. (2023)

모든 프롬프트는 그림 7의 첫 번째 인코딩 프롬프트를 제외하고 제로 샷이며, 이는 LLM에 작업 및 기술의 대략적인 세분성을 보여주기 위해 소수의 샷 데모를 활용한다. 또한 이러한 프롬프트는 실제로 매우 잘 작동하기 때문에 선택합니다. 그리고 최근의 신속한 최적화 기법인 Fernando et al. (2023); Yang et al. (2023)은 우리의 프레임워크에 원활하게 통합될 수 있으며, 우리는 그것들을 미래의 작업으로 남겨둔다.

\begin{table}
\begin{tabular}{l||c c||c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{2}{c||}{**LLaMA-7B vs. Gemini-Pro**} & \multicolumn{2}{c}{**LLaMA-13B vs. Gemini-Pro**} \\ \cline{2-5}  & **ChatGPT** & **GPT4** & **ChatGPT** & **GPT4** \\ \hline Self-Instruct & 0.00 & 0.00 & 0.00 & 0.00 \\ Alpagasus & +3.21 & +1.38 & +4.13 & +1.83 \\ Tree-Instruct & +3.21 & +2.29 & +6.88 & +4.59 \\ WizardLM & +2.29 & +0.46 & +6.42 & +3.21 \\ WizardLM+ & +3.67 & +2.29 & +8.72 & +5.50 \\ CodeCLM (ours) & **+7.80** & **+8.26** & **+11.01** & **+8.72** \\ \hline \hline \end{tabular}
\end{table}
표 6: ChatGPT 및 GPT4에 의해 각각 평가된 Evol-Instruct 상의 CRR 측면에서 Self-Instruct까지의 성능 차이. 각 방법은 LLaMA-7B 또는 -13B를 기반으로 목표 모델을 학습하고 강력한 모델인 제미니-프로와 비교한다. 우리는 두 개의 LLM 기반 자동 평가자가 일관된 결과를 산출하는 것을 관찰한다.

\begin{table}
\begin{tabular}{l||c c c c||c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{2}{c||}{**LLaMA-7B vs. Gemini-Pro**} & \multicolumn{2}{c}{**LLaMA-13B vs. Gemini-Pro**} \\ \cline{2-9}  & **Wins** & **Ties** & **Losses** & **CRR** & **Wins** & **Ties** & **Losses** & **CRR** \\ \hline Self-Instruct & 17 & 140 & 61 & 72.02 & 29 & 136 & 53 & 75.69 \\ Alpagasus & 17 & 147 & 54 & 75.23 & 26 & 148 & 44 & 79.82 \\ Tree-Instruct & 23 & 141 & 54 & 75.23 & 26 & 154 & 38 & 82.57 \\ WizardLM & 19 & 143 & 56 & 74.31 & 30 & 149 & 39 & 82.11 \\ WizardLM+ & 19 & 146 & 53 & 75.69 & 31 & 153 & 34 & 84.40 \\ CodeCLM (ours) & **29** & **145** & **44** & **79.82** & **35** & **154** & **29** & **86.70** \\ \hline \hline \end{tabular}
\end{table}
표 5: Evol-Instruct 벤치마크에서 LLaMA 기반 모델과의 자세한 비교 결과. 각 방법은 LLaMA-7B 또는 -13B를 기반으로 목표 모델을 학습하고 강력한 모델인 제미니-프로와 비교한다. 용량 회복 비율(%), \(\texttt{CRR}=\frac{\texttt{wins+tiles}}{\texttt{total comparisons}}\).

그림 6: CodecLM의 수업 개선 과정에 대한 사례 연구. 공간을 절약하기 위해 반복적인 지침은 생략됩니다.

나는 네가 명령 분석기 역할을 했으면 좋겠어.

지시가 주어지면, 당신은 그것의 사용 사례와 기술(또는 지식)을 인식해야 한다.

대용량 언어 모델(LLM)이 질문에 답하기 위해 필요합니다.

설명 없이 필요한 사용 사례와 기술을 생성합니다.

최대 3개의 스킬을 나열하여 각 스킬을 이전할 수 있어야 LLM이 해당 스킬을 활용하여 답변할 수 있습니다.

similar questions.

스킬을 설명하기 위해 "스킬", "지식"을 사용하지 않도록 하며, 각 스킬은 간결해야 한다(2-3단어).

아래 예제를 따라 주어진 명령어를 분석합니다.

#Example 1#

스포츠 해설가로서, 챔피언 게임의 마지막 초에서 승리하는 플레이를 묘사하세요.

유스케이스: 창의적 글쓰기

기술: 역할극, 스포츠

#Example 2#

파이썬을 사용하여 대용량 파일(>2T)을 읽는 방법?

작업: 코드 생성

Skills: python

#Example 3#

논문의 메서드 섹션은 너무 짧으며 제안된 모델의 작동 방식을 설명하지 않습니다.

세부적으로. 계층적 인코더 및 계단식 선택기의 아키텍처, 입력, 출력 및 매개 변수와 같은 세부 정보를 어떻게 제공할 수 있습니까?

과제 : 일반지식 질의응답

기술: 학술적 글쓰기, 기계 학습

<input instruction>

<output metadata>

나는 네가 지도 작가로 활동하기를 바란다.

당신의 목적은 합리적이어야 하는 <지침의 수> 지침을 작성하는 것입니다.

그리고 인간에 의해 이해되고 대응되어야 한다.

생성된 명령어는 아래의 제약 조건을 따르면서 충분히 다양해야 한다:

지침의 사용 사례: <사용 사례>

지시에 응답하기 위해 필요한 기술: <기술>

번호가 매겨진 게시점에서 응답하지 않고 지침을 생성합니다.

<output instructions>

저는 당신이 도메인 전문 지식을 가진 교육 심사위원으로 활동하기를 바랍니다.

당신의 일은 <number_of_rubrics> 도메인 특정 루브릭을 생성하여 난이도 및

지도의 사용 사례에 따른 복잡성과 그에 대응하는 데 필요한 기술.

생성된 루브릭은 명확하고 간결하며 명확해야 합니다.

생성된 루브릭들에 기초하여, 대응하는 액션들을 생성하여 그에 의해 명령어를 개선한다.

그것을 더 어렵게 만드는 것.

명령어의 사용 사례: <사용 사례>.

수업을 해결하는 데 필요한 기술: <기술>입니다.

번호가 매겨진 게시점에서 설명 없이 도메인별 루브릭 및 액션을 생성합니다.

<output rubrics>

<output actions>

도 8: 메타데이터로부터 명령어들을 생성하기 위한 프롬프트 템플릿.

도 7: 입력을 메타데이터로 인코딩하기 위한 프롬프트 템플릿, 이는 그것의 사용 사례 및 전이가능한 스킬들로 구성된다.

나는 당신이 도메인 전문 지식을 가진 교육 개선자 역할을 하기를 바랍니다. 당신의 일은 주어진 개선 행동 항목에 따라 주어진 지시를 더 어렵게 만드는 것이며, 생성된 지침은 합리적이고 자기 일치적이어야 한다. 작업에서 단어 또는 구를 직접 복사하지 마십시오.

액션 개선: <액션> 입력 명령어: <입력 명령어>

개선된 명령어: <출력 명령어>

그림 11: 강한 LLM과 대상 LLM의 반응을 비교하기 위해 대조적 필터링에서 사용되는 프롬프트 템플릿. 이 템플릿을 스코어러 \(S\)로 사용하여 강력한 LLM을 직접 사용하여 타사 LLM을 호출하는 추가 비용을 방지합니다.

도 12: 판사로 LLM(_e.g._, ChatGPT, GPT-4)을 사용하여 자동 평가를 위한 프롬프트 템플릿.

그림 10: 생성된 액션에 따라 명령어를 개선하도록 템플릿을 프롬프트합니다.

<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>ChatQA: Building GPT-4 Level Conversational QA Models</title>
<!--Generated on Thu Jan 18 17:40:31 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="https://arxiv.org/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/addons.js"></script>
<script src="https://arxiv.org/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Machine Learning,  ICML" lang="en" name="keywords">
<base href="https://arxiv.org/html/2401.10225v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2401.10225v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2401.10225v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2401.10225v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC mobile collapse" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Conversational QA ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Conversational QA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS2" title="2.2 Retrieval for Multi-Turn QA ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Retrieval for Multi-Turn QA</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS2.SSS1" title="2.2.1 Conversational Query Rewriting ‣ 2.2 Retrieval for Multi-Turn QA ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Conversational Query Rewriting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS2.SSS2" title="2.2.2 Fine-tuning Retriever for multi-turn QA ‣ 2.2 Retrieval for Multi-Turn QA ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Fine-tuning Retriever for multi-turn QA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS3" title="2.3 Instruction Tuning ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Instruction Tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>ChatQA</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Stage-1: Supervised Fine-tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Stage-1: Supervised Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS2" title="3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Stage-2: Context-Enhanced Instruction Tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS1" title="3.2.1 Human Annotated Data ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Human Annotated Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS2" title="3.2.2 Synthetic Data Generation ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Synthetic Data Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS3" title="3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Training Blends</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Retrieval for Multi-Turn QA</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="4.1 Fine-tuning Retriever for Multi-turn QA ‣ 4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Fine-tuning Retriever for Multi-turn QA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="4.2 Conversational Query Rewriting ‣ 4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Conversational Query Rewriting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS3" title="4.3 Comparisons ‣ 4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparisons</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S5" title="5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS1" title="5.1 Baselines ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S5.SS2" title="5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluation Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS2.SSS1" title="5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Long Document Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS2.SSS2" title="5.2.2 Short Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Short Document Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS3" title="5.3 Evaluation Metrics ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S6" title="6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S6.SS1" title="6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Main Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS1" title="6.1.1 overview ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS2" title="6.1.2 Importance of Stage-1 SFT ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Importance of Stage-1 SFT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS3" title="6.1.3 Effectiveness of Single-Turn Data ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Effectiveness of Single-Turn Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS4" title="6.1.4 Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.4 </span>Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS5" title="6.1.5 Human Evaluation ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.5 </span>Human Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS2" title="6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Fine-grained Analyses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS3" title="6.3 Top-k Chunks for Stage-2 Instruction Tuning ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Top-<em class="ltx_emph ltx_font_italic">k</em> Chunks for Stage-2 Instruction Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS4" title="6.4 Ablation Studies for Inference Stage ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Ablation Studies for Inference Stage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S6.SS5" title="6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Evaluation of Unanswerable Case</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS5.SSS1" title="6.5.1 Evaluation Setup ‣ 6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.1 </span>Evaluation Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS5.SSS2" title="6.5.2 Results ‣ 6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS6" title="6.6 Case Study ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Case Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S7" title="7 Conclusion ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A1" title="Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>ChatQA Instruction Tuning</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A1.SS1" title="A.1 Stage-1: Supervised Fine-tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Stage-1: Supervised Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A1.SS2" title="A.2 Stage-2: Context-Enhanced Instruction Tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Stage-2: Context-Enhanced Instruction Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A1.SS3" title="A.3 Prompts for Synthetic Data Generation ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Prompts for Synthetic Data Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A2" title="Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>More Details and Results for Retrieval in Conversational QA</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A2.SS1" title="B.1 Query Rewriting Prompts for GPT-3.5-turbo ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Query Rewriting Prompts for GPT-3.5-turbo</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A2.SS2" title="B.2 More Results for Retrieval in Conversational QA ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>More Results for Retrieval in Conversational QA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A3" title="Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Conversational QA Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#A3.SS1" title="C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Data Statistics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px1" title="Doc2Dial ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">Doc2Dial</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px2" title="QuAC ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">QuAC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px3" title="QReCC ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">QReCC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px4" title="TopiOCQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">TopiOCQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px5" title="INSCIT ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">INSCIT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px6" title="CoQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">CoQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px7" title="DoQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">DoQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px8" title="ConvFinQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">ConvFinQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px9" title="SQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">SQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px10" title="HybridDial ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">HybridDial</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#A3.SS2" title="C.2 Prompts for the Benchmarks ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Prompts for the Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#A3.SS2.SSS1" title="C.2.1 ChatQA ‣ C.2 Prompts for the Benchmarks ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.1 </span>ChatQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#A3.SS2.SSS2" title="C.2.2 Llama2-Chat ‣ C.2 Prompts for the Benchmarks ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.2 </span>Llama2-Chat</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#A3.SS2.SSS3" title="C.2.3 GPT-3.5-turbo &amp; GPT-4 ‣ C.2 Prompts for the Benchmarks ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.3 </span>GPT-3.5-turbo &amp; GPT-4</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A4" title="Appendix D Human Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A5" title="Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Unanswerable Case Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A6" title="Appendix F Case Study ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Case Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A7" title="Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Guidelines for Conversational QA Data Collection</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A7.SS1" title="G.1 What does conversational QA samples look like ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.1 </span>What does conversational QA samples look like</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#A7.SS2" title="G.2 What kinds of multi-turn QA samples we need ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.2 </span>What kinds of multi-turn QA samples we need</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A7.SS2.SSS0.Px1" title="User’s Questions ‣ G.2 What kinds of multi-turn QA samples we need ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">User’s Questions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A7.SS2.SSS0.Px2" title="Agent’s Response ‣ G.2 What kinds of multi-turn QA samples we need ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">Agent’s Response</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A7.SS3" title="G.3 What we need to annotate ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.3 </span>What we need to annotate</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2401.10225v1 [cs.CL] 18 Jan 2024</div></div>
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">ChatQA: Building GPT-4 Level Conversational QA Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zihan Liu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Ping
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rajarshi Roy
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peng Xu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Shoeybi
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bryan Catanzaro
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.id1">본 연구에서는 GPT-4 수준의 정확도를 얻을 수 있는 대화형 질의응답(QA) 모델 패밀리 ChatQA를 소개한다. 구체적으로, 대용량 언어 모델(LLM)의 제로샷 대화 QA 결과를 크게 개선할 수 있는 2단계 명령어 튜닝 방법을 제안한다. 대화형 QA에서 검색을 처리하기 위해 다중 회전 QA 데이터 세트에서 조밀한 검색기를 미세 조정하며, 이는 배치 비용을 크게 줄이면서 최신 쿼리 재작성 모델을 사용하는 것과 유사한 결과를 제공한다. 특히, 우리의 ChatQA-70B는 10개의 대화 QA 데이터 세트(54.14 vs. 53.90)에서 평균 점수 측면에서 GPT-4를 능가할 수 있다. OpenAI GPT 모델의 합성 데이터에 의존하지 않습니다.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">가장 최근에 ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>와 그 후속작들 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib50" title="">2023</a>; Anthropic, <a class="ltx_ref" href="#bib.bib5" title="">2023b</a>; Google, <a class="ltx_ref" href="#bib.bib27" title="">2023</a>)</cite>는 생산 및 연구 커뮤니티에서 빌딩 질의응답(QA) 모델의 패러다임 전환을 이끌었다. 특히, QA 모델의 다음 측면은 실제 애플리케이션에서 선호된다: <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">i. 사용자가 대화 방식으로 QA 모델과 상호 작용할 수 있으므로 후속 질문을 쉽게 제기할 수 있습니다. <em class="ltx_emph ltx_font_italic" id="S1.p1.1.2">ii)</em>  일반 모델들은 미세 조정된 전문가 모델의 정확도와 일치하면서 데이터 세트별 미세 조정 없이 제로 샷 방식으로 답변을 생성할 수 있다. <em class="ltx_emph ltx_font_italic" id="S1.p1.1.3">iii)</em>  QA 모델은 오픈 도메인 또는 긴 문서 설정 모두에서 검색된 증거 청크를 통합할 수 있으며, 여기서 제공된 컨텍스트는 LLM<cite class="ltx_cite ltx_citemacro_citep">(e.g., Anthropic, <a class="ltx_ref" href="#bib.bib4" title="">2023a</a>; Xu et al., <a class="ltx_ref" href="#bib.bib74" title="">2023b</a>)</cite>의 컨텍스트 창보다 훨씬 길다. 이를 위해 이 세 가지 측면을 포괄하는 대화식 QA에 초점을 맞춘다.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">그러나 최신 블랙박스 모델, 즉 GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib50" title="">2023</a>)</cite>의 정확도와 일치할 수 있는 대화형 QA 모델을 구축하는 것은 여전히 연구 커뮤니티에 큰 도전 과제이다. 본 연구에서는 GPT-4 수준의 정확도를 갖는 화이트박스 대화 QA 모델인 ChatQA-70B를 제안 2단계 명령어 튜닝 레시피, 대화 QA에서 RAG(recovery-augmented generation)를 위한 향상된 리트리버, 세심한 데이터 큐레이션 과정을 통해 소개한다.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">구체적으로 다음과 같은 기여를 한다.</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">본 논문에서는 제로 샷 대화형 QA 태스크를 위한 사용자 제공 또는 검색된 컨텍스트를 통합하는 LLM 기능을 크게 향상시킬 수 있는 2단계 명령어 튜닝 방법과 데이터 세트 큐레이션 레시피를 제안한다. 본 논문에서 제안한 방법이 규칙적인 명령어 튜닝이나 RLHF 기반 레시피(예: Llama-2-Chat)를 훨씬 능가함을 보인다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">대화형 QA에서 RAG의 경우, 인간 주석이 달린 다중 회전 QA 데이터 세트에서 미세 조정된 최신 단일 회전 쿼리 검색기가 최신 LLM 기반 쿼리 재작성 모델, 즉 GPT-3.5-turbo <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>를 활용하는 것과 함께 작동한다는 것을 보여준다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Llama2-7B, Llama2-13B, Llama2-70B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib60" title="">2023</a>)</cite> 및 사내 8B 사전 훈련 GPT를 기반으로 ChatQA 모델 패밀리를 구축한다. 검색이 필요한 긴 문서가 포함된 5개의 데이터셋과 테이블이 포함된 3개의 데이터셋을 포함하여 10개의 대화형 질의응답 데이터셋에 대한 종합적인 연구를 수행한다. 평균 점수로 볼 때, ChatQA-70B 모델(54.14)은 ChatGPT 모델의 합성 데이터를 활용하지 않고도 GPT-3.5-터보 모델(50.37)과 GPT-4 모델(53.90)보다 우수한 성능을 보였다.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">우리는 원하는 답이 제공되거나 검색된 컨텍스트에 포함되지 않으므로 LLM이 쉽게 환각을 볼 수 있는 "응답할 수 없는" 시나리오를 연구한다. 우리는 명령어 튜닝에서 소량의 "응답할 수 없는" 샘플을 추가하는 것이 필요할 때 "응답할 수 없는" 출력을 생성하도록 모델을 조종하여 환각을 크게 줄일 수 있음을 보여준다. 우리의 ChatQA-70B는 이와 관련하여 GPT-3.5-터보를 능가하지만 GPT-4(약 3.5%)에 비해 약간의 격차가 있다.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">나머지 논문을 정리하면 다음과 같다. 우리는 §<a class="ltx_ref" href="#S2" title="2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서 관련 작업을 논의한다. 본 논문에서는 §<a class="ltx_ref" href="#S3" title="3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">3</span></a>의 ChatQA와 §<a class="ltx_ref" href="#S4" title="4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4</span></a>의 회화 QA의 2단계 명령어 튜닝 방법을 소개한다. 우리는 §<a class="ltx_ref" href="#S5" title="5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">5</span></a>의 실험 설정을 §<a class="ltx_ref" href="#S6" title="6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6</span></a>의 결과로 제시하고 §<a class="ltx_ref" href="#S7" title="7 Conclusion ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">7</span></a>의 논문으로 마무리한다.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="148" id="S1.F1.g1" src="https://arxiv.org/html/2401.10225v1/x1.png" width="706">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1:</span>Two-stage instruction tuning framework for ChatQA.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Conversational QA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">대화 방식의 질의 응답은 후속 질문을 해결함으로써 사용자 경험을 자연스럽게 향상시킨다. 이 모델은 또한 필요한 경우 사용자에게 명확화 질문을 제기할 수 있으며, 이는 환각을 줄일 수 있다. 따라서 프로덕션 <cite class="ltx_cite ltx_citemacro_citep">(e.g. OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>; Google, <a class="ltx_ref" href="#bib.bib27" title="">2023</a>; Anthropic, <a class="ltx_ref" href="#bib.bib5" title="">2023b</a>)</cite>에서 QA 모델을 배포하는 기본 형식이 됩니다.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">최근 몇 년 동안 많은 대화형 QA 데이터 세트가 소개되었으며, 여기서 모델은 제공된 컨텍스트 또는 문서를 기반으로 질문에 답하도록 요청된다. 제공된 컨텍스트 또는 문서는 다음과 같을 수 있습니다. <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">i</em>) text-only from various domains <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>; Anantha et al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>; Saeidi et al., <a class="ltx_ref" href="#bib.bib58" title="">2018</a>; Adlakha et al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>; Aliannejadi et al., <a class="ltx_ref" href="#bib.bib2" title="">2021</a>; Reddy et al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>; Qu et al., <a class="ltx_ref" href="#bib.bib53" title="">2020</a>; Wu et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>; Deng et al., <a class="ltx_ref" href="#bib.bib18" title="">2022</a>; Guo et al., <a class="ltx_ref" href="#bib.bib28" title="">2021</a>; Choi et al., <a class="ltx_ref" href="#bib.bib10" title="">2018</a>; Campos et al., <a class="ltx_ref" href="#bib.bib7" title="">2020</a>)</cite>, 또는 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.2">ii</em>) plain text with tables <cite class="ltx_cite ltx_citemacro_citep">(Pasupat &amp; Liang, <a class="ltx_ref" href="#bib.bib52" title="">2015</a>; Nakamura et al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>; Chen et al., <a class="ltx_ref" href="#bib.bib8" title="">2022a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">최신 LLM 기반 일반주의 솔루션 <cite class="ltx_cite ltx_citemacro_citep">(e.g., OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>와 달리 대부분의 이전 연구에서는 특정 도메인 또는 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>; Izacard &amp; Grave, <a class="ltx_ref" href="#bib.bib32" title="">2021</a>; Chen et al., <a class="ltx_ref" href="#bib.bib8" title="">2022a</a>; Gao et al., <a class="ltx_ref" href="#bib.bib25" title="">2022</a>; Nakamura et al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>; Adlakha et al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>; Wu et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>에 대한 미세 조정 전문가 모델에 초점을 맞추고 있다.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Retrieval for Multi-Turn QA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">대화형 QA는 오픈 도메인 환경에서 검색 강화 생성(RAG)을 포함하거나, 제공된 문서가 LLM의 컨텍스트 창보다 길 때 수행된다. 밀집 검색기는 일반적으로 단일 질문이 주어진 top-<em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">k</em> 관련 청크를 검색하도록 훈련됩니다. 대화 QA에서 후속 질문(예: 이전 대화에서 언급된 개체를 참조하는 대명사와 함께)은 검색에 대한 정보가 충분하지 않을 수 있지만 대화 이력과 함께 공급하면 중복되어 차선 결과를 초래할 수 있다.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Conversational Query Rewriting</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">이전 솔루션의 대부분은 쿼리 다시 쓰기 방법입니다. 최근 질문의 전환은 이전의 대화 히스토리 <cite class="ltx_cite ltx_citemacro_citep">(Vakulenko et al., <a class="ltx_ref" href="#bib.bib62" title="">2021a</a>; Ye et al., <a class="ltx_ref" href="#bib.bib75" title="">2023</a>; Mo et al., <a class="ltx_ref" href="#bib.bib45" title="">2023</a>)</cite>로부터 추가적인 정보 없이 독립형 질의로 재작성되므로 관련 컨텍스트 <cite class="ltx_cite ltx_citemacro_citep">(Vakulenko et al., <a class="ltx_ref" href="#bib.bib63" title="">2021b</a>; Mele et al., <a class="ltx_ref" href="#bib.bib43" title="">2021</a>; Raposo et al., <a class="ltx_ref" href="#bib.bib56" title="">2022</a>; Mo et al., <a class="ltx_ref" href="#bib.bib45" title="">2023</a>)</cite>를 검색하기 위해 검색 모델에 의해 직접 사용될 수 있다. 많은 데이터 세트가 이 연구 라인 <cite class="ltx_cite ltx_citemacro_citep">(Elgohary et al., <a class="ltx_ref" href="#bib.bib21" title="">2019</a>; Chu et al., <a class="ltx_ref" href="#bib.bib11" title="">2020</a>; Qu et al., <a class="ltx_ref" href="#bib.bib53" title="">2020</a>; Anantha et al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>; Brabant et al., <a class="ltx_ref" href="#bib.bib6" title="">2022</a>)</cite>와 함께 여러 제안된 쿼리 재작성 방법 <cite class="ltx_cite ltx_citemacro_citep">(Ishii et al., <a class="ltx_ref" href="#bib.bib30" title="">2022</a>; Yu et al., <a class="ltx_ref" href="#bib.bib76" title="">2020</a>; Wu et al., <a class="ltx_ref" href="#bib.bib71" title="">2022</a>; Del Tredici et al., <a class="ltx_ref" href="#bib.bib17" title="">2021</a>; Chen et al., <a class="ltx_ref" href="#bib.bib9" title="">2022b</a>; Galimzhanova et al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>를 용이하게 하기 위해 수집되었다. 예를 들어, 쿼리 재쓰기를 위한 강화 학습 방법들을 사용하기 위해 제안된 <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="#bib.bib71" title="">2022</a>)</cite>와 <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="#bib.bib9" title="">2022b</a>)</cite>이다. <cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a class="ltx_ref" href="#bib.bib76" title="">2020</a>)</cite>는 쿼리 재쓰기를 위해 GPT-2와 같은 소수의 샷 생성 모델을 조사했다. <cite class="ltx_cite ltx_citemacro_citet">Galimzhanova et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>는 명령어 튜닝된 GPT-3.5-turbo를 연구하여 대화 질의 재쓰기를 위한 최신 결과를 달성했음을 보여주었다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Fine-tuning Retriever for multi-turn QA</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">일부 이전 연구에서는 도메인 내 대화 쿼리 및 컨텍스트 쌍 <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>; Gao et al., <a class="ltx_ref" href="#bib.bib25" title="">2022</a>; Adlakha et al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>; Wu et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>에서 단일 회전 쿼리 검색기를 미세 조정하므로 대화 이력과 현재 쿼리의 연결을 입력으로 직접 사용할 수 있습니다. 이 작업에서는 제로샷 평가에 초점을 맞춘다. 고품질 다중 회전 데이터 세트에서 단일 회전 쿼리 검색기를 미세 조정합니다. 그런 다음, 5개의 벤치마크 데이터 세트에 대해 미세 조정된 리트리버의 제로 샷 능력을 평가한다. 놀랍게도, 우리는 이 간단한 접근법이 최신 쿼리 재작성 모델, 즉 GPT-3.5-터보와 유사한 제로 샷 결과를 얻을 수 있음을 발견했다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Instruction Tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">명령어 튜닝의 목표는 LLMs에 자연 언어 명령어 <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib69" title="">2022a</a>; Sanh et al., <a class="ltx_ref" href="#bib.bib59" title="">2022</a>; Mishra et al., <a class="ltx_ref" href="#bib.bib44" title="">2022</a>; Iyer et al., <a class="ltx_ref" href="#bib.bib31" title="">2022</a>; Du et al., <a class="ltx_ref" href="#bib.bib19" title="">2022</a>; Ouyang et al., <a class="ltx_ref" href="#bib.bib51" title="">2022</a>; Wang et al., <a class="ltx_ref" href="#bib.bib68" title="">2023b</a>; Zhang et al., <a class="ltx_ref" href="#bib.bib77" title="">2023</a>; Gao et al., <a class="ltx_ref" href="#bib.bib26" title="">2023</a>; Chung et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>; Muennighoff et al., <a class="ltx_ref" href="#bib.bib46" title="">2022</a>; Xu et al., <a class="ltx_ref" href="#bib.bib73" title="">2023a</a>; Wang et al., <a class="ltx_ref" href="#bib.bib67" title="">2022c</a>; Zhou et al., <a class="ltx_ref" href="#bib.bib78" title="">2023</a>)</cite>를 따를 수 있는 능력을 갖추는 것이다. FLAN <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>)</cite>, Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib66" title="">2022b</a>)</cite>, 부자연스러운 Instructions <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al., <a class="ltx_ref" href="#bib.bib29" title="">2022</a>)</cite>, Dolly <cite class="ltx_cite ltx_citemacro_citep">(Conover et al., <a class="ltx_ref" href="#bib.bib14" title="">2023b</a>)</cite>, OpenAssistant <cite class="ltx_cite ltx_citemacro_citep">(Köpf et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite> 등의 고품질 명령어 튜닝 데이터셋 개발이 급증하고 있다.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">수업 튜닝에 대한 많은 연구가 수행되었지만, QA를 위한 RAG 또는 상황 인식 생성을 개선하는 데 초점을 맞춘 연구는 몇 가지이다. <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="#bib.bib40" title="">2023b</a>)</cite>는 top-<em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.1">k</em> retrieved chunks for LLM fine-tuning을 추가한 검색 강화 명령어 튜닝 방법을 도입했다. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib64" title="">2023a</a>)</cite>는 검색 강화 사전 훈련 후 명령어 튜닝을 적용했습니다. 이에 반해 본 논문에서는 리트리벌(retrival) 또는 제공된 컨텍스트(context)를 갖는 생성을 개선하기 위한 2단계 명령어 튜닝 방법을 제안한다. 우리는 top-<em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.2">k</em> retrieved chunks for LLM fine-tuning does not help for a wide range of conversation QA tasks (see §<a class="ltx_ref" href="#S6.SS3" title="6.3 Top-k Chunks for Stage-2 Instruction Tuning ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6.3</span></a> for details)</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">ChatGPT<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>의 출시 이후, 명령어 튜닝은 광범위한 작업에 놀라운 제로 샷 기능을 갖는 최첨단 대화 에이전트를 구축하기 위한 필수 요소가 된다. 대화형 QA 능력은 대화 에이전트에서 중요한 역할을 하지만 이러한 중요한 측면에 초점을 맞춘 연구는 제한적이다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ChatQA</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">본 절에서는 ChatQA를 위한 2단계 명령어 튜닝 방법을 제안한다. 도면은 <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">1</span></a>를 참조한다. 우리의 방법은 사전 학습된 LLM 기초 모델로 시작한다. 단계-1에서는 명령 후속 및 대화 데이터 세트의 혼합에 <cite class="ltx_cite ltx_citemacro_citet">Ouyang et al. (<a class="ltx_ref" href="#bib.bib51" title="">2022</a>)</cite>에서와 같이 감독 미세 조정(SFT)을 적용한다. 그 후, 저희 모델은 대화 에이전트로서 지시를 따를 수 있는 좋은 능력을 보여줍니다. 그러나 상황화 또는 RAG 기반 QA에 대한 기능은 여전히 제한적이다. 따라서 본 논문에서는 상황인식이나 검색이 강화된 대화형 질의응답 생성 모델의 성능을 향상시키기 위한 상황강화 명령어 튜닝(context-enhanced instruction tuning)을 제안한다.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Stage-1: Supervised Fine-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">SFT(supervised fine-tuning) 데이터셋을 구성하기 위해 [cite idx=0></cite>]를 따라 고품질 명령어 튜닝 데이터셋에서 128K개의 SFT 샘플로 구성된 결합 집합을 수집한다. 1) 소셜 대화 데이터셋 Soda <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a class="ltx_ref" href="#bib.bib34" title="">2022</a>)</cite>, 2) 정교한 답변을 포함하는 긴 형식의 QA 데이터셋 ELI5 <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="#bib.bib22" title="">2019</a>)</cite>, 3) FLAN과 연쇄 사상 데이터셋 <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib70" title="">2022b</a>; Chung et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>; Longpre et al., <a class="ltx_ref" href="#bib.bib42" title="">2023</a>)</cite>, 4) LLM 합성 명령어 튜닝 데이터셋: Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib66" title="">2022b</a>)</cite>와 Unnatural Instructions <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al., <a class="ltx_ref" href="#bib.bib29" title="">2022</a>)</cite>, 5) 공개 인간 작성 대화 데이터 세트인 OpenAssistant <cite class="ltx_cite ltx_citemacro_citep">(Köpf et al., <a class="ltx_ref" href="#bib.bib36" title="">2023</a>)</cite>, Dolly <cite class="ltx_cite ltx_citemacro_citep">(Conover et al., <a class="ltx_ref" href="#bib.bib13" title="">2023a</a>)</cite>와 함께 개인 크라우드 소스 대화 데이터 세트입니다.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">우리는 모든 SFT 데이터의 구조를 대화 형식으로 통일한다. 우리는 먼저 "시스템" 역할을 추가하여 LLM을 안내하는 일반적인 지침을 설정하여 예의 바르고 도움이 되는 답변을 제공한다. 또한 명령어 튜닝 데이터 세트에서 명령어와 응답 쌍을 통합하기 위해 "사용자" 및 "보조" 역할을 추가한다. 우리는 LLM 기초 모델에 이 통합된 형식을 사용하여 미세 조정을 적용한다.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Stage-2: Context-Enhanced Instruction Tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">주어진 컨텍스트에 대한 모델의 대화 QA 기능을 더욱 향상시키기 위해, 컨텍스트화된 QA 데이터 세트를 명령어 튜닝 블렌드에 통합하는 2단계 명령어 튜닝을 수행한다. 구체적으로, 단계-2 명령어 튜닝 데이터 세트는 문맥화된 단일 턴 QA와 대화형 QA 데이터 세트의 혼합으로 구성된다. 아래 2단계 명령어 튜닝 데이터 세트에 대한 추가 세부 정보를 제시한다.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Human Annotated Data</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">공개 가능한 데이터 세트 외에도 단계-2의 핵심 요소 중 하나는 고품질 문서 기반 대화 QA 데이터 세트를 얻는 것이다. 우리는 7k개의 대화로 구성된 인간 주석 대화 QA(HumanAnnotatedConvQA) 데이터 세트를 생성한다. 이 데이터 세트를 구축하기 위해 먼저 인터넷에서 다양한 주제를 다루는 7k개의 문서를 수집했다. 그런 다음 주석자에게 문서에 대한 질문(및 후속 질문)을 묻는 호기심 많은 사용자 및 답변을 제공하는 에이전트 역할을 모두 수행하도록 지시한다. 각 문서에 대해 멀티 턴 대화를 생성하여 대화당 평균 5번의 사용자-에이전트 턴을 갖는 총 7k개의 대화 QA 대화를 생성한다. 데이터 수집 가이드라인의 자세한 내용은 부록 <a class="ltx_ref" href="#A7" title="Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">G</span></a>에서 확인할 수 있다.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">또한, 응답할 수 없는 경우에 환각 답변을 줄이기 위해 주어진 컨텍스트 내에서 답변을 찾을 수 없을 때 모델을 명시적으로 표시할 수 있도록 권한을 부여하는 것을 목표로 한다. 이러한 응답할 수 없는 데이터 샘플을 얻기 위해 주석자에게 모든 컨텍스트 위치를 사용자 질문에 제공하도록 요청했다. 따라서 컨텍스트의 해당 위치에서 텍스트를 삭제하여 응답할 수 없는 시나리오를 구성할 수 있었다. 질문에 관련 텍스트를 삭제한 후 "<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p2.1.1">Sorry. I cannot find the answer based on the context</span> 답변할 수 없는 질문에 대한 답변입니다. 마지막으로, 답변이 불가능한 경우와 답변이 불가능한 경우의 좋은 트레이드오프를 제공하는 또 다른 1.5k 사용자 에이전트 턴을 구성한다(자세한 내용은 §<a class="ltx_ref" href="#S6.SS5" title="6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6.5</span></a> 참조).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Synthetic Data Generation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">HumanAnnotatedConvQA의 품질을 검증하기 위해 GPT-3.5-turbo를 활용하여 강력한 명령어 후속 및 텍스트 생성 기능을 제공하는 합성 대화 QA 데이터 세트를 생성한다. 대화형 QA를 위한 대규모 합성 데이터도 <cite class="ltx_cite ltx_citemacro_citet">Dai et al. (<a class="ltx_ref" href="#bib.bib15" title="">2022</a>)</cite>에서 탐구되었다. 본 연구에서는 LLM 미세 조정을 위한 중간 크기의 고품질 합성 데이터에 초점을 맞춘다.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">GPT-3.5-turbo에 대한 지침은 1) 도움이 되는 답변을 제공하도록 모델을 안내하는 시스템 역할, 2) 필요한 데이터 유형을 나타내는 대화형 QA의 예, 3) 콘텐츠를 기반으로 대화형 QA를 생성하도록 모델을 지시하는 문서의 세 부분으로 구성된다. 우리는 광범위한 도메인을 포함하는 일반적인 크롤링으로부터 7k개의 문서(문서당 평균 <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.1.m1.1"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mo id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p2.1.m1.1d">∼</annotation></semantics></math>1k 단어)를 수집한다. 각 문서는 단일 대화 QA 샘플 생성에 사용되며, 이는 대화당 평균 4.4 사용자-에이전트 회전(SyntheticConvQA라고 함)으로 총 7k 개의 다중 회전 QA 대화로 이어진다.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">HumanAnnotatedConvQA와 유사하게 이 합성 데이터 세트에서 응답할 수 없는 주석으로 또 다른 1.5k 사용자 에이전트 회전을 구성합니다. 에이전트의 답변에 대한 컨텍스트 위치의 주석이 없기 때문에 SyntheticConvQA에서 응답할 수 없는 합성 샘플을 구성합니다. 구체적으로, 우리는 먼저 (각 대화의) 문서를 서로 다른 청크로 절단한다. 그런 다음 제거하려는 에이전트의 답변과 "높게 겹치는" 청크가 있고 나머지 청크는 에이전트의 답변과 "낮게 겹치는" 청크가 있는 경우에만 유효한 응답할 수 없는 샘플로 간주한다. 각 청크와 에이전트의 답변 사이의 4-그램 리콜 점수(답안의 4-그램 구가 각 청크 내에 있는 비율을 측정함)를 메트릭으로 사용하여 중복을 측정하고 0.5보다 높은 것을 “높은 중복”으로, 0.1보다 낮은 것을 “낮은 중복”으로 간주한다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Training Blends</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">이 부분에서는 2단계 명령어 튜닝을 위한 훈련 블렌드의 세부 사항을 소개한다. 표 문서 처리 및 연산 계산에서 QA 성능을 향상시키기 위해 두 요소를 모두 포함하는 TAT-QA 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="#bib.bib79" title="">2021</a>)</cite>를 추가한다. 또한, 컨텍스트화된 단일 턴 QA 데이터 세트를 통합하여 모델의 QA 기능을 더욱 강화한다. 또한 모델의 명령어 수행 능력을 유지하기 위해 훈련 블렌드에 1단계 SFT 데이터 세트를 여전히 유지한다.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">마지막으로, 단계-2에 대한 트레이닝 블렌드는 1) A 대화 QA 데이터세트: HumanAnnotatedConvQA 또는 SyntheticConvQA, <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Unless specified otherwise, the experiments use HumanAnnotatedConvQA as the default setting.</span></span></span> 2) 싱글 턴 QA 데이터세트: DROP<cite class="ltx_cite ltx_citemacro_citep">(Dua et al., <a class="ltx_ref" href="#bib.bib20" title="">2019</a>)</cite>, NarrativeQA<cite class="ltx_cite ltx_citemacro_citep">(Kočiskỳ et al., <a class="ltx_ref" href="#bib.bib35" title="">2018</a>)</cite>, Quoref<cite class="ltx_cite ltx_citemacro_citep">(Dasigi et al., <a class="ltx_ref" href="#bib.bib16" title="">2019</a>)</cite>, ROPES<cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib38" title="">2019</a>)</cite>, SQuAD1.1<cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al., <a class="ltx_ref" href="#bib.bib54" title="">2016</a>)</cite>, SQuAD2.0<cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al., <a class="ltx_ref" href="#bib.bib55" title="">2018</a>)</cite>, NewsQA<cite class="ltx_cite ltx_citemacro_citep">(Trischler et al., <a class="ltx_ref" href="#bib.bib61" title="">2017</a>)</cite>, TAT-QA<cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="#bib.bib79" title="">2021</a>)</cite>, 3) 단계-1의 모든 SFT 데이터세트로 구성된다.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">우리는 모든 단일 전환 QA 및 대화 QA 데이터 세트를 통합하기 위해 단계-1에서와 유사한 템플릿을 따른다. 1) 시스템 역할에 따라 단일 회전 질문 또는 다중 회전 대화의 관련 컨텍스트를 추가하고, 2) 서로 다른 QA 데이터 세트(예: 짧은 답변, 긴 답변, 산술 계산)의 답변 유형을 기반으로 단일 회전 질문 또는 다중 회전 대화 직전에 추가 지침을 통합한다. 우리는 단계-1의 SFT 데이터 세트에 대한 형식을 사용합니다. <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Details of the templates for both stage-1 and stage-2 instruction tuning as well as the synthetic data generation can be found in the Appendix <a class="ltx_ref" href="#A1" title="Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span></p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.9" style="width:566.2pt;height:199pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S3.T1.9.9"><span class="ltx_text" id="S3.T1.9.9.9"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.9.9.9.9"> <span class="ltx_tbody"> <span class="ltx_tr" id="S3.T1.9.9.9.9.10.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S3.T1.9.9.9.9.10.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.1.1" style="color:#000000;">Models</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.2.1" style="color:#000000;">Average</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.3.1" style="color:#000000;">Doc2Dial</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.4.1" style="color:#000000;">QuAC</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.5.1" style="color:#000000;">QReCC</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.6.1" style="color:#000000;">TopiOCQA</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.7.1" style="color:#000000;">INSCIT</span></span></span> <span class="ltx_tr" id="S3.T1.9.9.9.9.11.2"> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.1.1" style="color:#000000;">top-1</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.2.1" style="color:#000000;">top-5</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.3.1" style="color:#000000;">top-1</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.4.1" style="color:#000000;">top-5</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.5.1" style="color:#000000;">top-1</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.6.1" style="color:#000000;">top-5</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.7.1" style="color:#000000;">top-1</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.8.1" style="color:#000000;">top-5</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.9.1" style="color:#000000;">top-5*</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.10.1" style="color:#000000;">top-20*</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.11.1" style="color:#000000;">top-5*</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.12.1" style="color:#000000;">top-20*</span></span></span> <span class="ltx_tr" id="S3.T1.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><cite class="ltx_cite ltx_citemacro_citet">Adlakha et&nbsp;al. <span class="ltx_text" id="S3.T1.1.1.1.1.1.2.1.1.1.1" style="color:#000000;">(</span><a class="ltx_ref" href="#bib.bib1" title="">2022</a><span class="ltx_text" id="S3.T1.1.1.1.1.1.2.2.2.2.1" style="color:#000000;">)</span></cite></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.3.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.4.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.5.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.6.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.7.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.8.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.9.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.10.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.11.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.1.1" style="color:#000000;">70.40</span><math alttext="{}^{\triangle}" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.1.1.m1.1a"><msup id="S3.T1.1.1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.1.1.1.m1.1.1a" xref="S3.T1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mi id="S3.T1.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S3.T1.1.1.1.1.1.1.m1.1.1.1.cmml">△</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1"><ci id="S3.T1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1.1">△</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.1.m1.1c">{}^{\triangle}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT △ end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.12.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.13.1" style="color:#000000;">-</span></span></span> <span class="ltx_tr" id="S3.T1.2.2.2.2.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.2.2.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><cite class="ltx_cite ltx_citemacro_citet">Wu et&nbsp;al. <span class="ltx_text" id="S3.T1.2.2.2.2.2.2.1.1.1.1" style="color:#000000;">(</span><a class="ltx_ref" href="#bib.bib72" title="">2023</a><span class="ltx_text" id="S3.T1.2.2.2.2.2.2.2.2.2.1" style="color:#000000;">)</span></cite></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.3.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.4.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.5.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.6.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.7.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.8.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.9.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.10.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.11.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.12.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.13.1" style="color:#000000;">-</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.1.1" style="color:#000000;">71.10</span><math alttext="{}^{\triangle}" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.2.1.m1.1"><semantics id="S3.T1.2.2.2.2.2.1.m1.1a"><msup id="S3.T1.2.2.2.2.2.1.m1.1.1" xref="S3.T1.2.2.2.2.2.1.m1.1.1.cmml"><mi id="S3.T1.2.2.2.2.2.1.m1.1.1a" xref="S3.T1.2.2.2.2.2.1.m1.1.1.cmml"></mi><mi id="S3.T1.2.2.2.2.2.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S3.T1.2.2.2.2.2.1.m1.1.1.1.cmml">△</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.2.1.m1.1b"><apply id="S3.T1.2.2.2.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.2.2.2.1.m1.1.1"><ci id="S3.T1.2.2.2.2.2.1.m1.1.1.1.cmml" xref="S3.T1.2.2.2.2.2.1.m1.1.1.1">△</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.2.1.m1.1c">{}^{\triangle}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT △ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span> <span class="ltx_tr" id="S3.T1.3.3.3.3.3"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.3.3.3.3.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.1.1" style="color:#000000;">E5-unsupervised</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.3.1.m1.1"><semantics id="S3.T1.3.3.3.3.3.1.m1.1a"><msup id="S3.T1.3.3.3.3.3.1.m1.1.1" xref="S3.T1.3.3.3.3.3.1.m1.1.1.cmml"><mi id="S3.T1.3.3.3.3.3.1.m1.1.1a" xref="S3.T1.3.3.3.3.3.1.m1.1.1.cmml"></mi><mo id="S3.T1.3.3.3.3.3.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.3.3.3.3.3.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.3.1.m1.1b"><apply id="S3.T1.3.3.3.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.3.3.3.1.m1.1.1"><ci id="S3.T1.3.3.3.3.3.1.m1.1.1.1.cmml" xref="S3.T1.3.3.3.3.3.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.3.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.T1.3.3.3.3.3.1.2" style="color:#000000;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.3.3.3.3.3.1.3.1" style="color:#000000;">(</span>Wang et&nbsp;al.<span class="ltx_text" id="S3.T1.3.3.3.3.3.1.4.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="#bib.bib65" title="">2022a</a><span class="ltx_text" id="S3.T1.3.3.3.3.3.1.5.3" style="color:#000000;">)</span></cite></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.2.1" style="color:#000000;">31.56</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.3.1" style="color:#000000;">59.22</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.4.1" style="color:#000000;">23.02</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.5.1" style="color:#000000;">55.33</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.6.1" style="color:#000000;">43.49</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.7.1" style="color:#000000;">77.68</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.8.1" style="color:#000000;">44.71</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.9.1" style="color:#000000;">84.99</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.10.1" style="color:#000000;">26.25</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.11.1" style="color:#000000;">37.67</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.12.1" style="color:#000000;">20.32</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.13.1" style="color:#000000;">40.44</span></span></span> <span class="ltx_tr" id="S3.T1.4.4.4.4.4"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.4.4.4.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.1.1" style="color:#000000;">E5-unsupervised + Rewrite</span><math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.4.4.4.4.4.1.m1.1"><semantics id="S3.T1.4.4.4.4.4.1.m1.1a"><msup id="S3.T1.4.4.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.4.4.1.m1.1.1.cmml"><mi id="S3.T1.4.4.4.4.4.1.m1.1.1a" xref="S3.T1.4.4.4.4.4.1.m1.1.1.cmml"></mi><mo id="S3.T1.4.4.4.4.4.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.4.4.4.4.4.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.4.1.m1.1b"><apply id="S3.T1.4.4.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.4.4.1.m1.1.1"><ci id="S3.T1.4.4.4.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.4.4.4.1.m1.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.4.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.2.1" style="color:#000000;">33.23</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.3.1" style="color:#000000;">61.02</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.4.1" style="color:#000000;">25.56</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.5.1" style="color:#000000;">58.00</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.6.1" style="color:#000000;">46.00</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.7.1" style="color:#000000;">80.01</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.8.1" style="color:#000000;">45.50</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.9.1" style="color:#000000;">85.89</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.10.1" style="color:#000000;">27.58</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.11.1" style="color:#000000;">39.15</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.12.1" style="color:#000000;">21.53</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.13.1" style="color:#000000;">42.04</span></span></span> <span class="ltx_tr" id="S3.T1.5.5.5.5.5"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.5.5.5.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.1.1" style="color:#000000;">E5-unsupervised + Fine-tune</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.5.5.5.5.5.1.m1.1"><semantics id="S3.T1.5.5.5.5.5.1.m1.1a"><msup id="S3.T1.5.5.5.5.5.1.m1.1.1" xref="S3.T1.5.5.5.5.5.1.m1.1.1.cmml"><mi id="S3.T1.5.5.5.5.5.1.m1.1.1a" xref="S3.T1.5.5.5.5.5.1.m1.1.1.cmml"></mi><mo id="S3.T1.5.5.5.5.5.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.5.5.5.5.5.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.5.1.m1.1b"><apply id="S3.T1.5.5.5.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.5.5.5.1.m1.1.1"><ci id="S3.T1.5.5.5.5.5.1.m1.1.1.1.cmml" xref="S3.T1.5.5.5.5.5.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.5.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.5.5.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.5.5.5.5.2.1" style="color:#000000;">47.79</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.5.5.5.5.3.1" style="color:#000000;">75.00</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.4.1" style="color:#000000;">45.28</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.5.1" style="color:#000000;">80.96</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.6.1" style="color:#000000;">46.52</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.7.1" style="color:#000000;">80.74</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.8.1" style="color:#000000;">53.37</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.9.1" style="color:#000000;">89.91</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.10.1" style="color:#000000;">41.01</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.11.1" style="color:#000000;">51.07</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.12.1" style="color:#000000;">52.79</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.13.1" style="color:#000000;">72.31</span></span></span> <span class="ltx_tr" id="S3.T1.6.6.6.6.6"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.6.6.6.6.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.1.1" style="color:#000000;">Dragon</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.6.6.6.6.6.1.m1.1"><semantics id="S3.T1.6.6.6.6.6.1.m1.1a"><msup id="S3.T1.6.6.6.6.6.1.m1.1.1" xref="S3.T1.6.6.6.6.6.1.m1.1.1.cmml"><mi id="S3.T1.6.6.6.6.6.1.m1.1.1a" xref="S3.T1.6.6.6.6.6.1.m1.1.1.cmml"></mi><mo id="S3.T1.6.6.6.6.6.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.6.6.6.6.6.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.6.1.m1.1b"><apply id="S3.T1.6.6.6.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.6.6.6.1.m1.1.1"><ci id="S3.T1.6.6.6.6.6.1.m1.1.1.1.cmml" xref="S3.T1.6.6.6.6.6.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.6.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.6.6.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.T1.6.6.6.6.6.1.2" style="color:#000000;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.6.6.6.6.6.1.3.1" style="color:#000000;">(</span>Lin et&nbsp;al.<span class="ltx_text" id="S3.T1.6.6.6.6.6.1.4.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="#bib.bib39" title="">2023a</a><span class="ltx_text" id="S3.T1.6.6.6.6.6.1.5.3" style="color:#000000;">)</span></cite></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.2.1" style="color:#000000;">46.29</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.3.1" style="color:#000000;">73.09</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.4.1" style="color:#000000;">43.33</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.5.1" style="color:#000000;">75.61</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.6.1" style="color:#000000;">56.80</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.7.1" style="color:#000000;">82.86</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.8.1" style="color:#000000;">46.17</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.9.1" style="color:#000000;">81.96</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.10.1" style="color:#000000;">57.68</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.11.1" style="color:#000000;">78.80</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.12.1" style="color:#000000;">27.49</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.13.1" style="color:#000000;">46.22</span></span></span> <span class="ltx_tr" id="S3.T1.7.7.7.7.7"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.7.7.7.7.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.1.1" style="color:#000000;">Dragon + Rewrite</span><math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.7.7.7.7.7.1.m1.1"><semantics id="S3.T1.7.7.7.7.7.1.m1.1a"><msup id="S3.T1.7.7.7.7.7.1.m1.1.1" xref="S3.T1.7.7.7.7.7.1.m1.1.1.cmml"><mi id="S3.T1.7.7.7.7.7.1.m1.1.1a" xref="S3.T1.7.7.7.7.7.1.m1.1.1.cmml"></mi><mo id="S3.T1.7.7.7.7.7.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.7.7.7.7.7.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.7.7.1.m1.1b"><apply id="S3.T1.7.7.7.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.7.7.7.1.m1.1.1"><ci id="S3.T1.7.7.7.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.7.7.7.1.m1.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.7.7.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.7.7.7.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.7.7.7.7.7.2.1" style="color:#000000;">54.46</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.3.1" style="color:#000000;">80.13</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.4.1" style="color:#000000;">47.60</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.5.1" style="color:#000000;">80.60</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.6.1" style="color:#000000;">47.10</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.7.1" style="color:#000000;">77.15</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.8.1" style="color:#000000;">51.73</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.9.1" style="color:#000000;">85.78</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.10.1" style="color:#000000;">73.07</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.11.1" style="color:#000000;">88.19</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.12.1" style="color:#000000;">52.79</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.13.1" style="color:#000000;">68.92</span></span></span> <span class="ltx_tr" id="S3.T1.8.8.8.8.8"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.8.8.8.8.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.1.1" style="color:#000000;">Dragon + Fine-tune</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.8.8.8.8.8.1.m1.1"><semantics id="S3.T1.8.8.8.8.8.1.m1.1a"><msup id="S3.T1.8.8.8.8.8.1.m1.1.1" xref="S3.T1.8.8.8.8.8.1.m1.1.1.cmml"><mi id="S3.T1.8.8.8.8.8.1.m1.1.1a" xref="S3.T1.8.8.8.8.8.1.m1.1.1.cmml"></mi><mo id="S3.T1.8.8.8.8.8.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.8.8.8.8.8.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.8.8.1.m1.1b"><apply id="S3.T1.8.8.8.8.8.1.m1.1.1.cmml" xref="S3.T1.8.8.8.8.8.1.m1.1.1"><ci id="S3.T1.8.8.8.8.8.1.m1.1.1.1.cmml" xref="S3.T1.8.8.8.8.8.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.8.8.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.8.8.8.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.2.1" style="color:#000000;">52.72</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.8.8.8.8.8.3.1" style="color:#000000;">80.67</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.4.1" style="color:#000000;">48.94</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.5.1" style="color:#000000;">83.01</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.6.1" style="color:#000000;">52.64</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.7.1" style="color:#000000;">81.95</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.8.1" style="color:#000000;">50.73</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.9.1" style="color:#000000;">87.17</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.10.1" style="color:#000000;">67.86</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.11.1" style="color:#000000;">86.28</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.12.1" style="color:#000000;">43.43</span></span> <span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.13.1" style="color:#000000;">64.94</span></span></span> <span class="ltx_tr" id="S3.T1.9.9.9.9.9"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.9.9.9.9.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.1.1" style="color:#000000;">- SyntheticConvQA</span><math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S3.T1.9.9.9.9.9.1.m1.1"><semantics id="S3.T1.9.9.9.9.9.1.m1.1a"><msup id="S3.T1.9.9.9.9.9.1.m1.1.1" xref="S3.T1.9.9.9.9.9.1.m1.1.1.cmml"><mi id="S3.T1.9.9.9.9.9.1.m1.1.1a" xref="S3.T1.9.9.9.9.9.1.m1.1.1.cmml"></mi><mi id="S3.T1.9.9.9.9.9.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S3.T1.9.9.9.9.9.1.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.9.9.1.m1.1b"><apply id="S3.T1.9.9.9.9.9.1.m1.1.1.cmml" xref="S3.T1.9.9.9.9.9.1.m1.1.1"><ci id="S3.T1.9.9.9.9.9.1.m1.1.1.1.cmml" xref="S3.T1.9.9.9.9.9.1.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.9.9.1.m1.1c">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.9.9.9.1.m1.1d">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.2.1" style="color:#000000;">52.98</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.9.9.9.9.9.3.1" style="color:#000000;">81.15</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.4.1" style="color:#000000;">48.64</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.5.1" style="color:#000000;">83.47</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.6.1" style="color:#000000;">54.75</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.7.1" style="color:#000000;">83.23</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.8.1" style="color:#000000;">49.63</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.9.1" style="color:#000000;">86.70</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.10.1" style="color:#000000;">64.48</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.11.1" style="color:#000000;">85.24</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.12.1" style="color:#000000;">47.41</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.13.1" style="color:#000000;">67.13</span></span></span> </span> </span><span class="ltx_text" id="S3.T1.9.9.9.10" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>평균 top-1 및 top-5 리콜 점수를 갖는 5개의 다중 턴 QA 데이터 세트에 걸친 검색 결과.</figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores.
Compared to rewriting, fine-tuning performs much better on E5-unsupervised and comparable on Dragon.
*Since the average context length in TopiOCQA and INSCIT are smaller than other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in other datasets. <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.14.m1.1"><semantics id="S3.T1.14.m1.1b"><msup id="S3.T1.14.m1.1.1" xref="S3.T1.14.m1.1.1.cmml"><mi id="S3.T1.14.m1.1.1b" xref="S3.T1.14.m1.1.1.cmml"></mi><mo id="S3.T1.14.m1.1.1.1" xref="S3.T1.14.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.14.m1.1c"><apply id="S3.T1.14.m1.1.1.cmml" xref="S3.T1.14.m1.1.1"><ci id="S3.T1.14.m1.1.1.1.cmml" xref="S3.T1.14.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.m1.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.14.m1.1e">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>The inputs of these two models are concatenation of the dialogue history and current query. <math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.15.m2.1"><semantics id="S3.T1.15.m2.1b"><msup id="S3.T1.15.m2.1.1" xref="S3.T1.15.m2.1.1.cmml"><mi id="S3.T1.15.m2.1.1b" xref="S3.T1.15.m2.1.1.cmml"></mi><mo id="S3.T1.15.m2.1.1.1" xref="S3.T1.15.m2.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.15.m2.1c"><apply id="S3.T1.15.m2.1.1.cmml" xref="S3.T1.15.m2.1.1"><ci id="S3.T1.15.m2.1.1.1.cmml" xref="S3.T1.15.m2.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.m2.1d">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.15.m2.1e">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math>The inputs of this model is the rewritten query.
<math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S3.T1.16.m3.1"><semantics id="S3.T1.16.m3.1b"><msup id="S3.T1.16.m3.1.1" xref="S3.T1.16.m3.1.1.cmml"><mi id="S3.T1.16.m3.1.1b" xref="S3.T1.16.m3.1.1.cmml"></mi><mi id="S3.T1.16.m3.1.1.1" mathvariant="normal" xref="S3.T1.16.m3.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.16.m3.1c"><apply id="S3.T1.16.m3.1.1.cmml" xref="S3.T1.16.m3.1.1"><ci id="S3.T1.16.m3.1.1.1.cmml" xref="S3.T1.16.m3.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.m3.1d">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.16.m3.1e">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math>denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. <math alttext="{}^{\triangle}" class="ltx_Math" display="inline" id="S3.T1.17.m4.1"><semantics id="S3.T1.17.m4.1b"><msup id="S3.T1.17.m4.1.1" xref="S3.T1.17.m4.1.1.cmml"><mi id="S3.T1.17.m4.1.1b" xref="S3.T1.17.m4.1.1.cmml"></mi><mi id="S3.T1.17.m4.1.1.1" mathvariant="normal" xref="S3.T1.17.m4.1.1.1.cmml">△</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.17.m4.1c"><apply id="S3.T1.17.m4.1.1.cmml" xref="S3.T1.17.m4.1.1"><ci id="S3.T1.17.m4.1.1.1.cmml" xref="S3.T1.17.m4.1.1.1">△</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.m4.1d">{}^{\triangle}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.17.m4.1e">start_FLOATSUPERSCRIPT △ end_FLOATSUPERSCRIPT</annotation></semantics></math>The numbers are not apple-to-apple comparison (e.g., they use training set for fine-tuning).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="S3.F2.g1" src="https://arxiv.org/html/2401.10225v1/x2.png" width="823">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 2:</span>Illustration of fine-tuning retriever for multi-turn QA.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Retrieval for Multi-Turn QA</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">대화 질의 처리 작업에서 문서가 LLMs에 직접 입력되기에는 너무 길어지면 대화 질의를 처리할 수 있는 검색기가 필수적이다. 이 대화 검색기는 대화 이력과 현재 쿼리의 연결을 인코딩한 다음 문서에서 관련 컨텍스트를 검색합니다. 그 후, 관련 컨텍스트만이 LLM에 대한 입력으로서 사용될 것이다. 최첨단 검색기(예: Dragon<cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib39" title="">2023a</a>)</cite>)는 단일 회전 쿼리에 최적화되어 다중 회전 대화 쿼리에 대한 일반화 능력이 제한된다. <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서는 이 문제를 완화하기 위해 리트리버 미세 조정 방법을 설명합니다. 본 논문에서는 단일 턴 리트리버를 보다 정교하게 조정하기 위해 대화형 질의와 컨텍스트 쌍을 사용하여 대화형 입력에 더 잘 대처할 수 있도록 제안한다.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">대안적인 해결책은 질의 재작성기를 사용하여 대화 히스토리를 기반으로 현재 질문을 재작성하는 대화 질의 재작성 방법이다. 그런 다음 다시 작성된 쿼리는 관련 컨텍스트를 검색하기 위해 단일 회전 쿼리 검색기에 대한 입력으로 직접 사용됩니다. 쿼리 재작성 모델은 임베딩 및 검색 비용 외에도 재작성된 쿼리를 생성하기 위해 많은 양의 추가 계산 비용을 도입한다.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fine-tuning Retriever for Multi-turn QA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">고품질 미세 조정 데이터 세트를 구축하기 위해 HumanAnnotatedConvQA 또는 SyntheticConvQA의 대화형 QA 데이터 세트를 활용하여 대화형 쿼리 및 컨텍스트 쌍을 구성합니다.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">HumanAnnotatedConvQA의 경우 대화형 쿼리와 컨텍스트 쌍의 주석을 직접 가져와서 단일 회전 쿼리 검색기를 추가로 미세 조정 하는 데 사용 합니다. SyntheticConvQA의 경우 먼저 대화 QA 데이터 세트의 각 문서를 다른 청크로 절단한다. 그런 다음 에이전트의 답변과 각 청크 사이의 4그램 리콜 점수를 계산한다. 그 후 회상 점수가 가장 높은 청크를 현재 사용자의 질문에 대한 골드 청크로 간주한다. 마지막으로, 구축된 대화 질의와 문맥 쌍은 단일 회전 질의 검색기를 미세 조정하기 위해 사용된다.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Conversational Query Rewriting</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">강력한 대화 질의 재작성 모델을 구축하기 위해, <cite class="ltx_cite ltx_citemacro_citet">Galimzhanova et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>가 GPT-3.5-turbo를 사용하여 최신 질의 재작성 결과를 입증했다는 점을 고려하여 GPT-3.5-turbo를 재작성자로 사용한다. <cite class="ltx_cite ltx_citemacro_citet">Galimzhanova et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>와 유사하게 GPT-3.5-turbo를 재작성 작업 지도와 함께 제공할 뿐만 아니라, 재작성 결과의 질을 높이기 위해 적은 수의 재작성 예제를 제공한다. 보다 자세한 내용은 부록 <a class="ltx_ref" href="#A2.SS1" title="B.1 Query Rewriting Prompts for GPT-3.5-turbo ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">B.1</span></a>에서 확인할 수 있다.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparisons</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">표 <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">1</span></a>에서 제로 샷 설정의 5개 데이터 세트에 대한 쿼리 다시 쓰기 및 미세 조정 방법을 비교한다. 이러한 데이터 세트에 대한 자세한 내용은§<a class="ltx_ref" href="#S5.SS2.SSS1" title="5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>에서 찾을 수 있다. 최첨단 리트리버인 Dragon <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib39" title="">2023a</a>)</cite>와 강한 비감독 리트리버인 E5-unsupervised <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib65" title="">2022a</a>)</cite>에 대해 실험을 진행하였으며, MS MACRO <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al., <a class="ltx_ref" href="#bib.bib48" title="">2016</a>)</cite>에서는 미세 조정되지 않았다. Dragon을 대상으로 실험한 결과, Fine-tuning은 평균 Top-1 리콜에서 1.74% 정도 질의 재작성보다 약간 더 나쁜 성능을 보였으며, Top-5 리콜에서는 평균 0.54% 정도 더 좋은 성능을 보였다. 이를 통해 대화 검색을 위한 미세 조정 기법의 효용성을 입증한다. 또한 HumanAnnotatedConvQA와 SyntheticConvQA를 이용하여 미세조정을 수행한 결과 유사한 결과를 얻을 수 있었다. 이것은 인간의 주석이 달린 데이터 세트가 고품질임을 강조하며, 우리는 <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.1">do not rely on</em> ChatGPT models for building the state-of-the-art multi-turn query retriever.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">놀랍게도 미세 조정은 감독되지 않은 E5에서 다시 쓰는 것보다 훨씬 더 나은 성능을 보인다. 우리는 E5-unsupervised이 사전 훈련 단계에서 사람 주석이 달린 질의와 문맥 쌍을 사용하지 않기 때문에 고품질 재작성 질의에 대한 약한 일반화로 이어질 수 있다고 추측한다. 대조적으로, E5를 감독하지 않고 미세 조정하기 위해 고품질 데이터 세트를 사용하면 평균 Top-1 및 Top-5 리콜 점수 모두에서 15% 이상의 개선으로 엄청난 부스트가 발생한다.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">따라서 고품질 대화 쿼리 컨텍스트 쌍에서 좋은 싱글 턴 리트리버를 미세 조정하면 최신 재기록기를 활용하는 것과 동등하게 수행됩니다. 그러나, 재작성 방법은 자동 회귀 생성 과정을 위한 추가적인 계산 시간과 GPT-3.5-turbo와 같은 강력한 모델을 사용하기 위한 API 비용도 필요로 한다. 대조적으로, 제안된 다중 회전 미세 조정은 이러한 문제를 우회한다. 이 5개의 데이터 세트에 대한 QA 평가를 위해 모든 QA 모델에 대해 미세 조정 접근법에서 검색된 상위 5개의 결과를 일관되게 사용한다. 우리는 부록 <a class="ltx_ref" href="#A2.SS2" title="B.2 More Results for Retrieval in Conversational QA ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">B.2</span></a>에서 다시 쓰기와 미세 조정 방법 간의 비교에 더 많은 결과를 넣었다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">이 섹션에서는 대화 질의 응답 작업에 대한 실험 설정에 대한 세부 사항을 제시한다.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Baselines</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">우리는 다양한 모델 크기에 대한 실험을 수행한다. 먼저 단계-2 문맥 강화 명령어 튜닝의 효과를 보이기 위해 단계-1 감독 미세 조정(SFT) 후 Llama2-7B/13B/70B 기초 모델인 <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Llama2-SFT-7B/13B/70B</span>과 비교한다. 둘째, <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.2">Llama2-Chat-7B/13B/70B</span>과 비교하였는데, 이는 Llama2-Chat 모델들이 강력한 명령어 추종 및 대화 QA 능력들을 가지고 있기 때문이다. <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib60" title="">2023</a>)</cite>. Llama2 모델 외에도 사내 GPT-8B 기반 모델에 대한 실험을 수행하고 stage-1 SFT 기준선(<span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.3">GPT-8B-SFT</span>)과 비교한다. 마지막으로 강력한 두 OpenAI 모델인 <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.4">GPT-3.5-turbo (4k)</span>과 <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.5">GPT-4 (8k)</span>과 비교한다. 공정한 비교를 위해 우리는 모델과 기준선 모두에 대한 입력과 동일한 컨텍스트를 사용한다. 모든 기준선이 가능한 한 좋은 결과를 얻을 수 있도록 지침을 주의 깊게 조정했습니다. <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The prompts for these baselines can be found in Appendix <a class="ltx_ref" href="#A3" title="Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">C</span></a>.</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Benchmarks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Long Document Datasets</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">4K 토큰의 시퀀스 길이를 갖는 LLMs에 직접 피팅될 수 없는 긴 문서를 갖는 5개의 대화 QA 데이터 세트를 수집한다. 따라서 우리는 입력으로 상위 5개의 관련 청크를 얻기 위해 멀티턴 리트리버를 실행한다(실험은 §<a class="ltx_ref" href="#S4.SS3" title="4.3 Comparisons ‣ 4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4.3</span></a>에서 찾을 수 있다).</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Doc2Dial <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>)</cite>는 DMV, SSA, VA 및 Student Aid의 네 가지 도메인을 포함하는 문서 기반 대화 QA 데이터 세트이다. 각 샘플은 사용자가 문서에 관한 질의를 제기하는 대화로 구성되며, 에이전트는 이러한 질문에 응답한다. 평균 문서 길이는 약 101K 단어입니다.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">QuAC<cite class="ltx_cite ltx_citemacro_citep">(Choi et al., <a class="ltx_ref" href="#bib.bib10" title="">2018</a>)</cite>는 위키피디아 문서를 기반으로 한다. 원래 그 문서는 짧다. 각 대화는 여러 개의 위키피디아 URL에 연결되어 있기 때문에 이러한 링크에서 텍스트를 추출하여 문서 크기를 대략 평균 15K 단어로 증가시킨다. 여기에는 주어진 컨텍스트 내에서 답변을 찾을 수 없는 답변할 수 없는 경우가 포함되어 있습니다.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">QReCC <cite class="ltx_cite ltx_citemacro_citep">(Anantha et al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>)</cite>는 여러 소스에서 열린 도메인 대화 QA 데이터 세트입니다. QuAC와 유사하게, 각각의 대화는 또한 대응하는 URL을 갖는다. 이러한 URL에서 텍스트를 추출하여 문서를 구성한다. 결국, 평균 문서 크기는 약 5K 단어이며, 최대 문서 크기는 20K 단어이다.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">TopiOCQA<cite class="ltx_cite ltx_citemacro_citep">(Adlakha et al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>)</cite>는 전체 위키피디아에 접지되어 있다. 이것은 토픽 전환을 통합하고 에이전트가 사용자 질문에 대한 답변을 위해 전체 위키피디아를 검색하도록 요구한다.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">INSCIT <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>도 전체 위키피디아에 기반을 두고 있다. 사용자 질문이 과소 지정되어 설명이 필요한 경우를 연구한다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">Doc2Dial, QuAC 및 QReCC의 경우 문서를 약 300개의 단어 청크로 분할하고 각 사용자 질문에 대한 컨텍스트로 상위 5개의 관련 청크를 검색한다. TopioCQA 및 INSCIT의 경우 원래 분할을 따르므로 청크가 더 작아진다. 따라서 우리는 처음 세 개의 데이터 세트와 유사한 컨텍스트 길이를 얻기 위해 상위 20개의 청크를 검색했다.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.2" style="width:493.0pt;height:307pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S5.T2.2.2"><span class="ltx_text" id="S5.T2.2.2.2"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.2.2.2.2"> <span class="ltx_tbody"> <span class="ltx_tr" id="S5.T2.2.2.2.2.3.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.2.2.2.2.3.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.1.1" style="color:#000000;">Models</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.2.1" style="color:#000000;">Average</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.3.1" style="color:#000000;">Doc2Dial</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.4.1" style="color:#000000;">QuAC</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.5.1" style="color:#000000;">QReCC</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.6.1" style="color:#000000;">CoQA</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.7.1" style="color:#000000;">DoQA</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.8.1" style="color:#000000;">ConvFinQA</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.9.1" style="color:#000000;">SQA</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.10.1" style="color:#000000;">TopiOCQA</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.11.1" style="color:#000000;">HybridDial</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.12.1" style="color:#000000;">INSCIT</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.4.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.4.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.1.1" style="color:#000000;">GPT-8B-SFT</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.2.1" style="color:#000000;">34.46</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.3.1" style="color:#000000;">31.03</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.4.1" style="color:#000000;">20.07</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.5.1" style="color:#000000;">37.69</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.6.1" style="color:#000000;">59.24</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.7.1" style="color:#000000;">21.72</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.8.1" style="color:#000000;">15.44</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.9.1" style="color:#000000;">40.06</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.10.1" style="color:#000000;">38.17</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.11.1" style="color:#000000;">52.29</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.12.1" style="color:#000000;">28.86</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.5.3"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.5.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.1.1" style="color:#000000;">ChatQA-8B</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.5.3.2.1" style="color:#000000;">49.36</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.3.1" style="color:#000000;">36.76</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.4.1" style="color:#000000;">33.95</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.5.1" style="color:#000000;">45.54</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.6.1" style="color:#000000;">77.90</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.7.1" style="color:#000000;">44.65</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.8.1" style="color:#000000;">61.68</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.9.1" style="color:#000000;">60.74</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.10.1" style="color:#000000;">47.03</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.11.1" style="color:#000000;">53.81</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.12.1" style="color:#000000;">31.50</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.6.4"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.6.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.1.1" style="color:#000000;">Llama2-7B-SFT</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.2.1" style="color:#000000;">34.81</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.3.1" style="color:#000000;">30.26</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.4.1" style="color:#000000;">19.21</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.5.1" style="color:#000000;">37.55</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.6.1" style="color:#000000;">62.75</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.7.1" style="color:#000000;">21.76</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.8.1" style="color:#000000;">34.43</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.9.1" style="color:#000000;">32.18</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.10.1" style="color:#000000;">32.88</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.11.1" style="color:#000000;">48.96</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.12.1" style="color:#000000;">28.16</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.7.5"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.7.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.1.1" style="color:#000000;">Llama2-7B-Chat</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.2.1" style="color:#000000;">38.86</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.3.1" style="color:#000000;">33.27</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.4.1" style="color:#000000;">25.83</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.5.1" style="color:#000000;">46.02</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.6.1" style="color:#000000;">72.28</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.7.1" style="color:#000000;">33.15</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.8.1" style="color:#000000;">36.58</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.9.1" style="color:#000000;">26.14</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.10.1" style="color:#000000;">36.68</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.11.1" style="color:#000000;">47.02</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.12.1" style="color:#000000;">31.67</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.8.6"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.8.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.1.1" style="color:#000000;">ChatQA-7B</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.8.6.2.1" style="color:#000000;">47.71</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.3.1" style="color:#000000;">37.88</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.4.1" style="color:#000000;">29.69</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.5.1" style="color:#000000;">46.97</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.6.1" style="color:#000000;">76.61</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.7.1" style="color:#000000;">41.57</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.8.1" style="color:#000000;">51.61</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.9.1" style="color:#000000;">61.87</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.10.1" style="color:#000000;">45.45</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.11.1" style="color:#000000;">54.51</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.12.1" style="color:#000000;">30.96</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.9.7"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.9.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.1.1" style="color:#000000;">Llama2-13B-SFT</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.2.1" style="color:#000000;">37.69</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.3.1" style="color:#000000;">30.68</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.4.1" style="color:#000000;">21.59</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.5.1" style="color:#000000;">38.25</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.6.1" style="color:#000000;">69.52</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.7.1" style="color:#000000;">21.70</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.8.1" style="color:#000000;">41.14</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.9.1" style="color:#000000;">37.85</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.10.1" style="color:#000000;">35.26</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.11.1" style="color:#000000;">52.22</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.12.1" style="color:#000000;">28.73</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.10.8"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.10.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.1.1" style="color:#000000;">Llama2-13B-Chat</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.2.1" style="color:#000000;">40.34</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.3.1" style="color:#000000;">34.74</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.4.1" style="color:#000000;">27.89</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.5.1" style="color:#000000;">47.19</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.6.1" style="color:#000000;">72.50</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.7.1" style="color:#000000;">32.60</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.8.1" style="color:#000000;">41.54</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.9.1" style="color:#000000;">25.39</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.10.1" style="color:#000000;">39.25</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.11.1" style="color:#000000;">49.82</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.12.1" style="color:#000000;">32.52</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.11.9"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.11.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.1.1" style="color:#000000;">ChatQA-13B</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.11.9.2.1" style="color:#000000;">50.86</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.3.1" style="color:#000000;">38.05</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.4.1" style="color:#000000;">34.28</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.5.1" style="color:#000000;">48.06</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.6.1" style="color:#000000;">77.23</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.7.1" style="color:#000000;">43.31</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.8.1" style="color:#000000;">65.44</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.9.1" style="color:#000000;">66.41</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.10.1" style="color:#000000;">48.88</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.11.1" style="color:#000000;">56.19</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.12.1" style="color:#000000;">30.79</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.12.10"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.12.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.1.1" style="color:#000000;">Llama2-70B-SFT</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.2.1" style="color:#000000;">43.22</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.3.1" style="color:#000000;">34.42</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.4.1" style="color:#000000;">25.65</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.5.1" style="color:#000000;">41.88</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.6.1" style="color:#000000;">73.04</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.7.1" style="color:#000000;">28.21</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.8.1" style="color:#000000;">46.64</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.9.1" style="color:#000000;">58.90</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.10.1" style="color:#000000;">37.20</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.11.1" style="color:#000000;">55.52</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.12.1" style="color:#000000;">30.71</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.13.11"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.13.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.1.1" style="color:#000000;">Llama2-70B-Chat</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.2.1" style="color:#000000;">45.21</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.3.1" style="color:#000000;">36.87</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.4.1" style="color:#000000;">32.47</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.5.1" style="color:#000000;">49.40</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.6.1" style="color:#000000;">80.41</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.7.1" style="color:#000000;">38.97</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.8.1" style="color:#000000;">46.85</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.9.1" style="color:#000000;">37.62</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.10.1" style="color:#000000;">44.31</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.11.1" style="color:#000000;">50.35</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.12.1" style="color:#000000;">34.88</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.14.12"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.14.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.1.1" style="color:#000000;">ChatQA-70B</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.14.12.2.1" style="color:#000000;">54.14</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.3.1" style="color:#000000;">38.90</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.4.1" style="color:#000000;">41.82</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.5.1" style="color:#000000;">48.05</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.6.1" style="color:#000000;">78.57</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.7.1" style="color:#000000;">51.94</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.8.1" style="color:#000000;">73.69</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.9.1" style="color:#000000;">69.14</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.10.1" style="color:#000000;">50.98</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.11.1" style="color:#000000;">56.44</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.12.1" style="color:#000000;">31.90</span></span></span> <span class="ltx_tr" id="S5.T2.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.1.1" style="color:#000000;">- SyntheticConvQA</span><math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.1.1.m1.1a"><msup id="S5.T2.1.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T2.1.1.1.1.1.1.m1.1.1a" xref="S5.T2.1.1.1.1.1.1.m1.1.1.cmml"></mi><mi id="S5.T2.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S5.T2.1.1.1.1.1.1.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.1.m1.1b"><apply id="S5.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.m1.1.1"><ci id="S5.T2.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.1.m1.1c">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.2.1" style="color:#000000;">54.08</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.3.1" style="color:#000000;">39.19</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.4.1" style="color:#000000;">38.33</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.5.1" style="color:#000000;">48.73</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.6.1" style="color:#000000;">79.83</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.7.1" style="color:#000000;">48.65</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.8.1" style="color:#000000;">76.44</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.9.1" style="color:#000000;">68.63</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.10.1" style="color:#000000;">51.30</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.11.1" style="color:#000000;">55.68</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.12.1" style="color:#000000;">33.98</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.1.1" style="color:#000000;">- w/o stage-1</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.2.2.1.m1.1a"><msup id="S5.T2.2.2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.2.2.1.m1.1.1.cmml"><mi id="S5.T2.2.2.2.2.2.1.m1.1.1a" xref="S5.T2.2.2.2.2.2.1.m1.1.1.cmml"></mi><mo id="S5.T2.2.2.2.2.2.1.m1.1.1.1" mathcolor="#000000" xref="S5.T2.2.2.2.2.2.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.2.1.m1.1b"><apply id="S5.T2.2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.2.1.m1.1.1"><ci id="S5.T2.2.2.2.2.2.1.m1.1.1.1.cmml" xref="S5.T2.2.2.2.2.2.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.2.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.2.1" style="color:#000000;">52.18</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.3.1" style="color:#000000;">38.43</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.4.1" style="color:#000000;">37.52</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.5.1" style="color:#000000;">46.08</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.6.1" style="color:#000000;">73.51</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.7.1" style="color:#000000;">49.42</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.8.1" style="color:#000000;">72.15</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.9.1" style="color:#000000;">72.08</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.10.1" style="color:#000000;">51.28</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.11.1" style="color:#000000;">50.74</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.12.1" style="color:#000000;">30.56</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.15.13"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.15.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.1.1" style="color:#000000;">- w/o single-turn*</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.2.1" style="color:#000000;">52.25</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.3.1" style="color:#000000;">38.30</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.4.1" style="color:#000000;">37.89</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.5.1" style="color:#000000;">47.08</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.6.1" style="color:#000000;">76.74</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.7.1" style="color:#000000;">46.43</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.8.1" style="color:#000000;">72.42</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.9.1" style="color:#000000;">67.41</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.10.1" style="color:#000000;">49.85</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.11.1" style="color:#000000;">53.16</span></span> <span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.12.1" style="color:#000000;">33.18</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.16.14"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.16.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.1.1" style="color:#000000;">GPT-3.5-turbo (4k)</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.2.1" style="color:#000000;">50.37</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.3.1" style="color:#000000;">34.83</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.4.1" style="color:#000000;">37.17</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.5.1" style="color:#000000;">50.46</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.6.1" style="color:#000000;">79.33</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.7.1" style="color:#000000;">41.11</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.8.1" style="color:#000000;">73.15</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.9.1" style="color:#000000;">60.63</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.10.1" style="color:#000000;">44.30</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.11.1" style="color:#000000;">47.42</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.12.1" style="color:#000000;">35.27</span></span></span> <span class="ltx_tr" id="S5.T2.2.2.2.2.17.15"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.2.2.2.2.17.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.1.1" style="color:#000000;">GPT-4 (8k)</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.17.15.2.1" style="color:#000000;">53.90</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.3.1" style="color:#000000;">34.16</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.4.1" style="color:#000000;">40.29</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.5.1" style="color:#000000;">52.01</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.6.1" style="color:#000000;">77.42</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.7.1" style="color:#000000;">43.39</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.8.1" style="color:#000000;">81.28</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.9.1" style="color:#000000;">79.21</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.10.1" style="color:#000000;">45.09</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.11.1" style="color:#000000;">49.81</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.12.1" style="color:#000000;">36.34</span></span></span> </span> </span><span class="ltx_text" id="S5.T2.2.2.2.3" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span>10개의 데이터 세트에 걸쳐 제로샷 대화 QA 결과. <math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S5.T2.5.m1.1"><semantics id="S5.T2.5.m1.1b"><msup id="S5.T2.5.m1.1.1" xref="S5.T2.5.m1.1.1.cmml"><mi id="S5.T2.5.m1.1.1b" xref="S5.T2.5.m1.1.1.cmml"></mi><mi id="S5.T2.5.m1.1.1.1" mathvariant="normal" xref="S5.T2.5.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S5.T2.5.m1.1c"><apply id="S5.T2.5.m1.1.1.cmml" xref="S5.T2.5.m1.1.1"><ci id="S5.T2.5.m1.1.1.1.cmml" xref="S5.T2.5.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.m1.1d">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.m1.1e">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math>는 HumanAnnotatedConvQA가 SyntheticConvQA로 대체되었음을 나타낸다. <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S5.T2.6.m2.1"><semantics id="S5.T2.6.m2.1b"><msup id="S5.T2.6.m2.1.1" xref="S5.T2.6.m2.1.1.cmml"><mi id="S5.T2.6.m2.1.1b" xref="S5.T2.6.m2.1.1.cmml"></mi><mo id="S5.T2.6.m2.1.1.1" xref="S5.T2.6.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T2.6.m2.1c"><apply id="S5.T2.6.m2.1.1.cmml" xref="S5.T2.6.m2.1.1"><ci id="S5.T2.6.m2.1.1.1.cmml" xref="S5.T2.6.m2.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.m2.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.m2.1e">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>는 스테이지-1(SFT)이 제거되고 ChatQA 스테이지-2 튜닝만 적용됨을 나타낸다. *단일 전환 QA 데이터 세트가 ChatQA 단계 2 훈련 혼합에서 제거됨을 나타냅니다. "w/o 스테이지-1" 및 "w/o 싱글 턴" 설정은 모두 SyntheticConvQA 데이터를 사용합니다. 평균 점수 측면에서 우리의 ChatQA 모델은 SFT 및 Chat 대응 모델을 크게 능가하고 최고의 모델 ChatQA-70B는 GPT-4를 약간 능가한다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Short Document Datasets</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">문서 길이의 다양성을 높이기 위해 짧은 문서(1.5K 단어 미만)를 가진 5개의 대화식 QA 데이터 세트를 수집한다. 평균적으로 1 단어는  1.5 토큰으로 토큰화될 것이다. 따라서, 문서는 4K 토큰의 시퀀스 길이를 갖는 LLM에 직접 피팅될 수 있다.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1">CoQA<cite class="ltx_cite ltx_citemacro_citep">(Reddy et al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>)</cite>는 짧은 패시지에 각 대화를 기반으로 하는 대화형 QA 데이터 세트이다. 답은 일반적으로 짧고, 구절은 아동 이야기, 문학, 중/고등학교 시험, 뉴스, 위키피디아와 같은 광범위한 영역을 다룬다.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1">DoQA<cite class="ltx_cite ltx_citemacro_citep">(Campos et al., <a class="ltx_ref" href="#bib.bib7" title="">2020</a>)</cite>는 활성 Stack Exchange<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stackexchange.com/" title="">https://stackexchange.com/</a></span></span></span> 포럼에서 수집된 요리, 여행 및 영화의 세 가지 영역을 다룹니다. 데이터 세트에는 지정된 문서 내에서 답변을 찾을 수 없는 답변할 수 없는 경우가 포함되어 있습니다.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1">ConvFinQA<cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="#bib.bib8" title="">2022a</a>)</cite>는 파이낸셜 도메인을 기반으로 한다. 각 문서에는 테이블을 둘러싼 관련 텍스트와 함께 단일 재무 보고서 테이블이 포함되어 있습니다. 이 데이터 세트는 산술 계산과 복잡한 숫자 추론을 포함한다.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i4.p1">
<p class="ltx_p" id="S5.I2.i4.p1.1">SQA<cite class="ltx_cite ltx_citemacro_citep">(Pasupat &amp; Liang, <a class="ltx_ref" href="#bib.bib52" title="">2015</a>)</cite>는 주변 텍스트 없이 단일 테이블만 포함하는 문서에 근거한다. 문서들은 위키피디아에서 수집되고, 질문들은 매우 구성적이어서 정답을 제시할 수 있는 견고한 표 이해 능력을 가진 모델이 필요하다.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i5.p1">
<p class="ltx_p" id="S5.I2.i5.p1.1">HybridDial<cite class="ltx_cite ltx_citemacro_citep">(Nakamura et al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>)</cite>는 위키피디아 표와 텍스트 데이터를 모두 포함하는 문서에 기반을 둔 대화형 QA 데이터 세트이다. 질문은 복잡하기 때문에 문서에 대한 추론이 필요하다.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">10개의 모든 데이터 세트에 걸쳐 ConvFinQA, SQA 및 HybridDial 데이터 세트는 문서에 표 형식의 데이터를 포함하고 나머지 데이터 세트의 문서는 텍스트 전용입니다. <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Details of these benchmark datasets are in the Appendix <a class="ltx_ref" href="#A3" title="Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">C</span></a>.</span></span></span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluation Metrics</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">F1 점수가 QA 모델을 평가하는 데 가장 일반적으로 사용되는 자동 메트릭이라는 점을 감안할 때 ConvFinQA를 제외한 모든 데이터 세트에 사용한다. ConvFinQA의 답변은 산술적 계산뿐만 아니라 문서로부터 숫자를 추출하는 것이기 때문에 ConvFinQA에서는 정확한 일치 메트릭을 사용하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="#bib.bib8" title="">2022a</a>)</cite>를 따른다. 따라서 답은 답과 정확히 같을 때만 말이 된다. 모델들이 산술 공식을 생성할 때, 우리는 계산기를 기반으로 최종 결과를 계산하고 금답과 비교할 것이다.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">또한, 우리는 또한 우리의 최상의 모델과 GPT-4 사이의 생성된 답변의 정확성을 평가하기 위해 인간 평가를 수행한다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Main Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서 우리는 10개의 대화 QA 데이터 세트에 걸쳐 서로 다른 모델 변형과 OpenAI 모델을 비교한다.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<p class="ltx_p" id="S6.SS1.SSS1.p2.1">우리는 ChatQA 방법이 모델의 대화 QA 능력을 크게 향상시킨다는 것을 발견했다. 평균 점수 측면에서 Llama2-Chat 모델은 SFT 모델 대응물을 약간 능가하는 반면, 우리의 ChatQA 모델은 SFT 및 Chat 대응물에 비해 약 10점 이상의 절대 개선을 달성한다. 예를 들어, ChatQA-13B는 Llama2-13B-SFT 및 Llama2-13B-Chat에서 각각 13.17(37.69에서 50.86) 및 10.52(40.34에서 50.86) 향상된다. 이는 컨텍스트 강화 명령어 미세 조정을 통해 모델이 검색된 또는 관련 컨텍스트에서 유용한 정보를 효과적으로 추출하는 방법을 학습할 수 있기 때문이다.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p3">
<p class="ltx_p" id="S6.SS1.SSS1.p3.1">OpenAI 모델과 비교하여 최고의 모델 ChatQA-70B는 GPT-3.5-터보를 평균 3.77점 능가하고 GPT-4를 평균 0.24점 능가한다. 또한 훨씬 작은 크기의 ChatQA-13B는 평균 점수 0.49만큼 GPT-3.5-터보를 약간 능가할 수 있다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Importance of Stage-1 SFT</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1"><a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서는 ChatQA의 명령어 수행 능력을 향상시키는 stage-1 SFT의 중요도에 대한 절제 연구를 수행한다. 미세 조정 단계에서 단계-1 SFT를 제거하고 기초 LLM 위에 단계-2 컨텍스트 강화 명령 튜닝을 적용한다. 우리는 평균 점수가 1.9 (54.08에서 52.18로) 떨어진다는 것을 발견한다. SQA 외에도 stage-1을 제거하면 모델이 다른 데이터 세트에서 일관되게 성능이 저하됩니다. 결과는 단계-1의 모든 SFT 데이터 세트가 단계-2 명령어 튜닝에도 혼합되더라도 단계-1이 여전히 중요한 역할을 한다는 것을 나타낸다. 먼저 명령어 수행 능력을 구축하는 것이 2단계 튜닝에 도움이 된다고 생각한다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>Effectiveness of Single-Turn Data</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">단일 회전 QA 데이터 세트가 모델의 다중 회전 QA 능력에 어떻게 영향을 미치는지 조사하기 위해 단계 2의 ChatQA-70B 훈련 블렌드에서 이를 제거하여 절제 연구를 수행한다. 표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>와 같이 단계 2 훈련 블렌드(ChatQA-70B)에 단일 회전 QA 데이터 세트를 통합하면 일반적으로 모든 벤치마크 데이터 세트에 걸쳐 점수가 증가하여 평균 1.83점 개선된다. 흥미롭게도 ConvFinQA, SQA 및 HybridDial(테이블 기반 데이터 세트)의 개선은 추가된 단일 턴 QA 데이터 세트에도 불구하고 문서에 테이블 데이터가 없다. 이러한 결과는 우리의 직관과 일치한다. 단일 회전 데이터 세트를 추가하면 컨텍스트에서 답변을 추출할 수 있는 모델의 기능이 향상되어 대화형 QA 데이터 세트에서 더 나은 점수를 얻을 수 있다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4 </span>Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS4.p1">
<p class="ltx_p" id="S6.SS1.SSS4.p1.1">표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서 7k GPT-3.5-Turbo 합성 데이터세트(SyntheticConvQA)와 수집된 7k 인간 주석 데이터세트(HumanAnnotatedConvQA)를 사용하여 ChatQA 모델을 비교한 결과, 첫째, OpenAI 모델의 합성 데이터에 의존할 필요가 없다는 것을 알 수 있었고, 둘째, 인간 주석 데이터를 사용하여 QuAC 및 DoQA 데이터세트에서 상당한 개선을 달성했음을 알 수 있었으며, 이는 인간 주석 데이터가 QuAC 및 DoQA 데이터세트에 존재하는 응답 불가 사례에 대해 더 높은 품질을 가지고 있다는 사실에 기인할 수 있으며, 결국 이 두 데이터세트에 대한 전반적인 개선으로 이어지며, 응답 불가 사례에 대한 자세한 결과와 분석은 §<a class="ltx_ref" href="#S6.SS5" title="6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6.5</span></a>에서 찾을 수 있다.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.3" style="width:182.7pt;height:216pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T3.3.1"><span class="ltx_text" id="S6.T3.3.1.1"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.3.1.1.1"> <span class="ltx_thead"> <span class="ltx_tr" id="S6.T3.3.1.1.1.1.1"> <span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T3.3.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.1.1.2.1" style="color:#000000;">Ours Win</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.1.1.3.1" style="color:#000000;">Tie</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.1.1.4.1" style="color:#000000;">GPT-4 Win</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.2.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T3.3.1.1.1.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.2.2.1.1" style="color:#000000;">Average</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.1.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.2.2.2.1" style="color:#000000;">13.81%</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.1.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.2.2.3.1" style="color:#000000;">69.09%</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.1.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.2.2.4.1" style="color:#000000;">17.10%</span></span></span> </span> <span class="ltx_tbody"> <span class="ltx_tr" id="S6.T3.3.1.1.1.3.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.3.1.1.1.3.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.3.1.1.1" style="color:#000000;">Doc2Dial</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.1.1.3.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.3.1.2.1" style="color:#000000;">14.29%</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.1.1.3.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.3.1.3.1" style="color:#000000;">68.00%</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.1.1.3.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.3.1.4.1" style="color:#000000;">17.71%</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.4.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.4.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.4.2.1.1" style="color:#000000;">QuAC</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.4.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.4.2.2.1" style="color:#000000;">11.67%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.4.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.4.2.3.1" style="color:#000000;">73.33%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.4.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.4.2.4.1" style="color:#000000;">15.00%</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.5.3"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.5.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.5.3.1.1" style="color:#000000;">QReCC</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.5.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.5.3.2.1" style="color:#000000;">11.11%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.5.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.5.3.3.1" style="color:#000000;">77.22%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.5.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.5.3.4.1" style="color:#000000;">11.67%</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.6.4"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.6.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.6.4.1.1" style="color:#000000;">CoQA</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.6.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.6.4.2.1" style="color:#000000;">7.78%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.6.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.6.4.3.1" style="color:#000000;">80.00%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.6.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.6.4.4.1" style="color:#000000;">12.22%</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.7.5"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.7.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.7.5.1.1" style="color:#000000;">DoQA</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.7.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.7.5.2.1" style="color:#000000;">22.78%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.7.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.7.5.3.1" style="color:#000000;">57.78%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.7.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.7.5.4.1" style="color:#000000;">19.44%</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.8.6"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.8.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.8.6.1.1" style="color:#000000;">ConvFinQA</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.8.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.8.6.2.1" style="color:#000000;">16.67%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.8.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.8.6.3.1" style="color:#000000;">67.78%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.8.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.8.6.4.1" style="color:#000000;">15.55%</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.9.7"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.9.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.9.7.1.1" style="color:#000000;">SQA</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.9.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.9.7.2.1" style="color:#000000;">11.11%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.9.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.9.7.3.1" style="color:#000000;">61.67%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.9.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.9.7.4.1" style="color:#000000;">27.22%</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.10.8"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.10.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.10.8.1.1" style="color:#000000;">TopiOCQA</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.10.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.10.8.2.1" style="color:#000000;">19.31%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.10.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.10.8.3.1" style="color:#000000;">60.69%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.10.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.10.8.4.1" style="color:#000000;">20.00%</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.11.9"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.11.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.11.9.1.1" style="color:#000000;">HybridDial</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.11.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.11.9.2.1" style="color:#000000;">7.78%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.11.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.11.9.3.1" style="color:#000000;">78.33%</span></span> <span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.11.9.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.11.9.4.1" style="color:#000000;">13.89%</span></span></span> <span class="ltx_tr" id="S6.T3.3.1.1.1.12.10"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T3.3.1.1.1.12.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.12.10.1.1" style="color:#000000;">INSCIT</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.1.1.12.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.12.10.2.1" style="color:#000000;">15.56%</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.1.1.12.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.12.10.3.1" style="color:#000000;">66.11%</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.1.1.12.10.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.12.10.4.1" style="color:#000000;">18.33%</span></span></span> </span> </span><span class="ltx_text" id="S6.T3.3.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span>Human evaluation (A/B testing) comparing our ChatQA-70B to GPT-4 over 10 datasets. 평균 점수에서, 우리의 모델과 GPT-4는 대부분의 시간(69.09%)에서 동점이고, GPT-4는 우리보다 약간 더 높은 승률(<math alttext="\sim" class="ltx_Math" display="inline" id="S6.T3.2.m1.1"><semantics id="S6.T3.2.m1.1b"><mo id="S6.T3.2.m1.1.1" xref="S6.T3.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.m1.1c"><csymbol cd="latexml" id="S6.T3.2.m1.1.1.cmml" xref="S6.T3.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S6.T3.2.m1.1e">∼</annotation></semantics></math>3.3%)을 달성한다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.5 </span>Human Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS5.p1">
<p class="ltx_p" id="S6.SS1.SSS5.p1.1">F1 점수가 QA 모델의 품질을 평가하는 데 가장 일반적으로 사용되는 척도임에도 불구하고, 종종 질문에 답하는 여러 가지 방법이 있어 자동 척도가 완벽하지 않다. 따라서 우리는 인간 평가를 사용하여 우리의 ChatQA-70B와 GPT-4를 추가로 비교한다. 이 인간 평가에서 주석자에게 ChatQA-70B 및 GPT-4의 출력에서 사실을 확인하고 어떤 모델이 질문 <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>More human evaluation setup can be found in the Appendix <a class="ltx_ref" href="#A4" title="Appendix D Human Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">D</span></a>.</span></span></span>에 더 정확한 응답을 제공하는지 결정한다. 10개의 데이터 세트에 대한 인간 평가 결과는 표 <a class="ltx_ref" href="#S6.T3" title="Table 3 ‣ 6.1.4 Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">3</span></a>에 나와 있다. 먼저, ChatQA-70B와 GPT-4는 대부분의 시간(69.09%)에서 동점이고, GPT-4는 우리보다 약간 높은 승률(<math alttext="\sim" class="ltx_Math" display="inline" id="S6.SS1.SSS5.p1.1.m1.1"><semantics id="S6.SS1.SSS5.p1.1.m1.1a"><mo id="S6.SS1.SSS5.p1.1.m1.1.1" xref="S6.SS1.SSS5.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS5.p1.1.m1.1b"><csymbol cd="latexml" id="S6.SS1.SSS5.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS5.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS5.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS5.p1.1.m1.1d">∼</annotation></semantics></math>3.3%)을 달성한다. 이는 우리의 모델이 정답을 생성할 수 있는 강력한 능력을 가지고 있음을 더욱 확인시켜준다. 둘째, ConvFinQA에서 GPT-4보다 우리 모델의 승률이 약간 더 좋은 것으로 나타났으며, 이는 우리 모델의 강력한 산술 계산 능력을 나타낸다. 셋째, SQA 과제에서 GPT-4가 훨씬 더 나은 승률을 달성한다는 것을 발견했는데, 이는 표식 추론 과제에서 우리의 모델과 GPT-4 사이에 여전히 격차가 있음을 시사한다.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T4.1" style="width:254.5pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T4.1.1"><span class="ltx_text" id="S6.T4.1.1.1"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.1.1.1.1"> <span class="ltx_thead"> <span class="ltx_tr" id="S6.T4.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T4.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.1.1" style="color:#000000;">Models</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.2.1" style="color:#000000;">Avg-text</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.3.1" style="color:#000000;">Avg-table</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.4.1" style="color:#000000;">Avg-ret</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.5.1" style="color:#000000;">Avg-nonret</span></span></span> </span> <span class="ltx_tbody"> <span class="ltx_tr" id="S6.T4.1.1.1.1.2.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.1.1" style="color:#000000;">ChatQA-13B</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.2.1" style="color:#000000;">45.80</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.3.1" style="color:#000000;">62.68</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.2.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.4.1" style="color:#000000;">40.01</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.2.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.5.1" style="color:#000000;">61.72</span></span></span> <span class="ltx_tr" id="S6.T4.1.1.1.1.3.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.3.2.1.1" style="color:#000000;">ChatQA-70B</span></span> <span class="ltx_td ltx_align_center" id="S6.T4.1.1.1.1.3.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.3.2.2.1" style="color:#000000;">48.88</span></span> <span class="ltx_td ltx_align_center" id="S6.T4.1.1.1.1.3.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.3.2.3.1" style="color:#000000;">66.42</span></span> <span class="ltx_td ltx_align_center" id="S6.T4.1.1.1.1.3.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.3.2.4.1" style="color:#000000;">42.33</span></span> <span class="ltx_td ltx_align_center" id="S6.T4.1.1.1.1.3.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.3.2.5.1" style="color:#000000;">65.96</span></span></span> <span class="ltx_tr" id="S6.T4.1.1.1.1.4.3"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.1.1.1.1.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.1.1" style="color:#000000;">GPT-3.5-turbo (4k)</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.4.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.2.1" style="color:#000000;">46.07</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.4.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.3.1" style="color:#000000;">60.40</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.4.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.4.1" style="color:#000000;">40.41</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.4.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.5.1" style="color:#000000;">60.33</span></span></span> <span class="ltx_tr" id="S6.T4.1.1.1.1.5.4"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T4.1.1.1.1.5.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.5.4.1.1" style="color:#000000;">GPT-4 (8k)</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.1.1.1.5.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.5.4.2.1" style="color:#000000;">46.96</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.1.1.1.5.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.5.4.3.1" style="color:#000000;">70.10</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.1.1.1.5.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.5.4.4.1" style="color:#000000;">41.58</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.1.1.1.5.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.5.4.5.1" style="color:#000000;">66.22</span></span></span> </span> </span><span class="ltx_text" id="S6.T4.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4: </span> 서로 다른 데이터 세트 유형의 평균 점수에 대한 세밀한 연구. Avg-text는 Doc2Dial, QuAC, QReCC, CoQA, DoQA, TopiOCQA 및 INSCIT와 같이 문서에만 텍스트가 있는 데이터 세트를 다룬다.</figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Fine-grained studies on average scores of different dataset types. Avg-text covers datasets where the documents only have text, including Doc2Dial, QuAC, QReCC, CoQA, DoQA, TopiOCQA, and INSCIT.
Avg-table covers datasets having table in the documents, including ConvFinQA, SQA, and HybridDial.
Avg-ret covers datasets having long documents requiring retrieval, including Doc2Dial, QuAC, QReCC, TopiOCQA, and INSCIT.
Avg-nonret covers datasets having short documents which do not require retrieval, including CoQA, DoQA, ConvFinQA, SQA, and HybridDial.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Fine-grained Analyses</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><a class="ltx_ref" href="#S6.T4" title="Table 4 ‣ 6.1.5 Human Evaluation ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4</span></a>에서 대화 QA 벤치마크에서 서로 다른 데이터 세트 유형에 걸쳐 모델과 OpenAI 모델을 추가로 비교했다. ChatQA-70B와 GPT-4의 비교에서, ChatQA-70B는 텍스트 전용 문서(avg-text)에서 더 나은 결과를 얻었으며, 이는 우수한 텍스트 이해 능력을 보여준다. 반면에 GPT-4는 avg-table의 비교를 감안할 때 표 데이터에서 더 나은 QA 능력을 보여준다. 검색이 필요하거나 필요하지 않은 데이터 세트의 경우 ChatQA-70B와 GPT-4가 비슷합니다(간격은 평균 점수 1 이내).</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">ChatQA-13B와 GPT-3.5-터보 간의 비교에서 ChatQA-13B는 Avg-table의 점수를 감안할 때 더 나은 표식 QA 능력을 보여준다. ChatQA-13B는 또한 검색이 필요하지 않은 문서가 있는 데이터 세트에서 더 나은 점수를 보여주지만 텍스트 전용 문서 및 검색이 필요한 문서에서는 GPT-3.5-터보와 동등하다.</p>
</div>
<figure class="ltx_table" id="S6.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T5.1" style="width:207.1pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T5.1.1"><span class="ltx_text" id="S6.T5.1.1.1"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T5.1.1.1.1"> <span class="ltx_thead"> <span class="ltx_tr" id="S6.T5.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T5.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.1.1.1.1" style="color:#000000;">Models</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.1.1.2.1" style="color:#000000;">Avg-all</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.1.1.3.1" style="color:#000000;">Avg-ret</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.1.1.4.1" style="color:#000000;">Avg-nonret</span></span></span> </span> <span class="ltx_tbody"> <span class="ltx_tr" id="S6.T5.1.1.1.1.2.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T5.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.2.1.1.1" style="color:#000000;">ChatQA-70B</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.2.1.2.1" style="color:#000000;">54.14</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.2.1.3.1" style="color:#000000;">42.33</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.1.2.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.2.1.4.1" style="color:#000000;">65.96</span></span></span> <span class="ltx_tr" id="S6.T5.1.1.1.1.3.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T5.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.3.2.1.1" style="color:#000000;">- w/ “top-5” chunks</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.1.1.1.3.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.3.2.2.1" style="color:#000000;">54.04</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.1.1.1.3.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.3.2.3.1" style="color:#000000;">42.91</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.1.1.1.3.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.3.2.4.1" style="color:#000000;">65.16</span></span></span> </span> </span><span class="ltx_text" id="S6.T5.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span>Ablation study on using "top-5" retrieved chunk for the context for the stage-2 instruction tuning. 모든 데이터 세트(Avg-all), 5개의 검색 데이터 세트(Avg-ret) 및 5개의 비검색 데이터 세트(Avg-nonret)에 대한 평균 점수를 보고한다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T6.1" style="width:284.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T6.1.1"><span class="ltx_text" id="S6.T6.1.1.1"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T6.1.1.1.1"> <span class="ltx_thead"> <span class="ltx_tr" id="S6.T6.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T6.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.1.1" style="color:#000000;">Models</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.2.1" style="color:#000000;">Avg.</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.3.1" style="color:#000000;">D2D</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.4.1" style="color:#000000;">QuAC</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.5.1" style="color:#000000;">QReCC</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.6.1" style="color:#000000;">TopiO</span></span> <span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.7.1" style="color:#000000;">INSCIT</span></span></span> </span> <span class="ltx_tbody"> <span class="ltx_tr" id="S6.T6.1.1.1.1.2.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T6.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.1.1" style="color:#000000;">ChatQA-70B</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.2.1" style="color:#000000;">42.31</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.3.1" style="color:#000000;">39.19</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.4.1" style="color:#000000;">38.33</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.5.1" style="color:#000000;">48.73</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.6.1" style="color:#000000;">51.30</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.7.1" style="color:#000000;">33.98</span></span></span> <span class="ltx_tr" id="S6.T6.1.1.1.1.3.2"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.1.1" style="color:#000000;">- # of ctx: top-3</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.2.1" style="color:#000000;">41.91</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.3.1" style="color:#000000;">37.20</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.4.1" style="color:#000000;">38.35</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.5.1" style="color:#000000;">48.94</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.6.1" style="color:#000000;">52.78</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.7.1" style="color:#000000;">32.27</span></span></span> <span class="ltx_tr" id="S6.T6.1.1.1.1.4.3"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.1.1" style="color:#000000;">- # of ctx: top-10</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.2.1" style="color:#000000;">40.71</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.3.1" style="color:#000000;">37.06</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.4.1" style="color:#000000;">36.95</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.5.1" style="color:#000000;">47.61</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.6.1" style="color:#000000;">49.40</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.7.1" style="color:#000000;">32.53</span></span></span> <span class="ltx_tr" id="S6.T6.1.1.1.1.5.4"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.5.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.1.1" style="color:#000000;">- ctx reverse ordering</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.2.1" style="color:#000000;">42.48</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.3.1" style="color:#000000;">39.08</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.4.1" style="color:#000000;">38.85</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.5.1" style="color:#000000;">49.63</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.6.1" style="color:#000000;">51.16</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.7.1" style="color:#000000;">33.69</span></span></span> <span class="ltx_tr" id="S6.T6.1.1.1.1.6.5"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.6.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.1.1" style="color:#000000;">- ctx swing ordering</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.2.1" style="color:#000000;">42.30</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.3.1" style="color:#000000;">39.35</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.4.1" style="color:#000000;">38.09</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.5.1" style="color:#000000;">49.09</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.6.1" style="color:#000000;">50.98</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.7.1" style="color:#000000;">33.99</span></span></span> <span class="ltx_tr" id="S6.T6.1.1.1.1.7.6"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.7.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.1.1" style="color:#000000;">- ctx random ordering</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.2.1" style="color:#000000;">42.01</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.3.1" style="color:#000000;">39.32</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.4.1" style="color:#000000;">38.28</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.5.1" style="color:#000000;">48.79</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.6.1" style="color:#000000;">50.13</span></span> <span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.7.1" style="color:#000000;">33.51</span></span></span> <span class="ltx_tr" id="S6.T6.1.1.1.1.8.7"> <span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T6.1.1.1.1.8.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.1.1" style="color:#000000;">- Dragon Retrieval</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.2.1" style="color:#000000;">40.50</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.3.1" style="color:#000000;">37.92</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.4.1" style="color:#000000;">38.44</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.5.1" style="color:#000000;">47.88</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.6.1" style="color:#000000;">50.39</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.7.1" style="color:#000000;">27.87</span></span></span> </span> </span><span class="ltx_text" id="S6.T6.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 6:</span>검색이 필요한 데이터 세트에 걸쳐 입력 컨텍스트에 대한 어블레이션 연구. 모든 모델은 SyntheticConvQA를 사용하고 있습니다. D2D는 Doc2Dial을 나타내고, TopiO는 TopiOCQA를 나타낸다. 입력(# of ctx), 컨텍스트 순서(역, 스윙, 랜덤) 및 원본 드래곤에서 검색된 컨텍스트를 사용하는 컨텍스트의 수에 대해 연구한다. 이에 비해 ChatQA-70B(기본 설정)는 "Dragon + Fine-tune" 검색한 Top-5 컨텍스트와 Top-5의 첫 번째 컨텍스트에서 다섯 번째 컨텍스트까지의 순차적 순서를 사용하고 있다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T7.5" style="width:498.6pt;height:163pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T7.5.5"><span class="ltx_text" id="S6.T7.5.5.5"> <span class="ltx_tabular ltx_align_middle" id="S6.T7.5.5.5.5"> <span class="ltx_tbody"> <span class="ltx_tr" id="S6.T7.5.5.5.5.6.1"> <span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T7.5.5.5.5.6.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.1.1" style="color:#000000;">Models</span></span> <span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T7.5.5.5.5.6.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.2.1" style="color:#000000;">Avg-Both</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.3.1" style="color:#000000;">Avg-QuAC</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.4.1" style="color:#000000;">QuAC (no*)</span></span> <span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T7.5.5.5.5.6.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.5.1" style="color:#000000;">QuAC (yes*)</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.6.1" style="color:#000000;">Avg-DoQA</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.7.1" style="color:#000000;">DoQA (no*)</span></span> <span class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="S6.T7.5.5.5.5.6.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.8.1" style="color:#000000;">DoQA (yes*)</span></span> <span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.9.1" style="color:#000000;">Avg-CQA</span></span></span> <span class="ltx_tr" id="S6.T7.5.5.5.5.7.2"> <span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.7.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.7.2.1.1" style="color:#000000;">ChatQA-70B</span></span> <span class="ltx_td ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.7.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.7.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td ltx_border_rr ltx_border_t" id="S6.T7.5.5.5.5.7.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span></span> <span class="ltx_tr" id="S6.T7.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.1.1" style="color:#000000;">- 1k unanswerable</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.1.1.1.1.1.1.m1.1"><semantics id="S6.T7.1.1.1.1.1.1.m1.1a"><msup id="S6.T7.1.1.1.1.1.1.m1.1.1" xref="S6.T7.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S6.T7.1.1.1.1.1.1.m1.1.1a" xref="S6.T7.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S6.T7.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" xref="S6.T7.1.1.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.1.1.1.1.1.1.m1.1b"><apply id="S6.T7.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T7.1.1.1.1.1.1.m1.1.1"><ci id="S6.T7.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S6.T7.1.1.1.1.1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.1.1.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.2.1" style="color:#000000;">76.88</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.3.1" style="color:#000000;">80.89</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.4.1" style="color:#000000;">75.10</span></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.5.1" style="color:#000000;">86.67</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.6.1" style="color:#000000;">72.88</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.7.1" style="color:#000000;">64.49</span></span> <span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.1.1.1.1.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.8.1" style="color:#000000;">81.26</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.1.1.9.1" style="color:#000000;">54.16</span></span></span> <span class="ltx_tr" id="S6.T7.2.2.2.2.2"> <span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.2.2.2.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.1.2" style="color:#000000;">- </span><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.1.1" style="color:#000000;">1.5k unanswerable<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.2.2.2.2.2.1.1.m1.1"><semantics id="S6.T7.2.2.2.2.2.1.1.m1.1a"><msup id="S6.T7.2.2.2.2.2.1.1.m1.1.1" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1.cmml"><mi id="S6.T7.2.2.2.2.2.1.1.m1.1.1a" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1.cmml"></mi><mo id="S6.T7.2.2.2.2.2.1.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.2.2.2.2.2.1.1.m1.1b"><apply id="S6.T7.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1"><ci id="S6.T7.2.2.2.2.2.1.1.m1.1.1.1.cmml" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.2.2.2.2.2.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.2.2.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.2.2.2.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.2.1" style="color:#000000;">77.25</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.3.1" style="color:#000000;">80.76</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.4.1" style="color:#000000;">77.66</span></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.2.2.2.2.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.5.1" style="color:#000000;">83.85</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.6.1" style="color:#000000;">73.74</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.7.1" style="color:#000000;">68.81</span></span> <span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.2.2.2.2.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.8.1" style="color:#000000;">78.67</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.9.1" style="color:#000000;">54.14</span></span></span> <span class="ltx_tr" id="S6.T7.3.3.3.3.3"> <span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.3.3.3.3.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.1.1" style="color:#000000;">- 2k unanswerable</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.3.3.3.3.3.1.m1.1"><semantics id="S6.T7.3.3.3.3.3.1.m1.1a"><msup id="S6.T7.3.3.3.3.3.1.m1.1.1" xref="S6.T7.3.3.3.3.3.1.m1.1.1.cmml"><mi id="S6.T7.3.3.3.3.3.1.m1.1.1a" xref="S6.T7.3.3.3.3.3.1.m1.1.1.cmml"></mi><mo id="S6.T7.3.3.3.3.3.1.m1.1.1.1" mathcolor="#000000" xref="S6.T7.3.3.3.3.3.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.3.3.3.3.3.1.m1.1b"><apply id="S6.T7.3.3.3.3.3.1.m1.1.1.cmml" xref="S6.T7.3.3.3.3.3.1.m1.1.1"><ci id="S6.T7.3.3.3.3.3.1.m1.1.1.1.cmml" xref="S6.T7.3.3.3.3.3.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.3.3.3.3.3.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.3.3.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.3.3.3.3.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.2.1" style="color:#000000;">77.10</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.3.1" style="color:#000000;">80.82</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.4.1" style="color:#000000;">77.59</span></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.3.3.3.3.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.5.1" style="color:#000000;">84.05</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.6.1" style="color:#000000;">73.38</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.7.1" style="color:#000000;">67.95</span></span> <span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.3.3.3.3.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.8.1" style="color:#000000;">78.80</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.9.1" style="color:#000000;">53.86</span></span></span> <span class="ltx_tr" id="S6.T7.4.4.4.4.4"> <span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.4.4.4.4.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.1.1" style="color:#000000;">- 2.5k unanswerable</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.4.4.4.4.4.1.m1.1"><semantics id="S6.T7.4.4.4.4.4.1.m1.1a"><msup id="S6.T7.4.4.4.4.4.1.m1.1.1" xref="S6.T7.4.4.4.4.4.1.m1.1.1.cmml"><mi id="S6.T7.4.4.4.4.4.1.m1.1.1a" xref="S6.T7.4.4.4.4.4.1.m1.1.1.cmml"></mi><mo id="S6.T7.4.4.4.4.4.1.m1.1.1.1" mathcolor="#000000" xref="S6.T7.4.4.4.4.4.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.4.4.4.4.4.1.m1.1b"><apply id="S6.T7.4.4.4.4.4.1.m1.1.1.cmml" xref="S6.T7.4.4.4.4.4.1.m1.1.1"><ci id="S6.T7.4.4.4.4.4.1.m1.1.1.1.cmml" xref="S6.T7.4.4.4.4.4.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.4.4.4.4.4.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.4.4.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.4.4.4.4.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.2.1" style="color:#000000;">75.87</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.3.1" style="color:#000000;">78.81</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.4.1" style="color:#000000;">73.76</span></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.4.4.4.4.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.5.1" style="color:#000000;">83.85</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.6.1" style="color:#000000;">72.93</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.7.1" style="color:#000000;">66.54</span></span> <span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.4.4.4.4.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.8.1" style="color:#000000;">79.31</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.9.1" style="color:#000000;">53.78</span></span></span> <span class="ltx_tr" id="S6.T7.5.5.5.5.5"> <span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.5.5.5.5.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.1.1" style="color:#000000;">- SyntheticConvQA</span><math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S6.T7.5.5.5.5.5.1.m1.1"><semantics id="S6.T7.5.5.5.5.5.1.m1.1a"><msup id="S6.T7.5.5.5.5.5.1.m1.1.1" xref="S6.T7.5.5.5.5.5.1.m1.1.1.cmml"><mi id="S6.T7.5.5.5.5.5.1.m1.1.1a" xref="S6.T7.5.5.5.5.5.1.m1.1.1.cmml"></mi><mi id="S6.T7.5.5.5.5.5.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S6.T7.5.5.5.5.5.1.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S6.T7.5.5.5.5.5.1.m1.1b"><apply id="S6.T7.5.5.5.5.5.1.m1.1.1.cmml" xref="S6.T7.5.5.5.5.5.1.m1.1.1"><ci id="S6.T7.5.5.5.5.5.1.m1.1.1.1.cmml" xref="S6.T7.5.5.5.5.5.1.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.5.5.5.5.5.1.m1.1c">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.5.5.5.5.5.1.m1.1d">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.5.5.5.5.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.2.1" style="color:#000000;">69.84</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.3.1" style="color:#000000;">72.92</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.4.1" style="color:#000000;">55.38</span></span> <span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.5.5.5.5.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.5.1" style="color:#000000;">90.42</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.6.1" style="color:#000000;">66.77</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.7.1" style="color:#000000;">45.09</span></span> <span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.5.5.5.5.5.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.8.1" style="color:#000000;">88.45</span></span> <span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.9.1" style="color:#000000;">54.08</span></span></span> <span class="ltx_tr" id="S6.T7.5.5.5.5.8.3"> <span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.8.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.1.1" style="color:#000000;">GPT-3.5-turbo (4k)</span></span> <span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.8.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.2.1" style="color:#000000;">73.27</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.3.1" style="color:#000000;">78.34</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.4.1" style="color:#000000;">61.91</span></span> <span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.8.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.5.1" style="color:#000000;">94.76</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.6.1" style="color:#000000;">68.21</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.7.1" style="color:#000000;">51.99</span></span> <span class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S6.T7.5.5.5.5.8.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.8.1" style="color:#000000;">84.43</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.9.1" style="color:#000000;">50.37</span></span></span> <span class="ltx_tr" id="S6.T7.5.5.5.5.9.4"> <span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S6.T7.5.5.5.5.9.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.1.1" style="color:#000000;">GPT-4 (8k)</span></span> <span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T7.5.5.5.5.9.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.5.5.5.5.9.4.2.1" style="color:#000000;">80.73</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.5.5.5.5.9.4.3.1" style="color:#000000;">87.42</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.4.1" style="color:#000000;">83.45</span></span> <span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T7.5.5.5.5.9.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.5.1" style="color:#000000;">91.38</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.5.5.5.5.9.4.6.1" style="color:#000000;">74.05</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.7.1" style="color:#000000;">74.28</span></span> <span class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S6.T7.5.5.5.5.9.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.8.1" style="color:#000000;">73.82</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.5.5.5.5.9.4.9.1" style="color:#000000;">53.90</span></span></span> </span> </span><span class="ltx_text" id="S6.T7.5.5.5.6" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 7:</span>Accuracies on answerable and unanswerable samples across QuAC and DoQA datasets. Avg-둘 다 QuAC와 DoQA 사이의 평균 점수이다. <math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S6.T7.8.m1.1"><semantics id="S6.T7.8.m1.1b"><msup id="S6.T7.8.m1.1.1" xref="S6.T7.8.m1.1.1.cmml"><mi id="S6.T7.8.m1.1.1b" xref="S6.T7.8.m1.1.1.cmml"></mi><mi id="S6.T7.8.m1.1.1.1" mathvariant="normal" xref="S6.T7.8.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S6.T7.8.m1.1c"><apply id="S6.T7.8.m1.1.1.cmml" xref="S6.T7.8.m1.1.1"><ci id="S6.T7.8.m1.1.1.1.cmml" xref="S6.T7.8.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.8.m1.1d">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.8.m1.1e">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math>는 HumanAnnotatedConvQA가 SyntheticConvQA로 대체되었음을 나타낸다. *"아니오"는 답변할 수 없는 샘플을 나타내고, "예"는 답변할 수 있는 샘플을 나타낸다. <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.9.m2.1"><semantics id="S6.T7.9.m2.1b"><msup id="S6.T7.9.m2.1.1" xref="S6.T7.9.m2.1.1.cmml"><mi id="S6.T7.9.m2.1.1b" xref="S6.T7.9.m2.1.1.cmml"></mi><mo id="S6.T7.9.m2.1.1.1" xref="S6.T7.9.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.9.m2.1c"><apply id="S6.T7.9.m2.1.1.cmml" xref="S6.T7.9.m2.1.1"><ci id="S6.T7.9.m2.1.1.1.cmml" xref="S6.T7.9.m2.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.9.m2.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.9.m2.1e">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>우리는 HumanAnnotatedConvQA에서 응답할 수 없는 샘플의 수로 절제 연구를 수행한다. Avg-CQA는 10개의 대화형 QA 데이터 세트에 대한 평균 점수이다. 우리는 고품질 생성과 환각을 모두 생성하기 때문에 최종 ChatQA-70B에 1.5k의 응답할 수 없는 샘플을 사용한다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Top-<em class="ltx_emph ltx_font_italic" id="S6.SS3.1.1">k</em> Chunks for Stage-2 Instruction Tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">2단계 튜닝에 의해 사용되는 모든 데이터 세트에 대해, 컨텍스트는 답변을 포함하는 연속적인 단락 또는 문서로 제공된다. 대조적으로, 모델은 긴 문서에 대한 추론에서 상위-<em class="ltx_emph ltx_font_italic" id="S6.SS3.p1.1.1">k</em> 검색된 청크를 처리할 필요가 있다. 이러한 열차/테스트 불일치를 줄이기 위해, 우리는 일부 연속 문단을 검색된 top-k 청크로 대체하는 것이 모델의 견고성을 향상시킬지 여부를 조사한다.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">각 질문에는 해당 긴 문서가 있기 때문에 이 연구에서는 NarrativeQA를 사용한다. NarrativeQA의 경우 원래 답변이 포함된 긴 문서의 요약을 문맥으로 사용한다. 불연속적인 컨텍스트를 통합하기 위해 먼저 긴 문서를 300개의 단어 청크로 절단한다. 그런 다음 드래곤 리트리버를 사용하여 질문에 대한 상위 4개의 청크를 추가 컨텍스트로 검색한다. 마지막으로, 우리는 검색된 4개의 청크와 긴 문서의 요약을 "top-5" 청크로 취한다. <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Note that, we did not directly use top-5 retrieved chunks for training, because they may not contain the answer. In such cases, fine-tuning the model to generate answer could encourage hallucination.</span></span></span> 이 재구성된 NarrativeQA를 사용하여 stage-2 명령어 튜닝을 위해 원래 NarrativeQA를 대체합니다.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">표 <a class="ltx_ref" href="#S6.T5" title="Table 5 ‣ 6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">5</span></a>에서 "top-5" 청크를 훈련의 컨텍스트로 사용하면 검색이 필요한 데이터 세트의 개선으로 이어진다는 것을 발견했다. 그러나 검색되지 않은 데이터 세트의 성능을 낮춥니다. 전반적으로 이 두 모델은 비슷합니다. 2단계 튜닝에서 "top-5" 검색된 청크를 통합하는 것은 검색이 필요한 추론 단계와 일치하므로 Avg-ret 점수가 향상되기 때문이다. 그러나 연속 및 불연속 문서의 혼합은 2단계 튜닝을 덜 안정적으로 만들 수 있으며, 이는 비검색 데이터 세트에 대한 차선 결과를 초래할 수 있다. 연속 컨텍스트와 top-<em class="ltx_emph ltx_font_italic" id="S6.SS3.p3.1.1">k</em> 검색된 청크를 통합하는 균형과 관련하여 향후 더 많은 작업이 수행될 수 있다고 믿는다.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Ablation Studies for Inference Stage</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1"><a class="ltx_ref" href="#S6.T6" title="Table 6 ‣ 6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6</span></a>에서는 검색된 컨텍스트/청크 수, 컨텍스트 순서 및 서로 다른 검색자가 대화 QA 결과에 어떤 영향을 미치는지에 대한 절제 연구를 보여준다.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">첫째, 더 많은 컨텍스트를 입력으로 사용하는 것이 항상 결과를 향상시키는 것은 아니라는 것을 발견한다. Top-5 컨텍스트를 입력으로 사용하면 Top-3 또는 Top-10 컨텍스트를 사용하는 것보다 더 나은 결과를 얻을 수 있다. 직관적으로, 더 많은 컨텍스트는 정답(더 나은 회상 점수)을 포함할 확률이 더 높다. 결과적으로 top-5 컨텍스트를 사용하는 것이 top-3 컨텍스트를 사용하는 것보다 더 나은 결과를 얻을 수 있다. 그러나 컨텍스트의 수가 더 증가함에 따라 모델은 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib41" title="">2023</a>)</cite>와 같은 "중간에서 손실" 현상을 겪을 수 있으며 제공된 컨텍스트에서 답변을 추출하는 어려움도 증가할 수 있으며, 이는 Top-10 컨텍스트를 사용하여 열등한 결과를 초래할 수 있다.</p>
</div>
<div class="ltx_para" id="S6.SS4.p3">
<p class="ltx_p" id="S6.SS4.p3.9">둘째, 상위 5개 맥락의 서로 다른 순서를 사용하는 것이 결과에 어떤 영향을 미치는지 연구한다. 순차 순서(<math alttext="1" class="ltx_Math" display="inline" id="S6.SS4.p3.1.m1.1"><semantics id="S6.SS4.p3.1.m1.1a"><mn id="S6.SS4.p3.1.m1.1.1" xref="S6.SS4.p3.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.1.m1.1b"><cn id="S6.SS4.p3.1.m1.1.1.cmml" type="integer" xref="S6.SS4.p3.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.1.m1.1d">1</annotation></semantics></math>st context to <math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p3.2.m2.1"><semantics id="S6.SS4.p3.2.m2.1a"><mn id="S6.SS4.p3.2.m2.1.1" xref="S6.SS4.p3.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.2.m2.1b"><cn id="S6.SS4.p3.2.m2.1.1.cmml" type="integer" xref="S6.SS4.p3.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.2.m2.1d">5</annotation></semantics></math>th context)를 역 순서(<math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p3.3.m3.1"><semantics id="S6.SS4.p3.3.m3.1a"><mn id="S6.SS4.p3.3.m3.1.1" xref="S6.SS4.p3.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.3.m3.1b"><cn id="S6.SS4.p3.3.m3.1.1.cmml" type="integer" xref="S6.SS4.p3.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.3.m3.1c">5</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.3.m3.1d">5</annotation></semantics></math>th to <math alttext="1" class="ltx_Math" display="inline" id="S6.SS4.p3.4.m4.1"><semantics id="S6.SS4.p3.4.m4.1a"><mn id="S6.SS4.p3.4.m4.1.1" xref="S6.SS4.p3.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.4.m4.1b"><cn id="S6.SS4.p3.4.m4.1.1.cmml" type="integer" xref="S6.SS4.p3.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.4.m4.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.4.m4.1d">1</annotation></semantics></math>st context)로 비교하고, 스윙 순서(“중간에서 잃어버린” 현상이 주어졌을 때, 입력 컨텍스트의 시작과 끝에 나타날 가장 관련성이 높은 컨텍스트를 배열한다. 따라서, 오더링은 {<math alttext="1" class="ltx_Math" display="inline" id="S6.SS4.p3.5.m5.1"><semantics id="S6.SS4.p3.5.m5.1a"><mn id="S6.SS4.p3.5.m5.1.1" xref="S6.SS4.p3.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.5.m5.1b"><cn id="S6.SS4.p3.5.m5.1.1.cmml" type="integer" xref="S6.SS4.p3.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.5.m5.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.5.m5.1d">1</annotation></semantics></math>st, <math alttext="3" class="ltx_Math" display="inline" id="S6.SS4.p3.6.m6.1"><semantics id="S6.SS4.p3.6.m6.1a"><mn id="S6.SS4.p3.6.m6.1.1" xref="S6.SS4.p3.6.m6.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.6.m6.1b"><cn id="S6.SS4.p3.6.m6.1.1.cmml" type="integer" xref="S6.SS4.p3.6.m6.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.6.m6.1c">3</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.6.m6.1d">3</annotation></semantics></math>rd, <math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p3.7.m7.1"><semantics id="S6.SS4.p3.7.m7.1a"><mn id="S6.SS4.p3.7.m7.1.1" xref="S6.SS4.p3.7.m7.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.7.m7.1b"><cn id="S6.SS4.p3.7.m7.1.1.cmml" type="integer" xref="S6.SS4.p3.7.m7.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.7.m7.1c">5</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.7.m7.1d">5</annotation></semantics></math>th, <math alttext="4" class="ltx_Math" display="inline" id="S6.SS4.p3.8.m8.1"><semantics id="S6.SS4.p3.8.m8.1a"><mn id="S6.SS4.p3.8.m8.1.1" xref="S6.SS4.p3.8.m8.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.8.m8.1b"><cn id="S6.SS4.p3.8.m8.1.1.cmml" type="integer" xref="S6.SS4.p3.8.m8.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.8.m8.1c">4</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.8.m8.1d">4</annotation></semantics></math>th, <math alttext="2" class="ltx_Math" display="inline" id="S6.SS4.p3.9.m9.1"><semantics id="S6.SS4.p3.9.m9.1a"><mn id="S6.SS4.p3.9.m9.1.1" xref="S6.SS4.p3.9.m9.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.9.m9.1b"><cn id="S6.SS4.p3.9.m9.1.1.cmml" type="integer" xref="S6.SS4.p3.9.m9.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.9.m9.1c">2</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.9.m9.1d">2</annotation></semantics></math>nd}가 되고, 랜덤 오더링(랜덤 셔플 the top-5 context)이 된다. 순차 순서를 사용하는 것이 역방향 순서와 스윙 순서를 사용하는 것과 비슷하고 무작위 셔플링이 약간 더 나쁘다는 것을 발견했다. 결과는 우리의 모델이 답의 위치에 관계없이 긴 컨텍스트에서 정답을 추출하는 데 탁월하다는 것을 나타낸다. ChatQA 미세 조정 과정에서 맥락 내에서 답의 위치가 무작위로 발생하기 때문이다.</p>
</div>
<div class="ltx_para" id="S6.SS4.p4">
<p class="ltx_p" id="S6.SS4.p4.1">셋째, “Dragon + Fine-tune”을 원래의 non-finetuned Dragon retriever로 교체하면 평균 점수가 1.81(42.31에서 40.50) 하락하는 것을 관찰할 수 있다. 또한, INSCIT 데이터 세트에서 (33.98에서 27.87로) 두 검색기 사이의 큰 성능 차이로 인해 점수가 크게 떨어진다(표 <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">1</span></a>에 나타낸 바와 같이). 기본적으로 검색의 품질이 향상되면 질의 응답의 성능을 직접적으로 향상시킨다.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Evaluation of Unanswerable Case</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S6.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.1 </span>Evaluation Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS5.SSS1.p1">
<p class="ltx_p" id="S6.SS5.SSS1.p1.1">이 절에서는 제공된 맥락 내에서 질문에 답할 수 있는지 식별하는 모델의 능력의 또 다른 측면을 연구한다. 답할 수 없는 경우에 답을 생성하는 것은 환각을 초래할 것이다. 이 평가를 허용하기 위해 주어진 맥락에서 답을 찾을 수 없을 때 모델을 표시해야 한다.</p>
</div>
<div class="ltx_para" id="S6.SS5.SSS1.p2">
<p class="ltx_p" id="S6.SS5.SSS1.p2.1">QAC 및 DoQA 데이터 세트를 사용하여 이러한 성능을 평가합니다. 구체적으로, 응답할 수 없는 경우에 대해, 질문이 정답일 수 없다는 것을 나타내는 모델 <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We collect a set of heuristic matching patterns from all generated samples to determine if the model suggests that the question is unanswerable. More details can be found in the Appendix <a class="ltx_ref" href="#A5" title="Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">E</span></a>.</span></span></span>을 고려하고, 응답할 수 있는 경우에 대해, 질문이 정답일 수 없다는 것을 나타내지 않는 모델(즉, 답변을 주는 모델)을 고려한다. 답변 가능한 경우 올바른 컨텍스트가 검색되는 샘플만 선택합니다. 우리의 모델 체크포인트(ChatQA-70B w/1.5k unanswerable 및 ChatQA-70B w/SyntheticConvQA)는 표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에 보고된 것과 동일하다.</p>
</div>
<div class="ltx_para" id="S6.SS5.SSS1.p3">
<p class="ltx_p" id="S6.SS5.SSS1.p3.1">결국, 우리는 답할 수 없는 경우와 답할 수 없는 경우의 평균 정확도 점수를 최종 메트릭으로 계산한다. 이 평균 정확도는 정확도와 재현율 점수의 조화 평균을 측정하는 F1 메트릭과 동일하기 때문에 신뢰할 수 있는 메트릭으로 간주한다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.2 </span>Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS5.SSS2.p1">
<p class="ltx_p" id="S6.SS5.SSS2.p1.1">표 <a class="ltx_ref" href="#S6.T7" title="Table 7 ‣ 6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">7</span></a>에서 우리는 QuAC 및 DoQA 데이터 세트에 걸쳐 OpenAI 모델과 모델을 비교한다. 먼저, HumanAnnotatedConvQA를 사용하면 SyntheticConvQA를 사용하는 것과 비교하여 QuAC와 DoQA 모두에서 평균 정확도가 크게 증가한다는 것을 발견했다. 이는 사람이 주석을 달지 않은 데이터에 대해 응답할 수 없는 주석이 더 품질이 높기 때문에 응답할 수 없는 경우에 대한 정확도가 크게 향상되기 때문이다. 둘째, OpenAI 모델은 GPT-4에서 가장 높은 성능을 보였으며, GPT-3.5-turbo 모델에 비해 평균 정확도는 크게 향상되었으나, GPT-4(약 3.5%)에 비해서는 다소 차이가 있었다. 셋째, 응답할 수 없는 표본에서 더 높은 정확도를 달성하는 모델은 응답할 수 있는 표본에서 더 낮은 정확도를 얻는 경향이 있고 그 반대의 경우도 마찬가지임을 알 수 있다. 우리는 모델이 "공격적인" 경향이 있고 답할 수 없는 질문에 대해 다소 적절한 답변을 제공할 때 답할 수 있는 경우에 대한 정확도를 높이지만 답할 수 없는 경우에 대한 정확도를 감소시킬 것이라고 추측한다. 반대로 어떤 모델이 더 '보수적'이고 질문에 답할 수 있는지 엄격하게 체크하면 반대의 효과가 발생한다.</p>
</div>
<div class="ltx_para" id="S6.SS5.SSS2.p2">
<p class="ltx_p" id="S6.SS5.SSS2.p2.1">우리는 HumanAnnotatedConvQA에서 응답할 수 없는 샘플의 수로 절제 연구를 수행한다. 그 결과, 1.5k의 미응답 샘플을 사용하는 것이 전체 대화 QA 점수(Avg-CQA)뿐만 아니라 미응답 평가에서도 놀라운 결과를 얻을 수 있음을 알 수 있었다. 흥미롭게도, 더 많은 응답할 수 없는 샘플을 통합하는 것이 반드시 응답할 수 없는 평가에서 더 높은 정확도 점수로 이어지는 것은 아니다.</p>
</div>
<figure class="ltx_table" id="S6.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T8.1" style="width:976.9pt;height:670.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T8.1.1"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.1.1.1" style="color:#000000;">Context</span></span> <span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.1. If you’re appealing a VA decision</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.2.2.1.1.1" style="color:#000000;">for a hearing. <span> <span class="ltx_text" id="S6.T8.1.1.1.1.1.1.2.2.1.3.1.2" style="color:#0000FF;">You’ll follow the new process if you’re : Appealing a VA decision</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1" style="color:#0000FF;">Participating in the Rapid Appeals</span></span> <span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="color:#000000;">Participating in the Rapid Appeals</span></span> <span class="ltx_text" id="S6.T8 <span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.2"><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.2.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.2.1.1" style="color:#000000;">Assistant: Do you want to learn exactly what to expect? <span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.2.2.2.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.2.2.2.1.3.1" style="color:#000000;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.1.2.2.2.1.3.1" style="color:#000000;"><span class="ltx_td</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.1.1.1.2.2.1.1. #000000;">Assistant: You can request a video conference hearing if you want</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.1.2.2.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.2.2.2.1.1.5.1" style="color:#000000;">to provide testimony to a judge about your appeal. <span class="ltx_tr" id="S6.T8.1.1.1.1.1.2.2.2.1.7"><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.2.2.2.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.2.2.2.1.7.1.1" style="color:#000000;">User: When will I follow the new process for a Board hearing; <span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tan></span></span></span> <span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.2.2.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.1.1.2.2.3.1" style="padding-top:0.15pt;"><span class="ltx_tan></span></span></span #41B729;">GPT-4:</span><span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.2.2.3.1.3.1.1" style="color:#000000;">GPT-4:</span></span><span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.1.2.2.3.1.1.1" style="color:#41B729;">Span class="ltx_tan></span></span><span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.1.1.2.3.1.1.1.1.1" style="color:#41B729;">Span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.1. </span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.1.2.2"><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.3.3.1.1.1.2.1.1" style="color:#000000;">There was no time or intention to turn her to White Walker. Let us look the</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.3.3.1.1.1.1.1.1.3.1.1.1.3.1.1.3" style="color:#0000FF;">scene from S04E04.</span><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.3.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.3.3.1.1.1.1.1.4.1" style="color:#0000FF;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.1.1.1. Arya</span></span></span><span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.5""><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.3.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.3.3.1.1.1.5.1" style="color:#000000;">drops the dagger, Night King looks surprised. 이는 he doesn't't know what</span></span></span><span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.6"><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.3.3.1.1.6.1.1" style="color:#000000;">would happen next. 생각할 시간이 없었고 <span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.1.7"> <span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.1.1.7.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.3.3.1.1.7.1" 스타일="color:#000000;">touch에 의해. 이 장면에서는 S04E04와 같은 계획이 없었습니다. …</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.3.3.2" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.1.3.3.2.1.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1. #000000;">Assistant: Arya was quick enough to react, drop the dagger and</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.3.3.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.3.3.2.1.1" style="color:#000000;">do the rest. <span class="ltx_tr" id="S6.T8.1.1.1.1.1.3.3.2.1.4"><span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.3.3.2.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.3.3.2.1.4.1.1" style="color:#000000;">User: How old is the child that lay on the altar of ice; <span class="ltx_tbular ltx_align_left" id="S6.T8.1.1.1.1.1.1.3.3.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tbular ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.3.3.1.1" style="padding-top:0.15pt;"><span class="ltx_tbular ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.3.3.2.1.1.5" style="padding-bottom:0.15pt;"><span class="ltx_tbular ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.1.1.1.1.1 <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.3.3.1.2.1" style="color:#41B729;">GPT-4:</span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.3.3.1.2.1" style="color:#41B729;">The text does not provide the exact age of the</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.1.3.3.1.1.2" style="padding-top:0.15pt;">The text does not provide the exact age of the</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.1.3.3.1.2.1.2" style="color:#000000;">The (../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../ <span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.4.4.2.1.1" style="color:#000000;">Assistant: Chornohora, Uholka / Wide Meadow, Svydovets,</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.4.4.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.4.4.2.1.1.3.1" style="color:#000000;">Maramoros, Kuziy / Trybushany</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1 <span class="ltx_tan></span></span> <span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.4.4.2.1.1" style="color:#000000;"><span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.1.1.1.1.4.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tan></span></span></span></span> <span class="ltx_tabular ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.1.4.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt; #000000;">Assistant: 24 April 1944, 27 May 1944, 18 August 1944, 13 September</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.6.6.2.1.3.1" style="color:#000000;"><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.6.6.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.6.6.2.1.1.1.4.1" style="padding-top:0.15pt;"><span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.1.1 (../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 8:</span>ChatQA-70B 및 GPT-4에 대한 사례 연구. <span class="ltx_text" id="S6.T8.5.1" style="color:#0000FF;">blue</span> color를 사용하여 마지막 사용자 질문에 대한 관련 컨텍스트를 강조 표시합니다. 그리고 <span class="ltx_text" id="S6.T8.6.2" style="color:#41B729;">green</span> and <span class="ltx_text" id="S6.T8.7.3" style="color:#FF0000;">red</span> color를 사용하여 어시스턴트 응답에서 올바르고 잘못된 부분을 강조 표시합니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Case Study</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS6.p1">
<p class="ltx_p" id="S6.SS6.p1.1">표 <a class="ltx_ref" href="#S6.T8" title="Table 8 ‣ 6.5.2 Results ‣ 6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">8</span></a>에서 ChatQA-70B 및 GPT-4 출력의 네 가지 예를 보여준다. <span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>More examples can be found in Appendix <a class="ltx_ref" href="#A6" title="Appendix F Case Study ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">F</span></a>.</span></span></span></p>
</div>
<div class="ltx_para" id="S6.SS6.p2">
<p class="ltx_p" id="S6.SS6.p2.1">첫 번째 예제(약 <span class="ltx_text ltx_font_italic" id="S6.SS6.p2.1.1">Board hearing</span>)는 간단한 정보 탐색 문제이며 ChatQA-70B와 GPT-4 모두 올바르게 응답합니다. 두 번째 예(약 <span class="ltx_text ltx_font_italic" id="S6.SS6.p2.1.2">Arya</span>)에서 모델은 답변을 제공하기 위해 암시적 정보(파란색으로 강조 표시됨)를 찾아야 합니다. GPT-4는 답변을 제공하는 데 안전한 경향이 있으며 맥락이 연령에 대한 정확한 정보를 제공하지 않는다는 답변이 정확하다.</p>
</div>
<div class="ltx_para" id="S6.SS6.p3">
<p class="ltx_p" id="S6.SS6.p3.1">세 번째 예와 네 번째 예 모두 모델이 좋은 표의 이해와 추론 능력을 가질 것을 요구한다. 세 번째 예제(약 <span class="ltx_text ltx_font_italic" id="S6.SS6.p3.1.1">Massif</span>)에서 ChatQA-70B는 보존 영역의 크기를 3100 ha와 비교하여 정답을 제공하는 반면 GPT-4는 그렇지 않습니다. 네 번째 예(약 <span class="ltx_text ltx_font_italic" id="S6.SS6.p3.1.2">John B. England</span>)에서 ChatQA-70B는 세 날짜를 올바르게 나열하지만 한 날짜를 놓치는 반면 GPT-4는 질문에 올바르게 응답합니다.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">이 논문에서 우리는 7B에서 70B까지 모델 크기가 다양한 ChatQA 모델 패밀리를 구축한다. 10개의 대화형 QA 데이터 세트에 대한 종합적인 평가는 우리의 최상의 ChatQA-70B 모델이 ChatGPT 모델의 합성 데이터를 사용하지 않고 GPT-3.5-터보보다 현저하게 능가하고 GPT-4와 동등하게 수행할 수 있음을 보여준다. 또한, 큐레이션된 대화 QA 데이터를 사용하여 단일 회전 쿼리 리트리버를 미세 조정하면 추가 계산 시간과 재작성으로 인한 잠재적인 API 비용 없이 최신 LLM 기반 쿼리 재작성 모델에 필적하는 성능을 보여준다. 또한 소량의 "응답할 수 없는" 샘플을 통합하면 답변을 사용할 수 없는 시나리오를 처리할 수 있는 모델의 기능이 크게 향상될 수 있음을 보여 줍니다. 응답할 수 없는 사례 평가는 우리의 최상의 모델 ChatQA-70B가 GPT-4와 비교하여 약간의 차이만 있음을 강조한다.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adlakha et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Adlakha, V., Dhuliawala, S., Suleman, K., de&nbsp;Vries, H., and Reddy, S.

</span>
<span class="ltx_bibblock">Topiocqa: Open-domain conversational question answering with topic switching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">TACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aliannejadi et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aliannejadi, M., Kiseleva, J., Chuklin, A., Dalton, J., and Burtsev, M.

</span>
<span class="ltx_bibblock">Building and evaluating open-domain dialogue corpora with clarifying questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">EMNLP</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anantha et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman, S., and Chappidi, S.

</span>
<span class="ltx_bibblock">Open-domain question answering goes conversational via question rewriting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">NAACL</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing 100k context windows, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing Claude, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brabant et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Brabant, Q., Lecorvé, G., and Barahona, L. M.&nbsp;R.

</span>
<span class="ltx_bibblock">Coqar: Question rewriting on coqa.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">LREC</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Campos et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Campos, J.&nbsp;A., Otegi, A., Soroa, A., Deriu, J.&nbsp;M., Cieliebak, M., and Agirre, E.

</span>
<span class="ltx_bibblock">Doqa-accessing domain-specific faqs via conversational qa.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">ACL</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., and Wang, W.&nbsp;Y.

</span>
<span class="ltx_bibblock">Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">EMNLP</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chen, Z., Zhao, J., Fang, A., Fetahu, B., Rokhlenko, O., and Malmasi, S.

</span>
<span class="ltx_bibblock">Reinforced question rewriting for conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">EMNLP</em>, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi, Y., Liang, P., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">Quac: Question answering in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">EMNLP</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chu, Z., Chen, M., Chen, J., Wang, M., Gimpel, K., Faruqui, M., and Si, X.

</span>
<span class="ltx_bibblock">How to ask better questions? a large-scale multi-domain dataset for rewriting ill-formed questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">AAAI</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chung, H.&nbsp;W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S.&nbsp;S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E.&nbsp;H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.&nbsp;V., and Wei, J.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv: 2210.11416</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Conover, M., Hayes, M., Mathur, A., Meng, X., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Free dolly: Introducing the world’s first truly open instruction-tuned llm, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R.

</span>
<span class="ltx_bibblock">Free Dolly: Introducing the world’s first truly open instruction-tuned llm, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dai, Z., Chaganty, A.&nbsp;T., Zhao, V., Amini, A., Green, M., Rashid, Q., and Guu, K.

</span>
<span class="ltx_bibblock">Dialog inpainting: Turning documents to dialogs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ICML</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasigi et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dasigi, P., Liu, N.&nbsp;F., Marasović, A., Smith, N.&nbsp;A., and Gardner, M.

</span>
<span class="ltx_bibblock">Quoref: A reading comprehension dataset with questions requiring coreferential reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">EMNLP</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Del&nbsp;Tredici et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Del&nbsp;Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and de&nbsp;Gispert, A.

</span>
<span class="ltx_bibblock">Question rewriting for open-domain conversational qa: Best practices and limitations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CIKM</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Deng, Y., Lei, W., Zhang, W., Lam, W., and Chua, T.-S.

</span>
<span class="ltx_bibblock">Pacific: Towards proactive conversational question answering over tabular and textual data in finance.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J.

</span>
<span class="ltx_bibblock">Glm: General language model pretraining with autoregressive blank infilling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M.

</span>
<span class="ltx_bibblock">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">NAACL</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elgohary et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elgohary, A., Peskov, D., and Boyd-Graber, J.

</span>
<span class="ltx_bibblock">Can you unpack that? learning to rewrite questions-in-context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">EMNLP</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and Auli, M.

</span>
<span class="ltx_bibblock">Eli5: Long form question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">ACL</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Feng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and Lastras, L.

</span>
<span class="ltx_bibblock">doc2dial: A goal-oriented document-grounded dialogue dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">EMNLP</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galimzhanova et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Galimzhanova, E., Muntean, C.&nbsp;I., Nardini, F.&nbsp;M., Perego, R., and Rocchietti, G.

</span>
<span class="ltx_bibblock">Rewriting conversational utterances with instructed large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gao, C., Zhang, W., and Lam, W.

</span>
<span class="ltx_bibblock">Unigdd: A unified generative framework for goal-oriented document-grounded dialogue.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Introducing bard, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guo, M., Zhang, M., Reddy, S., and Alikhani, M.

</span>
<span class="ltx_bibblock">Abg-coqa: Clarifying ambiguity in conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">AKBC</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Honovich, O., Scialom, T., Levy, O., and Schick, T.

</span>
<span class="ltx_bibblock">Unnatural instructions: Tuning language models with (almost) no human labor.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2212.09689</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ishii et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ishii, E., Xu, Y., Cahyawijaya, S., and Wilie, B.

</span>
<span class="ltx_bibblock">Can question rewriting help conversational question answering?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the Third Workshop on Insights from Negative Results in NLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Iyer, S., Lin, X.&nbsp;V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P.&nbsp;S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Opt-iml: Scaling language model instruction meta learning through the lens of generalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2212.12017</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard &amp; Grave (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Izacard, G. and Grave, É.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y., Zhou, P., Bras, R.&nbsp;L., Alikhani, M., Kim, G., Sap, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Soda: Million-scale dialogue distillation with social commonsense contextualization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2212.10465</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kočiskỳ et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kočiskỳ, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.&nbsp;M., Melis, G., and Grefenstette, E.

</span>
<span class="ltx_bibblock">The narrativeqa reading comprehension challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">TACL</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.&nbsp;M., Stanley, O., Nagyfi, R., et&nbsp;al.

</span>
<span class="ltx_bibblock">Openassistant conversations–democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2304.07327</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.&nbsp;M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A.

</span>
<span class="ltx_bibblock">Openassistant conversations - democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv: 2304.07327</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, K., Tafjord, O., Clark, P., and Gardner, M.

</span>
<span class="ltx_bibblock">Reasoning over paragraph effects in situations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y., Yih, W.-t., and Chen, X.

</span>
<span class="ltx_bibblock">How to train your dragon: Diverse augmentation towards generalizable dense retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2302.07452</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, X.&nbsp;V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2310.01352</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Liu, N.&nbsp;F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2307.03172</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.&nbsp;W., Tay, Y., Zhou, D., Le, Q.&nbsp;V., Zoph, B., Wei, J., et&nbsp;al.

</span>
<span class="ltx_bibblock">The flan collection: Designing data and methods for effective instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2301.13688</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mele et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mele, I., Muntean, C.&nbsp;I., Nardini, F.&nbsp;M., Perego, R., Tonellotto, N., and Frieder, O.

</span>
<span class="ltx_bibblock">Adaptive utterance rewriting for conversational search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Information Processing &amp; Management</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mo, F., Mao, K., Zhu, Y., Wu, Y., Huang, K., and Nie, J.-Y.

</span>
<span class="ltx_bibblock">Convgqr: Generative query reformulation for conversational search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2305.15645</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Scao, T.&nbsp;L., Bari, M.&nbsp;S., Shen, S., Yong, Z.-X., Schoelkopf, H., et&nbsp;al.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2211.01786</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakamura et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nakamura, K., Levy, S., Tuan, Y.-L., Chen, W., and Wang, W.&nbsp;Y.

</span>
<span class="ltx_bibblock">Hybridialogue: An information-seeking dialogue dataset grounded on tabular and textual data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Findings of ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L.

</span>
<span class="ltx_bibblock">Ms marco: A human generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">choice</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing ChatGPT, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">NeurIPS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat &amp; Liang (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pasupat, P. and Liang, P.

</span>
<span class="ltx_bibblock">Compositional semantic parsing on semi-structured tables.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">ACL</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qu, C., Yang, L., Chen, C., Qiu, M., Croft, W.&nbsp;B., and Iyyer, M.

</span>
<span class="ltx_bibblock">Open-retrieval conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">SIGIR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.

</span>
<span class="ltx_bibblock">Squad: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">EMNLP</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rajpurkar, P., Jia, R., and Liang, P.

</span>
<span class="ltx_bibblock">Know what you don’t know: Unanswerable questions for squad.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ACL</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raposo et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Raposo, G., Ribeiro, R., Martins, B., and Coheur, L.

</span>
<span class="ltx_bibblock">Question rewriting? assessing its importance for conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">ECIR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Reddy, S., Chen, D., and Manning, C.&nbsp;D.

</span>
<span class="ltx_bibblock">Coqa: A conversational question answering challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">TACL</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saeidi et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Saeidi, M., Bartolo, M., Lewis, P., Singh, S., Rocktäschel, T., Sheldon, M., Bouchard, G., and Riedel, S.

</span>
<span class="ltx_bibblock">Interpretation of natural language rules in conversational machine reading.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">EMNLP</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sanh, V., Webson, A., Raffel, C., Bach, S.&nbsp;H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T.&nbsp;L., Raja, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">ICLR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trischler et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K.

</span>
<span class="ltx_bibblock">Newsqa: A machine comprehension dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 2nd Workshop on Representation Learning for NLP</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vakulenko et&nbsp;al. (2021a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vakulenko, S., Longpre, S., Tu, Z., and Anantha, R.

</span>
<span class="ltx_bibblock">Question rewriting for conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">WSDM</em>, 2021a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vakulenko et&nbsp;al. (2021b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vakulenko, S., Voskarides, N., Tu, Z., and Longpre, S.

</span>
<span class="ltx_bibblock">A comparison of question rewriting methods for conversational passage retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">ECIR</em>, 2021b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Instructretro: Instruction tuning post retrieval-augmented pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2310.07713</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F.

</span>
<span class="ltx_bibblock">Text embeddings by weakly-supervised contrastive pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2212.03533</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.&nbsp;A., Khashabi, D., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language model with self generated instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2212.10560</em>, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A.&nbsp;S., Arunkumar, A., Stap, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">EMNLP</em>, 2022c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K.&nbsp;R., Wadden, D., MacMillan, K., Smith, N.&nbsp;A., Beltagy, I., et&nbsp;al.

</span>
<span class="ltx_bibblock">How far can camels go? exploring the state of instruction tuning on open resources.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2306.04751</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei, J., Bosma, M., Zhao, V.&nbsp;Y., Guu, K., Yu, A.&nbsp;W., Lester, B., Du, N., Dai, A.&nbsp;M., and Le, Q.&nbsp;V.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">ICLR</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.&nbsp;V., Zhou, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">NeurIPS</em>, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wu, Z., Luan, Y., Rashkin, H., Reitter, D., Hajishirzi, H., Ostendorf, M., and Tomar, G.&nbsp;S.

</span>
<span class="ltx_bibblock">Conqrr: Conversational query rewriting for retrieval with reinforcement learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wu, Z., Parish, R., Cheng, H., Min, S., Ammanabrolu, P., Ostendorf, M., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Inscit: Information-seeking conversations with mixed-initiative interactions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">TACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2304.12244</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Retrieval meets long context large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2310.03025</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ye, F., Fang, M., Li, S., and Yilmaz, E.

</span>
<span class="ltx_bibblock">Enhancing conversational search: Large language model-aided informative query rewriting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">EMNLP</em>, pp.&nbsp; 5985–6006, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yu, S., Liu, J., Yang, J., Xiong, C., Bennett, P., Gao, J., and Liu, Z.

</span>
<span class="ltx_bibblock">Few-shot generative conversational query rewriting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">SIGIR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et&nbsp;al.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2308.10792</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et&nbsp;al.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2305.11206</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., and Chua, T.-S.

</span>
<span class="ltx_bibblock">Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">ACL</em>, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>ChatQA Instruction Tuning</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Stage-1: Supervised Fine-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">스테이지-1에서 LLM 입력의 포맷 템플릿은 다음과 같다:</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p2.1">System: This is a chat between a user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user’s questions
based on the context. The assistant should also indicate when the answer cannot be
found in the context.

User: {Question 1}

Assistant: {Answer 1}

...

User: {Latest Question}

Assistant:
</pre>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1">모델 출력에 대한 감독으로 어시스턴트의 <math alttext="\{\texttt{Latest Answer}\}" class="ltx_Math" display="inline" id="A1.SS1.p3.1.m1.1"><semantics id="A1.SS1.p3.1.m1.1a"><mrow id="A1.SS1.p3.1.m1.1.2.2" xref="A1.SS1.p3.1.m1.1.2.1.cmml"><mo id="A1.SS1.p3.1.m1.1.2.2.1" stretchy="false" xref="A1.SS1.p3.1.m1.1.2.1.cmml">{</mo><mtext id="A1.SS1.p3.1.m1.1.1" mathvariant="monospace" xref="A1.SS1.p3.1.m1.1.1a.cmml">Latest Answer</mtext><mo id="A1.SS1.p3.1.m1.1.2.2.2" stretchy="false" xref="A1.SS1.p3.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.1.m1.1b"><set id="A1.SS1.p3.1.m1.1.2.1.cmml" xref="A1.SS1.p3.1.m1.1.2.2"><ci id="A1.SS1.p3.1.m1.1.1a.cmml" xref="A1.SS1.p3.1.m1.1.1"><mtext id="A1.SS1.p3.1.m1.1.1.cmml" mathvariant="monospace" xref="A1.SS1.p3.1.m1.1.1">Latest Answer</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.1.m1.1c">\{\texttt{Latest Answer}\}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p3.1.m1.1d">{ Latest Answer }</annotation></semantics></math>를 사용합니다.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Stage-2: Context-Enhanced Instruction Tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.2">스테이지-1 형식 템플릿을 기반으로, 스테이지-2의 LLM 입력은 아래에 묘사된 바와 같이 <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p1.2.1">{Context for Latest Question}</span> 및 <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p1.2.2">{Instruction}</span> from <span class="ltx_text ltx_font_typewriter" id="A1.SS2.p1.2.3">User</span>을 추가합니다.</p>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS2.p1.3">System: This is a chat between a user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user’s questions
based on the context. The assistant should also indicate when the answer cannot be
found in the context.

{Context for Latest Question}

User: {Instruction} + {Question 1}

Assistant: {Answer 1}

...

User: {Latest Question}

Assistant:
</pre>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1">스테이지-1과 마찬가지로 모델 출력에 대한 감독으로 어시스턴트의 <math alttext="\{\texttt{Latest Answer}\}" class="ltx_Math" display="inline" id="A1.SS2.p2.1.m1.1"><semantics id="A1.SS2.p2.1.m1.1a"><mrow id="A1.SS2.p2.1.m1.1.2.2" xref="A1.SS2.p2.1.m1.1.2.1.cmml"><mo id="A1.SS2.p2.1.m1.1.2.2.1" stretchy="false" xref="A1.SS2.p2.1.m1.1.2.1.cmml">{</mo><mtext id="A1.SS2.p2.1.m1.1.1" mathvariant="monospace" xref="A1.SS2.p2.1.m1.1.1a.cmml">Latest Answer</mtext><mo id="A1.SS2.p2.1.m1.1.2.2.2" stretchy="false" xref="A1.SS2.p2.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.1.m1.1b"><set id="A1.SS2.p2.1.m1.1.2.1.cmml" xref="A1.SS2.p2.1.m1.1.2.2"><ci id="A1.SS2.p2.1.m1.1.1a.cmml" xref="A1.SS2.p2.1.m1.1.1"><mtext id="A1.SS2.p2.1.m1.1.1.cmml" mathvariant="monospace" xref="A1.SS2.p2.1.m1.1.1">Latest Answer</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.1.m1.1c">\{\texttt{Latest Answer}\}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.1.m1.1d">{ Latest Answer }</annotation></semantics></math>를 사용한다.</p>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1"><math alttext="\{\texttt{Instruction}\}" class="ltx_Math" display="inline" id="A1.SS2.p3.1.m1.1"><semantics id="A1.SS2.p3.1.m1.1a"><mrow id="A1.SS2.p3.1.m1.1.2.2" xref="A1.SS2.p3.1.m1.1.2.1.cmml"><mo id="A1.SS2.p3.1.m1.1.2.2.1" stretchy="false" xref="A1.SS2.p3.1.m1.1.2.1.cmml">{</mo><mtext id="A1.SS2.p3.1.m1.1.1" xref="A1.SS2.p3.1.m1.1.1a.cmml">𝙸𝚗𝚜𝚝𝚛𝚞𝚌𝚝𝚒𝚘𝚗</mtext><mo id="A1.SS2.p3.1.m1.1.2.2.2" stretchy="false" xref="A1.SS2.p3.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.1.m1.1b"><set id="A1.SS2.p3.1.m1.1.2.1.cmml" xref="A1.SS2.p3.1.m1.1.2.2"><ci id="A1.SS2.p3.1.m1.1.1a.cmml" xref="A1.SS2.p3.1.m1.1.1"><mtext id="A1.SS2.p3.1.m1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1">𝙸𝚗𝚜𝚝𝚛𝚞𝚌𝚝𝚒𝚘𝚗</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.1.m1.1c">\{\texttt{Instruction}\}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p3.1.m1.1d">{ Instruction }</annotation></semantics></math>는 답변 유형에 따라 서로 다른 데이터 세트에 대해 서로 다른 지침을 사용합니다. 자세한 내용은 아래에서 확인할 수 있습니다.</p>
</div>
<div class="ltx_para" id="A1.SS2.p4">
<p class="ltx_p" id="A1.SS2.p4.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p4.1.1">Please give a full and complete answer for the question.</span>” 이것은 긴 답변이 있는 데이터 세트에 대한 것입니다. 우리는 HumanAnnotatedConvQA 또는 SyntheticConvQA에 사용합니다.</p>
</div>
<div class="ltx_para" id="A1.SS2.p5">
<p class="ltx_p" id="A1.SS2.p5.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p5.1.1">Answer the following question with a short span. The answer needs to be just in a few words.</span>” 이것은 짧은 답변을 가진 데이터 세트에 대한 것입니다. 우리는 그것을 SQuAD1.1, SQuAD2.0, NarrativeQA, DROP, ROPES, NewsQA, Quoref에 사용한다.</p>
</div>
<div class="ltx_para" id="A1.SS2.p6">
<p class="ltx_p" id="A1.SS2.p6.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p6.1.1">Answer a number from context or the math arithmetic using +, -, *, or /.</span>” 이것은 산술 계산을 사용하거나 컨텍스트에서 숫자를 추출하는 데이터 세트에 대한 것이다. 우리는 문제가 산술 계산이나 문맥에서 숫자를 추출해야 하는 TAT-QA 데이터 세트에 사용한다.</p>
</div>
<div class="ltx_para" id="A1.SS2.p7">
<p class="ltx_p" id="A1.SS2.p7.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p7.1.1">Answer the following question with a short span, or a full and complete answer.</span>” 이것은 짧은 답과 긴 답 모두를 가진 데이터 세트에 대한 것이다. 모델은 질문을 기반으로 단답형 또는 장답형 답변을 생성할지 여부를 알려야 한다. 우리는 해당 답변의 길이가 짧고 길 수 있기 때문에 산술 계산이 필요하지 않은 TAT-QA 데이터 세트에 사용한다.</p>
</div>
<div class="ltx_para" id="A1.SS2.p8">
<p class="ltx_p" id="A1.SS2.p8.2">스테이지-2 명령어 튜닝에 사용되는 스테이지-1 SFT 데이터의 경우, 스테이지-1에서와 동일한 포맷 템플릿을 유지한다. 즉, <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p8.2.1">{Context for Latest Question}</span> 및 <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p8.2.2">{Instruction}</span>은 비어 있다.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Prompts for Synthetic Data Generation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p1.1.1">고품질 멀티턴 QA 대화를 얻기 위해 GPT-3.5-turbo에서 2단계 프롬프트를 사용합니다. </span> 단계-1에서 GPT-3.5-turbo가 주어진 문서를 기반으로 다중 턴 QA 대화를 생성하도록 프롬프트한다. 그러나, 생성된 대화는 대화사에서 단어 또는 구를 다시 참조할 대명사나 보통 명사가 부족하다는 것을 발견했다. 따라서 2단계에서는 대명사나 보통명사를 추가하여 생성된 대화를 수정하도록 GPT-3.5-turbo를 프롬프트한다. 두 단계 모두에 대해 고품질 생성을 보장하기 위해 GPT-3.5-터보 소수의 샷 예를 제공한다.</p>
</div>
<div class="ltx_para" id="A1.SS3.p2">
<p class="ltx_p" id="A1.SS3.p2.1">상기 단계-1에서의 인-컨텍스트 소-샷 프롬프트는 다음과 같다(멀티-턴 QA 대화를 생성하기 위한aims):</p>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS3.p2.2">You are a helpful, pattern-following assistant.

role-user:
PersonA has access to information in the following passage:
###There are certain important aspects of college preparation. The student preparing
for college should define their goals and interests. The student should also plan
financially and academically. We have a checklist that helps students get ready
financially and academically.###
Generate a conversation of around four turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
PersonU: I need to know how to prepare for college.
PersonA: You should first define your goals and interest and be sure to know the
costs and how to plan financially and academically for college.
PersonU: Is there something I can use as a guide to get ready for it?
PersonA: you can use our checklist to get ready financially and academically.
PersonU: Any other tip?
PersonA: Be sure to check out the reasons to attend a College or Career schools.
PersonU: Ok, that sounds good. Can you give me more tips?
PersonA: Another thing to take into consideration is to explore your career options.

role-user:
PersonA has access to information in the following passage:
###There are several disability compensation benefits. One of the benefits is the
annual clothing allowance. This allowance covers the cost of purchasing new clothes
up to a limit of 200 dollars annually.###
Generate a conversation of around two turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
PersonU: I heard about a disability benefit for clothing. What is it about?
PersonA: This is a disability compensation benefit known as an annual clothing
allowance.
PersonU: I am quite interested about such benefits.
PersonA: Would you like to find out if you are eligible?

role-user:
PersonA has access to information in the following passage:
###Vascepa (icosapent ethyl) is a prescription fish oil medication approved for
treating adults with extremely high triglycerides, defined as a triglyceride level
of 500 mg/dL or higher. While using this medicine, you may need frequent blood tests
at your doctor’s office. Vascepa is only part of a treatment program that may also
include diet, exercise, and weight control. Follow your doctor’s instructions very
closely. You should not use Vascepa if you are allergic to icosapent. To make sure
Vascepa is safe for you, tell your doctor if you have: 1 liver disease; 2 diabetes;
3 a thyroid disorder; 4 problems with your pancreas; 5 a bleeding or blood-clotting
disorder; 6 if you are allergic to fish or shellfish; or. 7 if you drink large
amounts of alcohol, Vascepa (icosapent) works in the liver and bloodstream to reduce
very low-density triglycerides.###
Generate a conversation of around three turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
PersonU: I heard about a medication called Vascepa. What is it used for?
PersonA: Vascepa is a prescription fish oil medication used for treating adults with
extremely high triglyceride levels.
PersonU: Are there any side effects or precautions I should be aware of?
PersonA: While using Vascepa, you may need frequent blood tests, and it’s important
to follow your doctor’s instructions closely. It’s important to talk to your doctor
about any allergies or existing conditions you have.
PersonU: Is it effective in lowering cholesterol levels?
PersonA: Vascepa is used together with a low-fat diet and other treatments to lower
total cholesterol in people with severe high cholesterol.

role-user:
PersonA has access to information in the following passage:

{Document}

Generate a conversation of around three turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
</pre>
</div>
<div class="ltx_para" id="A1.SS3.p3">
<p class="ltx_p" id="A1.SS3.p3.1">상기 단계-2에서 인컨텍스트 소샷 프롬프트는 다음과 같다(대명사와 보통명사를 추가하여 멀티턴 QA 대화를 수정하고자 함):</p>
</div>
<div class="ltx_para" id="A1.SS3.p4">
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS3.p4.1">You are a helpful, pattern-following assistant.

role-user:
Given the following conversation between PersonU and PersonA:
###PersonU: I’ve heard about adobo sauce in Mexican cuisine. Can you tell me more
about it?
PersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and
vinegar. It’s used as a marinade and cooking sauce in Mexican and Southwestern
cuisines.
PersonU: What are the main ingredients in adobo sauce?
PersonA: The main ingredients are typically chile peppers, garlic, and vinegar.
However, there can be variations in the recipe.
PersonU: How can I make adobo sauce at home?
PersonA: One way to make adobo sauce at home is by using smoked jalapeno peppers
called chipotles. You can stew the chipotles in a sauce with tomatoes, garlic,
vinegar, salt, and spices. It’s also possible to find canned adobo sauce in the
Hispanic foods section of many grocery stores.
PersonU: Are there any other uses for adobo sauce?
PersonA: Adobo sauce can be used as a condiment or as a marinade for various dishes.
It enhances the flavor of meats and can be used as a cooking and serving sauce.
Additionally, adobo is also used in a stew-like presentation of meat prepared in
this sauce.
PersonU: Is adobo sauce easy to preserve?
PersonA: Yes, you can preserve adobo sauce by freezing or canning it when you’re
done making it. This allows you to store it for up to a year and enjoy its flavors
whenever you want.###
Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.

role-assistant:
PersonU: I’ve heard about adobo sauce in Mexican cuisine. Can you provide more
information about it?
PersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and vinegar.
It’s used as a marinade and cooking sauce in Mexican and Southwestern cuisines.
PersonU: What are the main ingredients in the sauce?
PersonA: The main ingredients are typically chile peppers, garlic, and vinegar.
However, there can be variations in the recipe.
PersonU: How can I make the sauce at home?
PersonA: One way to make it at home is by using smoked jalapeno peppers called
chipotles. You can stew them in a sauce with tomatoes, garlic, vinegar, salt, and
spices. It’s also possible to find canned adobo sauce in the Hispanic foods section
of many grocery stores.
PersonU: Are there any other uses for it?
PersonA: It can be used as a condiment or as a marinade for various dishes. It
enhances the flavor of meats and can be used as a cooking and serving sauce.
Additionally, it is also used in a stew-like presentation of meat prepared in
this sauce.
PersonU: Is it easy to preserve?
PersonA: Yes, you can preserve it by freezing or canning it when you’re done
making it. This allows you to store it for up to a year and enjoy its flavors
whenever you want.

role-user:
Given the following conversation between PersonU and PersonA:
###PersonU: I’ve heard about a test called urinalysis. What is it used for?
PersonA: Urinalysis is a test that evaluates a sample of your urine. It is used to
detect and assess a wide range of disorders, such as urinary tract infection,
kidney disease, and diabetes. It examines the appearance, concentration, and content
of urine.
PersonU: Can it help diagnose bladder cancer?
PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for
bladder cancer. One way to test for bladder cancer is to check for blood in the
urine, which is called hematuria. This can be done during a urinalysis by examining
the urine cells under a special microscope.
PersonU: What other conditions or diseases can be detected through urinalysis?
PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,
evaluate causes of kidney failure, screen for the progression of chronic conditions
like diabetes mellitus and high blood pressure, and it can be used in combination
with other tests to diagnose certain diseases. It is a versatile test that helps
detect abnormalities in the urinary tract.
PersonU: Where can I get a urinalysis done?
PersonA: Urinalysis can be performed in various healthcare settings, including
doctors’ offices, urgent care facilities, laboratories, and hospitals. You can
provide a urine sample in a specimen cup, and the sample can be analyzed on-site
or sent to a laboratory for testing.
PersonU: How much urine is typically needed for a urinalysis?
PersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are required
for urinalysis testing. It’s a non-invasive and relatively simple test that provides
valuable information about your health.###
Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.

role-assistant:
PersonU: I’ve heard about a test called urinalysis. What is it used for?
PersonA: Urinalysis is a test that evaluates a sample of your urine. It’s used to
detect and assess a wide range of disorders, such as urinary tract infection,
kidney disease, and diabetes. It examines the appearance, concentration, and
content of urine.
PersonU: Can it help diagnose bladder cancer?
PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for bladder
cancer. One way to test for bladder cancer is to check for blood in the urine, which
is called hematuria. This can be done during a urinalysis by examining the urine
cells under a special microscope.
PersonU: What other conditions or diseases can be detected through this test?
PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,
evaluate causes of kidney failure, screen for the progression of chronic conditions
like diabetes mellitus and high blood pressure, and it can be used in combination
with other tests to diagnose certain diseases. It is a versatile test that helps
detect abnormalities in the urinary tract.
PersonU: Where can I go to get this test done?
PersonA: Urinalysis can be performed in various healthcare settings, including
doctors’ offices, urgent care facilities, laboratories, and hospitals. You can
provide a urine sample in a specimen cup, and the sample can be analyzed on-site
or sent to a laboratory for testing.
PersonU: How much urine is typically needed for the test?
PersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are
required for urinalysis testing. It’s a non-invasive and relatively simple test
that provides valuable information about your health.

role-user:
Given the following conversation between PersonU and PersonA:

{multi-turn QA conversation}

Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.

role-assistant:


</pre>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>More Details and Results for Retrieval in Conversational QA</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Query Rewriting Prompts for GPT-3.5-turbo</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">GPT-3.5-터보에 사용하는 인컨텍스트 소샷 쿼리 다시 쓰기 프롬프트는 다음과 같습니다.</p>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS1.p2.1">You are a helpful, pattern-following assistant.

role-user:
Given the following conversation between PersonU and PersonA:
PersonU: Hello, I would like to know what to do if I do not agree with any decision.
PersonA: disagree with our decision about your monthly income adjustment amounts?
PersonU: no. Where can I find my SHIP contact information?
PersonA: You can find your local SHIP contact information in the back of your
Medicare &amp; You 2020 Handbook online.
PersonU: and how do they calculate the adjustments?
Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.

role-assistant:
How is the calculation for adjustments made by SHIP determined?

role-user:
Given the following conversation between PersonU and PersonA:
PersonU: I need to know how to prepare for college.
PersonA: You should first define your goals and interest and be sure to know the
costs and how to plan financially and academically for college.
PersonU: Is there something I can use as a guide to get ready for it?
Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.

role-assistant:
What resources or guides can I use to help me prepare for college?

role-user:
Given the following conversation between PersonU and PersonA:

{Dialogue History + Latest Question}

Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.

role-assistant:

</pre>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>More Results for Retrieval in Conversational QA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A2.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T9.1" style="width:526.9pt;height:127pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A2.T9.1.1"><span class="ltx_text" id="A2.T9.1.1.1"> <span class="ltx_tabular ltx_align_middle" id="A2.T9.1.1.1.1"> <span class="ltx_tbody"> <span class="ltx_tr" id="A2.T9.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2" id="A2.T9.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.1.1" style="color:#000000;">Models</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.2.1" style="color:#000000;">Average</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.3.1" style="color:#000000;">Doc2Dial</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.4.1" style="color:#000000;">QuAC</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.5.1" style="color:#000000;">QReCC</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.6.1" style="color:#000000;">TopiOCQA</span></span> <span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.7.1" style="color:#000000;">INSCIT</span></span></span> <span class="ltx_tr" id="A2.T9.1.1.1.1.2.2"> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.1.1" style="color:#000000;">top-1</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.2.1" style="color:#000000;">top-5</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.3.1" style="color:#000000;">top-1</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.4.1" style="color:#000000;">top-5</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.5.1" style="color:#000000;">top-1</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.6.1" style="color:#000000;">top-5</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.7.1" style="color:#000000;">top-1</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.8.1" style="color:#000000;">top-5</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.9.1" style="color:#000000;">top-5*</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.10.1" style="color:#000000;">top-20*</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.11.1" style="color:#000000;">top-5*</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.12.1" style="color:#000000;">top-20*</span></span></span> <span class="ltx_tr" id="A2.T9.1.1.1.1.3.3"> <span class="ltx_td ltx_align_left ltx_border_t" id="A2.T9.1.1.1.1.3.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.1.1" style="color:#000000;">Dragon (w/ dialog history)</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.2.1" style="color:#000000;">46.29</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.3.1" style="color:#000000;">73.09</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.4.1" style="color:#000000;">43.33</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.5.1" style="color:#000000;">75.61</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.6.1" style="color:#000000;">56.8</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.7.1" style="color:#000000;">82.86</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.8.1" style="color:#000000;">46.17</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.9.1" style="color:#000000;">81.96</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.10.1" style="color:#000000;">57.68</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.11.1" style="color:#000000;">78.80</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.12.1" style="color:#000000;">27.49</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.13.1" style="color:#000000;">46.22</span></span></span> <span class="ltx_tr" id="A2.T9.1.1.1.1.4.4"> <span class="ltx_td ltx_align_left" id="A2.T9.1.1.1.1.4.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.1.1" style="color:#000000;">Dragon + Rewrite (w/ dialog history)</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.2.1" style="color:#000000;">47.57</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.3.1" style="color:#000000;">74.12</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.4.1" style="color:#000000;">44.54</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.5.1" style="color:#000000;">76.98</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.6.1" style="color:#000000;">57.23</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.7.1" style="color:#000000;">83.04</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.8.1" style="color:#000000;">46.45</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.9.1" style="color:#000000;">82.60</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.10.1" style="color:#000000;">60.94</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.11.1" style="color:#000000;">81.74</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.12.1" style="color:#000000;">28.69</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.13.1" style="color:#000000;">46.22</span></span></span> <span class="ltx_tr" id="A2.T9.1.1.1.1.5.5"> <span class="ltx_td ltx_align_left" id="A2.T9.1.1.1.1.5.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.1.1" style="color:#000000;">Dragon + Rewrite (w/ single query only)</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.5.5.2.1" style="color:#000000;">54.46</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.5.5.3.1" style="color:#000000;">80.13</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.4.1" style="color:#000000;">47.60</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.5.1" style="color:#000000;">80.60</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.6.1" style="color:#000000;">47.10</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.7.1" style="color:#000000;">77.15</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.8.1" style="color:#000000;">51.73</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.9.1" style="color:#000000;">85.78</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.10.1" style="color:#000000;">73.07</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.11.1" style="color:#000000;">88.19</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.12.1" style="color:#000000;">52.79</span></span> <span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.13.1" style="color:#000000;">68.92</span></span></span> <span class="ltx_tr" id="A2.T9.1.1.1.1.6.6"> <span class="ltx_td ltx_align_left ltx_border_t" id="A2.T9.1.1.1.1.6.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.1.1" style="color:#000000;">Dragon + Fine-tune (w/ dialog history)</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.2.1" style="color:#000000;">52.72</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.3.1" style="color:#000000;">80.67</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.4.1" style="color:#000000;">48.94</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.5.1" style="color:#000000;">83.01</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.6.1" style="color:#000000;">52.64</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.7.1" style="color:#000000;">81.95</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.8.1" style="color:#000000;">50.73</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.9.1" style="color:#000000;">87.17</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.10.1" style="color:#000000;">67.86</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.11.1" style="color:#000000;">86.28</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.12.1" style="color:#000000;">43.43</span></span> <span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.13.1" style="color:#000000;">64.94</span></span></span> <span class="ltx_tr" id="A2.T9.1.1.1.1.7.7"> <span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T9.1.1.1.1.7.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.1.1" style="color:#000000;">Dragon + Fine-tune + Rewrite (w/ dialog hisotry)</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.7.7.2.1" style="color:#000000;">53.17</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.7.7.3.1" style="color:#000000;">80.84</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.4.1" style="color:#000000;">49.30</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.5.1" style="color:#000000;">84.64</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.6.1" style="color:#000000;">55.04</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.7.1" style="color:#000000;">83.23</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.8.1" style="color:#000000;">51.23</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.9.1" style="color:#000000;">87.99</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.10.1" style="color:#000000;">60.50</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.11.1" style="color:#000000;">81.03</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.12.1" style="color:#000000;">49.80</span></span> <span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.13.1" style="color:#000000;">67.33</span></span></span> </span> </span><span class="ltx_text" id="A2.T9.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 9:</span>5개의 데이터셋에 걸친 종합적인 멀티턴 검색 결과.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1"><a class="ltx_ref" href="#A2.T9" title="Table 9 ‣ B.2 More Results for Retrieval in Conversational QA ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">9</span></a>에서는 미세 조정 방법과 다시 쓰기 방법을 종합적으로 비교한다.</p>
</div>
<div class="ltx_para" id="A2.SS2.p2">
<p class="ltx_p" id="A2.SS2.p2.1">흥미롭게도, 재작성된 질의만을 입력으로 사용하는 것과 비교하여(Dragon + Rewrite (w/single query only)), 추가 대화 이력(Dragon + Rewrite (w/ dialog history))을 부여하면 평균 점수가 크게 떨어짐을 알 수 있다. 드래곤은 원래 싱글턴 질의에 대해 사전 훈련되기 때문에 멀티턴 대화 대신 싱글턴 재작성 질의가 제공될 때 자연스럽게 더 나은 일반화 능력을 가질 것이기 때문이다. 상기 재작성된 쿼리는 상기 다이얼로그 이력으로부터 이미 충분한 정보를 포함하는 것을 특징으로 하는 방법.</p>
</div>
<div class="ltx_para" id="A2.SS2.p3">
<p class="ltx_p" id="A2.SS2.p3.1">또한 "Dragon + Fine-tune"이 "Dragon + Fine-tune + Rewrite"와 동등하게 수행됨을 관찰한다. 즉, 다중 회전 미세 조정 방법의 경우 원래 쿼리를 입력으로 재작성된 쿼리로 대체하면 비슷한 결과를 얻을 수 있다. 이는 대화 이력이 이미 제공되었기 때문에 재작성된 쿼리가 모델에 대한 많은 추가 정보를 제공하지 않을 것이며, 전체 대화 입력을 자연스럽지 않게 만들기 때문에 부정적인 영향(예: TopiOCQA 데이터 세트에 대한 결과)을 유발할 수도 있기 때문이다. 이는 다시 한번 미세 조정 방법이 멀티 턴 컨텍스트를 이해할 수 있는 능력을 모델에 갖추는 데 얼마나 효과적인지를 보여준다.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Conversational QA Benchmarks</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Data Statistics</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Doc2Dial</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px1.p1.1">평가를 위해 Doc2Dial의 테스트 세트를 사용한다. 3939 사용자-에이전트 회전이 있는 719개의 대화 상자로 구성됩니다.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">QuAC</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px2.p1.1">QAC의 검증 집합은 직접 구할 수 없기 때문에 평가에 사용한다. 유효성 검사 세트는 7354개의 사용자 에이전트 회전이 있는 1000개의 대화 상자로 구성됩니다. 이 7354개의 사용자-에이전트 회전 중 1486개(약 20.2%)의 대답할 수 없는 질문이 있다. 답변 가능한 질문과 답변 불가능한 질문의 결합된 평가를 위해, 우리는 답변 불가능한 질문에 대한 그라운드 트루스 응답을 “<span class="ltx_text ltx_font_typewriter" id="A3.SS1.SSS0.Px2.p1.1.1">Sorry. I cannot find the answer based on the context.</span> 2단계 튜닝에서 설정한 것과 동일합니다. 공정한 비교를 위해 기준 모델(즉, Llama2-SFT/Chat, GPT-3.5-turbo, GPT-4)에서 응답할 수 없는 응답을 동일한 문장으로 대체한다(자세한 내용은 부록 <a class="ltx_ref" href="#A5" title="Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">E</span></a>에서 찾을 수 있다).</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">QReCC</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px3.p1.1">평가에는 QReCC의 테스트 세트를 사용한다. 테스트 세트에는 QuAC 데이터 세트의 일부 대화 샘플 소스가 포함되어 있습니다. 다른 벤치마크 데이터 세트와의 중복을 방지하기 위해 QuAC 소스 샘플을 제거하여 2805개의 사용자 에이전트 회전을 생성했다.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">TopiOCQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px4.p1.1">우리는 TopiOCQA의 테스트 세트를 아직 사용할 수 없기 때문에 검증 세트를 사용한다. 검증 세트는 2514개의 사용자-에이전트 회전이 있는 205개의 대화 상자로 구성된다. 각 질문에는 총 4개의 인간이 작성한 답변이 있습니다. 답변할 수 없는 경우가 있습니다. 그러나 이 데이터셋은 답변이 없는 46개의 질문만 발견하여 답변이 없는 사례 평가에 충분하지 않기 때문에 답변이 없는 사례 평가에 포함하지 않는다. 이 데이터 세트는 전체 위키피디아를 검색 코퍼스로 사용한다는 점을 감안할 때, 일반적으로 특정 주제 또는 도메인으로 좁혀지는 문서보다 대화 QA에 더 중점을 두었기 때문에 다중 전환 검색을 위해 주제 정보를 활용한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">INSCIT</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px5.p1.1">우리는 TopiOCQA의 테스트 세트를 아직 사용할 수 없기 때문에 검증 세트를 사용한다. 그것의 검증 세트는 502개의 사용자-에이전트 턴을 갖는 86개의 대화로 구성된다. 각 질문에는 평균 1.9개의 인간이 작성한 답변이 있습니다. 몇 가지 질문에 대해 대답할 수 없는 응답이 있습니다. 그러나 모든 질문에는 항상 하나 이상의 응답 가능한 응답이 있으므로 이 데이터 세트는 응답할 수 없는 경우 평가에 포함하지 않습니다. INSCIT는 또한 전체 위키피디아를 검색 코퍼스로 사용한다. 따라서 TopiOCQA에서 언급한 것과 동일한 이유로 멀티턴 검색을 위해 토픽 정보를 활용한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px6">
<h5 class="ltx_title ltx_title_paragraph">CoQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px6.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px6.p1.1">우리는 CoQA의 테스트 세트를 직접 얻을 수 없기 때문에 CoQA의 검증 세트를 사용한다. 검증 세트는 7983개의 사용자-에이전트 회전과 함께 500개의 대화로 구성된다. 각 질문에는 총 4개의 인간이 작성한 답변이 있습니다. 답변할 수 없는 경우가 있습니다. 그러나 TopiOCQA 데이터 세트에서와 동일한 이유로 이 데이터 세트를 응답할 수 없는 사례 평가에 포함하지 않는다. 우리는 답을 찾을 수 없는 13개의 질문만 찾는데, 이는 답할 수 없는 사례 평가에 충분하지 않다.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px7">
<h5 class="ltx_title ltx_title_paragraph">DoQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px7.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px7.p1.1">평가에는 DoQA의 테스트 세트를 사용한다. 테스트 세트는 요리, 여행 및 영화 도메인에서 5394개의 사용자 에이전트가 회전하는 1200개의 대화로 구성됩니다. 5394개의 사용자-에이전트 회전 중 1479개(약 27.4%)의 응답할 수 없는 질문이 있다. QuAC 데이터 세트에서 언급한 것과 동일한 전략을 사용하여 응답할 수 없는 샘플을 평가에 통합한다.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px8">
<h5 class="ltx_title ltx_title_paragraph">ConvFinQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px8.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px8.p1.1">우리는 CoQA의 테스트 세트를 직접 얻을 수 없기 때문에 CoQA의 검증 세트를 사용한다. 검증 세트는 1490개의 사용자-에이전트 회전과 함께 421개의 대화로 구성된다.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px9">
<h5 class="ltx_title ltx_title_paragraph">SQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px9.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px9.p1.1">평가에는 SQA의 테스트 세트를 사용합니다. 테스트 세트는 3100개의 사용자 에이전트 회전과 함께 1025개의 대화로 구성됩니다.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px10">
<h5 class="ltx_title ltx_title_paragraph">HybridDial</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px10.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px10.p1.1">평가에는 HybridDial의 테스트 세트를 사용합니다. 테스트 세트는 1111개의 사용자 에이전트 회전과 함께 243개의 대화로 구성됩니다.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Prompts for the Benchmarks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">ChatQA, Llama2-Chat, GPT-3.5-turbo 및 GPT-4의 벤치마크에 대해 §<a class="ltx_ref" href="#A1.SS2" title="A.2 Stage-2: Context-Enhanced Instruction Tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">A.2</span></a>에서 언급한 프롬프트 형식을 사용합니다. 모든 모델에 대해 동일한 <span class="ltx_text ltx_font_typewriter" id="A3.SS2.p1.1.1">{Context for Latest Question}</span>을 유지하는 반면, 기준선(즉, Llama2-Chat, GPT-3.5-turbo 및 GPT-4)에 대해 <span class="ltx_text ltx_font_typewriter" id="A3.SS2.p1.1.2">{Instruction}</span>을 조정하여 최적의 결과를 얻지 않도록 합니다. 우리는 다른 답변 유형(예: 긴 답변, 짧은 답변, 산술 계산)을 가진 테스트 벤치마크에 대해 다른 지침을 사용한다. 벤치마크 상의 모든 모델에 대한 <span class="ltx_text ltx_font_typewriter" id="A3.SS2.p1.1.3">{Instruction}</span>은 다음과 같습니다.</p>
</div>
<section class="ltx_subsubsection" id="A3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.1 </span>ChatQA</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.SSS1.p1">
<p class="ltx_p" id="A3.SS2.SSS1.p1.1">우리는 <span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p1.1.1">{Instruction}</span>을 스테이지-2의 데이터 블렌드와 다른 답변 유형을 기반으로 하는 테스트 벤치마크 간에 일관성을 유지합니다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS1.p2">
<p class="ltx_p" id="A3.SS2.SSS1.p2.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p2.1.1">Please give a full and complete answer for the question.</span> DoQA, INSCIT, HybridDial, Doc2Dial, QuAC 및 QReCC의 경우 이러한 데이터 세트는 일반적으로 질문에 대한 답변이 길기 때문이다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS1.p3">
<p class="ltx_p" id="A3.SS2.SSS1.p3.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p3.1.1">Answer the following question with a short span, or a full and complete answer.</span> SQA 및 TopiOCQA의 경우 이러한 데이터 세트는 질문에 따라 짧은 답변과 긴 답변을 모두 가지고 있기 때문이다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS1.p4">
<p class="ltx_p" id="A3.SS2.SSS1.p4.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p4.1.1">Answer the answer to be just in a few words.</span> CoQA의 경우 일반적으로 질문에 대한 답변이 짧기 때문입니다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS1.p5">
<p class="ltx_p" id="A3.SS2.SSS1.p5.1">"<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p5.1.1">문맥의 숫자 또는 +, -, * 또는 /.</span>을 사용하여 수학 산술로 다음 질문에 답하세요." ConvFinQA의 경우 이 데이터 세트는 모델이 컨텍스트에서 숫자를 추출하거나 산술 계산을 수행해야 하기 때문이다. 우리는 모델이 생성하는 산술 공식을 바탕으로 수를 계산하고 금답과 비교할 것이다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.2 </span>Llama2-Chat</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.SSS2.p1">
<p class="ltx_p" id="A3.SS2.SSS2.p1.1">우리는 §<a class="ltx_ref" href="#A1.SS2" title="A.2 Stage-2: Context-Enhanced Instruction Tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">A.2</span></a>에서 사용한 것과 비교하여 약간 더 나쁜 결과를 제공하는 원래 Llama2-Chat 프롬프트 템플릿<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gpus.llm-utils.org/llama-2-prompt-template/" title="">https://gpus.llm-utils.org/llama-2-prompt-template/</a></span></span></span>을 시도했다. 우리는 여러 <span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p1.1.1">{Instruction}</span> for Llama2-Chat을 시도했다. 우리는 아래의 것이 가장 잘 작동한다는 것을 발견합니다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p2">
<p class="ltx_p" id="A3.SS2.SSS2.p2.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p2.1.1">Please give a answer in just one sentence.</span> DoQA, INSCIT, HybridDial, Doc2Dial, QuAC 및 QReCC의 경우 이러한 데이터 세트는 일반적으로 한 문장 내에서 긴 답변을 가지고 있기 때문이다. 우리는 모델이 매우 긴 답변을 생성하는 것을 방지하기 위해 "완전하고 완전한 답변" 대신 "하나의 문장"과 같은 Llama2-Chat 특정 지시를 내리는 것이 중요하다는 것을 알아챘다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p3">
<p class="ltx_p" id="A3.SS2.SSS2.p3.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p3.1.1">짧은 스팬 또는 하나의 문장으로 다음 질문에 답합니다.</span> TopiOCQA의 경우 이 데이터 세트는 질문에 따라 짧은 답변과 긴 답변을 모두 가지고 있으며 긴 답변은 일반적으로 한 문장 내에 있기 때문이다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p4">
<p class="ltx_p" id="A3.SS2.SSS2.p4.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p4.1.1">Answer the following questions with one or a list of entities.</span> SQA의 경우 이 데이터 세트에 대한 답은 항상 컨텍스트의 엔터티 목록 또는 하나로 구성되기 때문입니다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p5">
<p class="ltx_p" id="A3.SS2.SSS2.p5.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p5.1.1">Answer the answer to be just in a few words.</span> CoQA의 경우 일반적으로 질문에 대한 답변이 짧기 때문입니다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p6">
<p class="ltx_p" id="A3.SS2.SSS2.p6.1">우리는 "<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p6.1.1">문맥의 숫자 또는 +, -, * 또는 /.</span>을 사용하여 수학 산술로만 다음 질문에 답하세요." ConvFinQA의 경우 이 데이터 세트는 모델이 컨텍스트에서 숫자를 추출하거나 산술 계산을 수행해야 하기 때문이다. 우리는 모델에 의해 생성된 산술식을 추출하고 계산기를 사용하여 최종 결과를 얻는다.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.3 </span>GPT-3.5-turbo &amp; GPT-4</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.SSS3.p1">
<p class="ltx_p" id="A3.SS2.SSS3.p1.1">우리는 여러 <span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p1.1.1">{Instruction}</span> for GPT-3.5-turbo and GPT-4, 우리는 아래 것들이 가장 잘 작동한다는 것을 발견했다(GPT-3.5-turbo and GPT-4 모두에 적용됨).</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p2">
<p class="ltx_p" id="A3.SS2.SSS3.p2.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p2.1.1">Please give a answer in just one sentence.</span> DoQA, INSCIT, HybridDial, Doc2Dial, QuAC 및 QReCC의 경우 이러한 데이터 세트는 일반적으로 한 문장 내에서 긴 답변을 가지고 있기 때문이다. Llama2-Chat과 유사하게, 우리는 또한 OpenAI 모델이 "완전하고 완전한 답변"의 지시를 고려할 때 상당히 긴 답변을 생성하는 경향이 있음을 발견한다. 따라서, 우리는 모델이 매우 긴 답변을 생성하는 것을 방지하기 위해 명령어를 더 구체적으로 만든다(즉, "한 문장").</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p3">
<p class="ltx_p" id="A3.SS2.SSS3.p3.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p3.1.1">Answer the following questions in Just a few words or one sentence.</span> TopiOCQA의 경우 이 데이터 세트는 질문에 따라 짧은 답변과 긴 답변을 모두 가지고 있으며 긴 답변은 일반적으로 한 문장 내에 있기 때문이다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p4">
<p class="ltx_p" id="A3.SS2.SSS3.p4.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p4.1.1">Answer the following questions with one or list of entities. Do not give the detailed explanation. Answer needs to be possible.</span> SQA요 우리는 OpenAI 모델이 특별히 지시하지 않는 한 SQA 데이터 세트에 대해 종종 자세한 설명을 제공한다는 것을 발견했다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p5">
<p class="ltx_p" id="A3.SS2.SSS3.p5.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p5.1.1">Answer the answer to be just in a few words.</span> CoQA의 경우 일반적으로 질문에 대한 답변이 짧기 때문입니다.</p>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p6">
<p class="ltx_p" id="A3.SS2.SSS3.p6.1">"<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p6.1.1">문맥의 숫자 또는 +, -, * 또는 /.</span>을 사용하여 수학 산술로만 다음 질문에 답하세요." ConvFinQA의 경우 이 데이터 세트는 모델이 컨텍스트에서 숫자를 추출하거나 산술 계산을 수행해야 하기 때문이다. 우리는 모델에 의해 생성된 산술식을 추출하고 계산기를 사용하여 최종 결과를 얻는다.</p>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Human Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">우리는 10개의 테스트 벤치마크 데이터 세트에 걸쳐 인간 평가를 수행한다. 각 데이터 세트에 대해 무작위로 60개의 샘플을 선택하고 각 샘플은 3개의 주석기로 레이블이 지정되어 총 1800개의 주석이 생성된다.</p>
</div>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">우리는 주석이 모델의 출력에서 사실을 확인하고 어떤 모델이 질문에 더 정확한 응답을 제공하는지 결정하도록 요청한다. 우리는 아마존 기계 터키 플랫폼을 사용하여 인간 평가를 수행합니다. 인간 평가 지침의 세부 사항과 주석자에게 표시된 인터페이스는 그림 <a class="ltx_ref" href="#A4.F3" title="Figure 3 ‣ Appendix D Human Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">3</span></a>에 나와 있다.</p>
</div>
<figure class="ltx_figure" id="A4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="547" id="A4.F3.g1" src="https://arxiv.org/html/2401.10225v1/x3.png" width="829">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span>Human evaluation instructions and the interface for annotators.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Unanswerable Case Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">답변할 수 없는 경우에, 우리는 질문이 정답으로 대답될 수 없다는 것을 나타내는 모델을 고려한다. 답변할 수 있는 경우에, 우리는 질문이 정답으로 대답할 수 없다는 것을 나타내지 않는 모델(즉, 답을 주는 모델)을 고려한다. ChatQA가 항상 “<span class="ltx_text ltx_font_typewriter" id="A5.p1.1.1">Sorry. I cannot find answer based on the context</span> 이 문장을 2단계 튜닝에서 답할 수 없는 질문에 대한 응답으로 사용하기 때문에 답할 수 없는 질문을 찾을 때 답할 수 없다. 라마2-챗, GPT-3.5-터보 및 GPT-4의 경우 응답할 수 없는 질문에 대한 출력은 보통 몇 가지 특정 패턴을 따른다. 아래에서는 DoQA 및 QuAC 데이터 세트에서 생성된 모든 샘플에서 파생된 휴리스틱 일치 패턴을 나열하여 모델이 질문이 답할 수 없다고 제안하는지 확인한다. 생성된 출력에 아래에 나열된 패턴이 포함되어 있음을 찾으면 질문이 응답할 수 없다는 표시로 해석하고 해당 답변을 “<span class="ltx_text ltx_font_typewriter" id="A5.p1.1.2">Sorry. I cannot find the answer based on the context.</span> (p<0.05).</p>
</div>
<div class="ltx_para" id="A5.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A5.p2.1">i’m not sure, cannot find, does not provide, cannot provide, cannot answer,
cannot be found, cannot be determined, don’t have information, do not have
information, couldn’t find, no information in the context, does not mention,
not explicitly mentioned, i don’t have any, i do not have any, does not
specify, doesn’t provide, not able to, unable to, doesn’t specify, there is
no information, there is no mention, not mentioned, i don’t have enough
information, there is no specific information, there is no specific mention,
no information found, I don’t have that information
</pre>
</div>
<figure class="ltx_table" id="A5.tab1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.tab1.1" style="width:551.7pt;height:964pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A5.tab1.1.1"><span class="ltx_text" id="A5.tab1.1.1.1"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.tab1.1.1.1.1"> <span class="ltx_thead"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.tab1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.1.1.1.1" style="color:#000000;">Context</span></span> <span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.tab1.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.1.1.2.1" style="color:#000000;">Conversation</span></span> <span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.tab1.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.1.1.3.1" style="color:#000000;">Assistant Responses</span></span></span> </span> <span class="ltx_tbody"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1"> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.2.1.1.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.2.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.2.1.1.1.1.1.1" style="color:#000000;">Pos</span></span> <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.2.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.2.1.1.1.1.2.1" style="color:#000000;">Rider</span></span> <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.2.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.2.1.1.1.1.3.1" style="color:#000000;">Time/Retired</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.2"> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.2.1.1" style="color:#000000;">1</span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.1.1.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.2.2.1" style="color:#000000;">Loris Capirossi</span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.1.1.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.2.3.1" style="color:#000000;">38:04.730</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.3.1.1" style="color:#000000;">2</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.3.2.1" style="color:#000000;">Valentino Rossi</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.3.3.1" style="color:#000000;">+0.180</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.4.1.1" style="color:#000000;">3</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.4.2.1" style="color:#000000;">Jeremy McWilliams</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.4.3.1" style="color:#000000;">+0.534</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.5.1.1" style="color:#000000;">4</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.5.2.1" style="color:#000000;">Tohru Ukawa</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.5.3.1" style="color:#000000;">+0.537</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.6"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.6.1.1" style="color:#000000;">5</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.6.2.1" style="color:#000000;">Shinya Nakano</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.6.3.1" style="color:#000000;">+0.742</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.7"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.7.1.1" style="color:#000000;">6</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.7.2.1" style="color:#000000;">Ralf Waldmann</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.7.3.1" style="color:#000000;">+7.019</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.8"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.8.1.1" style="color:#000000;">7</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.8.2.1" style="color:#000000;">Franco Battaini</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.8.3.1" style="color:#000000;">+20.889</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.9"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.9.1.1" style="color:#000000;">8</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.9.2.1" style="color:#000000;">Stefano Perugini</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.9.3.1" style="color:#000000;">+20.891</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.10"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.10.1.1" style="color:#000000;">9</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.10.2.1" style="color:#000000;">Jason Vincent</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.10.3.1" style="color:#000000;">+21.310</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.11"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.11.1.1" style="color:#000000;">10</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.11.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.11.2.1" style="color:#000000;">Anthony West</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.11.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.11.3.1" style="color:#000000;">+26.816</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.12"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.12.1.1" style="color:#000000;">11</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.12.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.12.2.1" style="color:#000000;">Alex Hofmann</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.12.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.12.3.1" style="color:#000000;">+26.933</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.13"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.13.1.1" style="color:#000000;">…</span></span> <span class="ltx_td" id="A5.tab1.1.1.1.1.2.1.1.1.13.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td" id="A5.tab1.1.1.1.1.2.1.1.1.13.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.14"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.14.1.1" style="color:#000000;">23</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.14.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.14.2.1" style="color:#000000;">Arno Visscher</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.14.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.14.3.1" style="color:#000000;">+1:40.635</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.15"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.15.1.1" style="color:#000000;">24</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.15.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.15.2.1" style="color:#000000;">Henk Van De Lagemaat</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.15.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.15.3.1" style="color:#0000FF;">+1 Lap</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.16"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.16.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.16.1.1" style="color:#000000;">Ret</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.16.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.16.2.1" style="color:#000000;">Maurice Bolwerk</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.16.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.16.3.1" style="color:#000000;">Retirement</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.17"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.17.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.17.1.1" style="color:#000000;">Ret</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.17.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.17.2.1" style="color:#000000;">Andre Romein</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.17.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.17.3.1" style="color:#000000;">Retirement</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.18"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.18.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.18.1.1" style="color:#000000;">Ret</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.18.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.18.2.1" style="color:#000000;">Roberto Rolfo</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.18.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.18.3.1" style="color:#000000;">Retirement</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.19"> <span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.2.1.1.1.19.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.19.1.1" style="color:#000000;">Ret</span></span> <span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.2.1.1.1.19.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.19.2.1" style="color:#000000;">Marcellino Lucchi</span></span> <span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.2.1.1.1.19.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.19.3.1" style="color:#000000;">Retirement</span></span></span> </span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.2.1.2.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.1"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.1.1.1" style="color:#000000;">User: what racers are shown?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.2"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.2.1.1" style="color:#000000;">Assistant: Loris Capirossi, Valentino Rossi,</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.3.1.1" style="color:#000000;">Jeremy McWilliams, Tohru Ukawa, … ,</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.4.1.1" style="color:#000000;">Henk Van De Lagemaat, Maurice Bolwerk,</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.5.1.1" style="color:#000000;">Roberto Rolfo, Marcellino Lucchi.</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.6"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.6.1.1" style="color:#000000;">User: of those, what times were recorded?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.7"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.7.1.1" style="color:#000000;">Assistant: 38:04.730, +0.180, +0.534,</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.8"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.8.1.1" style="color:#000000;">+0.537, … , +1:40.635, +1 Lap, Retirement.</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.9"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.9.1.1" style="color:#000000;">User: of those times, which were not</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.10"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.10.1.1" style="color:#000000;">actual times?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.11"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.11.1.1" style="color:#000000;">Assistant: +1 Lap, Retirement</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.12"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.12.1.1" style="color:#000000;">User: which of was an actual finish?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.13"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.13.1.1" style="color:#000000;">Assistant:</span></span></span> </span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.2.1.3.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.1"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.1.1.1" style="color:#000000;">ChatQA-13B: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.1.1.2" style="color:#FF0000;">Loris Capirossi</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.2"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.2.1.1" style="color:#000000;">ChatQA-70B: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.2.1.2" style="color:#41B729;">+1 Lap</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.3.1.1" style="color:#000000;">GPT-3.5-turbo: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.3.1.2" style="color:#FF0000;">38:04.730</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.4.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.4.1.2" style="color:#FF0000;">38:04.730</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.5.1.1" style="color:#000000;">Ground Truth: +1 Lap</span></span></span> </span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2"> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.1.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1"> <span class="ltx_td ltx_align_left ltx_colspan ltx_colspan_3" id="A5.tab1.1.1.1.1.3.2.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.1.1.1" style="color:#000000;">2014 compared to 2013 mst 2019s net sales</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.2"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.2.1.1" style="color:#000000;">decreased $ 305 million, or 3% in 2014 as</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.3.1.1" style="color:#000000;">compared to 2013 … space systems 2019</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.4.1.1" style="color:#000000;">operating results included the following</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.5.1.1" style="color:#000000;">(in millions):</span></span></span> </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.2" style="color:#000000;"></span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.2"> <span class="ltx_td ltx_border_tt" id="A5.tab1.1.1.1.1.3.2.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.3.2.1.1.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.3.2.1.1.2.2.1" style="color:#000000;">net sales</span></span> <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.3.2.1.1.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.3.2.1.1.2.3.1" style="color:#000000;">backlog at year-end</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.3"> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.3.1.1" style="color:#000000;">2015</span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.1.1.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.3.2.1" style="color:#000000;">$ 9105</span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.1.1.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.3.3.1" style="color:#0000FF;">$ 17400</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.4.1.1" style="color:#000000;">2014</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.4.2.1" style="color:#000000;">$ 9202</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.4.3.1" style="color:#0000FF;">$ 20300</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.5.1.1" style="color:#000000;">2013</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.5.2.1" style="color:#000000;">$ 9288</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.5.3.1" style="color:#0000FF;">$ 21400</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6"> <span class="ltx_td ltx_align_left ltx_border_tt ltx_colspan ltx_colspan_3" id="A5.tab1.1.1.1.1.3.2.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.1"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.1.1.1" style="color:#000000;">2015 compared to 2014 space systems 2019</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.2"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.2.1.1" style="color:#000000;">net sales in 2015 decreased $ 97 million, or</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.3.1.1" style="color:#000000;">1%, compared to 2014. these decreases were</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.4.1.1" style="color:#000000;">partially offset by higher net sales of</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.5.1.1" style="color:#000000;">approximately $ 235 million for businesses</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.6"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.6.1.1" style="color:#000000;">acquired in 2014; and approximately $ 75</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.7"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.7.1.1" style="color:#000000;">million for the orion program due to</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.8"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.8.1.1" style="color:#000000;">increased volume.</span></span></span> </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.2" style="color:#000000;"></span></span></span> </span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.2.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.1"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.1.1.1" style="color:#000000;">User: what was the average backlog in 2015?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.2"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.2.1.1" style="color:#000000;">Assistant: 17400</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.3.1.1" style="color:#000000;">User: what was it in 2014?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.4.1.1" style="color:#000000;">Assistant: 20300</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.5.1.1" style="color:#000000;">User: what is the sum of those years?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.6"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.6.1.1" style="color:#000000;">Assistant: 17400 + 20300</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.7"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.7.1.1" style="color:#000000;">User: what was the average backlog in 2013?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.8"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.8.1.1" style="color:#000000;">Assistant: 21400</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.9"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.9.1.1" style="color:#000000;">User: what is the total sum for all 3 years?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.10"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.10.1.1" style="color:#000000;">Assistant:</span></span></span> </span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.3.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.1"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.1.1.1" style="color:#000000;">ChatQA-13B: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.1.1.2" style="color:#41B729;">17400 + 20300 + 21400</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.2"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.2.1.1" style="color:#000000;">ChatQA-70B: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.2.1.2" style="color:#41B729;">17400 + 20300 + 21400</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.3.1.1" style="color:#000000;">GPT-3.5-turbo: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.3.1.2" style="color:#FF0000;">9288 + 9202 + 9105</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.4.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.4.1.2" style="color:#41B729;">17400 + 20300 + 21400</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.5.1.1" style="color:#000000;">Ground Truth: 17400 + 20300 + 21400</span></span></span> </span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3"> <span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.4.3.1.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.1"> <span class="ltx_td ltx_align_left ltx_colspan ltx_colspan_4" id="A5.tab1.1.1.1.1.4.3.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.1.1.1" style="color:#000000;">The XVI World Rhythmic Gymnastics</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.2"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.2.1.1" style="color:#000000;">Championships were held in Brussels,</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.3.1.1" style="color:#000000;">Belgium, on November 20-22, 1992.</span></span></span> </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.2" style="color:#000000;"></span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.2"> <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.4.3.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.4.3.1.1.2.1.1" style="color:#000000;">Rank</span></span> <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.4.3.1.1.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.4.3.1.1.2.2.1" style="color:#000000;">Gymnast</span></span> <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.4.3.1.1.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.4.3.1.1.2.3.1" style="color:#000000;">Country</span></span> <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.4.3.1.1.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.4.3.1.1.2.4.1" style="color:#000000;">Point</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.3"> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.3.1.1" style="color:#000000;">1</span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1.1.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.3.2.1" style="color:#000000;">Oxana Kostina</span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1.1.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.3.3.1" style="color:#000000;">Russia</span></span> <span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1.1.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.3.4.1" style="color:#000000;">9.775</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.4.1.1" style="color:#000000;">2</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.4.2.1" style="color:#000000;">Maria Petrova</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.4.3.1" style="color:#000000;">Bulgaria</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.4.4.1" style="color:#000000;">9.700</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.5.1.1" style="color:#000000;">3</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.5.2.1" style="color:#000000;">Diana Popova</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.5.3.1" style="color:#000000;">Bulgaria</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.5.4.1" style="color:#000000;">9.625</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.6"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.6.1.1" style="color:#000000;">4</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.6.2.1" style="color:#000000;">Carmen Acedo</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.6.3.1" style="color:#000000;">Spain</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.6.4.1" style="color:#000000;">9.625</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.7"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.7.1.1" style="color:#000000;">5</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.7.2.1" style="color:#000000;">Irina Deleanu</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.7.3.1" style="color:#000000;">Romania</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.7.4.1" style="color:#000000;">9.550</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.8"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.8.1.1" style="color:#000000;">6</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.8.2.1" style="color:#000000;">Ekaterina S.</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.8.3.1" style="color:#000000;">Ukraine</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.8.4.1" style="color:#000000;">9.525</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.9"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.9.1.1" style="color:#000000;">7</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.9.2.1" style="color:#000000;">Viktoriya Yani</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.9.3.1" style="color:#000000;">Ukraine</span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.9.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.9.4.1" style="color:#000000;">9.475</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.10"> <span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.4.3.1.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.10.1.1" style="color:#0000FF;">8</span></span> <span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.4.3.1.1.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.10.2.1" style="color:#000000;">Maria Sansaridou</span></span> <span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.4.3.1.1.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.10.3.1" style="color:#000000;">Greece</span></span> <span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.4.3.1.1.10.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.10.4.1" style="color:#000000;">9.425</span></span></span> </span></span> <span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.tab1.1.1.1.1.4.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.4.3.2.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.1"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.1.1.1" style="color:#000000;">User: What is the XVI World Rhythmic</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.2"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.2.1.1" style="color:#000000;">Gymnastics Championships?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.3.1.1" style="color:#000000;">Assistant: The XVI World Rhythmic</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.4.1.1" style="color:#000000;">Gymnastics Championships were held in</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.5.1.1" style="color:#000000;">Brussels, Belgium, on November</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.6"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.6.1.1" style="color:#000000;">20-22, 1992</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.7"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.7.1.1" style="color:#000000;">User: How many </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.7.1.2" style="color:#FF8000;">Gymnastics</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.7.1.3" style="color:#000000;"> participated</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.8"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.8.1.1" style="color:#000000;">this championship?</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.9"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.9.1.1" style="color:#000000;">Assistant:</span></span></span> </span></span> <span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.tab1.1.1.1.1.4.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"> <span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.4.3.3.1"> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.1"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.1.1.1" style="color:#000000;">ChatQA-13B: There are </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.1.1.2" style="color:#41B729;">8</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.1.1.3" style="color:#000000;"> Gymnastics</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.2"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.2.1.1" style="color:#000000;">participated in the championship.</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.3"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.3.1.1" style="color:#000000;">ChatQA-70B: There are </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.3.1.2" style="color:#41B729;">8</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.3.1.3" style="color:#000000;"> </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.3.1.4" style="color:#FF8000;">gymnasts*</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.4"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.4.1.1" style="color:#000000;">participated in the championship.</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.5"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.5.1.1" style="color:#000000;">GPT-3.5-turbo: There were a total of </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.5.1.2" style="color:#41B729;">8</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.6"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.6.1.1" style="color:#FF8000;">gymnasts*</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.6.1.2" style="color:#000000;"> who participated in</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.7"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.7.1.1" style="color:#000000;">this championship.</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.8"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.8.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.8.1.2" style="color:#FF0000;">The context does not provide</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.9"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.9.1.1" style="color:#FF0000;">the total number of gymnasts</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.9.1.2" style="color:#000000;"> who</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.10"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.10.1.1" style="color:#000000;">participated in the XVI World</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.11"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.11.1.1" style="color:#000000;">Rhythmic Gymnastics Championships.</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.12"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.12.1.1" style="color:#000000;">Ground Truth: There are 8 gymnasts</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.13"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.13.1.1" style="color:#000000;">participated this championship.</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.14"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.14.1.1" style="color:#FF8000;">*Both models correct the User’s typo of</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.15"> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.15.1.1" style="color:#FF8000;">“Gymnastics” to “gymnasts”</span></span></span> </span></span></span> </span> </span><span class="ltx_text" id="A5.tab1.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A5.T10">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T10.1" style="width:730.8pt;height:1084.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A5.T10.1.1"><span class="ltx_text" id="A5.T10.1.1.1.1"> <span class="ltx_tabular ltx_guessed_headers ltx_align_left ltx_th ltx_column ltx_border_tt" id="A5.T10.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tx_font_bold" id="A5.T10.1.1.1.1.1.1.1.1.2" style="color:#000000;">Conversation</span></span> <span class="ltx_td ltx_align_left ltx_th ltx_column ltx_border_tt" id="A5.T10.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;pad 그것은 장애물입니다.</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1.4"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.4" 스타일="color:#000000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.4" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1" 스타일="color:#000000;"> #000000;"><span class="ltx_td ltx_align_left ltx_border_tt" id="A5.T10.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.2" style="color:#000000;"><span class="ltx_td ltx_align_left ltx_border_tt" id="A5.T10.1.1.1.1.1.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.1.1.1.1.1.1.1.2.1" style="color:#000000;">Year</span></span></span> <span class="ltx_td ltx_align_left lt #000000;">Assistant: The 3000 metres steeplechase is the</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.2.1.2.1.1" style="color:#000000;"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.2.1.1.1.1.3.1" style="padding-top:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1 <span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.2.1.8"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.2.1.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.2.1.2.1.8.1.1" style="color:#000000;">User: The men that is part of it? #000000;">Assistant: Some of men are Henry Rono and</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.2.1.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.2.1.1.9.1" style="color:#000000;">Michael Karst. <span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.2.1.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.2.1.1.11.1.1" style="color:#000000;">User: Which locations did Kerry O’Brien ran for? <span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.2.1.1.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.1.1.1.3" style="padding-top:0.15pt;"><span class="ltx_tan></span></span></span> <span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.2.1.1.1.1.1" style="padding-top:0.15pt;"><span class="ltx_tabular ltx_align_left" id="A5. <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.2.1.3.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.2.1.3.1.2.1" style="color:#000000;">ChatQA-70B: Kerry O'Brien ran in </span><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.2.1.3.1.2.2" style="color:#41B729;">Adelaide and Berlin. #41B729;">Berlin, Adelaide</span><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.2.1.3.1.3.1.1" style="color:#000000;">GPT-3.5-turbo:Kerry O’Brien ran in </span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.3.1.1" style="color:#000000;">Champaign</span><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.3.1.1.3" style="color:#000000;">Berlin, Adelaide</span></span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1. <span class="ltx_tr" id="A5.T10.1.1.1.1.1.2.1.3.1.5.1" style="color:#000000;">GPT-4: Kerry O'Brien ran in </span><span class="ltx_text" id="A5.T10.1.1.1.1.1.2.1.3.1.5.1.2" style="color:#41B729;">Berlin and Adelaide</span><span class="ltx_text" id="A5.T10.1.1.1.1.1.2.1.3.1.5.1.3" style="color:#000000;"> <span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.6"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.2.1.3.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.2.1.3.1.6.1" style="color:#000000;">Ground Truth: He ran for both Berlin and Adelaide. <span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.1.1.1.1.2.1" style="padding-top:0.15pt;"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.1.1.1" style=" Source: You can travel visa-free only if you</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.3.2.1.1.1" style="color:#000000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.3.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.3.2.1.1.1.1.1" style="color:#000000;"><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id Back</span></span></span></span><span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.1.9"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.1.1.1" style="color:#000000;">of ears is dark brown with black margin. * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.12"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.4.3.1.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.4.3.1.1.1.1.1" style="color:#000000;">title: Red fox. #000000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.4.3.1.1.1.1" style="color:#000000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.3.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.14.1" style="color:#000000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span 귀는</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.4.3.1.1.1.18"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.18.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.18.1" 스타일="color:#0000FF;"><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.1.1.1.19" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1. 코트 색상의 광범위한 변동은 존재</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.4.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.21.1" 스타일="color:#0000FF;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.2" 스타일="color:#0000FF;">…</span></span></span></span> <span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.4.3.1.1.1.1.1.2" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">< <span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.4.3.2.1.1" style="color:#000000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.4.3.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.4.3.2.1.1" style="color:#000000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1. <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.5"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.4.3.2.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.1.1" style="color:#000000;">User: where did it originate? <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.6"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.4.3.2.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.6.1.1" style="color:#000000;">Assistant: The species is Eurasian in origin. <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.7"><span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.4.3.2.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.7.1.1" style="color:#000000;">User: how did it evolve? (../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../ #000000;">User: what is the appearance of </span><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.4.3.2.1.2" style="color:#0000FF;">this third type</span></span></span></span></span><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.2.1.2" style="color:#0000FF;">of fox</span><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.4.3.1.1.2" style="color:#000000;"> <span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.4.3.1" style="color:#000000;"><span class="ltx_tbular ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tbular ltx_align_middle" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tbular ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1 <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1" style="color:#41B729;">span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.4.3.3.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="color:#41B729;">span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="color:#41B729;">populations, but generally vary from pale</span></ [color:#41B729;">ChatQA-70B:</span><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1.2" style="color:#41B729;">ChatQA-70B:</span><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1" style="color:#41B729;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.4.3.3.1.1.1" style="color:#41B729;"><span class="ltx_tan></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1. #000000;">GPT-3.5-turbo:</span><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1.1.1.2" style="color:#000000;">GPT-3.5-turbo:</span><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.4.3.3.1.1" style="color:#000000;">I’m sorry, but I couldn’t find</span></span></span></span><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" style="padding-top:#FF0000;"><span class="ltx_ #FF0000;">GPT-4:</span><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.4.3.3.1.14.1.1.1" style="color:#000000;">GPT-4:</span><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.4.3.3.1.1.1.14.1.1.14" style="color:#0000;">The context does not provide information on</span></span></span><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.14.1.1.14" style="color:#0000;">The third type of fox. <span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.4.3.3.1.1" style="color:#000000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.4.3.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.16.1" style="color:#000000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.16.1" style="padding-top: </span></span></span></span></span></span></span></span><span><span class="ltx_text" id="A5.T10.1.1.1.2" style="color:#000000;"></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 10:</span>Case study for ChatQA-13B, ChatQA-70B, GPT-3.5-turbo, and GPT-4. We use <span class="ltx_text" id="A5.T10.5.1" style="color:#0000FF;">blue</span> color to highlight the relevant context for the last user question. 그리고 <span class="ltx_text" id="A5.T10.6.2" style="color:#41B729;">green</span> and <span class="ltx_text" id="A5.T10.7.3" style="color:#FF0000;">red</span> color를 사용하여 어시스턴트 응답에서 올바르고 잘못된 부분을 강조 표시합니다.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Case Study</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">ChatQA-13B, ChatQA-70B, GPT-3.5-turbo 및 GPT-4에 대한 더 많은 예는 표 <a class="ltx_ref" href="#A5.T10" title="Table 10 ‣ Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">10</span></a>에서 찾을 수 있다.</p>
</div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Guidelines for Conversational QA Data Collection</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">이 섹션에서는 대화형 QA 데이터 수집에 대해 제공하는 지침을 보여준다. 주석이 주어진 문서를 기반으로 대화형 QA 샘플을 구성하기 위해 주석이 사용자와 에이전트 역할을 하도록 요청하여 주석을 보다 효율적으로 만든다. 지침은 1) 대화 QA 샘플이 어떻게 생겼는지, 2) 우리가 필요로 하는 대화 QA 샘플이 어떤 종류인지, 3) 주석을 달아야 하는 것의 세 부분으로 구성된다.</p>
</div>
<section class="ltx_subsection" id="A7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.1 </span>What does conversational QA samples look like</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS1.p1">
<p class="ltx_p" id="A7.SS1.p1.1">대화형 QA 샘플 하나는 우리가 제공하는 문서를 기반으로 합니다. 사용자 및 에이전트 행동은 다음과 같다:</p>
</div>
<div class="ltx_para" id="A7.SS1.p2">
<ul class="ltx_itemize" id="A7.I1">
<li class="ltx_item" id="A7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i1.p1.1">사용자 행동: 1) 주어진 문서에 기초하여 에이전트에게 질문하기; 2) 에이전트가 무언가를 명확히 하고자 할 때 에이전트로부터 질문에 대답하기.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i2.p1.1">에이전트 행동: 1) 문서를 기반으로 사용자의 질문에 답하기; 2) 사용자의 질문이 명확하지 않거나 너무 일반적이거나 광범위할 때 사용자에게 질문하기.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.2 </span>What kinds of multi-turn QA samples we need</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS2.p1">
<p class="ltx_p" id="A7.SS2.p1.1">아래에 사용자의 질문 및 에이전트의 응답에 대한 요구 사항을 나열합니다.</p>
</div>
<section class="ltx_paragraph" id="A7.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">User’s Questions</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS2.SSS0.Px1.p1">
<ul class="ltx_itemize" id="A7.I2">
<li class="ltx_item" id="A7.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i1.p1">
<p class="ltx_p" id="A7.I2.i1.p1.1">사용자의 질문은 자신의 이전(또는 여러 차례 이전) 질문을 참조할 수 있다.</p>
</div>
</li>
<li class="ltx_item" id="A7.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i2.p1">
<p class="ltx_p" id="A7.I2.i2.p1.1">사용자의 질문은 에이전트의 이전(또는 이전 여러 차례) 답변을 참조할 수도 있습니다.</p>
</div>
</li>
<li class="ltx_item" id="A7.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i3.p1">
<p class="ltx_p" id="A7.I2.i3.p1.1">앞에서 언급한 개체들을 대체하기 위해 대명사나 보통명사를 사용해 보세요.</p>
</div>
</li>
<li class="ltx_item" id="A7.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i4.p1">
<p class="ltx_p" id="A7.I2.i4.p1.1">사용자의 질문을 다양하게 만들어 보세요. 동일한 유형의 질문에 대해 서로 다른 대화 주석으로 표현하기 위해 서로 다른 방법을 사용하려고 시도한다.</p>
</div>
</li>
<li class="ltx_item" id="A7.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i5.p1">
<p class="ltx_p" id="A7.I2.i5.p1.1">필요한 다른 사용자 질문 유형</p>
<ul class="ltx_itemize" id="A7.I2.i5.I1">
<li class="ltx_item" id="A7.I2.i5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A7.I2.i5.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="A7.I2.i5.I1.i1.p1">
<p class="ltx_p" id="A7.I2.i5.I1.i1.p1.1">에이전트의 답변을 감안할 때 추가 정보(예: 기타…; 기타…; 더…)를 요청하십시오.</p>
</div>
</li>
<li class="ltx_item" id="A7.I2.i5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A7.I2.i5.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="A7.I2.i5.I1.i2.p1">
<p class="ltx_p" id="A7.I2.i5.I1.i2.p1.1">주제를 전환하고 대화에서 새 스레드를 시작합니다.</p>
</div>
</li>
<li class="ltx_item" id="A7.I2.i5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A7.I2.i5.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="A7.I2.i5.I1.i3.p1">
<p class="ltx_p" id="A7.I2.i5.I1.i3.p1.1">동시에 두 가지 질문을 하세요.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A7.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Agent’s Response</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS2.SSS0.Px2.p1">
<ul class="ltx_itemize" id="A7.I3">
<li class="ltx_item" id="A7.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I3.i1.p1">
<p class="ltx_p" id="A7.I3.i1.p1.1">에이전트의 답변을 1-2문장 안에 넣도록 해보세요. 대답이 길어야 한다면, 간결하게 해보세요.</p>
</div>
</li>
<li class="ltx_item" id="A7.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I3.i2.p1">
<p class="ltx_p" id="A7.I3.i2.p1.1">문서의 전체 관련 컨텍스트를 답변으로 직접 복사하지 마십시오. 대신에, 선택된 문맥을 바꿔서 답을 구성해 보세요.</p>
</div>
</li>
<li class="ltx_item" id="A7.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I3.i3.p1">
<p class="ltx_p" id="A7.I3.i3.p1.1">에이전트가 무언가를 명확히 하기 위해 사용자에게 질문을 하는 경우의 적은 비율을 생각해내도록 노력하세요. 구체적으로, 사용자의 질문이 너무 광범위하거나 명확하지 않은 경우, 에이전트는 사용자가 어떤 특정 측면에 더 관심이 있는지 알기 위해 명확화 질문을 함으로써 범위를 좁힐 필요가 있다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="A7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.3 </span>What we need to annotate</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS3.p1">
<p class="ltx_p" id="A7.SS3.p1.1">우리는 각 대화에 대해 주석을 달 필요가 있는 것을 아래에 나열한다.</p>
<ul class="ltx_itemize" id="A7.I4">
<li class="ltx_item" id="A7.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I4.i1.p1">
<p class="ltx_p" id="A7.I4.i1.p1.1">각 문서에 대해 사용자의 질문과 해당 에이전트의 응답에 주석을 달아야 합니다. 대화당 평균 사용자-에이전트 회전 수는 약 5개여야 합니다.</p>
</div>
</li>
<li class="ltx_item" id="A7.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I4.i2.p1">
<p class="ltx_p" id="A7.I4.i2.p1.1">각 사용자의 질문에 대해 문서 내의 모든 관련 컨텍스트에 주석을 달아야 합니다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
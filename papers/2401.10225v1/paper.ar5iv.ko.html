<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.10225] ChatQA: Building GPT-4 Level Conversational QA Models</title><meta property="og:description" content="In this work, we introduce ChatQA, a family of conversational question answering&nbsp;(QA) models that obtain GPT-4 level accuracies.
Specifically, we propose a two-stage instruction tuning method that can significantly imp…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ChatQA: Building GPT-4 Level Conversational QA Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ChatQA: Building GPT-4 Level Conversational QA Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.10225">

<!--Generated on Tue Feb 27 09:19:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">ChatQA: Building GPT-4 Level Conversational QA Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zihan Liu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Ping
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rajarshi Roy
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peng Xu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chankyu Lee
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Shoeybi
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bryan Catanzaro
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">본 연구에서는 GPT-4 수준의 정확도를 얻을 수 있는 대화형 질의응답(QA) 모델 계열인 ChatQA를 소개한다. 구체적으로, 대용량 언어 모델(LLM)의 제로샷 대화 QA 결과를 크게 개선할 수 있는 2단계 명령어 튜닝 방법을 제안한다. 대화형 QA에서 검색 강화 생성을 처리하기 위해 다중 회전 QA 데이터 세트에서 조밀한 검색기를 미세 조정하며, 이는 배치 비용을 크게 줄이면서 최신 쿼리 재작성 모델을 사용하는 것과 유사한 결과를 제공한다. 특히, 우리의 ChatQA-70B는 10개의 대화 QA 데이터 세트(54.14 vs. 53.90)에서 평균 점수 측면에서 GPT-4를 능가할 수 있다. OpenAI GPT 모델의 합성 데이터에 의존하지 않습니다.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">가장 최근에 ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>와 그 후속 조치들 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib50" title="">2023</a>; Anthropic, <a class="ltx_ref" href="#bib.bib5" title="">2023b</a>; Google, <a class="ltx_ref" href="#bib.bib27" title="">2023</a>)</cite>는 생산 및 연구 커뮤니티에서 질의 응답 모델을 구축하는 패러다임 전환을 이끌었다. 특히, QA 모델의 다음 측면은 실제 애플리케이션에서 선호된다: <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">i)</em> 사용자는 대화 방식으로 QA 모델과 상호 작용할 수 있으므로 후속 질문을 쉽게 제기할 수 있다; <em class="ltx_emph ltx_font_italic" id="S1.p1.1.2">ii)</em> 일반 모델은 데이터 세트별 미세 조정 없이 제로 샷 방식으로 답변을 생성할 수 있지만 미세 조정 전문가 모델의 정확도는 일치하다; <em class="ltx_emph ltx_font_italic" id="S1.p1.1.3">iii) </em>> QA 모델은 오픈 도메인 또는 긴 문서 설정 모두에서 검색된 증거 청크를 통합할 수 있으며, 여기서 제공된 컨텍스트는 LLM<cite class="ltx_cite ltx_citemacro_citep">(e.g., Anthropic, <a class="ltx_ref" href="#bib.bib4" title="">2023a</a>; Xu et al., <a class="ltx_ref" href="#bib.bib74" title="">2023b</a>)</cite>의 컨텍스트 창보다 훨씬 길다. 이를 위해 이 세 가지 측면을 포괄하는 대화식 QA에 초점을 맞춘다.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">그러나, 최신 블랙박스 모델, 즉 GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib50" title="">2023</a>)</cite>의 정확도와 일치할 수 있는 대화형 QA 모델을 구축하는 것은 여전히 연구 커뮤니티에 큰 도전 과제이다. 본 논문에서는 ChatQA-70B, 상세 사양과 GPT-4 수준의 정확도를 갖는 화이트박스 대화 QA 모델, 제안된 2단계 명령어 튜닝 레시피, 대화 QA에서 검색 강화 생성을 위한 향상된 리트리버(RAG) 및 상세 데이터 큐레이션 프로세스를 소개한다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">구체적으로 다음과 같은 기여를 한다.</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i1.p1.1">본 논문에서는 제로 샷 대화형 QA 태스크를 위한 사용자 제공 또는 검색된 컨텍스트를 통합하는 LLM 기능을 크게 향상시킬 수 있는 2단계 명령어 튜닝 방법과 데이터 세트 큐레이션 레시피를 제안한다. 본 논문에서 제안한 방법이 규칙적인 명령어 튜닝이나 RLHF 기반 레시피(예: Llama2-Chat)를 훨씬 능가함을 보인다.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i2.p1.1">대화형 QA에서 RAG의 경우, 인간 주석 또는 합성 다중 회전 QA 데이터 세트 모두에서 최신 단일 회전 쿼리 리트리버를 미세 조정하는 것은 최신 LLM 기반 쿼리 재작성 모델, 즉 GPT-3.5-turbo <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>를 활용하는 것과 마찬가지로 작동한다는 것을 보여준다.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i3.p1.1">Llama2-7B, Llama2-13B, Llama2-70B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib60" title="">2023</a>)</cite> 및 사내 GPT-8B, GPT-22B 모델을 기반으로 ChatQA 모델 패밀리를 구축한다. 검색이 필요한 긴 문서가 포함된 5개의 데이터셋과 테이블이 포함된 3개의 데이터셋을 포함하여 10개의 대화형 질의응답 데이터셋에 대한 종합적인 연구를 수행한다. 평균 점수로 볼 때, ChatQA-70B 모델(54.14)은 ChatGPT 모델의 합성 데이터를 활용하지 않고도 GPT-3.5-터보 모델(50.37)과 GPT-4 모델(53.90)보다 우수한 성능을 보였다.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S1.I1.i4.p1.1">우리는 원하는 답변이 제공되거나 검색된 컨텍스트에 포함되지 않으므로 LLM이 환각을 피하기 위해 "답변할 수 없는" 시나리오를 연구한다. 우리의 ChatQA-70B는 이와 관련하여 GPT-3.5-터보를 능가하지만 GPT-4(약 3.5%)에 비해 약간의 격차가 있다.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">나머지 논문을 정리하면 다음과 같다. 우리는 §<a class="ltx_ref" href="#S2" title="2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서 관련 작업을 논의한다. 본 논문에서는 §<a class="ltx_ref" href="#S3" title="3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">3</span></a>의 ChatQA에 대한 2단계 명령어 튜닝 방법과 데이터 큐레이션을 소개하고 §<a class="ltx_ref" href="#S4" title="4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4</span></a>의 회화 QA에 대한 학습 검색을 소개한다. 우리는 §<a class="ltx_ref" href="#S5" title="5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">5</span></a>, 결과는 §<a class="ltx_ref" href="#S6" title="6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6</span></a>, 논문은 §<a class="ltx_ref" href="#S7" title="7 Conclusion ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">7</span></a>로 정리한다.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.10225/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="82" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1:</span>Two-stage instruction tuning framework for ChatQA.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Conversational QA</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p1.1">대화 방식으로 질문에 답하는 것은 후속 질문을 해결함으로써 사용자 경험을 자연스럽게 향상시킨다. 이 모델은 또한 필요한 경우 사용자에게 명확화 질문을 제기할 수 있으며, 이는 환각을 줄일 수 있다. 따라서 프로덕션 <cite class="ltx_cite ltx_citemacro_citep">(e.g. OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>; Google, <a class="ltx_ref" href="#bib.bib27" title="">2023</a>; Anthropic, <a class="ltx_ref" href="#bib.bib5" title="">2023b</a>)</cite>에서 QA 모델을 배포하는 기본 형식이 됩니다.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p2.1">최근 몇 년 동안 많은 대화형 QA 데이터 세트가 소개되었으며, 여기서 모델은 제공된 컨텍스트 또는 문서를 기반으로 질문에 답하도록 요청된다. 제공된 컨텍스트 또는 문서들은 다음과 같을 수 있다 : <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">i</em>) 다양한 도메인들로부터의 텍스트 전용 문서들 <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>; Anantha et al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>; Saeidi et al., <a class="ltx_ref" href="#bib.bib58" title="">2018</a>; Adlakha et al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>; Aliannejadi et al., <a class="ltx_ref" href="#bib.bib2" title="">2021</a>; Reddy et al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>; Qu et al., <a class="ltx_ref" href="#bib.bib53" title="">2020</a>; Wu et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>; Deng et al., <a class="ltx_ref" href="#bib.bib18" title="">2022</a>; Guo et al., <a class="ltx_ref" href="#bib.bib28" title="">2021</a>; Choi et al., <a class="ltx_ref" href="#bib.bib10" title="">2018</a>; Campos et al., <a class="ltx_ref" href="#bib.bib7" title="">2020</a>)</cite>, 또는 <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.2">ii</em>) 테이블들 <cite class="ltx_cite ltx_citemacro_citep">(Pasupat &amp; Liang, <a class="ltx_ref" href="#bib.bib52" title="">2015</a>; Nakamura et al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>; Chen et al., <a class="ltx_ref" href="#bib.bib8" title="">2022a</a>)</cite>와 함께 일반 텍스트를 포함하는 문서들.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS1.p3.1">최신 LLM 기반 일반주의 솔루션 <cite class="ltx_cite ltx_citemacro_citep">(e.g., OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>와 달리 대부분의 이전 연구는 특정 도메인 또는 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>; Izacard &amp; Grave, <a class="ltx_ref" href="#bib.bib32" title="">2021</a>; Chen et al., <a class="ltx_ref" href="#bib.bib8" title="">2022a</a>; Gao et al., <a class="ltx_ref" href="#bib.bib25" title="">2022</a>; Nakamura et al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>; Adlakha et al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>; Wu et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>에 대한 미세 조정 전문가 모델에 초점을 맞추고 있다.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Retrieval for Multi-Turn QA</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.p1.1">대화형 QA는 오픈 도메인 환경에서 검색 강화 생성(RAG)을 포함하거나, 제공된 문서가 LLM의 컨텍스트 창보다 길 때 수행된다. 밀집 검색기는 일반적으로 단일 질문이 주어진 top-<em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">k</em> 관련 청크를 검색하도록 훈련됩니다. 대화 QA에서 후속 질문(예: 이전 대화에서 언급된 개체를 참조하는 대명사와 함께)은 검색에 대한 정보가 충분하지 않을 수 있지만 대화 이력과 함께 공급하면 중복될 수 있으므로 차선책으로 이어질 수 있다.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Conversational Query Rewriting</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">이전 솔루션의 대부분은 쿼리 다시 쓰기 방법입니다. 최근 질문의 전환은 이전의 대화 히스토리 <cite class="ltx_cite ltx_citemacro_citep">(Vakulenko et al., <a class="ltx_ref" href="#bib.bib62" title="">2021a</a>; Ye et al., <a class="ltx_ref" href="#bib.bib75" title="">2023</a>; Mo et al., <a class="ltx_ref" href="#bib.bib45" title="">2023</a>)</cite>로부터 추가적인 정보 없이 독립형 질의로 재작성되므로 관련 컨텍스트 <cite class="ltx_cite ltx_citemacro_citep">(Vakulenko et al., <a class="ltx_ref" href="#bib.bib63" title="">2021b</a>; Mele et al., <a class="ltx_ref" href="#bib.bib43" title="">2021</a>; Raposo et al., <a class="ltx_ref" href="#bib.bib56" title="">2022</a>; Mo et al., <a class="ltx_ref" href="#bib.bib45" title="">2023</a>)</cite>를 검색하기 위해 검색 모델에 의해 직접 사용될 수 있다. 많은 데이터 세트가 이 연구 라인 <cite class="ltx_cite ltx_citemacro_citep">(Elgohary et al., <a class="ltx_ref" href="#bib.bib21" title="">2019</a>; Chu et al., <a class="ltx_ref" href="#bib.bib11" title="">2020</a>; Qu et al., <a class="ltx_ref" href="#bib.bib53" title="">2020</a>; Anantha et al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>; Brabant et al., <a class="ltx_ref" href="#bib.bib6" title="">2022</a>)</cite>와 함께 여러 제안된 쿼리 재작성 방법 <cite class="ltx_cite ltx_citemacro_citep">(Ishii et al., <a class="ltx_ref" href="#bib.bib30" title="">2022</a>; Yu et al., <a class="ltx_ref" href="#bib.bib76" title="">2020</a>; Wu et al., <a class="ltx_ref" href="#bib.bib71" title="">2022</a>; Del Tredici et al., <a class="ltx_ref" href="#bib.bib17" title="">2021</a>; Chen et al., <a class="ltx_ref" href="#bib.bib9" title="">2022b</a>; Galimzhanova et al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>를 용이하게 하기 위해 수집되었다. 예를 들어, 쿼리 재쓰기를 위한 강화 학습 방법들을 사용하기 위해 제안된 <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="#bib.bib71" title="">2022</a>)</cite>와 <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="#bib.bib9" title="">2022b</a>)</cite>이다. <cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a class="ltx_ref" href="#bib.bib76" title="">2020</a>)</cite>는 쿼리 재쓰기를 위해 GPT-2와 같은 소수의 샷 생성 모델을 조사했다. <cite class="ltx_cite ltx_citemacro_citet">Galimzhanova et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>는 명령어 튜닝된 GPT-3.5-turbo를 연구하여 대화 질의 재쓰기를 위한 최신 결과를 달성했음을 보여주었다.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Fine-tuning Retriever for multi-turn QA</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">일부 이전 연구에서는 도메인 내 대화 쿼리 및 컨텍스트 쌍 <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>; Gao et al., <a class="ltx_ref" href="#bib.bib25" title="">2022</a>; Adlakha et al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>; Wu et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>에서 단일 회전 쿼리 검색기를 미세 조정하므로 대화 이력과 현재 쿼리의 연결을 입력으로 직접 사용할 수 있습니다. 이 작업에서는 제로샷 평가에 초점을 맞춘다. 고품질 다중 회전 데이터 세트에서 단일 회전 쿼리 검색기를 미세 조정합니다. 그런 다음, 5개의 벤치마크 데이터 세트에 대해 미세 조정된 리트리버의 제로 샷 능력을 평가한다. 놀랍게도, 우리는 이 간단한 접근법이 최신 쿼리 재작성 모델, 즉 GPT-3.5-터보와 유사한 제로 샷 결과를 얻을 수 있음을 발견했다.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Instruction Tuning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p1.1">명령어 튜닝의 목표는 LLMs에 자연 언어 명령어 <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib69" title="">2022a</a>; Sanh et al., <a class="ltx_ref" href="#bib.bib59" title="">2022</a>; Mishra et al., <a class="ltx_ref" href="#bib.bib44" title="">2022</a>; Iyer et al., <a class="ltx_ref" href="#bib.bib31" title="">2022</a>; Du et al., <a class="ltx_ref" href="#bib.bib19" title="">2022</a>; Ouyang et al., <a class="ltx_ref" href="#bib.bib51" title="">2022</a>; Wang et al., <a class="ltx_ref" href="#bib.bib68" title="">2023b</a>; Zhang et al., <a class="ltx_ref" href="#bib.bib78" title="">2023b</a>; Gao et al., <a class="ltx_ref" href="#bib.bib26" title="">2023</a>; Chung et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>; Muennighoff et al., <a class="ltx_ref" href="#bib.bib46" title="">2022</a>; Xu et al., <a class="ltx_ref" href="#bib.bib73" title="">2023a</a>; Wang et al., <a class="ltx_ref" href="#bib.bib67" title="">2022c</a>; Zhou et al., <a class="ltx_ref" href="#bib.bib79" title="">2023</a>)</cite>를 따를 수 있는 능력을 갖추는 것이다. FLAN <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>)</cite>, Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib66" title="">2022b</a>)</cite>, 부자연스러운 Instructions <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al., <a class="ltx_ref" href="#bib.bib29" title="">2022</a>)</cite>, Dolly <cite class="ltx_cite ltx_citemacro_citep">(Conover et al., <a class="ltx_ref" href="#bib.bib14" title="">2023b</a>)</cite>, OpenAssistant <cite class="ltx_cite ltx_citemacro_citep">(Köpf et al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite> 등의 고품질 명령어 튜닝 데이터셋 개발이 급증하고 있다.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p2.1">수업 튜닝에 대한 많은 연구가 수행되었지만, QA를 위한 RAG 또는 상황 인식 생성을 개선하는 데 중점을 둔 몇 가지 작업이 수행되었다. <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="#bib.bib40" title="">2023b</a>)</cite>는 top-<em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.1">k</em> retrieved chunks for LLM fine-tuning을 추가한 검색 강화 명령어 튜닝 방법을 도입했다. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib64" title="">2023a</a>)</cite>는 검색 강화 사전 훈련 후 명령어 튜닝을 적용했습니다. 이와는 대조적으로, 우리는 검색 또는 제공된 컨텍스트를 사용하여 생성을 개선하기 위한 2단계 명령어 튜닝 방법을 제안한다. 우리는 top-<em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.2">k</em> retrieved chunks for LLM fine-tuning does not help for a wide range of conversation QA tasks (see §<a class="ltx_ref" href="#S6.SS3" title="6.3 Top-k Chunks for Stage-2 Instruction Tuning ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6.3</span></a> for details)</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p3.1">최신 작업 <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="#bib.bib77" title="">2023a</a>)</cite>와 유사하게 명령어 튜닝에서 소량의 "응답할 수 없는" 샘플을 추가하면 모델이 필요할 때 "응답할 수 없는" 출력을 생성할 수 있으므로 환각을 크게 줄일 수 있음을 보여준다.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p class="ltx_p" id="S2.SS3.p4.1">ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>가 출시된 후, 명령어 튜닝은 광범위한 작업에 놀라운 제로 샷 기능을 가진 최첨단 대화 에이전트를 구축하는 데 필수 요소가 되었다. 대화형 QA 능력은 대화 에이전트에서 중요한 역할을 하지만 이러한 중요한 측면에 초점을 맞춘 연구는 제한적이다.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ChatQA</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">본 절에서는 ChatQA를 위한 2단계 명령어 튜닝 방법을 제안한다. 도면은 <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">1</span></a>를 참조한다. 우리의 방법은 사전 학습된 LLM 기초 모델로 시작한다. 단계-1에서는 명령 후속 및 대화 데이터 세트의 혼합에 <cite class="ltx_cite ltx_citemacro_citet">Ouyang et al. (<a class="ltx_ref" href="#bib.bib51" title="">2022</a>)</cite>에서와 같이 감독 미세 조정(SFT)을 적용한다. 그 후, 저희 모델은 대화 에이전트로서 지시를 따를 수 있는 좋은 능력을 보여줍니다. 그러나 상황화 또는 RAG 기반 QA에 대한 기능은 여전히 제한적이다. 따라서 본 논문에서는 상황인식이나 검색이 강화된 대화형 질의응답 생성 모델의 성능을 향상시키기 위한 상황강화 명령어 튜닝(context-enhanced instruction tuning)을 제안한다.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Stage-1: Supervised Fine-tuning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p1.1">SFT(supervised fine-tuning) 데이터셋을 구성하기 위해 [<cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="#bib.bib74" title="">2023b</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="#bib.bib64" title="">2023a</a>)</cite>를 따라 고품질 명령어 튜닝 데이터셋에서 128K개의 SFT 샘플 조합을 수집한다. 1) 소셜 대화 데이터셋 Soda <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a class="ltx_ref" href="#bib.bib34" title="">2022</a>)</cite>, 2) 정교한 답변을 포함하는 롱폼 QA 데이터셋 ELI5 <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="#bib.bib22" title="">2019</a>)</cite>, 3) FLAN과 연쇄 사상 데이터셋 <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="#bib.bib70" title="">2022b</a>; Chung et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>; Longpre et al., <a class="ltx_ref" href="#bib.bib42" title="">2023</a>)</cite>, 4) Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib66" title="">2022b</a>)</cite>와 Unnatural Instructions <cite class="ltx_cite ltx_citemacro_citep">(Honovich et al., <a class="ltx_ref" href="#bib.bib29" title="">2022</a>)</cite>, 5) LLM 합성 명령어 튜닝 데이터셋으로 구성된다. 공개 인간 작성 대화 데이터 세트인 OpenAssistant <cite class="ltx_cite ltx_citemacro_citep">(Köpf et al., <a class="ltx_ref" href="#bib.bib36" title="">2023</a>)</cite>, Dolly <cite class="ltx_cite ltx_citemacro_citep">(Conover et al., <a class="ltx_ref" href="#bib.bib13" title="">2023a</a>)</cite>와 함께 개인 크라우드 소스 대화 데이터 세트입니다.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS1.p2.1">우리는 모든 SFT 데이터의 구조를 대화 형식으로 통일한다. 우리는 먼저 "시스템" 역할을 추가하여 LLM을 안내하는 일반적인 지침을 설정하여 예의 바르고 도움이 되는 답변을 제공한다. 또한 명령어 튜닝 데이터 세트에서 명령어와 응답 쌍을 통합하기 위해 "사용자" 및 "보조" 역할을 추가한다. 우리는 LLM 기초 모델에 이 통합된 형식을 사용하여 미세 조정을 적용한다.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Stage-2: Context-Enhanced Instruction Tuning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.p1.1">주어진 컨텍스트에 대한 모델의 대화 QA 기능을 더욱 향상시키기 위해, 컨텍스트화된 QA 데이터 세트를 명령어 튜닝 블렌드에 통합하는 2단계 명령어 튜닝을 수행한다. 구체적으로, 단계-2 명령어 튜닝 데이터 세트는 문맥화된 단일 턴 QA와 대화형 QA 데이터 세트의 혼합으로 구성된다. 아래 2단계 명령어 튜닝 데이터 세트에 대한 추가 세부 정보를 제시한다.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Human Annotated Data</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">공개 가능한 데이터 세트 외에도 단계-2의 핵심 요소 중 하나는 고품질 문서 기반 대화 QA 데이터 세트를 얻는 것이다. 우리는 7k개의 대화로 구성된 인간 주석 대화 QA(HumanAnnotatedConvQA) 데이터 세트를 생성한다. 이 데이터 세트를 구축하기 위해 먼저 인터넷에서 다양한 주제를 다루는 7k개의 문서를 수집했다. 그런 다음 주석자에게 문서에 대한 질문(및 후속 질문)을 묻는 호기심 많은 사용자 및 답변을 제공하는 에이전트 역할을 모두 수행하도록 지시한다. 각 문서에 대해 멀티 턴 대화를 생성하여 대화당 평균 5번의 사용자-에이전트 턴을 갖는 총 7k개의 대화 QA 대화를 생성한다. 데이터 수집 가이드라인의 자세한 내용은 부록 <a class="ltx_ref" href="#A7.SS2" title="G.2 Guidelines for Conversational QA Data Collection ‣ Appendix G Human Annotated Conversational QA Data ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">G.2</span></a>에서 확인할 수 있다.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">답변할 수 없는 경우에 환각된 답변을 줄이기 위해 주어진 컨텍스트 내에서 답변을 찾을 수 없을 때 모델을 명시적으로 표시할 수 있도록 권한을 부여하는 것을 목표로 한다. 이러한 응답할 수 없는 데이터 샘플을 얻기 위해 주석자에게 사용자의 질문과 관련된 컨텍스트 위치의 모든 부분을 식별하도록 요청했다. 따라서 컨텍스트의 해당 위치에서 텍스트를 삭제하여 응답할 수 없는 시나리오를 구성할 수 있었다. 질문에 관련 텍스트를 삭제한 후 "<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p2.1.1">Sorry. I cannot find the answer based on the context</span> 답변할 수 없는 질문에 대한 답변입니다. 마지막으로, 답변이 불가능한 경우와 답변이 불가능한 경우의 좋은 트레이드오프를 제공하는 또 다른 1.5k 사용자 에이전트 턴을 구성한다(자세한 내용은 §<a class="ltx_ref" href="#S6.SS5" title="6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6.5</span></a> 참조).</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Synthetic Data Generation</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">HumanAnnotatedConvQA의 품질을 검증하기 위해 GPT-3.5-turbo를 활용하여 강력한 명령어 후속 및 텍스트 생성 기능을 제공하는 합성 대화 QA 데이터 세트를 생성한다. 대화형 QA를 위한 대규모 합성 데이터도 <cite class="ltx_cite ltx_citemacro_citet">Dai et al. (<a class="ltx_ref" href="#bib.bib15" title="">2022</a>)</cite>에서 탐구되었다. 본 연구에서는 LLM 미세 조정을 위한 중간 크기의 고품질 합성 데이터에 초점을 맞춘다.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">GPT-3.5-turbo에 대한 지침은 1) 도움이 되는 답변을 제공하도록 모델을 안내하는 시스템 역할, 2) 필요한 데이터 유형을 나타내는 대화형 QA의 예, 3) 콘텐츠를 기반으로 대화형 QA를 생성하도록 모델을 지시하는 문서의 세 부분으로 구성된다. 우리는 광범위한 도메인을 포함하는 일반적인 크롤링으로부터 7k개의 문서(문서당 평균 <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.1.m1.1"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mo id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">\sim</annotation></semantics></math>1k 단어)를 수집한다. 각 문서는 단일 대화 QA 샘플 생성에 사용되며, 이는 대화당 평균 4.4 사용자-에이전트 회전(SyntheticConvQA라고 함)으로 총 7k 개의 다중 회전 QA 대화로 이어진다.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">HumanAnnotatedConvQA와 유사하게 이 합성 데이터 세트에서 응답할 수 없는 주석으로 또 다른 1.5k 사용자 에이전트 회전을 구성합니다. 에이전트의 답변에 대한 컨텍스트 위치의 주석이 없기 때문에 SyntheticConvQA에서 응답할 수 없는 합성 샘플을 구성합니다. 구체적으로, 우리는 먼저 (각 대화의) 문서를 서로 다른 청크로 절단한다. 그런 다음 제거하려는 에이전트의 답변과 "높게 겹치는" 청크가 있고 나머지 청크는 에이전트의 답변과 "낮게 겹치는" 청크가 있는 경우에만 유효한 응답할 수 없는 샘플로 간주한다. 각 청크와 에이전트의 답변 사이의 4-그램 리콜 점수(답변 4-그램 구의 비율이 각 청크 내에 있음을 측정함)를 메트릭으로 사용하여 중첩을 측정하고 0.5보다 높은 것을 “높은 중첩”으로, 0.1보다 낮은 것을 “낮은 중첩”으로 간주한다.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Training Blends</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">이 부분에서는 2단계 명령어 튜닝을 위한 훈련 블렌드의 세부 사항을 소개한다. 표 문서 처리 및 연산 계산에서 QA 성능을 향상시키기 위해 두 요소를 모두 포함하는 TAT-QA 데이터 세트 <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="#bib.bib80" title="">2021</a>)</cite>를 추가한다. 또한, 컨텍스트화된 단일 턴 QA 데이터 세트를 통합하여 모델의 QA 기능을 더욱 강화한다. 또한 모델의 명령어 수행 능력을 유지하기 위해 훈련 블렌드에 1단계 SFT 데이터 세트를 유지한다.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">마지막으로, 단계-2에 대한 트레이닝 블렌드는 1) A 대화 QA 데이터세트: HumanAnnotatedConvQA 또는 SyntheticConvQA, <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Unless specified otherwise, the experiments use HumanAnnotatedConvQA as the default setting.</span></span></span> 2) 싱글 턴 QA 데이터세트: DROP<cite class="ltx_cite ltx_citemacro_citep">(Dua et al., <a class="ltx_ref" href="#bib.bib20" title="">2019</a>)</cite>, NarrativeQA<cite class="ltx_cite ltx_citemacro_citep">(Kočiskỳ et al., <a class="ltx_ref" href="#bib.bib35" title="">2018</a>)</cite>, Quoref<cite class="ltx_cite ltx_citemacro_citep">(Dasigi et al., <a class="ltx_ref" href="#bib.bib16" title="">2019</a>)</cite>, ROPES<cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib38" title="">2019</a>)</cite>, SQuAD1.1<cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al., <a class="ltx_ref" href="#bib.bib54" title="">2016</a>)</cite>, SQuAD2.0<cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al., <a class="ltx_ref" href="#bib.bib55" title="">2018</a>)</cite>, NewsQA<cite class="ltx_cite ltx_citemacro_citep">(Trischler et al., <a class="ltx_ref" href="#bib.bib61" title="">2017</a>)</cite>, TAT-QA<cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="#bib.bib80" title="">2021</a>)</cite>, 3) 단계-1의 모든 SFT 데이터세트로 구성된다.</p>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">우리는 모든 단일 전환 QA 및 대화 QA 데이터 세트를 통합하기 위해 단계-1에서와 유사한 템플릿을 따른다. 1) 시스템 역할에 따라 단일 회전 질문 또는 다중 회전 대화의 관련 컨텍스트를 추가하고, 2) 단일 회전 질문 또는 다중 회전 대화 직전에 서로 다른 QA 데이터 세트(예: 짧은 답변, 긴 답변, 산술 계산)의 답변 유형에 따라 추가 지침을 통합한다. 우리는 단계-1의 SFT 데이터 세트에 대한 형식을 사용합니다. <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Details of the templates for both stage-1 and stage-2 instruction tuning as well as the synthetic data generation can be found in the Appendix <a class="ltx_ref" href="#A1" title="Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span></p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:722.2pt;height:199pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S3.T1.9.9" class="ltx_p"><span id="S3.T1.9.9.9" class="ltx_text"> <span id="S3.T1.9.9.9.9" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_tbody"> <span id="S3.T1.9.9.9.9.10.1" class="ltx_tr"> <span id="S3.T1.9.9.9.9.10.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_rowspan ltx_rowspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S3.T1.9.9.9.9.10.1.1.1" class="ltx_text">Models</span></span> <span id="S3.T1.9.9.9.9.10.1.2" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">Average</span> <span id="S3.T1.9.9.9.9.10.1.3" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">Doc2Dial</span> <span id="S3.T1.9.9.9.9.10.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">QuAC</span> <span id="S3.T1.9.9.9.9.10.1.5" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">QReCC</span> <span id="S3.T1.9.9.9.9.10.1.6" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">TopiOCQA</span> <span id="S3.T1.9.9.9.9.10.1.7" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">INSCIT</span></span> <span id="S3.T1.9.9.9.9.11.2" class="ltx_tr"> <span id="S3.T1.9.9.9.9.11.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-1</span> <span id="S3.T1.9.9.9.9.11.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5</span> <span id="S3.T1.9.9.9.9.11.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-1</span> <span id="S3.T1.9.9.9.9.11.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5</span> <span id="S3.T1.9.9.9.9.11.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-1</span> <span id="S3.T1.9.9.9.9.11.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5</span> <span id="S3.T1.9.9.9.9.11.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-1</span> <span id="S3.T1.9.9.9.9.11.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5</span> <span id="S3.T1.9.9.9.9.11.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5*</span> <span id="S3.T1.9.9.9.9.11.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-20*</span> <span id="S3.T1.9.9.9.9.11.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5*</span> <span id="S3.T1.9.9.9.9.11.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-20*</span></span> <span id="S3.T1.1.1.1.1.1" class="ltx_tr"> <span id="S3.T1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"><cite class="ltx_cite ltx_citemacro_citet">Adlakha et&nbsp;al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite></span> <span id="S3.T1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">70.40<sup id="S3.T1.1.1.1.1.1.1.1" class="ltx_sup">△</sup></span> <span id="S3.T1.1.1.1.1.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.1.1.1.1.1.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span></span> <span id="S3.T1.2.2.2.2.2" class="ltx_tr"> <span id="S3.T1.2.2.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;"><cite class="ltx_cite ltx_citemacro_citet">Wu et&nbsp;al. (<a href="#bib.bib72" title="" class="ltx_ref">2023</a>)</cite></span> <span id="S3.T1.2.2.2.2.2.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.13" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">-</span> <span id="S3.T1.2.2.2.2.2.1" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">71.10<sup id="S3.T1.2.2.2.2.2.1.1" class="ltx_sup">△</sup></span></span> <span id="S3.T1.3.3.3.3.3" class="ltx_tr"> <span id="S3.T1.3.3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">E5-unsupervised<sup id="S3.T1.3.3.3.3.3.1.1" class="ltx_sup">†</sup>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a href="#bib.bib65" title="" class="ltx_ref">2022a</a>)</cite></span> <span id="S3.T1.3.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">31.56</span> <span id="S3.T1.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">59.22</span> <span id="S3.T1.3.3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">23.02</span> <span id="S3.T1.3.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">55.33</span> <span id="S3.T1.3.3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">43.49</span> <span id="S3.T1.3.3.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.68</span> <span id="S3.T1.3.3.3.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">44.71</span> <span id="S3.T1.3.3.3.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">84.99</span> <span id="S3.T1.3.3.3.3.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">26.25</span> <span id="S3.T1.3.3.3.3.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.67</span> <span id="S3.T1.3.3.3.3.3.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">20.32</span> <span id="S3.T1.3.3.3.3.3.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">40.44</span></span> <span id="S3.T1.4.4.4.4.4" class="ltx_tr"> <span id="S3.T1.4.4.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">E5-unsupervised + Rewrite<sup id="S3.T1.4.4.4.4.4.1.1" class="ltx_sup">‡</sup></span> <span id="S3.T1.4.4.4.4.4.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.23</span> <span id="S3.T1.4.4.4.4.4.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">61.02</span> <span id="S3.T1.4.4.4.4.4.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">25.56</span> <span id="S3.T1.4.4.4.4.4.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">58.00</span> <span id="S3.T1.4.4.4.4.4.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.00</span> <span id="S3.T1.4.4.4.4.4.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.01</span> <span id="S3.T1.4.4.4.4.4.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">45.50</span> <span id="S3.T1.4.4.4.4.4.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">85.89</span> <span id="S3.T1.4.4.4.4.4.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">27.58</span> <span id="S3.T1.4.4.4.4.4.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">39.15</span> <span id="S3.T1.4.4.4.4.4.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">21.53</span> <span id="S3.T1.4.4.4.4.4.13" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">42.04</span></span> <span id="S3.T1.5.5.5.5.5" class="ltx_tr"> <span id="S3.T1.5.5.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">E5-unsupervised + Fine-tune<sup id="S3.T1.5.5.5.5.5.1.1" class="ltx_sup">†</sup></span> <span id="S3.T1.5.5.5.5.5.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S3.T1.5.5.5.5.5.2.1" class="ltx_text ltx_font_bold">47.79</span></span> <span id="S3.T1.5.5.5.5.5.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S3.T1.5.5.5.5.5.3.1" class="ltx_text ltx_font_bold">75.00</span></span> <span id="S3.T1.5.5.5.5.5.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">45.28</span> <span id="S3.T1.5.5.5.5.5.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.96</span> <span id="S3.T1.5.5.5.5.5.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.52</span> <span id="S3.T1.5.5.5.5.5.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.74</span> <span id="S3.T1.5.5.5.5.5.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">53.37</span> <span id="S3.T1.5.5.5.5.5.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">89.91</span> <span id="S3.T1.5.5.5.5.5.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.01</span> <span id="S3.T1.5.5.5.5.5.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.07</span> <span id="S3.T1.5.5.5.5.5.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.79</span> <span id="S3.T1.5.5.5.5.5.13" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.31</span></span> <span id="S3.T1.6.6.6.6.6" class="ltx_tr"> <span id="S3.T1.6.6.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">Dragon<sup id="S3.T1.6.6.6.6.6.1.1" class="ltx_sup">†</sup>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2023a</a>)</cite></span> <span id="S3.T1.6.6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.29</span> <span id="S3.T1.6.6.6.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.09</span> <span id="S3.T1.6.6.6.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">43.33</span> <span id="S3.T1.6.6.6.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">75.61</span> <span id="S3.T1.6.6.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">56.80</span> <span id="S3.T1.6.6.6.6.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">82.86</span> <span id="S3.T1.6.6.6.6.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.17</span> <span id="S3.T1.6.6.6.6.6.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">81.96</span> <span id="S3.T1.6.6.6.6.6.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">57.68</span> <span id="S3.T1.6.6.6.6.6.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">78.80</span> <span id="S3.T1.6.6.6.6.6.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">27.49</span> <span id="S3.T1.6.6.6.6.6.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.22</span></span> <span id="S3.T1.7.7.7.7.7" class="ltx_tr"> <span id="S3.T1.7.7.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">Dragon + Rewrite<sup id="S3.T1.7.7.7.7.7.1.1" class="ltx_sup">‡</sup></span> <span id="S3.T1.7.7.7.7.7.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S3.T1.7.7.7.7.7.2.1" class="ltx_text ltx_font_bold">54.46</span></span> <span id="S3.T1.7.7.7.7.7.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.13</span> <span id="S3.T1.7.7.7.7.7.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.60</span> <span id="S3.T1.7.7.7.7.7.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.60</span> <span id="S3.T1.7.7.7.7.7.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.10</span> <span id="S3.T1.7.7.7.7.7.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.15</span> <span id="S3.T1.7.7.7.7.7.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.73</span> <span id="S3.T1.7.7.7.7.7.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">85.78</span> <span id="S3.T1.7.7.7.7.7.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.07</span> <span id="S3.T1.7.7.7.7.7.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">88.19</span> <span id="S3.T1.7.7.7.7.7.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.79</span> <span id="S3.T1.7.7.7.7.7.13" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">68.92</span></span> <span id="S3.T1.8.8.8.8.8" class="ltx_tr"> <span id="S3.T1.8.8.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">Dragon + Fine-tune<sup id="S3.T1.8.8.8.8.8.1.1" class="ltx_sup">†</sup></span> <span id="S3.T1.8.8.8.8.8.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.72</span> <span id="S3.T1.8.8.8.8.8.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S3.T1.8.8.8.8.8.3.1" class="ltx_text ltx_font_bold">80.67</span></span> <span id="S3.T1.8.8.8.8.8.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.94</span> <span id="S3.T1.8.8.8.8.8.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">83.01</span> <span id="S3.T1.8.8.8.8.8.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.64</span> <span id="S3.T1.8.8.8.8.8.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">81.95</span> <span id="S3.T1.8.8.8.8.8.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.73</span> <span id="S3.T1.8.8.8.8.8.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">87.17</span> <span id="S3.T1.8.8.8.8.8.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">67.86</span> <span id="S3.T1.8.8.8.8.8.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">86.28</span> <span id="S3.T1.8.8.8.8.8.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">43.43</span> <span id="S3.T1.8.8.8.8.8.13" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">64.94</span></span> <span id="S3.T1.9.9.9.9.9" class="ltx_tr"> <span id="S3.T1.9.9.9.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">- SyntheticConvQA<sup id="S3.T1.9.9.9.9.9.1.1" class="ltx_sup">♢</sup></span> <span id="S3.T1.9.9.9.9.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.98</span> <span id="S3.T1.9.9.9.9.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S3.T1.9.9.9.9.9.3.1" class="ltx_text ltx_font_bold">81.15</span></span> <span id="S3.T1.9.9.9.9.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.64</span> <span id="S3.T1.9.9.9.9.9.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">83.47</span> <span id="S3.T1.9.9.9.9.9.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">54.75</span> <span id="S3.T1.9.9.9.9.9.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">83.23</span> <span id="S3.T1.9.9.9.9.9.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.63</span> <span id="S3.T1.9.9.9.9.9.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">86.70</span> <span id="S3.T1.9.9.9.9.9.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">64.48</span> <span id="S3.T1.9.9.9.9.9.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">85.24</span> <span id="S3.T1.9.9.9.9.9.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.41</span> <span id="S3.T1.9.9.9.9.9.13" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">67.13</span></span> </span> </span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>평균 top-1 및 top-5 리콜 점수를 갖는 5개의 다중 턴 QA 데이터 세트에 걸친 검색 결과.</figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores.
Compared to rewriting, fine-tuning performs much better on E5-unsupervised and is comparable on Dragon.
*Since the average context length in TopiOCQA and INSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in those datasets. <sup id="S3.T1.22.1" class="ltx_sup">†</sup>The inputs for these two models are a concatenation of the dialogue history and the current query. <sup id="S3.T1.23.2" class="ltx_sup">‡</sup>The input for this model is the rewritten query.
<sup id="S3.T1.24.3" class="ltx_sup">♢</sup>denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. <sup id="S3.T1.25.4" class="ltx_sup">△</sup>The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.10225/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="457" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 2:</span>Illustration of fine-tuning retriever for multi-turn QA.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Retrieval for Multi-Turn QA</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">대화형 QA 작업에서 문서가 LLMs에 직접 입력하기에는 너무 길어지면 대화형 쿼리를 처리할 수 있는 검색기가 필수적입니다. 이 대화 검색기는 대화 이력과 현재 쿼리의 연결을 인코딩한 다음 문서로부터 관련 컨텍스트를 검색합니다. 그 후, 관련 컨텍스트만이 LLM에 대한 입력으로서 사용될 것이다. 최첨단 검색기(예: Dragon<cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib39" title="">2023a</a>)</cite>)는 단일 회전 쿼리에 최적화되어 다중 회전 대화 쿼리에 대한 일반화 능력이 제한된다. <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서는 이 문제를 완화하기 위해 리트리버 미세 조정 방법을 설명합니다. 본 논문에서는 단일 턴 리트리버를 보다 정교하게 조정하기 위해 대화형 질의와 컨텍스트 쌍을 사용하여 대화형 입력에 더 잘 대처할 수 있도록 제안한다.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p" id="S4.p2.1">대안적인 해결책은 질의 재작성기를 사용하여 대화 히스토리를 기반으로 현재 질문을 재작성하는 대화 질의 재작성 방법이다. 그런 다음 다시 작성된 쿼리는 관련 컨텍스트를 검색하기 위해 단일 회전 쿼리 검색기에 대한 입력으로 직접 사용됩니다. 쿼리 재작성 모델은 임베딩 및 검색 비용 외에도 재작성된 쿼리를 생성하기 위해 많은 양의 추가 계산 비용을 도입한다.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fine-tuning Retriever for Multi-turn QA</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p1.1">고품질 미세 조정 데이터 세트를 구축하기 위해 HumanAnnotatedConvQA 또는 SyntheticConvQA의 대화형 QA 데이터 세트를 활용하여 대화형 쿼리 및 컨텍스트 쌍을 구성합니다.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS1.p2.1">HumanAnnotatedConvQA의 경우 대화형 쿼리와 컨텍스트 쌍의 주석을 직접 가져와서 단일 회전 쿼리 검색기를 추가로 미세 조정 하는 데 사용 합니다. SyntheticConvQA의 경우 먼저 대화 QA 데이터 세트의 각 문서를 다른 청크로 절단한다. 그런 다음 에이전트의 답변과 각 청크 사이의 4그램 리콜 점수를 계산한다. 그 후 회상 점수가 가장 높은 청크를 현재 사용자의 질문에 대한 골드 청크로 간주한다. 마지막으로, 구축된 대화 질의와 문맥 쌍은 단일 회전 질의 검색기를 미세 조정하기 위해 사용된다.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Conversational Query Rewriting</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS2.p1.1">강력한 대화 질의 재작성 모델을 구축하기 위해, <cite class="ltx_cite ltx_citemacro_citet">Galimzhanova et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>가 GPT-3.5-turbo를 사용하여 최신 질의 재작성 결과를 입증했다는 점을 고려하여 GPT-3.5-turbo를 재작성자로 사용한다. <cite class="ltx_cite ltx_citemacro_citet">Galimzhanova et al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>와 유사하게 GPT-3.5-turbo를 재작성 작업 지도와 함께 제공할 뿐만 아니라, 재작성 결과의 질을 높이기 위해 적은 수의 재작성 예제를 제공한다. 보다 자세한 내용은 부록 <a class="ltx_ref" href="#A2.SS1" title="B.1 Query Rewriting Prompts for GPT-3.5-turbo ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">B.1</span></a>에서 확인할 수 있다.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparisons</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p1.1"><a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">1</span></a>에서는 제로샷 설정에서 5개의 데이터 세트에 걸쳐 쿼리 다시 쓰기 및 미세 조정 방법을 비교한다. 이러한 데이터 세트에 대한 자세한 내용은§<a class="ltx_ref" href="#S5.SS2.SSS1" title="5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>에서 찾을 수 있다. 최첨단 리트리버인 Dragon <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="#bib.bib39" title="">2023a</a>)</cite>와 강한 비감독 리트리버인 E5-unsupervised <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="#bib.bib65" title="">2022a</a>)</cite>에 대해 실험을 진행하였으며, MS MACRO <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al., <a class="ltx_ref" href="#bib.bib48" title="">2016</a>)</cite>에서는 미세 조정되지 않았다. Dragon을 대상으로 실험한 결과, Fine-tuning은 평균 Top-1 리콜에서 1.74% 정도 질의 재작성보다 약간 더 나쁜 성능을 보였으며, Top-5 리콜에서는 평균 0.54% 정도 더 좋은 성능을 보였다. 이를 통해 대화 검색을 위한 미세 조정 기법의 효용성을 입증한다. 또한 HumanAnnotatedConvQA와 SyntheticConvQA를 이용하여 미세조정을 수행한 결과 유사한 결과를 얻을 수 있었다. 이것은 인간의 주석이 달린 데이터 세트가 고품질임을 강조하며, 우리는 <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.1">do not rely on</em> ChatGPT models for building the state-of-the-art multi-turn query retriever.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p2.1">놀랍게도 미세 조정은 감독되지 않은 E5에서 다시 쓰는 것보다 훨씬 더 나은 성능을 보인다. 우리는 E5-unsupervised이 사전 학습 단계에서 사람 주석이 있는 질의와 문맥 쌍을 사용하지 않기 때문에 고품질 재작성 질의에 대해 약한 일반화를 초래한다고 추측한다. 대조적으로, E5를 감독하지 않고 미세 조정하기 위해 고품질 데이터 세트를 사용하면 평균 Top-1 및 Top-5 리콜 점수가 모두 15% 이상 개선되어 엄청난 부스트가 발생한다.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S4.SS3.p3.1">따라서 고품질 대화 쿼리 컨텍스트 쌍에서 좋은 싱글 턴 리트리버를 미세 조정하면 최신 재기록기를 활용하는 것과 동등하게 수행됩니다. 그러나, 재작성 방법은 자동 회귀 생성 과정을 위한 추가적인 계산 시간과 GPT-3.5-turbo와 같은 강력한 모델을 사용하기 위한 API 비용도 필요로 한다. 대조적으로, 제안된 다중 회전 미세 조정은 이러한 문제를 우회한다. 이 5개의 데이터 세트에 대한 QA 평가를 위해 모든 QA 모델에 대해 미세 조정 접근법에서 검색된 상위 5개의 결과를 일관되게 사용한다. 우리는 부록 <a class="ltx_ref" href="#A2.SS2" title="B.2 More Results for Retrieval in Conversational QA ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">B.2</span></a>에서 다시 쓰기와 미세 조정 방법 간의 비교에 더 많은 결과를 넣었다.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">이 섹션에서는 대화 질의 응답 작업에 대한 실험 설정에 대한 세부 사항을 제시한다.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Baselines</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p1.1">우리는 다양한 모델 크기에 대한 실험을 수행한다. 먼저, Stage-2 context-enhanced instruction tuning의 효과를 보이기 위해 Stage-1 supervised fine-tuning (SFT) 이후 Llama2-7B/13B/70B 기초 모델인 <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Llama2-SFT-{7B, 13B, 70B}</span>과 비교한다. 둘째, <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.2">Llama2-Chat-{7B, 13B, 70B}</span>과 비교하였는데, 이는 Llama2-Chat 모델들이 강력한 명령어 추종 및 대화 QA 능력들을 가지고 있음을 보여주기 때문이다. <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib60" title="">2023</a>)</cite>. Llama2 모델 외에도 3.5조 토큰으로 사전 훈련된 사내 GPT-{8B, 22B} 기초 모델에 대한 실험을 수행하고 스테이지-1 SFT 기준선 <span 클래스="ltx_text ltx_font_bold" id="S5.SS1.p1.1.3">GPT-SFT-{8B, 22B}</span>과 비교한다. 마지막으로 강력한 두 OpenAI 모델인 <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.4">GPT-3.5-turbo-0613 (4k)</span>과 <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.5">GPT-4-0613 (8k)</span>과 비교한다. 공정한 비교를 위해 검색이 필요한 경우 동일한 top-<em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.6">k</em> retrieved chunks from our best retriever as the context for all baselines and our ChatQA models. 모든 기준선이 가능한 한 좋은 결과를 얻을 수 있도록 지침을 주의 깊게 조정했습니다. <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The prompts for these baselines can be found in Appendix <a class="ltx_ref" href="#A3" title="Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">C</span></a>.</span></span></span></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Benchmarks</h3>

<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Long Document Datasets</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">4K 토큰의 시퀀스 길이를 갖는 LLMs에 직접 피팅될 수 없는 긴 문서를 갖는 5개의 대화 QA 데이터 세트를 수집한다. 따라서 상위 5개의 관련 청크를 입력으로 얻기 위해 최고의 멀티턴 쿼리 리트리버를 실행합니다(실험은 §<a class="ltx_ref" href="#S4.SS3" title="4.3 Comparisons ‣ 4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4.3</span></a>에서 찾을 수 있습니다).</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i1.p1.1">Doc2Dial <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>)</cite>는 DMV, SSA, VA 및 Student Aid의 네 가지 도메인을 포함하는 문서 기반 대화 QA 데이터 세트이다. 각 샘플은 사용자가 문서에 관한 질의를 제기하는 대화로 구성되며, 에이전트는 이러한 질문에 응답한다. 평균 문서 길이는 약 101K 단어입니다.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i2.p1.1">QuAC<cite class="ltx_cite ltx_citemacro_citep">(Choi et al., <a class="ltx_ref" href="#bib.bib10" title="">2018</a>)</cite>는 위키피디아 문서를 기반으로 한다. 원래 그 문서는 짧다. 각 대화는 여러 개의 위키피디아 URL에 연결되어 있기 때문에 이러한 링크에서 텍스트를 추출하여 문서 크기를 대략 평균 15K 단어로 증가시킨다. 여기에는 주어진 컨텍스트 내에서 답변을 찾을 수 없는 답변할 수 없는 경우가 포함되어 있습니다.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i3.p1.1">QReCC <cite class="ltx_cite ltx_citemacro_citep">(Anantha et al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>)</cite>는 여러 소스에서 열린 도메인 대화 QA 데이터 세트입니다. QuAC와 유사하게, 각각의 대화는 또한 대응하는 URL을 갖는다. 이러한 URL에서 텍스트를 추출하여 문서를 구성한다. 결국, 평균 문서 크기는 약 5K 단어이며, 최대 문서 크기는 20K 단어이다.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i4.p1.1">TopiOCQA<cite class="ltx_cite ltx_citemacro_citep">(Adlakha et al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>)</cite>는 전체 위키피디아에 접지되어 있다. 이것은 토픽 전환을 통합하고 에이전트가 사용자 질문에 대한 답변을 위해 전체 위키피디아를 검색하도록 요구한다.</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p class="ltx_p" id="S5.I1.i5.p1.1">INSCIT <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>도 전체 위키피디아에 기반을 두고 있다. 사용자 질문이 과소 지정되어 설명이 필요한 경우를 연구한다.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">Doc2Dial, QuAC 및 QReCC의 경우 문서를 약 300개의 단어 청크로 분할하고 각 사용자 질문에 대한 컨텍스트로 상위 5개의 관련 청크를 검색한다. TopioCQA 및 INSCIT의 경우 원래 분할을 따르므로 청크가 더 작아진다. 따라서 우리는 처음 세 개의 데이터 세트와 유사한 컨텍스트 길이를 얻기 위해 상위 20개의 청크를 검색했다.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:638.3pt;height:361pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S5.T2.3.3" class="ltx_p"><span id="S5.T2.3.3.3" class="ltx_text"> <span id="S5.T2.3.3.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_tbody"> <span id="S5.T2.3.3.3.3.4.1" class="ltx_tr"> <span id="S5.T2.3.3.3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Models</span> <span id="S5.T2.3.3.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Average</span> <span id="S5.T2.3.3.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Doc2Dial</span> <span id="S5.T2.3.3.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">QuAC</span> <span id="S5.T2.3.3.3.3.4.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">QReCC</span> <span id="S5.T2.3.3.3.3.4.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">CoQA</span> <span id="S5.T2.3.3.3.3.4.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">DoQA</span> <span id="S5.T2.3.3.3.3.4.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">ConvFinQA</span> <span id="S5.T2.3.3.3.3.4.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">SQA</span> <span id="S5.T2.3.3.3.3.4.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">TopiOCQA</span> <span id="S5.T2.3.3.3.3.4.1.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">HybridDial</span> <span id="S5.T2.3.3.3.3.4.1.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">INSCIT</span></span> <span id="S5.T2.3.3.3.3.5.2" class="ltx_tr"> <span id="S5.T2.3.3.3.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">Llama2-7B-SFT</span> <span id="S5.T2.3.3.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.81</span> <span id="S5.T2.3.3.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">30.26</span> <span id="S5.T2.3.3.3.3.5.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">19.21</span> <span id="S5.T2.3.3.3.3.5.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.55</span> <span id="S5.T2.3.3.3.3.5.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">62.75</span> <span id="S5.T2.3.3.3.3.5.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">21.76</span> <span id="S5.T2.3.3.3.3.5.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.43</span> <span id="S5.T2.3.3.3.3.5.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">32.18</span> <span id="S5.T2.3.3.3.3.5.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">32.88</span> <span id="S5.T2.3.3.3.3.5.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.96</span> <span id="S5.T2.3.3.3.3.5.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">28.16</span></span> <span id="S5.T2.3.3.3.3.6.3" class="ltx_tr"> <span id="S5.T2.3.3.3.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">Llama2-7B-Chat</span> <span id="S5.T2.3.3.3.3.6.3.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.86</span> <span id="S5.T2.3.3.3.3.6.3.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.27</span> <span id="S5.T2.3.3.3.3.6.3.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">25.83</span> <span id="S5.T2.3.3.3.3.6.3.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.02</span> <span id="S5.T2.3.3.3.3.6.3.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.28</span> <span id="S5.T2.3.3.3.3.6.3.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.15</span> <span id="S5.T2.3.3.3.3.6.3.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">36.58</span> <span id="S5.T2.3.3.3.3.6.3.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">26.14</span> <span id="S5.T2.3.3.3.3.6.3.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">36.68</span> <span id="S5.T2.3.3.3.3.6.3.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.02</span> <span id="S5.T2.3.3.3.3.6.3.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">31.67</span></span> <span id="S5.T2.3.3.3.3.7.4" class="ltx_tr"> <span id="S5.T2.3.3.3.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-7B</span> <span id="S5.T2.3.3.3.3.7.4.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T2.3.3.3.3.7.4.2.1" class="ltx_text ltx_font_bold">47.71</span></span> <span id="S5.T2.3.3.3.3.7.4.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.88</span> <span id="S5.T2.3.3.3.3.7.4.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">29.69</span> <span id="S5.T2.3.3.3.3.7.4.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.97</span> <span id="S5.T2.3.3.3.3.7.4.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">76.61</span> <span id="S5.T2.3.3.3.3.7.4.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.57</span> <span id="S5.T2.3.3.3.3.7.4.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.61</span> <span id="S5.T2.3.3.3.3.7.4.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">61.87</span> <span id="S5.T2.3.3.3.3.7.4.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">45.45</span> <span id="S5.T2.3.3.3.3.7.4.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">54.51</span> <span id="S5.T2.3.3.3.3.7.4.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">30.96</span></span> <span id="S5.T2.3.3.3.3.8.5" class="ltx_tr"> <span id="S5.T2.3.3.3.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-8B-SFT</span> <span id="S5.T2.3.3.3.3.8.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.46</span> <span id="S5.T2.3.3.3.3.8.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">31.03</span> <span id="S5.T2.3.3.3.3.8.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">20.07</span> <span id="S5.T2.3.3.3.3.8.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.69</span> <span id="S5.T2.3.3.3.3.8.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">59.24</span> <span id="S5.T2.3.3.3.3.8.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">21.72</span> <span id="S5.T2.3.3.3.3.8.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">15.44</span> <span id="S5.T2.3.3.3.3.8.5.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">40.06</span> <span id="S5.T2.3.3.3.3.8.5.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.17</span> <span id="S5.T2.3.3.3.3.8.5.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.29</span> <span id="S5.T2.3.3.3.3.8.5.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">28.86</span></span> <span id="S5.T2.3.3.3.3.9.6" class="ltx_tr"> <span id="S5.T2.3.3.3.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-8B</span> <span id="S5.T2.3.3.3.3.9.6.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T2.3.3.3.3.9.6.2.1" class="ltx_text ltx_font_bold">49.36</span></span> <span id="S5.T2.3.3.3.3.9.6.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">36.76</span> <span id="S5.T2.3.3.3.3.9.6.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.95</span> <span id="S5.T2.3.3.3.3.9.6.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">45.54</span> <span id="S5.T2.3.3.3.3.9.6.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.90</span> <span id="S5.T2.3.3.3.3.9.6.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">44.65</span> <span id="S5.T2.3.3.3.3.9.6.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">61.68</span> <span id="S5.T2.3.3.3.3.9.6.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">60.74</span> <span id="S5.T2.3.3.3.3.9.6.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.03</span> <span id="S5.T2.3.3.3.3.9.6.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">53.81</span> <span id="S5.T2.3.3.3.3.9.6.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">31.50</span></span> <span id="S5.T2.3.3.3.3.10.7" class="ltx_tr"> <span id="S5.T2.3.3.3.3.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">Llama2-13B-SFT</span> <span id="S5.T2.3.3.3.3.10.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.69</span> <span id="S5.T2.3.3.3.3.10.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">30.68</span> <span id="S5.T2.3.3.3.3.10.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">21.59</span> <span id="S5.T2.3.3.3.3.10.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.25</span> <span id="S5.T2.3.3.3.3.10.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">69.52</span> <span id="S5.T2.3.3.3.3.10.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">21.70</span> <span id="S5.T2.3.3.3.3.10.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.14</span> <span id="S5.T2.3.3.3.3.10.7.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.85</span> <span id="S5.T2.3.3.3.3.10.7.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">35.26</span> <span id="S5.T2.3.3.3.3.10.7.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.22</span> <span id="S5.T2.3.3.3.3.10.7.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">28.73</span></span> <span id="S5.T2.3.3.3.3.11.8" class="ltx_tr"> <span id="S5.T2.3.3.3.3.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">Llama2-13B-Chat</span> <span id="S5.T2.3.3.3.3.11.8.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">40.34</span> <span id="S5.T2.3.3.3.3.11.8.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.74</span> <span id="S5.T2.3.3.3.3.11.8.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">27.89</span> <span id="S5.T2.3.3.3.3.11.8.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.19</span> <span id="S5.T2.3.3.3.3.11.8.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.50</span> <span id="S5.T2.3.3.3.3.11.8.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">32.60</span> <span id="S5.T2.3.3.3.3.11.8.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.54</span> <span id="S5.T2.3.3.3.3.11.8.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">25.39</span> <span id="S5.T2.3.3.3.3.11.8.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">39.25</span> <span id="S5.T2.3.3.3.3.11.8.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.82</span> <span id="S5.T2.3.3.3.3.11.8.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">32.52</span></span> <span id="S5.T2.3.3.3.3.12.9" class="ltx_tr"> <span id="S5.T2.3.3.3.3.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-13B</span> <span id="S5.T2.3.3.3.3.12.9.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T2.3.3.3.3.12.9.2.1" class="ltx_text ltx_font_bold">50.86</span></span> <span id="S5.T2.3.3.3.3.12.9.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.05</span> <span id="S5.T2.3.3.3.3.12.9.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.28</span> <span id="S5.T2.3.3.3.3.12.9.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.06</span> <span id="S5.T2.3.3.3.3.12.9.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.23</span> <span id="S5.T2.3.3.3.3.12.9.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">43.31</span> <span id="S5.T2.3.3.3.3.12.9.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">65.44</span> <span id="S5.T2.3.3.3.3.12.9.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">66.41</span> <span id="S5.T2.3.3.3.3.12.9.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.88</span> <span id="S5.T2.3.3.3.3.12.9.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">56.19</span> <span id="S5.T2.3.3.3.3.12.9.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">30.79</span></span> <span id="S5.T2.3.3.3.3.13.10" class="ltx_tr"> <span id="S5.T2.3.3.3.3.13.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-22B-SFT</span> <span id="S5.T2.3.3.3.3.13.10.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.83</span> <span id="S5.T2.3.3.3.3.13.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">32.98</span> <span id="S5.T2.3.3.3.3.13.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">22.83</span> <span id="S5.T2.3.3.3.3.13.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">39.92</span> <span id="S5.T2.3.3.3.3.13.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">68.19</span> <span id="S5.T2.3.3.3.3.13.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">22.83</span> <span id="S5.T2.3.3.3.3.13.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">30.54</span> <span id="S5.T2.3.3.3.3.13.10.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.01</span> <span id="S5.T2.3.3.3.3.13.10.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.07</span> <span id="S5.T2.3.3.3.3.13.10.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">54.22</span> <span id="S5.T2.3.3.3.3.13.10.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">31.66</span></span> <span id="S5.T2.3.3.3.3.14.11" class="ltx_tr"> <span id="S5.T2.3.3.3.3.14.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-22B</span> <span id="S5.T2.3.3.3.3.14.11.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T2.3.3.3.3.14.11.2.1" class="ltx_text ltx_font_bold">53.53</span></span> <span id="S5.T2.3.3.3.3.14.11.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">39.71</span> <span id="S5.T2.3.3.3.3.14.11.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">36.89</span> <span id="S5.T2.3.3.3.3.14.11.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.47</span> <span id="S5.T2.3.3.3.3.14.11.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.32</span> <span id="S5.T2.3.3.3.3.14.11.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">44.88</span> <span id="S5.T2.3.3.3.3.14.11.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.08</span> <span id="S5.T2.3.3.3.3.14.11.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">69.08</span> <span id="S5.T2.3.3.3.3.14.11.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.83</span> <span id="S5.T2.3.3.3.3.14.11.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">58.20</span> <span id="S5.T2.3.3.3.3.14.11.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.11</span></span> <span id="S5.T2.3.3.3.3.15.12" class="ltx_tr"> <span id="S5.T2.3.3.3.3.15.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">Llama2-70B-SFT</span> <span id="S5.T2.3.3.3.3.15.12.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">43.22</span> <span id="S5.T2.3.3.3.3.15.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.42</span> <span id="S5.T2.3.3.3.3.15.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">25.65</span> <span id="S5.T2.3.3.3.3.15.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.88</span> <span id="S5.T2.3.3.3.3.15.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.04</span> <span id="S5.T2.3.3.3.3.15.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">28.21</span> <span id="S5.T2.3.3.3.3.15.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.64</span> <span id="S5.T2.3.3.3.3.15.12.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">58.90</span> <span id="S5.T2.3.3.3.3.15.12.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.20</span> <span id="S5.T2.3.3.3.3.15.12.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">55.52</span> <span id="S5.T2.3.3.3.3.15.12.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">30.71</span></span> <span id="S5.T2.3.3.3.3.16.13" class="ltx_tr"> <span id="S5.T2.3.3.3.3.16.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">Llama2-70B-Chat</span> <span id="S5.T2.3.3.3.3.16.13.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">45.21</span> <span id="S5.T2.3.3.3.3.16.13.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">36.87</span> <span id="S5.T2.3.3.3.3.16.13.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">32.47</span> <span id="S5.T2.3.3.3.3.16.13.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.40</span> <span id="S5.T2.3.3.3.3.16.13.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.41</span> <span id="S5.T2.3.3.3.3.16.13.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.97</span> <span id="S5.T2.3.3.3.3.16.13.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.85</span> <span id="S5.T2.3.3.3.3.16.13.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.62</span> <span id="S5.T2.3.3.3.3.16.13.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">44.31</span> <span id="S5.T2.3.3.3.3.16.13.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.35</span> <span id="S5.T2.3.3.3.3.16.13.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.88</span></span> <span id="S5.T2.3.3.3.3.17.14" class="ltx_tr"> <span id="S5.T2.3.3.3.3.17.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-70B</span> <span id="S5.T2.3.3.3.3.17.14.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T2.3.3.3.3.17.14.2.1" class="ltx_text ltx_font_bold">54.14</span></span> <span id="S5.T2.3.3.3.3.17.14.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.90</span> <span id="S5.T2.3.3.3.3.17.14.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.82</span> <span id="S5.T2.3.3.3.3.17.14.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.05</span> <span id="S5.T2.3.3.3.3.17.14.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">78.57</span> <span id="S5.T2.3.3.3.3.17.14.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.94</span> <span id="S5.T2.3.3.3.3.17.14.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.69</span> <span id="S5.T2.3.3.3.3.17.14.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">69.14</span> <span id="S5.T2.3.3.3.3.17.14.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.98</span> <span id="S5.T2.3.3.3.3.17.14.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">56.44</span> <span id="S5.T2.3.3.3.3.17.14.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">31.90</span></span> <span id="S5.T2.1.1.1.1.1" class="ltx_tr"> <span id="S5.T2.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">- SyntheticConvQA<sup id="S5.T2.1.1.1.1.1.1.1" class="ltx_sup">♢</sup></span> <span id="S5.T2.1.1.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T2.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">54.08</span></span> <span id="S5.T2.1.1.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">39.19</span> <span id="S5.T2.1.1.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.33</span> <span id="S5.T2.1.1.1.1.1.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.73</span> <span id="S5.T2.1.1.1.1.1.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">79.83</span> <span id="S5.T2.1.1.1.1.1.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.65</span> <span id="S5.T2.1.1.1.1.1.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">76.44</span> <span id="S5.T2.1.1.1.1.1.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">68.63</span> <span id="S5.T2.1.1.1.1.1.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.30</span> <span id="S5.T2.1.1.1.1.1.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">55.68</span> <span id="S5.T2.1.1.1.1.1.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.98</span></span> <span id="S5.T2.2.2.2.2.2" class="ltx_tr"> <span id="S5.T2.2.2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">- w/o stage-1<sup id="S5.T2.2.2.2.2.2.1.1" class="ltx_sup">†</sup></span> <span id="S5.T2.2.2.2.2.2.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.18</span> <span id="S5.T2.2.2.2.2.2.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.43</span> <span id="S5.T2.2.2.2.2.2.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.52</span> <span id="S5.T2.2.2.2.2.2.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.08</span> <span id="S5.T2.2.2.2.2.2.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.51</span> <span id="S5.T2.2.2.2.2.2.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.42</span> <span id="S5.T2.2.2.2.2.2.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.15</span> <span id="S5.T2.2.2.2.2.2.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.08</span> <span id="S5.T2.2.2.2.2.2.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.28</span> <span id="S5.T2.2.2.2.2.2.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.74</span> <span id="S5.T2.2.2.2.2.2.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">30.56</span></span> <span id="S5.T2.3.3.3.3.18.15" class="ltx_tr"> <span id="S5.T2.3.3.3.3.18.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">- w/o single-turn*</span> <span id="S5.T2.3.3.3.3.18.15.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.25</span> <span id="S5.T2.3.3.3.3.18.15.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.30</span> <span id="S5.T2.3.3.3.3.18.15.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.89</span> <span id="S5.T2.3.3.3.3.18.15.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.08</span> <span id="S5.T2.3.3.3.3.18.15.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">76.74</span> <span id="S5.T2.3.3.3.3.18.15.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.43</span> <span id="S5.T2.3.3.3.3.18.15.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.42</span> <span id="S5.T2.3.3.3.3.18.15.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">67.41</span> <span id="S5.T2.3.3.3.3.18.15.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.85</span> <span id="S5.T2.3.3.3.3.18.15.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">53.16</span> <span id="S5.T2.3.3.3.3.18.15.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.18</span></span> <span id="S5.T2.3.3.3.3.3" class="ltx_tr"> <span id="S5.T2.3.3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">- w/o ConvQAData<sup id="S5.T2.3.3.3.3.3.1.1" class="ltx_sup">△</sup></span> <span id="S5.T2.3.3.3.3.3.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.97</span> <span id="S5.T2.3.3.3.3.3.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">35.47</span> <span id="S5.T2.3.3.3.3.3.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">28.19</span> <span id="S5.T2.3.3.3.3.3.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.68</span> <span id="S5.T2.3.3.3.3.3.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">75.80</span> <span id="S5.T2.3.3.3.3.3.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">31.68</span> <span id="S5.T2.3.3.3.3.3.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.83</span> <span id="S5.T2.3.3.3.3.3.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">68.05</span> <span id="S5.T2.3.3.3.3.3.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">45.90</span> <span id="S5.T2.3.3.3.3.3.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">58.40</span> <span id="S5.T2.3.3.3.3.3.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">30.70</span></span> <span id="S5.T2.3.3.3.3.19.16" class="ltx_tr"> <span id="S5.T2.3.3.3.3.19.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-3.5-turbo (4k)</span> <span id="S5.T2.3.3.3.3.19.16.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.37</span> <span id="S5.T2.3.3.3.3.19.16.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.83</span> <span id="S5.T2.3.3.3.3.19.16.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.17</span> <span id="S5.T2.3.3.3.3.19.16.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.46</span> <span id="S5.T2.3.3.3.3.19.16.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">79.33</span> <span id="S5.T2.3.3.3.3.19.16.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.11</span> <span id="S5.T2.3.3.3.3.19.16.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.15</span> <span id="S5.T2.3.3.3.3.19.16.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">60.63</span> <span id="S5.T2.3.3.3.3.19.16.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">44.30</span> <span id="S5.T2.3.3.3.3.19.16.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.42</span> <span id="S5.T2.3.3.3.3.19.16.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">35.27</span></span> <span id="S5.T2.3.3.3.3.20.17" class="ltx_tr"> <span id="S5.T2.3.3.3.3.20.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-4 (8k)</span> <span id="S5.T2.3.3.3.3.20.17.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S5.T2.3.3.3.3.20.17.2.1" class="ltx_text ltx_font_bold">53.90</span></span> <span id="S5.T2.3.3.3.3.20.17.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">34.16</span> <span id="S5.T2.3.3.3.3.20.17.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">40.29</span> <span id="S5.T2.3.3.3.3.20.17.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.01</span> <span id="S5.T2.3.3.3.3.20.17.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.42</span> <span id="S5.T2.3.3.3.3.20.17.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">43.39</span> <span id="S5.T2.3.3.3.3.20.17.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">81.28</span> <span id="S5.T2.3.3.3.3.20.17.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">79.21</span> <span id="S5.T2.3.3.3.3.20.17.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">45.09</span> <span id="S5.T2.3.3.3.3.20.17.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.81</span> <span id="S5.T2.3.3.3.3.20.17.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">36.34</span></span> </span> </span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span>10개의 데이터 세트에 걸쳐 제로샷 대화 QA 결과. <sup class="ltx_sup" id="S5.T2.13.1">♢</sup>은 HumanAnnotatedConvQA가 SyntheticConvQA로 대체되었음을 나타낸다. <sup class="ltx_sup" id="S5.T2.14.2">†</sup>는 stage-1(SFT)이 제거되고 ChatQA stage-2 튜닝만 적용됨을 나타낸다. *단일 전환 QA 데이터 세트가 ChatQA 단계 2 훈련 혼합에서 제거됨을 나타냅니다. "w/o stage-1" 및 "w/o single-turn" 설정 모두에서 SyntheticConvQA 데이터가 사용됩니다. 평균 점수 측면에서 우리의 ChatQA 모델은 SFT 및 Chat 대응 모델을 크게 능가한다. 최상의 모델인 ChatQA-70B는 GPT-4보다 약간 우수합니다. <sup class="ltx_sup" id="S5.T2.15.3">△</sup>은 ChatQA stage-2 훈련 블렌드에서 HumanAnnotatedConvQA가 제거되었음을 나타냅니다.</figcaption>
</figure>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Short Document Datasets</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">문서 길이의 다양성을 높이기 위해 짧은 문서(1.5K 단어 미만)를 가진 5개의 대화식 QA 데이터 세트를 수집한다. 평균적으로 1 단어는  1.5 토큰으로 토큰화될 것이다. 따라서, 문서는 4K 토큰의 시퀀스 길이를 갖는 LLM에 직접 피팅될 수 있다.</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para">
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i1.p1.1">CoQA<cite class="ltx_cite ltx_citemacro_citep">(Reddy et al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>)</cite>는 짧은 패시지에 각 대화를 기반으로 하는 대화형 QA 데이터 세트이다. 답은 일반적으로 짧고, 구절은 아동 이야기, 문학, 중/고등학교 시험, 뉴스, 위키피디아와 같은 광범위한 영역을 다룬다.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i2.p1.1">DoQA<cite class="ltx_cite ltx_citemacro_citep">(Campos et al., <a class="ltx_ref" href="#bib.bib7" title="">2020</a>)</cite>는 활성 Stack Exchange<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stackexchange.com/" target="_blank" title="">https://stackexchange.com/</a></span></span></span> 포럼에서 수집된 요리, 여행 및 영화의 세 가지 영역을 다룹니다. 데이터 세트에는 지정된 문서 내에서 답변을 찾을 수 없는 답변할 수 없는 경우가 포함되어 있습니다.</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i3.p1.1">ConvFinQA<cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="#bib.bib8" title="">2022a</a>)</cite>는 파이낸셜 도메인을 기반으로 한다. 각 문서에는 테이블을 둘러싼 관련 텍스트와 함께 단일 재무 보고서 테이블이 포함되어 있습니다. 이 데이터 세트는 산술 계산과 복잡한 숫자 추론을 포함한다.</p>
</div>
</li>
<li id="S5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i4.p1.1">SQA<cite class="ltx_cite ltx_citemacro_citep">(Pasupat &amp; Liang, <a class="ltx_ref" href="#bib.bib52" title="">2015</a>)</cite>는 주변 텍스트 없이 단일 테이블만 포함하는 문서에 근거한다. 문서들은 위키피디아에서 수집되고, 질문들은 매우 구성적이어서 정답을 제시할 수 있는 견고한 표 이해 능력을 가진 모델이 필요하다.</p>
</div>
</li>
<li id="S5.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i5.p1" class="ltx_para">
<p class="ltx_p" id="S5.I2.i5.p1.1">HybridDial<cite class="ltx_cite ltx_citemacro_citep">(Nakamura et al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>)</cite>는 위키피디아 표와 텍스트 데이터를 모두 포함하는 문서에 기반을 둔 대화형 QA 데이터 세트이다. 질문은 복잡하기 때문에 문서에 대한 추론이 필요하다.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">10개의 모든 데이터 세트에 걸쳐 ConvFinQA, SQA 및 HybridDial 데이터 세트는 문서에 표 형식의 데이터를 포함하고 나머지 데이터 세트의 문서는 텍스트 전용입니다. <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Details of these benchmark datasets are in the Appendix <a class="ltx_ref" href="#A3" title="Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">C</span></a>.</span></span></span></p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluation Metrics</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p1.1">F1 점수가 QA 모델을 평가하는 데 가장 일반적으로 사용되는 자동 메트릭이라는 점을 감안할 때 ConvFinQA를 제외한 모든 데이터 세트에 사용한다. ConvFinQA의 답변은 산술적 계산뿐만 아니라 문서로부터 숫자를 추출하는 것이기 때문에 ConvFinQA에서는 정확한 일치 메트릭을 사용하기 위해 <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="#bib.bib8" title="">2022a</a>)</cite>를 따른다. 따라서 답은 답과 정확히 같을 때만 말이 된다. 모델들이 산술 공식을 생성할 때, 우리는 계산기를 기반으로 최종 결과를 계산하고 금답과 비교할 것이다.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p2.1">또한, 우리는 또한 우리의 최상의 모델과 GPT-4 사이의 생성된 답변의 정확성을 평가하기 위해 인간 평가를 수행한다.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Main Results</h3>

<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>overview</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서 우리는 10개의 대화 QA 데이터 세트에 걸쳐 서로 다른 모델 변형과 OpenAI 모델을 비교한다.</p>
</div>
<div id="S6.SS1.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS1.p2.1">우리는 ChatQA 방법이 모델의 대화 QA 능력을 크게 향상시킨다는 것을 발견했다. 평균 점수 측면에서 Llama2-Chat 모델은 SFT 모델 대응물을 약간 능가하는 반면, 우리의 ChatQA 모델은 SFT 및 Chat 대응물에 비해 약 10점 이상의 절대 개선을 달성한다. 예를 들어, ChatQA-13B는 Llama2-13B-SFT 및 Llama2-13B-Chat에서 각각 13.17(37.69에서 50.86) 및 10.52(40.34에서 50.86) 향상된다. 이는 컨텍스트 강화 명령어 미세 조정을 통해 모델이 검색된 또는 관련 컨텍스트에서 유용한 정보를 효과적으로 추출하는 방법을 학습할 수 있기 때문이다.</p>
</div>
<div id="S6.SS1.SSS1.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS1.p3.1">OpenAI 모델에 비해 최고의 모델 ChatQA-70B는 GPT-3.5-터보를 평균 3.77점 능가하며 GPT-4를 평균 0.24점 능가할 수 있다. 또한 훨씬 작은 크기의 ChatQA-13B는 평균 점수 0.49만큼 GPT-3.5-터보를 약간 능가할 수 있다.</p>
</div>
<div id="S6.SS1.SSS1.p4" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS1.p4.1">특히, 제안된 2단계 명령어 튜닝은 GPT-22B-SFT를 평균 14.70점 향상시켰다. 그 결과, ChatQA-22B는 GPT-4보다 작은 간격(53.53 vs. 53.90)만큼만 뒤에 있다. Llama2-70B보다 모델 크기가 상당히 작음에도 불구하고. 이는 제안된 명령어 튜닝 방법의 효율성을 강조한다.</p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Importance of Stage-1 SFT</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1"><a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서는 ChatQA의 명령어 수행 능력을 향상시키는 stage-1 SFT의 중요도에 대한 절제 연구를 수행한다. 미세 조정 단계에서 단계-1 SFT를 제거하고 기초 LLM 위에 단계-2 컨텍스트 강화 명령 튜닝을 적용한다. 우리는 평균 점수가 1.9 (54.08에서 52.18로) 떨어진다는 것을 발견한다. SQA 외에도 단계-1을 제거하면 모델이 다른 데이터 세트에서 일관되게 성능이 저하된다. 결과는 단계-1의 모든 SFT 데이터 세트가 단계-2 명령어 튜닝에도 혼합되더라도 단계-1이 여전히 중요한 역할을 한다는 것을 나타낸다. 우리는 먼저 명령어 수행 능력을 구축하는 것이 2단계 튜닝에 도움이 된다고 결론지었다.</p>
</div>
</section>
<section id="S6.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>Effectiveness of Single-Turn Data</h4>

<div id="S6.SS1.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">단일 회전 QA 데이터 세트가 모델의 다중 회전 QA 능력에 어떻게 영향을 미치는지 조사하기 위해 단계 2의 ChatQA-70B 훈련 블렌드에서 제거하는 절제 연구를 수행한다. 표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>와 같이 단계 2 훈련 블렌드(ChatQA-70B)에 단일 회전 QA 데이터 세트를 통합하면 일반적으로 모든 벤치마크 데이터 세트에 걸쳐 점수가 증가하여 평균 1.83점 개선된다. 흥미롭게도 ConvFinQA, SQA 및 HybridDial(테이블 기반 데이터 세트)의 개선은 추가된 단일 턴 QA 데이터 세트가 문서에 테이블 데이터가 없음에도 불구하고 관찰된다. 이러한 결과는 우리의 직관과 일치한다. 단일 회전 데이터 세트를 추가하면 컨텍스트에서 답변을 추출할 수 있는 모델의 기능이 향상되어 대화형 QA 데이터 세트에서 더 나은 점수를 얻을 수 있다.</p>
</div>
</section>
<section id="S6.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4 </span>Effectiveness of Conversational QA Data</h4>

<div id="S6.SS1.SSS4.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS4.p1.1">대화형 QA 데이터가 ChatQA 단계-2 훈련 블렌드에서 HumanAnnotatedConvQA 데이터를 제거하여 모델의 다중 전환 QA 능력에 어떻게 영향을 미치는지 추가로 탐구한다. 표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에 예시된 바와 같이, "w/o ConvQAData"는 ChatQA-70B(평균 점수가 54.08에서 48.97로 저하됨)보다 훨씬 더 나쁜 결과를 만든다. QAC, QReCC 및 DoQA와 같은 텍스트 기반 문서를 사용하는 데이터 세트의 큰 열화를 관찰한다. 그러나 학습 블렌드에 TAT-QA <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="#bib.bib80" title="">2021</a>)</cite>가 있기 때문에 테이블 기반 문서(예: ConvFinQA, SQA)를 사용한 데이터 세트의 열화가 적다.</p>
</div>
<div id="S6.SS1.SSS4.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS4.p2.1">흥미롭게도, "w/o ConvQAData"는 여전히 Llama2-70B-SFT (48.97 대 43.22)를 능가할 수 있다. (48.97 vs. 45.21). 이것은 또한 맥락화된 QA 태스크를 처리하는 모델의 능력을 향상시키는 제안된 스테이지-2 튜닝의 효과를 입증한다.</p>
</div>
</section>
<section id="S6.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.5 </span>Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data</h4>

<div id="S6.SS1.SSS5.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS5.p1.1">표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에서 7k GPT-3.5-Turbo 합성 데이터 세트(SyntheticConvQA)와 수집된 7k 인간 주석 데이터 세트(HumanAnnotatedConvQA)를 사용하여 ChatQA 모델을 비교했다. 첫째, 평균 점수 측면에서 두 가지 모두 유사한 결과를 달성한다는 것을 발견했으며, 이는 최신 대화 QA 모델을 구축하기 위해 OpenAI 모델의 합성 데이터에 의존할 필요가 없음을 시사한다. 둘째, 사람 주석이 있는 데이터를 사용하여 QuAC 및 DoQA 데이터 세트에서 상당한 개선을 달성했음을 발견했다. 이는 QuAC 및 DoQA 데이터 세트에 존재하는 응답할 수 없는 경우에 대해 사람이 주석한 데이터가 더 높은 품질을 갖는다는 사실에 기인할 수 있다. 결국 이 두 데이터 세트의 전반적인 개선으로 이어집니다. 응답할 수 없는 사례에 대한 세부 결과 및 분석에서는 §<a class="ltx_ref" href="#S6.SS5" title="6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6.5</span></a>에서 찾을 수 있다.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<div id="S6.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:230.7pt;height:216pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S6.T3.3.1" class="ltx_p"><span id="S6.T3.3.1.1" class="ltx_text"> <span id="S6.T3.3.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S6.T3.3.1.1.1.1.1" class="ltx_tr"> <span id="S6.T3.3.1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span id="S6.T3.3.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Ours Win</span> <span id="S6.T3.3.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Tie</span> <span id="S6.T3.3.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-4 Win</span></span> <span id="S6.T3.3.1.1.1.2.2" class="ltx_tr"> <span id="S6.T3.3.1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">Average</span> <span id="S6.T3.3.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">13.81%</span> <span id="S6.T3.3.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">69.09%</span> <span id="S6.T3.3.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">17.10%</span></span> </span> <span class="ltx_tbody"> <span id="S6.T3.3.1.1.1.3.1" class="ltx_tr"> <span id="S6.T3.3.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">Doc2Dial</span> <span id="S6.T3.3.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">14.29%</span> <span id="S6.T3.3.1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">68.00%</span> <span id="S6.T3.3.1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">17.71%</span></span> <span id="S6.T3.3.1.1.1.4.2" class="ltx_tr"> <span id="S6.T3.3.1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">QuAC</span> <span id="S6.T3.3.1.1.1.4.2.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">11.67%</span> <span id="S6.T3.3.1.1.1.4.2.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.33%</span> <span id="S6.T3.3.1.1.1.4.2.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">15.00%</span></span> <span id="S6.T3.3.1.1.1.5.3" class="ltx_tr"> <span id="S6.T3.3.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">QReCC</span> <span id="S6.T3.3.1.1.1.5.3.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">11.11%</span> <span id="S6.T3.3.1.1.1.5.3.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.22%</span> <span id="S6.T3.3.1.1.1.5.3.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">11.67%</span></span> <span id="S6.T3.3.1.1.1.6.4" class="ltx_tr"> <span id="S6.T3.3.1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">CoQA</span> <span id="S6.T3.3.1.1.1.6.4.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">7.78%</span> <span id="S6.T3.3.1.1.1.6.4.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.00%</span> <span id="S6.T3.3.1.1.1.6.4.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">12.22%</span></span> <span id="S6.T3.3.1.1.1.7.5" class="ltx_tr"> <span id="S6.T3.3.1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">DoQA</span> <span id="S6.T3.3.1.1.1.7.5.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">22.78%</span> <span id="S6.T3.3.1.1.1.7.5.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">57.78%</span> <span id="S6.T3.3.1.1.1.7.5.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">19.44%</span></span> <span id="S6.T3.3.1.1.1.8.6" class="ltx_tr"> <span id="S6.T3.3.1.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">ConvFinQA</span> <span id="S6.T3.3.1.1.1.8.6.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">16.67%</span> <span id="S6.T3.3.1.1.1.8.6.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">67.78%</span> <span id="S6.T3.3.1.1.1.8.6.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">15.55%</span></span> <span id="S6.T3.3.1.1.1.9.7" class="ltx_tr"> <span id="S6.T3.3.1.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">SQA</span> <span id="S6.T3.3.1.1.1.9.7.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">11.11%</span> <span id="S6.T3.3.1.1.1.9.7.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">61.67%</span> <span id="S6.T3.3.1.1.1.9.7.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">27.22%</span></span> <span id="S6.T3.3.1.1.1.10.8" class="ltx_tr"> <span id="S6.T3.3.1.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">TopiOCQA</span> <span id="S6.T3.3.1.1.1.10.8.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">19.31%</span> <span id="S6.T3.3.1.1.1.10.8.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">60.69%</span> <span id="S6.T3.3.1.1.1.10.8.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">20.00%</span></span> <span id="S6.T3.3.1.1.1.11.9" class="ltx_tr"> <span id="S6.T3.3.1.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">HybridDial</span> <span id="S6.T3.3.1.1.1.11.9.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">7.78%</span> <span id="S6.T3.3.1.1.1.11.9.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">78.33%</span> <span id="S6.T3.3.1.1.1.11.9.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">13.89%</span></span> <span id="S6.T3.3.1.1.1.12.10" class="ltx_tr"> <span id="S6.T3.3.1.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">INSCIT</span> <span id="S6.T3.3.1.1.1.12.10.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">15.56%</span> <span id="S6.T3.3.1.1.1.12.10.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">66.11%</span> <span id="S6.T3.3.1.1.1.12.10.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">18.33%</span></span> </span> </span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span>Human evaluation (A/B testing) comparing our ChatQA-70B to GPT-4 over 10 datasets. 평균 점수에서, 우리의 모델과 GPT-4는 대부분의 시간(69.09%)에서 동점이고, GPT-4는 우리보다 약간 더 높은 승률(<math alttext="\sim" class="ltx_Math" display="inline" id="S6.T3.2.m1.1"><semantics id="S6.T3.2.m1.1b"><mo id="S6.T3.2.m1.1.1" xref="S6.T3.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.m1.1c"><csymbol cd="latexml" id="S6.T3.2.m1.1.1.cmml" xref="S6.T3.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.m1.1d">\sim</annotation></semantics></math>3.3%)을 달성한다.</figcaption>
</figure>
</section>
<section id="S6.SS1.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.6 </span>Human Evaluation</h4>

<div id="S6.SS1.SSS6.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS1.SSS6.p1.1">F1 점수가 QA 모델의 품질을 평가하는 데 가장 일반적으로 사용되는 척도임에도 불구하고 질문에 답하는 여러 가지 방법이 있어 자동 척도가 완벽하지 않다. 따라서 우리는 인간 평가를 사용하여 우리의 ChatQA-70B와 GPT-4를 추가로 비교한다. 이 인간 평가에서 주석자에게 ChatQA-70B 및 GPT-4의 출력에서 사실을 확인하고 어떤 모델이 <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>More human evaluation setup can be found in the Appendix <a class="ltx_ref" href="#A4" title="Appendix D Human Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">D</span></a>.</span></span></span> 질문에 더 정확한 응답을 제공하는지 결정한다. 10개의 데이터 세트에 대한 인간 평가 결과는 표 <a class="ltx_ref" href="#S6.T3" title="Table 3 ‣ 6.1.5 Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">3</span></a>에 나와 있다. 먼저, ChatQA-70B와 GPT-4는 대부분의 시간(69.09%)에서 동점이고, GPT-4는 우리보다 약간 높은 승률(<math alttext="\sim" class="ltx_Math" display="inline" id="S6.SS1.SSS6.p1.1.m1.1"><semantics id="S6.SS1.SSS6.p1.1.m1.1a"><mo id="S6.SS1.SSS6.p1.1.m1.1.1" xref="S6.SS1.SSS6.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS6.p1.1.m1.1b"><csymbol cd="latexml" id="S6.SS1.SSS6.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS6.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS6.p1.1.m1.1c">\sim</annotation></semantics></math>3.3%)을 달성한다. 이는 우리의 모델이 정답을 생성할 수 있는 강력한 능력을 가지고 있음을 더욱 확인시켜준다. 둘째, ConvFinQA에서 GPT-4보다 우리 모델의 승률이 약간 더 좋은 것으로 나타났으며, 이는 우리 모델의 강력한 산술 계산 능력을 나타낸다. 셋째, SQA 과제에서 GPT-4가 훨씬 더 나은 승률을 달성한다는 것을 발견했는데, 이는 표식 추론 과제에서 우리의 모델과 GPT-4 사이에 여전히 격차가 있음을 시사한다.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<div id="S6.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:314.5pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S6.T4.1.1" class="ltx_p"><span id="S6.T4.1.1.1" class="ltx_text"> <span id="S6.T4.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S6.T4.1.1.1.1.1.1" class="ltx_tr"> <span id="S6.T4.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Models</span> <span id="S6.T4.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-text</span> <span id="S6.T4.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-table</span> <span id="S6.T4.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-ret</span> <span id="S6.T4.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-nonret</span></span> </span> <span class="ltx_tbody"> <span id="S6.T4.1.1.1.1.2.1" class="ltx_tr"> <span id="S6.T4.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-13B</span> <span id="S6.T4.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">45.80</span> <span id="S6.T4.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">62.68</span> <span id="S6.T4.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">40.01</span> <span id="S6.T4.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">61.72</span></span> <span id="S6.T4.1.1.1.1.3.2" class="ltx_tr"> <span id="S6.T4.1.1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-70B</span> <span id="S6.T4.1.1.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T4.1.1.1.1.3.2.2.1" class="ltx_text ltx_font_bold">48.88</span></span> <span id="S6.T4.1.1.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">66.42</span> <span id="S6.T4.1.1.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T4.1.1.1.1.3.2.4.1" class="ltx_text ltx_font_bold">42.33</span></span> <span id="S6.T4.1.1.1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">65.96</span></span> <span id="S6.T4.1.1.1.1.4.3" class="ltx_tr"> <span id="S6.T4.1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-3.5-turbo (4k)</span> <span id="S6.T4.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.07</span> <span id="S6.T4.1.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">60.40</span> <span id="S6.T4.1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">40.41</span> <span id="S6.T4.1.1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">60.33</span></span> <span id="S6.T4.1.1.1.1.5.4" class="ltx_tr"> <span id="S6.T4.1.1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-4 (8k)</span> <span id="S6.T4.1.1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.96</span> <span id="S6.T4.1.1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T4.1.1.1.1.5.4.3.1" class="ltx_text ltx_font_bold">70.10</span></span> <span id="S6.T4.1.1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.58</span> <span id="S6.T4.1.1.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T4.1.1.1.1.5.4.5.1" class="ltx_text ltx_font_bold">66.22</span></span></span> </span> </span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4: </span> 서로 다른 데이터 세트 유형의 평균 점수에 대한 세밀한 연구. Avg-text는 Doc2Dial, QuAC, QReCC, CoQA, DoQA, TopiOCQA 및 INSCIT와 같이 문서에만 텍스트가 있는 데이터 세트를 다룬다.</figcaption><figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Fine-grained studies on average scores of different dataset types. Avg-text covers datasets where the documents only have text, including Doc2Dial, QuAC, QReCC, CoQA, DoQA, TopiOCQA, and INSCIT.
Avg-table covers datasets with table in the documents, including ConvFinQA, SQA, and HybridDial.
Avg-ret covers datasets with long documents requiring retrieval, including Doc2Dial, QuAC, QReCC, TopiOCQA, and INSCIT.
Avg-nonret covers datasets with short documents which do not require retrieval, including CoQA, DoQA, ConvFinQA, SQA, and HybridDial.</figcaption>
</figure>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Fine-grained Analyses</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS2.p1.1"><a class="ltx_ref" href="#S6.T4" title="Table 4 ‣ 6.1.6 Human Evaluation ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4</span></a>에서 대화 QA 벤치마크에서 서로 다른 데이터 세트 유형에 걸쳐 모델과 OpenAI 모델을 추가로 비교했다. ChatQA-70B와 GPT-4의 비교에서, ChatQA-70B는 텍스트 전용 문서(avg-text)에서 더 나은 결과를 달성하여 우수한 텍스트 이해 능력을 보여준다. 한편, GPT-4는 avg-table의 비교를 고려할 때 표 데이터에서 더 나은 QA 능력을 보여준다. 검색이 필요하거나 필요하지 않은 데이터 세트와 관련하여 ChatQA-70B와 GPT-4는 비슷합니다(간격은 평균 점수 1 이내).</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS2.p2.1">ChatQA-13B와 GPT-3.5-터보 간의 비교에서 ChatQA-13B는 Avg-table의 점수를 감안할 때 더 나은 표식 QA 능력을 보여준다. ChatQA-13B는 또한 텍스트 전용 문서 및 검색이 필요한 문서에서 GPT-3.5-터보와 동등한 반면, 문서가 검색이 필요하지 않은 데이터 세트에서 더 나은 점수를 보여준다.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<div id="S6.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:267.0pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S6.T5.1.1" class="ltx_p"><span id="S6.T5.1.1.1" class="ltx_text"> <span id="S6.T5.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S6.T5.1.1.1.1.1.1" class="ltx_tr"> <span id="S6.T5.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Models</span> <span id="S6.T5.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-CQA</span> <span id="S6.T5.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-ret</span> <span id="S6.T5.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-nonret</span></span> </span> <span class="ltx_tbody"> <span id="S6.T5.1.1.1.1.2.1" class="ltx_tr"> <span id="S6.T5.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-70B</span> <span id="S6.T5.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T5.1.1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">54.14</span></span> <span id="S6.T5.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">42.33</span> <span id="S6.T5.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T5.1.1.1.1.2.1.4.1" class="ltx_text ltx_font_bold">65.96</span></span></span> <span id="S6.T5.1.1.1.1.3.2" class="ltx_tr"> <span id="S6.T5.1.1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">- w/ “top-5” chunks</span> <span id="S6.T5.1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">54.04</span> <span id="S6.T5.1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T5.1.1.1.1.3.2.3.1" class="ltx_text ltx_font_bold">42.91</span></span> <span id="S6.T5.1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">65.16</span></span> </span> </span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span>Ablation study on using "top-5" retrieved chunk for the context for the stage-2 instruction tuning. 10개의 대화형 QA 데이터 세트(Avg-CQA), 5개의 검색 데이터 세트(Avg-ret) 및 5개의 비검색 데이터 세트(Avg-nonret)에 대한 평균 점수를 보고한다.</figcaption>
</figure>
<figure id="S6.T6" class="ltx_table">
<div id="S6.T6.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:606.6pt;height:163pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S6.T6.5.5" class="ltx_p"><span id="S6.T6.5.5.5" class="ltx_text"> <span id="S6.T6.5.5.5.5" class="ltx_tabular ltx_align_middle"> <span class="ltx_tbody"> <span id="S6.T6.5.5.5.5.6.1" class="ltx_tr"> <span id="S6.T6.5.5.5.5.6.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Models</span> <span id="S6.T6.5.5.5.5.6.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-Both</span> <span id="S6.T6.5.5.5.5.6.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-QuAC</span> <span id="S6.T6.5.5.5.5.6.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">QuAC (no*)</span> <span id="S6.T6.5.5.5.5.6.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">QuAC (yes*)</span> <span id="S6.T6.5.5.5.5.6.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-DoQA</span> <span id="S6.T6.5.5.5.5.6.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">DoQA (no*)</span> <span id="S6.T6.5.5.5.5.6.1.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">DoQA (yes*)</span> <span id="S6.T6.5.5.5.5.6.1.9" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg-CQA</span></span> <span id="S6.T6.5.5.5.5.7.2" class="ltx_tr"> <span id="S6.T6.5.5.5.5.7.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-70B</span> <span id="S6.T6.5.5.5.5.7.2.2" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span id="S6.T6.5.5.5.5.7.2.3" class="ltx_td ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span id="S6.T6.5.5.5.5.7.2.4" class="ltx_td ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span id="S6.T6.5.5.5.5.7.2.5" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span id="S6.T6.5.5.5.5.7.2.6" class="ltx_td ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span id="S6.T6.5.5.5.5.7.2.7" class="ltx_td ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span id="S6.T6.5.5.5.5.7.2.8" class="ltx_td ltx_border_rr ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span> <span id="S6.T6.5.5.5.5.7.2.9" class="ltx_td ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span></span> <span id="S6.T6.1.1.1.1.1" class="ltx_tr"> <span id="S6.T6.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">- 1k unanswerable<sup id="S6.T6.1.1.1.1.1.1.1" class="ltx_sup">†</sup></span> <span id="S6.T6.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">76.88</span> <span id="S6.T6.1.1.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.89</span> <span id="S6.T6.1.1.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">75.10</span> <span id="S6.T6.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">86.67</span> <span id="S6.T6.1.1.1.1.1.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.88</span> <span id="S6.T6.1.1.1.1.1.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">64.49</span> <span id="S6.T6.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.15pt;padding-bottom:0.15pt;">81.26</span> <span id="S6.T6.1.1.1.1.1.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T6.1.1.1.1.1.9.1" class="ltx_text ltx_font_bold">54.16</span></span></span> <span id="S6.T6.2.2.2.2.2" class="ltx_tr"> <span id="S6.T6.2.2.2.2.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">- <span id="S6.T6.2.2.2.2.2.1.1" class="ltx_text ltx_font_bold">1.5k unanswerable<sup id="S6.T6.2.2.2.2.2.1.1.1" class="ltx_sup"><span id="S6.T6.2.2.2.2.2.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span></span> <span id="S6.T6.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T6.2.2.2.2.2.2.1" class="ltx_text ltx_font_bold">77.25</span></span> <span id="S6.T6.2.2.2.2.2.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T6.2.2.2.2.2.3.1" class="ltx_text ltx_font_bold">80.76</span></span> <span id="S6.T6.2.2.2.2.2.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.66</span> <span id="S6.T6.2.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">83.85</span> <span id="S6.T6.2.2.2.2.2.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T6.2.2.2.2.2.6.1" class="ltx_text ltx_font_bold">73.74</span></span> <span id="S6.T6.2.2.2.2.2.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">68.81</span> <span id="S6.T6.2.2.2.2.2.8" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.15pt;padding-bottom:0.15pt;">78.67</span> <span id="S6.T6.2.2.2.2.2.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T6.2.2.2.2.2.9.1" class="ltx_text ltx_font_bold">54.14</span></span></span> <span id="S6.T6.3.3.3.3.3" class="ltx_tr"> <span id="S6.T6.3.3.3.3.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">- 2k unanswerable<sup id="S6.T6.3.3.3.3.3.1.1" class="ltx_sup">†</sup></span> <span id="S6.T6.3.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.10</span> <span id="S6.T6.3.3.3.3.3.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.82</span> <span id="S6.T6.3.3.3.3.3.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.59</span> <span id="S6.T6.3.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">84.05</span> <span id="S6.T6.3.3.3.3.3.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.38</span> <span id="S6.T6.3.3.3.3.3.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">67.95</span> <span id="S6.T6.3.3.3.3.3.8" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.15pt;padding-bottom:0.15pt;">78.80</span> <span id="S6.T6.3.3.3.3.3.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">53.86</span></span> <span id="S6.T6.4.4.4.4.4" class="ltx_tr"> <span id="S6.T6.4.4.4.4.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">- 2.5k unanswerable<sup id="S6.T6.4.4.4.4.4.1.1" class="ltx_sup">†</sup></span> <span id="S6.T6.4.4.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">75.87</span> <span id="S6.T6.4.4.4.4.4.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">78.81</span> <span id="S6.T6.4.4.4.4.4.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.76</span> <span id="S6.T6.4.4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">83.85</span> <span id="S6.T6.4.4.4.4.4.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.93</span> <span id="S6.T6.4.4.4.4.4.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">66.54</span> <span id="S6.T6.4.4.4.4.4.8" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.15pt;padding-bottom:0.15pt;">79.31</span> <span id="S6.T6.4.4.4.4.4.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">53.78</span></span> <span id="S6.T6.5.5.5.5.5" class="ltx_tr"> <span id="S6.T6.5.5.5.5.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">- SyntheticConvQA<sup id="S6.T6.5.5.5.5.5.1.1" class="ltx_sup">♢</sup></span> <span id="S6.T6.5.5.5.5.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">69.84</span> <span id="S6.T6.5.5.5.5.5.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">72.92</span> <span id="S6.T6.5.5.5.5.5.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">55.38</span> <span id="S6.T6.5.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">90.42</span> <span id="S6.T6.5.5.5.5.5.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">66.77</span> <span id="S6.T6.5.5.5.5.5.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">45.09</span> <span id="S6.T6.5.5.5.5.5.8" class="ltx_td ltx_align_center ltx_border_rr" style="padding-top:0.15pt;padding-bottom:0.15pt;">88.45</span> <span id="S6.T6.5.5.5.5.5.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">54.08</span></span> <span id="S6.T6.5.5.5.5.8.3" class="ltx_tr"> <span id="S6.T6.5.5.5.5.8.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-3.5-turbo (4k)</span> <span id="S6.T6.5.5.5.5.8.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.27</span> <span id="S6.T6.5.5.5.5.8.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">78.34</span> <span id="S6.T6.5.5.5.5.8.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">61.91</span> <span id="S6.T6.5.5.5.5.8.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">94.76</span> <span id="S6.T6.5.5.5.5.8.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">68.21</span> <span id="S6.T6.5.5.5.5.8.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.99</span> <span id="S6.T6.5.5.5.5.8.3.8" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">84.43</span> <span id="S6.T6.5.5.5.5.8.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.37</span></span> <span id="S6.T6.5.5.5.5.9.4" class="ltx_tr"> <span id="S6.T6.5.5.5.5.9.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-4 (8k)</span> <span id="S6.T6.5.5.5.5.9.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T6.5.5.5.5.9.4.2.1" class="ltx_text ltx_font_bold">80.73</span></span> <span id="S6.T6.5.5.5.5.9.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T6.5.5.5.5.9.4.3.1" class="ltx_text ltx_font_bold">87.42</span></span> <span id="S6.T6.5.5.5.5.9.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">83.45</span> <span id="S6.T6.5.5.5.5.9.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:0.15pt;padding-bottom:0.15pt;">91.38</span> <span id="S6.T6.5.5.5.5.9.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T6.5.5.5.5.9.4.6.1" class="ltx_text ltx_font_bold">74.05</span></span> <span id="S6.T6.5.5.5.5.9.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">74.28</span> <span id="S6.T6.5.5.5.5.9.4.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.82</span> <span id="S6.T6.5.5.5.5.9.4.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="S6.T6.5.5.5.5.9.4.9.1" class="ltx_text ltx_font_bold">53.90</span></span></span> </span> </span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 6:</span>Accuracies on answerable and unanswerable samples across QuAC and DoQA datasets. Avg-둘 다 QuAC와 DoQA 사이의 평균 점수이고 Avg-CQA는 10개의 대화 QA 데이터 세트에 걸친 평균 점수이다. <sup class="ltx_sup" id="S6.T6.12.1">♢</sup>은 HumanAnnotatedConvQA가 SyntheticConvQA로 대체되었음을 나타낸다. *"아니오"는 답변할 수 없는 샘플을 나타내고, "예"는 답변할 수 있는 샘플을 나타낸다. <sup class="ltx_sup" id="S6.T6.13.2">†</sup>우리는 HumanAnnotatedConvQA에서 unanswerable 샘플 수 측면에서 ablation 연구를 수행합니다. 우리는 고품질 생성과 환각을 모두 생성하기 때문에 최종 ChatQA-70B에 1.5k의 응답할 수 없는 샘플을 사용한다.</figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Top-<em id="S6.SS3.1.1" class="ltx_emph ltx_font_italic">k</em> Chunks for Stage-2 Instruction Tuning</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS3.p1.1">스테이지-2 튜닝에 사용되는 모든 데이터 세트에 대해, 컨텍스트는 답변을 포함하는 연속적인 단락 또는 문서로 제공된다. 대조적으로, 모델은 긴 문서에 대한 추론 동안 상위-<em class="ltx_emph ltx_font_italic" id="S6.SS3.p1.1.1">k</em> 검색된 청크를 처리할 필요가 있다. 이 train/test 불일치를 해결하기 위해 일부 연속 문단을 검색된 top-<em class="ltx_emph ltx_font_italic" id="S6.SS3.p1.1.2">k</em> 청크로 대체하면 모델의 견고성이 향상되는지 여부를 조사합니다.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS3.p2.1">각 질문에는 해당 긴 문서가 있기 때문에 이 연구에서는 NarrativeQA를 사용한다. 처음에는 답변이 포함된 긴 문서의 요약을 문맥으로 사용한다. 불연속적인 컨텍스트를 통합하기 위해 먼저 긴 문서를 300개의 단어 청크로 절단한다. 그런 다음 드래곤 리트리버를 사용하여 질문과 관련된 상위 4개의 청크를 추가 컨텍스트로 검색한다. 마지막으로, 긴 문서의 요약과 함께 검색된 4개의 청크를 "top-5" 청크로 간주한다. <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Note that, we do not directly use top-5 retrieved chunks for training, because they may not contain the answer. In such cases, fine-tuning the model to generate answer could encourage hallucination.</span></span></span> 이 재구성된 NarrativeQA를 사용하여 stage-2 명령어 튜닝을 위해 원래 NarrativeQA를 대체합니다.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS3.p3.1"><a class="ltx_ref" href="#S6.T5" title="Table 5 ‣ 6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">5</span></a>에서 "top-5" 청크를 훈련의 컨텍스트로 사용하면 검색이 필요한 데이터 세트의 개선으로 이어진다는 것을 관찰한다. 그러나 검색되지 않은 데이터 세트에서는 성능이 저하됩니다. 전반적으로 이 두 모델은 비슷합니다. 단계-2 튜닝에서 "top-5" 검색된 청크를 통합하는 것은 검색이 필요한 추론 단계와 일치하여 Avg-ret 점수가 향상되기 때문이다. 그러나 연속 및 불연속 문서의 혼합은 2단계 튜닝을 덜 안정적으로 만들 수 있으며, 이는 비검색 데이터 세트에 대한 차선 결과를 초래할 수 있다. 연속 컨텍스트와 top-<em class="ltx_emph ltx_font_italic" id="S6.SS3.p3.1.1">k</em> 검색된 청크를 통합하는 균형과 관련하여 더 많은 미래 작업이 수행될 수 있다고 믿는다.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<div id="S6.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:373.4pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S6.T7.1.1" class="ltx_p"><span id="S6.T7.1.1.1" class="ltx_text"> <span id="S6.T7.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle"> <span class="ltx_thead"> <span id="S6.T7.1.1.1.1.1.1" class="ltx_tr"> <span id="S6.T7.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Models</span> <span id="S6.T7.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">Avg.</span> <span id="S6.T7.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">D2D</span> <span id="S6.T7.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">QuAC</span> <span id="S6.T7.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">QReCC</span> <span id="S6.T7.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">TopiO</span> <span id="S6.T7.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.15pt;padding-bottom:0.15pt;">INSCIT</span></span> </span> <span class="ltx_tbody"> <span id="S6.T7.1.1.1.1.2.1" class="ltx_tr"> <span id="S6.T7.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-70B</span> <span id="S6.T7.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">42.31</span> <span id="S6.T7.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">39.19</span> <span id="S6.T7.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.33</span> <span id="S6.T7.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.73</span> <span id="S6.T7.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.30</span> <span id="S6.T7.1.1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.98</span></span> <span id="S6.T7.1.1.1.1.3.2" class="ltx_tr"> <span id="S6.T7.1.1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">- # of ctx: top-3</span> <span id="S6.T7.1.1.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">41.91</span> <span id="S6.T7.1.1.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.20</span> <span id="S6.T7.1.1.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.35</span> <span id="S6.T7.1.1.1.1.3.2.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.94</span> <span id="S6.T7.1.1.1.1.3.2.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.78</span> <span id="S6.T7.1.1.1.1.3.2.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">32.27</span></span> <span id="S6.T7.1.1.1.1.4.3" class="ltx_tr"> <span id="S6.T7.1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">- # of ctx: top-10</span> <span id="S6.T7.1.1.1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">40.71</span> <span id="S6.T7.1.1.1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.06</span> <span id="S6.T7.1.1.1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">36.95</span> <span id="S6.T7.1.1.1.1.4.3.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.61</span> <span id="S6.T7.1.1.1.1.4.3.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.40</span> <span id="S6.T7.1.1.1.1.4.3.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">32.53</span></span> <span id="S6.T7.1.1.1.1.5.4" class="ltx_tr"> <span id="S6.T7.1.1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">- ctx reverse ordering</span> <span id="S6.T7.1.1.1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">42.48</span> <span id="S6.T7.1.1.1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">39.08</span> <span id="S6.T7.1.1.1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.85</span> <span id="S6.T7.1.1.1.1.5.4.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.63</span> <span id="S6.T7.1.1.1.1.5.4.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.16</span> <span id="S6.T7.1.1.1.1.5.4.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.69</span></span> <span id="S6.T7.1.1.1.1.6.5" class="ltx_tr"> <span id="S6.T7.1.1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">- ctx swing ordering</span> <span id="S6.T7.1.1.1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">42.30</span> <span id="S6.T7.1.1.1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">39.35</span> <span id="S6.T7.1.1.1.1.6.5.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.09</span> <span id="S6.T7.1.1.1.1.6.5.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.09</span> <span id="S6.T7.1.1.1.1.6.5.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.98</span> <span id="S6.T7.1.1.1.1.6.5.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.99</span></span> <span id="S6.T7.1.1.1.1.7.6" class="ltx_tr"> <span id="S6.T7.1.1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.15pt;padding-bottom:0.15pt;">- ctx random ordering</span> <span id="S6.T7.1.1.1.1.7.6.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">42.01</span> <span id="S6.T7.1.1.1.1.7.6.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">39.32</span> <span id="S6.T7.1.1.1.1.7.6.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.28</span> <span id="S6.T7.1.1.1.1.7.6.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.79</span> <span id="S6.T7.1.1.1.1.7.6.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.13</span> <span id="S6.T7.1.1.1.1.7.6.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">33.51</span></span> <span id="S6.T7.1.1.1.1.8.7" class="ltx_tr"> <span id="S6.T7.1.1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">- Dragon Retrieval</span> <span id="S6.T7.1.1.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">40.50</span> <span id="S6.T7.1.1.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">37.92</span> <span id="S6.T7.1.1.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">38.44</span> <span id="S6.T7.1.1.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.88</span> <span id="S6.T7.1.1.1.1.8.7.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.39</span> <span id="S6.T7.1.1.1.1.8.7.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">27.87</span></span> </span> </span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 7:</span>검색이 필요한 데이터 세트에 걸쳐 입력 컨텍스트에 대한 어블레이션 연구. 모든 모델은 SyntheticConvQA를 사용합니다. D2D는 Doc2Dial을 나타내고, TopiO는 TopiOCQA를 나타낸다. 입력(# of ctx), 컨텍스트 순서(역, 스윙, 랜덤)에 사용된 컨텍스트의 수, 원본 드래곤에서 검색된 컨텍스트의 사용에 대해 연구한다. 이에 비해 ChatQA-70B(기본 설정)는 “Dragon + Fine-tune”을 사용하여 Top-5 컨텍스트를 검색하고, Top-5에서 첫 번째부터 다섯 번째 컨텍스트까지 순차적으로 배열한다.</figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Ablation Studies for Inference Stage</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS4.p1.1"><a class="ltx_ref" href="#S6.T7" title="Table 7 ‣ 6.3 Top-k Chunks for Stage-2 Instruction Tuning ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">7</span></a>에서는 검색된 컨텍스트/청크 수, 컨텍스트 순서 및 서로 다른 검색자가 대화 QA 결과에 어떤 영향을 미치는지에 대한 절제 연구를 보여준다.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS4.p2.1">첫째, 더 많은 컨텍스트를 입력으로 사용하는 것이 항상 결과를 향상시키는 것은 아니라는 것을 발견한다. Top-5 컨텍스트를 입력으로 사용하면 Top-3 또는 Top-10 컨텍스트를 사용하는 것보다 더 나은 결과를 얻을 수 있다. 직관적으로, 더 많은 컨텍스트는 정답(더 나은 회상 점수)을 포함할 확률이 더 높다. 결과적으로 top-5 컨텍스트를 사용하는 것이 top-3 컨텍스트를 사용하는 것보다 더 나은 결과를 얻을 수 있다. 그러나 컨텍스트의 수가 더 증가함에 따라 모델은 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib41" title="">2023</a>)</cite>와 같은 "중간에서 손실" 현상을 겪을 수 있으며 제공된 컨텍스트에서 답변을 추출하는 어려움도 증가할 수 있으며, 이는 Top-10 컨텍스트를 사용하여 열등한 결과를 초래할 수 있다.</p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS4.p3.9">둘째, 상위 5개 맥락의 서로 다른 순서를 사용하는 것이 결과에 어떤 영향을 미치는지 연구한다. 순차 순서(<math alttext="1" class="ltx_Math" display="inline" id="S6.SS4.p3.1.m1.1"><semantics id="S6.SS4.p3.1.m1.1a"><mn id="S6.SS4.p3.1.m1.1.1" xref="S6.SS4.p3.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.1.m1.1b"><cn id="S6.SS4.p3.1.m1.1.1.cmml" type="integer" xref="S6.SS4.p3.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.1.m1.1c">1</annotation></semantics></math>st context to <math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p3.2.m2.1"><semantics id="S6.SS4.p3.2.m2.1a"><mn id="S6.SS4.p3.2.m2.1.1" xref="S6.SS4.p3.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.2.m2.1b"><cn id="S6.SS4.p3.2.m2.1.1.cmml" type="integer" xref="S6.SS4.p3.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.2.m2.1c">5</annotation></semantics></math>th context)를 역 순서(<math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p3.3.m3.1"><semantics id="S6.SS4.p3.3.m3.1a"><mn id="S6.SS4.p3.3.m3.1.1" xref="S6.SS4.p3.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.3.m3.1b"><cn id="S6.SS4.p3.3.m3.1.1.cmml" type="integer" xref="S6.SS4.p3.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.3.m3.1c">5</annotation></semantics></math>th to <math alttext="1" class="ltx_Math" display="inline" id="S6.SS4.p3.4.m4.1"><semantics id="S6.SS4.p3.4.m4.1a"><mn id="S6.SS4.p3.4.m4.1.1" xref="S6.SS4.p3.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.4.m4.1b"><cn id="S6.SS4.p3.4.m4.1.1.cmml" type="integer" xref="S6.SS4.p3.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.4.m4.1c">1</annotation></semantics></math>st context)로 비교하고, 스윙 순서(“중간에서 잃어버린” 현상이 주어졌을 때, 입력 컨텍스트의 시작과 끝에 나타날 가장 관련성이 높은 컨텍스트를 배열한다. 따라서, 오더링은 {<math alttext="1" class="ltx_Math" display="inline" id="S6.SS4.p3.5.m5.1"><semantics id="S6.SS4.p3.5.m5.1a"><mn id="S6.SS4.p3.5.m5.1.1" xref="S6.SS4.p3.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.5.m5.1b"><cn id="S6.SS4.p3.5.m5.1.1.cmml" type="integer" xref="S6.SS4.p3.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.5.m5.1c">1</annotation></semantics></math>st, <math alttext="3" class="ltx_Math" display="inline" id="S6.SS4.p3.6.m6.1"><semantics id="S6.SS4.p3.6.m6.1a"><mn id="S6.SS4.p3.6.m6.1.1" xref="S6.SS4.p3.6.m6.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.6.m6.1b"><cn id="S6.SS4.p3.6.m6.1.1.cmml" type="integer" xref="S6.SS4.p3.6.m6.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.6.m6.1c">3</annotation></semantics></math>rd, <math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p3.7.m7.1"><semantics id="S6.SS4.p3.7.m7.1a"><mn id="S6.SS4.p3.7.m7.1.1" xref="S6.SS4.p3.7.m7.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.7.m7.1b"><cn id="S6.SS4.p3.7.m7.1.1.cmml" type="integer" xref="S6.SS4.p3.7.m7.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.7.m7.1c">5</annotation></semantics></math>th, <math alttext="4" class="ltx_Math" display="inline" id="S6.SS4.p3.8.m8.1"><semantics id="S6.SS4.p3.8.m8.1a"><mn id="S6.SS4.p3.8.m8.1.1" xref="S6.SS4.p3.8.m8.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.8.m8.1b"><cn id="S6.SS4.p3.8.m8.1.1.cmml" type="integer" xref="S6.SS4.p3.8.m8.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.8.m8.1c">4</annotation></semantics></math>th, <math alttext="2" class="ltx_Math" display="inline" id="S6.SS4.p3.9.m9.1"><semantics id="S6.SS4.p3.9.m9.1a"><mn id="S6.SS4.p3.9.m9.1.1" xref="S6.SS4.p3.9.m9.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.9.m9.1b"><cn id="S6.SS4.p3.9.m9.1.1.cmml" type="integer" xref="S6.SS4.p3.9.m9.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.9.m9.1c">2</annotation></semantics></math>nd}가 되고, 랜덤 오더링(랜덤 셔플 the top-5 context)이 된다. 순차 순서를 사용하는 것이 역방향 순서와 스윙 순서를 사용하는 것과 비슷하고 무작위 셔플링이 약간 더 나쁘다는 것을 발견했다. 결과는 우리의 모델이 답의 위치에 관계없이 긴 컨텍스트에서 정답을 추출하는 데 탁월하다는 것을 나타낸다. ChatQA 미세 조정 과정에서 답변의 위치가 맥락 내에서 무작위로 분포되기 때문이다.</p>
</div>
<div id="S6.SS4.p4" class="ltx_para">
<p class="ltx_p" id="S6.SS4.p4.1">셋째, “Dragon + Fine-tune”을 원래의 non-finetuned Dragon retriever로 교체하면 평균 점수가 1.81(42.31에서 40.50) 하락하는 것을 관찰할 수 있다. 또한, INSCIT 데이터 세트에서 (33.98에서 27.87로) 두 검색기 사이의 큰 성능 차이로 인해 점수가 크게 떨어진다(표 <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">1</span></a>에 나타낸 바와 같이). 기본적으로 검색의 품질이 향상되면 질의 응답의 성능을 직접적으로 향상시킨다.</p>
</div>
<figure id="S6.T8" class="ltx_table">
<div id="S6.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:1030.9pt;height:670.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T8.1.1"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;"><span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1. VA 결정을 어필하는 경우</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.2.2.1.1.2"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.2.2.1.1.2" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">for the new process, keep reading to learn how to request and prepare</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.2.2.1.1.3"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.1.2.2.1. <span class="ltx_text" id="S6.T8.1.1.1.1.1.2.2.1.3.1.1" 스타일="color:#0000FF;">You’ll follow the new process if you’re : Appealing a VA decision</span></span></span> <span class="ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.2.2.1.1" style="color:#0000FF;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.1.1.1.1.1.1" style="color:#0000FF;"><span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.1.1.1.1.1.1.1 </span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.2"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: Do you want to learn exactly what to expect? </span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.2.2.2.1.3"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.2.2.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">…</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.2.2.2.1.4"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.1.1.1.1.1.1.4.1" style="padding-top:0.15pt;pad </span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.2.2.2.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: You can request a video conference hearing if you want</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.2.2.1.6"><span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.2.2.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">to provide testimony to a judge about your appeal. </span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.7"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: When will I follow the new process for a Board hearing; <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">span class="ltx_tabular ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.1.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant:</span></span></span></span> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.1.2.2.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Chat <span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.2.2.3.1.1" 스타일="color:#41B729;">GPT-4: <span class="ltx_text" id="S6.T8.1.1.1.1.1.1.2.2.3.1.1.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.2.3.1.1.1.1" 스타일="color:#41B729;">2월 19일 또는 그 이후에 받은 VA 결정 "2019</span></span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.1.2.2.3.1 </span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.2"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">There was no time or intention to turn her to White Walker. Let us look the</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.1.3.1.1.3" style="color:#0000FF;">span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.3.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">scene from S04E04. <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.3.1.1.1.4" style="padding-top:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.1.1.1.3.3.1.1.1.1" style="color:#0000FF;"> Arya</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.5"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">drops the dagger, Night King look surprised. 이것은 그가 무엇</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.6"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">would happen next. 생각할 시간이 없었고 그녀를 흰색 워커로 바꿀 시간이 없었다</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.7"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.7.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">터치로. 이 장면에서는 S04E04와 같은 계획이 없었습니다. …</span></span></span></span></span></span></span></span></span></span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.3.3.2" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.3.3.2.1.1"><span class="ltx_tr" id="S6.T8.1.1.1.1.1.3.3.2.1.1"><span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.3.3.2 <span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.3.3.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: Arya was quick enough to react, drop the dagger and</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.2.1.3"><span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.3.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">do the rest. </span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.2.1.4"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.3.3.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: How old is the child that lay on the altar of ice? <span class="ltx_tabular ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.3.3.2.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;">span class="ltx_tabular ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.3.3.2.1.5" style="padding-top:0.15pt;">a baby</span>">Assistant:</span></span></span></span></span></span></span></span></span></span></span></span><span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.3.3.3 <span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.3.3.1.2.1" style="color:#41B729;">The text does not provide the exact age of the</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.3.3.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-4: <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.3.3.3.1.2.1.2"><span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.3.3.1.1. <span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.1.1.1.1.4.1.1.1.4.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tan></span></span> <span class="ltx_tabular ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding <span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.4.4.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: Chornohora, Uholka / Wide Meadow, Svydovets,</span></span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.1.4.4.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">Maramoros, Kuziy / Trybushany</span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.4.4.2.1.4" style="padding-top:0.15pt;">Span class="ltx_td ltx_nopad <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.4.4.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.1.1.1.1.4.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.1.1.4.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_ <span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.6.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: 24 April 1944, 27 May 1944, 18 August 1944, 13 September</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.6.6.2.1.1.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;">1944, 6 October 1944, 27 November 1944, 14 January 1945 …</span></span> <span class="ltx_tr" id="S6.T8.1.1.1.1.1.6.6.2.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: on <span class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T8.1.1.1.1.1.6.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant:</span></span> <span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.1.1.6.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-70B: <span class="ltx_text" id="S6.T8.1.1.1.1.1.1.6.3.1.1" style="color:#41B729;">6 October 1944, 13 September 1944, 27 November 1944</span></span></span> <span class="ltx_tr" id="S6.</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 8:</span>ChatQA-70B 및 GPT-4에 대한 사례 연구. <span class="ltx_text" id="S6.T8.5.1" style="color:#0000FF;">blue</span> color를 사용하여 마지막 사용자 질문에 대한 관련 컨텍스트를 강조 표시합니다. 그리고 <span class="ltx_text" id="S6.T8.6.2" style="color:#41B729;">green</span> and <span class="ltx_text" id="S6.T8.7.3" style="color:#FF0000;">red</span> color를 사용하여 어시스턴트 응답에서 올바르고 잘못된 부분을 강조 표시합니다.</figcaption>
</figure>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Evaluation of Unanswerable Case</h3>

<section id="S6.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.1 </span>Evaluation Setup</h4>

<div id="S6.SS5.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS5.SSS1.p1.1">이 절에서는 제공된 맥락 내에서 질문에 답할 수 있는지 식별하는 모델의 능력의 또 다른 측면을 연구한다. 답할 수 없는 경우에 답을 생성하는 것은 환각을 초래할 것이다. 이 평가를 허용하기 위해 주어진 맥락에서 답을 찾을 수 없을 때 모델을 표시해야 한다.</p>
</div>
<div id="S6.SS5.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS5.SSS1.p2.1">QAC 및 DoQA 데이터 세트를 사용하여 이러한 성능을 평가합니다. 구체적으로, 응답할 수 없는 경우에 대해, 질문이 정답일 수 없다는 것을 나타내는 모델 <span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We collect a set of heuristic matching patterns from all generated samples to determine if the model suggests that the question is unanswerable. More details can be found in the Appendix <a class="ltx_ref" href="#A5" title="Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">E</span></a>.</span></span></span>을 고려하고, 응답할 수 있는 경우에 대해, 질문이 정답일 수 없다는 것을 나타내지 않는 모델(즉, 답변을 주는 모델)을 고려한다. 답변 가능한 경우 올바른 컨텍스트가 검색되는 샘플만 선택합니다. 우리의 모델 체크포인트(ChatQA-70B w/1.5k unanswerable 및 ChatQA-70B w/SyntheticConvQA)는 표 <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>에 보고된 것과 동일하다.</p>
</div>
<div id="S6.SS5.SSS1.p3" class="ltx_para">
<p class="ltx_p" id="S6.SS5.SSS1.p3.1">결국, 우리는 답할 수 없는 경우와 답할 수 없는 경우의 평균 정확도 점수를 최종 메트릭으로 계산한다. 이 평균 정확도는 정확도와 재현율 점수의 조화 평균을 측정하는 F1 메트릭과 동일하기 때문에 신뢰할 수 있는 메트릭으로 간주한다.</p>
</div>
</section>
<section id="S6.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.2 </span>Results</h4>

<div id="S6.SS5.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS5.SSS2.p1.1">표 <a class="ltx_ref" href="#S6.T6" title="Table 6 ‣ 6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6</span></a>에서 우리는 QuAC 및 DoQA 데이터 세트에 걸쳐 OpenAI 모델과 모델을 비교한다. 먼저, HumanAnnotatedConvQA를 사용하면 SyntheticConvQA를 사용하는 것과 비교하여 QuAC와 DoQA 모두에서 평균 정확도가 크게 증가한다는 것을 발견했다. 이는 사람이 주석을 달지 않은 데이터에 대해 응답할 수 없는 주석이 더 품질이 높기 때문에 응답할 수 없는 경우에 대한 정확도가 크게 향상되기 때문이다. 둘째, OpenAI 모델은 GPT-4의 경우, GPT-3.5-turbo 모델보다 높은 평균 정확도를 보였으나, GPT-4(약 3.5%)에 비해 약간의 차이를 보였다. 셋째, 응답할 수 없는 표본에서 더 높은 정확도를 달성하는 모델은 응답할 수 있는 표본에서 더 낮은 정확도를 얻는 경향이 있고 그 반대의 경우도 마찬가지임을 알 수 있다. 우리는 모델이 "공격적인" 경향이 있고 답할 수 없는 질문에 대해 다소 적절한 답변을 제공할 때 답할 수 있는 경우에 대한 정확도를 높이지만 답할 수 없는 경우에 대한 정확도를 감소시킬 것이라고 추측한다. 반대로 어떤 모델이 더 '보수적'이고 질문에 답할 수 있는지 엄격하게 확인할 때 반대의 결과를 낳게 된다.</p>
</div>
<div id="S6.SS5.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS5.SSS2.p2.1">우리는 HumanAnnotatedConvQA에서 응답할 수 없는 샘플의 수로 절제 연구를 수행한다. 그 결과, 1.5k의 미응답 샘플을 사용하는 것이 전체 대화 QA 점수(Avg-CQA)뿐만 아니라 미응답 평가에서도 놀라운 결과를 얻을 수 있음을 알 수 있었다. 흥미롭게도, 더 많은 응답할 수 없는 샘플을 통합하는 것이 반드시 응답할 수 없는 평가에서 더 높은 정확도 점수로 이어지는 것은 아니다.</p>
</div>
</section>
</section>
<section id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Case Study</h3>

<div id="S6.SS6.p1" class="ltx_para">
<p class="ltx_p" id="S6.SS6.p1.1">표 <a class="ltx_ref" href="#S6.T8" title="Table 8 ‣ 6.4 Ablation Studies for Inference Stage ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">8</span></a>에서 ChatQA-70B 및 GPT-4 출력의 네 가지 예를 보여준다. 더 많은 예는 부록<a class="ltx_ref" href="#A6" title="Appendix F Case Study ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">F</span></a>에서 찾을 수 있다. 첫 번째 예제(약 <span class="ltx_text ltx_font_italic" id="S6.SS6.p1.1.1">Board hearing</span>)는 간단한 정보 탐색 문제이며 ChatQA-70B와 GPT-4 모두 올바르게 응답합니다. 두 번째 예(약 <span class="ltx_text ltx_font_italic" id="S6.SS6.p1.1.2">Arya</span>)에서 모델은 답변을 제공하기 위해 암시적 정보(파란색으로 강조 표시됨)를 찾아야 합니다. GPT-4는 답변을 제공하는 데 안전한 경향이 있으며 맥락이 연령에 대한 정확한 정보를 제공하지 않는다는 답변이 정확하다.</p>
</div>
<div id="S6.SS6.p2" class="ltx_para">
<p class="ltx_p" id="S6.SS6.p2.1">세 번째 예와 네 번째 예 모두 모델이 좋은 표의 이해와 추론 능력을 가질 것을 요구한다. 세 번째 예제(약 <span class="ltx_text ltx_font_italic" id="S6.SS6.p2.1.1">Massif</span>)에서 ChatQA-70B는 보존 영역의 크기를 3100 ha와 비교하여 정답을 제공하는 반면 GPT-4는 그렇지 않습니다. 네 번째 예(약 <span class="ltx_text ltx_font_italic" id="S6.SS6.p2.1.2">John B. England</span>)에서 ChatQA-70B는 세 날짜를 올바르게 나열하지만 한 날짜를 놓치는 반면 GPT-4는 질문에 올바르게 응답합니다.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p" id="S7.p1.1">이 논문에서 우리는 7B에서 70B까지 모델 크기가 다양한 ChatQA 모델 패밀리를 구축한다. 10개의 대화형 QA 데이터 세트에 대한 포괄적인 평가는 우리의 최상의 ChatQA-70B 모델이 ChatGPT 모델의 합성 데이터를 사용하지 않고도 GPT-3.5-터보를 현저하게 능가하고 GPT-4와 동등하게 수행할 수 있음을 보여준다. 또한, 큐레이션된 대화 QA 데이터를 사용하여 단일 회전 쿼리 리트리버를 미세 조정하면 재작성과 관련된 추가 계산 시간 및 잠재적인 API 비용이 발생하지 않고 최신 LLM 기반 쿼리 재작성 모델과 비교할 수 있음을 보여준다. 또한 소량의 "응답할 수 없는" 샘플을 통합하면 답변을 사용할 수 없는 시나리오를 처리할 수 있는 모델의 기능이 크게 향상될 수 있음을 보여 줍니다. 응답할 수 없는 사례 평가는 우리의 최상의 모델 ChatQA-70B가 GPT-4와 비교하여 약간의 차이만 있음을 강조한다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adlakha et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Adlakha, V., Dhuliawala, S., Suleman, K., de&nbsp;Vries, H., and Reddy, S.

</span>
<span class="ltx_bibblock">Topiocqa: Open-domain conversational question answering with topic switching.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">TACL</em>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aliannejadi et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Aliannejadi, M., Kiseleva, J., Chuklin, A., Dalton, J., and Burtsev, M.

</span>
<span class="ltx_bibblock">Building and evaluating open-domain dialogue corpora with clarifying questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anantha et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Anantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman, S., and Chappidi, S.

</span>
<span class="ltx_bibblock">Open-domain question answering goes conversational via question rewriting.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023a)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing 100k context windows, 2023a.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023b)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing Claude, 2023b.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brabant et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Brabant, Q., Lecorvé, G., and Barahona, L. M.&nbsp;R.

</span>
<span class="ltx_bibblock">Coqar: Question rewriting on coqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">LREC</em>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Campos et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Campos, J.&nbsp;A., Otegi, A., Soroa, A., Deriu, J.&nbsp;M., Cieliebak, M., and Agirre, E.

</span>
<span class="ltx_bibblock">Doqa-accessing domain-specific faqs via conversational qa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Chen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., and Wang, W.&nbsp;Y.

</span>
<span class="ltx_bibblock">Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2022a.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Chen, Z., Zhao, J., Fang, A., Fetahu, B., Rokhlenko, O., and Malmasi, S.

</span>
<span class="ltx_bibblock">Reinforced question rewriting for conversational question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2022b.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi, Y., Liang, P., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">Quac: Question answering in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Chu, Z., Chen, M., Chen, J., Wang, M., Gimpel, K., Faruqui, M., and Si, X.

</span>
<span class="ltx_bibblock">How to ask better questions? a large-scale multi-domain dataset for rewriting ill-formed questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Chung, H.&nbsp;W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S.&nbsp;S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E.&nbsp;H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.&nbsp;V., and Wei, J.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv: 2210.11416</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Conover, M., Hayes, M., Mathur, A., Meng, X., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Free dolly: Introducing the world’s first truly open instruction-tuned llm, 2023a.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R.

</span>
<span class="ltx_bibblock">Free Dolly: Introducing the world’s first truly open instruction-tuned llm, 2023b.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Dai, Z., Chaganty, A.&nbsp;T., Zhao, V., Amini, A., Green, M., Rashid, Q., and Guu, K.

</span>
<span class="ltx_bibblock">Dialog inpainting: Turning documents to dialogs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasigi et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Dasigi, P., Liu, N.&nbsp;F., Marasović, A., Smith, N.&nbsp;A., and Gardner, M.

</span>
<span class="ltx_bibblock">Quoref: A reading comprehension dataset with questions requiring coreferential reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Del&nbsp;Tredici et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Del&nbsp;Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and de&nbsp;Gispert, A.

</span>
<span class="ltx_bibblock">Question rewriting for open-domain conversational qa: Best practices and limitations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">CIKM</em>, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Deng, Y., Lei, W., Zhang, W., Lam, W., and Chua, T.-S.

</span>
<span class="ltx_bibblock">Pacific: Towards proactive conversational question answering over tabular and textual data in finance.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J.

</span>
<span class="ltx_bibblock">Glm: General language model pretraining with autoregressive blank infilling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M.

</span>
<span class="ltx_bibblock">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">NAACL</em>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elgohary et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Elgohary, A., Peskov, D., and Boyd-Graber, J.

</span>
<span class="ltx_bibblock">Can you unpack that? learning to rewrite questions-in-context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and Auli, M.

</span>
<span class="ltx_bibblock">Eli5: Long form question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Feng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and Lastras, L.

</span>
<span class="ltx_bibblock">doc2dial: A goal-oriented document-grounded dialogue dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galimzhanova et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Galimzhanova, E., Muntean, C.&nbsp;I., Nardini, F.&nbsp;M., Perego, R., and Rocchietti, G.

</span>
<span class="ltx_bibblock">Rewriting conversational utterances with instructed large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology</em>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Gao, C., Zhang, W., and Lam, W.

</span>
<span class="ltx_bibblock">Unigdd: A unified generative framework for goal-oriented document-grounded dialogue.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2023)</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Introducing bard, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Guo, M., Zhang, M., Reddy, S., and Alikhani, M.

</span>
<span class="ltx_bibblock">Abg-coqa: Clarifying ambiguity in conversational question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">AKBC</em>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Honovich, O., Scialom, T., Levy, O., and Schick, T.

</span>
<span class="ltx_bibblock">Unnatural instructions: Tuning language models with (almost) no human labor.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.09689</em>, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ishii et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Ishii, E., Xu, Y., Cahyawijaya, S., and Wilie, B.

</span>
<span class="ltx_bibblock">Can question rewriting help conversational question answering?

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Workshop on Insights from Negative Results in NLP</em>, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Iyer, S., Lin, X.&nbsp;V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P.&nbsp;S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Opt-iml: Scaling language model instruction meta learning through the lens of generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.12017</em>, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard &amp; Grave (2021)</span>
<span class="ltx_bibblock">
Izacard, G. and Grave, É.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Kim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y., Zhou, P., Bras, R.&nbsp;L., Alikhani, M., Kim, G., Sap, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Soda: Million-scale dialogue distillation with social commonsense contextualization.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10465</em>, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kočiskỳ et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Kočiskỳ, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.&nbsp;M., Melis, G., and Grefenstette, E.

</span>
<span class="ltx_bibblock">The narrativeqa reading comprehension challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">TACL</em>, 2018.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.&nbsp;M., Stanley, O., Nagyfi, R., et&nbsp;al.

</span>
<span class="ltx_bibblock">Openassistant conversations–democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.07327</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.&nbsp;M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A.

</span>
<span class="ltx_bibblock">Openassistant conversations - democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv: 2304.07327</em>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Lin, K., Tafjord, O., Clark, P., and Gardner, M.

</span>
<span class="ltx_bibblock">Reasoning over paragraph effects in situations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</em>, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Lin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y., Yih, W.-t., and Chen, X.

</span>
<span class="ltx_bibblock">How to train your dragon: Diverse augmentation towards generalizable dense retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.07452</em>, 2023a.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Lin, X.&nbsp;V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01352</em>, 2023b.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Liu, N.&nbsp;F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.03172</em>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.&nbsp;W., Tay, Y., Zhou, D., Le, Q.&nbsp;V., Zoph, B., Wei, J., et&nbsp;al.

</span>
<span class="ltx_bibblock">The flan collection: Designing data and methods for effective instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.13688</em>, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mele et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Mele, I., Muntean, C.&nbsp;I., Nardini, F.&nbsp;M., Perego, R., Tonellotto, N., and Frieder, O.

</span>
<span class="ltx_bibblock">Adaptive utterance rewriting for conversational search.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Information Processing &amp; Management</em>, 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing instructions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2022.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Mo, F., Mao, K., Zhu, Y., Wu, Y., Huang, K., and Nie, J.-Y.

</span>
<span class="ltx_bibblock">Convgqr: Generative query reformulation for conversational search.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15645</em>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Scao, T.&nbsp;L., Bari, M.&nbsp;S., Shen, S., Yong, Z.-X., Schoelkopf, H., et&nbsp;al.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.01786</em>, 2022.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakamura et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Nakamura, K., Levy, S., Tuan, Y.-L., Chen, W., and Wang, W.&nbsp;Y.

</span>
<span class="ltx_bibblock">Hybridialogue: An information-seeking dialogue dataset grounded on tabular and textual data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Findings of ACL</em>, 2022.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L.

</span>
<span class="ltx_bibblock">Ms marco: A human generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">choice</em>, 2016.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing ChatGPT, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat &amp; Liang (2015)</span>
<span class="ltx_bibblock">
Pasupat, P. and Liang, P.

</span>
<span class="ltx_bibblock">Compositional semantic parsing on semi-structured tables.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2015.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Qu, C., Yang, L., Chen, C., Qiu, M., Croft, W.&nbsp;B., and Iyyer, M.

</span>
<span class="ltx_bibblock">Open-retrieval conversational question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">SIGIR</em>, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.

</span>
<span class="ltx_bibblock">Squad: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2016.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Rajpurkar, P., Jia, R., and Liang, P.

</span>
<span class="ltx_bibblock">Know what you don’t know: Unanswerable questions for squad.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2018.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raposo et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Raposo, G., Ribeiro, R., Martins, B., and Coheur, L.

</span>
<span class="ltx_bibblock">Question rewriting? assessing its importance for conversational question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">ECIR</em>, 2022.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Reddy, S., Chen, D., and Manning, C.&nbsp;D.

</span>
<span class="ltx_bibblock">Coqa: A conversational question answering challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">TACL</em>, 2019.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saeidi et&nbsp;al. (2018)</span>
<span class="ltx_bibblock">
Saeidi, M., Bartolo, M., Lewis, P., Singh, S., Rocktäschel, T., Sheldon, M., Bouchard, G., and Riedel, S.

</span>
<span class="ltx_bibblock">Interpretation of natural language rules in conversational machine reading.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2018.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Sanh, V., Webson, A., Raffel, C., Bach, S.&nbsp;H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T.&nbsp;L., Raja, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trischler et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K.

</span>
<span class="ltx_bibblock">Newsqa: A machine comprehension dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on Representation Learning for NLP</em>, 2017.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vakulenko et&nbsp;al. (2021a)</span>
<span class="ltx_bibblock">
Vakulenko, S., Longpre, S., Tu, Z., and Anantha, R.

</span>
<span class="ltx_bibblock">Question rewriting for conversational question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">WSDM</em>, 2021a.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vakulenko et&nbsp;al. (2021b)</span>
<span class="ltx_bibblock">
Vakulenko, S., Voskarides, N., Tu, Z., and Longpre, S.

</span>
<span class="ltx_bibblock">A comparison of question rewriting methods for conversational passage retrieval.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">ECIR</em>, 2021b.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Instructretro: Instruction tuning post retrieval-augmented pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.07713</em>, 2023a.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F.

</span>
<span class="ltx_bibblock">Text embeddings by weakly-supervised contrastive pre-training.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.03533</em>, 2022a.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.&nbsp;A., Khashabi, D., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language model with self generated instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.10560</em>, 2022b.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022c)</span>
<span class="ltx_bibblock">
Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A.&nbsp;S., Arunkumar, A., Stap, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2022c.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K.&nbsp;R., Wadden, D., MacMillan, K., Smith, N.&nbsp;A., Beltagy, I., et&nbsp;al.

</span>
<span class="ltx_bibblock">How far can camels go? exploring the state of instruction tuning on open resources.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.04751</em>, 2023b.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022a)</span>
<span class="ltx_bibblock">
Wei, J., Bosma, M., Zhao, V.&nbsp;Y., Guu, K., Yu, A.&nbsp;W., Lester, B., Du, N., Dai, A.&nbsp;M., and Le, Q.&nbsp;V.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2022a.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022b)</span>
<span class="ltx_bibblock">
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.&nbsp;V., Zhou, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022b.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Wu, Z., Luan, Y., Rashkin, H., Reitter, D., Hajishirzi, H., Ostendorf, M., and Tomar, G.&nbsp;S.

</span>
<span class="ltx_bibblock">Conqrr: Conversational query rewriting for retrieval with reinforcement learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Wu, Z., Parish, R., Cheng, H., Min, S., Ammanabrolu, P., Ostendorf, M., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Inscit: Information-seeking conversations with mixed-initiative interactions.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">TACL</em>, 2023.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.12244</em>, 2023a.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Retrieval meets long context large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.03025</em>, 2023b.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Ye, F., Fang, M., Li, S., and Yilmaz, E.

</span>
<span class="ltx_bibblock">Enhancing conversational search: Large language model-aided informative query rewriting.

</span>
<span class="ltx_bibblock">In <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, pp.&nbsp; 5985–6006, 2023.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Yu, S., Liu, J., Yang, J., Xiong, C., Bennett, P., Gao, J., and Liu, Z.

</span>
<span class="ltx_bibblock">Few-shot generative conversational query rewriting.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">SIGIR</em>, 2020.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023a)</span>
<span class="ltx_bibblock">
Zhang, H., Diao, S., Lin, Y., Fung, Y.&nbsp;R., Lian, Q., Wang, X., Chen, Y., Ji, H., and Zhang, T.

</span>
<span class="ltx_bibblock">R-tuning: Teaching large language models to refuse unknown questions.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.09677</em>, 2023a.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023b)</span>
<span class="ltx_bibblock">
Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et&nbsp;al.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.10792</em>, 2023b.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et&nbsp;al.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11206</em>, 2023.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., and Chua, T.-S.

</span>
<span class="ltx_bibblock">Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>ChatQA Instruction Tuning</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Stage-1: Supervised Fine-tuning</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS1.p1.1">스테이지-1에서 LLM 입력의 포맷 템플릿은 다음과 같다:</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<pre id="A1.SS1.p2.1" class="ltx_verbatim ltx_font_typewriter">System: This is a chat between a user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user’s questions
based on the context. The assistant should also indicate when the answer cannot be
found in the context.

User: {Question 1}

Assistant: {Answer 1}

...

User: {Latest Question}

Assistant:
</pre>
</div>
<div id="A1.SS1.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS1.p3.1">모델 출력에 대한 감독으로 어시스턴트의 <math alttext="\{\texttt{Latest Answer}\}" class="ltx_Math" display="inline" id="A1.SS1.p3.1.m1.1"><semantics id="A1.SS1.p3.1.m1.1a"><mrow id="A1.SS1.p3.1.m1.1.2.2" xref="A1.SS1.p3.1.m1.1.2.1.cmml"><mo id="A1.SS1.p3.1.m1.1.2.2.1" stretchy="false" xref="A1.SS1.p3.1.m1.1.2.1.cmml">{</mo><mtext class="ltx_mathvariant_monospace" id="A1.SS1.p3.1.m1.1.1" xref="A1.SS1.p3.1.m1.1.1a.cmml">Latest Answer</mtext><mo id="A1.SS1.p3.1.m1.1.2.2.2" stretchy="false" xref="A1.SS1.p3.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.1.m1.1b"><set id="A1.SS1.p3.1.m1.1.2.1.cmml" xref="A1.SS1.p3.1.m1.1.2.2"><ci id="A1.SS1.p3.1.m1.1.1a.cmml" xref="A1.SS1.p3.1.m1.1.1"><mtext class="ltx_mathvariant_monospace" id="A1.SS1.p3.1.m1.1.1.cmml" xref="A1.SS1.p3.1.m1.1.1">Latest Answer</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.1.m1.1c">\{\texttt{Latest Answer}\}</annotation></semantics></math>를 사용합니다.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Stage-2: Context-Enhanced Instruction Tuning</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS2.p1.2">스테이지-1 형식 템플릿을 기반으로, 스테이지-2의 LLM 입력은 아래에 묘사된 바와 같이 <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p1.2.1">{Context for Latest Question}</span> 및 <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p1.2.2">{Instruction}</span> from <span class="ltx_text ltx_font_typewriter" id="A1.SS2.p1.2.3">User</span>을 추가합니다.</p>
<pre id="A1.SS2.p1.3" class="ltx_verbatim ltx_font_typewriter">System: This is a chat between a user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user’s questions
based on the context. The assistant should also indicate when the answer cannot be
found in the context.

{Context for Latest Question}

User: {Instruction} + {Question 1}

Assistant: {Answer 1}

...

User: {Latest Question}

Assistant:
</pre>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS2.p2.1">스테이지-1과 마찬가지로 모델 출력에 대한 감독으로 어시스턴트의 <math alttext="\{\texttt{Latest Answer}\}" class="ltx_Math" display="inline" id="A1.SS2.p2.1.m1.1"><semantics id="A1.SS2.p2.1.m1.1a"><mrow id="A1.SS2.p2.1.m1.1.2.2" xref="A1.SS2.p2.1.m1.1.2.1.cmml"><mo id="A1.SS2.p2.1.m1.1.2.2.1" stretchy="false" xref="A1.SS2.p2.1.m1.1.2.1.cmml">{</mo><mtext class="ltx_mathvariant_monospace" id="A1.SS2.p2.1.m1.1.1" xref="A1.SS2.p2.1.m1.1.1a.cmml">Latest Answer</mtext><mo id="A1.SS2.p2.1.m1.1.2.2.2" stretchy="false" xref="A1.SS2.p2.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.1.m1.1b"><set id="A1.SS2.p2.1.m1.1.2.1.cmml" xref="A1.SS2.p2.1.m1.1.2.2"><ci id="A1.SS2.p2.1.m1.1.1a.cmml" xref="A1.SS2.p2.1.m1.1.1"><mtext class="ltx_mathvariant_monospace" id="A1.SS2.p2.1.m1.1.1.cmml" xref="A1.SS2.p2.1.m1.1.1">Latest Answer</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.1.m1.1c">\{\texttt{Latest Answer}\}</annotation></semantics></math>를 사용한다.</p>
</div>
<div id="A1.SS2.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS2.p3.1"><math alttext="\{\texttt{Instruction}\}" class="ltx_Math" display="inline" id="A1.SS2.p3.1.m1.1"><semantics id="A1.SS2.p3.1.m1.1a"><mrow id="A1.SS2.p3.1.m1.1.2.2" xref="A1.SS2.p3.1.m1.1.2.1.cmml"><mo id="A1.SS2.p3.1.m1.1.2.2.1" stretchy="false" xref="A1.SS2.p3.1.m1.1.2.1.cmml">{</mo><mtext class="ltx_mathvariant_monospace" id="A1.SS2.p3.1.m1.1.1" xref="A1.SS2.p3.1.m1.1.1a.cmml">Instruction</mtext><mo id="A1.SS2.p3.1.m1.1.2.2.2" stretchy="false" xref="A1.SS2.p3.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.1.m1.1b"><set id="A1.SS2.p3.1.m1.1.2.1.cmml" xref="A1.SS2.p3.1.m1.1.2.2"><ci id="A1.SS2.p3.1.m1.1.1a.cmml" xref="A1.SS2.p3.1.m1.1.1"><mtext class="ltx_mathvariant_monospace" id="A1.SS2.p3.1.m1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1">Instruction</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.1.m1.1c">\{\texttt{Instruction}\}</annotation></semantics></math>는 답변 유형에 따라 서로 다른 데이터 세트에 대해 서로 다른 지침을 사용합니다. 자세한 내용은 아래에서 확인할 수 있습니다.</p>
</div>
<div id="A1.SS2.p4" class="ltx_para">
<p class="ltx_p" id="A1.SS2.p4.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p4.1.1">Please give a full and complete answer for the question.</span>” 이것은 긴 답변이 있는 데이터 세트에 대한 것입니다. 우리는 HumanAnnotatedConvQA 또는 SyntheticConvQA에 사용합니다.</p>
</div>
<div id="A1.SS2.p5" class="ltx_para">
<p class="ltx_p" id="A1.SS2.p5.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p5.1.1">Answer the following question with a short span. The answer needs to be just in a few words.</span>” 이것은 짧은 답변을 가진 데이터 세트에 대한 것입니다. 우리는 그것을 SQuAD1.1, SQuAD2.0, NarrativeQA, DROP, ROPES, NewsQA, Quoref에 사용한다.</p>
</div>
<div id="A1.SS2.p6" class="ltx_para">
<p class="ltx_p" id="A1.SS2.p6.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p6.1.1">Answer a number from context or the math arithmetic using +, -, *, or /.</span>” 이것은 산술 계산을 사용하거나 컨텍스트에서 숫자를 추출하는 데이터 세트에 대한 것이다. 우리는 문제가 산술 계산이나 문맥에서 숫자를 추출해야 하는 TAT-QA 데이터 세트에 사용한다.</p>
</div>
<div id="A1.SS2.p7" class="ltx_para">
<p class="ltx_p" id="A1.SS2.p7.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p7.1.1">Answer the following question with a short span, or a full and complete answer.</span>” 이것은 짧은 답과 긴 답 모두를 가진 데이터 세트에 대한 것이다. 모델은 질문을 기반으로 단답형 또는 장답형 답변을 생성할지 여부를 알려야 한다. 우리는 해당 답변의 길이가 짧고 길 수 있기 때문에 산술 계산이 필요하지 않은 TAT-QA 데이터 세트에 사용한다.</p>
</div>
<div id="A1.SS2.p8" class="ltx_para">
<p class="ltx_p" id="A1.SS2.p8.2">스테이지-2 명령어 튜닝에 사용되는 스테이지-1 SFT 데이터의 경우, 스테이지-1에서와 동일한 포맷 템플릿을 유지한다. 즉, <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p8.2.1">{Context for Latest Question}</span> 및 <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p8.2.2">{Instruction}</span>은 비어 있다.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Prompts for Synthetic Data Generation</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p class="ltx_p" id="A1.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p1.1.1">고품질 멀티턴 QA 대화를 얻기 위해 GPT-3.5-turbo에서 2단계 프롬프트를 사용합니다. </span> 단계-1에서 GPT-3.5-turbo가 주어진 문서를 기반으로 다중 턴 QA 대화를 생성하도록 프롬프트한다. 그러나, 생성된 대화는 대화사에서 단어 또는 구를 다시 참조할 대명사나 보통 명사가 부족하다는 것을 발견했다. 따라서 2단계에서는 대명사나 보통명사를 추가하여 생성된 대화를 수정하도록 GPT-3.5-turbo를 프롬프트한다. 두 단계 모두에 대해 고품질 생성을 보장하기 위해 GPT-3.5-터보 소수의 샷 예를 제공한다.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p class="ltx_p" id="A1.SS3.p2.1">상기 단계-1에서의 인-컨텍스트 소-샷 프롬프트는 다음과 같다(멀티-턴 QA 대화를 생성하기 위한aims):</p>
<pre id="A1.SS3.p2.2" class="ltx_verbatim ltx_font_typewriter">You are a helpful, pattern-following assistant.

role-user:
PersonA has access to information in the following passage:
###There are certain important aspects of college preparation. The student preparing
for college should define their goals and interests. The student should also plan
financially and academically. We have a checklist that helps students get ready
financially and academically.###
Generate a conversation of around four turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
PersonU: I need to know how to prepare for college.
PersonA: You should first define your goals and interest and be sure to know the
costs and how to plan financially and academically for college.
PersonU: Is there something I can use as a guide to get ready for it?
PersonA: you can use our checklist to get ready financially and academically.
PersonU: Any other tip?
PersonA: Be sure to check out the reasons to attend a College or Career schools.
PersonU: Ok, that sounds good. Can you give me more tips?
PersonA: Another thing to take into consideration is to explore your career options.

role-user:
PersonA has access to information in the following passage:
###There are several disability compensation benefits. One of the benefits is the
annual clothing allowance. This allowance covers the cost of purchasing new clothes
up to a limit of 200 dollars annually.###
Generate a conversation of around two turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
PersonU: I heard about a disability benefit for clothing. What is it about?
PersonA: This is a disability compensation benefit known as an annual clothing
allowance.
PersonU: I am quite interested about such benefits.
PersonA: Would you like to find out if you are eligible?

role-user:
PersonA has access to information in the following passage:
###Vascepa (icosapent ethyl) is a prescription fish oil medication approved for
treating adults with extremely high triglycerides, defined as a triglyceride level
of 500 mg/dL or higher. While using this medicine, you may need frequent blood tests
at your doctor’s office. Vascepa is only part of a treatment program that may also
include diet, exercise, and weight control. Follow your doctor’s instructions very
closely. You should not use Vascepa if you are allergic to icosapent. To make sure
Vascepa is safe for you, tell your doctor if you have: 1 liver disease; 2 diabetes;
3 a thyroid disorder; 4 problems with your pancreas; 5 a bleeding or blood-clotting
disorder; 6 if you are allergic to fish or shellfish; or. 7 if you drink large
amounts of alcohol, Vascepa (icosapent) works in the liver and bloodstream to reduce
very low-density triglycerides.###
Generate a conversation of around three turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
PersonU: I heard about a medication called Vascepa. What is it used for?
PersonA: Vascepa is a prescription fish oil medication used for treating adults with
extremely high triglyceride levels.
PersonU: Are there any side effects or precautions I should be aware of?
PersonA: While using Vascepa, you may need frequent blood tests, and it’s important
to follow your doctor’s instructions closely. It’s important to talk to your doctor
about any allergies or existing conditions you have.
PersonU: Is it effective in lowering cholesterol levels?
PersonA: Vascepa is used together with a low-fat diet and other treatments to lower
total cholesterol in people with severe high cholesterol.

role-user:
PersonA has access to information in the following passage:

{Document}

Generate a conversation of around three turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
</pre>
</div>
<div id="A1.SS3.p3" class="ltx_para">
<p class="ltx_p" id="A1.SS3.p3.1">상기 단계-2에서 인컨텍스트 소샷 프롬프트는 다음과 같다(대명사와 보통명사를 추가하여 멀티턴 QA 대화를 수정하고자 함):</p>
</div>
<div id="A1.SS3.p4" class="ltx_para">
<pre id="A1.SS3.p4.1" class="ltx_verbatim ltx_font_typewriter">You are a helpful, pattern-following assistant.

role-user:
Given the following conversation between PersonU and PersonA:
###PersonU: I’ve heard about adobo sauce in Mexican cuisine. Can you tell me more
about it?
PersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and
vinegar. It’s used as a marinade and cooking sauce in Mexican and Southwestern
cuisines.
PersonU: What are the main ingredients in adobo sauce?
PersonA: The main ingredients are typically chile peppers, garlic, and vinegar.
However, there can be variations in the recipe.
PersonU: How can I make adobo sauce at home?
PersonA: One way to make adobo sauce at home is by using smoked jalapeno peppers
called chipotles. You can stew the chipotles in a sauce with tomatoes, garlic,
vinegar, salt, and spices. It’s also possible to find canned adobo sauce in the
Hispanic foods section of many grocery stores.
PersonU: Are there any other uses for adobo sauce?
PersonA: Adobo sauce can be used as a condiment or as a marinade for various dishes.
It enhances the flavor of meats and can be used as a cooking and serving sauce.
Additionally, adobo is also used in a stew-like presentation of meat prepared in
this sauce.
PersonU: Is adobo sauce easy to preserve?
PersonA: Yes, you can preserve adobo sauce by freezing or canning it when you’re
done making it. This allows you to store it for up to a year and enjoy its flavors
whenever you want.###
Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.

role-assistant:
PersonU: I’ve heard about adobo sauce in Mexican cuisine. Can you provide more
information about it?
PersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and vinegar.
It’s used as a marinade and cooking sauce in Mexican and Southwestern cuisines.
PersonU: What are the main ingredients in the sauce?
PersonA: The main ingredients are typically chile peppers, garlic, and vinegar.
However, there can be variations in the recipe.
PersonU: How can I make the sauce at home?
PersonA: One way to make it at home is by using smoked jalapeno peppers called
chipotles. You can stew them in a sauce with tomatoes, garlic, vinegar, salt, and
spices. It’s also possible to find canned adobo sauce in the Hispanic foods section
of many grocery stores.
PersonU: Are there any other uses for it?
PersonA: It can be used as a condiment or as a marinade for various dishes. It
enhances the flavor of meats and can be used as a cooking and serving sauce.
Additionally, it is also used in a stew-like presentation of meat prepared in
this sauce.
PersonU: Is it easy to preserve?
PersonA: Yes, you can preserve it by freezing or canning it when you’re done
making it. This allows you to store it for up to a year and enjoy its flavors
whenever you want.

role-user:
Given the following conversation between PersonU and PersonA:
###PersonU: I’ve heard about a test called urinalysis. What is it used for?
PersonA: Urinalysis is a test that evaluates a sample of your urine. It is used to
detect and assess a wide range of disorders, such as urinary tract infection,
kidney disease, and diabetes. It examines the appearance, concentration, and content
of urine.
PersonU: Can it help diagnose bladder cancer?
PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for
bladder cancer. One way to test for bladder cancer is to check for blood in the
urine, which is called hematuria. This can be done during a urinalysis by examining
the urine cells under a special microscope.
PersonU: What other conditions or diseases can be detected through urinalysis?
PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,
evaluate causes of kidney failure, screen for the progression of chronic conditions
like diabetes mellitus and high blood pressure, and it can be used in combination
with other tests to diagnose certain diseases. It is a versatile test that helps
detect abnormalities in the urinary tract.
PersonU: Where can I get a urinalysis done?
PersonA: Urinalysis can be performed in various healthcare settings, including
doctors’ offices, urgent care facilities, laboratories, and hospitals. You can
provide a urine sample in a specimen cup, and the sample can be analyzed on-site
or sent to a laboratory for testing.
PersonU: How much urine is typically needed for a urinalysis?
PersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are required
for urinalysis testing. It’s a non-invasive and relatively simple test that provides
valuable information about your health.###
Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.

role-assistant:
PersonU: I’ve heard about a test called urinalysis. What is it used for?
PersonA: Urinalysis is a test that evaluates a sample of your urine. It’s used to
detect and assess a wide range of disorders, such as urinary tract infection,
kidney disease, and diabetes. It examines the appearance, concentration, and
content of urine.
PersonU: Can it help diagnose bladder cancer?
PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for bladder
cancer. One way to test for bladder cancer is to check for blood in the urine, which
is called hematuria. This can be done during a urinalysis by examining the urine
cells under a special microscope.
PersonU: What other conditions or diseases can be detected through this test?
PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,
evaluate causes of kidney failure, screen for the progression of chronic conditions
like diabetes mellitus and high blood pressure, and it can be used in combination
with other tests to diagnose certain diseases. It is a versatile test that helps
detect abnormalities in the urinary tract.
PersonU: Where can I go to get this test done?
PersonA: Urinalysis can be performed in various healthcare settings, including
doctors’ offices, urgent care facilities, laboratories, and hospitals. You can
provide a urine sample in a specimen cup, and the sample can be analyzed on-site
or sent to a laboratory for testing.
PersonU: How much urine is typically needed for the test?
PersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are
required for urinalysis testing. It’s a non-invasive and relatively simple test
that provides valuable information about your health.

role-user:
Given the following conversation between PersonU and PersonA:

{multi-turn QA conversation}

Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.

role-assistant:


</pre>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>More Details and Results for Retrieval in Conversational QA</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Query Rewriting Prompts for GPT-3.5-turbo</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A2.SS1.p1.1">GPT-3.5-터보에 사용하는 인컨텍스트 소샷 쿼리 다시 쓰기 프롬프트는 다음과 같습니다.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<pre id="A2.SS1.p2.1" class="ltx_verbatim ltx_font_typewriter">You are a helpful, pattern-following assistant.

role-user:
Given the following conversation between PersonU and PersonA:
PersonU: Hello, I would like to know what to do if I do not agree with any decision.
PersonA: disagree with our decision about your monthly income adjustment amounts?
PersonU: no. Where can I find my SHIP contact information?
PersonA: You can find your local SHIP contact information in the back of your
Medicare &amp; You 2020 Handbook online.
PersonU: and how do they calculate the adjustments?
Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.

role-assistant:
How is the calculation for adjustments made by SHIP determined?

role-user:
Given the following conversation between PersonU and PersonA:
PersonU: I need to know how to prepare for college.
PersonA: You should first define your goals and interest and be sure to know the
costs and how to plan financially and academically for college.
PersonU: Is there something I can use as a guide to get ready for it?
Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.

role-assistant:
What resources or guides can I use to help me prepare for college?

role-user:
Given the following conversation between PersonU and PersonA:

{Dialogue History + Latest Question}

Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.

role-assistant:

</pre>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>More Results for Retrieval in Conversational QA</h3>

<figure id="A2.T9" class="ltx_table">
<div id="A2.T9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:682.9pt;height:127pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="A2.T9.1.1" class="ltx_p"><span id="A2.T9.1.1.1" class="ltx_text"> <span id="A2.T9.1.1.1.1" class="ltx_tabular ltx_align_middle"> <span class="ltx_tbody"> <span id="A2.T9.1.1.1.1.1.1" class="ltx_tr"> <span id="A2.T9.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="A2.T9.1.1.1.1.1.1.1.1" class="ltx_text">Models</span></span> <span id="A2.T9.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">Average</span> <span id="A2.T9.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">Doc2Dial</span> <span id="A2.T9.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">QuAC</span> <span id="A2.T9.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">QReCC</span> <span id="A2.T9.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">TopiOCQA</span> <span id="A2.T9.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" style="padding-top:0.15pt;padding-bottom:0.15pt;">INSCIT</span></span> <span id="A2.T9.1.1.1.1.2.2" class="ltx_tr"> <span id="A2.T9.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-1</span> <span id="A2.T9.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5</span> <span id="A2.T9.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-1</span> <span id="A2.T9.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5</span> <span id="A2.T9.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-1</span> <span id="A2.T9.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5</span> <span id="A2.T9.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-1</span> <span id="A2.T9.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5</span> <span id="A2.T9.1.1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5*</span> <span id="A2.T9.1.1.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-20*</span> <span id="A2.T9.1.1.1.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-5*</span> <span id="A2.T9.1.1.1.1.2.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">top-20*</span></span> <span id="A2.T9.1.1.1.1.3.3" class="ltx_tr"> <span id="A2.T9.1.1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">Dragon (w/ dialog history)</span> <span id="A2.T9.1.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.29</span> <span id="A2.T9.1.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.09</span> <span id="A2.T9.1.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">43.33</span> <span id="A2.T9.1.1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">75.61</span> <span id="A2.T9.1.1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">56.8</span> <span id="A2.T9.1.1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">82.86</span> <span id="A2.T9.1.1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.17</span> <span id="A2.T9.1.1.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">81.96</span> <span id="A2.T9.1.1.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">57.68</span> <span id="A2.T9.1.1.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">78.80</span> <span id="A2.T9.1.1.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">27.49</span> <span id="A2.T9.1.1.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.22</span></span> <span id="A2.T9.1.1.1.1.4.4" class="ltx_tr"> <span id="A2.T9.1.1.1.1.4.4.1" class="ltx_td ltx_align_left" style="padding-top:0.15pt;padding-bottom:0.15pt;">Dragon + Rewrite (w/ dialog history)</span> <span id="A2.T9.1.1.1.1.4.4.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.57</span> <span id="A2.T9.1.1.1.1.4.4.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">74.12</span> <span id="A2.T9.1.1.1.1.4.4.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">44.54</span> <span id="A2.T9.1.1.1.1.4.4.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">76.98</span> <span id="A2.T9.1.1.1.1.4.4.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">57.23</span> <span id="A2.T9.1.1.1.1.4.4.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">83.04</span> <span id="A2.T9.1.1.1.1.4.4.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.45</span> <span id="A2.T9.1.1.1.1.4.4.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">82.60</span> <span id="A2.T9.1.1.1.1.4.4.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">60.94</span> <span id="A2.T9.1.1.1.1.4.4.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">81.74</span> <span id="A2.T9.1.1.1.1.4.4.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">28.69</span> <span id="A2.T9.1.1.1.1.4.4.13" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">46.22</span></span> <span id="A2.T9.1.1.1.1.5.5" class="ltx_tr"> <span id="A2.T9.1.1.1.1.5.5.1" class="ltx_td ltx_align_left" style="padding-top:0.15pt;padding-bottom:0.15pt;">Dragon + Rewrite (w/ single query only)</span> <span id="A2.T9.1.1.1.1.5.5.2" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="A2.T9.1.1.1.1.5.5.2.1" class="ltx_text ltx_font_bold">54.46</span></span> <span id="A2.T9.1.1.1.1.5.5.3" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="A2.T9.1.1.1.1.5.5.3.1" class="ltx_text ltx_font_bold">80.13</span></span> <span id="A2.T9.1.1.1.1.5.5.4" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.60</span> <span id="A2.T9.1.1.1.1.5.5.5" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.60</span> <span id="A2.T9.1.1.1.1.5.5.6" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">47.10</span> <span id="A2.T9.1.1.1.1.5.5.7" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">77.15</span> <span id="A2.T9.1.1.1.1.5.5.8" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.73</span> <span id="A2.T9.1.1.1.1.5.5.9" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">85.78</span> <span id="A2.T9.1.1.1.1.5.5.10" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">73.07</span> <span id="A2.T9.1.1.1.1.5.5.11" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">88.19</span> <span id="A2.T9.1.1.1.1.5.5.12" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.79</span> <span id="A2.T9.1.1.1.1.5.5.13" class="ltx_td ltx_align_center" style="padding-top:0.15pt;padding-bottom:0.15pt;">68.92</span></span> <span id="A2.T9.1.1.1.1.6.6" class="ltx_tr"> <span id="A2.T9.1.1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">Dragon + Fine-tune (w/ dialog history)</span> <span id="A2.T9.1.1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.72</span> <span id="A2.T9.1.1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">80.67</span> <span id="A2.T9.1.1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">48.94</span> <span id="A2.T9.1.1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">83.01</span> <span id="A2.T9.1.1.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">52.64</span> <span id="A2.T9.1.1.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">81.95</span> <span id="A2.T9.1.1.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">50.73</span> <span id="A2.T9.1.1.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">87.17</span> <span id="A2.T9.1.1.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">67.86</span> <span id="A2.T9.1.1.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">86.28</span> <span id="A2.T9.1.1.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">43.43</span> <span id="A2.T9.1.1.1.1.6.6.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.15pt;padding-bottom:0.15pt;">64.94</span></span> <span id="A2.T9.1.1.1.1.7.7" class="ltx_tr"> <span id="A2.T9.1.1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">Dragon + Fine-tune + Rewrite (w/ dialog hisotry)</span> <span id="A2.T9.1.1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="A2.T9.1.1.1.1.7.7.2.1" class="ltx_text ltx_font_bold">53.17</span></span> <span id="A2.T9.1.1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span id="A2.T9.1.1.1.1.7.7.3.1" class="ltx_text ltx_font_bold">80.84</span></span> <span id="A2.T9.1.1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.30</span> <span id="A2.T9.1.1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">84.64</span> <span id="A2.T9.1.1.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">55.04</span> <span id="A2.T9.1.1.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">83.23</span> <span id="A2.T9.1.1.1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">51.23</span> <span id="A2.T9.1.1.1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">87.99</span> <span id="A2.T9.1.1.1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">60.50</span> <span id="A2.T9.1.1.1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">81.03</span> <span id="A2.T9.1.1.1.1.7.7.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">49.80</span> <span id="A2.T9.1.1.1.1.7.7.13" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.15pt;padding-bottom:0.15pt;">67.33</span></span> </span> </span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 9:</span>5개의 데이터셋에 걸친 종합적인 멀티턴 검색 결과.</figcaption>
</figure>
<div id="A2.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A2.SS2.p1.1"><a class="ltx_ref" href="#A2.T9" title="Table 9 ‣ B.2 More Results for Retrieval in Conversational QA ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">9</span></a>에서는 미세 조정 방법과 다시 쓰기 방법을 종합적으로 비교한다.</p>
</div>
<div id="A2.SS2.p2" class="ltx_para">
<p class="ltx_p" id="A2.SS2.p2.1">흥미롭게도, 재작성된 질의만을 입력으로 사용하는 것과 비교하여(Dragon + Rewrite (w/single query only)), 추가 대화 이력(Dragon + Rewrite (w/ dialog history))을 부여하면 평균 점수가 크게 떨어짐을 알 수 있다. 드래곤은 원래 싱글턴 질의에 대해 사전 훈련되기 때문에 멀티턴 대화 대신 싱글턴 재작성 질의가 제공될 때 자연스럽게 더 나은 일반화 능력을 가질 것이기 때문이다. 상기 재작성된 쿼리는 상기 다이얼로그 이력으로부터 이미 충분한 정보를 포함하는 것을 특징으로 하는 방법.</p>
</div>
<div id="A2.SS2.p3" class="ltx_para">
<p class="ltx_p" id="A2.SS2.p3.1">또한 "Dragon + Fine-tune"이 "Dragon + Fine-tune + Rewrite"와 동등하게 수행됨을 관찰한다. 즉, 다중 회전 미세 조정 방법의 경우 원래 쿼리를 입력으로 재작성된 쿼리로 대체하면 비슷한 결과를 얻을 수 있다. 이는 대화 이력이 이미 제공되었기 때문에 재작성된 쿼리가 모델에 대한 많은 추가 정보를 제공하지 않을 것이며, 전체 대화 입력을 자연스럽지 않게 만들기 때문에 부정적인 영향(예: TopiOCQA 데이터 세트에 대한 결과)을 유발할 수도 있기 때문이다. 이는 다시 한번 미세 조정 방법이 멀티 턴 컨텍스트를 이해할 수 있는 능력을 모델에 갖추는 데 얼마나 효과적인지를 보여준다.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Conversational QA Benchmarks</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Data Statistics</h3>

<section id="A3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Doc2Dial</h5>

<div id="A3.SS1.SSS0.Px1.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px1.p1.1">평가를 위해 Doc2Dial의 테스트 세트를 사용한다. 3939 사용자-에이전트 회전이 있는 719개의 대화 상자로 구성됩니다.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">QuAC</h5>

<div id="A3.SS1.SSS0.Px2.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px2.p1.1">QAC의 검증 집합은 직접 구할 수 없기 때문에 평가에 사용한다. 유효성 검사 세트는 7354개의 사용자 에이전트 회전이 있는 1000개의 대화 상자로 구성됩니다. 이 7354개의 사용자-에이전트 회전 중 1486개(약 20.2%)의 대답할 수 없는 질문이 있다. 답변 가능한 질문과 답변 불가능한 질문의 결합된 평가를 위해, 우리는 답변 불가능한 질문에 대한 그라운드 트루스 응답을 “<span class="ltx_text ltx_font_typewriter" id="A3.SS1.SSS0.Px2.p1.1.1">Sorry. I cannot find the answer based on the context.</span> 2단계 튜닝에서 설정한 것과 동일합니다. 공정한 비교를 위해 기준 모델(즉, Llama2-SFT/Chat, GPT-3.5-turbo, GPT-4)에서 응답할 수 없는 응답을 동일한 문장으로 대체한다(자세한 내용은 부록 <a class="ltx_ref" href="#A5" title="Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">E</span></a>에서 찾을 수 있다).</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">QReCC</h5>

<div id="A3.SS1.SSS0.Px3.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px3.p1.1">평가에는 QReCC의 테스트 세트를 사용한다. 테스트 세트에는 QuAC 데이터 세트의 일부 대화 샘플 소스가 포함되어 있습니다. 다른 벤치마크 데이터 세트와의 중복을 방지하기 위해 QuAC 소스 샘플을 제거하여 2805개의 사용자 에이전트 회전을 생성했다.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">TopiOCQA</h5>

<div id="A3.SS1.SSS0.Px4.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px4.p1.1">우리는 TopiOCQA의 테스트 세트를 아직 사용할 수 없기 때문에 검증 세트를 사용한다. 검증 세트는 2514개의 사용자-에이전트 회전이 있는 205개의 대화 상자로 구성된다. 각 질문에는 총 4개의 인간이 작성한 답변이 있습니다. 답변할 수 없는 경우가 있습니다. 그러나 이 데이터셋은 답변이 없는 46개의 질문만 발견하여 답변이 없는 사례 평가에 충분하지 않기 때문에 답변이 없는 사례 평가에 포함하지 않는다. 이 데이터 세트는 전체 위키피디아를 검색 코퍼스로 사용한다는 점을 감안할 때, 일반적으로 특정 주제 또는 도메인으로 좁혀지는 문서보다 대화 QA에 더 중점을 두었기 때문에 다중 전환 검색을 위해 주제 정보를 활용한다.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">INSCIT</h5>

<div id="A3.SS1.SSS0.Px5.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px5.p1.1">우리는 TopiOCQA의 테스트 세트를 아직 사용할 수 없기 때문에 검증 세트를 사용한다. 그것의 검증 세트는 502개의 사용자-에이전트 턴을 갖는 86개의 대화로 구성된다. 각 질문에는 평균 1.9개의 인간이 작성한 답변이 있습니다. 몇 가지 질문에 대해 대답할 수 없는 응답이 있습니다. 그러나 모든 질문에는 항상 하나 이상의 응답 가능한 응답이 있으므로 이 데이터 세트는 응답할 수 없는 경우 평가에 포함하지 않습니다. INSCIT는 또한 전체 위키피디아를 검색 코퍼스로 사용한다. 따라서 TopiOCQA에서 언급한 것과 동일한 이유로 멀티턴 검색을 위해 토픽 정보를 활용한다.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CoQA</h5>

<div id="A3.SS1.SSS0.Px6.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px6.p1.1">우리는 CoQA의 테스트 세트를 직접 얻을 수 없기 때문에 CoQA의 검증 세트를 사용한다. 검증 세트는 7983개의 사용자-에이전트 회전과 함께 500개의 대화로 구성된다. 각 질문에는 총 4개의 인간이 작성한 답변이 있습니다. 답변할 수 없는 경우가 있습니다. 그러나 TopiOCQA 데이터 세트에서와 동일한 이유로 이 데이터 세트를 응답할 수 없는 사례 평가에 포함하지 않는다. 우리는 답을 찾을 수 없는 13개의 질문만 찾는데, 이는 답할 수 없는 사례 평가에 충분하지 않다.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px7" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">DoQA</h5>

<div id="A3.SS1.SSS0.Px7.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px7.p1.1">평가에는 DoQA의 테스트 세트를 사용한다. 테스트 세트는 요리, 여행 및 영화 도메인에서 5394개의 사용자 에이전트가 회전하는 1200개의 대화로 구성됩니다. 5394개의 사용자-에이전트 회전 중 1479개(약 27.4%)의 응답할 수 없는 질문이 있다. QuAC 데이터 세트에서 언급한 것과 동일한 전략을 사용하여 응답할 수 없는 샘플을 평가에 통합한다.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px8" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">ConvFinQA</h5>

<div id="A3.SS1.SSS0.Px8.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px8.p1.1">테스트 세트를 직접 얻을 수 없기 때문에 ConvFinQA의 검증 세트를 사용한다. 검증 세트는 1490개의 사용자-에이전트 회전과 함께 421개의 대화로 구성된다.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px9" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SQA</h5>

<div id="A3.SS1.SSS0.Px9.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px9.p1.1">평가에는 SQA의 테스트 세트를 사용합니다. 테스트 세트는 3100개의 사용자 에이전트 회전과 함께 1025개의 대화로 구성됩니다.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px10" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">HybridDial</h5>

<div id="A3.SS1.SSS0.Px10.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.SSS0.Px10.p1.1">평가에는 HybridDial의 테스트 세트를 사용합니다. 테스트 세트는 1111개의 사용자 에이전트 회전과 함께 243개의 대화로 구성됩니다.</p>
</div>
</section>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Details of Incorporating Table as Context</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS2.p1.1">우리는 마크다운 형식을 사용하여 문서의 테이블을 처리합니다(즉, "<math alttext="\mid" class="ltx_Math" display="inline" id="A3.SS2.p1.1.m1.1"><semantics id="A3.SS2.p1.1.m1.1a"><mo id="A3.SS2.p1.1.m1.1.1" xref="A3.SS2.p1.1.m1.1.1.cmml">∣</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p1.1.m1.1b"><ci id="A3.SS2.p1.1.m1.1.1.cmml" xref="A3.SS2.p1.1.m1.1.1">∣</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p1.1.m1.1c">\mid</annotation></semantics></math>"를 사용하여 테이블의 각 셀을 분리합니다). 그러나 테이블의 행 수가 증가함에 따라 모델이 마지막 몇 개의 행에 있는 셀의 값을 테이블의 해당 헤더에 연결하는 것이 더 어렵습니다. 이 문제를 해결하기 위해 모델이 셀과 각 헤더 간의 연결을 더 쉽게 캡처할 수 있도록 각 행의 해당 셀 값에 헤더를 추가합니다. 예를 들어, 원래 마크다운 테이블</p>
</div>
<div id="A3.SS2.p2" class="ltx_para">
<pre id="A3.SS2.p2.1" class="ltx_verbatim ltx_font_typewriter">| Boat | Crew | Nation | Date | Meet | Location |
| M1x | Mahe | FR | 2009 | | Poznan |
| M2- | Hamish | US | 2012 | Olympics | Lucerne |
| M2+ | Igor | DE | 1994 | | Indianapolis |
</pre>
</div>
<div id="A3.SS2.p3" class="ltx_para">
<p class="ltx_p" id="A3.SS2.p3.1">…로 전환될 것이다.</p>
<pre id="A3.SS2.p3.2" class="ltx_verbatim ltx_font_typewriter">| M1x (Boat) | Crew: Mahe | Nation: FR | Date: 2009 | Meet: | Location: Poznan |
| M2- (Boat) | Crew: Hamish | Nation: US | Date: 2012 | Meet: Olympics | Location: Paris |
| M2+ (Boat) | Crew: Igor | Nation: DE | Date: 1994 | Meet: | Location: Indianapolis |
</pre>
</div>
<div id="A3.SS2.p4" class="ltx_para">
<p class="ltx_p" id="A3.SS2.p4.1">이 테이블 처리 전략을 모든 테이블 데이터 세트에 적용합니다.</p>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Prompts for the Benchmarks</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS3.p1.1">ChatQA, Llama2-Chat, GPT-3.5-turbo 및 GPT-4의 벤치마크에 대해 §<a class="ltx_ref" href="#A1.SS2" title="A.2 Stage-2: Context-Enhanced Instruction Tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">A.2</span></a>에서 언급한 프롬프트 형식을 사용합니다. 모든 모델에 대해 동일한 <span class="ltx_text ltx_font_typewriter" id="A3.SS3.p1.1.1">{Context for Latest Question}</span>을 유지하는 반면, 기준선(즉, Llama2-Chat, GPT-3.5-turbo 및 GPT-4)에 대해 <span class="ltx_text ltx_font_typewriter" id="A3.SS3.p1.1.2">{Instruction}</span>을 조정하여 최적의 결과를 얻지 않도록 합니다. 우리는 다른 답변 유형(예: 긴 답변, 짧은 답변, 산술 계산)을 가진 테스트 벤치마크에 대해 다른 지침을 사용한다. 벤치마크 상의 모든 모델에 대한 <span class="ltx_text ltx_font_typewriter" id="A3.SS3.p1.1.3">{Instruction}</span>은 다음과 같습니다.</p>
</div>
<section id="A3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.3.1 </span>ChatQA</h4>

<div id="A3.SS3.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS1.p1.1"><span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS1.p1.1.1">{Instruction}</span> 일관성을 유지 합니다.</p>
</div>
<div id="A3.SS3.SSS1.p2" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS1.p2.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS1.p2.1.1">Please give a full and complete answer for the question.</span> DoQA, INSCIT, HybridDial, Doc2Dial, QuAC 및 QReCC의 경우 이러한 데이터 세트는 일반적으로 질문에 대한 답변이 길기 때문이다.</p>
</div>
<div id="A3.SS3.SSS1.p3" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS1.p3.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS1.p3.1.1">Answer the following question with a short span, or a full and complete answer.</span> SQA 및 TopiOCQA의 경우 이러한 데이터 세트는 질문에 따라 짧은 답변과 긴 답변을 모두 가지고 있기 때문이다.</p>
</div>
<div id="A3.SS3.SSS1.p4" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS1.p4.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS1.p4.1.1">Answer the answer to be just in a few words.</span> CoQA의 경우 일반적으로 질문에 대한 답변이 짧기 때문입니다.</p>
</div>
<div id="A3.SS3.SSS1.p5" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS1.p5.1">"<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS1.p5.1.1">문맥의 숫자 또는 +, -, * 또는 /.</span>을 사용하여 수학 산술로 다음 질문에 답하세요." ConvFinQA의 경우 이 데이터 세트는 모델이 컨텍스트에서 숫자를 추출하거나 산술 계산을 수행해야 하기 때문이다. 우리는 모델이 생성하는 산술 공식을 바탕으로 수를 계산하고 금답과 비교할 것이다.</p>
</div>
</section>
<section id="A3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.3.2 </span>Llama2-Chat</h4>

<div id="A3.SS3.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS2.p1.1">우리는 §<a class="ltx_ref" href="#A1.SS2" title="A.2 Stage-2: Context-Enhanced Instruction Tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">A.2</span></a>에서 사용한 것과 비교하여 약간 더 나쁜 결과를 제공하는 원래 Llama2-Chat 프롬프트 템플릿<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gpus.llm-utils.org/llama-2-prompt-template/" target="_blank" title="">https://gpus.llm-utils.org/llama-2-prompt-template/</a></span></span></span>을 시도했다. 우리는 여러 <span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS2.p1.1.1">{Instruction}</span> for Llama2-Chat을 시도했다. 우리는 아래의 것이 가장 잘 작동한다는 것을 발견합니다.</p>
</div>
<div id="A3.SS3.SSS2.p2" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS2.p2.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS2.p2.1.1">Please give a answer in just one sentence.</span> DoQA, INSCIT, HybridDial, Doc2Dial, QuAC 및 QReCC의 경우 이러한 데이터 세트는 일반적으로 한 문장 내에서 긴 답변을 가지고 있기 때문이다. 우리는 모델이 매우 긴 답변을 생성하는 것을 방지하기 위해 "완전하고 완전한 답변" 대신 "하나의 문장"과 같은 Llama2-Chat 특정 지시를 내리는 것이 중요하다는 것을 알아챘다.</p>
</div>
<div id="A3.SS3.SSS2.p3" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS2.p3.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS2.p3.1.1">짧은 스팬 또는 하나의 문장으로 다음 질문에 답합니다.</span> TopiOCQA의 경우 이 데이터 세트는 질문에 따라 짧은 답변과 긴 답변을 모두 가지고 있으며 긴 답변은 일반적으로 한 문장 내에 있기 때문이다.</p>
</div>
<div id="A3.SS3.SSS2.p4" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS2.p4.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS2.p4.1.1">Answer the following questions with one or a list of entities.</span> SQA의 경우 이 데이터 세트에 대한 답은 항상 컨텍스트의 엔터티 목록 또는 하나로 구성되기 때문입니다.</p>
</div>
<div id="A3.SS3.SSS2.p5" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS2.p5.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS2.p5.1.1">Answer the answer to be just in a few words.</span> CoQA의 경우 일반적으로 질문에 대한 답변이 짧기 때문입니다.</p>
</div>
<div id="A3.SS3.SSS2.p6" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS2.p6.1">우리는 "<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS2.p6.1.1">문맥의 숫자 또는 +, -, * 또는 /.</span>을 사용하여 수학 산술로만 다음 질문에 답하세요." ConvFinQA의 경우 이 데이터 세트는 모델이 컨텍스트에서 숫자를 추출하거나 산술 계산을 수행해야 하기 때문이다. 우리는 모델에 의해 생성된 산술식을 추출하고 계산기를 사용하여 최종 결과를 얻는다.</p>
</div>
</section>
<section id="A3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.3.3 </span>GPT-3.5-turbo &amp; GPT-4</h4>

<div id="A3.SS3.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS3.p1.1">우리는 여러 <span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS3.p1.1.1">{Instruction}</span> for GPT-3.5-turbo and GPT-4, 우리는 아래 것들이 가장 잘 작동한다는 것을 발견했다(GPT-3.5-turbo and GPT-4 모두에 적용됨).</p>
</div>
<div id="A3.SS3.SSS3.p2" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS3.p2.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS3.p2.1.1">Please give a answer in just one sentence.</span> DoQA, INSCIT, HybridDial, Doc2Dial, QuAC 및 QReCC의 경우 이러한 데이터 세트는 일반적으로 한 문장 내에서 긴 답변을 가지고 있기 때문이다. Llama2-Chat과 유사하게, 우리는 또한 OpenAI 모델이 "완전하고 완전한 답변"의 지시를 고려할 때 상당히 긴 답변을 생성하는 경향이 있음을 발견한다. 따라서, 우리는 모델이 매우 긴 답변을 생성하는 것을 방지하기 위해 명령어를 더 구체적으로 만든다(즉, "한 문장").</p>
</div>
<div id="A3.SS3.SSS3.p3" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS3.p3.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS3.p3.1.1">Answer the following questions in Just a few words or one sentence.</span> TopiOCQA의 경우 이 데이터 세트는 질문에 따라 짧은 답변과 긴 답변을 모두 가지고 있으며 긴 답변은 일반적으로 한 문장 내에 있기 때문이다.</p>
</div>
<div id="A3.SS3.SSS3.p4" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS3.p4.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS3.p4.1.1">Answer the following questions with one or list of entities. Do not give the detailed explanation. Answer needs to be possible.</span> SQA요 우리는 OpenAI 모델이 특별히 지시하지 않는 한 SQA 데이터 세트에 대해 종종 자세한 설명을 제공한다는 것을 발견했다.</p>
</div>
<div id="A3.SS3.SSS3.p5" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS3.p5.1">우리는 “<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS3.p5.1.1">Answer the answer to be just in a few words.</span> CoQA의 경우 일반적으로 질문에 대한 답변이 짧기 때문입니다.</p>
</div>
<div id="A3.SS3.SSS3.p6" class="ltx_para">
<p class="ltx_p" id="A3.SS3.SSS3.p6.1">"<span class="ltx_text ltx_font_typewriter" id="A3.SS3.SSS3.p6.1.1">문맥의 숫자 또는 +, -, * 또는 /.</span>을 사용하여 수학 산술로만 다음 질문에 답하세요." ConvFinQA의 경우 이 데이터 세트는 모델이 컨텍스트에서 숫자를 추출하거나 산술 계산을 수행해야 하기 때문이다. 우리는 모델에 의해 생성된 산술식을 추출하고 계산기를 사용하여 최종 결과를 얻는다.</p>
</div>
</section>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Human Evaluation</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p" id="A4.p1.1">우리는 10개의 테스트 벤치마크 데이터 세트에 걸쳐 인간 평가를 수행한다. 각 데이터 세트에 대해 무작위로 60개의 샘플을 선택하고 각 샘플은 3개의 주석기로 레이블이 지정되어 총 1800개의 주석이 생성된다.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p class="ltx_p" id="A4.p2.1">우리는 주석이 모델의 출력에서 사실을 확인하고 어떤 모델이 질문에 더 정확한 응답을 제공하는지 결정하도록 요청한다. 우리는 아마존 기계 터키 플랫폼을 사용하여 인간 평가를 수행합니다. 인간 평가 지침의 세부 사항과 주석자에게 표시된 인터페이스는 그림 <a class="ltx_ref" href="#A4.F3" title="Figure 3 ‣ Appendix D Human Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">3</span></a>에 나와 있다.</p>
</div>
<figure id="A4.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.10225/assets/x3.png" id="A4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="303" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span>Human evaluation instructions and the interface for annotators.</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Unanswerable Case Evaluation</h2>

<div id="A5.p1" class="ltx_para">
<p class="ltx_p" id="A5.p1.1">답변할 수 없는 경우에, 우리는 질문이 정답으로 대답될 수 없다는 것을 나타내는 모델을 고려한다. 답변할 수 있는 경우에, 우리는 질문이 정답으로 대답할 수 없다는 것을 나타내지 않는 모델(즉, 답을 주는 모델)을 고려한다. ChatQA가 항상 “<span class="ltx_text ltx_font_typewriter" id="A5.p1.1.1">Sorry. I cannot find answer based on the context</span> 이 문장을 2단계 튜닝에서 답할 수 없는 질문에 대한 응답으로 사용하기 때문에 답할 수 없는 질문을 찾을 때 답할 수 없다. 라마2-챗, GPT-3.5-터보 및 GPT-4의 경우 응답할 수 없는 질문에 대한 출력은 보통 몇 가지 특정 패턴을 따른다. 아래에서는 DoQA 및 QuAC 데이터 세트에서 생성된 모든 샘플에서 파생된 휴리스틱 일치 패턴을 나열하여 모델이 질문이 답할 수 없다고 제안하는지 확인한다. 생성된 출력에 아래에 나열된 패턴이 포함되어 있음을 찾으면 질문이 응답할 수 없다는 표시로 해석하고 해당 답변을 “<span class="ltx_text ltx_font_typewriter" id="A5.p1.1.2">Sorry. I cannot find the answer based on the context.</span> (p<0.05).</p>
</div>
<div id="A5.p2" class="ltx_para">
<pre id="A5.p2.1" class="ltx_verbatim ltx_font_typewriter">i’m not sure, cannot find, does not provide, cannot provide, cannot answer,
cannot be found, cannot be determined, don’t have information, do not have
information, couldn’t find, no information in the context, does not mention,
not explicitly mentioned, i don’t have any, i do not have any, does not
specify, doesn’t provide, not able to, unable to, doesn’t specify, there is
no information, there is no mention, not mentioned, i don’t have enough
information, there is no specific information, there is no specific mention,
no information found, I don’t have that information
</pre>
</div>
<figure id="A5.tab1" class="ltx_table">
<div id="A5.tab1.1" class="ltx_inline-block ltx_transformed_outer" style="width:635.7pt;height:964pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A5.tab1.1.1"><span class="ltx_text" id="A5.tab1.1.1.1.1.1.1.1"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.tab1.tab1.1.1.1.1.1.1.1.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_thltx_column ltx_border_tt" id="A5.tab1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tbold" id="A5.tab1.1.1.1.1.1.1.1.1.1.1">Conversation</span></span> <span class="ltx_td ltx_align_left l <span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.2.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: Loris Capirossi, Valentino Rossi,</span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.1.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Jeremy McWilliams, Tohru Ukawa, …,</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.1.2.1.1.1.1" style="padding-top:0.15pt;padding-bottom </span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.6"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.tab1.1.1.1.1.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: of those, what times were recorded? <span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.2.1.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: 38:04.730, +0.180, +0.534,</span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.8"><span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.tab1.1.1.1.1.1.1.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">+0.537, …, +1:40.635, +1 Lap, Retirement. <span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.2.1.2.1.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: of those times, which were not</span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.2.1.2.1.10"><span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.tab1.1.1.1.1.1.1.1.1.1.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">actual times? <span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.2.1.2.1.1.11" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: +1 Lap, Retirement</span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.2.1.2.12"><span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.tab1.1.1.1.1.1.1.1.2.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: what of a actual finish? (../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../ <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.tab1.1.1.1.1.1.3.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.3.2.2.1" style="padding-top:0.15pt;"><span class="ltx_tan> <span class="ltx_nopad_r ltx_align_left" id="A5.tab1.tab1.1.1.1.1.1.3.2.2.1.1" style="padding-bottom:0.15pt;">[span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.tab1 <span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.3.2.2.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: 17400 + 20300</span></span> <span class="ltx_nopad_r ltx_align_left" id="A5.tab1.1.1.1.1.1.1.3.2.2.1.1" style="padding-top:0.15pt;">Assistant: 21400</span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.3.2.2.1.1" style="padding-bottom:0.15pt;">Assistant: 21400</span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1 <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.tab1.1.1.1.1.1.1.3.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.1.1.1.1.1.2.3.1" style="padding-top:0.15pt;"><span class="ltx_tan></span></span> <span class="ltx_td ltx_align_left" id="A5.tab1.tab1.1.1.1.1.1.1.1.3.2.1.1" style="padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_left" id > <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.1.1.4.3.1.1.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;">Ukraine</span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.1.1.4.3.1.1.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;">9.525</span> <span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.1.1.1.3.1.1.1.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;">7</span> <span class=" ></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.4.3.2.1.3" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: The XVI World Rhythmic</span></span> <span class="ltx_nopad_r ltx_align_left" id="A5.tab1.1.1.1.1.1.1.1.1.1.3.2.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">Gymnastics Championships were held in</span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;">Span class="ltx_td ltx_n (../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../ <span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.4.3.3.1.1" style="color:#41B729;">8</span> <span class="ltx_text" id="A5.tab1.1.1.1.1.1.1.4.3.3.1.2" style="color:#FF8000;">gymnasts*</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.4.3.3.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">participated in championship. ></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.1.4.3.3.1.1" style="color:#41B729;">8</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.1.1.4.3.3.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.1.1.1.1.3.1.6.1" style="color:#FF8000;">gymnasts*</span></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.1.1. ></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.1.4.3.3.1.1" 스타일="color:#FF0000;">컨텍스트가 제공되지 않음</span></span> <span class="ltx_tr" id="A5.tab1.tab1.1.1.1.1.1.1.1.1.1.1.3.1.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-4: <span class="ltx_text" id="A5.tab1.1.1.1.1.1.1.1.3.3.1.1.1.1.1" 스타일="color:#FF0000;"><span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.tab1.tab1.1.1.1 </span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.4.3.3.1.12" style="padding-top:0.15pt;padding-bottom:0.15pt;">Ground Truth: 8 gymnasts</span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.4.3.3.1.13"><span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.tab1.1.1.1.1.1.4.3.3.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">participated this championship. ></span></span> <span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1.1.4.3.3.1.14.1" style="color:#FF8000;"><span class="ltx_text" id="A5.tab1.tab1.1.1.1.1.1.1.4.3.3.1.14.1" style="color:#FF8000;"><span class="ltx_tan></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>
</span></div>
</figure>
<figure id="A5.T10" class="ltx_table">
<div id="A5.T10.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:784.8pt;height:1084.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A5.T10.1.1"><span class="ltx_text" id="A5.T10.1.1.1.1.1"> <span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"> <span class="ltx_td ltx_align_left ltx_thltx_column ltx_border_tt" id="A5.T10.1.1.1.1.1.1.1.1.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tx_font_bold" id="A5.T10.1.1.1.1.1.1.1.1.2" 스타일="padding-top:0.15pt;padding-bottom:0.15 그것은 장애물입니다.</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.1.1.1.1.4"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.4" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">경마용 첨탑에서 이름을 파생합니다. <span class="ltx_td ltx_align_left ltx_border_tt" id="A5.T10.1.1.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_font_bold" id="A5.T10.1.1.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_align_left ltx_border_tt" id="A5.T10.1.1.1.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_td ltx_align_left ltx_border_tt" id="A <span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.2.1.2.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: The 3000 metres steeplechase is the</span></span> <span class="ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.2.1.1.4" style="padding-top:0.15pt;">Assistant: The 3000 metres steeplechase in</span></span> <span class="ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.4.1" style="A5.T10.1.1.1.1.1.1. </span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.8"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: what is part of it? <span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.2.1.2.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: 일부 남성들은 Henry Rono and</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.10"><span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.1.2.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Michael Karst. </span></span> <span class="ltx_tr" id="A5.T10.1.1.1.2.1.2.1.11"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: Which locations did Kerry O’Brien ran for? <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.1.2.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">Span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.1.2.1.1.3.1" style="padding-top:0.15pt;">Span class="ltx_tabular ltx_align_left <span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.2.2"><span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-70B: Kerry O'Brien ran in <span class="ltx_text" id="A5.T10.1.1.1.1.1.2.1.3.1.2.1" style="color:#41B729;">Adelaide and Berlin. <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.2.1.3.1.1" style="color:#41B729;">Berlin, Adelaide</span>,</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.2.1.3.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1" style="color:#FF0000;">Champaign</span>. </span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.5"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.2.1.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-4: Kerry O'Brien ran in <span class="ltx_text" id="A5.T10.1.1.1.1.1.2.1.3.1.5.1.1" style="color:#41B729;">Berlin and Adelaide</span>. </span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.6"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.2.1.3.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Ground Truth: He ran for both Berlin and Adelaide. <span class="ltx_tabular ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.3.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.1.1.3.2.1.1.1"><span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.3.2.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">countries:Schengen</span></span> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5. Source: You can travel visa-free only if you</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.3.2.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">have a "residence" card of a family member of a union</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.3.2.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;">citizen" issued under the free movement</span></span> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.3.2.1.1.4.4"><span class="ltx_td ltx_ Back</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.9"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">of ears is dark brown with black margin. 그것의 rhinarium</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.1.1.1.10" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">is naked and the lips is black, with small black hair</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.11"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.4.3.1.11.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">patches on upper part of nuzzle (shaft in eyes). </span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.12"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">title: Red fox. 빨간색 fox는 a more로 간주됩니다.</span> <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.1.13"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.4.3.1.1.1.14" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;">및 Bengal foxes in the direction of size and adaptation</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">to carnivory; skull displays far fewer neotenous</span></span> <span class="ltx_tr" id="A5.T10.1. 귀는</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.4.3.1.1.1.18"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.18.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.4.3.1.18.1" 스타일="color:#0000FF;"><span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.1.1.1.19" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A 코트 색상의 광범위한 변동은 존재합니다.</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.4.3.1.1.1.1.1.1.1.1.1.1.1.1.20" 스타일="color:#0000FF;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.4.3.1.1.1.21" 스타일="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.21.1" 스타일="color:#0000FF;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.21.1" 스타일="padding-top:0.15pt;padding-bottom:0.15pt <span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: 가장 큰 true foxes 및</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.4.3.2.1.3"><span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.4.3.2.1.3">the order Carnivora></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1 </span></span> <span class="ltx_tr" id="A5.T10.1.1.1.4.3.2.1.5"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: Where did it originate? </span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.6"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant: The species is Eurasian in origin. </span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.7"> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">User: 어떻게 진화했는가? [Bengal</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.4.3.2.1.1" style="color:#0000FF;">Bengal</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1.4.3.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Bengal</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1.4.3.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Foxes</span> <span class="ltx_tr" id="A5.T10. #0000FF;">this third type</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.2.1.2.1.1" style="color:#0000FF;">this third type</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.4.3.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.4.3.2.1.1.1" style="color:#0000FF;">of fox</span> <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.1.4.3.1" style="A5.T10.1.1.1.1.1.1.1.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Assistant:</span></span></span></span></span></span></span></span class="ltx_align_left" id="A5.T10.1.1.1.1.1.1.1.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">ChatQA-13B:</span></span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.1.1.1. Extensive variation in coat colour</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.4.3.3.1.1.1" style="color:#41B729;">exists across population and seasonally within</span></span></span> <span class="ltx_text" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1.1.1" style="color:#41B729;">populations but generally vary from grey</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"> "Vulpes bengalensis" is a relatively</span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1" style="color:#41B729;">ChatQA-70B: <span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.4.3.3.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.8.1" style="color:#41B729;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.4.3.3.1.1.1" style="padding-bottom:0.15pt;"><sp <span class="ltx_td ltx_nopad_r ltx_align_left" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1" style="color:#FF0000;">I’m sorry but I couldn’t find</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.1.4.3.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">GPT-3.5-turbo: <span class="ltx_text" id="A5.T10.1.1.1.1.1.1.3.3.1.11.1.1" style="color:#FF0000;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.1 <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.3.1.14.1" style="color:#FF0000;">The context does not provide information on</span></span></span> <span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.1.4.3.3.1.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.1.1.1.1.4.3.3.1.1.14.1.14" style="color:#FF0000;">a third type of fox. <span></span></span></span></span class="ltx_tr" id="A5.T10.1.1.1.1.1.4.3.3.1.1.16.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">Ground Truth: a</span></span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1.16.1" style="padding-top:0.15pt;">ground Truth: a</span></span></span class="ltx_tr" id="A5.T10.1.1.1.1.1.1.4.3.3.1.1.17.1" style="padding-top:0.15pt;">ground Truth: a</span></span></span class="ltx_tr" id="A5.T10.1.1.1. </span></span></span></span></span></</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 10:</span>Case study for ChatQA-13B, ChatQA-70B, GPT-3.5-turbo, and GPT-4. We use <span class="ltx_text" id="A5.T10.5.1" style="color:#0000FF;">blue</span> color to highlight the relevant context for the last user question. 그리고 <span class="ltx_text" id="A5.T10.6.2" style="color:#41B729;">green</span> and <span class="ltx_text" id="A5.T10.7.3" style="color:#FF0000;">red</span> color를 사용하여 어시스턴트 응답에서 올바르고 잘못된 부분을 강조 표시합니다.</figcaption>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Case Study</h2>

<div id="A6.p1" class="ltx_para">
<p class="ltx_p" id="A6.p1.1">ChatQA-13B, ChatQA-70B, GPT-3.5-turbo 및 GPT-4에 대한 더 많은 예는 표 <a class="ltx_ref" href="#A5.T10" title="Table 10 ‣ Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">10</span></a>에서 찾을 수 있다. <span class="ltx_text" id="A6.p1.1.1" style="color:#0000FF;">blue</span> color를 사용하여 마지막 사용자 질문에 대한 관련 컨텍스트를 강조 표시합니다. 그리고 <span class="ltx_text" id="A6.p1.1.2" style="color:#41B729;">green</span> and <span class="ltx_text" id="A6.p1.1.3" style="color:#FF0000;">red</span> color를 사용하여 어시스턴트 응답에서 올바르고 잘못된 부분을 강조 표시합니다.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Human Annotated Conversational QA Data</h2>

<section id="A7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.1 </span>Statistics of User-Agent Turns</h3>

<figure id="A7.F4" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2401.10225/assets/x4.png" id="A7.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="193" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 4:</span>Statistics regarding the number of user-agent turn for each dialogue</figcaption>
</figure>
<div id="A7.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS1.p1.1">그림 <a class="ltx_ref" href="#A7.F4" title="Figure 4 ‣ G.1 Statistics of User-Agent Turns ‣ Appendix G Human Annotated Conversational QA Data ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4</span></a>는 7k 인간 주석이 달린 대화 QA 데이터에 대한 대화당 사용자-에이전트 회전 수에 관한 통계를 보여준다.</p>
</div>
</section>
<section id="A7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.2 </span>Guidelines for Conversational QA Data Collection</h3>

<div id="A7.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS2.p1.1">이 섹션에서는 대화형 QA 데이터 수집에 대해 제공하는 지침을 보여준다. 주석이 주어진 문서를 기반으로 대화형 QA 샘플을 구성하기 위해 주석이 사용자와 에이전트 역할을 하도록 요청하여 주석을 보다 효율적으로 만든다. 지침은 1) 대화 QA 샘플이 어떻게 생겼는지, 2) 우리가 필요로 하는 대화 QA 샘플이 어떤 종류인지, 3) 주석을 달아야 하는 것의 세 부분으로 구성된다.</p>
</div>
<section id="A7.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.2.1 </span>What does conversational QA samples look like</h4>

<div id="A7.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS2.SSS1.p1.1">대화형 QA 샘플 하나는 우리가 제공하는 문서를 기반으로 합니다. 사용자 및 에이전트 행동은 다음과 같다:</p>
</div>
<div id="A7.SS2.SSS1.p2" class="ltx_para">
<ul id="A7.I1" class="ltx_itemize">
<li id="A7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A7.I1.i1.p1.1">사용자 행동: 1) 주어진 문서에 기초하여 에이전트에게 질문하기; 2) 에이전트가 무언가를 명확히 하고자 할 때 에이전트로부터 질문에 대답하기.</p>
</div>
</li>
<li id="A7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A7.I1.i2.p1.1">에이전트 행동: 1) 문서를 기반으로 사용자의 질문에 답하기; 2) 사용자의 질문이 명확하지 않거나 너무 일반적이거나 광범위할 때 사용자에게 질문하기.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A7.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.2.2 </span>What kinds of multi-turn QA samples we need</h4>

<div id="A7.SS2.SSS2.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS2.SSS2.p1.1">아래에 사용자의 질문 및 에이전트의 응답에 대한 요구 사항을 나열합니다.</p>
</div>
<section id="A7.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">User’s Questions</h5>

<div id="A7.SS2.SSS2.Px1.p1" class="ltx_para">
<ul id="A7.I2" class="ltx_itemize">
<li id="A7.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i1.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i1.p1.1">사용자의 질문은 자신의 이전(또는 여러 차례 이전) 질문을 참조할 수 있다.</p>
</div>
</li>
<li id="A7.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i2.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i2.p1.1">사용자의 질문은 에이전트의 이전(또는 이전 여러 차례) 답변을 참조할 수도 있습니다.</p>
</div>
</li>
<li id="A7.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i3.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i3.p1.1">앞에서 언급한 개체들을 대체하기 위해 대명사나 보통명사를 사용해 보세요.</p>
</div>
</li>
<li id="A7.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i4.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i4.p1.1">사용자의 질문을 다양하게 만들어 보세요. 동일한 유형의 질문에 대해 서로 다른 대화 주석으로 표현하기 위해 서로 다른 방법을 사용하려고 시도한다.</p>
</div>
</li>
<li id="A7.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I2.i5.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i5.p1.1">필요한 다른 사용자 질문 유형</p>
<ul id="A7.I2.i5.I1" class="ltx_itemize">
<li id="A7.I2.i5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A7.I2.i5.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A7.I2.i5.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i5.I1.i1.p1.1">에이전트의 답변을 감안할 때 추가 정보(예: 기타…; 기타…; 더…)를 요청하십시오.</p>
</div>
</li>
<li id="A7.I2.i5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A7.I2.i5.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A7.I2.i5.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i5.I1.i2.p1.1">주제를 전환하고 대화에서 새 스레드를 시작합니다.</p>
</div>
</li>
<li id="A7.I2.i5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A7.I2.i5.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A7.I2.i5.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A7.I2.i5.I1.i3.p1.1">동시에 두 가지 질문을 하세요.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section id="A7.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Agent’s Response</h5>

<div id="A7.SS2.SSS2.Px2.p1" class="ltx_para">
<ul id="A7.I3" class="ltx_itemize">
<li id="A7.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i1.p1" class="ltx_para">
<p class="ltx_p" id="A7.I3.i1.p1.1">에이전트의 답변을 1-2문장 안에 넣도록 해보세요. 대답이 길어야 한다면, 간결하게 해보세요.</p>
</div>
</li>
<li id="A7.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i2.p1" class="ltx_para">
<p class="ltx_p" id="A7.I3.i2.p1.1">문서의 전체 관련 컨텍스트를 답변으로 직접 복사하지 마십시오. 대신에, 선택된 문맥을 바꿔서 답을 구성해 보세요.</p>
</div>
</li>
<li id="A7.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I3.i3.p1" class="ltx_para">
<p class="ltx_p" id="A7.I3.i3.p1.1">에이전트가 무언가를 명확히 하기 위해 사용자에게 질문을 하는 경우의 적은 비율을 생각해내도록 노력하세요. 구체적으로, 사용자의 질문이 너무 광범위하거나 명확하지 않은 경우, 에이전트는 사용자가 어떤 특정 측면에 더 관심이 있는지 알기 위해 명확화 질문을 함으로써 범위를 좁힐 필요가 있다.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="A7.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">G.2.3 </span>What we need to annotate</h4>

<div id="A7.SS2.SSS3.p1" class="ltx_para">
<p class="ltx_p" id="A7.SS2.SSS3.p1.1">우리는 각 대화에 대해 주석을 달 필요가 있는 것을 아래에 나열한다.</p>
<ul id="A7.I4" class="ltx_itemize">
<li id="A7.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I4.i1.p1" class="ltx_para">
<p class="ltx_p" id="A7.I4.i1.p1.1">각 문서에 대해 사용자의 질문과 해당 에이전트의 응답에 주석을 달아야 합니다. 대화당 평균 사용자-에이전트 회전 수는 약 5개여야 합니다.</p>
</div>
</li>
<li id="A7.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A7.I4.i2.p1" class="ltx_para">
<p class="ltx_p" id="A7.I4.i2.p1.1">각 사용자의 질문에 대해 문서 내의 모든 관련 컨텍스트에 주석을 달아야 합니다.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2401.10224" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2401.10225" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2401.10225">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.10225" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2401.10226" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 09:19:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
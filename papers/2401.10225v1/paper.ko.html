<!DOCTYPE html>
<html lang="en" data-lt-installed="true"><head>
  <meta charset="UTF-8">
  <title>Title</title>
  <script>
    const text = '' +
      '# ChatQA: GPT-4 레벨 대화식 QA 모델 구축\n' +
      '\n' +
      'Zihan Liu\n' +
      '\n' +
      'Wei Ping\n' +
      '\n' +
      'Rajarshi Roy\n' +
      '\n' +
      'Peng Xu\n' +
      '\n' +
      'Mohammad Shoeybi\n' +
      '\n' +
      'Bryan Catanzaro\n' +
      '\n' +
      '###### Abstract\n' +
      '\n' +
      '본 연구에서는 GPT-4 수준의 정확도를 얻을 수 있는 대화형 질의 응답(QA) 모델 계열인 ChatQA를 소개한다. 구체적으로, 대용량 언어 모델(LLM)의 제로 샷 대화 QA 결과를 크게 개선할 수 있는 2단계 명령어 튜닝 방법을 제안한다. 대화형 QA에서 검색을 처리하기 위해 다중 회전 QA 데이터 세트에서 조밀한 검색기를 미세 조정하며, 이는 배치 비용을 크게 줄이면서 최신 쿼리 재작성 모델을 사용하는 것과 유사한 결과를 제공한다. 특히, 우리의 ChatQA-70B는 10개의 대화형 QA 데이터 세트(54.14 대 53.90)에서 평균 점수 측면에서 GPT-4를 능가할 수 있다. OpenAI GPT 모델의 합성 데이터에 의존하지 않습니다.\n' +
      '\n' +
      '머신러닝, ICML\n' +
      '\n' +
      '## 1 Introduction\n' +
      '\n' +
      '최근 ChatGPT(OpenAI, 2022)와 그 후속 조치(OpenAI, 2023; Anthropic, 2023; Google, 2023)는 생산 및 연구 커뮤니티에서 질의 응답(QA) 모델을 구축하는 패러다임 전환을 이끌었다. 특히, QA 모델의 다음 측면은 실제 애플리케이션에서 선호된다: _i)_ 사용자는 대화 방식으로 QA 모델과 상호 작용할 수 있으므로 후속 질문을 쉽게 제기할 수 있다. _ii) _ 일반 모델은 데이터 세트별 미세 조정 없이 제로 샷 방식으로 답변을 생성하는 동시에 미세 조정된 전문가 모델의 정확도를 일치시킬 수 있습니다. _ iii)_ QA 모델들은 오픈-도메인 또는 긴 문서 설정 모두에서 검색된 증거 청크들을 통합할 수 있으며, 여기서 제공된 컨텍스트는 LLM(예를 들어, Anthropic, 2023; Xu 등, 2023b)의 컨텍스트 윈도우보다 훨씬 더 길다. 이를 위해 이 세 가지 측면을 포괄하는 대화식 QA에 초점을 맞춘다.\n' +
      '\n' +
      '그러나, 최첨단 블랙박스 모델, 즉 GPT-4(OpenAI, 2023)의 정확도와 일치할 수 있는 대화형 QA 모델을 구축하는 것은 여전히 연구 커뮤니티에 큰 도전 과제이다. 본 연구에서는 GPT-4 수준의 정확도를 갖는 화이트박스 대화 QA 모델인 ChatQA-70B를 제안 2단계 명령어 튜닝 레시피, 대화 QA에서 검색 강화 생성을 위한 향상된 리트리버(RAG) 및 신중한 데이터 큐레이션 과정을 통해 소개한다.\n' +
      '\n' +
      '구체적으로 다음과 같은 기여를 한다.\n' +
      '\n' +
      '1. 제로 샷 대화 QA 작업에 대해 사용자가 제공하거나 검색된 컨텍스트를 통합하는 LLM 기능을 크게 향상시킬 수 있는 2단계 명령어 튜닝 방법과 데이터 세트 큐레이션 레시피를 제안한다. 본 논문에서 제안한 방법이 규칙적인 명령어 튜닝이나 RLHF 기반 레시피(예: Llama-2-Chat)를 훨씬 능가함을 보인다.\n' +
      '2. 대화형 QA에서 RAG의 경우, 인간 주석이 달린 다중 회전 QA 데이터 세트에서 미세 조정된 최첨단 단일 회전 쿼리 검색기가 최신 LLM 기반 쿼리 재작성 모델, 즉 GPT-3.5-터보(OpenAI, 2022)를 활용할 뿐만 아니라 작동한다는 것을 보여준다.\n' +
      '3. Llama2-7B, Llama2-13B, Llama2-70B(Touvron et al., 2023) 및 사내 8B 사전 훈련된 GPT를 기반으로 ChatQA 모델 패밀리를 구축한다. 검색이 필요한 긴 문서가 포함된 5개의 데이터셋과 테이블이 포함된 3개의 데이터셋을 포함하여 10개의 대화형 질의응답 데이터셋에 대한 종합적인 연구를 수행한다. 평균 점수 측면에서 ChatQA-70B 모델(54.14)은 ChatGPT 모델의 합성 데이터를 활용하지 않고도 GPT-3.5-터보(50.37) 및 GPT-4(53.90)를 능가할 수 있다.\n' +
      '4. 원하는 대답이 제공되거나 검색된 컨텍스트에 포함되지 않으므로 LLM이 쉽게 환각을 볼 수 있는 "응답할 수 없는" 시나리오를 연구한다. 우리는 명령어 튜닝에서 소량의 "응답할 수 없는" 샘플을 추가하는 것이 필요할 때 "응답할 수 없는" 출력을 생성하도록 모델을 조종하여 환각을 크게 줄일 수 있음을 보여준다. 우리의 ChatQA-70B는 이와 관련하여 GPT-3.5-터보를 능가하지만 GPT-4(약 3.5%)에 비해 약간의 격차가 있다.\n' +
      '\n' +
      '나머지 논문을 정리하면 다음과 같다. 우리는 SS 2의 관련 작업에 대해 논의한다. SS 3에서 ChatQA를 위한 2단계 지시 튜닝 방법을 소개하고, SS 4에서 검색 논쟁적 QA를 연구한다. SS 5에서 실험 설정을 제시하고 SS 6에서 결론을 내린다.\n' +
      '\n' +
      '## 2 관련 작업\n' +
      '\n' +
      '### Conversational QA\n' +
      '\n' +
      '대화 방식의 질의 응답은 후속 질문을 해결함으로써 사용자 경험을 자연스럽게 향상시킨다. 이 모델은 또한 필요한 경우 사용자에게 명확화 질문을 제기할 수 있으며, 이는 환각을 줄일 수 있다. 따라서 프로덕션에서 QA 모델을 배포하는 기본 형식(예: OpenAI, 2022; Google, 2023; Anthropic, 2023)이 됩니다.\n' +
      '\n' +
      '최근 몇 년 동안 많은 대화형 QA 데이터 세트가 소개되었으며, 여기서 모델은 제공된 컨텍스트 또는 문서를 기반으로 질문에 답하도록 요청된다. 제공된 컨텍스트 또는 문서들은: _i_) 텍스트-온리 다양한 도메인들(Feng et al., 2020; Anantha et al., 2021; Saeidi et al., 2018; Adlakha et al., 2022; Aliannejadi et al., 2021; Reddy et al., 2019; Qu et al., 2020; Wu et al., 2023; Deng et al., 2022; Guo et al., 2021; Choi et al., 2018; Campos et al., 2020), 또는 _ii_) 평문과 함께 테이블들(Pasupat & Liang, 2015; Nakamura et al., 2022; Chen et al., 2022)을 포함할 수 있다.\n' +
      '\n' +
      '최신 LLM 기반 일반주의 솔루션(예를 들어, OpenAI, 2022)과 대조적으로, 대부분의 이전 연구는 특정 도메인 또는 데이터 세트에 대한 미세 조정된 전문가 모델에 초점을 맞춘다(Feng et al., 2020; Izacard & Grave, 2021; Chen et al., 2022; Gao et al., 2022; Nakamura et al., 2022; Adlakha et al., 2022; Wu et al., 2023).\n' +
      '\n' +
      '### Multi-Turn QA 검색\n' +
      '\n' +
      '대화형 QA는 개방형 도메인 설정에서 검색 강화 생성(RAG)을 포함하거나 제공된 문서가 LLM 컨텍스트 창보다 길 때 발생한다. 밀집 검색기들은 일반적으로 단일 질문이 주어진 상위-\\(k\\) 관련 청크들을 검색하도록 트레이닝된다(예를 들어, Lin 등, 2023; Wang 등, 2022; Izacard 등, 2022). 대화 QA에서, 후속 질문들(예를 들어, 이전 대화에서 언급된 엔티티들을 지칭하는 대명사와 함께)은 검색을 위한 불충분한 정보를 가질 수 있는 반면, 대화 이력과 함께 그것들을 공급하는 것은 중복될 수 있고 차선의 결과들로 이어질 수 있다.\n' +
      '\n' +
      '#### 2.2.1 대화 쿼리 재작성\n' +
      '\n' +
      '이전 솔루션의 대부분은 쿼리 다시 쓰기 방법입니다. 최근 질문의 차례는 이전의 대화 이력으로부터 추가적인 정보가 없는 독립형 질의가 되도록 재작성된다(Vakulenko et al., 2021; Ye et al., 2023; Mo et al., 2023), 따라서 관련 컨텍스트를 검색하기 위해 검색 모델에 의해 직접 사용될 수 있다(Vakulenko et al., 2021; Mele et al., 2021; Raposo et al., 2022; Mo et al., 2023). 많은 데이터세트가 이러한 연구 라인을 용이하게 하기 위해 수집되었다(Elgohary et al., 2019; Chu et al., 2020; Qu et al., 2020; Anantha et al., 2021; Brabant et al., 2022), 다수의 제안된 쿼리 재작성 방법(Ishii et al., 2022; Yu et al., 2020; Wu et al., 2022; Del Tredici et al., 2021; Chen et al., 2022; Galimzhanova et al., 2023). 예를 들어, Wu 등(2022) 및 Chen 등(2022)은 질의 재쓰기를 위해 강화 학습 방법들을 사용하도록 제안하였다. Yu et al.(2020)은 질의 재쓰기를 위해 GPT-2와 같은 소수의 샷 생성 모델을 조사했다. Galimzhanova et al. (2023)은 명령어 튜닝 GPT-3.5-turbo를 연구하였고, 이것이 대화 질의 재쓰기를 위한 최신 결과를 달성했음을 보여주었다.\n' +
      '\n' +
      '#### 2.2.2 Fine-tuning Retriever for multi-turn QA\n' +
      '\n' +
      '일부 이전 작업은 도메인 내 대화 쿼리 및 컨텍스트 쌍에서 단일 회전 쿼리 검색기를 미세 조정하므로(Feng 등, 2020; Gao 등, 2022; Adlakha 등, 2022; Wu 등, 2023), 대화 히스토리 및 현재 쿼리의 연계를 입력으로 직접 취할 수 있다. 이 작업에서는 제로샷 평가에 초점을 맞춘다. 고품질 다중 회전 데이터 세트에서 단일 회전 쿼리 검색기를 미세 조정합니다. 그런 다음, 5개의 벤치마크 데이터 세트에 대해 미세 조정된 리트리버의 제로 샷 능력을 평가한다. 놀랍게도, 우리는 이 간단한 접근법이 최신 쿼리 재작성 모델, 즉 GPT-3.5-터보와 유사한 제로 샷 결과를 얻을 수 있음을 발견했다.\n' +
      '\n' +
      '### Instruction Tuning\n' +
      '\n' +
      '명령어 튜닝의 목표는 LLM들이 자연 언어 명령들을 따르는 능력을 갖추도록 하는 것이다(Wei et al., 2022; Sanh et al., 2022; Mishra et al., 2022; Iyer et al., 2022; Du et al., 2022; Ouyang et al., 2022; Wang et al., 2023; Zhang et al., 2023; Gao et al., 2023; Chung et al., 2022; Muennighoff et al., 2022; Xu et al., 2023; Wang et al., 2022c; Zhou et al., 2023). FLAN(Chung et al., 2022), Self-Instructions(Wang et al., 2022), unnatural Instructions(Honovich et al., 2022), Dolly(Conover et al., 2023), OpenAssistant(Kopf et al., 2023) 등 고품질의 명령어 튜닝 데이터셋 개발이 급증하고 있다.\n' +
      '\n' +
      '수업 튜닝에 대한 많은 연구가 수행되었지만, QA를 위한 RAG 또는 상황 인식 생성을 개선하는 데 초점을 맞춘 연구는 몇 가지이다. Lin et al. (2023)은 LLM 미세 조정을 위해 상위\\(k\\)개의 검색된 청크를 추가하는 검색 강화 명령어 튜닝 방법을 도입했다. Wang et al. (2023)은 검색 강화 사전 훈련 후에 명령어 튜닝을 적용하였다. 이에 반해 본 논문에서는 리트리벌(retrival) 또는 제공된 컨텍스트(context)를 갖는 생성을 개선하기 위한 2단계 명령어 튜닝 방법을 제안한다. 우리는 LLM 미세 조정을 위해 상위\\(k\\) 검색된 청크를 추가하는 것이 광범위한 대화 QA 작업에 도움이 되지 않는다는 것을 발견했다(자세한 내용은 SS6.3 참조).\n' +
      '\n' +
      'ChatGPT(OpenAI, 2022)가 출시된 후, 명령어 튜닝은 광범위한 작업에 놀라운 제로 샷 기능을 가진 최첨단 대화 에이전트를 구축하는 데 필수 요소가 된다. 대화형 QA 능력은 대화 에이전트에서 중요한 역할을 하지만 이러한 중요한 측면에 초점을 맞춘 연구는 제한적이다.\n' +
      '\n' +
      '## 3 ChatQA\n' +
      '\n' +
      '본 절에서는 ChatQA를 위한 2단계 명령어 튜닝 방법을 제안한다. 그림은 그림 1을 참조하십시오. 우리의 방법은 사전 학습된 LLM 기초 모델로 시작한다. 단계-1에서는 명령어-추종 및 대화 데이터 세트의 혼합에 대해 Ouyang 등(2022)에서와 같이 감독 미세 조정(supervised fine-tuning, SFT)을 적용한다. 그 후, 저희 모델은 대화 에이전트로서 지시를 따를 수 있는 좋은 능력을 보여줍니다. 그러나 상황화 또는 RAG 기반 QA에 대한 기능은 여전히 제한적이다. 따라서 본 논문에서는 질의응답에서 상황인지 또는 검색증강 생성을 위한 모델의 성능을 향상시키기 위한 상황강화 명령어 튜닝(context-enhanced instruction tuning)을 제안한다.\n' +
      '\n' +
      '### Stage-1: 감독 미세 조정\n' +
      '\n' +
      '본 연구에서는 Xu et al. (2023b); Wang et al. (2023a); 그리고 고품질 명령어 튜닝 데이터 세트로부터 128K개의 SFT 샘플들의 조합된 세트를 수집한다. 1) 소셜 대화 데이터세트 Soda(Kim et al., 2022), 2) 정교한 답변을 포함하는 롱폼 QA 데이터세트 ELI5(Fan et al., 2019), 3) FLAN 및 연쇄 사상 데이터세트(Wei et al., 2022b; Chung et al., 2022; Longpre et al., 2023), 4) LLM 합성 명령어 튜닝 데이터세트: Self-Instruct(Wang et al., 2022b) 및 Unnatural Instructions(Honovich et al., 2022), 및 5) 개인 크라우드 소스 대화 데이터세트뿐만 아니라, 두 개의 공개 인간-작성 대화 데이터세트: OpenAssistant(Kopf et al., 2023), 및 Dolly(Conover et al., 2023a).\n' +
      '\n' +
      '우리는 모든 SFT 데이터의 구조를 대화 형식으로 통일한다. 우리는 먼저 "시스템" 역할을 추가하여 LLM을 안내하는 일반적인 지침을 설정하여 예의 바르고 도움이 되는 답변을 제공한다. 또한 명령어 튜닝 데이터 세트에서 명령어와 응답 쌍을 통합하기 위해 "사용자" 및 "보조" 역할을 추가한다. 우리는 LLM 기초 모델에 이 통합된 형식을 사용하여 미세 조정을 적용한다.\n' +
      '\n' +
      '### Stage-2: Context Enhanced Instruction Tuning\n' +
      '\n' +
      '주어진 컨텍스트에 대한 모델의 대화식 QA 기능을 더욱 향상시키기 위해 컨텍스트화된 QA 데이터 세트를 명령어 튜닝 블렌드에 통합하는 2단계 명령어 튜닝을 수행한다. 구체적으로, 단계-2 명령어 튜닝 데이터 세트는 문맥화된 단일 턴 QA와 대화형 QA 데이터 세트의 혼합으로 구성된다. 아래 2단계 명령어 튜닝 데이터 세트에 대한 추가 세부 정보를 제시한다.\n' +
      '\n' +
      '#### 3.2.1 Human Annotated Data\n' +
      '\n' +
      '공개 가능한 데이터 세트 외에도 단계-2의 핵심 요소 중 하나는 고품질 문서 기반 대화 QA 데이터 세트를 얻는 것이다. 우리는 7k개의 대화로 구성된 인간 주석 대화 QA(HumanAnnotatedConvQA) 데이터 세트를 생성한다. 이 데이터 세트를 구축하기 위해 먼저 인터넷에서 다양한 주제를 다루는 7k개의 문서를 수집했다. 그런 다음 주석자에게 문서에 대한 질문(및 후속 질문)을 묻는 호기심 많은 사용자 및 답변을 제공하는 에이전트 역할을 모두 수행하도록 지시한다. 각 문서에 대해 멀티 턴 대화를 생성하여 대화당 평균 5번의 사용자-에이전트 턴을 갖는 총 7k개의 대화 QA 대화를 생성한다. 데이터 수집 가이드라인의 자세한 내용은 부록 G에서 확인할 수 있다.\n' +
      '\n' +
      '또한, 응답할 수 없는 경우에 환각 답변을 줄이기 위해 주어진 컨텍스트 내에서 답변을 찾을 수 없을 때 모델을 명시적으로 표시할 수 있도록 권한을 부여하는 것을 목표로 한다. 이러한 응답할 수 없는 데이터 샘플을 얻기 위해 주석자에게 모든 컨텍스트 위치를 사용자 질문에 제공하도록 요청했다. 따라서 컨텍스트의 해당 위치에서 텍스트를 삭제하여 응답할 수 없는 시나리오를 구성할 수 있었다. 질문에 대한 관련 텍스트를 삭제한 후, 답할 수 없는 질문에 대한 응답으로 "죄송합니다. 문맥에 따라 답변을 찾을 수 없습니다."라는 문장을 사용한다. 마지막으로, 응답할 수 없는 주석으로 또 다른 1.5k 사용자-에이전트 회전을 구성하며, 이는 응답할 수 있는 경우와 응답할 수 없는 경우의 좋은 트레이드오프를 제공한다(자세한 내용은 SS6.5 참조).\n' +
      '\n' +
      '#### 3.2.2 합성 데이터 생성\n' +
      '\n' +
      'HumanAnnotatedConvQA의 품질을 검증하기 위해 GPT-3.5-turbo를 활용하여 합성 대화를 생성한다.\n' +
      '\n' +
      '그림 1: ChatQA를 위한 2단계 명령어 튜닝 프레임워크.\n' +
      '\n' +
      'QA 데이터 세트는 강력한 명령어 수행 및 텍스트 생성 기능을 제공합니다. 대화형 QA를 위한 대규모 합성 데이터는 Dai 등(2022)에서도 탐색되었다. 본 연구에서는 LLM 미세 조정을 위한 중간 크기의 고품질 합성 데이터에 초점을 맞춘다.\n' +
      '\n' +
      'GPT-3.5-turbo에 대한 지침은 1) 도움이 되는 답변을 제공하기 위해 모델을 안내하는 시스템 역할, 2) 필요한 데이터 유형을 나타내는 대화 QA의 예, 3) 모델을 지시하여 내용을 기반으로 대화 QA를 생성하는 문서로 구성된다. 우리는 광범위한 도메인을 포함하는 일반적인 크롤링으로부터 7k개의 문서(문서당 평균 \\(\\sim\\)1k개의 단어)를 수집한다. 각 문서는 단일 대화 QA 샘플 생성에 사용되며, 이는 대화당 평균 4.4 사용자-에이전트 회전(SyntheticConvQA라고 함)으로 총 7k 개의 다중 회전 QA 대화로 이어진다.\n' +
      '\n' +
      'HumanAnnotatedConvQA와 유사하게 이 합성 데이터 세트에서 응답할 수 없는 주석으로 또 다른 1.5k 사용자 에이전트 회전을 구성합니다. 에이전트의 답변에 대한 컨텍스트 위치의 주석이 없기 때문에 SyntheticConvQA에서 응답할 수 없는 합성 샘플을 구성합니다. 구체적으로, 우리는 먼저 (각 대화의) 문서를 서로 다른 청크로 절단한다. 그런 다음 제거하려는 에이전트의 답변과 "높게 겹치는" 청크가 있고 나머지 청크는 에이전트의 답변과 "낮게 겹치는" 청크가 있는 경우에만 유효한 응답 불가능한 샘플로 간주한다. 각 청크와 에이전트의 답변 사이의 4-그램 리콜 점수(응답의 4-그램 구가 각 청크 내에 있는 비율을 측정함)를 메트릭으로 사용하여 중복을 측정하고 0.5보다 높은 것을 "높은 중복"으로, 0.1보다 낮은 것을 "낮은 중복"으로 간주한다.\n' +
      '\n' +
      '#### 3.2.3 Training Blends\n' +
      '\n' +
      '이 부분에서는 2단계 명령어 튜닝을 위한 훈련 블렌드의 세부 사항을 소개한다. 표 문서 처리 및 연산 계산에서 QA 성능을 향상시키기 위해 두 요소를 모두 포함하는 TAT-QA 데이터 세트(Zhu et al., 2021)를 추가한다. 또한, 컨텍스트화된 단일 턴 QA 데이터 세트를 통합하여 모델의 QA 기능을 더욱 강화한다. 또한, 모델의 명령어 수행 능력을 유지하기 위해 훈련 블렌드에 1단계 SFT 데이터 세트를 여전히 유지한다.\n' +
      '\n' +
      '마지막으로, 스테이지-2에 대한 트레이닝 블렌드는, 1) 대화형 QA 데이터세트: HumanAnnotatedConvQA 또는 SyntheticConvQA, 1 2) 단일 턴 QA 데이터세트: DROP(Dua et al., 2019), NarrativeQA(Kocisky et al., 2018), Quoref(Dasigi et al., 2019), ROPES(Lin et al., 2019), SQuAD1.1(Rajpurkar et al., 2016), SQuAD2.0(Rajpurkar et al., 2018), NewsQA(Trischler et al., 2017), TAT-QA(Zhu et al., 2021), 및 3) 스테이지-1로부터의 모든 SFT 데이터세트이다.\n' +
      '\n' +
      '각주 1: 달리 명시되지 않는 한, 실험은 HumanAnnotatedConvQA를 기본 설정으로 사용한다.\n' +
      '\n' +
      '우리는 모든 단일 전환 QA 및 대화 QA 데이터 세트를 통합하기 위해 단계-1에서와 유사한 템플릿을 따른다. 1) 시스템 역할에 따라 단일 회전 질문 또는 다중 회전 대화의 관련 컨텍스트를 추가하고, 2) 서로 다른 QA 데이터 세트(예: 짧은 답변, 긴 답변, 산술 계산)의 답변 유형을 기반으로 단일 회전 질문 또는 다중 회전 대화 직전에 추가 지침을 통합한다. 우리는 단계-1의 SFT 데이터 세트에 대한 형식을 사용한다. 2\n' +
      '\n' +
      '각주 2: 합성 데이터 생성뿐만 아니라 단계-1 및 단계-2 명령어 튜닝을 위한 템플릿의 세부 사항은 부록 A에서 확인할 수 있다.\n' +
      '\n' +
      '## 4 Multi-Turn QA를 위한 검색\n' +
      '\n' +
      '대화 질의 처리 작업에서 문서가 LLMs에 직접 입력되기에는 너무 길어지면 대화 질의를 처리할 수 있는 검색기가 필수적이다. 이 대화 검색기는 대화 이력과 현재 쿼리의 연결을 인코딩한 다음 문서에서 관련 컨텍스트를 검색합니다. 그 후, 관련 컨텍스트만이 LLM에 대한 입력으로서 사용될 것이다. 최첨단 검색기들, 예를 들어, Dragon(Lin 등, 2023)은 단일 턴 쿼리들에 최적화되어, 다중 턴 대화 쿼리들에 대한 제한된 일반화 능력을 초래한다. 그림 2에서 이 문제를 완화하기 위해 리트리버 미세 조정 방법을 보여준다. 본 논문에서는 단일 턴 리트리버를 보다 정교하게 조정하기 위해 대화형 질의와 컨텍스트 쌍을 사용하여 대화형 입력에 더 잘 대처할 수 있도록 제안한다.\n' +
      '\n' +
      '대안적인 해결책은 질의 재작성기를 사용하여 대화 히스토리를 기반으로 현재 질문을 재작성하는 대화 질의 재작성 방법이다. 그런 다음 다시 작성된 쿼리는 관련 컨텍스트를 검색하기 위해 단일 회전 쿼리 검색기에 대한 입력으로 직접 사용됩니다. 쿼리 재작성 모델은 임베딩 및 검색 비용 외에도 재작성된 쿼리를 생성하기 위해 많은 양의 추가 계산 비용을 도입한다.\n' +
      '\n' +
      '그림 2: 다중 회전 QA에 대한 미세 조정 리트리버의 그림.\n' +
      '\n' +
      '### 멀티턴 QA를 위한 Fine-tuning Retriever\n' +
      '\n' +
      '고품질 미세 조정 데이터 세트를 구축하기 위해 HumanAnnotatedConvQA 또는 SyntheticConvQA의 대화형 QA 데이터 세트를 활용하여 대화형 쿼리 및 컨텍스트 쌍을 구성합니다.\n' +
      '\n' +
      'HumanAnnotatedConvQA의 경우 대화형 쿼리와 컨텍스트 쌍의 주석을 직접 가져와서 단일 회전 쿼리 검색기를 추가로 미세 조정 하는 데 사용 합니다. SyntheticConvQA의 경우 먼저 대화 QA 데이터 세트의 각 문서를 다른 청크로 절단한다. 그런 다음 에이전트의 답변과 각 청크 사이의 4그램 리콜 점수를 계산한다. 그 후, 리콜 점수가 가장 높은 청크를 현재 사용자의 질문에 대한 골드 청크로 간주한다. 마지막으로, 구축된 대화 질의와 문맥 쌍은 단일 회전 질의 검색기를 미세 조정하기 위해 사용된다.\n' +
      '\n' +
      '### 대화식 쿼리 다시 쓰기\n' +
      '\n' +
      'Galimzhanova et al. (2023)이 GPT-3.5-turbo를 사용하여 최신 질의 재작성 결과를 입증했다는 점을 고려하여, 강력한 대화 질의 재작성 모델을 구축하기 위해 GPT-3.5-turbo를 재작성자로 사용한다. 우리는 Galimzhanova et al. (2023)과 유사하게 GPT-3.5-turbo를 재작성 작업 지도와 함께 제공할 뿐만 아니라 재작성 결과의 질을 높이기 위해 몇 번의 재작성 사례를 제공한다. 자세한 내용은 부록 B.1에서 확인할 수 있습니다.\n' +
      '\n' +
      '### Comparisons\n' +
      '\n' +
      '표 1에서는 제로 샷 설정에서 5개의 데이터 세트에 걸쳐 쿼리 다시 쓰기 및 미세 조정 방법을 비교한다. 이러한 데이터 세트에 대한 더 자세한 내용은 SS5.2.1에서 찾을 수 있다. 우리는 MS MACRO(Nguyen et al., 2016)에서 미세 조정되지 않은 최첨단 리트리버 Dragon(Lin et al., 2023), 및 강력한 비감독 리트리버 E5-unsupervised(Wang et al., 2022)에 대한 실험을 수행한다. Dragon을 대상으로 실험한 결과, Fine-tuning은 평균 Top-1 리콜에서 1.74% 정도 질의 재작성보다 약간 더 나쁜 성능을 보였으며, Top-5 리콜에서는 평균 0.54% 정도 더 좋은 성능을 보였다. 이를 통해 대화 검색을 위한 미세 조정 기법의 효용성을 입증한다. 또한 HumanAnnotatedConvQA와 SyntheticConvQA를 이용하여 미세조정을 수행한 결과 유사한 결과를 얻을 수 있었다. 이것은 인간 주석이 달린 데이터 세트가 고품질임을 강조하며 최신 멀티턴 쿼리 검색기를 구축하기 위해 ChatGPT 모델에 의존하지 않습니다.\n' +
      '\n' +
      '놀랍게도 미세 조정은 감독되지 않은 E5에서 다시 쓰는 것보다 훨씬 더 나은 성능을 보인다. 우리는 E5-unsupervised이 사전 훈련 단계에서 사람 주석이 달린 질의와 문맥 쌍을 사용하지 않기 때문에 고품질 재작성 질의에 대한 약한 일반화로 이어질 수 있다고 추측한다. 대조적으로, E5를 감독하지 않고 미세 조정하기 위해 고품질 데이터 세트를 사용하면 평균 Top-1 및 Top-5 리콜 점수 모두에서 15% 이상의 개선으로 엄청난 부스트가 발생한다.\n' +
      '\n' +
      '따라서 고품질 대화 쿼리 컨텍스트 쌍에서 좋은 싱글 턴 리트리버를 미세 조정하면 최신 재기록기를 활용하는 것과 동등하게 수행됩니다. 그러나, 재작성 방법은 자동 회귀 생성 과정을 위한 추가적인 계산 시간과 GPT-3.5-turbo와 같은 강력한 모델을 사용하기 위한 API 비용도 필요로 한다. 대조적으로, 제안된 다중 회전 미세 조정은 이러한 문제를 우회한다. 이 5개의 데이터 세트에 대한 QA 평가를 위해 모든 QA 모델에 대해 미세 조정 접근법에서 검색된 상위 5개의 결과를 일관되게 사용한다. 부록 B.2의 재작성 방법과 미세 조정 방법 간의 비교에 더 많은 결과를 넣었다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Models} & \\multirow{2}{*}{Average} & \\multicolumn{2}{c}{Doc2Dial} & \\multicolumn{2}{c}{QuAC} & \\multicolumn{2}{c}{QReCC} & \\multicolumn{2}{c}{TopiOCQA} & \\multicolumn{2}{c}{INSCIT} \\\\ \\cline{2-11}  & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-5* & top-20* & top-5* & top-20* \\\\ \\hline Adlakha et al. (2022) & - & - & - & - & - & - & - & - & - & 70.40\\({}^{\\triangle}\\) & - & - \\\\ Wu et al. (2023) & - & - & - & - & - & - & - & - & - & - & - & 71.10\\({}^{\\triangle}\\) \\\\ \\hline E5-unsupervised\\({}^{\\dagger}\\)(Wang et al., 2022) & 31.56 & 59.22 & 23.02 & 55.33 & 43.49 & 77.68 & 44.71 & 84.99 & 26.25 & 37.67 & 20.32 & 40.44 \\\\ E5-unsupervised + Rewrite\\({}^{\\ddagger}\\) & 33.23 & 61.02 & 25.56 & 58.00 & 46.00 & 80.01 & 45.50 & 85.89 & 27.58 & 39.15 & 21.53 & 42.04 \\\\ ES-unsupervised + Fine-tune\\({}^{\\ddagger}\\) & **47.79** & **75.00** & 45.28 & 80.96 & 46.52 & 80.74 & 53.37 & 89.91 & 41.01 & 51.07 & 52.79 & 72.31 \\\\ \\hline Dragon\\({}^{\\dagger}\\)(Lin et al., 2023) & 46.29 & 73.09 & 43.33 & 75.61 & 56.80 & 82.86 & 46.17 & 81.96 & 57.68 & 78.80 & 27.49 & 46.22 \\\\ Dragon + Rewrite\\({}^{\\dagger}\\) & **54.46** & 80.13 & 47.60 & 80.60 & 47.10 & 77.15 & 51.73 & 58.78 & 73.07 & 88.19 & 52.79 & 68.92 \\\\ Dragon + Fine-tune\\({}^{\\dagger}\\) & 52.72 & **80.67** & 48.94 & 83.01 & 52.64 & 81.95 & 50.73 & 87.17 & 67.86 & 86.28 & 43.43 & 64.94 \\\\ - SyntheticConvQA\\({}^{\\diamond}\\) & 52.98 & **81.15** & 48.64 & 83.47 & 54.75 & 83.23 & 49.63 & 86.70 & 64.48 & 85.24 & 47.41 & 67.13 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 1: 평균 Top-1 및 Top-5 리콜 점수를 갖는 5개의 다중 턴 QA 데이터 세트에 걸친 검색 결과. 다시 쓰는 것에 비해 미세 조정은 감독되지 않은 E5에서 훨씬 더 잘 수행되고 드래곤에서 비교할 수 있다. *TopiOCQA 및 INSCIT의 평균 컨텍스트 길이가 다른 데이터 세트보다 작기 때문에 상위 5 및 상위 20을 보고 하 여 다른 데이터 세트에서 각각 상위 1 및 상위 5의 컨텍스트 길이와 대략 일치 합니다. \\ ({}^{\\dagger}\\) 이 두 모델의 입력은 대화 이력과 현재 쿼리의 연결입니다. \\ ({}^{\\ddagger}\\) 이 모델의 입력은 다시 작성된 쿼리입니다. \\ ({}^{\\diamond}\\)는 HumanAnnotatedConvQA 데이터 세트가 미세 조정을 위해 SyntheticConvQA로 대체되었음을 나타냅니다. \\ ({}^{\\triangle}\\) 숫자는 사과와 사과 사이의 비교가 아니다(예를 들어, 미세 조정을 위해 훈련 세트를 사용한다).\n' +
      '\n' +
      '## 5 실험 설정\n' +
      '\n' +
      '이 섹션에서는 대화 질의 응답 작업에 대한 실험 설정에 대한 세부 사항을 제시한다.\n' +
      '\n' +
      '### Baselines\n' +
      '\n' +
      '우리는 다양한 모델 크기에 대한 실험을 수행한다. 먼저 단계 2 컨텍스트 강화 명령 튜닝의 효과를 보여주기 위해 단계 1 감독 미세 조정 (SFT) 후 Llama2-7B/13B/70B 기초 모델인 **Llama2-SFT-7B/13B/70B** 와 비교 합니다. 둘째, Llama2-Chat 모델은 다음 및 대화 QA 기능이 강력한 것으로 나타났기 때문에 **Llama2-Chat-7B/13B/70B** 와 비교 합니다 (Touvron 등, 2023). Llama2 모델 외에도 사내 GPT-8B 기초 모델에 대한 실험도 수행하고 1단계 SFT 기준선(**GPT-8B-SFT**)과 비교한다. 마지막으로 두 개의 매우 강력한 OpenAI 모델인 **GPT-3.5-터보(4k)** 및 **GPT-4(8k)** 와 비교합니다. 공정한 비교를 위해 우리는 모델과 기준선 모두에 대한 입력과 동일한 컨텍스트를 사용한다. 모든 기준선이 가능한 한 좋은 결과를 얻을 수 있도록 주의 깊게 지침을 조정했습니다. 3\n' +
      '\n' +
      '각주 3: 이러한 기준선에 대한 프롬프트는 부록 C에서 찾을 수 있습니다.\n' +
      '\n' +
      '### Evaluation Benchmarks\n' +
      '\n' +
      '#### 5.2.1 Long Document Datasets\n' +
      '\n' +
      '4K 토큰의 시퀀스 길이를 갖는 LLMs에 직접 피팅될 수 없는 긴 문서를 갖는 5개의 대화 QA 데이터 세트를 수집한다. 따라서 멀티턴 리트리버를 실행하여 상위 5개의 관련 청크를 입력으로 한다(실험은 SS4.3에서 찾을 수 있음).\n' +
      '\n' +
      '* Doc2Dial (Feng 등, 2020)은 DMV, SSA, VA 및 Student Aid의 4개 도메인을 포함하는 문서 기반 대화 QA 데이터 세트입니다. 각 샘플은 사용자가 문서에 관한 질의를 제기하는 대화로 구성되며, 에이전트는 이러한 질문에 응답한다. 평균 문서 길이는 약 101K 단어입니다.\n' +
      '* QuAC(Choi et al., 2018)는 위키피디아 문서를 기반으로 한다. 원래 그 문서는 짧다. 각 대화는 여러 개의 위키피디아 URL에 연결되어 있기 때문에 이러한 링크에서 텍스트를 추출하여 문서 크기를 대략 평균 15K 단어로 증가시킨다. 여기에는 주어진 컨텍스트 내에서 답변을 찾을 수 없는 답변할 수 없는 경우가 포함되어 있습니다.\n' +
      '* QReCC (Anantha et al., 2021)는 여러 원본에 걸쳐 열린 도메인 대화 QA 데이터 세트입니다. QuAC와 유사하게, 각각의 대화는 또한 대응하는 URL을 갖는다. 이러한 URL에서 텍스트를 추출하여 문서를 구성한다. 결국, 평균 문서 크기는 약 5K 단어이며, 최대 문서 크기는 20K 단어이다.\n' +
      '* TopiOCQA (Adlakha et al., 2022)는 전체 위키피디아를 기반으로 합니다. 이것은 토픽 전환을 통합하고 에이전트가 사용자 질문에 대한 답변을 위해 전체 위키피디아를 검색하도록 요구한다.\n' +
      '* INSCIT(Wu et al., 2023)는 또한 전체 위키피디아에 기초한다. 사용자 질문이 과소 지정되어 설명이 필요한 경우를 연구한다.\n' +
      '\n' +
      'Doc2Dial, QuAC 및 QReCC의 경우 문서를 약 300개의 단어 청크로 분할하고 각 사용자 질문에 대한 컨텍스트로 상위 5개의 관련 청크를 검색한다. TopioCQA 및 INSCIT의 경우 원래 분할을 따르므로 청크가 더 작아진다. 따라서 우리는 처음 세 개의 데이터 세트와 유사한 컨텍스트 길이를 얻기 위해 상위 20개의 청크를 검색했다.\n' +
      '\n' +
      '#### 5.2.2 Short Document Datasets\n' +
      '\n' +
      '문서 길이의 다양성을 높이기 위해 짧은 문서(1.5K 단어 미만)를 가진 5개의 대화식 QA 데이터 세트를 수집한다. 평균적으로, 1 단어는 1.5 토큰으로 토큰화될 것이다. 따라서, 문서는 4K 토큰의 시퀀스 길이를 갖는 LLM에 직접 피팅될 수 있다.\n' +
      '\n' +
      '* CoQA (Reddy et al., 2019)는 짧은 구절에 기반을 둔 각 대화를 사용 하는 대화형 QA 데이터 세트입니다. 답은 일반적으로 짧고, 지문은 아동 이야기, 문학, 중/고등학교 시험, 뉴스, 위키피디아와 같은 광범위한 영역을 다룬다.\n' +
      '* DoQA (캠포스 등, 2020)는 활성 Stack Exchange4 포럼에서 수집 된 요리, 여행 및 영화의 세 가지 영역을 다룹니다. 데이터 세트에는 지정된 문서 내에서 답변을 찾을 수 없는 답변할 수 없는 경우가 포함되어 있습니다. 각주 4: [https://stackexchange.com/](https://stackexchange.com/)\n' +
      '* ConvFinQA(Chen 등, 2022)는 금융 도메인을 기반으로 한다. 각 문서에는 테이블을 둘러싼 관련 텍스트와 함께 단일 재무 보고서 테이블이 포함되어 있습니다. 이 데이터 세트는 산술 계산과 복잡한 숫자 추론을 포함한다.\n' +
      '* SQA(Pasupat and Liang, 2015)는 주변 텍스트 없이 단일 테이블만 포함하는 문서에 기반합니다. 문서들은 위키피디아에서 수집되고, 질문들은 매우 구성적이어서 정답을 제시할 수 있는 견고한 표 이해 능력을 가진 모델이 필요하다.\n' +
      '* HybridDial (Nakamura et al., 2022)은 Wikipedia 표 및 텍스트 데이터를 모두 포함하는 문서에 기반을 둔 대화형 QA 데이터 세트입니다. 질문은 복잡하기 때문에 문서에 대한 추론이 필요하다.\n' +
      '\n' +
      '모든 10개의 데이터 세트에서 ConvFinQA, SQA 및 Hybrid-Dial 데이터 세트는 문서에 표 형식의 데이터를 포함하고 나머지 데이터 세트의 문서는 텍스트 전용.5입니다.\n' +
      '\n' +
      '각주 5: 이러한 벤치마크 데이터 세트의 세부 정보는 부록 C에 있습니다.\n' +
      '\n' +
      '### Evaluation Metrics\n' +
      '\n' +
      'F1 점수가 QA 모델을 평가하는 데 가장 일반적으로 사용되는 자동 메트릭이라는 점을 감안할 때 ConvFinQA를 제외한 모든 데이터 세트에 사용한다. ConvFinQA에서는 Chen et al.(2022)에 따라 정확한 매칭 메트릭을 사용한다. ConvFinQA의 답변은 산술적 계산뿐만 아니라 문서로부터 숫자를 추출하는 것에 관한 것이기 때문이다. 따라서 답은 답과 정확히 같을 때만 말이 된다. 모델들이 산술 공식을 생성할 때, 우리는 계산기를 기반으로 최종 결과를 계산하고 금답과 비교할 것이다.\n' +
      '\n' +
      '또한, 우리는 또한 우리의 최상의 모델과 GPT-4 사이의 생성된 답변의 정확성을 평가하기 위해 인간 평가를 수행한다.\n' +
      '\n' +
      '## 6 Results\n' +
      '\n' +
      '### Main Results\n' +
      '\n' +
      '#### 6.1.1 overview\n' +
      '\n' +
      '표 2에서 우리는 10개의 대화 QA 데이터 세트에 걸쳐 서로 다른 모델 변형과 OpenAI 모델을 비교한다.\n' +
      '\n' +
      '우리는 ChatQA 방법이 모델의 대화 QA 능력을 크게 향상시킨다는 것을 발견했다. 평균 점수 측면에서 Llama2-Chat 모델은 SFT 모델 대응물을 약간 능가하는 반면, 우리의 ChatQA 모델은 SFT 및 Chat 대응물에 비해 약 10점 이상의 절대 개선을 달성한다. 예를 들어, ChatQA-13B는 Llama2-13B-SFT 및 Llama2-13B-Chat에서 각각 13.17(37.69에서 50.86) 및 10.52(40.34에서 50.86) 향상된다. 이는 컨텍스트 강화 명령어 미세 조정을 통해 모델이 검색된 또는 관련 컨텍스트에서 유용한 정보를 효과적으로 추출하는 방법을 학습할 수 있기 때문이다.\n' +
      '\n' +
      'OpenAI 모델과 비교하여 최고의 모델 ChatQA-70B는 GPT-3.5-터보를 평균 3.77점 능가하고 GPT-4를 평균 0.24점 능가한다. 또한 훨씬 작은 크기의 ChatQA-13B는 평균 점수 0.49만큼 GPT-3.5-터보를 약간 능가할 수 있다.\n' +
      '\n' +
      '#### 6.1.2 Importance of Stage-1 SFT\n' +
      '\n' +
      '표 2에서는 ChatQA의 수업 추종 능력을 향상시키는 단계-1 SFT의 중요성에 대한 절제 연구를 수행한다. 미세 조정 단계에서 단계-1 SFT를 제거하고 기초 LLM 위에 단계-2 컨텍스트 강화 명령 튜닝을 적용한다. 우리는 평균 점수가 1.9 (54.08에서 52.18로) 떨어진다는 것을 발견한다. SQA 외에도 stage-1을 제거하면 모델이 다른 데이터 세트에서 일관되게 성능이 저하됩니다. 결과는 단계-1의 모든 SFT 데이터 세트가 단계-2 명령어 튜닝에도 혼합되더라도 단계-1이 여전히 중요한 역할을 한다는 것을 나타낸다. 먼저 명령어 수행 능력을 구축하는 것이 2단계 튜닝에 도움이 된다고 생각한다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c} \\hline \\hline Models & Average & Doc2Dial & QuAC & QReCC & CoQA & DoQA & ConvFinQA & SQA & TopiOCQA & HybridDial & INSCIT \\\\ \\hline GPT-8B-SFT & 34.46 & 31.03 & 20.07 & 37.69 & 59.24 & 21.72 & 15.44 & 40.06 & 38.17 & 52.29 & 28.86 \\\\ ChatQA-8B & **49.36** & 36.76 & 33.95 & 45.54 & 77.90 & 44.65 & 61.68 & 60.74 & 47.03 & 53.81 & 31.50 \\\\ \\hline Llama2-7B-SFT & 34.81 & 30.26 & 19.21 & 37.55 & 62.75 & 21.76 & 34.43 & 32.18 & 32.88 & 48.96 & 28.16 \\\\ Llama2-7B-Chat & 38.86 & 33.27 & 25.83 & 46.02 & 72.28 & 33.15 & 36.58 & 26.14 & 36.68 & 47.02 & 31.67 \\\\ ChatQA-7B & **47.71** & 37.88 & 29.69 & 46.97 & 76.61 & 41.57 & 51.61 & 61.87 & 45.45 & 54.51 & 30.96 \\\\ \\hline Llama2-13B-SFT & 37.69 & 30.68 & 21.59 & 38.25 & 69.52 & 21.70 & 41.14 & 37.85 & 35.26 & 52.22 & 28.73 \\\\ Llama2-13B-Chat & 40.34 & 34.74 & 27.89 & 47.19 & 72.50 & 32.60 & 41.54 & 25.39 & 39.25 & 49.82 & 32.52 \\\\ ChatQA-13B & **50.86** & 38.05 & 34.28 & 48.06 & 77.23 & 43.31 & 65.44 & 66.41 & 48.88 & 56.19 & 30.79 \\\\ \\hline Llama2-70B-SFT & 43.22 & 34.42 & 25.65 & 41.88 & 73.04 & 28.21 & 46.64 & 58.90 & 37.20 & 55.52 & 30.71 \\\\ Llama2-70B-Chat & 45.21 & 36.87 & 32.47 & 49.40 & 80.41 & 38.97 & 46.85 & 37.62 & 44.31 & 50.35 & 34.88 \\\\ ChatQA-70B & **54.14** & 38.90 & 41.82 & 48.05 & 78.57 & 51.94 & 73.69 & 69.14 & 50.98 & 56.44 & 31.90 \\\\ - SyntheticConvQA\\({}^{\\diamond}\\) & **54.08** & 39.19 & 38.33 & 48.73 & 79.83 & 48.65 & 76.44 & 68.63 & 51.30 & 55.68 & 33.98 \\\\ - w/o stage-1\\({}^{\\dagger}\\) & 52.18 & 38.43 & 37.52 & 46.08 & 73.51 & 49.42 & 72.15 & 72.08 & 51.28 & 50.74 & 30.56 \\\\ - w/o single-turn* & 52.25 & 38.30 & 37.89 & 47.08 & 76.74 & 46.43 & 72.42 & 67.41 & 49.85 & 53.16 & 33.18 \\\\ \\hline GPT-3.5-turbo (4k) & 50.37 & 34.83 & 37.17 & 50.46 & 79.33 & 41.11 & 73.15 & 60.63 & 44.30 & 47.42 & 35.27 \\\\ GPT-4 (8k) & **53.90** & 34.16 & 40.29 & 52.01 & 77.42 & 43.39 & 81.28 & 79.21 & 45.09 & 49.81 & 36.34 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 2: 10개의 데이터 세트에 걸쳐 제로 샷 대화 QA 결과입니다. \\ ({}^{\\diamond}\\)는 HumanAnnotatedConvQA가 SyntheticConvQA로 대체되었음을 나타냅니다. \\ ({}^{\\dagger}\\)는 스테이지-1(SFT)이 제거되고 ChatQA 스테이지-2 튜닝만 적용됨을 나타낸다. *단일 전환 QA 데이터 세트가 ChatQA 단계 2 훈련 혼합에서 제거됨을 나타냅니다. "w/o 스테이지-1" 및 "w/o 싱글 턴" 설정은 모두 SyntheticConvQA 데이터를 사용합니다. 평균 점수 측면에서 우리의 ChatQA 모델은 SFT 및 Chat 대응 모델을 크게 능가하고 최고의 모델 ChatQA-70B는 GPT-4를 약간 능가한다.\n' +
      '\n' +
      '#### 6.1.3 단일 턴 데이터의 효과성\n' +
      '\n' +
      '단일 회전 QA 데이터 세트가 모델의 다중 회전 QA 능력에 어떻게 영향을 미치는지 조사하기 위해 2단계의 ChatQA-70B 훈련 블렌드에서 이를 제거하여 절제 연구를 수행한다. 표 2에서 볼 수 있듯이, 2단계 훈련 블렌드(ChatQA-70B)에 단일 회전 QA 데이터 세트를 통합하면 일반적으로 모든 벤치마크 데이터 세트에 걸쳐 점수가 증가하여 평균 1.83점 개선된다. 흥미롭게도 ConvFinQA, SQA 및 HybridDial(테이블 기반 데이터 세트)의 개선은 추가된 단일 턴 QA 데이터 세트에도 불구하고 문서에 테이블 데이터가 없다. 이러한 결과는 우리의 직관과 일치한다. 단일 회전 데이터 세트를 추가하면 컨텍스트에서 답변을 추출할 수 있는 모델의 기능이 향상되어 대화형 QA 데이터 세트에서 더 나은 점수를 얻을 수 있다.\n' +
      '\n' +
      '#### 6.1.4 인간 주석 데이터 대. GPT-3.5-Turbo 합성 데이터\n' +
      '\n' +
      '표 2에서는 7k GPT-3.5-Turbo 합성 데이터 셋(SyntheticConvQA)과 수집된 7k 인간 주석 데이터 셋(HumanAnnotatedConvQA)을 사용하여 ChatQA 모델을 비교한 결과, 두 모델 모두 평균 점수 면에서 유사한 결과를 얻었으며, 이는 최신 대화형 QA 모델을 구축하기 위해 OpenAI 모델의 합성 데이터에 의존할 필요가 없음을 시사한다. 둘째, 인간 주석 데이터를 사용하면 QuAC 및 DoQA 데이터 셋에서 상당한 개선을 달성했으며, 이는 인간 주석 데이터가 QuAC 및 DoQA 데이터 셋에 존재하는 응답 불가 사례에 대해 더 높은 품질을 가지고 있기 때문에 결국 이 두 데이터 셋에 대한 전반적인 개선으로 이어지며, 응답 불가 사례에 대한 자세한 결과와 분석은 SS6.5에서 찾을 수 있다.\n' +
      '\n' +
      '#### 6.1.5 Human Evaluation\n' +
      '\n' +
      'F1 점수가 QA 모델의 품질을 평가하는 데 가장 일반적으로 사용되는 척도임에도 불구하고, 종종 질문에 답하는 여러 가지 방법이 있어 자동 척도가 완벽하지 않다. 따라서 우리는 인간 평가를 사용하여 우리의 ChatQA-70B와 GPT-4를 추가로 비교한다. 이 인간 평가에서 주석자에게 ChatQA-70B와 GPT-4의 출력에서 사실을 확인하고 질문 6에 대한 더 정확한 응답을 제공하는 모델을 결정한다. 10개의 데이터 세트에 대한 인간 평가 결과는 표 3에 나와 있다. 먼저 우리의 ChatQA-70B와 GPT-4는 대부분의 시간(69.09%)에서 동점이고 GPT-4는 우리보다 약간 높은 승률(\\(\\sim\\)3.3%)을 달성한다. 이는 우리의 모델이 정답을 생성할 수 있는 강력한 능력을 가지고 있음을 더욱 확인시켜준다. 둘째, ConvFinQA에서 GPT-4보다 우리 모델의 승률이 약간 더 좋은 것으로 나타났으며, 이는 우리 모델의 강력한 산술 계산 능력을 나타낸다. 셋째, SQA 과제에서 GPT-4가 훨씬 더 나은 승률을 달성한다는 것을 발견했는데, 이는 표식 추론 과제에서 우리의 모델과 GPT-4 사이에 여전히 격차가 있음을 시사한다.\n' +
      '\n' +
      '각주 6: 더 많은 인간 평가 설정은 부록 D에서 찾을 수 있다.\n' +
      '\n' +
      '### Fine-grained Analyses\n' +
      '\n' +
      '표 4에서 대화 QA 벤치마크에서 다양한 데이터 세트 유형에 걸쳐 모델과 OpenAI 모델을 추가로 비교했다. ChatQA-70B와 GPT-4의 비교에서, ChatQA-70B는 텍스트 전용 문서(avg-text)에서 더 나은 결과를 얻었으며, 이는 우수한 텍스트 이해 능력을 보여준다. 반면에 GPT-4는 avg-table의 비교를 감안할 때 표 데이터에서 더 나은 QA 능력을 보여준다. 검색이 필요하거나 필요하지 않은 데이터 세트의 경우 ChatQA-70B와 GPT-4가 비슷합니다(간격은 평균 점수 1 이내).\n' +
      '\n' +
      'ChatQA-13B와 GPT-3.5-터보의 비교에서,\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c} \\hline \\hline Models & Avg-text & Avg-table & Avg-ret & Avg-nonret \\\\ \\hline ChatQA-13B & 45.80 & 62.68 & 40.01 & 61.72 \\\\ ChatQA-70B & **48.88** & 66.42 & **42.33** & 65.96 \\\\ \\hline GPT-3.5-turbo (4k) & 46.07 & 60.40 & 40.41 & 60.33 \\\\ GPT-4 (8k) & 46.96 & **70.10** & 41.58 & **66.22** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 4: 서로 다른 데이터 세트 유형의 평균 점수에 대한 세밀한 연구. Avg-text는 Doc2Dial, QuAC, QReCC, CoQA, DoQA, Topi-OCQA 및 INSCIT와 같이 문서에만 텍스트가 있는 데이터 세트를 다룬다. Avg-table은 ConvFinQA, SQA 및 HybridDial을 포함한 문서에 테이블이 있는 데이터 세트를 포함합니다. Avg-ret은 Doc2Dial, QuAC, QReCC, TopiOCQA, INSCIT 등 검색이 필요한 긴 문서를 가진 데이터셋을 대상으로 한다. Avg-nonret은 CoQA, DoQA, ConvFinQA, SQA 및 HybridDial을 포함하여 검색이 필요하지 않은 짧은 문서를 가진 데이터 세트를 다룬다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c} \\hline \\hline  & Ours Win & Tie & GPT-4 Win \\\\ \\hline Average & 13.81\\% & 69.09\\% & 17.10\\% \\\\ \\hline Doc2Dial & 14.29\\% & 68.00\\% & 17.71\\% \\\\ QuAC & 11.67\\% & 73.33\\% & 15.00\\% \\\\ QReCC & 11.11\\% & 77.22\\% & 11.67\\% \\\\ CoQA & 7.78\\% & 80.00\\% & 12.22\\% \\\\ DoQA & 22.78\\% & 57.78\\% & 19.44\\% \\\\ ConvFinQA & 16.67\\% & 67.78\\% & 15.55\\% \\\\ SQA & 11.11\\% & 61.67\\% & 27.22\\% \\\\ TopiOCQA & 19.31\\% & 60.69\\% & 20.00\\% \\\\ HybridDial & 7.78\\% & 78.33\\% & 13.89\\% \\\\ INSCIT & 15.56\\% & 66.11\\% & 18.33\\% \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 3: 우리의 ChatQA-70B를 10개 이상의 데이터 세트에 걸쳐 GPT-4와 비교하는 인간 평가(A/B 테스트)이다. 평균 점수로 볼 때, 우리의 모델과 GPT-4는 대부분의 시간(69.09%)에서 동점이고, GPT-4는 우리보다 약간 높은 승률(\\(\\sim\\)3.3%)을 달성한다.\n' +
      '\n' +
      'ChatQA-13B는 Avg-table의 점수를 감안할 때 더 나은 표식 QA 기능을 보여준다. ChatQA-13B는 또한 검색이 필요하지 않은 문서가 있는 데이터 세트에서 더 나은 점수를 보여주지만 텍스트 전용 문서 및 검색이 필요한 문서에서는 GPT-3.5-터보와 동등하다.\n' +
      '\n' +
      '### Stage-2 명령 튜닝을 위한 Top-\\(k\\) Chunks\n' +
      '\n' +
      '2단계 튜닝에 의해 사용되는 모든 데이터 세트에 대해, 컨텍스트는 답변을 포함하는 연속적인 단락 또는 문서로 제공된다. 이와는 대조적으로, 모델은 긴 문서들에 대한 추론에서 상위\\(k\\)개의 검색된 청크들을 처리할 필요가 있다. 이러한 트레인/테스트 불일치를 줄이기 위해 일부 연속 문단을 검색된 상위 k 청크로 대체하면 모델의 견고성이 향상되는지 여부를 조사한다.\n' +
      '\n' +
      '각 질문에는 해당 긴 문서가 있기 때문에 이 연구에서는 NarrativeQA를 사용한다. NarrativeQA의 경우 원래 답변이 포함된 긴 문서의 요약을 문맥으로 사용한다. 불연속적인 컨텍스트를 통합하기 위해 먼저 긴 문서를 300개의 단어 청크로 절단한다. 그런 다음 드래곤 리트리버를 사용하여 질문에 대한 상위 4개의 청크를 추가 컨텍스트로 검색한다. 마지막으로, 검색된 4개의 청크와 긴 문서의 요약을 "top-5" 청크로 취한다. 7 우리는 이 재구성된 NarrativeQA를 사용하여 stage-2 명령어 튜닝을 위해 원래 NarrativeQA를 대체한다.\n' +
      '\n' +
      '각주 7: 우리는 상위 5개의 검색된 청크를 훈련에 직접 사용하지 않았는데, 그 이유는 답이 포함되어 있지 않을 수 있기 때문이다. 이러한 경우, 답을 생성하기 위해 모델을 미세 조정하면 환각을 조장할 수 있다.\n' +
      '\n' +
      '표 5에서 우리는 "top-5" 청크를 훈련의 컨텍스트로 사용하면 검색이 필요한 데이터 세트의 개선으로 이어진다는 것을 발견했다. 그러나 검색되지 않은 데이터 세트의 성능을 낮춥니다. 전반적으로 이 두 모델은 비슷합니다. 2단계 튜닝에 "top-5" 검색된 청크를 통합하면 검색이 필요한 추론 단계와 일치하므로 Avg-ret 점수가 향상되기 때문이다. 그러나 연속 및 불연속 문서의 혼합은 2단계 튜닝을 덜 안정적으로 만들 수 있으며, 이는 비검색 데이터 세트에 대한 차선 결과를 초래할 수 있다. 우리는 2단계 튜닝에서 연속 컨텍스트와 상위\\(k\\) 검색된 청크를 통합하는 균형과 관련하여 더 많은 미래 작업이 수행될 수 있다고 믿는다.\n' +
      '\n' +
      '### 추론 단계에 대한 제거 연구\n' +
      '\n' +
      '표 6에서는 검색된 컨텍스트/청크 수, 컨텍스트 순서 및 서로 다른 검색자가 대화 QA 결과에 어떻게 영향을 미치는지에 대한 절제 연구를 보여준다.\n' +
      '\n' +
      '첫째, 더 많은 컨텍스트를 입력으로 사용하는 것이 항상 결과를 향상시키는 것은 아니라는 것을 발견한다. Top-5 컨텍스트를 입력으로 사용하면 Top-3 또는 Top-10 컨텍스트를 사용하는 것보다 더 나은 결과를 얻을 수 있다. 직관적으로, 더 많은 컨텍스트는 정답(더 나은 회상 점수)을 포함할 확률이 더 높다. 결과적으로 Top-5 컨텍스트를 사용하는 것이 Top-3을 사용하는 것보다 더 나은 결과를 얻을 수 있다. 그러나 컨텍스트의 수가 더 증가함에 따라 모델은 "중간에서 손실" 현상을 겪을 수 있고(Liu et al., 2023), 제공된 컨텍스트에서 답변을 추출하는 어려움도 증가할 수 있으며, 이는 Top-10 컨텍스트를 사용함으로써 열등한 결과를 초래한다.\n' +
      '\n' +
      '둘째, 상위 5개 맥락의 서로 다른 순서를 사용하는 것이 결과에 어떤 영향을 미치는지 연구한다. 순차적 순서( \\(1\\)st context에서 \\(5\\)th context)와 역 순서( \\(5\\)th context에서 \\(1\\)st context)를 비교하고, 스윙 순서("중간에서 잃어버린" 현상이 주어졌을 때 입력 문맥의 시작과 끝에서 가장 적절한 문맥을 배열한다. 따라서 순서화는 \\(\\{1\\)st, \\(3\\)rd, \\(5\\)th, \\(4\\)th, \\(2\\)nd\\(\\}\\))가 되고, 랜덤 순서화는 상위 5개의 컨텍스트를 무작위로 섞는다. 순차 순서를 사용하는 것이 역방향 순서와 스윙 순서를 사용하는 것과 비슷하고 무작위 셔플링이 약간 더 나쁘다는 것을 발견했다. 결과는 우리의 모델이 정답의 위치에 관계없이 긴 컨텍스트에서 정답을 추출하는 데 탁월하다는 것을 나타낸다. ChatQA 미세 조정 과정에서 답변의 위치가 문맥 내에서 무작위로 발생하기 때문이다.\n' +
      '\n' +
      '셋째, \'드래곤+파인튜닝\'을 기존의 비파인튜닝드래곤 리트리버로 교체하면 평균점수가 1.81점(42.31점에서 40.50점) 하락하는 것으로 나타났다. 또한, INSCIT 데이터 세트(33.98에서 27.87)에서는 성능 차이가 커서 점수가 크게 떨어졌다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l l l l} \\hline \\hline Models & Avg. & D2D & QuAC & QReCC & TopIO & INSCIT \\\\ \\hline ChatQA-70B & 42.31 & 39.19 & 38.33 & 48.73 & 51.30 & 33.98 \\\\ - \\# of ctx: top-3 & 41.91 & 37.20 & 38.35 & 48.94 & 52.78 & 32.27 \\\\ - \\# of ctx: top-10 & 40.71 & 37.06 & 36.95 & 47.61 & 49.40 & 32.53 \\\\ - \\# ctx reverse ordering & 42.48 & 39.08 & 38.85 & 49.63 & 51.16 & 33.69 \\\\ - ctx swing ordering & 42.30 & 39.35 & 38.09 & 49.09 & 50.98 & 33.99 \\\\ - ctx random ordering & 42.01 & 39.32 & 38.28 & 48.79 & 50.13 & 33.51 \\\\ - Dragon Retrieval & 40.50 & 37.92 & 38.44 & 47.88 & 50.39 & 27.87 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 6: 검색이 필요한 데이터 세트에 걸쳐 입력 컨텍스트에 대한 절제 연구. 모든 모델은 SyntheticConvQA를 사용하고 있습니다. D2D는 Doc2Dial을 나타내고, TopIO는 TopiOCQA를 나타낸다. 입력(# of ctx), 컨텍스트 순서(역, 스윙, 랜덤) 및 원본 드래곤에서 검색된 컨텍스트를 사용하는 컨텍스트의 수에 대해 연구한다. 이에 비해 ChatQA-70B(기본 설정)는 "Dragon + Fine-tune" 검색한 Top-5 컨텍스트와 Top-5의 첫 번째 컨텍스트에서 다섯 번째 컨텍스트까지의 순차적 순서를 사용하고 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline Models & Avg-all & Avg-ret & Avg-nonret \\\\ \\hline ChatQA-70B & **54.14** & 42.33 & **65.96** \\\\ - \\# of “top-5” chunks & 54.04 & **42.91** & 65.16 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 5: "top-5" 검색된 청크를 스테이지-2 명령어 튜닝을 위한 컨텍스트로 사용하는 것에 대한 절제 연구. 모든 데이터 세트(Avg-all), 5개의 검색 데이터 세트(Avg-ret) 및 5개의 비검색 데이터 세트(Avg-nonret)에 대한 평균 점수를 보고한다.\n' +
      '\n' +
      '(표 1에 표시된 대로) 두 개의 검색기입니다. 기본적으로 검색의 품질이 향상되면 질의 응답의 성능을 직접적으로 향상시킨다.\n' +
      '\n' +
      '### Unanswerable Case 평가\n' +
      '\n' +
      '#### 6.5.1 평가 설정\n' +
      '\n' +
      '이 섹션에서는 제공된 컨텍스트 내에서 질문에 답할 수 있는지 식별하는 모델의 능력의 또 다른 측면을 연구한다. 답할 수 없는 경우에 답을 생성하는 것은 환각을 초래할 것이다. 이 평가를 허용하기 위해 주어진 맥락에서 답을 찾을 수 없을 때 모델을 표시해야 한다.\n' +
      '\n' +
      'QAC 및 DoQA 데이터 세트를 사용하여 이러한 성능을 평가합니다. 구체적으로, 답할 수 없는 경우에 대해, 질문이 정답일 수 없다는 것을 나타내는 모델을 올바른 것으로 간주하고, 답할 수 있는 경우에 대해, 질문이 정답일 수 없다는 것을 나타내지 않는 모델(즉, 답변을 주는 모델)을 고려한다. 답변 가능한 경우 올바른 컨텍스트가 검색되는 샘플만 선택합니다. 우리의 모델 체크포인트(ChatQA-70B w/1.5k 응답 불가 및 ChatQA-70B w/ SyntheticConvQA)는 표 2에 보고된 것과 동일하다.\n' +
      '\n' +
      '각주 8: 생성된 모든 샘플로부터 휴리스틱 매칭 패턴 세트를 수집하여 모델이 질문이 답할 수 없다고 제안하는지 여부를 결정한다. 자세한 내용은 부록 E에서 확인할 수 있다.\n' +
      '\n' +
      '결국, 우리는 답할 수 없는 경우와 답할 수 없는 경우의 평균 정확도 점수를 최종 메트릭으로 계산한다. 이 평균 정확도는 정확도와 재현율 점수의 조화 평균을 측정하는 F1 메트릭과 동일하기 때문에 신뢰할 수 있는 메트릭으로 간주한다.\n' +
      '\n' +
      '#### 6.5.2 Results\n' +
      '\n' +
      '표 7에서 우리는 QuAC 및 DoQA 데이터 세트에 걸쳐 OpenAI 모델과 모델을 비교한다. 먼저, HumanAnnotatedConvQA를 사용하면 SyntheticConvQA를 사용하는 것과 비교하여 QuAC와 DoQA 모두에서 평균 정확도가 크게 증가한다는 것을 발견했다. 이는 사람이 주석을 달지 않은 데이터에 대해 응답할 수 없는 주석이 더 품질이 높기 때문에 응답할 수 없는 경우에 대한 정확도가 크게 향상되기 때문이다. 둘째, OpenAI 모델은 GPT-4에서 가장 높은 성능을 보였으며, GPT-3.5-turbo 모델에 비해 평균 정확도는 크게 향상되었으나, GPT-4(약 3.5%)에 비해서는 다소 차이가 있었다. 셋째, 응답할 수 없는 표본에서 더 높은 정확도를 달성하는 모델은 응답할 수 있는 표본에서 더 낮은 정확도를 얻는 경향이 있고 그 반대의 경우도 마찬가지임을 알 수 있다. 우리는 모델이 "공격적인" 경향이 있고 답할 수 없는 질문에 대해 다소 적절한 답변을 제공할 때 답할 수 있는 경우에 대한 정확도를 높이지만 답할 수 없는 경우에 대한 정확도를 감소시킬 것이라고 추측한다. 반대로 어떤 모델이 더 "보수적"이고 질문에 답할 수 있는지 엄격하게 체크하면 반대의 효과가 발생한다.\n' +
      '\n' +
      '우리는 HumanAnnotatedConvQA에서 응답할 수 없는 샘플의 수로 절제 연구를 수행한다. 미응답 샘플(예: 1.5k)을 사용하면 전체 대화 QA 점수(예: Avg-CQA)뿐만 아니라 미응답 평가에서도 놀라운 결과를 얻을 수 있음을 발견했다. 흥미롭게도, 더 많은 응답할 수 없는 샘플을 통합하는 것이 반드시 응답할 수 없는 평가에서 더 높은 정확도 점수로 이어지는 것은 아니다.\n' +
      '\n' +
      '### Case Study\n' +
      '\n' +
      '표 8에서 ChatQA-70B 및 GPT-4 출력의 4가지 예를 보여준다.\n' +
      '\n' +
      '각주 9: 더 많은 예는 부록 F에서 찾을 수 있다.\n' +
      '\n' +
      '첫 번째 예제는 간단한 정보 탐색 문제이며 ChatQA-70B와 GPT-4 모두 올바르게 답합니다. 두 번째 예제(약 _Arya_)에서 모델은 답변을 제공하기 위해 암시적 정보(파란색으로 강조 표시됨)를 찾아야 합니다. GPT-4는 안전한 경향이 있다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l|c|c c c|c c c||c} \\hline \\hline Models & Avg-Both & Avg-QuAC & QuAC (no*) & QuAC (yes*) & Avg-DoQA & DoQA (no*) & DoQA (yes*) & Avg-CQA \\\\ \\hline ChatQA-70B & & & & & & & & \\\\ - 1k unanswerable\\({}^{\\dagger}\\) & 76.88 & 80.89 & 75.10 & 86.67 & 72.88 & 64.49 & 81.26 & **54.16** \\\\\n' +
      '**- 1.5k unanswerable\\({}^{\\dagger}\\)** & **77.25** & **80.76** & 77.66 & 83.85 & **73.74** & 68.81 & 78.67 & **54.14** \\\\\n' +
      '**- 2k unanswerable\\({}^{\\dagger}\\)** & 77.10 & 80.82 & 77.59 & 84.05 & 73.38 & 67.95 & 78.80 & 53.86 \\\\\n' +
      '**- 2.5k unanswerable\\({}^{\\dagger}\\)** & 75.87 & 78.81 & 73.76 & 83.85 & 72.93 & 66.54 & 79.31 & 53.78 \\\\\n' +
      '**- SyntheticConvQA\\({}^{\\diamond}\\)** & 69.84 & 72.92 & 55.38 & 90.42 & 66.77 & 45.09 & 88.45 & 54.08 \\\\ \\hline GPT-3.5-turbo (4k) & 73.27 & 78.34 & 61.91 & 94.76 & 68.21 & 51.99 & 84.43 & 50.37 \\\\ GPT-4 (8k) & **80.73** & **87.42** & 83.45 & 91.38 & **74.05** & 74.28 & 73.82 & **53.90** \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 7: QuAC 및 DoQA 데이터 세트에 걸쳐 답변 가능 및 답변 불가능 샘플에 대한 정확성. Avg-둘 다 QuAC와 DoQA 사이의 평균 점수입니다. \\ ({}^{\\diamond}\\)는 HumanAnnotatedConvQA가 SyntheticConvQA로 대체되었음을 나타낸다. ***-no"** 는 응답할 수 없는 샘플을 나타내고, "yes" 는 응답할 수 있는 샘플을 나타냅니다. \\ ({}^{\\dagger}\\) 우리는 HumanAnnotatedConvQA에서 응답할 수 없는 샘플의 수로 절제 연구를 수행한다. Avg-CQA는 10개의 대화형 QA 데이터 세트에 대한 평균 점수이다. 우리는 고품질 생성과 환각을 모두 생성하기 때문에 최종 ChatQA-70B에 1.5k의 응답할 수 없는 샘플을 사용한다.\n' +
      '\n' +
      '답변을 제공하고, 맥락이 나이에 대한 정확한 정보를 제공하지 않는다는 답변이 또한 정확하다.\n' +
      '\n' +
      '세 번째 예와 네 번째 예 모두 모델이 좋은 표의 이해와 추론 능력을 가질 것을 요구한다. 세 번째 예(약 _Massif_)에서 ChatQA-70B는 보존 면적의 크기를 3100ha와 비교하여 정답을 제공하는 반면 GPT-4는 그렇지 않다. 네 번째 예(약 _John B. England_)에서 ChatQA-70B는 세 날짜를 정확하게 나열하지만 한 날짜를 놓치는 반면 GPT-4는 질문에 올바르게 대답합니다.\n' +
      '\n' +
      '## 7 Conclusion\n' +
      '\n' +
      '이 논문에서 우리는 7B에서 70B까지 모델 크기가 다양한 ChatQA 모델 패밀리를 구축한다. 10개의 대화형 QA 데이터 세트에 대한 종합적인 평가는 우리의 최상의 ChatQA-70B 모델이 ChatGPT 모델의 합성 데이터를 사용하지 않고 GPT-3.5-터보보다 현저하게 능가하고 GPT-4와 동등하게 수행할 수 있음을 보여준다. 또한, 큐레이션된 대화 QA 데이터를 사용하여 단일 회전 쿼리 리트리버를 미세 조정하면 추가 계산 시간과 재작성으로 인한 잠재적인 API 비용 없이 최신 LLM 기반 쿼리 재작성 모델에 필적하는 성능을 보여준다. 또한 소량의 "응답할 수 없는" 샘플을 통합하면 답변을 사용할 수 없는 시나리오를 처리할 수 있는 모델의 기능이 크게 향상될 수 있음을 보여줍니다. 응답할 수 없는 사례 평가는 우리의 최상의 모델 ChatQA-70B가 GPT-4와 비교하여 약간의 차이만 있음을 강조한다.\n' +
      '\n' +
      '## References\n' +
      '\n' +
      '*[1]A. 아들라카 두리아왈라 Suleman, H. de Vries, S. Reddy(2022) TopiocQA: 주제 전환과 함께 열린 도메인 대화 질문 답변. TACL 로 인용: SS1.\n' +
      '*[2]M. Aliannejadi, J. Kiseleva, A. Chuklin, J. Dalton, and M. Burtsev (2021)는 질문을 명확히 하는 개방형 대화 말뭉치를 구축하고 평가합니다. EMNLP에서 인용: SS1.\n' +
      '*[3]R. 아난타 바쿨렌코 투성 롱프리 Pulman, S. 채피디(2021) 오픈 도메인 질문 답변은 질문 다시 쓰기를 통해 대화한다. NAACL에서 인용: SS1.\n' +
      '*[4]T. Anantho(2023) 100k 컨텍스트 윈도우를 소개합니다. 외부 링크: 연결된 링크: SS1입니다.\n' +
      '*[5]Q. Brabant, G. Lecorve, and L. M. Barahona (2022) Coqar: coqa에 대한 질문 다시 쓰기. LREC에서 인용: SS1.\n' +
      '\n' +
      '[MISSING_PAGE_POST]\n' +
      '\n' +
      '* Chen et al.(2022a) Chen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., and Wang, W. Y. Convfinaq: Exploring the chain of numerical reasoning in conversationational finance question answering. 2022a, _EMNLP_에서.\n' +
      '* Chen et al.(2022b) Chen, Z., Zhao, J., Fang, A., Fetahu, B., Rokhlenko, O., and Malmasi, S. 대화형 질문 응답을 위한 강화된 질문 재작성 *EMNLP_, 2022b.\n' +
      '* Choi et al. (2018) Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W. -t., Choi, Y., Liang, P., and Zettlemoyer, L. Quac: 문맥에 따른 질의 응답. 2018년 \'EMNLP\'에서요\n' +
      '* Chu et al. (2020) Chu, Z., Chen, M., Chen, J., Wang, M., Gimpel, K., Faruqui, M., and Si, X. 어떻게 하면 더 좋은 질문을 할 수 있을까요? 잘못된 형식의 질문을 다시 쓰기 위한 대규모 다중 도메인 데이터 세트입니다. 2020년 _AAAI_에서.\n' +
      '* Chung et al.(2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. Scaling instruction-finetuned language models. _ arXiv preprint arXiv: 2210.11416_, 2022.\n' +
      '* Conover et al.(2023a) Conover, M., Hayes, M., Mathur, A., Meng, X., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., et al. Free dolly: Introducing the world first truly open instruction-tuned llm, 2023a.\n' +
      '* Dai et al.(2022b) Dai, Z., Chaganty, A. T., Zhao, V., Amini, A., Green, M., Rashid, Q., and Guu, K. 대화 상자 인페인팅: 문서를 대화 상자로 전환합니다. *ICML_, 2022b.\n' +
      '* Dasigi et al. (2019) Dasigi, P., Liu, N. F., Marasovic, A., Smith, N. A., and Gardner, M. Quoref: 협력 추론이 필요한 질문이 있는 읽기 이해 데이터 세트입니다. 2019년 \'EMNLP\'에서\n' +
      '* Del Tredici et al. (2021) Del Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and de Gispert, A. Question rewriting for open-domain conversational qa: Best practices and limitations. 2021년, _CIKM_에서.\n' +
      '* Deng et al.(2022) Deng, Y., Lei, W., Zhang, W., Lam, W., and Chua, T. - S. 태평양: 금융의 표와 텍스트 데이터에 대한 사전 대화식 질의응답을 지향합니다. 2022년 \'EMNLP\'에서\n' +
      '* Du et al.(2022) Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. GIm: General language model pretraining with autoregressive blank infilling. 2022년 ACL에서\n' +
      '* Dua et al. (2019) Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. 드롭: 단락에 대한 이산 추론이 필요한 읽기 이해 벤치마크. 2019년 NAACL에서.\n' +
      '* Elgohary et al. (2019) Elgohary, A., Peskov, D., and Boyd-Graber, J. Unpack it? 문항을 맥락에 맞게 다시 쓰는 학습. 2019년 \'EMNLP\'에서\n' +
      '* Fan et al. (2019) Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and Auli, M. Eli5: 긴 형태의 질문 답변. 2019년 _ACL_ 입니다.\n' +
      '* Feng et al.(2020) Feng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and Lastras, L. doc2dial: 목표 지향 문서 기반 대화 데이터 세트입니다. _EMNLP_, 2020.\n' +
      '* Galimzhanova et al. (2023) Galimzhanova, E., Muntean, C. I., Nardini, F. M., Perego, R., and Rocchietti, G. Rewriting conversation utterance with instructed large language models. IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology_, 2023에서.\n' +
      '*Gao et al.(2022) Gao, C., Zhang, W., and Lam, W. Unigdd: 목표 지향 문서 기반 대화를 위한 통일된 생성 프레임워크. 2022년 ACL에서\n' +
      '* Gao et al. (2023) Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H. Retrieval-augmented generation for large language models: A survey _ arXiv preprint arXiv:2312.10997_, 2023.\n' +
      '* Google (2023) Google. 하드, 2023\n' +
      '* Guo et al.(2021) Guo, M., Zhang, M., Reddy, S., and Alikhani, M. Abgo-coqa: 대화 질문 답변에서 모호함을 명확히 한다. 2021년, _AKBC_에서.\n' +
      '* Honovich et al. (2022) Honovich, O., Scialom, T., Levy, O., and Schick, T. 부자연스러운 명령: 인간 노동력이 거의 없는 언어 모델 조정 _ arXiv preprint arXiv:2212.09689_, 2022.\n' +
      '* Ishii et al. (2022) Ishii, E., Xu, Y., Cahyawijaya, S., and Wilie, B. Can question rewriting help a conversationational question answering? 2022년 NLP의 부정적인 결과에서 통찰력에 대한 세 번째 워크샵의 진행에서.\n' +
      '* Iyer et al.(2022) Iyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S., et al. Opt-impl: Scaling language model instruction meta learning through the lens of generalization. _ arXiv preprint arXiv:2212.12017_, 2022.\n' +
      '\n' +
      '* Izacard & Grave (2021) Izacard, G. and Grave, E. Leveraging passage retrieval with Generative models for open domain question answering. 2021년 <컴퓨팅 언어학 협회 유럽 제16차 회의>에서.\n' +
      '* Izacard et al. (2022) Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning _ Transactions on Machine Learning Research_, 2022.\n' +
      '* Kim et al.(2022) Kim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y., Zhou, P., Bras, R. L., Alikhani, M., Kim, G., Sap, M., et al. Soda: Million-scale dialogue distillation with social commonsense contextualization. _ arXiv preprint arXiv:2212.10465_, 2022.\n' +
      '* Kocisky et al. (2018) Kocisky, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The narrativeqa reading comprehension challenge. _ TACL_, 2018.\n' +
      '* Kopf 등(2023) Kopf, A., Kilcher, Y., von Rutte, D., Anagnostidis, S., Tam, Z. - R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., et al. Openassistant conversations-democratizing large language model alignment. _ arXiv preprint arXiv:2304.07327_, 2023.\n' +
      '* 대규모 언어 모델 정렬을 민주화합니다. _ arXiv preprint arXiv: 2304.07327_, 2023.\n' +
      '* Lin et al. (2019) Lin, K., Tafjord, O., Clark, P., and Gardner, M. 상황에서 단락 효과를 추론합니다. 2019년, _질문에 응답하기 위한 기계 읽기에 대한 두 번째 워크샵의 진행_ 에서.\n' +
      '* Lin et al.(2023a) Lin, S. - C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y., Yih, W. -t., 및 Chen, X. 드래곤을 훈련시키는 방법: 일반화할 수 있는 고밀도 검색을 위한 다양한 확대. _ arXiv preprint arXiv:2302.07452_, 2023a.\n' +
      '* Lin et al.(2023b) Lin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et al. Ra-dit: Retrieval-augmented dual instruction tuning _ arXiv preprint arXiv:2310.01352_, 2023b.\n' +
      '* Liu et al. (2023) Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long context _ arXiv preprint arXiv:2307.03172_, 2023.\n' +
      '* Longpre 등(2023) Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The final collection: Designing data and methods for effective instruction tuning _ arXiv preprint arXiv:2301.13688_, 2023.\n' +
      '* Mele et al.(2021) Mele, I., Muntean, C. I., Nardini, F. M., Perego, R., Tonellotto, N., and Frieder, O. 대화 검색을 위한 적응형 발화 다시 쓰기 _ 2021년 정보 처리 및 관리_.\n' +
      '* Mishra et al. (2022) Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-task Generalization via natural language crowdsourcing instructions. 2022년 ACL에서\n' +
      '*Mo et al. (2023) Mo, F., Mao, K., Zhu, Y., Wu, Y., Huang, K., and Nie, J.-Y. Convgqr: 대화형 검색을 위한 생성 쿼리 재구성입니다. _ arXiv preprint arXiv:2305.15645_, 2023.\n' +
      '* Muennighoff et al. (2022) Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Scao, T. L., Bari, M. S., Shen, S., Yong, Z. -X., Schoelkopf, H., et al. Crosslingual Generalization through multitask finetuning. _ arXiv preprint arXiv:2211.01786_, 2022.\n' +
      '* Nakamura et al. (2022) Nakamura, K., Levy, S., Tuan, Y. -L., Chen, W., and Wang, W. Y. Hybridialogue: Information-seeking dialogue dataset based on tableular and textual data. 2022년 Acl의 발견에서.\n' +
      '* Nguyen et al. (2016) Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L. 마르코 씨: 인간이 만든 기계 읽기 이해 데이터 세트입니다. _ choice_, 2016.\n' +
      '* OpenAI (2022) OpenAI. 2022년 ChatGPT를 소개합니다.\n' +
      '* OpenAI (2023) OpenAI. GPT-4, 2023\n' +
      '* Ouyang et al.(2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. _ NeurIPS_, 2022.\n' +
      '* Pasupat & Liang (2015) Pasupat, P. and Liang, P. Compositional semantic parsing on semi-structured tables. 2015년 _ACL_에서.\n' +
      '* Qu et al.(2020) Qu, C., Yang, L., Chen, C., Qiu, M., Croft, W. B., and Iyyer, M. 공개 검색 대화식 질문 답변입니다. _SIGIR_, 2020.\n' +
      '* Rajpurkar et al. (2016) Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions for machine comprehension of text. 2016년 \'EMNLP\'에서\n' +
      '* Rajpurkar et al. (2018) Rajpurkar, P., Jia, R., and Liang, P. Know what you don\'t know: Unanswerable questions for squad. 2018년 _ACL_에서.\n' +
      '* Raposo et al. (2022) Raposo, G., Ribeiro, R., Martins, B., and Coheur, L. 질문 다시 쓰는 거? 대화형 질문 답변의 중요성을 평가합니다. 2022년 ECIR에서\n' +
      '\n' +
      '* [Reddy et al.2019] Reddy, S., Chen, D., and Manning, C. D. Coqa: A conversationational question answering challenge. _ TACL_, 2019.\n' +
      '* [Saeidi et al.2018] Saeidi, M., Bartolo, M., Lewis, P., Singh, S., Rocktaschel, T., Sheldon, M., Bouchard, G., and Riedel, S. 회화 기계 판독에서 자연어 규칙의 해석. 2018년 \'EMNLP\'에서요\n' +
      '* [Sanh et al.2022] Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. 2022년 ICLR에서\n' +
      '* [Touvron et al.2023] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _ arXiv preprint arXiv:2307.09288_, 2023.\n' +
      '* [Trischler et al.2017] Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K. Newsqa: 기계 이해 데이터 세트입니다. 2017년 _NLP에 대한 표현 학습에 대한 2차 워크샵 진행_ 에서.\n' +
      '* [Vakulenko et al.2021a] Vakulenko, S., Longpre, S., Tu, Z., and Anantha, R. 대화형 질문 응답을 위한 질문 다시 작성 *WSDM_, 2021a.\n' +
      '* [Vakulenko et al.2021b] Vakulenko, S., Voskarides, N., Tu, Z., and Longpre, S. 대화형 지문 검색을 위한 질문 재작성 방법의 비교 *ECIR_, 2021b.\n' +
      '* [Wang et al.2023a] Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro: Instruction tuning post retrieval-augmented pretraining. _ arXiv preprint arXiv:2310.07713_, 2023a.\n' +
      '* [Wang et al.2022a] Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. Text embeddings by weakly-supervised contrastive pre-training. _ arXiv preprint arXiv:2212.03533_, 2022a.\n' +
      '* [Wang et al.2022b] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions _ arXiv preprint arXiv:2212.10560_, 2022b.\n' +
      '* [Wang et al.2022c] Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S., Arunkumar, A., Stap, D., et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. *EMNLP_, 2022c.\n' +
      '* [Wang et al.2023b] Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K. R., Wadden, D., MacMillan, K., Smith, N. A., Beltagy, I., et al. 얼마나 멀리 낙타가 갈 수 있을까? 오픈 리소스에서 명령어 튜닝 상태를 탐색합니다. _ arXiv preprint arXiv:2306.04751_, 2023b.\n' +
      '* [Wei et al.2022a] Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. 2022a, _ICLR_에서.\n' +
      '* [Wei et al.2022b] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits in large language models. _ NeurIPS_, 2022b.\n' +
      '* [Wu et al.2022] Wu, Z., Luan, Y., Rashkin, H., Reitter, D., Hajishirzi, H., Ostendorf, M., and Tomar, G. S. Conqrr: Conversational query rewriting for retrieval with reinforcement learning. 2022년 \'EMNLP\'에서\n' +
      '* [Wu et al.2023] Wu, Z., Parish, R., Cheng, H., Min, S., Ammanabrolu, P., Ostendorf, M., and Hajishirzi, H. Inscit: Information-seeking conversation with mixed-initiative interactions _ TACL_, 2023.\n' +
      '* [Xu et al.2023a] Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions _ arXiv preprint arXiv:2304.12244_, 2023a.\n' +
      '* [Xu et al.2023b] Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturin, E., Shoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. _ arXiv preprint arXiv:2310.03025_, 2023b.\n' +
      '* [Ye et al.2023] Ye, F., Fang, M., Li, S., and Yilmaz, E. Enhancing conversationational search: Large language model-aided informative query rewriting. 《EMNLP》, pp. 5985-6006, 2023.\n' +
      '* [Yu et al.2020] Yu, S., Liu, J., Yang, J., Xiong, C., Bennett, P., Gao, J., and Liu, Z. 생성 대화 쿼리 다시 쓰기는 거의 없습니다. _SIGIR_, 2020.\n' +
      '* [Zhang et al.2023] Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et al. Instruction tuning for large language models: A survey. _ arXiv preprint arXiv:2308.10792_, 2023.\n' +
      '* [Zhou et al.2023] Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment _ arXiv preprint arXiv:2305.11206_, 2023.\n' +
      '* [Zhu et al.2021] Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., and Chua, T. - S. Tat-qa: 금융에서 표와 텍스트 콘텐츠의 혼합에 대한 질의 응답 벤치마크. 2021년 _ACL_ 에서요.\n' +
      '\n' +
      'ChatQA 명령어 튜닝\n' +
      '\n' +
      '### Stage-1: 감독 미세 조정\n' +
      '\n' +
      '스테이지-1에서 LLM 입력의 포맷 템플릿은 다음과 같다:\n' +
      '\n' +
      '시스템: 이것은 사용자와 인공 지능 어시스턴트 사이의 채팅이다. 어시스턴트는 컨텍스트에 기초하여 사용자의 질문에 대해 도움이 되고, 상세하며 예의 바른 답변을 제공한다. 어시스턴트는 또한 문맥에서 답변을 찾을 수 없는 때를 표시해야 한다.\n' +
      '\n' +
      '사용자: {질문 1} 어시스턴트: {답변 1} 사용자: {최신 질문} 어시스턴트: 모델 출력에 대한 감독으로 어시스턴트의 {최신 답변}을 사용합니다.\n' +
      '\n' +
      '### Stage-2: Context Enhanced Instruction Tuning\n' +
      '\n' +
      '스테이지-1 포맷 템플릿에 기초하여, 스테이지-2에서의 LLM 입력들은 아래에서 묘사되는 바와 같이, 사용자로부터 {Context for Latest Question} 및 {Instruction} 을 추가한다:\n' +
      '\n' +
      '시스템: 이것은 사용자와 인공 지능 어시스턴트 사이의 채팅이다. 어시스턴트는 컨텍스트에 기초하여 사용자의 질문에 대해 도움이 되고, 상세하며 예의 바른 답변을 제공한다. 어시스턴트는 또한 문맥에서 답변을 찾을 수 없는 때를 표시해야 한다.\n' +
      '\n' +
      '{최근 질문의 컨텍스트} 사용자: {명령} + {질문 1} 보조자: {답변 1} 사용자: {최신 질문} 어시스턴트: Stage-1과 동일하게, 모델 출력에 대한 감독으로 어시스턴트의 {최신 답변}을 사용한다.\n' +
      '\n' +
      '{명령어]에 대해서는 답변 유형에 따라 서로 다른 데이터 세트에 대해 서로 다른 지침을 사용합니다. 자세한 내용은 아래에서 확인할 수 있습니다.\n' +
      '\n' +
      '"질문에 대해 완전하고 완전한 답변을 주세요." 이것은 긴 답변을 가진 데이터 세트에 대한 것입니다. 우리는 HumanAnnotatedConvQA 또는 SyntheticConvQA에 사용합니다.\n' +
      '\n' +
      '"다음 질문에 짧은 기간으로 답하세요. 답은 몇 단어로만 답하면 됩니다." 이것은 짧은 답변이 있는 데이터 세트에 대한 것입니다. 우리는 그것을 SQuAD1.1, SQuAD2.0, NarrativeQA, DROP, ROPES, NewsQA, Quoref에 사용한다.\n' +
      '\n' +
      '"다음 질문에 문맥의 숫자로 대답하거나 +, -, * 또는 /를 사용하여 수학 연산을 합니다." 이는 산술 계산을 사용하거나 문맥에서 숫자를 추출하는 데이터 세트에 대한 것입니다. 우리는 문제가 산술 계산이나 문맥에서 숫자를 추출해야 하는 TAT-QA 데이터 세트에 사용한다.\n' +
      '\n' +
      '"다음 질문에 짧은 기간 또는 완전하고 완전한 답변으로 답변하세요." 이것은 짧은 답변과 긴 답변을 모두 가진 데이터 세트에 대한 것입니다. 모델은 질문을 기반으로 단답형 또는 장답형 답변을 생성할지 여부를 알려야 한다. 우리는 해당 답변의 길이가 짧고 길 수 있기 때문에 산술 계산이 필요하지 않은 TAT-QA 데이터 세트에 사용한다.\n' +
      '\n' +
      '스테이지-2 명령어 튜닝에 사용되는 스테이지-1 SFT 데이터의 경우, 스테이지-1과 동일한 포맷 템플릿을 유지한다. 즉, {Context for latest Question} 및 {Instruction}은 비어 있다.\n' +
      '\n' +
      '### 합성 데이터 생성 프롬프트\n' +
      '\n' +
      '**고품질 멀티턴 QA 대화를 얻으려면 GPT-3.5-터보에서 2단계 프롬프트를 사용합니다.* * 1단계에서 GPT-3.5-터보에서 지정된 문서를 기반으로 멀티턴 QA 대화를 생성하도록 프롬프트합니다. 그러나, 생성된 대화는 대화사에서 단어 또는 구를 다시 참조할 대명사나 보통 명사가 부족하다는 것을 발견했다. 따라서 2단계에서는 대명사나 보통명사를 추가하여 생성된 대화를 수정하도록 GPT-3.5-turbo를 프롬프트한다. 두 단계 모두에 대해 고품질 생성을 보장하기 위해 GPT-3.5-터보 소수의 샷 예를 제공한다.\n' +
      '\n' +
      '상기 단계-1에서의 인-컨텍스트 소-샷 프롬프트는 다음과 같다(멀티-턴 QA 대화를 생성하기 위한aims):\n' +
      '\n' +
      '당신은 도움이 되는 패턴을 따르는 조수입니다.\n' +
      '\n' +
      'role-user:\n' +
      '\n' +
      'PersonA는 다음 구절의 정보에 액세스할 수 있습니다.\n' +
      '\n' +
      '##대학 준비에는 몇 가지 중요한 측면이 있습니다. 대학을 준비하는 학생은 자신의 목표와 관심사를 정의해야 한다. 학생은 재정적으로나 학문적으로 계획을 세워야 합니다. 학생들이 재정적, 학업적으로 준비하는 데 도움이 되는 체크리스트가 있습니다. ##\n' +
      '\n' +
      'PersonalU가 패시지에 있는 PersonA로부터 정보를 필요로 하는 PersonA와 PersonU 사이의 약 4번의 회전에 대한 대화를 생성한다. 통로에 있는 정보만 사용합니다.\n' +
      '\n' +
      '페르소누는 대화를 시작하고 질문만 한다. PersonU의 질문은 대부분 기출문제의 후속 또는 연속이다.\n' +
      '\n' +
      'Conversation:\n' +
      '\n' +
      'role-assistant:\n' +
      '\n' +
      'PersonU: 나는 대학을 준비하는 방법을 알아야 한다.\n' +
      '\n' +
      '개인A: 당신은 먼저 당신의 목표와 관심을 정의하고 비용을 확실히 알아야 하며 대학을 위해 재정적으로 그리고 학문적으로 계획하는 방법을 알아야 한다.\n' +
      '\n' +
      '개인: 제가 그것을 준비하기 위한 가이드로 사용할 수 있는 것이 있나요?\n' +
      '\n' +
      '개인 A: 재정적으로나 학문적으로 준비하기 위해 우리의 체크리스트를 사용할 수 있습니다.\n' +
      '\n' +
      '개인: 다른 팁은 없나요?\n' +
      '\n' +
      '개인A: 대학이나 진로 학교에 다니는 이유를 반드시 확인해라.\n' +
      '\n' +
      'PersonU: 좋아요. 팁 좀 더 주실 수 있나요?\n' +
      '\n' +
      'PersonA: 고려해야 할 또 다른 것은 당신의 직업 선택권을 탐구하는 것이다.\n' +
      '\n' +
      'role-user:\n' +
      '\n' +
      'PersonA는 다음 구절의 정보에 액세스할 수 있습니다.\n' +
      '\n' +
      '##몇 가지 장애 보상 혜택이 있습니다. 혜택 중 하나는 연간 의류 수당입니다. 이 허용량은 연간 200달러의 한도까지 새 옷을 구매하는 비용을 포함합니다. ##\n' +
      '\n' +
      'PersonalU가 패시지에 있는 PersonA로부터 정보를 필요로 하는 PersonA와 PersonU 사이의 약 두 번의 회전에 대한 대화를 생성한다. 통로에 있는 정보만 사용합니다.\n' +
      '\n' +
      '페르소누는 대화를 시작하고 질문만 한다. 대부분의 PersonU의 질문은 이전 질문의 후속 또는 연속이다. 대화:\n' +
      '\n' +
      'role-assistant:\n' +
      '\n' +
      'PersonU: 저는 의류에 대한 장애 혜택에 대해 들었습니다. 무슨 내용인데요?\n' +
      '\n' +
      '개인A: 이것은 연간 의류 수당으로 알려진 장애 보상 혜택이다.\n' +
      '\n' +
      'PersonU: 나는 그러한 혜택에 꽤 관심이 있다.\n' +
      '\n' +
      'PersonA: 당신이 자격이 있는지 알고 싶습니까?\n' +
      '\n' +
      'role-user:\n' +
      '\n' +
      'PersonA는 다음 구절의 정보에 액세스할 수 있습니다.\n' +
      '\n' +
      '##바세파(이코사펜트 에틸)는 500 mg/dL 이상의 트리글리세리드 수준으로 정의되는 극도로 높은 트리글리세리드를 가진 성인 치료를 위해 승인된 처방 어유 약물이다. 이 약을 사용하는 동안 진료실에서 빈번한 혈액 검사가 필요할 수 있습니다. 바세파는 식단, 운동 및 체중 조절도 포함할 수 있는 치료 프로그램의 일부일 뿐이다. 의사의 지시에 매우 주의 깊게 따르세요. 아이코서펜트에 알레르기가 있다면 바세파를 사용해서는 안 된다. 확신해\n' +
      '\n' +
      '바세파는 당신에게 안전합니다: 1개의 간 질환, 2개의 당뇨병, 3개의 갑상선 질환, 4개의 췌장 문제, 5개의 출혈 또는 혈액 응고 장애, 6개의 생선이나 조개류에 알레르기가 있는 경우, 또는 7개의 많은 양의 술을 마시는 경우, 바세파(icosapent)는 간과 혈류에서 매우 저밀도 중성지방을 감소시킵니다. ##\n' +
      '\n' +
      'PersonA와 PersonU 사이의 약 3회전 대화를 생성합니다.\n' +
      '\n' +
      'PersonU는 지문에 있는 PersonA의 정보가 필요합니다. 정보만 사용\n' +
      '\n' +
      '통로에서 발견되었다.\n' +
      '\n' +
      '페르소누는 대화를 시작하고 질문만 한다. PersonU의 질문은 대부분 기출문제의 후속 또는 연속이다. 대화:\n' +
      '\n' +
      'role-assistant:\n' +
      '\n' +
      '페르난두: 바세파라는 약에 대해 들었어요. 무엇에 사용됩니까?\n' +
      '\n' +
      'PersonA: 바세파는 매우 높은 중성지방 수치를 가진 성인들을 치료하는데 사용되는 처방 어유 약물이다.\n' +
      '\n' +
      '개인: 제가 주의해야 할 부작용이나 주의사항이 있나요?\n' +
      '\n' +
      'PersonA: 바세파를 사용하는 동안, 여러분은 빈번한 혈액 검사가 필요할 수 있고, 의사의 지시를 잘 따르는 것이 중요합니다. 알레르기나 기존 상태에 대해 의사와 상담하는 것이 중요합니다.\n' +
      '\n' +
      '개인: 콜레스테롤 수치를 낮추는 데 효과가 있나요?\n' +
      '\n' +
      'PersonA: 바세파는 심각한 고콜레스테롤을 가진 사람들의 총 콜레스테롤을 낮추기 위해 저지방 식단과 다른 치료법과 함께 사용된다.\n' +
      '\n' +
      'role-user:\n' +
      '\n' +
      'PersonA는 다음 구절의 정보에 액세스할 수 있습니다.\n' +
      '\n' +
      '{Document}\n' +
      '\n' +
      'PersonA와 PersonU 사이의 약 3회전 대화를 생성합니다.\n' +
      '\n' +
      'PersonU는 지문에 있는 PersonA의 정보가 필요합니다. 정보만 사용\n' +
      '\n' +
      '통로에서 발견되었다.\n' +
      '\n' +
      '페르소누는 대화를 시작하고 질문만 한다. PersonU의 질문은 대부분 기출문제의 후속 또는 연속이다.\n' +
      '\n' +
      'Conversation:role-assistant:\n' +
      '\n' +
      '상기 단계-2에서 인컨텍스트 소샷 프롬프트는 다음과 같다(대명사와 보통명사를 추가하여 멀티턴 QA 대화를 수정하고자 함):\n' +
      '\n' +
      '당신은 도움이 되는 패턴을 따르는 조수입니다.\n' +
      '\n' +
      'role-user:\n' +
      '\n' +
      'PersonU와 PersonA 사이의 다음과 같은 대화를 감안할 때:\n' +
      '\n' +
      '###PersonU: 멕시코 요리의 아도보 소스에 대해 들어본 적이 있습니다. 추억에 대해 말해줄 수 있어?\n' +
      '\n' +
      '사람A: 아도보 소스는 갈은 고추, 허브, 식초로 만든 진한 붉은 소스입니다. 멕시코와 남서부 요리에서 양념장과 요리 소스로 사용됩니다.\n' +
      '\n' +
      '페르소누: 아도보 소스의 주요 재료는 무엇인가요?\n' +
      '\n' +
      'PersonA: 주요 재료는 일반적으로 고추, 마늘, 식초입니다. 그러나 레시피에는 변형이 있을 수 있습니다.\n' +
      '\n' +
      '개인: 집에서 어떻게 아도보 소스를 만들 수 있을까요?\n' +
      '\n' +
      '사람A: 집에서 아도보 소스를 만드는 한 가지 방법은 치폴레라고 불리는 훈제 할라피뇨 고추를 사용하는 것입니다. 토마토, 마늘, 식초, 소금, 향신료와 함께 소스에 치폴레를 끓일 수 있습니다. 많은 식료품점의 히스패닉 음식 코너에서 통조림 아도보 소스를 찾는 것도 가능합니다.\n' +
      '\n' +
      'PersonU: 아도보 소스의 다른 용도가 있나요?\n' +
      '\n' +
      '사람A: 아도보 소스는 헌신과 다양한 요리의 양념장으로 사용될 수 있다. 고기의 풍미를 높이고 요리 및 소스 서빙으로 사용할 수 있습니다. 또한, 아도보는 이 소스에 준비된 고기의 스튜 같은 프레젠테이션에도 사용됩니다.\n' +
      '\n' +
      '개인: 아도보 소스는 보존하기 쉬운가요?\n' +
      '\n' +
      'PersonA: 네, 아도보 소스를 다 만들면 얼리거나 통조림으로 보존할 수 있습니다. 이를 통해 최대 1년 동안 보관하고 원할 때마다 맛을 즐길 수 있습니다. ##\n' +
      '\n' +
      'PersonU의 질문을 수정하여 대명사 또는 보통 명사를 사용하여 PersonU의 이전 질문 또는 PersonA의 이전 응답에서 단어 또는 구를 다시 참조합니다.\n' +
      '\n' +
      'role-assistant:\n' +
      '\n' +
      '페르소나: 멕시코 요리의 아도보 소스에 대해 들어본 적이 있어요. 그것에 대한 더 많은 정보를 제공할 수 있습니까?\n' +
      '\n' +
      '사람A: 아도보 소스는 갈은 고추, 허브, 식초로 만든 진한 붉은 소스입니다. 멕시코와 남서부 요리에서 양념장과 요리 소스로 사용됩니다. 개인: 소스의 주요 재료는 무엇인가요?\n' +
      '\n' +
      'PersonA: 주요 재료는 일반적으로 고추, 마늘, 식초입니다. 그러나 레시피에는 변형이 있을 수 있습니다.\n' +
      '\n' +
      '개인: 집에서 소스를 어떻게 만들 수 있나요?\n' +
      '\n' +
      '사람A: 집에서 만드는 한 가지 방법은 치폴레라고 불리는 훈제 할라피뇨 고추를 사용하는 것이다. 토마토, 마늘, 식초, 소금, 향신료와 함께 소스에 찌면 됩니다. 많은 식료품점의 히스패닉 음식 코너에서 통조림 아도보 소스를 찾는 것도 가능합니다.\n' +
      '\n' +
      'PersonU: 다른 용도가 있나요?\n' +
      '\n' +
      'PersonA: 조미료로 사용하거나 다양한 요리의 양념장으로 사용할 수 있습니다. 고기의 풍미를 높이고 요리 및 소스 서빙으로 사용할 수 있습니다. 또한 이 소스에 준비된 고기의 스튜 같은 프레젠테이션에도 사용됩니다.\n' +
      '\n' +
      'PersonU: 보존하기 쉬운가요?\n' +
      '\n' +
      'PersonA: 네, 다 만들었을 때 얼리거나 통조림으로 보존할 수 있습니다. 이를 통해 최대 1년 동안 보관하고 원할 때마다 맛을 즐길 수 있습니다.\n' +
      '\n' +
      'role-user:\n' +
      '\n' +
      'PersonU와 PersonA 사이의 다음과 같은 대화를 감안할 때:\n' +
      '\n' +
      '***PersonU: 소변 검사라는 검사에 대해 들었어요. 무엇에 사용됩니까?\n' +
      '\n' +
      'PersonA: 소변검사는 당신의 소변 샘플을 평가하는 검사이다. 요로 감염, 신장 질환, 당뇨병과 같은 광범위한 장애를 감지하고 평가하는 데 사용된다. 소변의 모양, 농도, 함유량 등을 검사한다.\n' +
      '\n' +
      '개인: 방광암 진단에 도움이 될까요?\n' +
      '\n' +
      'PersonA: 네, 소변검사는 방광암의 선별 및 진단 도구로 사용될 수 있습니다. 방광암을 검사하는 한 가지 방법은 소변에서 혈뇨를 확인하는 것인데, 이를 혈뇨라고 한다. 이것은 소변 검사 중에 특수 현미경으로 소변 세포를 검사하여 수행할 수 있다.\n' +
      '\n' +
      'PersonU: 소변 검사를 통해 어떤 다른 질환이나 질병을 발견할 수 있나요?\n' +
      '\n' +
      '개인A: 소변 검사는 일반적으로 요로 또는 신장 감염을 진단하고, 신부전의 원인을 평가하며, 당뇨병 및 고혈압과 같은 만성 질환의 진행을 검사하고, 특정 질병을 진단하기 위해 다른 검사와 함께 사용할 수 있다. 요로계의 이상을 감지하는 데 도움이 되는 다용도 검사입니다.\n' +
      '\n' +
      'PersonU: 소변검사를 어디서 받을 수 있나요?\n' +
      '\n' +
      'PersonA: 소변 검사는 의사 사무실, 긴급 진료 시설, 실험실 및 병원을 포함한 다양한 의료 환경에서 수행할 수 있다. 표본 컵에 소변 샘플을 제공할 수 있으며 샘플은 현장에서 분석하거나 실험실로 보내 테스트할 수 있다.\n' +
      '\n' +
      'PersonU: 소변 검사에 일반적으로 얼마나 많은 소변이 필요한가요?\n' +
      '\n' +
      '개인A: 보통 소변 검사 검사에는 30~60ml 범위의 소량의 소변만 필요하다. 건강에 대한 귀중한 정보를 제공하는 비침습적이고 비교적 간단한 검사입니다.***\n' +
      '\n' +
      'PersonU의 질문을 수정하여 대명사 또는 보통 명사를 사용하여 PersonU의 이전 질문 또는 PersonA의 이전 응답에서 단어 또는 구를 다시 참조합니다.\n' +
      '\n' +
      'role-assistant:\n' +
      '\n' +
      'PersonU: 소변검사라고 불리는 검사에 대해 들어본 적이 있습니다. 무엇에 사용됩니까?\n' +
      '\n' +
      'PersonA: 소변검사는 당신의 소변 샘플을 평가하는 검사이다. 요로 감염, 신장 질환 및 당뇨병과 같은 광범위한 장애를 감지하고 평가하는 데 사용됩니다. 소변의 모양, 농도, 함유량 등을 검사한다.\n' +
      '\n' +
      '개인: 방광암 진단에 도움이 될까요?\n' +
      '\n' +
      'PersonA: 네, 소변검사는 방광암의 선별 및 진단 도구로 사용될 수 있습니다. 방광암을 검사하는 한 가지 방법은 소변에서 혈뇨를 확인하는 것인데, 이를 혈뇨라고 한다. 이것은 소변 검사 중에 특수 현미경으로 소변 세포를 검사하여 수행할 수 있다.\n' +
      '\n' +
      'PersonU: 이 검사를 통해 어떤 다른 질환이나 질병을 발견할 수 있을까요?\n' +
      '\n' +
      '개인A: 소변 검사는 일반적으로 요로 또는 신장 감염을 진단하고, 신부전의 원인을 평가하며, 당뇨병 및 고혈압과 같은 만성 질환의 진행을 검사하고, 특정 질병을 진단하기 위해 다른 검사와 함께 사용할 수 있다. 요로계의 이상을 감지하는 데 도움이 되는 다용도 검사입니다.\n' +
      '\n' +
      '페르소나: 이 시험을 보려면 어디로 가야 하나요?\n' +
      '\n' +
      'PersonA: 소변 검사는 의사 사무실, 긴급 진료 시설, 실험실 및 병원을 포함한 다양한 의료 환경에서 수행할 수 있다. 표본 컵에 소변 샘플을 제공할 수 있으며 샘플은 현장에서 분석하거나 실험실로 보내 검사를 받을 수 있다. PersonU: 검사에 일반적으로 얼마나 많은 소변이 필요한가요? 개인A: 보통 소변 검사 검사에는 30~60ml 범위의 소량의 소변만 필요하다. 비침습적이고 비교적 간단한 검사로 건강에 대한 귀중한 정보를 제공합니다.\n' +
      '\n' +
      'role-user: PersonU와 PersonA 사이의 다음과 같은 대화가 주어지면:\n' +
      '\n' +
      '멀티턴 QA 대화{multi-turn QA conversation}\n' +
      '\n' +
      'PersonU의 질문을 수정하여 대명사 또는 보통 명사를 사용하여 PersonU의 이전 질문 또는 PersonA의 이전 응답에서 단어 또는 구를 다시 참조합니다.\n' +
      '\n' +
      'role-assistant:\n' +
      '\n' +
      '## 부록 B 대화식 QA에서 검색을 위한 자세한 내용 및 결과\n' +
      '\n' +
      '### GPT-3.5-turbo에 대한 쿼리 다시 쓰기 프롬프트\n' +
      '\n' +
      'GPT-3.5-터보에 사용하는 인컨텍스트 소샷 쿼리 다시 쓰기 프롬프트는 다음과 같습니다.\n' +
      '\n' +
      '당신은 도움이 되는 패턴을 따르는 조수입니다.\n' +
      '\n' +
      '역할 사용자: PersonU와 PersonA 간의 다음과 같은 대화를 감안할 때 PersonU: 안녕하세요, 저는 어떤 결정에도 동의하지 않으면 무엇을 해야 하는지 알고 싶습니다. 개인 A: 당신의 월별 소득 조정액에 대한 우리의 결정에 동의하지 않습니까? 개인: 아니요. 제 선박 연락처를 어디서 찾을 수 있나요? PersonA: 온라인에서 메디케어 & 유 2020 핸드북 뒷면에서 지역 SHIP 연락처를 찾을 수 있습니다. 퍼스널U: 그리고 조정은 어떻게 계산하나요? 이 모든 대화를 하는 대신, PersonU는 어떻게 한 가지 질문을 사용하여 원하는 것을 얻을 수 있을까요? 그 질문에 대답해\n' +
      '\n' +
      '역할 보조자: SHIP에 의해 이루어진 조정에 대한 계산은 어떻게 결정되나요?\n' +
      '\n' +
      '역할 사용자: PersonU와 PersonA 간의 다음과 같은 대화를 고려할 때 PersonU: 나는 대학을 준비하는 방법을 알아야 한다. 개인A: 당신은 먼저 당신의 목표와 관심을 정의하고 비용을 확실히 알아야 하며 대학을 위해 재정적으로 그리고 학문적으로 계획하는 방법을 알아야 한다. 개인: 제가 그것을 준비하기 위한 가이드로 사용할 수 있는 것이 있나요? 이 모든 대화를 하는 대신, PersonU는 어떻게 한 가지 질문을 사용하여 원하는 것을 얻을 수 있을까요? 그 질문에 대답해\n' +
      '\n' +
      '역할 보조: 대학 준비를 돕기 위해 어떤 자원이나 가이드를 사용할 수 있나요?\n' +
      '\n' +
      'role-user: PersonU와 PersonA 사이의 다음과 같은 대화가 주어지면:\n' +
      '[대화 히스토리 + 최신 질문]\n' +
      '\n' +
      '이 모든 대화를 하는 대신, PersonU는 어떻게 한 가지 질문을 사용하여 원하는 것을 얻을 수 있을까요? 그 질문에 대답해\n' +
      '\n' +
      'role-assistant:\n' +
      '\n' +
      '### 대화식 QA 검색을 위한 추가 결과\n' +
      '\n' +
      '표 9에서 미세 조정과 다시 쓰기 방법 간의 포괄적인 비교를 보여준다.\n' +
      '\n' +
      '흥미롭게도, 재작성된 질의만을 입력으로 사용하는 것과 비교하여(Dragon + Rewrite (w/single query only)), 추가 대화 이력(Dragon + Rewrite (w/ dialog history))을 부여하면 평균 점수가 크게 떨어짐을 알 수 있다. 드래곤은 원래 싱글턴 질의에 대해 사전 훈련되기 때문에 멀티턴 대화 대신 싱글턴 재작성 질의가 제공될 때 자연스럽게 더 나은 일반화 능력을 가질 것이기 때문이다. 상기 재작성된 쿼리는 상기 다이얼로그 이력으로부터 이미 충분한 정보를 포함하는 것을 특징으로 하는 방법.\n' +
      '\n' +
      '또한 "Dragon + Fine-tune"이 "Dragon + Fine-tune + Rewrite"와 동등하게 수행됨을 관찰한다. 즉, 다중 회전 미세 조정 방법의 경우 원래 쿼리를 입력으로 재작성된 쿼리로 대체하면 비슷한 결과를 얻을 수 있다. 이는 대화 이력이 이미 제공되었기 때문에 재작성된 쿼리가 모델에 대한 많은 추가 정보를 제공하지 않을 것이며, 전체 대화 입력을 자연스럽지 않게 만들기 때문에 부정적인 영향(예: TopiOCQA 데이터 세트에 대한 결과)을 유발할 수도 있기 때문이다. 이는 다시 한번 미세 조정 방법이 멀티 턴 컨텍스트를 이해할 수 있는 능력을 모델에 갖추는 데 얼마나 효과적인지를 보여준다.\n' +
      '\n' +
      '## 부록 C 대화식 QA 벤치마크\n' +
      '\n' +
      '### Data Statistics\n' +
      '\n' +
      'Doc2DialWe는 평가를 위해 Doc2Dial의 테스트 세트를 사용한다. 3939 사용자-에이전트 회전이 있는 719개의 대화 상자로 구성됩니다.\n' +
      '\n' +
      'QuAC는 테스트 세트를 직접 얻을 수 없기 때문에 평가에 QuAC의 검증 세트를 사용한다. 유효성 검사 세트는 7354개의 사용자 에이전트 회전이 있는 1000개의 대화 상자로 구성됩니다. 이 7354개의 사용자-에이전트 회전 중 1486개(약 20.2%)의 대답할 수 없는 질문이 있다. 답변할 수 있는 질문과 대답할 수 없는 질문의 결합 평가를 위해 응답할 수 없는 질문에 대한 진실 응답을 "죄송합니다. 문맥에 따라 답변을 찾을 수 없습니다."로 설정했으며, 이는 2단계 튜닝에서 설정한 것과 같다. 공정한 비교를 위해 기준 모델(즉, Llama2-SFT/Chat, GPT-3.5-turbo, GPT-4)에서 응답할 수 없는 응답을 동일한 문장(자세한 내용은 부록 E에서 찾을 수 있음)으로 대체한다.\n' +
      '\n' +
      'QReCC는 QReCC의 테스트 세트를 평가에 사용한다. 테스트 세트에는 QuAC 데이터 세트의 일부 대화 샘플 소스가 포함되어 있습니다. 다른 벤치마크 데이터 세트와의 중복을 방지하기 위해 QuAC 소스 샘플을 제거하여 2805개의 사용자 에이전트 회전을 생성했다.\n' +
      '\n' +
      'TopicOCA 테스트 세트를 아직 사용할 수 없기 때문에 TopiOCQA의 유효성 검사 세트를 사용합니다. 검증 세트는 2514개의 사용자-에이전트 회전이 있는 205개의 대화 상자로 구성된다. 각 질문에는 총 4개의 인간이 작성한 답변이 있습니다. 답변할 수 없는 경우가 있습니다. 그러나 응답할 수 없는 사례 평가에는 이 데이터 세트를 포함하지 않습니다. 왜냐하면 없는 46개의 질문만 찾기 때문입니다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l c c c c c c c c c c c} \\hline \\hline \\multirow{2}{*}{Models} & \\multicolumn{2}{c}{Average} & \\multicolumn{2}{c}{Doc2Dial} & \\multicolumn{2}{c}{QuAC} & \\multicolumn{2}{c}{QReCC} & \\multicolumn{2}{c}{TopiOCQA} & \\multicolumn{2}{c}{INSCIT} \\\\ \\cline{2-13}  & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-1 & top-5 & top-5* & top-20* & top-5* & top-20* \\\\ \\hline Dragon (w/ dialog history) & 46.29 & 73.09 & 43.33 & 75.61 & 56.8 & 82.86 & 46.17 & 81.96 & 57.68 & 78.80 & 27.49 & 46.22 \\\\ Dragon + Rewrite (w/ dialog history) & 47.57 & 74.12 & 44.54 & 76.98 & 57.23 & 83.04 & 46.45 & 82.60 & 60.94 & 81.74 & 28.69 & 46.22 \\\\ Dragon + Rewrite (w/ single query only) & **54.46** & **80.13** & 47.60 & 80.60 & 47.10 & 77.15 & 51.73 & 85.78 & 73.07 & 88.19 & 52.79 & 68.92 \\\\ \\hline Dragon + Fine-tune (w/ dialog history) & 52.72 & 80.67 & 48.94 & 83.01 & 52.64 & 81.95 & 50.73 & 87.17 & 67.86 & 86.28 & 43.43 & 64.94 \\\\ Dragon + Fine-tune + Rewrite (w/ dialog history) & **53.17** & **80.84** & 49.30 & 84.64 & 55.04 & 83.23 & 51.23 & 87.99 & 60.50 & 81.03 & 49.80 & 67.33 \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\end{table}\n' +
      '표 9: 5개의 데이터 세트에 걸친 종합적인 멀티턴 검색 결과.\n' +
      '\n' +
      '응답이 발견되었으며, 이는 응답할 수 없는 사례 평가에 충분하지 않다. 이 데이터 세트는 전체 위키피디아를 검색 코퍼스로 사용한다는 점을 감안할 때, 일반적으로 특정 주제 또는 도메인으로 좁혀지는 문서보다 대화 QA에 더 중점을 두었기 때문에 다중 전환 검색을 위해 주제 정보를 활용한다.\n' +
      '\n' +
      'Inscit우리는 TopiOCQA의 테스트 세트를 아직 사용할 수 없기 때문에 검증 세트를 사용한다. 그것의 검증 세트는 502개의 사용자-에이전트 턴을 갖는 86개의 대화로 구성된다. 각 질문에는 평균 1.9개의 인간이 작성한 답변이 있습니다. 몇 가지 질문에 대해 대답할 수 없는 응답이 있습니다. 그러나 모든 질문에는 항상 하나 이상의 응답 가능한 응답이 있으므로 이 데이터 세트는 응답할 수 없는 경우 평가에 포함하지 않습니다. INSCIT는 또한 전체 위키피디아를 검색 코퍼스로 사용한다. 따라서 TopiOCQA에서 언급한 것과 동일한 이유로 멀티턴 검색을 위해 토픽 정보를 활용한다.\n' +
      '\n' +
      'CoQA 테스트 세트는 직접 얻을 수 없기 때문에 CoQA의 검증 세트를 사용한다. 검증 세트는 7983개의 사용자-에이전트 회전과 함께 500개의 대화로 구성된다. 각 질문에는 총 4개의 인간이 작성한 답변이 있습니다. 답변할 수 없는 경우가 있습니다. 그러나 TopiOCQA 데이터 세트에서와 동일한 이유로 이 데이터 세트를 응답할 수 없는 사례 평가에 포함하지 않는다. 우리는 답을 찾을 수 없는 13개의 질문만 찾는데, 이는 답할 수 없는 사례 평가에 충분하지 않다.\n' +
      '\n' +
      'DoQA 평가를 위해 DoQA의 테스트 세트를 사용한다. 테스트 세트는 요리, 여행 및 영화 도메인에서 5394개의 사용자 에이전트가 회전하는 1200개의 대화로 구성됩니다. 5394개의 사용자-에이전트 회전 중 1479개(약 27.4%)의 응답할 수 없는 질문이 있다. QuAC 데이터 세트에서 언급한 것과 동일한 전략을 사용하여 응답할 수 없는 샘플을 평가에 통합한다.\n' +
      '\n' +
      'ConvFinQA 테스트 세트는 직접 얻을 수 없기 때문에 CoQA의 유효성 검사 세트를 사용한다. 검증 세트는 1490개의 사용자-에이전트 회전과 함께 421개의 대화로 구성된다.\n' +
      '\n' +
      'SqaWe는 평가에 SQA의 테스트 세트를 사용한다. 테스트 세트는 3100개의 사용자 에이전트 회전과 함께 1025개의 대화로 구성됩니다.\n' +
      '\n' +
      'HybridDialWe는 HybridDial의 테스트 세트를 평가에 사용한다. 테스트 세트는 1111개의 사용자 에이전트 회전과 함께 243개의 대화로 구성됩니다.\n' +
      '\n' +
      '### Benchmarks에 대한 프롬프트\n' +
      '\n' +
      'ChatQA, Llama2-Chat, GPT-3.5-turbo 및 GPT-4의 벤치마크에 대해 SSA.2에서 언급한 프롬프트 형식을 사용합니다. 모든 모델에 대해 {Context for Latest Question}을 동일하게 유지하면서 기준선(예: Llama2-Chat, GPT-3.5-turbo 및 GPT-4)에 대해 {Instruction}을 조정하여 차선 결과를 얻지 않도록 합니다. 우리는 다른 답변 유형(예: 긴 답변, 짧은 답변, 산술 계산)을 가진 테스트 벤치마크에 대해 다른 지침을 사용한다. 벤치마크 상의 모든 모델에 대한 {Instruction}은 다음과 같다:\n' +
      '\n' +
      '#### c.2.1 ChatQA\n' +
      '\n' +
      '우리는 단계-2의 데이터 혼합과 다른 답변 유형을 기반으로 하는 테스트 벤치마크 간에 {명령}을 일관되게 유지한다.\n' +
      '\n' +
      '우리는 DoQA, INSCIT, HybridDial, Doc2Dial, QuAC 및 QReCC에 대해 "질문에 대해 완전하고 완전한 답변을 주세요."를 사용합니다. 이러한 데이터 세트는 일반적으로 질문에 대한 답변이 길기 때문입니다.\n' +
      '\n' +
      'SQA 및 TopiOCQA에 대해 "짧은 범위 또는 완전하고 완전한 답변으로 다음 질문에 답변"을 사용합니다. 이러한 데이터 세트는 질문에 따라 짧은 답변과 긴 답변을 모두 가지고 있기 때문입니다.\n' +
      '\n' +
      '우리는 CoQA의 경우 일반적으로 질문에 대한 답변이 짧기 때문에 "짧은 범위로 다음 질문에 답하세요. 대답은 몇 단어로 되어 있어야 합니다."를 사용합니다.\n' +
      '\n' +
      'ConvFinQA에 대해 "다음 질문에 컨텍스트의 숫자로 대답하거나 +, -, * 또는 /를 사용하여 수학 산술"을 사용합니다. 이 데이터 세트는 모델이 컨텍스트에서 숫자를 추출하거나 산술 계산을 수행해야 하기 때문입니다. 우리는 모델이 생성하는 산술 공식을 바탕으로 수를 계산하고 금답과 비교할 것이다.\n' +
      '\n' +
      '#### c.2.2. Llama2-Chat\n' +
      '\n' +
      '우리는 원래 Llama2-Chat 프롬프트 템플릿10을 시도했는데, 이는 SSA.2에서 사용한 것과 비교하여 약간 더 나쁜 결과를 제공한다. 우리는 Llama2-Chat에 대해 여러 {명령}을 시도했다. 우리는 아래의 것이 가장 잘 작동한다는 것을 발견합니다.\n' +
      '\n' +
      '각주 10: [https://gpus.llm-utils.org/llama-2-prompt-template/](https://gpus.llm-utils.org/llama-2-prompt-template/)\n' +
      '\n' +
      'DoQA, INSCIT, HybridDial, Doc2Dial, QuAC, QReCC는 일반적으로 긴 답변이지만 한 문장 내에 있기 때문에 "한 문장으로 답변해 주세요."를 사용한다. 우리는 모델이 매우 긴 답변을 생성하는 것을 방지하기 위해 "완전하고 완전한 답변" 대신 "하나의 문장"과 같은 라마2-챗 특정 지시를 내리는 것이 중요하다는 것을 알아챘다.\n' +
      '\n' +
      '우리는 Topi-OCQA에 대해 "짧은 스팬 또는 한 문장으로 다음 질문에 답하세요."를 사용합니다. 이 데이터 세트는 질문을 기반으로 짧은 답변과 긴 답변을 모두 가지고 있고 긴 답변은 일반적으로 한 문장 내에 있기 때문입니다.\n' +
      '\n' +
      '이 데이터 세트에 대한 답은 항상 컨텍스트의 하나 또는 엔터티 목록으로 구성되기 때문에 SQA에 대해 "다음 질문에 하나 또는 엔터티 목록으로 답변"을 사용합니다.\n' +
      '\n' +
      '우리는 CoQA의 경우 일반적으로 질문에 대한 답변이 짧기 때문에 "짧은 범위로 다음 질문에 답하세요. 대답은 몇 단어로 되어 있어야 합니다."를 사용합니다.\n' +
      '\n' +
      '이 데이터 세트는 모델이 컨텍스트에서 숫자를 추출하거나 산술 계산을 수행해야 하기 때문에 ConvFinQA에 대해 "컨텍스트에서 숫자만 응답하거나 +, -, * 또는 /를 사용하여 수학 산술만 응답"을 사용합니다. 우리는 모델에 의해 생성된 산술식을 추출하고 계산기를 사용하여 최종 결과를 얻는다.\n' +
      '\n' +
      '#### c.2.3. GPT-3.5-turbo & GPT-4\n' +
      '\n' +
      '우리는 GPT-3.5-turbo와 GPT-4에 대해 몇 가지 {명령}을 시도했는데, 아래 것들이 가장 잘 작동한다는 것을 발견했다(GPT-3.5-turbo와 GPT-4 모두에 적용됨).\n' +
      '\n' +
      'DoQA, INSCIT, HybridDial, Doc2Dial, QuAC, QReCC는 일반적으로 긴 답변이지만 한 문장 내에 있기 때문에 "한 문장으로 답변해 주세요."를 사용한다. 우리는 또한 Llama2-Chat과 유사하게 OpenAI 모델이 "완전하고 완전한 답변"의 지시를 고려할 때 상당히 긴 답변을 생성하는 경향이 있음을 발견했다. 따라서 본 논문에서는 모델이 매우 긴 답변을 생성하는 것을 방지하기 위해 명령어를 보다 구체화(즉, 한 문장)한다.\n' +
      '\n' +
      '우리는 Topi-OCQA에 대해 "다음 질문에 몇 단어 또는 한 문장으로 답하세요."를 사용합니다. 이 데이터 세트는 질문에 기반한 짧은 답변과 긴 답변을 모두 가지고 있고 긴 답변은 일반적으로 한 문장 내에 있기 때문입니다.\n' +
      '\n' +
      '우리는 SQA에 대해 "다음 질문에 하나 또는 개체 목록으로 답하세요. 자세한 설명은 하지 마세요. 답은 가능한 한 짧아야 합니다."를 사용합니다. 우리는 OpenAI 모델이 특별히 지시하지 않는 한 SQA 데이터 세트에 대해 종종 자세한 설명을 제공한다는 것을 발견했다.\n' +
      '\n' +
      '우리는 CoQA의 경우 일반적으로 질문에 대한 답변이 짧기 때문에 "짧은 범위로 다음 질문에 답하세요. 대답은 몇 단어로 되어 있어야 합니다."를 사용합니다.\n' +
      '\n' +
      '이 데이터 세트는 모델이 컨텍스트에서 숫자를 추출하거나 산술 계산을 수행해야 하기 때문에 ConvFinQA에 대해 "컨텍스트에서 숫자만 응답하거나 +, -, * 또는 /를 사용하여 수학 산술만 응답"을 사용합니다. 우리는 모델에 의해 생성된 산술식을 추출하고 계산기를 사용하여 최종 결과를 얻는다.\n' +
      '\n' +
      '## 부록 D 인간 평가\n' +
      '\n' +
      '우리는 10개의 테스트 벤치마크 데이터 세트에 걸쳐 인간 평가를 수행한다. 각 데이터 세트에 대해 무작위로 60개의 샘플을 선택하고 각 샘플은 3개의 주석기로 레이블이 지정되어 총 1800개의 주석이 생성된다.\n' +
      '\n' +
      '우리는 주석이 모델의 출력에서 사실을 확인하고 어떤 모델이 질문에 더 정확한 응답을 제공하는지 결정하도록 요청한다. 우리는 아마존 기계 터키 플랫폼을 사용하여 인간 평가를 수행합니다. 인간 평가 지침의 세부 사항과 주석자에게 표시된 인터페이스는 그림 3에 나와 있다.\n' +
      '\n' +
      '## Appendix E Unanswerable Case Evaluation\n' +
      '\n' +
      '답변할 수 없는 경우에, 우리는 질문이 정답으로 대답될 수 없다는 것을 나타내는 모델을 고려한다. 답변할 수 있는 경우에, 우리는 질문이 정답으로 대답할 수 없다는 것을 나타내지 않는 모델(즉, 답을 주는 모델)을 고려한다. 우리는 ChatQA가 2단계 튜닝에서 응답할 수 없는 질문에 대한 응답으로 이 문장을 사용하기 때문에 질문이 응답할 수 없을 때 항상 "죄송합니다. 컨텍스트에 따라 답변을 찾을 수 없습니다."를 생성한다는 것을 발견한다. 라마2-챗, GPT-3.5-터보 및 GPT-4의 경우 응답할 수 없는 질문에 대한 출력은 보통 몇 가지 특정 패턴을 따른다. 아래에서는 DoQA 및 QuAC 데이터 세트에서 생성된 모든 샘플에서 파생된 휴리스틱 일치 패턴을 나열하여 모델이 질문이 답할 수 없다고 제안하는지 확인한다. 생성된 출력에 아래에 나열된 패턴이 포함되어 있으면 질문이 답변할 수 없다는 표시로 해석하고 F1 평가를 위해 "죄송합니다. 컨텍스트에 따라 답변을 찾을 수 없습니다."로 답변을 대체합니다.\n' +
      '\n' +
      '확신할 수 없고, 찾을 수 없고, 제공할 수 없고, 제공할 수 없고, 답변할 수 없고, 찾을 수 없고, 결정할 수 없고, 정보가 없고, 정보가 없고, 찾을 수 없고, 문맥에서 정보가 없고, 언급할 수 없고, 명시적으로 언급할 수 없고, 존재하지 않고, 존재하지 않고, 특정할 수 없고, 제공할 수 없고, 제공할 수 없고, 불가능하고, 지정할 수 없고, 정보가 없고, 언급할 수 없고, 언급할 수 없고, 정보가 충분하지 않고, 정보가 충분하지 않고, 특정 정보가 없고, 특정 언급할 수 없고, 정보가 발견되지 않고, 정보가 발견되지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지 않고, 정보가 존재하지\n' +
      '\n' +
      '## 부록 F 사례 연구\n' +
      '\n' +
      'ChatQA-13B, ChatQA-70B, GPT-3.5-터보 및 GPT-4에 대한 더 많은 예는 표 10에서 찾을 수 있다.\n' +
      '\n' +
      '그림 3: 사람 평가 지침 및 주석자를 위한 인터페이스.\n' +
      '\n' +
      '제목: 벵골 여우. "발페스 벵갈렌시스"는 비교적 작은 여우로서 길쭉한 주둥이, 길고 뾰족한 귀, 머리와 몸길이의 50∼60% 정도의 바쁜 키를 가지고 있다. 등쪽은 매우 다양하지만 대부분 회색이고 전체적으로 창백하며 다리는 갈색을 띠거나 롤러스가 되는 경향이 있다. 그것은 "발페스 큐브"보다 훨씬 더 단조롭다. 꼬리는 "V. vulpes"와 구별되는 두드러진 검은색 끝으로 분주하다. 귀 뒤쪽은 어두운 갈색이고 가장자리는 검은색입니다. 그것의 표절은 나체이고 입술은 검은색이며, 눈 앞에 위 또는 멀리 후각(헤어드)에 작은 검은색 털 패치가 있다.\n' +
      '\n' +
      '제목: 붉은 여우. 붉은색은 크기와 육식에 대한 적응 방향에 대해 아프간, 코르삭 및 벵골보다 더 전문화된 형태의 "발페스"로 간주되며 두개골은 다른 종보다 훨씬 적은 신생 특성을 나타낸다.\n' +
      '\n' +
      '제목: 벵골 여우. 귀는 지도와 같은 색상을 갖거나 더 어두울 수 있지만 "V. vulpes"에서와 같이 어두운 패치가 없다. 코트 색상의 광범위한 변화는 개체군 전반에 걸쳐 그리고 계절적으로 개체군 내에서 존재하지만 일반적으로 회색에서 옅은 갈색으로 다양하다.\n' +
      '\n' +
      '\\begin{table}\n' +
      '\\begin{tabular}{l l l l} \\hline \\hline\n' +
      '**Context** & & **Conversation** & **Assistant Responses** \\\\ \\hline The 3000 meters steeplechasse or 3000-meter & User: Hello, can you tell me about the 3000 & \\\\ steeplechasse is the most common distance for the stereplechasse? & & metres steeplechasse? & \\\\ race over the distance of the 3000 metres, which & Assistant: The 3000 metres steeplechasse is the most common distance for the steeplechasse in & ChatQA-13B: He ran in Berlin, Adelaide, and Stockholm. \\\\ \\hline\n' +
      '1968 & Jouko Kuha (FIN) & Stockholm \\\\\n' +
      '1969 & Vladimir Indians (URS) & Kiev \\\\\n' +
      '1970 & Kerry O’Brien (AUS) & Berlin \\\\\n' +
      '1971 & Kerry O’Brien (AUS) & Adelaide \\\\\n' +
      '1972 & Anders Gartord (SWE) & Helsinki \\\\\n' +
      '1973 & Ben Jirbo (KEN) & Helsinki \\\\\n' +
      '1974 & Anders Gartord (SWE) & Helsinki \\\\\n' +
      '1975 & Anders Gartord (SWE) & Stockholm \\\\\n' +
      '1976 & Anders Gartord (SWE) & Montreal \\\\\n' +
      '1977 & Michael Karst (FRG) & Stockholm \\\\ \\hline \\hline \\end{tabular}\n' +
      '\\begin{tabular}{l l} \\hline \\hline title: Non-EU family member traveling to Schengen countries. source: You can travel visa-free only if you have a “residence” card of a family member of a citizen [FOOTNOTE:]Footnote : footnotemark: [ENDFOOTNOTE]\n' +
      '\n' +
      'User: Can I travel from Romania to the Schengen area visa-free? \\\\ \\hline Assistant: You can travel visa-free only if you have a “residence card of a family member of a citizen” issued under article 10 of the free movement & ChaQA-70B: Sorry. I cannot find the answer based on the context. \\\\ \\hline Assistant: You can travel visa-free only if you have a “residence card of a family member of a union citizen” issued under article 10 card, EU countries. name this issue you a “residence card of a family member of a union citizen” issued under article 10 card, EU countries. name this issue you can travel together visa-free? & ChatQA-13B: The grounds for refusal of that visa are strictly limited. These provisions are also dependent on your traveling with or requiring your spouse. In case you are not, you’ll need a regular Schengen visa. As [ENDFOOTNOTE]\n' +
      '\n' +
      '\\end{table}\n' +
      '표 10: ChatQA-13B, ChatQA-70B, GPT-3.5-터보 및 GPT-4에 대한 사례 연구. 마지막 사용자 질문에 대한 관련 컨텍스트를 강조하기 위해 파란색 색상을 사용한다. 그리고 보조 응답에서 정확한 부분과 잘못된 부분을 강조하기 위해 녹색과 빨간색을 사용합니다.\n' +
      '\n' +
      '## 대화식 QA 데이터 수집을 위한 부록 G 지침\n' +
      '\n' +
      '이 섹션에서는 대화형 QA 데이터 수집에 대해 제공하는 지침을 보여준다. 주석이 주어진 문서를 기반으로 대화형 QA 샘플을 구성하기 위해 주석이 사용자와 에이전트 역할을 하도록 요청하여 주석을 보다 효율적으로 만든다. 지침은 1) 대화 QA 샘플이 어떻게 생겼는지, 2) 우리가 필요로 하는 대화 QA 샘플이 어떤 종류인지, 3) 주석을 달아야 하는 것의 세 부분으로 구성된다.\n' +
      '\n' +
      '### 대화형 QA 샘플은 다음과 같습니다.\n' +
      '\n' +
      '대화형 QA 샘플 하나는 우리가 제공하는 문서를 기반으로 합니다. 사용자 및 에이전트 행동은 다음과 같다:\n' +
      '\n' +
      '* 사용자 동작: 1) 주어진 문서를 기반으로 에이전트에 질문을 하고, 2) 에이전트가 무언가를 명확히 하고자 할 때 에이전트의 질문에 답합니다.\n' +
      '* 에이전트 동작: 1) 문서를 기반으로 사용자의 질문에 응답합니다. 2) 사용자의 질문이 명확하지 않거나 너무 일반적이거나 광범위할 때 사용자에게 질문을 합니다.\n' +
      '\n' +
      '### 필요한 다중 전환 QA 샘플 종류\n' +
      '\n' +
      '아래에 사용자의 질문 및 에이전트의 응답에 대한 요구 사항을 나열합니다.\n' +
      '\n' +
      '#### User\'s Questions\n' +
      '\n' +
      '* 사용자의 질문은 이전(또는 여러 차례 이전) 질문을 참조할 수 있습니다.\n' +
      '* 사용자의 질문은 에이전트의 이전(또는 이전 여러 차례) 답변을 참조할 수도 있습니다.\n' +
      '* 이전에 언급한 엔터티를 대체하기 위해 대명사 또는 일반 명사를 사용하려고 합니다.\n' +
      '* 사용자의 질문을 다양 하도록 설정 합니다. 동일한 유형의 질문에 대해 서로 다른 대화 주석으로 표현하기 위해 서로 다른 방법을 사용하려고 시도한다.\n' +
      '* 필요한 기타 사용자 질문 유형\n' +
      '* 에이전트의 답변이 주어지면 추가 정보(예: 기타...; 기타...; 추가...)를 요청합니다.\n' +
      '* 주제를 전환 하 고 대화에서 새 스레드를 시작 합니다.\n' +
      '* 동시에 두 가지 질문을 합니다.\n' +
      '\n' +
      '#### Agent\'s Response\n' +
      '\n' +
      '* 1-2 문장 내에서 에이전트의 답변을 만듭니다. 대답이 길어야 한다면, 간결하게 해보세요.\n' +
      '* 문서에서 전체 관련 컨텍스트를 답변으로 직접 복사하지 않도록 합니다. 대신에, 선택된 문맥을 바꿔서 답을 구성해 보세요.\n' +
      '* 에이전트가 사용자에게 질문하여 무엇인가를 명확히 하는 경우의 작은 비율을 생각해 보십시오. 구체적으로, 사용자의 질문이 너무 광범위하거나 명확하지 않은 경우, 에이전트는 사용자가 어떤 특정 측면에 더 관심이 있는지 알기 위해 명확화 질문을 함으로써 범위를 좁힐 필요가 있다.\n' +
      '\n' +
      '### 주석이 필요한 사항\n' +
      '\n' +
      '우리는 각 대화에 대해 주석을 달 필요가 있는 것을 아래에 나열한다.\n' +
      '\n' +
      '* 각 문서에 대해 사용자의 질문 및 해당 에이전트의 응답에 주석을 달아야 합니다. 대화당 평균 사용자-에이전트 회전 수는 약 5개여야 합니다.\n' +
      '* 각 사용자의 질문에 대해 문서 내의 모든 관련 컨텍스트에 주석을 달아야 합니다.\n' +
      '\n';
  </script>
  <style>
    #content {
      max-width: 800px;
      margin: auto;
    }
  </style>
  <script>
    let script = document.createElement('script');
    script.src = "https://cdn.jsdelivr.net/npm/mathpix-markdown-it@1.0.40/es5/bundle.js";
    document.head.append(script);

    script.onload = function() {
      const isLoaded = window.loadMathJax();
      if (isLoaded) {
        console.log('Styles loaded!')
      }

      const el = window.document.getElementById('content-text');
      if (el) {
        const options = {
          htmlTags: true
        };
        const html = window.render(text, options);
        el.outerHTML = html;
      }
    };
  </script>
</head>
<body>
  <div id="content"><div id="content-text"></div></div>
</body>
</html>
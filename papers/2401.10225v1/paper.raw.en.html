<html lang="en" data-theme="light"><head>
<meta content="text/html; charset=utf-8" http-equiv="content-type">
<title>ChatQA: Building GPT-4 Level Conversational QA Models</title>
<!--Generated on Thu Jan 18 17:40:31 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css">
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css">
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Machine Learning,  ICML" lang="en" name="keywords">
<base href="/html/2401.10225v1/"><link rel="stylesheet" href="https://use.typekit.net/rwr5zpx.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2401.10225v1">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <span class="color-scheme-icon" aria-label="Light mode"></span>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main >.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="https://services.dev.arxiv.org/html/static/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="#myForm" onclick="event.preventDefault(); var modal = document.getElementById('myForm'); modal.style.display = 'block'; bugReportState.setInitiateWay('Header');">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2401.10225v1">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2401.10225v1" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
          <span class="color-scheme-icon"></span>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Conversational QA ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Conversational QA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS2" title="2.2 Retrieval for Multi-Turn QA ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Retrieval for Multi-Turn QA</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS2.SSS1" title="2.2.1 Conversational Query Rewriting ‣ 2.2 Retrieval for Multi-Turn QA ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Conversational Query Rewriting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS2.SSS2" title="2.2.2 Fine-tuning Retriever for multi-turn QA ‣ 2.2 Retrieval for Multi-Turn QA ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Fine-tuning Retriever for multi-turn QA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS3" title="2.3 Instruction Tuning ‣ 2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Instruction Tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>ChatQA</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Stage-1: Supervised Fine-tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Stage-1: Supervised Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S3.SS2" title="3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Stage-2: Context-Enhanced Instruction Tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS1" title="3.2.1 Human Annotated Data ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Human Annotated Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS2" title="3.2.2 Synthetic Data Generation ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Synthetic Data Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S3.SS2.SSS3" title="3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Training Blends</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Retrieval for Multi-Turn QA</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="4.1 Fine-tuning Retriever for Multi-turn QA ‣ 4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Fine-tuning Retriever for Multi-turn QA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="4.2 Conversational Query Rewriting ‣ 4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Conversational Query Rewriting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS3" title="4.3 Comparisons ‣ 4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparisons</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S5" title="5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS1" title="5.1 Baselines ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S5.SS2" title="5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluation Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS2.SSS1" title="5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Long Document Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S5.SS2.SSS2" title="5.2.2 Short Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Short Document Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S5.SS3" title="5.3 Evaluation Metrics ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S6" title="6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S6.SS1" title="6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Main Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS1" title="6.1.1 overview ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS2" title="6.1.2 Importance of Stage-1 SFT ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Importance of Stage-1 SFT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS3" title="6.1.3 Effectiveness of Single-Turn Data ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Effectiveness of Single-Turn Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS4" title="6.1.4 Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.4 </span>Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS1.SSS5" title="6.1.5 Human Evaluation ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.5 </span>Human Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS2" title="6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Fine-grained Analyses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS3" title="6.3 Top-k Chunks for Stage-2 Instruction Tuning ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Top-<em class="ltx_emph ltx_font_italic">k</em> Chunks for Stage-2 Instruction Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS4" title="6.4 Ablation Studies for Inference Stage ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Ablation Studies for Inference Stage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S6.SS5" title="6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Evaluation of Unanswerable Case</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS5.SSS1" title="6.5.1 Evaluation Setup ‣ 6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.1 </span>Evaluation Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S6.SS5.SSS2" title="6.5.2 Results ‣ 6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S6.SS6" title="6.6 Case Study ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Case Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S7" title="7 Conclusion ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A1" title="Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>ChatQA Instruction Tuning</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A1.SS1" title="A.1 Stage-1: Supervised Fine-tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Stage-1: Supervised Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A1.SS2" title="A.2 Stage-2: Context-Enhanced Instruction Tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Stage-2: Context-Enhanced Instruction Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A1.SS3" title="A.3 Prompts for Synthetic Data Generation ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Prompts for Synthetic Data Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A2" title="Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>More Details and Results for Retrieval in Conversational QA</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A2.SS1" title="B.1 Query Rewriting Prompts for GPT-3.5-turbo ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Query Rewriting Prompts for GPT-3.5-turbo</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A2.SS2" title="B.2 More Results for Retrieval in Conversational QA ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>More Results for Retrieval in Conversational QA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A3" title="Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Conversational QA Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#A3.SS1" title="C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Data Statistics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px1" title="Doc2Dial ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">Doc2Dial</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px2" title="QuAC ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">QuAC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px3" title="QReCC ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">QReCC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px4" title="TopiOCQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">TopiOCQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px5" title="INSCIT ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">INSCIT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px6" title="CoQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">CoQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px7" title="DoQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">DoQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px8" title="ConvFinQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">ConvFinQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px9" title="SQA ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">SQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A3.SS1.SSS0.Px10" title="HybridDial ‣ C.1 Data Statistics ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">HybridDial</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#A3.SS2" title="C.2 Prompts for the Benchmarks ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Prompts for the Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#A3.SS2.SSS1" title="C.2.1 ChatQA ‣ C.2 Prompts for the Benchmarks ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.1 </span>ChatQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#A3.SS2.SSS2" title="C.2.2 Llama2-Chat ‣ C.2 Prompts for the Benchmarks ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.2 </span>Llama2-Chat</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#A3.SS2.SSS3" title="C.2.3 GPT-3.5-turbo &amp; GPT-4 ‣ C.2 Prompts for the Benchmarks ‣ Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2.3 </span>GPT-3.5-turbo &amp; GPT-4</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A4" title="Appendix D Human Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A5" title="Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Unanswerable Case Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A6" title="Appendix F Case Study ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Case Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="#A7" title="Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Guidelines for Conversational QA Data Collection</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A7.SS1" title="G.1 What does conversational QA samples look like ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.1 </span>What does conversational QA samples look like</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#A7.SS2" title="G.2 What kinds of multi-turn QA samples we need ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.2 </span>What kinds of multi-turn QA samples we need</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A7.SS2.SSS0.Px1" title="User’s Questions ‣ G.2 What kinds of multi-turn QA samples we need ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">User’s Questions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#A7.SS2.SSS0.Px2" title="Agent’s Response ‣ G.2 What kinds of multi-turn QA samples we need ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title">Agent’s Response</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#A7.SS3" title="G.3 What we need to annotate ‣ Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.3 </span>What we need to annotate</span></a></li>
</ol>
</li>
</ol></nav>

<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2401.10225v1 [cs.CL] 18 Jan 2024</div></div>
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">ChatQA: Building GPT-4 Level Conversational QA Models</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zihan Liu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Ping
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rajarshi Roy
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peng Xu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Shoeybi
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bryan Catanzaro
</span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p" id="id1.id1">In this work, we introduce ChatQA, a family of conversational question answering&nbsp;(QA) models, that obtain GPT-4 level accuracies.
Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models&nbsp;(LLMs).
To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost.
Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets&nbsp;(54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break">
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Most recently, ChatGPT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite> and its follow ups&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib50" title="">2023</a>; Anthropic, <a class="ltx_ref" href="#bib.bib5" title="">2023b</a>; Google, <a class="ltx_ref" href="#bib.bib27" title="">2023</a>)</cite> have led to the paradigm shift of building question answering&nbsp;(QA) models in production and research community.
In particular, the following aspects of the QA models are preferred in real-world applications:
<em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">i)</em>&nbsp;The users can interact with the QA models in a conversational way, thus one can easily raise follow-up questions.
<em class="ltx_emph ltx_font_italic" id="S1.p1.1.2">ii)</em>&nbsp;The generalist models can generate answers in zero-shot manner without dataset-specific fine-tuning, while matching the accuracies of fine-tuned expert models.
<em class="ltx_emph ltx_font_italic" id="S1.p1.1.3">iii)</em>&nbsp;The QA models are capable of integrating retrieved chunks of evidence in both open-domain or long document settings, where the provided context is much longer than the context window of LLM&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(e.g., Anthropic, <a class="ltx_ref" href="#bib.bib4" title="">2023a</a>; Xu et&nbsp;al., <a class="ltx_ref" href="#bib.bib74" title="">2023b</a>)</cite>.
To this end, we focus on the conversational QA covering these three aspects.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, building a conversational QA model, that can match the accuracy of the state-of-the-art black-box model, i.e., GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib50" title="">2023</a>)</cite>, is still a grand challenge for the research community.
In this work, we introduce ChatQA-70B, a white-box conversational QA model with GPT-4 level accuracy, through a proposed two-stage instruction tuning recipe, an enhanced retriever for retrieval-augmented generation&nbsp;(RAG) in conversational QA, and careful data curation process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Specifically, we make the following contributions:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a two-stage instruction tuning method and a dataset curation recipe that can largely enhance LLM’s capability of integrating user provided or retrieved context for zero-shot conversational QA tasks.
We demonstrate that our method significantly outperforms regular instruction tuning or RLHF-based recipes&nbsp;(e.g., Llama-2-Chat).
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">For RAG in conversational QA, we show that fine-tuning state-of-the-art single-turn query retrievers on human-annotated multi-turn QA dataset works as well as utilizing the state-of-the-art LLM-based query rewriting model, i.e., GPT-3.5-turbo&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We build a family of ChatQA models based on Llama2-7B, Llama2-13B, Llama2-70B&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="#bib.bib60" title="">2023</a>)</cite> and a in-house 8B pretrained GPT.
We conduct comprehensive study on 10 conversational QA datasets, including 5 datasets with long documents that need retrieval and 3 datasets with tables.
In terms of average score, our ChatQA-70B model&nbsp;(54.14) can outperform GPT-3.5-turbo&nbsp;(50.37) and GPT-4&nbsp;(53.90) without utilizing any synthetic data from ChatGPT models.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We study the “unanswerable” scenario, where the desired answer is not included in the provided or retrieved context, thus the LLM can easily hallucinate.
We demonstrate that adding a small amount of “unanswerable” samples in instruction tuning can steer the model to generate “can not answer” output when it is necessary, thus largely reduce hallucination. Our ChatQA-70B outperforms GPT-3.5-turbo in this regard, while still has a slight gap compared to GPT-4&nbsp;(around 3.5%).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We organize the rest of the paper as follows.
We discuss related work in §&nbsp;<a class="ltx_ref" href="#S2" title="2 Related Work ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>.
We introduce the two-stage instruction tuning method for ChatQA in §&nbsp;<a class="ltx_ref" href="#S3" title="3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">3</span></a>, and study retrieval in conversational QA in §&nbsp;<a class="ltx_ref" href="#S4" title="4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4</span></a>.
We present the experimental setup in §&nbsp;<a class="ltx_ref" href="#S5" title="5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">5</span></a> results in §&nbsp;<a class="ltx_ref" href="#S6" title="6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6</span></a> and conclude the paper in §&nbsp;<a class="ltx_ref" href="#S7" title="7 Conclusion ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">7</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="148" id="S1.F1.g1" src="x1.png" width="706">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Two-stage instruction tuning framework for ChatQA.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Conversational QA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Question answering in conversational way naturally improves user experiences by addressing followup questions. The model can also raise clarification questions for users if necessary, which can reduce hallucination.
Thus, it becomes the default format of deploying QA models in production&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(e.g. OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>; Google, <a class="ltx_ref" href="#bib.bib27" title="">2023</a>; Anthropic, <a class="ltx_ref" href="#bib.bib5" title="">2023b</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In recent years, many conversational QA datasets have been introduced, where the models are asked to answer questions based on provided context or documents.
The provided context or documents can be: <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">i</em>) text-only from various domains&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>; Anantha et&nbsp;al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>; Saeidi et&nbsp;al., <a class="ltx_ref" href="#bib.bib58" title="">2018</a>; Adlakha et&nbsp;al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>; Aliannejadi et&nbsp;al., <a class="ltx_ref" href="#bib.bib2" title="">2021</a>; Reddy et&nbsp;al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>; Qu et&nbsp;al., <a class="ltx_ref" href="#bib.bib53" title="">2020</a>; Wu et&nbsp;al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>; Deng et&nbsp;al., <a class="ltx_ref" href="#bib.bib18" title="">2022</a>; Guo et&nbsp;al., <a class="ltx_ref" href="#bib.bib28" title="">2021</a>; Choi et&nbsp;al., <a class="ltx_ref" href="#bib.bib10" title="">2018</a>; Campos et&nbsp;al., <a class="ltx_ref" href="#bib.bib7" title="">2020</a>)</cite>, or <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.2">ii</em>) plain text along with tables&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pasupat &amp; Liang, <a class="ltx_ref" href="#bib.bib52" title="">2015</a>; Nakamura et&nbsp;al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>; Chen et&nbsp;al., <a class="ltx_ref" href="#bib.bib8" title="">2022a</a>)</cite>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">In contrast to the latest LLM-based generalist solution&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(e.g., OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>, most of previous studies focus on fine-tuned expert models on specific domains or datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>; Izacard &amp; Grave, <a class="ltx_ref" href="#bib.bib32" title="">2021</a>; Chen et&nbsp;al., <a class="ltx_ref" href="#bib.bib8" title="">2022a</a>; Gao et&nbsp;al., <a class="ltx_ref" href="#bib.bib25" title="">2022</a>; Nakamura et&nbsp;al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>; Adlakha et&nbsp;al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>; Wu et&nbsp;al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Retrieval for Multi-Turn QA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Conversational QA involves retrieval-augmented generation&nbsp;(RAG) in open-domain setting, or when the provided documents are longer than the context window of LLM.
The dense retrievers are usually trained to retrieve the top-<em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">k</em> relevant chunks given a single question&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(e.g., Lin et&nbsp;al., <a class="ltx_ref" href="#bib.bib39" title="">2023a</a>; Wang et&nbsp;al., <a class="ltx_ref" href="#bib.bib65" title="">2022a</a>; Izacard et&nbsp;al., <a class="ltx_ref" href="#bib.bib33" title="">2022</a>)</cite>.
In conversational QA, the follow-up questions&nbsp;(e.g., with pronouns referring to entities mentioned in the previous conversation) may have insufficient information for retrieval, while feeding them along with the dialogue history can be redundant and lead to sub-optimal results.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Conversational Query Rewriting</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Most of previous solutions are query rewriting methods.
The latest turn of question is rewritten to be a standalone query without additional information from previous dialogue history&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vakulenko et&nbsp;al., <a class="ltx_ref" href="#bib.bib62" title="">2021a</a>; Ye et&nbsp;al., <a class="ltx_ref" href="#bib.bib75" title="">2023</a>; Mo et&nbsp;al., <a class="ltx_ref" href="#bib.bib45" title="">2023</a>)</cite>, so it can be directly used by retrieval model to retrieve relevant context&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vakulenko et&nbsp;al., <a class="ltx_ref" href="#bib.bib63" title="">2021b</a>; Mele et&nbsp;al., <a class="ltx_ref" href="#bib.bib43" title="">2021</a>; Raposo et&nbsp;al., <a class="ltx_ref" href="#bib.bib56" title="">2022</a>; Mo et&nbsp;al., <a class="ltx_ref" href="#bib.bib45" title="">2023</a>)</cite>.
Many datasets have been collected to facilitate this line of research&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Elgohary et&nbsp;al., <a class="ltx_ref" href="#bib.bib21" title="">2019</a>; Chu et&nbsp;al., <a class="ltx_ref" href="#bib.bib11" title="">2020</a>; Qu et&nbsp;al., <a class="ltx_ref" href="#bib.bib53" title="">2020</a>; Anantha et&nbsp;al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>; Brabant et&nbsp;al., <a class="ltx_ref" href="#bib.bib6" title="">2022</a>)</cite>, alongside multiple proposed query rewriting methods&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ishii et&nbsp;al., <a class="ltx_ref" href="#bib.bib30" title="">2022</a>; Yu et&nbsp;al., <a class="ltx_ref" href="#bib.bib76" title="">2020</a>; Wu et&nbsp;al., <a class="ltx_ref" href="#bib.bib71" title="">2022</a>; Del&nbsp;Tredici et&nbsp;al., <a class="ltx_ref" href="#bib.bib17" title="">2021</a>; Chen et&nbsp;al., <a class="ltx_ref" href="#bib.bib9" title="">2022b</a>; Galimzhanova et&nbsp;al., <a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>.
For example, <cite class="ltx_cite ltx_citemacro_citet">Wu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib71" title="">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Chen et&nbsp;al. (<a class="ltx_ref" href="#bib.bib9" title="">2022b</a>)</cite> proposed to use reinforcement learning methods for the query rewriting.
<cite class="ltx_cite ltx_citemacro_citet">Yu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib76" title="">2020</a>)</cite> investigated few-shot generative models like GPT-2 for query rewriting.
<cite class="ltx_cite ltx_citemacro_citet">Galimzhanova et&nbsp;al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite> studied instruction tuned GPT-3.5-turbo and showed that it achieved state-of-the-art results for conversational query rewriting.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Fine-tuning Retriever for multi-turn QA</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Some previous work fine-tune a single-turn query retriever on in-domain conversational query and context pairs&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>; Gao et&nbsp;al., <a class="ltx_ref" href="#bib.bib25" title="">2022</a>; Adlakha et&nbsp;al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>; Wu et&nbsp;al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite>, so it can directly take a concatenation of dialog history and current query as input.
In this work, we focus on the zero-shot evaluation.
We fine-tuning a single-turn query retriever on a high-quality multi-turn dataset. Then, we evaluate zero-shot capability of the fine-tuned retriever on five benchmark datasets.
Surprisingly, we find this simple approach can obtain comparable zero-shot results as the state-of-the-art query rewriting model, i.e., GPT-3.5-turbo.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Instruction Tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The goal of instruction tuning is to equip LLMs with the capability to follow natural language instructions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a class="ltx_ref" href="#bib.bib69" title="">2022a</a>; Sanh et&nbsp;al., <a class="ltx_ref" href="#bib.bib59" title="">2022</a>; Mishra et&nbsp;al., <a class="ltx_ref" href="#bib.bib44" title="">2022</a>; Iyer et&nbsp;al., <a class="ltx_ref" href="#bib.bib31" title="">2022</a>; Du et&nbsp;al., <a class="ltx_ref" href="#bib.bib19" title="">2022</a>; Ouyang et&nbsp;al., <a class="ltx_ref" href="#bib.bib51" title="">2022</a>; Wang et&nbsp;al., <a class="ltx_ref" href="#bib.bib68" title="">2023b</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="#bib.bib77" title="">2023</a>; Gao et&nbsp;al., <a class="ltx_ref" href="#bib.bib26" title="">2023</a>; Chung et&nbsp;al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>; Muennighoff et&nbsp;al., <a class="ltx_ref" href="#bib.bib46" title="">2022</a>; Xu et&nbsp;al., <a class="ltx_ref" href="#bib.bib73" title="">2023a</a>; Wang et&nbsp;al., <a class="ltx_ref" href="#bib.bib67" title="">2022c</a>; Zhou et&nbsp;al., <a class="ltx_ref" href="#bib.bib78" title="">2023</a>)</cite>.
There has been a surge in the development of high-quality instruction tuning datasets, including FLAN&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chung et&nbsp;al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>)</cite>, Self-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="#bib.bib66" title="">2022b</a>)</cite>, unnatural Instructions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Honovich et&nbsp;al., <a class="ltx_ref" href="#bib.bib29" title="">2022</a>)</cite>, Dolly&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Conover et&nbsp;al., <a class="ltx_ref" href="#bib.bib14" title="">2023b</a>)</cite>, and OpenAssistant <cite class="ltx_cite ltx_citemacro_citep">(Köpf et&nbsp;al., <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Although numerous research about instruction tuning has been conducted, a few work focused on improving RAG or context awareness generation for QA.
<cite class="ltx_cite ltx_citemacro_citet">Lin et&nbsp;al. (<a class="ltx_ref" href="#bib.bib40" title="">2023b</a>)</cite> introduced a retrieval-augmented instruction tuning method, which appends top-<em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.1">k</em> retrieved chunks for LLM fine-tuning.
<cite class="ltx_cite ltx_citemacro_citet">Wang et&nbsp;al. (<a class="ltx_ref" href="#bib.bib64" title="">2023a</a>)</cite> applied instruction tuning after retrieval-augmented pretraining.
In contrast, we propose a two-stage instruction tuning method to improve generation with retrival or provided context. We find that appending top-<em class="ltx_emph ltx_font_italic" id="S2.SS3.p2.1.2">k</em> retrieved chunks for LLM fine-tuning does not help for a wide range of conversation QA tasks&nbsp;(see §<a class="ltx_ref" href="#S6.SS3" title="6.3 Top-k Chunks for Stage-2 Instruction Tuning ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6.3</span></a> for details).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">After the release of ChatGPT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib49" title="">2022</a>)</cite>, instruction tuning becomes an indispensable ingredient to build the state-of-the-art dialogue agent which has astonishing zero-shot capability to a wide range of tasks.
The conversational QA capability plays a crucial role in a dialogue agent, yet there has been limited research dedicated to this vital aspect.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ChatQA</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we propose a two-stage instruction tuning method for ChatQA. See Figure&nbsp;<a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">1</span></a> for an illustration.
Our method starts with an pretrained LLM foundation model.
At stage-1, we apply supervised fine-tuning (SFT) as in&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Ouyang et&nbsp;al. (<a class="ltx_ref" href="#bib.bib51" title="">2022</a>)</cite> on a blend of instruction-following and dialog datasets.
After that, our model exhibits good capability to follow instructions as a conversational agent. However, its capability for contextualized or RAG-based QA remains limited. Hence, we introduce a subsequent stage, called context-enhanced instruction tuning, which is designed specifically for enhancing our model’s capability for context-aware or retrieval-augmented generation in conversational QA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Stage-1: Supervised Fine-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To construct a large and comprehensive supervised fine-tuning&nbsp;(SFT) dataset, we follow&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Xu et&nbsp;al. (<a class="ltx_ref" href="#bib.bib74" title="">2023b</a>); Wang et&nbsp;al. (<a class="ltx_ref" href="#bib.bib64" title="">2023a</a>)</cite> and gather a combined set of 128K SFT samples from high-quality instruction tuning datasets. It consists of 1) a social dialogue dataset Soda&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kim et&nbsp;al., <a class="ltx_ref" href="#bib.bib34" title="">2022</a>)</cite>, 2) a long-form QA dataset ELI5 containing elaborate answers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Fan et&nbsp;al., <a class="ltx_ref" href="#bib.bib22" title="">2019</a>)</cite>, 3) FLAN and chain-of-thought datasets&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wei et&nbsp;al., <a class="ltx_ref" href="#bib.bib70" title="">2022b</a>; Chung et&nbsp;al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>; Longpre et&nbsp;al., <a class="ltx_ref" href="#bib.bib42" title="">2023</a>)</cite>, 4) LLM synthetic instructions tuning datasets: Self-Instruct&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="#bib.bib66" title="">2022b</a>)</cite> and Unnatural Instructions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Honovich et&nbsp;al., <a class="ltx_ref" href="#bib.bib29" title="">2022</a>)</cite>, and 5) a private crowd-sourced conversational dataset, as well as two public human-written conversation datasets: OpenAssistant&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Köpf et&nbsp;al., <a class="ltx_ref" href="#bib.bib36" title="">2023</a>)</cite>, and Dolly&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Conover et&nbsp;al., <a class="ltx_ref" href="#bib.bib13" title="">2023a</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We unify the structure of all the SFT data in a conversational format. We first add a “System” role at the beginning to setup a general instruction guiding LLM to provide polite and helpful answers.
We also add “User” and “Assistant” roles to incorporate instruction and response pairs from the instruction tuning datasets.
We apply fine-tuning using this unified format on an LLM foundation model.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Stage-2: Context-Enhanced Instruction Tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To further enhance the model’s conversational QA capability over a given context, we conduct a second stage instruction tuning, which integrate contextualized QA datasets into the instruction tuning blend.
Specifically, the stage-2 instruction tuning datasets consist of a blend of contextualized single-turn QA and conversational QA datasets.
We present further details for the stage-2 instruction tuning datasets below.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Human Annotated Data</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">In addition to the public available datasets, one of the key elements for stage-2 is to obtain a high-quality document-grounded conversational QA dataset.
We create a human-annotated conversational QA (called HumanAnnotatedConvQA) dataset only consisting of 7k dialogues. To build this dataset, we first collected 7k documents covering diverse topics from Internet. Then, we instruct the annotator to act both as a curious user asking questions (and follow-up questions) about the document and as a agent to give the answers. We create a multi-turn conversation for each document, resulting in a total of 7k conversational QA dialogues with an average of 5 user-agent turns per dialogue. Details of data collection guideline can be found in Appendix&nbsp;<a class="ltx_ref" href="#A7" title="Appendix G Guidelines for Conversational QA Data Collection ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">G</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">In addition, to reduce hallucinated answers in unanswerable cases, we aim to empower our model to explicitly indicate it when the answer cannot be found within the given context.
To obtain these unanswerable data samples, we requested annotators to provide all context locations to the user question. Hence, it enabled us to construct unanswerable scenarios by deleting the text from the corresponding locations in the context.
After deleting the relevant text to the question, we use a sentence, “<span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p2.1.1">Sorry.&nbsp;I cannot find the answer based on the context</span>”, as the response for the unanswerable questions.
Finally, we construct another 1.5k user-agent turns with unanswerable annotations, which provides a good trade-off of answerable and unanswerable cases&nbsp;(see §<a class="ltx_ref" href="#S6.SS5" title="6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6.5</span></a> for details).
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Synthetic Data Generation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">To validate the quality of the HumanAnnotatedConvQA, we leverage GPT-3.5-turbo to generate synthetic conversational QA dataset given its powerful instruction-following and text generation capability.
Note that, large-scale synthetic data for conversational QA has also been explored in <cite class="ltx_cite ltx_citemacro_citet">Dai et&nbsp;al. (<a class="ltx_ref" href="#bib.bib15" title="">2022</a>)</cite>.
In this work, we focus on midsize high-quality synthetic data for LLM fine-tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">The instruction for GPT-3.5-turbo comprises three parts: 1)&nbsp;system role guiding the model to provide helpful answer, 2)&nbsp;examples of conversational QA indicating the required data types, and 3)&nbsp;a document that directs the model to generate conversational QA based on its content.
We collect 7k documents (average <math alttext="\sim" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.1.m1.1"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mo id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p2.1.m1.1d">∼</annotation></semantics></math>1k words per document) from common crawl, which cover a wide range of domains. Each document is used for generation of a single conversational QA sample, which leads to a total of 7k multi-turn QA dialogues with an average of 4.4 user-agent turns per dialogue (called SyntheticConvQA).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">Similar to the HumanAnnotatedConvQA, we construct another 1.5k user-agent turns with unanswerable annotations in this synthetic dataset.
Since there is no annotations of the context location for the agent’s answer, we construct synthetic unanswerable samples from SyntheticConvQA.
Specifically, we first cut the document (for each dialogue) into different chunks. Then, we consider it as a valid unanswerable sample only when there are chunks that have “high overlaps” with the agent’s answer to be removed and the rest of the chunks shows “low overlaps” with the agent’s answer. We use the 4-gram recall score between each chunk and agent’s answer (it measures the ratios of the answer’s 4-gram phrases are within each chunk) as the metric to measure their overlaps, and consider it higher than 0.5 as “high overlaps” and lower than 0.1 as “low overlaps”.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Training Blends</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">In this part, we introduce the details of training blends for stage-2 instruction tuning.
To boost the QA capability in handling tabular documents and arithmetic calculation, we add the TAT-QA dataset&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhu et&nbsp;al., <a class="ltx_ref" href="#bib.bib79" title="">2021</a>)</cite> which contains both elements. In addition, we integrate contextualized single-turn QA datasets to further strengthen the QA capability of our model. Furthermore, we still keep the stage-1 SFT dataset in the training blend to maintain the model’s instruction-following capability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">Finally, the training blend for stage-2 consists of:
1)&nbsp;A conversational QA dataset: HumanAnnotatedConvQA or SyntheticConvQA,&nbsp;<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Unless specified otherwise, the experiments use HumanAnnotatedConvQA as the default setting.</span></span></span>
2)&nbsp;single-turn QA datasets: DROP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dua et&nbsp;al., <a class="ltx_ref" href="#bib.bib20" title="">2019</a>)</cite>, NarrativeQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Kočiskỳ et&nbsp;al., <a class="ltx_ref" href="#bib.bib35" title="">2018</a>)</cite>, Quoref&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dasigi et&nbsp;al., <a class="ltx_ref" href="#bib.bib16" title="">2019</a>)</cite>, ROPES&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="#bib.bib38" title="">2019</a>)</cite>, SQuAD1.1&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et&nbsp;al., <a class="ltx_ref" href="#bib.bib54" title="">2016</a>)</cite>, SQuAD2.0&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et&nbsp;al., <a class="ltx_ref" href="#bib.bib55" title="">2018</a>)</cite>, NewsQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Trischler et&nbsp;al., <a class="ltx_ref" href="#bib.bib61" title="">2017</a>)</cite>, TAT-QA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhu et&nbsp;al., <a class="ltx_ref" href="#bib.bib79" title="">2021</a>)</cite>, and 3)&nbsp;All of SFT datasets from stage-1.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">We follow the similar template as in stage-1 to unify all single-turn QA and conversational QA datasets. The differences are in two parts: 1) Following the system role, we append relevant context for the single-turn question or the multi-turn conversation; and 2) We integrate further instruction just before the single-turn question or multi-turn conversation based on the answer types of different QA datasets (e.g., short answer, long answer, arithmetic calculation). We use the format for SFT dataset from stage-1.&nbsp;<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Details of the templates for both stage-1 and stage-2 instruction tuning as well as the synthetic data generation can be found in the Appendix&nbsp;<a class="ltx_ref" href="#A1" title="Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.9" style="width:566.2pt;height:199pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S3.T1.9.9"><span class="ltx_text" id="S3.T1.9.9.9">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.9.9.9.9">
<span class="ltx_tbody">
<span class="ltx_tr" id="S3.T1.9.9.9.9.10.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S3.T1.9.9.9.9.10.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.1.1" style="color:#000000;">Models</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.2.1" style="color:#000000;">Average</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.3.1" style="color:#000000;">Doc2Dial</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.4.1" style="color:#000000;">QuAC</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.5.1" style="color:#000000;">QReCC</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.6.1" style="color:#000000;">TopiOCQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S3.T1.9.9.9.9.10.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.10.1.7.1" style="color:#000000;">INSCIT</span></span></span>
<span class="ltx_tr" id="S3.T1.9.9.9.9.11.2">
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.1.1" style="color:#000000;">top-1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.2.1" style="color:#000000;">top-5</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.3.1" style="color:#000000;">top-1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.4.1" style="color:#000000;">top-5</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.5.1" style="color:#000000;">top-1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.6.1" style="color:#000000;">top-5</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.7.1" style="color:#000000;">top-1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.8.1" style="color:#000000;">top-5</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.9.1" style="color:#000000;">top-5*</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.10.1" style="color:#000000;">top-20*</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.11.1" style="color:#000000;">top-5*</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.9.11.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.11.2.12.1" style="color:#000000;">top-20*</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><cite class="ltx_cite ltx_citemacro_citet">Adlakha et&nbsp;al. <span class="ltx_text" id="S3.T1.1.1.1.1.1.2.1.1.1.1" style="color:#000000;">(</span><a class="ltx_ref" href="#bib.bib1" title="">2022</a><span class="ltx_text" id="S3.T1.1.1.1.1.1.2.2.2.2.1" style="color:#000000;">)</span></cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.3.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.4.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.5.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.6.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.7.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.8.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.9.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.10.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.11.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.1.1" style="color:#000000;">70.40</span><math alttext="{}^{\triangle}" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.1.1.m1.1a"><msup id="S3.T1.1.1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.1.1.1.m1.1.1a" xref="S3.T1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mi id="S3.T1.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S3.T1.1.1.1.1.1.1.m1.1.1.1.cmml">△</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1"><ci id="S3.T1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1.1">△</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.1.m1.1c">{}^{\triangle}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT △ end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.12.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1.1.1.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.1.1.1.1.1.13.1" style="color:#000000;">-</span></span></span>
<span class="ltx_tr" id="S3.T1.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.2.2.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><cite class="ltx_cite ltx_citemacro_citet">Wu et&nbsp;al. <span class="ltx_text" id="S3.T1.2.2.2.2.2.2.1.1.1.1" style="color:#000000;">(</span><a class="ltx_ref" href="#bib.bib72" title="">2023</a><span class="ltx_text" id="S3.T1.2.2.2.2.2.2.2.2.2.1" style="color:#000000;">)</span></cite></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.3.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.4.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.5.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.6.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.7.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.8.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.9.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.10.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.11.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.12.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.13.1" style="color:#000000;">-</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.2.2.2.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.2.2.2.2.2.1.1" style="color:#000000;">71.10</span><math alttext="{}^{\triangle}" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.2.1.m1.1"><semantics id="S3.T1.2.2.2.2.2.1.m1.1a"><msup id="S3.T1.2.2.2.2.2.1.m1.1.1" xref="S3.T1.2.2.2.2.2.1.m1.1.1.cmml"><mi id="S3.T1.2.2.2.2.2.1.m1.1.1a" xref="S3.T1.2.2.2.2.2.1.m1.1.1.cmml"></mi><mi id="S3.T1.2.2.2.2.2.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S3.T1.2.2.2.2.2.1.m1.1.1.1.cmml">△</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.2.1.m1.1b"><apply id="S3.T1.2.2.2.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.2.2.2.1.m1.1.1"><ci id="S3.T1.2.2.2.2.2.1.m1.1.1.1.cmml" xref="S3.T1.2.2.2.2.2.1.m1.1.1.1">△</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.2.1.m1.1c">{}^{\triangle}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT △ end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>
<span class="ltx_tr" id="S3.T1.3.3.3.3.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.3.3.3.3.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.1.1" style="color:#000000;">E5-unsupervised</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.3.1.m1.1"><semantics id="S3.T1.3.3.3.3.3.1.m1.1a"><msup id="S3.T1.3.3.3.3.3.1.m1.1.1" xref="S3.T1.3.3.3.3.3.1.m1.1.1.cmml"><mi id="S3.T1.3.3.3.3.3.1.m1.1.1a" xref="S3.T1.3.3.3.3.3.1.m1.1.1.cmml"></mi><mo id="S3.T1.3.3.3.3.3.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.3.3.3.3.3.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.3.1.m1.1b"><apply id="S3.T1.3.3.3.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.3.3.3.1.m1.1.1"><ci id="S3.T1.3.3.3.3.3.1.m1.1.1.1.cmml" xref="S3.T1.3.3.3.3.3.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.3.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.T1.3.3.3.3.3.1.2" style="color:#000000;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.3.3.3.3.3.1.3.1" style="color:#000000;">(</span>Wang et&nbsp;al.<span class="ltx_text" id="S3.T1.3.3.3.3.3.1.4.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="#bib.bib65" title="">2022a</a><span class="ltx_text" id="S3.T1.3.3.3.3.3.1.5.3" style="color:#000000;">)</span></cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.2.1" style="color:#000000;">31.56</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.3.1" style="color:#000000;">59.22</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.4.1" style="color:#000000;">23.02</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.5.1" style="color:#000000;">55.33</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.6.1" style="color:#000000;">43.49</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.7.1" style="color:#000000;">77.68</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.8.1" style="color:#000000;">44.71</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.9.1" style="color:#000000;">84.99</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.10.1" style="color:#000000;">26.25</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.11.1" style="color:#000000;">37.67</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.12.1" style="color:#000000;">20.32</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3.3.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.3.3.3.3.3.13.1" style="color:#000000;">40.44</span></span></span>
<span class="ltx_tr" id="S3.T1.4.4.4.4.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.4.4.4.4.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.1.1" style="color:#000000;">E5-unsupervised + Rewrite</span><math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.4.4.4.4.4.1.m1.1"><semantics id="S3.T1.4.4.4.4.4.1.m1.1a"><msup id="S3.T1.4.4.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.4.4.1.m1.1.1.cmml"><mi id="S3.T1.4.4.4.4.4.1.m1.1.1a" xref="S3.T1.4.4.4.4.4.1.m1.1.1.cmml"></mi><mo id="S3.T1.4.4.4.4.4.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.4.4.4.4.4.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.4.1.m1.1b"><apply id="S3.T1.4.4.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.4.4.1.m1.1.1"><ci id="S3.T1.4.4.4.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.4.4.4.1.m1.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.4.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.2.1" style="color:#000000;">33.23</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.3.1" style="color:#000000;">61.02</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.4.1" style="color:#000000;">25.56</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.5.1" style="color:#000000;">58.00</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.6.1" style="color:#000000;">46.00</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.7.1" style="color:#000000;">80.01</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.8.1" style="color:#000000;">45.50</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.9.1" style="color:#000000;">85.89</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.10.1" style="color:#000000;">27.58</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.11.1" style="color:#000000;">39.15</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.12.1" style="color:#000000;">21.53</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.4.4.4.4.4.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.4.4.4.4.4.13.1" style="color:#000000;">42.04</span></span></span>
<span class="ltx_tr" id="S3.T1.5.5.5.5.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.5.5.5.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.1.1" style="color:#000000;">E5-unsupervised + Fine-tune</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.5.5.5.5.5.1.m1.1"><semantics id="S3.T1.5.5.5.5.5.1.m1.1a"><msup id="S3.T1.5.5.5.5.5.1.m1.1.1" xref="S3.T1.5.5.5.5.5.1.m1.1.1.cmml"><mi id="S3.T1.5.5.5.5.5.1.m1.1.1a" xref="S3.T1.5.5.5.5.5.1.m1.1.1.cmml"></mi><mo id="S3.T1.5.5.5.5.5.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.5.5.5.5.5.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.5.1.m1.1b"><apply id="S3.T1.5.5.5.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.5.5.5.1.m1.1.1"><ci id="S3.T1.5.5.5.5.5.1.m1.1.1.1.cmml" xref="S3.T1.5.5.5.5.5.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.5.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.5.5.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.5.5.5.5.2.1" style="color:#000000;">47.79</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.5.5.5.5.5.3.1" style="color:#000000;">75.00</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.4.1" style="color:#000000;">45.28</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.5.1" style="color:#000000;">80.96</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.6.1" style="color:#000000;">46.52</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.7.1" style="color:#000000;">80.74</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.8.1" style="color:#000000;">53.37</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.9.1" style="color:#000000;">89.91</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.10.1" style="color:#000000;">41.01</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.11.1" style="color:#000000;">51.07</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.12.1" style="color:#000000;">52.79</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.5.5.5.5.5.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.5.5.5.5.5.13.1" style="color:#000000;">72.31</span></span></span>
<span class="ltx_tr" id="S3.T1.6.6.6.6.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.6.6.6.6.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.1.1" style="color:#000000;">Dragon</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.6.6.6.6.6.1.m1.1"><semantics id="S3.T1.6.6.6.6.6.1.m1.1a"><msup id="S3.T1.6.6.6.6.6.1.m1.1.1" xref="S3.T1.6.6.6.6.6.1.m1.1.1.cmml"><mi id="S3.T1.6.6.6.6.6.1.m1.1.1a" xref="S3.T1.6.6.6.6.6.1.m1.1.1.cmml"></mi><mo id="S3.T1.6.6.6.6.6.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.6.6.6.6.6.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.6.1.m1.1b"><apply id="S3.T1.6.6.6.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.6.6.6.1.m1.1.1"><ci id="S3.T1.6.6.6.6.6.1.m1.1.1.1.cmml" xref="S3.T1.6.6.6.6.6.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.6.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.6.6.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.T1.6.6.6.6.6.1.2" style="color:#000000;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S3.T1.6.6.6.6.6.1.3.1" style="color:#000000;">(</span>Lin et&nbsp;al.<span class="ltx_text" id="S3.T1.6.6.6.6.6.1.4.2.1.1" style="color:#000000;">, </span><a class="ltx_ref" href="#bib.bib39" title="">2023a</a><span class="ltx_text" id="S3.T1.6.6.6.6.6.1.5.3" style="color:#000000;">)</span></cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.2.1" style="color:#000000;">46.29</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.3.1" style="color:#000000;">73.09</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.4.1" style="color:#000000;">43.33</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.5.1" style="color:#000000;">75.61</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.6.1" style="color:#000000;">56.80</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.7.1" style="color:#000000;">82.86</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.8.1" style="color:#000000;">46.17</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.9.1" style="color:#000000;">81.96</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.10.1" style="color:#000000;">57.68</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.11.1" style="color:#000000;">78.80</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.12.1" style="color:#000000;">27.49</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.6.6.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.6.6.6.6.6.13.1" style="color:#000000;">46.22</span></span></span>
<span class="ltx_tr" id="S3.T1.7.7.7.7.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.7.7.7.7.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.1.1" style="color:#000000;">Dragon + Rewrite</span><math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.7.7.7.7.7.1.m1.1"><semantics id="S3.T1.7.7.7.7.7.1.m1.1a"><msup id="S3.T1.7.7.7.7.7.1.m1.1.1" xref="S3.T1.7.7.7.7.7.1.m1.1.1.cmml"><mi id="S3.T1.7.7.7.7.7.1.m1.1.1a" xref="S3.T1.7.7.7.7.7.1.m1.1.1.cmml"></mi><mo id="S3.T1.7.7.7.7.7.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.7.7.7.7.7.1.m1.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.7.7.1.m1.1b"><apply id="S3.T1.7.7.7.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.7.7.7.1.m1.1.1"><ci id="S3.T1.7.7.7.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.7.7.7.1.m1.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.7.7.1.m1.1c">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.7.7.7.1.m1.1d">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.7.7.7.7.7.2.1" style="color:#000000;">54.46</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.3.1" style="color:#000000;">80.13</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.4.1" style="color:#000000;">47.60</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.5.1" style="color:#000000;">80.60</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.6.1" style="color:#000000;">47.10</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.7.1" style="color:#000000;">77.15</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.8.1" style="color:#000000;">51.73</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.9.1" style="color:#000000;">85.78</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.10.1" style="color:#000000;">73.07</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.11.1" style="color:#000000;">88.19</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.12.1" style="color:#000000;">52.79</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.7.7.7.7.7.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.7.7.7.7.7.13.1" style="color:#000000;">68.92</span></span></span>
<span class="ltx_tr" id="S3.T1.8.8.8.8.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.8.8.8.8.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.1.1" style="color:#000000;">Dragon + Fine-tune</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.8.8.8.8.8.1.m1.1"><semantics id="S3.T1.8.8.8.8.8.1.m1.1a"><msup id="S3.T1.8.8.8.8.8.1.m1.1.1" xref="S3.T1.8.8.8.8.8.1.m1.1.1.cmml"><mi id="S3.T1.8.8.8.8.8.1.m1.1.1a" xref="S3.T1.8.8.8.8.8.1.m1.1.1.cmml"></mi><mo id="S3.T1.8.8.8.8.8.1.m1.1.1.1" mathcolor="#000000" xref="S3.T1.8.8.8.8.8.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.8.8.1.m1.1b"><apply id="S3.T1.8.8.8.8.8.1.m1.1.1.cmml" xref="S3.T1.8.8.8.8.8.1.m1.1.1"><ci id="S3.T1.8.8.8.8.8.1.m1.1.1.1.cmml" xref="S3.T1.8.8.8.8.8.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.8.8.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.8.8.8.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.2.1" style="color:#000000;">52.72</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.8.8.8.8.8.3.1" style="color:#000000;">80.67</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.4.1" style="color:#000000;">48.94</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.5.1" style="color:#000000;">83.01</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.6.1" style="color:#000000;">52.64</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.7.1" style="color:#000000;">81.95</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.8.1" style="color:#000000;">50.73</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.9.1" style="color:#000000;">87.17</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.10.1" style="color:#000000;">67.86</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.11.1" style="color:#000000;">86.28</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.12.1" style="color:#000000;">43.43</span></span>
<span class="ltx_td ltx_align_center" id="S3.T1.8.8.8.8.8.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.8.8.8.8.8.13.1" style="color:#000000;">64.94</span></span></span>
<span class="ltx_tr" id="S3.T1.9.9.9.9.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.9.9.9.9.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.1.1" style="color:#000000;">- SyntheticConvQA</span><math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S3.T1.9.9.9.9.9.1.m1.1"><semantics id="S3.T1.9.9.9.9.9.1.m1.1a"><msup id="S3.T1.9.9.9.9.9.1.m1.1.1" xref="S3.T1.9.9.9.9.9.1.m1.1.1.cmml"><mi id="S3.T1.9.9.9.9.9.1.m1.1.1a" xref="S3.T1.9.9.9.9.9.1.m1.1.1.cmml"></mi><mi id="S3.T1.9.9.9.9.9.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S3.T1.9.9.9.9.9.1.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.9.9.1.m1.1b"><apply id="S3.T1.9.9.9.9.9.1.m1.1.1.cmml" xref="S3.T1.9.9.9.9.9.1.m1.1.1"><ci id="S3.T1.9.9.9.9.9.1.m1.1.1.1.cmml" xref="S3.T1.9.9.9.9.9.1.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.9.9.1.m1.1c">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.9.9.9.1.m1.1d">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.2.1" style="color:#000000;">52.98</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.9.9.9.9.9.3.1" style="color:#000000;">81.15</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.4.1" style="color:#000000;">48.64</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.5.1" style="color:#000000;">83.47</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.6.1" style="color:#000000;">54.75</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.7.1" style="color:#000000;">83.23</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.8.1" style="color:#000000;">49.63</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.9.1" style="color:#000000;">86.70</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.10.1" style="color:#000000;">64.48</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.11.1" style="color:#000000;">85.24</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.12.1" style="color:#000000;">47.41</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.9.9.9.9.9.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S3.T1.9.9.9.9.9.13.1" style="color:#000000;">67.13</span></span></span>
</span>
</span><span class="ltx_text" id="S3.T1.9.9.9.10" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores.
Compared to rewriting, fine-tuning performs much better on E5-unsupervised and comparable on Dragon.
*Since the average context length in TopiOCQA and INSCIT are smaller than other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively, in other datasets. <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S3.T1.14.m1.1"><semantics id="S3.T1.14.m1.1b"><msup id="S3.T1.14.m1.1.1" xref="S3.T1.14.m1.1.1.cmml"><mi id="S3.T1.14.m1.1.1b" xref="S3.T1.14.m1.1.1.cmml"></mi><mo id="S3.T1.14.m1.1.1.1" xref="S3.T1.14.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.14.m1.1c"><apply id="S3.T1.14.m1.1.1.cmml" xref="S3.T1.14.m1.1.1"><ci id="S3.T1.14.m1.1.1.1.cmml" xref="S3.T1.14.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.m1.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.14.m1.1e">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>The inputs of these two models are concatenation of the dialogue history and current query. <math alttext="{}^{\ddagger}" class="ltx_Math" display="inline" id="S3.T1.15.m2.1"><semantics id="S3.T1.15.m2.1b"><msup id="S3.T1.15.m2.1.1" xref="S3.T1.15.m2.1.1.cmml"><mi id="S3.T1.15.m2.1.1b" xref="S3.T1.15.m2.1.1.cmml"></mi><mo id="S3.T1.15.m2.1.1.1" xref="S3.T1.15.m2.1.1.1.cmml">‡</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T1.15.m2.1c"><apply id="S3.T1.15.m2.1.1.cmml" xref="S3.T1.15.m2.1.1"><ci id="S3.T1.15.m2.1.1.1.cmml" xref="S3.T1.15.m2.1.1.1">‡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.m2.1d">{}^{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.15.m2.1e">start_FLOATSUPERSCRIPT ‡ end_FLOATSUPERSCRIPT</annotation></semantics></math>The inputs of this model is the rewritten query.
<math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S3.T1.16.m3.1"><semantics id="S3.T1.16.m3.1b"><msup id="S3.T1.16.m3.1.1" xref="S3.T1.16.m3.1.1.cmml"><mi id="S3.T1.16.m3.1.1b" xref="S3.T1.16.m3.1.1.cmml"></mi><mi id="S3.T1.16.m3.1.1.1" mathvariant="normal" xref="S3.T1.16.m3.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.16.m3.1c"><apply id="S3.T1.16.m3.1.1.cmml" xref="S3.T1.16.m3.1.1"><ci id="S3.T1.16.m3.1.1.1.cmml" xref="S3.T1.16.m3.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.m3.1d">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.16.m3.1e">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math>denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning. <math alttext="{}^{\triangle}" class="ltx_Math" display="inline" id="S3.T1.17.m4.1"><semantics id="S3.T1.17.m4.1b"><msup id="S3.T1.17.m4.1.1" xref="S3.T1.17.m4.1.1.cmml"><mi id="S3.T1.17.m4.1.1b" xref="S3.T1.17.m4.1.1.cmml"></mi><mi id="S3.T1.17.m4.1.1.1" mathvariant="normal" xref="S3.T1.17.m4.1.1.1.cmml">△</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T1.17.m4.1c"><apply id="S3.T1.17.m4.1.1.cmml" xref="S3.T1.17.m4.1.1"><ci id="S3.T1.17.m4.1.1.1.cmml" xref="S3.T1.17.m4.1.1.1">△</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.m4.1d">{}^{\triangle}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.17.m4.1e">start_FLOATSUPERSCRIPT △ end_FLOATSUPERSCRIPT</annotation></semantics></math>The numbers are not apple-to-apple comparison (e.g., they use training set for fine-tuning).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="S3.F2.g1" src="x2.png" width="823">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of fine-tuning retriever for multi-turn QA.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Retrieval for Multi-Turn QA</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In conversational QA tasks, when a document becomes too lengthy to feed directly into LLMs, a retriever that can handle conversational query becomes essential.
This conversational retriever encodes the concatenation of dialogue history and current query, and then retrieve relevant context from documents. After that, only relevant context will be used as inputs for LLMs.
The state-of-the-art retrievers, e.g., Dragon&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="#bib.bib39" title="">2023a</a>)</cite>, are optimized for single-turn queries, resulting in a limited generalization capability for multi-turn conversational queries.
In Figure&nbsp;<a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>, we depict our retriever fine-tuning method to alleviate this issue. We propose to use conversational query and context pairs for further fine-tuning a single-turn retriever to better cope with conversational inputs.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">An alternative solution is conversational query rewriting method which uses a query rewriter to rewrite the current question based on the conversational history.
The rewritten query is then directly used as the input to a single-turn query retriever for retrieving relevant context.
In addition to the embedding and search cost, the query rewriting model introduces a large amount of extra computational expense to generate the rewritten query.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fine-tuning Retriever for Multi-turn QA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To build a high-quality fine-tuning dataset, we leverage the conversational QA dataset from either the HumanAnnotatedConvQA or the SyntheticConvQA to construct conversational query and context pairs.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">For the HumanAnnotatedConvQA, we directly take the annotations of the conversational query and context pairs, and use them to further fine-tune a single-turn query retriever.
For the SyntheticConvQA, we first cut each document in the conversational QA dataset into different chunks. Then, we calculate the 4-gram recall score between agent’s answer and each chunk. After that, we consider the chunk that has the highest recall score as the gold chunk for the current user’s question. Finally, the constructed conversational query and context pairs are used to fine-tune a single-turn query retriever.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Conversational Query Rewriting</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To build powerful conversational query rewriting model, we take GPT-3.5-turbo as the rewriter given that <cite class="ltx_cite ltx_citemacro_citet">Galimzhanova et&nbsp;al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite> demonstrated the state-of-the-art query rewriting results using GPT-3.5-turbo.
Similar to&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Galimzhanova et&nbsp;al. (<a class="ltx_ref" href="#bib.bib24" title="">2023</a>)</cite>, we not only provide GPT-3.5-turbo with the rewriting task instruction, but also give it few-shot rewriting examples to enhance the quality of rewriting results.
More details can be found in Appendix&nbsp;<a class="ltx_ref" href="#A2.SS1" title="B.1 Query Rewriting Prompts for GPT-3.5-turbo ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">B.1</span></a>.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparisons</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare the quer rewriting and fine-tuning methods across five datasets in the zero-shot setting. More details about these datasets can be found in&nbsp;§<a class="ltx_ref" href="#S5.SS2.SSS1" title="5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>. We conduct experiments on a state-of-the-art retriever, Dragon&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="#bib.bib39" title="">2023a</a>)</cite>, and a strong unsupervised retriever, E5-unsupervised&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="#bib.bib65" title="">2022a</a>)</cite>, which is not finetuned on MS MACRO&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nguyen et&nbsp;al., <a class="ltx_ref" href="#bib.bib48" title="">2016</a>)</cite>.
In terms of the experiments on Dragon, we find that fine-tuning performs marginally worse than query rewriting in average top-1 recall by 1.74%, while it achieves better results on average top-5 recall by 0.54%.
It demonstrates the effectiveness of the fine-tuning approach for the conversational retrieval.
In addition, we observe that the results are comparable between using HumanAnnotatedConvQA and SyntheticConvQA for fine-tuning. This highlights that our human-annotated dataset is in high-quality, and we <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.1">do not rely on</em> ChatGPT models for building the state-of-the-art multi-turn query retriever.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Surprisingly, fine-tuning performs significant better than rewriting on E5-unsupervised. We conjecture that since E5-unsupervised does not use human-annotated query and context pairs in the pre-training stage, leading to a weak generalization to the high-quality rewritten query. In contrast, using high-quality dataset to fine-tune E5-unsupervised brings a giant boost with more than 15% improvements on both average top-1 and top-5 recall scores.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Therefore, fine-tuning a good single-turn retriever on high-quality conversational query context pairs performs on par with leveraging the state-of-the-art rewriter. However, rewriting method requires extra computational time for autoregressive generation process and probably also API cost for using powerful models like GPT-3.5-turbo. In contrast, our proposed multi-turn fine-tuning bypasses these issues.
For the QA evaluations across these five datasets, we consistently use the retrieved top-5 results from the fine-tuning approach for all the QA models. We put more results on comparisons between rewriting and fine-tuning methods in the Appendix&nbsp;<a class="ltx_ref" href="#A2.SS2" title="B.2 More Results for Retrieval in Conversational QA ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">B.2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present the details of our experimental setup for the conversational question answering task.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Baselines</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We conduct experiments on different model sizes. First, to show the effectiveness of stage-2 context-enhanced instruction tuning, we compare against the <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Llama2-SFT-7B/13B/70B</span>, which is the Llama2-7B/13B/70B foundation model after the stage-1 supervised fine-tuning (SFT). Second, we compare against <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.2">Llama2-Chat-7B/13B/70B</span> since Llama2-Chat models are shown to possess strong instruction following and conversational QA capabilities&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a class="ltx_ref" href="#bib.bib60" title="">2023</a>)</cite>.
Aside from Llama2 models, we also conduct experiments on our in-house GPT-8B foundation model, and we compare against its stage-1 SFT baseline (<span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.3">GPT-8B-SFT</span>).
Finally, we compare against two very strong OpenAI models: <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.4">GPT-3.5-turbo (4k)</span> and <span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.5">GPT-4 (8k)</span>.
For fair comparison, we use the same context as inputs for both our models and baselines. Note that we have carefully tuned the instructions for all the baselines to ensure they achieve as good as possible results.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The prompts for these baselines can be found in Appendix&nbsp;<a class="ltx_ref" href="#A3" title="Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">C</span></a>.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Benchmarks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Long Document Datasets</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">We collect five conversational QA datasets with long documents which cannot be directly fitted into LLMs with a sequence length of 4K tokens. Hence, we run a multi-turn retriever to get top-5 relevant chunks as the inputs (experiments can be found in §<a class="ltx_ref" href="#S4.SS3" title="4.3 Comparisons ‣ 4 Retrieval for Multi-Turn QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Doc2Dial&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Feng et&nbsp;al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>)</cite> is a document-grounded conversational QA dataset covering four domains: DMV, SSA, VA, and Student Aid. Each sample comprises a dialogue where a user poses queries regarding the document, and an agent responds those questions. The average document length is around 101K words.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">QuAC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Choi et&nbsp;al., <a class="ltx_ref" href="#bib.bib10" title="">2018</a>)</cite> is based on Wikipedia documents. Originally, the document is short. Since each dialogue is linked to multiple Wikipedia URLs, we extract text from these links to increase the document size to approximately an average of 15K words. It contains unanswerable cases where answers cannot be found within the given context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">QReCC&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Anantha et&nbsp;al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>)</cite> is an open-domain conversational QA dataset across multiple sources.
Similar to QuAC, each dialogue also has corresponding URLs. We extract text from those URLs to construct the documents. In the end, the average document size is around 5K words, with a maximum document size of 20K words.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">TopiOCQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Adlakha et&nbsp;al., <a class="ltx_ref" href="#bib.bib1" title="">2022</a>)</cite> is grounded on the whole Wikipedia. It incorporates topic switching and requires the agent to search the entire Wikipedia for answers to user questions.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1">INSCIT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wu et&nbsp;al., <a class="ltx_ref" href="#bib.bib72" title="">2023</a>)</cite> is also grounded on the whole Wikipedia. It studies the case where user questions are under-specified and require clarification.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">For Doc2Dial, QuAC, and QReCC, we segment documents into around 300-word chunks, and we retrieve top-5 relevant chunks as context for each user question. For TopioCQA and INSCIT, we follow their original segmentation, resulting in smaller chunks. Hence, we retrieved top-20 chunks to obtain similar context length to the first three datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.2" style="width:493.0pt;height:307pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S5.T2.2.2"><span class="ltx_text" id="S5.T2.2.2.2">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.2.2.2.2">
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T2.2.2.2.2.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.2.2.2.2.3.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.1.1" style="color:#000000;">Models</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.2.1" style="color:#000000;">Average</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.3.1" style="color:#000000;">Doc2Dial</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.4.1" style="color:#000000;">QuAC</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.5.1" style="color:#000000;">QReCC</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.6.1" style="color:#000000;">CoQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.7.1" style="color:#000000;">DoQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.8.1" style="color:#000000;">ConvFinQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.9.1" style="color:#000000;">SQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.10.1" style="color:#000000;">TopiOCQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.11.1" style="color:#000000;">HybridDial</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.2.2.2.2.3.1.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.3.1.12.1" style="color:#000000;">INSCIT</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.4.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.4.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.1.1" style="color:#000000;">GPT-8B-SFT</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.2.1" style="color:#000000;">34.46</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.3.1" style="color:#000000;">31.03</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.4.1" style="color:#000000;">20.07</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.5.1" style="color:#000000;">37.69</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.6.1" style="color:#000000;">59.24</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.7.1" style="color:#000000;">21.72</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.8.1" style="color:#000000;">15.44</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.9.1" style="color:#000000;">40.06</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.10.1" style="color:#000000;">38.17</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.11.1" style="color:#000000;">52.29</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.4.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.4.2.12.1" style="color:#000000;">28.86</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.5.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.5.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.1.1" style="color:#000000;">ChatQA-8B</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.5.3.2.1" style="color:#000000;">49.36</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.3.1" style="color:#000000;">36.76</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.4.1" style="color:#000000;">33.95</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.5.1" style="color:#000000;">45.54</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.6.1" style="color:#000000;">77.90</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.7.1" style="color:#000000;">44.65</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.8.1" style="color:#000000;">61.68</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.9.1" style="color:#000000;">60.74</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.10.1" style="color:#000000;">47.03</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.11.1" style="color:#000000;">53.81</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.5.3.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.5.3.12.1" style="color:#000000;">31.50</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.6.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.6.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.1.1" style="color:#000000;">Llama2-7B-SFT</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.2.1" style="color:#000000;">34.81</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.3.1" style="color:#000000;">30.26</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.4.1" style="color:#000000;">19.21</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.5.1" style="color:#000000;">37.55</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.6.1" style="color:#000000;">62.75</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.7.1" style="color:#000000;">21.76</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.8.1" style="color:#000000;">34.43</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.9.1" style="color:#000000;">32.18</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.10.1" style="color:#000000;">32.88</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.11.1" style="color:#000000;">48.96</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.6.4.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.6.4.12.1" style="color:#000000;">28.16</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.7.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.7.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.1.1" style="color:#000000;">Llama2-7B-Chat</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.2.1" style="color:#000000;">38.86</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.3.1" style="color:#000000;">33.27</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.4.1" style="color:#000000;">25.83</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.5.1" style="color:#000000;">46.02</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.6.1" style="color:#000000;">72.28</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.7.1" style="color:#000000;">33.15</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.8.1" style="color:#000000;">36.58</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.9.1" style="color:#000000;">26.14</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.10.1" style="color:#000000;">36.68</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.11.1" style="color:#000000;">47.02</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.7.5.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.7.5.12.1" style="color:#000000;">31.67</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.8.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.8.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.1.1" style="color:#000000;">ChatQA-7B</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.8.6.2.1" style="color:#000000;">47.71</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.3.1" style="color:#000000;">37.88</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.4.1" style="color:#000000;">29.69</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.5.1" style="color:#000000;">46.97</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.6.1" style="color:#000000;">76.61</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.7.1" style="color:#000000;">41.57</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.8.1" style="color:#000000;">51.61</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.9.1" style="color:#000000;">61.87</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.10.1" style="color:#000000;">45.45</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.11.1" style="color:#000000;">54.51</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.8.6.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.8.6.12.1" style="color:#000000;">30.96</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.9.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.9.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.1.1" style="color:#000000;">Llama2-13B-SFT</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.2.1" style="color:#000000;">37.69</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.3.1" style="color:#000000;">30.68</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.4.1" style="color:#000000;">21.59</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.5.1" style="color:#000000;">38.25</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.6.1" style="color:#000000;">69.52</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.7.1" style="color:#000000;">21.70</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.8.1" style="color:#000000;">41.14</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.9.1" style="color:#000000;">37.85</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.10.1" style="color:#000000;">35.26</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.11.1" style="color:#000000;">52.22</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.9.7.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.9.7.12.1" style="color:#000000;">28.73</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.10.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.10.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.1.1" style="color:#000000;">Llama2-13B-Chat</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.2.1" style="color:#000000;">40.34</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.3.1" style="color:#000000;">34.74</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.4.1" style="color:#000000;">27.89</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.5.1" style="color:#000000;">47.19</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.6.1" style="color:#000000;">72.50</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.7.1" style="color:#000000;">32.60</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.8.1" style="color:#000000;">41.54</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.9.1" style="color:#000000;">25.39</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.10.1" style="color:#000000;">39.25</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.11.1" style="color:#000000;">49.82</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.10.8.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.10.8.12.1" style="color:#000000;">32.52</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.11.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.11.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.1.1" style="color:#000000;">ChatQA-13B</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.11.9.2.1" style="color:#000000;">50.86</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.3.1" style="color:#000000;">38.05</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.4.1" style="color:#000000;">34.28</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.5.1" style="color:#000000;">48.06</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.6.1" style="color:#000000;">77.23</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.7.1" style="color:#000000;">43.31</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.8.1" style="color:#000000;">65.44</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.9.1" style="color:#000000;">66.41</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.10.1" style="color:#000000;">48.88</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.11.1" style="color:#000000;">56.19</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.11.9.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.11.9.12.1" style="color:#000000;">30.79</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.12.10">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.12.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.1.1" style="color:#000000;">Llama2-70B-SFT</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.2.1" style="color:#000000;">43.22</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.3.1" style="color:#000000;">34.42</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.4.1" style="color:#000000;">25.65</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.5.1" style="color:#000000;">41.88</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.6.1" style="color:#000000;">73.04</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.7.1" style="color:#000000;">28.21</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.8.1" style="color:#000000;">46.64</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.9.1" style="color:#000000;">58.90</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.10.1" style="color:#000000;">37.20</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.11.1" style="color:#000000;">55.52</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.12.10.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.12.10.12.1" style="color:#000000;">30.71</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.13.11">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.13.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.1.1" style="color:#000000;">Llama2-70B-Chat</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.2.1" style="color:#000000;">45.21</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.3.1" style="color:#000000;">36.87</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.4.1" style="color:#000000;">32.47</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.5.1" style="color:#000000;">49.40</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.6.1" style="color:#000000;">80.41</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.7.1" style="color:#000000;">38.97</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.8.1" style="color:#000000;">46.85</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.9.1" style="color:#000000;">37.62</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.10.1" style="color:#000000;">44.31</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.11.1" style="color:#000000;">50.35</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.13.11.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.13.11.12.1" style="color:#000000;">34.88</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.14.12">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.14.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.1.1" style="color:#000000;">ChatQA-70B</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.14.12.2.1" style="color:#000000;">54.14</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.3.1" style="color:#000000;">38.90</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.4.1" style="color:#000000;">41.82</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.5.1" style="color:#000000;">48.05</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.6.1" style="color:#000000;">78.57</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.7.1" style="color:#000000;">51.94</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.8.1" style="color:#000000;">73.69</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.9.1" style="color:#000000;">69.14</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.10.1" style="color:#000000;">50.98</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.11.1" style="color:#000000;">56.44</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.14.12.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.14.12.12.1" style="color:#000000;">31.90</span></span></span>
<span class="ltx_tr" id="S5.T2.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.1.1" style="color:#000000;">- SyntheticConvQA</span><math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.1.1.m1.1a"><msup id="S5.T2.1.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T2.1.1.1.1.1.1.m1.1.1a" xref="S5.T2.1.1.1.1.1.1.m1.1.1.cmml"></mi><mi id="S5.T2.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S5.T2.1.1.1.1.1.1.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.1.m1.1b"><apply id="S5.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.m1.1.1"><ci id="S5.T2.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.1.m1.1c">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.2.1" style="color:#000000;">54.08</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.3.1" style="color:#000000;">39.19</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.4.1" style="color:#000000;">38.33</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.5.1" style="color:#000000;">48.73</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.6.1" style="color:#000000;">79.83</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.7.1" style="color:#000000;">48.65</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.8.1" style="color:#000000;">76.44</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.9.1" style="color:#000000;">68.63</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.10.1" style="color:#000000;">51.30</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.11.1" style="color:#000000;">55.68</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.1.1.1.1.1.12.1" style="color:#000000;">33.98</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.1.1" style="color:#000000;">- w/o stage-1</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.2.2.1.m1.1a"><msup id="S5.T2.2.2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.2.2.1.m1.1.1.cmml"><mi id="S5.T2.2.2.2.2.2.1.m1.1.1a" xref="S5.T2.2.2.2.2.2.1.m1.1.1.cmml"></mi><mo id="S5.T2.2.2.2.2.2.1.m1.1.1.1" mathcolor="#000000" xref="S5.T2.2.2.2.2.2.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.2.1.m1.1b"><apply id="S5.T2.2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.2.1.m1.1.1"><ci id="S5.T2.2.2.2.2.2.1.m1.1.1.1.cmml" xref="S5.T2.2.2.2.2.2.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.2.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.2.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.2.1" style="color:#000000;">52.18</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.3.1" style="color:#000000;">38.43</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.4.1" style="color:#000000;">37.52</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.5.1" style="color:#000000;">46.08</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.6.1" style="color:#000000;">73.51</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.7.1" style="color:#000000;">49.42</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.8.1" style="color:#000000;">72.15</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.9.1" style="color:#000000;">72.08</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.10.1" style="color:#000000;">51.28</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.11.1" style="color:#000000;">50.74</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.2.12.1" style="color:#000000;">30.56</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.15.13">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.15.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.1.1" style="color:#000000;">- w/o single-turn*</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.2.1" style="color:#000000;">52.25</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.3.1" style="color:#000000;">38.30</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.4.1" style="color:#000000;">37.89</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.5.1" style="color:#000000;">47.08</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.6.1" style="color:#000000;">76.74</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.7.1" style="color:#000000;">46.43</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.8.1" style="color:#000000;">72.42</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.9.1" style="color:#000000;">67.41</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.10.1" style="color:#000000;">49.85</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.11.1" style="color:#000000;">53.16</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.15.13.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.15.13.12.1" style="color:#000000;">33.18</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.16.14">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.2.2.2.2.16.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.1.1" style="color:#000000;">GPT-3.5-turbo (4k)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.2.1" style="color:#000000;">50.37</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.3.1" style="color:#000000;">34.83</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.4.1" style="color:#000000;">37.17</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.5.1" style="color:#000000;">50.46</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.6.1" style="color:#000000;">79.33</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.7.1" style="color:#000000;">41.11</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.8.1" style="color:#000000;">73.15</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.9.1" style="color:#000000;">60.63</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.10.1" style="color:#000000;">44.30</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.11.1" style="color:#000000;">47.42</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.2.2.2.2.16.14.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.16.14.12.1" style="color:#000000;">35.27</span></span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.17.15">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.2.2.2.2.17.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.1.1" style="color:#000000;">GPT-4 (8k)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.2.2.2.2.17.15.2.1" style="color:#000000;">53.90</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.3.1" style="color:#000000;">34.16</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.4.1" style="color:#000000;">40.29</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.5.1" style="color:#000000;">52.01</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.6.1" style="color:#000000;">77.42</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.7.1" style="color:#000000;">43.39</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.8.1" style="color:#000000;">81.28</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.9.1" style="color:#000000;">79.21</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.10.1" style="color:#000000;">45.09</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.11.1" style="color:#000000;">49.81</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.2.2.2.2.17.15.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S5.T2.2.2.2.2.17.15.12.1" style="color:#000000;">36.34</span></span></span>
</span>
</span><span class="ltx_text" id="S5.T2.2.2.2.3" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Zero-shot conversational QA results across 10 datasets. <math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S5.T2.5.m1.1"><semantics id="S5.T2.5.m1.1b"><msup id="S5.T2.5.m1.1.1" xref="S5.T2.5.m1.1.1.cmml"><mi id="S5.T2.5.m1.1.1b" xref="S5.T2.5.m1.1.1.cmml"></mi><mi id="S5.T2.5.m1.1.1.1" mathvariant="normal" xref="S5.T2.5.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S5.T2.5.m1.1c"><apply id="S5.T2.5.m1.1.1.cmml" xref="S5.T2.5.m1.1.1"><ci id="S5.T2.5.m1.1.1.1.cmml" xref="S5.T2.5.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.m1.1d">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.m1.1e">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math> denotes that the HumanAnnotatedConvQA is replaced with the SyntheticConvQA. <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S5.T2.6.m2.1"><semantics id="S5.T2.6.m2.1b"><msup id="S5.T2.6.m2.1.1" xref="S5.T2.6.m2.1.1.cmml"><mi id="S5.T2.6.m2.1.1b" xref="S5.T2.6.m2.1.1.cmml"></mi><mo id="S5.T2.6.m2.1.1.1" xref="S5.T2.6.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T2.6.m2.1c"><apply id="S5.T2.6.m2.1.1.cmml" xref="S5.T2.6.m2.1.1"><ci id="S5.T2.6.m2.1.1.1.cmml" xref="S5.T2.6.m2.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.m2.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.m2.1e">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math> denotes that the stage-1 (SFT) is removed and only ChatQA stage-2 tuning is applied. *denotes that the single-turn QA datasets are removed from the ChatQA stage-2 training blends. Both “w/o stage-1” and “w/o single-turn” settings use the SyntheticConvQA data. In terms of average scores, our ChatQA models greatly surpass SFT and Chat counterparts, and our best model ChatQA-70B slightly outperforms GPT-4.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Short Document Datasets</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">To increase the diversity of document lengths, we collect five conversational QA datasets with short documents (less than 1.5K words). On average, 1 word will be tokenized into &nbsp;1.5 tokens. Hence, the document can be directly fitted into LLMs with a sequence length of 4K tokens.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1">CoQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Reddy et&nbsp;al., <a class="ltx_ref" href="#bib.bib57" title="">2019</a>)</cite> is a conversational QA dataset with each dialogue grounded on a short passage. The answers are generally short, and the passages cover a wide range of domains like children’s stories, literature, mid/high school exams, news, Wikipedia.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1">DoQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Campos et&nbsp;al., <a class="ltx_ref" href="#bib.bib7" title="">2020</a>)</cite> covers three domains: cooking, travel, and movies collected from active Stack Exchange<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stackexchange.com/" title="">https://stackexchange.com/</a></span></span></span> forums. The dataset contains unanswerable cases where answers cannot be found within the given document.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1">ConvFinQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a class="ltx_ref" href="#bib.bib8" title="">2022a</a>)</cite> is based on the Financial domain. Each document contains a single financial report table along with relevant text surrounding the table. This dataset involves arithmetic calculation and complex numerical reasoning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i4.p1">
<p class="ltx_p" id="S5.I2.i4.p1.1">SQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Pasupat &amp; Liang, <a class="ltx_ref" href="#bib.bib52" title="">2015</a>)</cite> is grounded on documents which contain only a single Table without any surrounding text. The documents are collected from Wikipedia, and the questions are highly compositional, which requires the model with robust table comprehension ability to give correct answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S5.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i5.p1">
<p class="ltx_p" id="S5.I2.i5.p1.1">HybridDial&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Nakamura et&nbsp;al., <a class="ltx_ref" href="#bib.bib47" title="">2022</a>)</cite> is a conversational QA dataset grounded on documents containing both Wikipedia tabular and textual data. The questions are complex which requires reasoning over the documents.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">Over all the 10 datasets, ConvFinQA, SQA, and HybridDial datasets contain tabular data in the documents, while documents of the remaining datasets are text-only.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Details of these benchmark datasets are in the Appendix&nbsp;<a class="ltx_ref" href="#A3" title="Appendix C Conversational QA Benchmarks ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">C</span></a>.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluation Metrics</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Given that F1 score is the most commonly used automatic metric to assess QA models, we use it for all datasets except for ConvFinQA. In ConvFinQA, we follow&nbsp;<cite class="ltx_cite ltx_citemacro_citet">Chen et&nbsp;al. (<a class="ltx_ref" href="#bib.bib8" title="">2022a</a>)</cite> to use exact match metric since the answers in ConvFinQA are about extracting numbers from documents as well as arithmetic calculations. Hence, the answer only makes sense when it is exactly the same as the answer. When models generate the arithmetic formula, we will calculate its final result based on a calculator and compare it with the gold answer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">In addition, we also conduct human evaluations to assess the correctness of generated answers between our best model and GPT-4.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Main Results</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>overview</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>, we compare different model variants and OpenAI models across 10 conversational QA datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<p class="ltx_p" id="S6.SS1.SSS1.p2.1">We find that our ChatQA method greatly improves the model’s conversational QA capability.
In terms of averaged scores, Llama2-Chat models surpass SFT model counterparts by a small margin, while our ChatQA models achieve around or over 10 point of absolute improvement over SFT and Chat counterparts. For example, ChatQA-13B improves on Llama2-13B-SFT and Llama2-13B-Chat by 13.17 (from 37.69 to 50.86) and 10.52 (from 40.34 to 50.86), respectively. This is because context-enhanced instruction fine-tuning enables the model to learn how to effectively extract useful information from retrieved or relevant context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p3">
<p class="ltx_p" id="S6.SS1.SSS1.p3.1">Compared to OpenAI models, our best model ChatQA-70B surpasses GPT-3.5-turbo by 3.77 average score and slightly outperforms GPT-4 by 0.24 average score. In addition, a much smaller size of ChatQA-13B is able to marginally outperform GPT-3.5-turbo by an average score of 0.49.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Importance of Stage-1 SFT</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>, we conduct ablation study on the importance of stage-1 SFT, which enhance the instruction following capability of ChatQA. We remove the stage-1 SFT from the fine-tuning stages and only apply stage-2 context-enhanced instruction tuning on top of foundation LLM. We find that the average score drops 1.9 (from 54.08 to 52.18). Aside from the SQA, removing stage-1 makes the model perform consistently worse on other datasets. Results indicate that stage-1 still plays an important role, even through all of SFT datasets of stage-1 are blended in stage-2 instruction tuning as well.
We figure that building instruction-following capability first is beneficial for stage-2 tuning.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>Effectiveness of Single-Turn Data</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">To investigate how single-turn QA datasets affect model’s multi-turn QA capability, we conduct ablation study by removing them from the ChatQA-70B training blends in stage-2.
As shown in Table&nbsp;<a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>, incorporating single-turn QA datasets in the stage-2 training blends (ChatQA-70B) generally make the scores increase across all benchmark datasets, which leads to an average improvement of 1.83 score. Interestingly, we observe improvement in ConvFinQA, SQA, and HybridDial (table-based datasets), despite the added single-turn QA datasets do no have tabular data in the documents.
These results align with our intuitions. Adding single-turn datasets improves the model’s capability to extract answers from context, resulting in better scores in conversational QA datasets.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4 </span>Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS4.p1">
<p class="ltx_p" id="S6.SS1.SSS4.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>, we also compare our ChatQA models using the 7k GPT-3.5-Turbo synthetic dataset (SyntheticConvQA) and our collected 7k human-annotated dataset (HumanAnnotatedConvQA). First, we find that both achieve comparable results in terms of average scores, which suggests that we do not need to rely on synthetic data from OpenAI models to build the state-of-the-art conversational QA models. Second, we find that using human-annotated data achieved significant improvements on QuAC and DoQA datasets. This can be attributed to the fact that the human-annotated data have higher quality on unanswerable cases which exists in QuAC and DoQA datasets. Eventually, it leads to the overall improvements on these two datasets. Detail results and analyses on unanswerable cases can be found in §<a class="ltx_ref" href="#S6.SS5" title="6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6.5</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.3" style="width:182.7pt;height:216pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T3.3.1"><span class="ltx_text" id="S6.T3.3.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.3.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S6.T3.3.1.1.1.1.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T3.3.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.1.1.2.1" style="color:#000000;">Ours Win</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.1.1.3.1" style="color:#000000;">Tie</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T3.3.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.1.1.4.1" style="color:#000000;">GPT-4 Win</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S6.T3.3.1.1.1.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.2.2.1.1" style="color:#000000;">Average</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.1.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.2.2.2.1" style="color:#000000;">13.81%</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.1.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.2.2.3.1" style="color:#000000;">69.09%</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.1.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.2.2.4.1" style="color:#000000;">17.10%</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T3.3.1.1.1.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.3.1.1.1.3.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.3.1.1.1" style="color:#000000;">Doc2Dial</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.1.1.3.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.3.1.2.1" style="color:#000000;">14.29%</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.1.1.3.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.3.1.3.1" style="color:#000000;">68.00%</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.3.1.1.1.3.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.3.1.4.1" style="color:#000000;">17.71%</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.4.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.4.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.4.2.1.1" style="color:#000000;">QuAC</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.4.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.4.2.2.1" style="color:#000000;">11.67%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.4.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.4.2.3.1" style="color:#000000;">73.33%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.4.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.4.2.4.1" style="color:#000000;">15.00%</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.5.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.5.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.5.3.1.1" style="color:#000000;">QReCC</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.5.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.5.3.2.1" style="color:#000000;">11.11%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.5.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.5.3.3.1" style="color:#000000;">77.22%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.5.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.5.3.4.1" style="color:#000000;">11.67%</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.6.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.6.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.6.4.1.1" style="color:#000000;">CoQA</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.6.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.6.4.2.1" style="color:#000000;">7.78%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.6.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.6.4.3.1" style="color:#000000;">80.00%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.6.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.6.4.4.1" style="color:#000000;">12.22%</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.7.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.7.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.7.5.1.1" style="color:#000000;">DoQA</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.7.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.7.5.2.1" style="color:#000000;">22.78%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.7.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.7.5.3.1" style="color:#000000;">57.78%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.7.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.7.5.4.1" style="color:#000000;">19.44%</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.8.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.8.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.8.6.1.1" style="color:#000000;">ConvFinQA</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.8.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.8.6.2.1" style="color:#000000;">16.67%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.8.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.8.6.3.1" style="color:#000000;">67.78%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.8.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.8.6.4.1" style="color:#000000;">15.55%</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.9.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.9.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.9.7.1.1" style="color:#000000;">SQA</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.9.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.9.7.2.1" style="color:#000000;">11.11%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.9.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.9.7.3.1" style="color:#000000;">61.67%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.9.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.9.7.4.1" style="color:#000000;">27.22%</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.10.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.10.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.10.8.1.1" style="color:#000000;">TopiOCQA</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.10.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.10.8.2.1" style="color:#000000;">19.31%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.10.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.10.8.3.1" style="color:#000000;">60.69%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.10.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.10.8.4.1" style="color:#000000;">20.00%</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.11.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T3.3.1.1.1.11.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.11.9.1.1" style="color:#000000;">HybridDial</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.11.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.11.9.2.1" style="color:#000000;">7.78%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.11.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.11.9.3.1" style="color:#000000;">78.33%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T3.3.1.1.1.11.9.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.11.9.4.1" style="color:#000000;">13.89%</span></span></span>
<span class="ltx_tr" id="S6.T3.3.1.1.1.12.10">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T3.3.1.1.1.12.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.12.10.1.1" style="color:#000000;">INSCIT</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.1.1.12.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.12.10.2.1" style="color:#000000;">15.56%</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.1.1.12.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.12.10.3.1" style="color:#000000;">66.11%</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.3.1.1.1.12.10.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T3.3.1.1.1.12.10.4.1" style="color:#000000;">18.33%</span></span></span>
</span>
</span><span class="ltx_text" id="S6.T3.3.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Human evaluation (A/B testing) comparing our ChatQA-70B to GPT-4 over 10 datasets. In terms of average scores, our model and GPT-4 are tie most of the time (69.09%), and GPT-4 achieves slightly higher win rate (<math alttext="\sim" class="ltx_Math" display="inline" id="S6.T3.2.m1.1"><semantics id="S6.T3.2.m1.1b"><mo id="S6.T3.2.m1.1.1" xref="S6.T3.2.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.m1.1c"><csymbol cd="latexml" id="S6.T3.2.m1.1.1.cmml" xref="S6.T3.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.m1.1d">\sim</annotation><annotation encoding="application/x-llamapun" id="S6.T3.2.m1.1e">∼</annotation></semantics></math>3.3%) than ours.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.5 </span>Human Evaluation</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS1.SSS5.p1">
<p class="ltx_p" id="S6.SS1.SSS5.p1.1">Despite F1 scores are the most commonly used metrics for evaluating the quality of QA models, there are often multiple ways to answer questions, which makes the automatic metrics less than perfect. Therefore, we use human evaluations to further compare our ChatQA-70B with GPT-4. In this human evaluation, we ask annotators to verify the facts in ChatQA-70B and GPT-4’s outputs and determine which model provides a more accurate response to the question<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>More human evaluation setup can be found in the Appendix&nbsp;<a class="ltx_ref" href="#A4" title="Appendix D Human Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">D</span></a>.</span></span></span>.
The human evaluation results over the 10 datasets are shown in Table&nbsp;<a class="ltx_ref" href="#S6.T3" title="Table 3 ‣ 6.1.4 Human Annotated Data vs. GPT-3.5-Turbo Synthetic Data ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">3</span></a>. We first find that our ChatQA-70B and GPT-4 are tie most of the time (69.09%), and GPT-4 achieves slightly higher win rate (<math alttext="\sim" class="ltx_Math" display="inline" id="S6.SS1.SSS5.p1.1.m1.1"><semantics id="S6.SS1.SSS5.p1.1.m1.1a"><mo id="S6.SS1.SSS5.p1.1.m1.1.1" xref="S6.SS1.SSS5.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS5.p1.1.m1.1b"><csymbol cd="latexml" id="S6.SS1.SSS5.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS5.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS5.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS5.p1.1.m1.1d">∼</annotation></semantics></math>3.3%) than ours. This further confirms our model has powerful capability to produce correct answers. Second, we find that our model achieves slightly better win rate than GPT-4 in ConvFinQA, which indicates the strong arithmetic calculation ability of our model. Third, we find that GPT-4 achieves significantly better win rate in SQA task, which suggests that there is still a gap between our model and GPT-4 on tabular reasoning task.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T4.1" style="width:254.5pt;height:90pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T4.1.1"><span class="ltx_text" id="S6.T4.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S6.T4.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T4.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.1.1" style="color:#000000;">Models</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.2.1" style="color:#000000;">Avg-text</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.3.1" style="color:#000000;">Avg-table</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.4.1" style="color:#000000;">Avg-ret</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.1.1.5.1" style="color:#000000;">Avg-nonret</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T4.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.1.1" style="color:#000000;">ChatQA-13B</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.2.1" style="color:#000000;">45.80</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.3.1" style="color:#000000;">62.68</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.2.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.4.1" style="color:#000000;">40.01</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.2.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.2.1.5.1" style="color:#000000;">61.72</span></span></span>
<span class="ltx_tr" id="S6.T4.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.3.2.1.1" style="color:#000000;">ChatQA-70B</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.1.1.1.1.3.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.3.2.2.1" style="color:#000000;">48.88</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.1.1.1.1.3.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.3.2.3.1" style="color:#000000;">66.42</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.1.1.1.1.3.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.3.2.4.1" style="color:#000000;">42.33</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.1.1.1.1.3.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.3.2.5.1" style="color:#000000;">65.96</span></span></span>
<span class="ltx_tr" id="S6.T4.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.1.1.1.1.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.1.1" style="color:#000000;">GPT-3.5-turbo (4k)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.4.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.2.1" style="color:#000000;">46.07</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.4.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.3.1" style="color:#000000;">60.40</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.4.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.4.1" style="color:#000000;">40.41</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.1.1.1.1.4.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.4.3.5.1" style="color:#000000;">60.33</span></span></span>
<span class="ltx_tr" id="S6.T4.1.1.1.1.5.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T4.1.1.1.1.5.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.5.4.1.1" style="color:#000000;">GPT-4 (8k)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.1.1.1.5.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.5.4.2.1" style="color:#000000;">46.96</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.1.1.1.5.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.5.4.3.1" style="color:#000000;">70.10</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.1.1.1.5.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T4.1.1.1.1.5.4.4.1" style="color:#000000;">41.58</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.1.1.1.1.5.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T4.1.1.1.1.5.4.5.1" style="color:#000000;">66.22</span></span></span>
</span>
</span><span class="ltx_text" id="S6.T4.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Fine-grained studies on average scores of different dataset types. Avg-text covers datasets where the documents only have text, including Doc2Dial, QuAC, QReCC, CoQA, DoQA, TopiOCQA, and INSCIT.
Avg-table covers datasets having table in the documents, including ConvFinQA, SQA, and HybridDial.
Avg-ret covers datasets having long documents requiring retrieval, including Doc2Dial, QuAC, QReCC, TopiOCQA, and INSCIT.
Avg-nonret covers datasets having short documents which do not require retrieval, including CoQA, DoQA, ConvFinQA, SQA, and HybridDial.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Fine-grained Analyses</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S6.T4" title="Table 4 ‣ 6.1.5 Human Evaluation ‣ 6.1 Main Results ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">4</span></a>, we further compared our models and OpenAI models across different dataset types in our conversational QA benchmarks. In the comparison between ChatQA-70B and GPT-4, ChatQA-70B achieves better results in text-only documents (avg-text), which demonstrate its superior text understanding ability. While, GPT-4 shows better QA capability in tabular data given the comparisons in avg-table.
As for the datasets that require or do not require retrieval, ChatQA-70B and GPT-4 are comparable (gaps are within an average score of 1).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">In the comparison between ChatQA-13B and GPT-3.5-turbo, ChatQA-13B demonstrates better tabular QA capability given the scores in Avg-table. ChatQA-13B also shows better scores in datasets where documents that do not need retrieval, while it is on par with GPT-3.5-turbo in text-only documents and documents that need retrieval.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T5.1" style="width:207.1pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T5.1.1"><span class="ltx_text" id="S6.T5.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T5.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S6.T5.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T5.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.1.1.1.1" style="color:#000000;">Models</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.1.1.2.1" style="color:#000000;">Avg-all</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.1.1.3.1" style="color:#000000;">Avg-ret</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T5.1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.1.1.4.1" style="color:#000000;">Avg-nonret</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T5.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T5.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.2.1.1.1" style="color:#000000;">ChatQA-70B</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.2.1.2.1" style="color:#000000;">54.14</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.2.1.3.1" style="color:#000000;">42.33</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1.1.2.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.2.1.4.1" style="color:#000000;">65.96</span></span></span>
<span class="ltx_tr" id="S6.T5.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T5.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.3.2.1.1" style="color:#000000;">- w/ “top-5” chunks</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.1.1.1.3.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.3.2.2.1" style="color:#000000;">54.04</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.1.1.1.3.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.1.1.3.2.3.1" style="color:#000000;">42.91</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.1.1.1.3.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T5.1.1.1.1.3.2.4.1" style="color:#000000;">65.16</span></span></span>
</span>
</span><span class="ltx_text" id="S6.T5.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation study on using “top-5” retrieved chunk as the context for the stage-2 instruction tuning. We report average scores on all datasets (Avg-all), five retrieval datasets (Avg-ret) and five non-retrieval datasets (Avg-nonret).</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T6.1" style="width:284.7pt;height:144pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T6.1.1"><span class="ltx_text" id="S6.T6.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T6.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="S6.T6.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T6.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.1.1" style="color:#000000;">Models</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.2.1" style="color:#000000;">Avg.</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.3.1" style="color:#000000;">D2D</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.4.1" style="color:#000000;">QuAC</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.5.1" style="color:#000000;">QReCC</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.6.1" style="color:#000000;">TopiO</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S6.T6.1.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.1.1.7.1" style="color:#000000;">INSCIT</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T6.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T6.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.1.1" style="color:#000000;">ChatQA-70B</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.2.1" style="color:#000000;">42.31</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.3.1" style="color:#000000;">39.19</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.4.1" style="color:#000000;">38.33</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.5.1" style="color:#000000;">48.73</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.6.1" style="color:#000000;">51.30</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.1.1.1.1.2.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.2.1.7.1" style="color:#000000;">33.98</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.1.1" style="color:#000000;">- # of ctx: top-3</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.2.1" style="color:#000000;">41.91</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.3.1" style="color:#000000;">37.20</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.4.1" style="color:#000000;">38.35</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.5.1" style="color:#000000;">48.94</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.6.1" style="color:#000000;">52.78</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.3.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.3.2.7.1" style="color:#000000;">32.27</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.1.1" style="color:#000000;">- # of ctx: top-10</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.2.1" style="color:#000000;">40.71</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.3.1" style="color:#000000;">37.06</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.4.1" style="color:#000000;">36.95</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.5.1" style="color:#000000;">47.61</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.6.1" style="color:#000000;">49.40</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.4.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.4.3.7.1" style="color:#000000;">32.53</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.1.1.5.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.5.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.1.1" style="color:#000000;">- ctx reverse ordering</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.2.1" style="color:#000000;">42.48</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.3.1" style="color:#000000;">39.08</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.4.1" style="color:#000000;">38.85</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.5.1" style="color:#000000;">49.63</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.6.1" style="color:#000000;">51.16</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.5.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.5.4.7.1" style="color:#000000;">33.69</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.1.1.6.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.6.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.1.1" style="color:#000000;">- ctx swing ordering</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.2.1" style="color:#000000;">42.30</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.3.1" style="color:#000000;">39.35</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.4.1" style="color:#000000;">38.09</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.5.1" style="color:#000000;">49.09</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.6.1" style="color:#000000;">50.98</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.6.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.6.5.7.1" style="color:#000000;">33.99</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.1.1.7.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.1.1.1.1.7.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.1.1" style="color:#000000;">- ctx random ordering</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.2.1" style="color:#000000;">42.01</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.3.1" style="color:#000000;">39.32</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.4.1" style="color:#000000;">38.28</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.5.1" style="color:#000000;">48.79</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.6.1" style="color:#000000;">50.13</span></span>
<span class="ltx_td ltx_align_center" id="S6.T6.1.1.1.1.7.6.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.7.6.7.1" style="color:#000000;">33.51</span></span></span>
<span class="ltx_tr" id="S6.T6.1.1.1.1.8.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T6.1.1.1.1.8.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.1.1" style="color:#000000;">- Dragon Retrieval</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.2.1" style="color:#000000;">40.50</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.3.1" style="color:#000000;">37.92</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.4.1" style="color:#000000;">38.44</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.5.1" style="color:#000000;">47.88</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.6.1" style="color:#000000;">50.39</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.1.1.1.1.8.7.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T6.1.1.1.1.8.7.7.1" style="color:#000000;">27.87</span></span></span>
</span>
</span><span class="ltx_text" id="S6.T6.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Ablation studies on input context across datasets that need retrieval. All models are using the SyntheticConvQA. D2D denotes Doc2Dial, and TopiO denotes TopiOCQA. We study number of contexts used in inputs (# of ctx), context ordering (reverse, swing, random), and using retrieved context from original Dragon. In comparison, ChatQA-70B (default setting) is using “Dragon + Fine-tune” retrieved top-5 contexts, and the sequential ordering from first to fifth context in top-5.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S6.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T7.5" style="width:498.6pt;height:163pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T7.5.5"><span class="ltx_text" id="S6.T7.5.5.5">
<span class="ltx_tabular ltx_align_middle" id="S6.T7.5.5.5.5">
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T7.5.5.5.5.6.1">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T7.5.5.5.5.6.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.1.1" style="color:#000000;">Models</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T7.5.5.5.5.6.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.2.1" style="color:#000000;">Avg-Both</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.3.1" style="color:#000000;">Avg-QuAC</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.4.1" style="color:#000000;">QuAC (no*)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T7.5.5.5.5.6.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.5.1" style="color:#000000;">QuAC (yes*)</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.6.1" style="color:#000000;">Avg-DoQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.7.1" style="color:#000000;">DoQA (no*)</span></span>
<span class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="S6.T7.5.5.5.5.6.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.8.1" style="color:#000000;">DoQA (yes*)</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.5.5.5.5.6.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.6.1.9.1" style="color:#000000;">Avg-CQA</span></span></span>
<span class="ltx_tr" id="S6.T7.5.5.5.5.7.2">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.7.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.7.2.1.1" style="color:#000000;">ChatQA-70B</span></span>
<span class="ltx_td ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.7.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.7.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_rr ltx_border_t" id="S6.T7.5.5.5.5.7.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_t" id="S6.T7.5.5.5.5.7.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span></span>
<span class="ltx_tr" id="S6.T7.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.1.1" style="color:#000000;">- 1k unanswerable</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.1.1.1.1.1.1.m1.1"><semantics id="S6.T7.1.1.1.1.1.1.m1.1a"><msup id="S6.T7.1.1.1.1.1.1.m1.1.1" xref="S6.T7.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S6.T7.1.1.1.1.1.1.m1.1.1a" xref="S6.T7.1.1.1.1.1.1.m1.1.1.cmml"></mi><mo id="S6.T7.1.1.1.1.1.1.m1.1.1.1" mathcolor="#000000" xref="S6.T7.1.1.1.1.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.1.1.1.1.1.1.m1.1b"><apply id="S6.T7.1.1.1.1.1.1.m1.1.1.cmml" xref="S6.T7.1.1.1.1.1.1.m1.1.1"><ci id="S6.T7.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S6.T7.1.1.1.1.1.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.1.1.1.1.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.2.1" style="color:#000000;">76.88</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.3.1" style="color:#000000;">80.89</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.4.1" style="color:#000000;">75.10</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.5.1" style="color:#000000;">86.67</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.6.1" style="color:#000000;">72.88</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.7.1" style="color:#000000;">64.49</span></span>
<span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.1.1.1.1.1.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.1.1.1.1.1.8.1" style="color:#000000;">81.26</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.1.1.1.1.1.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.1.1.1.9.1" style="color:#000000;">54.16</span></span></span>
<span class="ltx_tr" id="S6.T7.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.2.2.2.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.1.2" style="color:#000000;">- </span><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.1.1" style="color:#000000;">1.5k unanswerable<math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.2.2.2.2.2.1.1.m1.1"><semantics id="S6.T7.2.2.2.2.2.1.1.m1.1a"><msup id="S6.T7.2.2.2.2.2.1.1.m1.1.1" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1.cmml"><mi id="S6.T7.2.2.2.2.2.1.1.m1.1.1a" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1.cmml"></mi><mo id="S6.T7.2.2.2.2.2.1.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.2.2.2.2.2.1.1.m1.1b"><apply id="S6.T7.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1"><ci id="S6.T7.2.2.2.2.2.1.1.m1.1.1.1.cmml" xref="S6.T7.2.2.2.2.2.1.1.m1.1.1.1">normal-†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.2.2.2.2.2.1.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.2.2.2.2.2.1.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.2.2.2.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.2.1" style="color:#000000;">77.25</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.3.1" style="color:#000000;">80.76</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.4.1" style="color:#000000;">77.66</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.2.2.2.2.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.5.1" style="color:#000000;">83.85</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.6.1" style="color:#000000;">73.74</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.7.1" style="color:#000000;">68.81</span></span>
<span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.2.2.2.2.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.2.2.2.2.2.8.1" style="color:#000000;">78.67</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.2.2.2.2.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.2.2.2.2.2.9.1" style="color:#000000;">54.14</span></span></span>
<span class="ltx_tr" id="S6.T7.3.3.3.3.3">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.3.3.3.3.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.1.1" style="color:#000000;">- 2k unanswerable</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.3.3.3.3.3.1.m1.1"><semantics id="S6.T7.3.3.3.3.3.1.m1.1a"><msup id="S6.T7.3.3.3.3.3.1.m1.1.1" xref="S6.T7.3.3.3.3.3.1.m1.1.1.cmml"><mi id="S6.T7.3.3.3.3.3.1.m1.1.1a" xref="S6.T7.3.3.3.3.3.1.m1.1.1.cmml"></mi><mo id="S6.T7.3.3.3.3.3.1.m1.1.1.1" mathcolor="#000000" xref="S6.T7.3.3.3.3.3.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.3.3.3.3.3.1.m1.1b"><apply id="S6.T7.3.3.3.3.3.1.m1.1.1.cmml" xref="S6.T7.3.3.3.3.3.1.m1.1.1"><ci id="S6.T7.3.3.3.3.3.1.m1.1.1.1.cmml" xref="S6.T7.3.3.3.3.3.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.3.3.3.3.3.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.3.3.3.3.3.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.3.3.3.3.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.2.1" style="color:#000000;">77.10</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.3.1" style="color:#000000;">80.82</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.4.1" style="color:#000000;">77.59</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.3.3.3.3.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.5.1" style="color:#000000;">84.05</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.6.1" style="color:#000000;">73.38</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.7.1" style="color:#000000;">67.95</span></span>
<span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.3.3.3.3.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.8.1" style="color:#000000;">78.80</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.3.3.3.3.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.3.3.3.3.3.9.1" style="color:#000000;">53.86</span></span></span>
<span class="ltx_tr" id="S6.T7.4.4.4.4.4">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.4.4.4.4.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.1.1" style="color:#000000;">- 2.5k unanswerable</span><math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.4.4.4.4.4.1.m1.1"><semantics id="S6.T7.4.4.4.4.4.1.m1.1a"><msup id="S6.T7.4.4.4.4.4.1.m1.1.1" xref="S6.T7.4.4.4.4.4.1.m1.1.1.cmml"><mi id="S6.T7.4.4.4.4.4.1.m1.1.1a" xref="S6.T7.4.4.4.4.4.1.m1.1.1.cmml"></mi><mo id="S6.T7.4.4.4.4.4.1.m1.1.1.1" mathcolor="#000000" xref="S6.T7.4.4.4.4.4.1.m1.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.4.4.4.4.4.1.m1.1b"><apply id="S6.T7.4.4.4.4.4.1.m1.1.1.cmml" xref="S6.T7.4.4.4.4.4.1.m1.1.1"><ci id="S6.T7.4.4.4.4.4.1.m1.1.1.1.cmml" xref="S6.T7.4.4.4.4.4.1.m1.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.4.4.4.4.4.1.m1.1c">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.4.4.4.4.4.1.m1.1d">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.4.4.4.4.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.2.1" style="color:#000000;">75.87</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.3.1" style="color:#000000;">78.81</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.4.1" style="color:#000000;">73.76</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.4.4.4.4.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.5.1" style="color:#000000;">83.85</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.6.1" style="color:#000000;">72.93</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.7.1" style="color:#000000;">66.54</span></span>
<span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.4.4.4.4.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.8.1" style="color:#000000;">79.31</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.4.4.4.4.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.4.4.4.4.4.9.1" style="color:#000000;">53.78</span></span></span>
<span class="ltx_tr" id="S6.T7.5.5.5.5.5">
<span class="ltx_td ltx_align_left ltx_border_r" id="S6.T7.5.5.5.5.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.1.1" style="color:#000000;">- SyntheticConvQA</span><math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S6.T7.5.5.5.5.5.1.m1.1"><semantics id="S6.T7.5.5.5.5.5.1.m1.1a"><msup id="S6.T7.5.5.5.5.5.1.m1.1.1" xref="S6.T7.5.5.5.5.5.1.m1.1.1.cmml"><mi id="S6.T7.5.5.5.5.5.1.m1.1.1a" xref="S6.T7.5.5.5.5.5.1.m1.1.1.cmml"></mi><mi id="S6.T7.5.5.5.5.5.1.m1.1.1.1" mathcolor="#000000" mathvariant="normal" xref="S6.T7.5.5.5.5.5.1.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S6.T7.5.5.5.5.5.1.m1.1b"><apply id="S6.T7.5.5.5.5.5.1.m1.1.1.cmml" xref="S6.T7.5.5.5.5.5.1.m1.1.1"><ci id="S6.T7.5.5.5.5.5.1.m1.1.1.1.cmml" xref="S6.T7.5.5.5.5.5.1.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.5.5.5.5.5.1.m1.1c">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.5.5.5.5.5.1.m1.1d">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.5.5.5.5.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.2.1" style="color:#000000;">69.84</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.3.1" style="color:#000000;">72.92</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.4.1" style="color:#000000;">55.38</span></span>
<span class="ltx_td ltx_align_center ltx_border_r" id="S6.T7.5.5.5.5.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.5.1" style="color:#000000;">90.42</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.6.1" style="color:#000000;">66.77</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.7.1" style="color:#000000;">45.09</span></span>
<span class="ltx_td ltx_align_center ltx_border_rr" id="S6.T7.5.5.5.5.5.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.8.1" style="color:#000000;">88.45</span></span>
<span class="ltx_td ltx_align_center" id="S6.T7.5.5.5.5.5.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.5.9.1" style="color:#000000;">54.08</span></span></span>
<span class="ltx_tr" id="S6.T7.5.5.5.5.8.3">
<span class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.8.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.1.1" style="color:#000000;">GPT-3.5-turbo (4k)</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.8.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.2.1" style="color:#000000;">73.27</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.3.1" style="color:#000000;">78.34</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.4.1" style="color:#000000;">61.91</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T7.5.5.5.5.8.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.5.1" style="color:#000000;">94.76</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.6.1" style="color:#000000;">68.21</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.7.1" style="color:#000000;">51.99</span></span>
<span class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S6.T7.5.5.5.5.8.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.8.1" style="color:#000000;">84.43</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.5.5.5.5.8.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.8.3.9.1" style="color:#000000;">50.37</span></span></span>
<span class="ltx_tr" id="S6.T7.5.5.5.5.9.4">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S6.T7.5.5.5.5.9.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.1.1" style="color:#000000;">GPT-4 (8k)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T7.5.5.5.5.9.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.5.5.5.5.9.4.2.1" style="color:#000000;">80.73</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.5.5.5.5.9.4.3.1" style="color:#000000;">87.42</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.4.1" style="color:#000000;">83.45</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T7.5.5.5.5.9.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.5.1" style="color:#000000;">91.38</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.5.5.5.5.9.4.6.1" style="color:#000000;">74.05</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.7.1" style="color:#000000;">74.28</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S6.T7.5.5.5.5.9.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T7.5.5.5.5.9.4.8.1" style="color:#000000;">73.82</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.5.5.5.5.9.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T7.5.5.5.5.9.4.9.1" style="color:#000000;">53.90</span></span></span>
</span>
</span><span class="ltx_text" id="S6.T7.5.5.5.6" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Accuracies on answerable and unanswerable samples across QuAC and DoQA datasets. Avg-Both is the averaged score between QuAC and DoQA. <math alttext="{}^{\diamondsuit}" class="ltx_Math" display="inline" id="S6.T7.8.m1.1"><semantics id="S6.T7.8.m1.1b"><msup id="S6.T7.8.m1.1.1" xref="S6.T7.8.m1.1.1.cmml"><mi id="S6.T7.8.m1.1.1b" xref="S6.T7.8.m1.1.1.cmml"></mi><mi id="S6.T7.8.m1.1.1.1" mathvariant="normal" xref="S6.T7.8.m1.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="S6.T7.8.m1.1c"><apply id="S6.T7.8.m1.1.1.cmml" xref="S6.T7.8.m1.1.1"><ci id="S6.T7.8.m1.1.1.1.cmml" xref="S6.T7.8.m1.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.8.m1.1d">{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.8.m1.1e">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math> denotes that the HumanAnnotatedConvQA is replaced with the SyntheticConvQA. *“no” denotes the unanswerable samples, while “yes” denotes the answerable samples. <math alttext="{}^{\dagger}" class="ltx_Math" display="inline" id="S6.T7.9.m2.1"><semantics id="S6.T7.9.m2.1b"><msup id="S6.T7.9.m2.1.1" xref="S6.T7.9.m2.1.1.cmml"><mi id="S6.T7.9.m2.1.1b" xref="S6.T7.9.m2.1.1.cmml"></mi><mo id="S6.T7.9.m2.1.1.1" xref="S6.T7.9.m2.1.1.1.cmml">†</mo></msup><annotation-xml encoding="MathML-Content" id="S6.T7.9.m2.1c"><apply id="S6.T7.9.m2.1.1.cmml" xref="S6.T7.9.m2.1.1"><ci id="S6.T7.9.m2.1.1.1.cmml" xref="S6.T7.9.m2.1.1.1">†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.9.m2.1d">{}^{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S6.T7.9.m2.1e">start_FLOATSUPERSCRIPT † end_FLOATSUPERSCRIPT</annotation></semantics></math>We conduct ablation studies in terms of the number of unanswerable samples in HumanAnnotatedConvQA. Avg-CQA is the average score across the 10 conversational QA datasets. We use 1.5k unanswerable samples for our final ChatQA-70B, as it produces both high-quality generation and less hallucination. </figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Top-<em class="ltx_emph ltx_font_italic" id="S6.SS3.1.1">k</em> Chunks for Stage-2 Instruction Tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">For all the datasets used by stage-2 tuning, the context are provided as continuous paragraphs or documents that contain the answer. In contrast, the model needs to handle the top-<em class="ltx_emph ltx_font_italic" id="S6.SS3.p1.1.1">k</em> retrieved chunks at inference for long documents.
To reduce such train/test mismatch, we investigate whether replacing some continuous paragraphs with retrieved top-k chunks will enhance the model’s robustness.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">We use NarrativeQA for this study, since each question has a corresponding long document. For NarrativeQA, we originally use a summary of the long document as the context, which contains the answer. To incorporate discontinuous contexts, we first cut the long document into 300-word chunks. Then, we use Dragon retriever to retrieve top-4 chunks to the question as the additional context. Finally, we take the retrieved four chunks and the summary of the long document as the “top-5” chunks.
<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Note that, we did not directly use top-5 retrieved chunks for training, because they may not contain the answer. In such cases, fine-tuning the model to generate answer could encourage hallucination.</span></span></span>
We use this reconstructed NarrativeQA to replace the original one for the stage-2 instruction tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">In Table&nbsp;<a class="ltx_ref" href="#S6.T5" title="Table 5 ‣ 6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">5</span></a>, we find that using “top-5” chunks as the context for training leads to improvements on the datasets that need retrieval. But it downgrades the performance on non-retrieval datasets. Overall, these two models perform comparable.
It is because incorporating “top-5” retrieved chunks in stage-2 tuning aligns with the inference stage where retrieval is needed and hence, improves the Avg-ret score. However, mixing continuous and discontinuous documents could make the stage-2 tuning less stable, leading to sub-optimal results on non-retrieval datasets. We believe more future work can be conducted regarding the balance of incorporating continuous context and top-<em class="ltx_emph ltx_font_italic" id="S6.SS3.p3.1.1">k</em> retrieved chunks in the stage-2 tuning.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Ablation Studies for Inference Stage</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S6.T6" title="Table 6 ‣ 6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">6</span></a>, we show ablation studies on how the number of retrieved context/chunks, context ordering, and different retrievers affect the conversational QA results.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">First, we find that using more contexts as inputs do not always improve the results. Utilizing top-5 contexts as input yields better results compared to using either top-3 or top-10 contexts. Intuitively, more contexts have higher probability to contain correct answers (better recall scores). As a result, using top-5 contexts achieves better results than using top-3. However, as the number of contexts further increases, the model may suffer from “lost in the middle” phenomenon&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu et&nbsp;al., <a class="ltx_ref" href="#bib.bib41" title="">2023</a>)</cite> and the difficulty of extracting answers from the provided context could also increase, which leads to inferior results by using top-10 contexts.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS4.p3">
<p class="ltx_p" id="S6.SS4.p3.9">Second, we study how using different orderings of top-5 contexts affects the results. We compare sequential ordering (from <math alttext="1" class="ltx_Math" display="inline" id="S6.SS4.p3.1.m1.1"><semantics id="S6.SS4.p3.1.m1.1a"><mn id="S6.SS4.p3.1.m1.1.1" xref="S6.SS4.p3.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.1.m1.1b"><cn id="S6.SS4.p3.1.m1.1.1.cmml" type="integer" xref="S6.SS4.p3.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.1.m1.1d">1</annotation></semantics></math>st context to <math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p3.2.m2.1"><semantics id="S6.SS4.p3.2.m2.1a"><mn id="S6.SS4.p3.2.m2.1.1" xref="S6.SS4.p3.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.2.m2.1b"><cn id="S6.SS4.p3.2.m2.1.1.cmml" type="integer" xref="S6.SS4.p3.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.2.m2.1d">5</annotation></semantics></math>th context) to reverse ordering (from <math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p3.3.m3.1"><semantics id="S6.SS4.p3.3.m3.1a"><mn id="S6.SS4.p3.3.m3.1.1" xref="S6.SS4.p3.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.3.m3.1b"><cn id="S6.SS4.p3.3.m3.1.1.cmml" type="integer" xref="S6.SS4.p3.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.3.m3.1c">5</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.3.m3.1d">5</annotation></semantics></math>th to <math alttext="1" class="ltx_Math" display="inline" id="S6.SS4.p3.4.m4.1"><semantics id="S6.SS4.p3.4.m4.1a"><mn id="S6.SS4.p3.4.m4.1.1" xref="S6.SS4.p3.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.4.m4.1b"><cn id="S6.SS4.p3.4.m4.1.1.cmml" type="integer" xref="S6.SS4.p3.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.4.m4.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.4.m4.1d">1</annotation></semantics></math>st context), swing ordering (given the “lost in the middle” phenomenon, we arrange the most relevant context to appear at the beginning and the end of the input context. Hence, the ordering becomes {<math alttext="1" class="ltx_Math" display="inline" id="S6.SS4.p3.5.m5.1"><semantics id="S6.SS4.p3.5.m5.1a"><mn id="S6.SS4.p3.5.m5.1.1" xref="S6.SS4.p3.5.m5.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.5.m5.1b"><cn id="S6.SS4.p3.5.m5.1.1.cmml" type="integer" xref="S6.SS4.p3.5.m5.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.5.m5.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.5.m5.1d">1</annotation></semantics></math>st, <math alttext="3" class="ltx_Math" display="inline" id="S6.SS4.p3.6.m6.1"><semantics id="S6.SS4.p3.6.m6.1a"><mn id="S6.SS4.p3.6.m6.1.1" xref="S6.SS4.p3.6.m6.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.6.m6.1b"><cn id="S6.SS4.p3.6.m6.1.1.cmml" type="integer" xref="S6.SS4.p3.6.m6.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.6.m6.1c">3</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.6.m6.1d">3</annotation></semantics></math>rd, <math alttext="5" class="ltx_Math" display="inline" id="S6.SS4.p3.7.m7.1"><semantics id="S6.SS4.p3.7.m7.1a"><mn id="S6.SS4.p3.7.m7.1.1" xref="S6.SS4.p3.7.m7.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.7.m7.1b"><cn id="S6.SS4.p3.7.m7.1.1.cmml" type="integer" xref="S6.SS4.p3.7.m7.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.7.m7.1c">5</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.7.m7.1d">5</annotation></semantics></math>th, <math alttext="4" class="ltx_Math" display="inline" id="S6.SS4.p3.8.m8.1"><semantics id="S6.SS4.p3.8.m8.1a"><mn id="S6.SS4.p3.8.m8.1.1" xref="S6.SS4.p3.8.m8.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.8.m8.1b"><cn id="S6.SS4.p3.8.m8.1.1.cmml" type="integer" xref="S6.SS4.p3.8.m8.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.8.m8.1c">4</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.8.m8.1d">4</annotation></semantics></math>th, <math alttext="2" class="ltx_Math" display="inline" id="S6.SS4.p3.9.m9.1"><semantics id="S6.SS4.p3.9.m9.1a"><mn id="S6.SS4.p3.9.m9.1.1" xref="S6.SS4.p3.9.m9.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p3.9.m9.1b"><cn id="S6.SS4.p3.9.m9.1.1.cmml" type="integer" xref="S6.SS4.p3.9.m9.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p3.9.m9.1c">2</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p3.9.m9.1d">2</annotation></semantics></math>nd}), and random ordering (random shuffle the top-5 contexts). We find using sequential ordering is comparable to using reverse and swing orderings, and random shuffling is slightly worse. Results indicate that our model excels in extracting the correct answer from lengthy contexts, regardless of the answer’s location. It is because during the ChatQA fine-tuning, the answer’s location occurs randomly within the context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS4.p4">
<p class="ltx_p" id="S6.SS4.p4.1">Third, we observe that when we replace “Dragon + Fine-tune” with the original non-finetuned Dragon retriever, the average score drops by 1.81 (from 42.31 to 40.50). In addition, the score drops significantly in INSCIT dataset (from 33.98 to 27.87) due to the large performance gap between the two retrievers (as shown in Table&nbsp;<a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.2.3 Training Blends ‣ 3.2 Stage-2: Context-Enhanced Instruction Tuning ‣ 3 ChatQA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">1</span></a>). Basically, when the quality of retrieval improves, it directly enhances the performance of question answering.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Evaluation of Unanswerable Case</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S6.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.1 </span>Evaluation Setup</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS5.SSS1.p1">
<p class="ltx_p" id="S6.SS5.SSS1.p1.1">In this section, we study another aspect of the model’s capability, which is to discern if a question can be answered within the provided context. Generating an answer in unanswerable case will lead to hallucination.
To allow this evaluation, we require the model to indicate it when no answer can be found in the given context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS5.SSS1.p2">
<p class="ltx_p" id="S6.SS5.SSS1.p2.1">We use QuAC and DoQA datasets which have such unanswerable cases to evaluate such capability. Specifically, for unanswerable case, we consider the model indicating that the question cannot be answered as correct<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>We collect a set of heuristic matching patterns from all generated samples to determine if the model suggests that the question is unanswerable. More details can be found in the Appendix&nbsp;<a class="ltx_ref" href="#A5" title="Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">E</span></a>.</span></span></span>, and as for answerable cases, we consider the model not indicating the question is unanswerable as correct (i.e., the model giving an answer). Note that for answerable cases, we only select the samples where correct context is retrieved. Our model checkpoints (ChatQA-70B w/ 1.5k unanswerable and ChatQA-70B w/ SyntheticConvQA) are same ones reported in Table&nbsp;<a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2.1 Long Document Datasets ‣ 5.2 Evaluation Benchmarks ‣ 5 Experimental Setup ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">2</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS5.SSS1.p3">
<p class="ltx_p" id="S6.SS5.SSS1.p3.1">In the end, we calculate the average accuracy score of unanswerable and answerable cases as the final metric.
We consider this average accuracy as a reliable metric since it is in the same spirit of F1 metric which measures the harmonic mean of precision and recall scores.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.2 </span>Results</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS5.SSS2.p1">
<p class="ltx_p" id="S6.SS5.SSS2.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S6.T7" title="Table 7 ‣ 6.2 Fine-grained Analyses ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">7</span></a>, we compare our models with OpenAI models across QuAC and DoQA datasets. First, we find that compared to using SyntheticConvQA, using HumanAnnotatedConvQA significantly increase the average accuracies on both QuAC and DoQA. It is because the unanswerable annotations are in higher quality for human-annotated data, which significantly improve the accuracy for unanswerable cases. Second, OpenAI models show powerful capability in this task, especially for GPT-4. Compared to them, our best model achieved significantly better average accuracy than GPT-3.5-turbo, while we still has a slight gap compared to GPT-4 (around 3.5%).
Third, we find that models achieving higher accuracy on unanswerable samples tends to get lower accuracy on answerable samples, and vice versa. We speculate that when a model tends to be “aggressive” and offer somewhat relevant answers to those unanswerable questions, it will boost the accuracy for answerable cases, but reduces accuracy for unanswerable ones. Conversely, when a model is more “conservative” and strictly check if the question can be answered will result in the opposite effects.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS5.SSS2.p2">
<p class="ltx_p" id="S6.SS5.SSS2.p2.1">We conduct ablation studies in terms of the number of unanswerable samples in HumanAnnotatedConvQA. We find that using a small amount of unanswerable samples (e.g., 1.5k) is able to achieve remarkable results on the unanswerable evaluation, as well as the overall conversational QA score&nbsp;(i.e., Avg-CQA). Interestingly, incorporating more unanswerable samples does not necessarily lead to higher accuracy scores in the unanswerable evaluation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S6.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T8.1" style="width:976.9pt;height:670.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T8.1.1"><span class="ltx_text" id="S6.T8.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="S6.T8.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.1.1.1.1" style="color:#000000;">Context</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.1.1.2.1" style="color:#000000;">Conversation</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.1.1.3.1" style="color:#000000;">Assistant Responses</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.2.2.1.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.1.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.1.1.1" style="color:#000000;">The process for Board hearings has changed. If you’re appealing a VA decision</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.1.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.2.1.1" style="color:#000000;">under the new process, keep reading below to learn how to request and prepare</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.1.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.3.1.1" style="color:#000000;">for a hearing. </span><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.3.1.2" style="color:#0000FF;">You’ll follow the new process if you’re : Appealing a VA decision</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.1.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.4.1.1" style="color:#0000FF;">you received on or after February 19 , 2019.</span><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.4.1.2" style="color:#000000;"> Participating in the Rapid Appeals</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.1.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.5.1.1" style="color:#000000;">Modernization Program Opting into the new process using VA Form 10182 If</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.1.1.6">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.6.1.1" style="color:#000000;">you re using VA Form 9 to appeal a VA decision you received before February</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.1.1.7">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.7.1.1" style="color:#000000;">19, 2019, go to the Board of Veterans’ Appeals BVA Hearing page … The Board</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.1.1.8">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.8.1.1" style="color:#000000;">of Veterans Appeals schedules hearings in docket order the order in which they</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.1.1.9">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.1.1.9.1.1" style="color:#000000;">receive requests …</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.2.2.2.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.2.1.1.1.1" style="color:#000000;">User: What can I learn at my Board hearing?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.2.1.2.1.1" style="color:#000000;">Assistant: Do you want to learn exactly what to expect?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.2.1.3.1.1" style="color:#000000;">…</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.2.1.4.1.1" style="color:#000000;">User: Is it necessary for me to consider requesting a Board hearing?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.2.1.5.1.1" style="color:#000000;">Assistant: You can request a video conference hearing if you want</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.6">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.2.1.6.1.1" style="color:#000000;">to provide testimony to a judge about your appeal.</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.7">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.2.1.7.1.1" style="color:#000000;">User: when will I follow the new process for a Board hearing?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.2.1.8">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.2.1.8.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.2.2.3.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.3.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.1.1.1" style="color:#000000;">ChatQA-70B: </span><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.1.1.2" style="color:#41B729;">If you’re appealing a VA decision you received on or</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.3.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.2.1.1" style="color:#41B729;">after February 19, 2019, you’ll follow the new process</span><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.2.1.2" style="color:#000000;">.</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.3.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.3.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.3.1.2" style="color:#41B729;">You’ll follow the new process for a Board hearing if you’re</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.3.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.4.1.1" style="color:#41B729;">appealing a VA decision you received on or after February 19, 2019</span><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.4.1.2" style="color:#000000;">,</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.3.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.5.1.1" style="color:#000000;">participating in the Rapid Appeals Modernization Program, or opting</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.3.1.6">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.3.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.6.1.1" style="color:#000000;">into the new process using VA Form 10182.</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.3.1.7">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.3.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.7.1.1" style="color:#000000;">Ground Truth: You will follow the new process if appealing a VA</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.2.2.3.1.8">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.2.2.3.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.2.2.3.1.8.1.1" style="color:#000000;">decision if you received it on or after February 19, 2019.</span></span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.3.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.3.3.1.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.1.1.1.1.1" style="color:#000000;">title: How did Arya survive this confrontation unscathed?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.1.1.2.1.1" style="color:#000000;">There was no time or intention to turn her into White Walker. Let us look at the</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.1.1.3.1.1" style="color:#000000;">scene from S04E04. </span><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.1.1.3.1.2" style="color:#0000FF;">A white walker brings a baby holding in his lap and puts it</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.1.1.4.1.1" style="color:#0000FF;">on an ice table</span><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.1.1.4.1.2" style="color:#000000;"> (or whatever it is) for the arrival of the Night King. … When Arya</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.1.1.5.1.1" style="color:#000000;">drops the dagger, Night King looks surprised. This shows he doesn’t know what</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.6">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.1.1.6.1.1" style="color:#000000;">would happen next. There was no time to think and turn her into a white walker</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.1.1.7">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.1.1.7.1.1" style="color:#000000;">by touch. There was no planning in this scene like S04E04. …</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.3.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.3.3.2.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.2.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.2.1.1.1.1" style="color:#000000;">User: How did Arya survive this confrontation unscathed?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.2.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.2.1.2.1.1" style="color:#000000;">Assistant: Arya was quick enough to react, drop the dagger and</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.2.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.2.1.3.1.1" style="color:#000000;">do the rest.</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.2.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.2.1.4.1.1" style="color:#000000;">User: How old is the child that they lay on the altar of ice?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.2.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.2.1.5.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.3.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.3.3.3.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.3.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.3.1.1.1.1" style="color:#000000;">ChatQA-70B: The child appeared to be </span><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.3.1.1.1.2" style="color:#41B729;">a baby</span><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.3.1.1.1.3" style="color:#000000;">.</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.3.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.3.1.2.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.3.1.2.1.2" style="color:#41B729;">The text does not provide information on the exact age of the</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.3.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.3.1.3.1.1" style="color:#41B729;">child</span><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.3.1.3.1.2" style="color:#000000;"> that was laid on the ice altar.</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.3.3.3.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.3.3.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.3.3.3.1.4.1.1" style="color:#000000;">Ground Truth: a baby</span></span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4">
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.4.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.4.4.1.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.4.4.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.4.4.1.1.1.1.1" style="color:#000000;">Massif</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.4.4.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.4.4.1.1.1.2.1" style="color:#000000;">Type of nature reserve</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.4.4.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.4.4.1.1.1.3.1" style="color:#000000;">Preserved area</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.4.4.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.2.1.1" style="color:#000000;">Chornohora</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.4.4.1.1.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.2.2.1" style="color:#000000;">Carpathian Biosphere</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.4.4.1.1.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.2.3.1" style="color:#000000;">2476.8 ha</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.1.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.3.1.1" style="color:#0000FF;">Uholka / Wide Meadow</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.3.2.1" style="color:#000000;">Carpathian Biosphere</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.3.3.1" style="color:#0000FF;">11860 ha</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.1.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.4.1.1" style="color:#000000;">Svydovets</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.4.2.1" style="color:#000000;">Carpathian Biosphere</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.4.3.1" style="color:#000000;">3030.5 ha</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.1.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.5.1.1" style="color:#000000;">Maramoros</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.5.2.1" style="color:#000000;">Carpathian Biosphere</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.5.3.1" style="color:#000000;">2243.6 ha</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.1.1.6">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.6.1.1" style="color:#000000;">Rozok</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.6.2.1" style="color:#000000;">Presov Preserved</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.6.3.1" style="color:#000000;">67.1 ha</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.1.1.7">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.7.1.1" style="color:#000000;">Jasmund</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.7.2.1" style="color:#000000;">Jasmund National Park</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.1.1.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.7.3.1" style="color:#000000;">492.5 ha</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.1.1.8">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S6.T8.1.1.1.1.4.4.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.1.1.8.1.1" style="color:#000000;">…</span></span>
<span class="ltx_td ltx_border_bb" id="S6.T8.1.1.1.1.4.4.1.1.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_bb" id="S6.T8.1.1.1.1.4.4.1.1.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.4.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.4.4.2.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.2.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.2.1.1.1.1" style="color:#000000;">User: which of the forests is a carpathian biosphere reserve?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.2.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.2.1.2.1.1" style="color:#000000;">Assistant: Chornohora, Uholka / Wide Meadow, Svydovets,</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.2.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.2.1.3.1.1" style="color:#000000;">Maramoros, Kuziy / Trybushany</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.2.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.2.1.4.1.1" style="color:#000000;">User: which of those covers more than 3100 ha?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.2.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.2.1.5.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.4.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.4.4.3.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.3.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.3.1.1.1.1" style="color:#000000;">ChatQA-70B: </span><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.3.1.1.1.2" style="color:#41B729;">Uholka / Wide Meadow</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.3.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.3.1.2.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.3.1.2.1.2" style="color:#FF0000;">Chornohora</span><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.3.1.2.1.3" style="color:#000000;">, </span><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.3.1.2.1.4" style="color:#41B729;">Uholka / Wide Meadow</span><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.3.1.2.1.5" style="color:#000000;">, </span><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.3.1.2.1.6" style="color:#FF0000;">Svydovets, Maramoros</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.4.4.3.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.4.4.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.4.4.3.1.3.1.1" style="color:#000000;">Ground Truth: Uholka / Wide Meadow</span></span></span>
</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.5.5">
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.5.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.5.5.1.1" style="color:#000000;">title: Summary of enemy aircraft damaged/destroyed (John B. England)</span></span>
<span class="ltx_td ltx_border_t" id="S6.T8.1.1.1.1.5.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_t" id="S6.T8.1.1.1.1.5.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S6.T8.1.1.1.1.6.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.6.6.1.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.6.6.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.6.6.1.1.1.1.1" style="color:#000000;">Date</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.6.6.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.6.6.1.1.1.2.1" style="color:#000000;">Location</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.6.6.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.6.6.1.1.1.3.1" style="color:#000000;">Number</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.6.6.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.6.6.1.1.1.4.1" style="color:#000000;">Type</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="S6.T8.1.1.1.1.6.6.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="S6.T8.1.1.1.1.6.6.1.1.1.5.1" style="color:#000000;">Status</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.6.6.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.2.1.1" style="color:#0000FF;">24 April 1944</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.6.6.1.1.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.2.2.1" style="color:#000000;">Munich, Germany</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.6.6.1.1.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.2.3.1" style="color:#0000FF;">3</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.6.6.1.1.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.2.4.1" style="color:#000000;">Me-110</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T8.1.1.1.1.6.6.1.1.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.2.5.1" style="color:#0000FF;">Destroyed</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.3.1.1" style="color:#000000;">27 May 1944</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.3.2.1" style="color:#000000;">Strasbourg, France</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.3.3.1" style="color:#000000;">1</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.3.4.1" style="color:#000000;">Me-109</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.3.5.1" style="color:#000000;">Damaged</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.4.1.1" style="color:#000000;">18 August 1944</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.4.2.1" style="color:#000000;">Paris, France</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.4.3.1" style="color:#000000;">0.5</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.4.4.1" style="color:#000000;">Me-109</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.4.5.1" style="color:#000000;">Destroyed</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.5.1.1" style="color:#0000FF;">13 September 1944</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.5.2.1" style="color:#000000;">Nordhausen, Germany</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.5.3.1" style="color:#0000FF;">2.5</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.5.4.1" style="color:#000000;">Me-109</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.5.5.1" style="color:#0000FF;">Destroyed</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.6">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.6.1.1" style="color:#000000;">6 October 1944</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.6.2.1" style="color:#000000;">Berlin, Germany</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.6.3.1" style="color:#000000;">1</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.6.4.1" style="color:#000000;">Me-109</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.6.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.6.5.1" style="color:#000000;">Damaged</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.7">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.7.1.1" style="color:#0000FF;">6 October 1944</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.7.2.1" style="color:#000000;">Berlin, Germany</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.7.3.1" style="color:#0000FF;">2</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.7.4.1" style="color:#000000;">Me-109</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.7.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.7.5.1" style="color:#0000FF;">Destroyed</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.8">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.8.1.1" style="color:#0000FF;">27 November 1944</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.8.2.1" style="color:#000000;">Magdeburg, Germany</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.8.3.1" style="color:#0000FF;">4</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.8.4.1" style="color:#000000;">FW-190</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.8.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.8.5.1" style="color:#0000FF;">Destroyed</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.9">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.9.1.1" style="color:#000000;">14 January 1945</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.9.2.1" style="color:#000000;">Berlin, Germany</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.9.3.1" style="color:#000000;">1</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.9.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.9.4.1" style="color:#000000;">Me-109</span></span>
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.1.1.9.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.9.5.1" style="color:#000000;">Destroyed</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.1.1.10">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S6.T8.1.1.1.1.6.6.1.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.1.1.10.1.1" style="color:#000000;">…</span></span>
<span class="ltx_td ltx_border_bb" id="S6.T8.1.1.1.1.6.6.1.1.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_bb" id="S6.T8.1.1.1.1.6.6.1.1.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_bb" id="S6.T8.1.1.1.1.6.6.1.1.10.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_bb" id="S6.T8.1.1.1.1.6.6.1.1.10.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S6.T8.1.1.1.1.6.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.6.6.2.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.2.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.2.1.1.1.1" style="color:#000000;">User: what were all of the dates in which john b. england destroyed or</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.2.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.2.1.2.1.1" style="color:#000000;">damaged enemy planes?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.2.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.2.1.3.1.1" style="color:#000000;">Assistant: 24 April 1944, 27 May 1944, 18 August 1944, 13 September</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.2.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.2.1.4.1.1" style="color:#000000;">1944, 6 October 1944, 27 November 1944, 14 January 1945 …</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.2.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.2.1.5.1.1" style="color:#000000;">User: on which of those dates did john b. england destroy or damage</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.2.1.6">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.2.1.6.1.1" style="color:#000000;">more than one enemy plane?</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.2.1.7">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.2.1.7.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S6.T8.1.1.1.1.6.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="S6.T8.1.1.1.1.6.6.3.1">
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.3.1.1">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.3.1.1.1.1" style="color:#000000;">ChatQA-70B: </span><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.3.1.1.1.2" style="color:#41B729;">6 October 1944, 13 September 1944, 27 November 1944</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.3.1.2">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.3.1.2.1.1" style="color:#FF0000;">(missing 24 April 1944)</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.3.1.3">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.3.1.3.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.3.1.3.1.2" style="color:#41B729;">24 April 1944, 13 September 1944, 6 October 1944,</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.3.1.4">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.3.1.4.1.1" style="color:#41B729;">27 November 1944</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.3.1.5">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.3.1.5.1.1" style="color:#000000;">Ground Truth: 24 April 1944, 13 September 1944, 6 October 1944,</span></span></span>
<span class="ltx_tr" id="S6.T8.1.1.1.1.6.6.3.1.6">
<span class="ltx_td ltx_align_left" id="S6.T8.1.1.1.1.6.6.3.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="S6.T8.1.1.1.1.6.6.3.1.6.1.1" style="color:#000000;">27 November 1944</span></span></span>
</span></span></span>
</span>
</span><span class="ltx_text" id="S6.T8.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Case studies on ChatQA-70B and GPT-4. We use <span class="ltx_text" id="S6.T8.5.1" style="color:#0000FF;">blue</span> color to highlight the relevant context for the last user question. And we use <span class="ltx_text" id="S6.T8.6.2" style="color:#41B729;">green</span> and <span class="ltx_text" id="S6.T8.7.3" style="color:#FF0000;">red</span> color to highlight the correct and incorrect parts in the assistant responses.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Case Study</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S6.SS6.p1">
<p class="ltx_p" id="S6.SS6.p1.1">In Table&nbsp;<a class="ltx_ref" href="#S6.T8" title="Table 8 ‣ 6.5.2 Results ‣ 6.5 Evaluation of Unanswerable Case ‣ 6 Results ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">8</span></a>, we show four examples of ChatQA-70B and GPT-4 outputs.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>More examples can be found in Appendix&nbsp;<a class="ltx_ref" href="#A6" title="Appendix F Case Study ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">F</span></a>.</span></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS6.p2">
<p class="ltx_p" id="S6.SS6.p2.1">The first example (about <span class="ltx_text ltx_font_italic" id="S6.SS6.p2.1.1">Board hearing</span>) is a simple information seeking problem, and both ChatQA-70B and GPT-4 answer correctly. In the second example (about <span class="ltx_text ltx_font_italic" id="S6.SS6.p2.1.2">Arya</span>), the model needs to find implicit information (highlighted in blue) for giving the answer. GPT-4 tends to be safe on providing an answer, and it replies that the context does not provide exact information about the age which is also correct.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="S6.SS6.p3">
<p class="ltx_p" id="S6.SS6.p3.1">Both the third and fourth examples require the model to have good tabular understanding and reasoning ability. In the third example (about <span class="ltx_text ltx_font_italic" id="S6.SS6.p3.1.1">Massif</span>), ChatQA-70B gives correct answer by comparing the size of preserved area against 3100 ha, while GPT-4 fails to do so.
In the fourth example (about <span class="ltx_text ltx_font_italic" id="S6.SS6.p3.1.2">John B. England</span>), ChatQA-70B correctly lists three dates but misses one date, while GPT-4 correctly answers the question.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we build a family of ChatQA models, varying in model sizes from 7B to 70B.
Comprehensive evaluations on 10 conversational QA datasets show that our best ChatQA-70B model can remarkably outperforms GPT-3.5-turbo and perform on par with GPT-4 without using any synthetic data from ChatGPT models.
In addition, we demonstrate that fine-tuning a single-turn query retriever using our curated conversational QA data performs comparable to the state-of-the-art LLM-based query rewriting model, without the need of extra computational time and potential API cost from rewriting.
Furthermore, we show that incorporating a small amount of “unanswerable” samples can significantly enhance our model’s capability to handle scenarios where answers are unavailable. The unanswerable case evaluation highlights that our best model ChatQA-70B only has a slight gap compared to GPT-4.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adlakha et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Adlakha, V., Dhuliawala, S., Suleman, K., de&nbsp;Vries, H., and Reddy, S.

</span>
<span class="ltx_bibblock">Topiocqa: Open-domain conversational question answering with topic switching.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">TACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aliannejadi et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aliannejadi, M., Kiseleva, J., Chuklin, A., Dalton, J., and Burtsev, M.

</span>
<span class="ltx_bibblock">Building and evaluating open-domain dialogue corpora with clarifying questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">EMNLP</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anantha et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman, S., and Chappidi, S.

</span>
<span class="ltx_bibblock">Open-domain question answering goes conversational via question rewriting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">NAACL</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing 100k context windows, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing Claude, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brabant et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Brabant, Q., Lecorvé, G., and Barahona, L. M.&nbsp;R.

</span>
<span class="ltx_bibblock">Coqar: Question rewriting on coqa.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">LREC</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Campos et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Campos, J.&nbsp;A., Otegi, A., Soroa, A., Deriu, J.&nbsp;M., Cieliebak, M., and Agirre, E.

</span>
<span class="ltx_bibblock">Doqa-accessing domain-specific faqs via conversational qa.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">ACL</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., and Wang, W.&nbsp;Y.

</span>
<span class="ltx_bibblock">Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">EMNLP</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chen, Z., Zhao, J., Fang, A., Fetahu, B., Rokhlenko, O., and Malmasi, S.

</span>
<span class="ltx_bibblock">Reinforced question rewriting for conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">EMNLP</em>, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Choi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi, Y., Liang, P., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">Quac: Question answering in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">EMNLP</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chu, Z., Chen, M., Chen, J., Wang, M., Gimpel, K., Faruqui, M., and Si, X.

</span>
<span class="ltx_bibblock">How to ask better questions? a large-scale multi-domain dataset for rewriting ill-formed questions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">AAAI</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chung, H.&nbsp;W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S.&nbsp;S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Chi, E.&nbsp;H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.&nbsp;V., and Wei, J.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv: 2210.11416</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Conover, M., Hayes, M., Mathur, A., Meng, X., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Free dolly: Introducing the world’s first truly open instruction-tuned llm, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R.

</span>
<span class="ltx_bibblock">Free Dolly: Introducing the world’s first truly open instruction-tuned llm, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dai, Z., Chaganty, A.&nbsp;T., Zhao, V., Amini, A., Green, M., Rashid, Q., and Guu, K.

</span>
<span class="ltx_bibblock">Dialog inpainting: Turning documents to dialogs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ICML</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasigi et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dasigi, P., Liu, N.&nbsp;F., Marasović, A., Smith, N.&nbsp;A., and Gardner, M.

</span>
<span class="ltx_bibblock">Quoref: A reading comprehension dataset with questions requiring coreferential reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">EMNLP</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Del&nbsp;Tredici et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Del&nbsp;Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and de&nbsp;Gispert, A.

</span>
<span class="ltx_bibblock">Question rewriting for open-domain conversational qa: Best practices and limitations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CIKM</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Deng, Y., Lei, W., Zhang, W., Lam, W., and Chua, T.-S.

</span>
<span class="ltx_bibblock">Pacific: Towards proactive conversational question answering over tabular and textual data in finance.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J.

</span>
<span class="ltx_bibblock">Glm: General language model pretraining with autoregressive blank infilling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M.

</span>
<span class="ltx_bibblock">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">NAACL</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elgohary et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elgohary, A., Peskov, D., and Boyd-Graber, J.

</span>
<span class="ltx_bibblock">Can you unpack that? learning to rewrite questions-in-context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">EMNLP</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and Auli, M.

</span>
<span class="ltx_bibblock">Eli5: Long form question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">ACL</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Feng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and Lastras, L.

</span>
<span class="ltx_bibblock">doc2dial: A goal-oriented document-grounded dialogue dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">EMNLP</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Galimzhanova et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Galimzhanova, E., Muntean, C.&nbsp;I., Nardini, F.&nbsp;M., Perego, R., and Rocchietti, G.

</span>
<span class="ltx_bibblock">Rewriting conversational utterances with instructed large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gao, C., Zhang, W., and Lam, W.

</span>
<span class="ltx_bibblock">Unigdd: A unified generative framework for goal-oriented document-grounded dialogue.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Introducing bard, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Guo, M., Zhang, M., Reddy, S., and Alikhani, M.

</span>
<span class="ltx_bibblock">Abg-coqa: Clarifying ambiguity in conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">AKBC</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honovich et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Honovich, O., Scialom, T., Levy, O., and Schick, T.

</span>
<span class="ltx_bibblock">Unnatural instructions: Tuning language models with (almost) no human labor.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2212.09689</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ishii et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ishii, E., Xu, Y., Cahyawijaya, S., and Wilie, B.

</span>
<span class="ltx_bibblock">Can question rewriting help conversational question answering?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the Third Workshop on Insights from Negative Results in NLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyer et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Iyer, S., Lin, X.&nbsp;V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P.&nbsp;S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Opt-iml: Scaling language model instruction meta learning through the lens of generalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2212.12017</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard &amp; Grave (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Izacard, G. and Grave, É.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E.

</span>
<span class="ltx_bibblock">Unsupervised dense information retrieval with contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Transactions on Machine Learning Research</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y., Zhou, P., Bras, R.&nbsp;L., Alikhani, M., Kim, G., Sap, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Soda: Million-scale dialogue distillation with social commonsense contextualization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2212.10465</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kočiskỳ et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kočiskỳ, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.&nbsp;M., Melis, G., and Grefenstette, E.

</span>
<span class="ltx_bibblock">The narrativeqa reading comprehension challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">TACL</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.&nbsp;M., Stanley, O., Nagyfi, R., et&nbsp;al.

</span>
<span class="ltx_bibblock">Openassistant conversations–democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2304.07327</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köpf et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.&nbsp;M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A.

</span>
<span class="ltx_bibblock">Openassistant conversations - democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv: 2304.07327</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, K., Tafjord, O., Clark, P., and Gardner, M.

</span>
<span class="ltx_bibblock">Reasoning over paragraph effects in situations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y., Yih, W.-t., and Chen, X.

</span>
<span class="ltx_bibblock">How to train your dragon: Diverse augmentation towards generalizable dense retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2302.07452</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Lin, X.&nbsp;V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., et&nbsp;al.

</span>
<span class="ltx_bibblock">Ra-dit: Retrieval-augmented dual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2310.01352</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Liu, N.&nbsp;F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2307.03172</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.&nbsp;W., Tay, Y., Zhou, D., Le, Q.&nbsp;V., Zoph, B., Wei, J., et&nbsp;al.

</span>
<span class="ltx_bibblock">The flan collection: Designing data and methods for effective instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2301.13688</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mele et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mele, I., Muntean, C.&nbsp;I., Nardini, F.&nbsp;M., Perego, R., Tonellotto, N., and Frieder, O.

</span>
<span class="ltx_bibblock">Adaptive utterance rewriting for conversational search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Information Processing &amp; Management</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Cross-task generalization via natural language crowdsourcing instructions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mo, F., Mao, K., Zhu, Y., Wu, Y., Huang, K., and Nie, J.-Y.

</span>
<span class="ltx_bibblock">Convgqr: Generative query reformulation for conversational search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2305.15645</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Scao, T.&nbsp;L., Bari, M.&nbsp;S., Shen, S., Yong, Z.-X., Schoelkopf, H., et&nbsp;al.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2211.01786</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakamura et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nakamura, K., Levy, S., Tuan, Y.-L., Chen, W., and Wang, W.&nbsp;Y.

</span>
<span class="ltx_bibblock">Hybridialogue: An information-seeking dialogue dataset grounded on tabular and textual data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Findings of ACL</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., and Deng, L.

</span>
<span class="ltx_bibblock">Ms marco: A human generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">choice</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing ChatGPT, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">NeurIPS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat &amp; Liang (2015)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pasupat, P. and Liang, P.

</span>
<span class="ltx_bibblock">Compositional semantic parsing on semi-structured tables.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">ACL</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qu, C., Yang, L., Chen, C., Qiu, M., Croft, W.&nbsp;B., and Iyyer, M.

</span>
<span class="ltx_bibblock">Open-retrieval conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">SIGIR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et&nbsp;al. (2016)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.

</span>
<span class="ltx_bibblock">Squad: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">EMNLP</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rajpurkar, P., Jia, R., and Liang, P.

</span>
<span class="ltx_bibblock">Know what you don’t know: Unanswerable questions for squad.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ACL</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raposo et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Raposo, G., Ribeiro, R., Martins, B., and Coheur, L.

</span>
<span class="ltx_bibblock">Question rewriting? assessing its importance for conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">ECIR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy et&nbsp;al. (2019)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Reddy, S., Chen, D., and Manning, C.&nbsp;D.

</span>
<span class="ltx_bibblock">Coqa: A conversational question answering challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">TACL</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saeidi et&nbsp;al. (2018)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Saeidi, M., Bartolo, M., Lewis, P., Singh, S., Rocktäschel, T., Sheldon, M., Bouchard, G., and Riedel, S.

</span>
<span class="ltx_bibblock">Interpretation of natural language rules in conversational machine reading.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">EMNLP</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sanh, V., Webson, A., Raffel, C., Bach, S.&nbsp;H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T.&nbsp;L., Raja, A., et&nbsp;al.

</span>
<span class="ltx_bibblock">Multitask prompted training enables zero-shot task generalization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">ICLR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trischler et&nbsp;al. (2017)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K.

</span>
<span class="ltx_bibblock">Newsqa: A machine comprehension dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 2nd Workshop on Representation Learning for NLP</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vakulenko et&nbsp;al. (2021a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vakulenko, S., Longpre, S., Tu, Z., and Anantha, R.

</span>
<span class="ltx_bibblock">Question rewriting for conversational question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">WSDM</em>, 2021a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vakulenko et&nbsp;al. (2021b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Vakulenko, S., Voskarides, N., Tu, Z., and Longpre, S.

</span>
<span class="ltx_bibblock">A comparison of question rewriting methods for conversational passage retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">ECIR</em>, 2021b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Instructretro: Instruction tuning post retrieval-augmented pretraining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2310.07713</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F.

</span>
<span class="ltx_bibblock">Text embeddings by weakly-supervised contrastive pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2212.03533</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.&nbsp;A., Khashabi, D., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language model with self generated instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2212.10560</em>, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A.&nbsp;S., Arunkumar, A., Stap, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">EMNLP</em>, 2022c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K.&nbsp;R., Wadden, D., MacMillan, K., Smith, N.&nbsp;A., Beltagy, I., et&nbsp;al.

</span>
<span class="ltx_bibblock">How far can camels go? exploring the state of instruction tuning on open resources.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">arXiv preprint arXiv:2306.04751</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei, J., Bosma, M., Zhao, V.&nbsp;Y., Guu, K., Yu, A.&nbsp;W., Lester, B., Du, N., Dai, A.&nbsp;M., and Le, Q.&nbsp;V.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">ICLR</em>, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et&nbsp;al. (2022b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.&nbsp;V., Zhou, D., et&nbsp;al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">NeurIPS</em>, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wu, Z., Luan, Y., Rashkin, H., Reitter, D., Hajishirzi, H., Ostendorf, M., and Tomar, G.&nbsp;S.

</span>
<span class="ltx_bibblock">Conqrr: Conversational query rewriting for retrieval with reinforcement learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">EMNLP</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wu, Z., Parish, R., Cheng, H., Min, S., Ammanabrolu, P., Ostendorf, M., and Hajishirzi, H.

</span>
<span class="ltx_bibblock">Inscit: Information-seeking conversations with mixed-initiative interactions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">TACL</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2304.12244</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et&nbsp;al. (2023b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B.

</span>
<span class="ltx_bibblock">Retrieval meets long context large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2310.03025</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ye, F., Fang, M., Li, S., and Yilmaz, E.

</span>
<span class="ltx_bibblock">Enhancing conversational search: Large language model-aided informative query rewriting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">EMNLP</em>, pp.&nbsp; 5985–6006, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et&nbsp;al. (2020)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yu, S., Liu, J., Yang, J., Xiong, C., Bennett, P., Gao, J., and Liu, Z.

</span>
<span class="ltx_bibblock">Few-shot generative conversational query rewriting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">SIGIR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et&nbsp;al.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2308.10792</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et&nbsp;al.

</span>
<span class="ltx_bibblock">Lima: Less is more for alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2305.11206</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., and Chua, T.-S.

</span>
<span class="ltx_bibblock">Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">ACL</em>, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>ChatQA Instruction Tuning</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Stage-1: Supervised Fine-tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">The format template of LLM inputs in stage-1 is as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS1.p2.1">System: This is a chat between a user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user’s questions
based on the context. The assistant should also indicate when the answer cannot be
found in the context.

User: {Question 1}

Assistant: {Answer 1}

...

User: {Latest Question}

Assistant:
</pre>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1">We use the <math alttext="\{\texttt{Latest Answer}\}" class="ltx_Math" display="inline" id="A1.SS1.p3.1.m1.1"><semantics id="A1.SS1.p3.1.m1.1a"><mrow id="A1.SS1.p3.1.m1.1.2.2" xref="A1.SS1.p3.1.m1.1.2.1.cmml"><mo id="A1.SS1.p3.1.m1.1.2.2.1" stretchy="false" xref="A1.SS1.p3.1.m1.1.2.1.cmml">{</mo><mtext id="A1.SS1.p3.1.m1.1.1" mathvariant="monospace" xref="A1.SS1.p3.1.m1.1.1a.cmml">Latest Answer</mtext><mo id="A1.SS1.p3.1.m1.1.2.2.2" stretchy="false" xref="A1.SS1.p3.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.1.m1.1b"><set id="A1.SS1.p3.1.m1.1.2.1.cmml" xref="A1.SS1.p3.1.m1.1.2.2"><ci id="A1.SS1.p3.1.m1.1.1a.cmml" xref="A1.SS1.p3.1.m1.1.1"><mtext id="A1.SS1.p3.1.m1.1.1.cmml" mathvariant="monospace" xref="A1.SS1.p3.1.m1.1.1">Latest Answer</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.1.m1.1c">\{\texttt{Latest Answer}\}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p3.1.m1.1d">{ Latest Answer }</annotation></semantics></math> from Assistant as the supervision for model outputs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Stage-2: Context-Enhanced Instruction Tuning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.2">Based on the stage-1 format template, the LLM inputs in stage-2 adds <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p1.2.1">{Context for Latest Question}</span> and <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p1.2.2">{Instruction}</span> from <span class="ltx_text ltx_font_typewriter" id="A1.SS2.p1.2.3">User</span>, as depicted below:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS2.p1.3">System: This is a chat between a user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user’s questions
based on the context. The assistant should also indicate when the answer cannot be
found in the context.

{Context for Latest Question}

User: {Instruction} + {Question 1}

Assistant: {Answer 1}

...

User: {Latest Question}

Assistant:
</pre>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1">Same as Stage-1, we use the <math alttext="\{\texttt{Latest Answer}\}" class="ltx_Math" display="inline" id="A1.SS2.p2.1.m1.1"><semantics id="A1.SS2.p2.1.m1.1a"><mrow id="A1.SS2.p2.1.m1.1.2.2" xref="A1.SS2.p2.1.m1.1.2.1.cmml"><mo id="A1.SS2.p2.1.m1.1.2.2.1" stretchy="false" xref="A1.SS2.p2.1.m1.1.2.1.cmml">{</mo><mtext id="A1.SS2.p2.1.m1.1.1" mathvariant="monospace" xref="A1.SS2.p2.1.m1.1.1a.cmml">Latest Answer</mtext><mo id="A1.SS2.p2.1.m1.1.2.2.2" stretchy="false" xref="A1.SS2.p2.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.1.m1.1b"><set id="A1.SS2.p2.1.m1.1.2.1.cmml" xref="A1.SS2.p2.1.m1.1.2.2"><ci id="A1.SS2.p2.1.m1.1.1a.cmml" xref="A1.SS2.p2.1.m1.1.1"><mtext id="A1.SS2.p2.1.m1.1.1.cmml" mathvariant="monospace" xref="A1.SS2.p2.1.m1.1.1">Latest Answer</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.1.m1.1c">\{\texttt{Latest Answer}\}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.1.m1.1d">{ Latest Answer }</annotation></semantics></math> from Assistant as the supervision for model outputs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1">As for the <math alttext="\{\texttt{Instruction}\}" class="ltx_Math" display="inline" id="A1.SS2.p3.1.m1.1"><semantics id="A1.SS2.p3.1.m1.1a"><mrow id="A1.SS2.p3.1.m1.1.2.2" xref="A1.SS2.p3.1.m1.1.2.1.cmml"><mo id="A1.SS2.p3.1.m1.1.2.2.1" stretchy="false" xref="A1.SS2.p3.1.m1.1.2.1.cmml">{</mo><mtext id="A1.SS2.p3.1.m1.1.1" xref="A1.SS2.p3.1.m1.1.1a.cmml">𝙸𝚗𝚜𝚝𝚛𝚞𝚌𝚝𝚒𝚘𝚗</mtext><mo id="A1.SS2.p3.1.m1.1.2.2.2" stretchy="false" xref="A1.SS2.p3.1.m1.1.2.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.1.m1.1b"><set id="A1.SS2.p3.1.m1.1.2.1.cmml" xref="A1.SS2.p3.1.m1.1.2.2"><ci id="A1.SS2.p3.1.m1.1.1a.cmml" xref="A1.SS2.p3.1.m1.1.1"><mtext id="A1.SS2.p3.1.m1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1">𝙸𝚗𝚜𝚝𝚛𝚞𝚌𝚝𝚒𝚘𝚗</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.1.m1.1c">\{\texttt{Instruction}\}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p3.1.m1.1d">{ Instruction }</annotation></semantics></math>, we use different instructions for different datasets based on the answer types. Details can be found below:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS2.p4">
<p class="ltx_p" id="A1.SS2.p4.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p4.1.1">Please give a full and complete answer for the question.</span>” This is for datasets with long answers. We use it for the HumanAnnotatedConvQA or SyntheticConvQA.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS2.p5">
<p class="ltx_p" id="A1.SS2.p5.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p5.1.1">Answer the following question with a short span. The answer needs to be just in a few words.</span>” This is for datasets with short answers. We use it for SQuAD1.1, SQuAD2.0, NarrativeQA, DROP, ROPES, NewsQA, and Quoref.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS2.p6">
<p class="ltx_p" id="A1.SS2.p6.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p6.1.1">Answer the following question with a number from context or the math arithmetic using +, -, *, or /.</span>” This is for datasets using arithmetic calculation or extracting numbers from the context. We use it for the TAT-QA dataset where questions need the arithmetic calculation or extracting numbers from the context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS2.p7">
<p class="ltx_p" id="A1.SS2.p7.1">“<span class="ltx_text ltx_font_typewriter" id="A1.SS2.p7.1.1">Answer the following question with a short span, or a full and complete answer.</span>” This is for datasets with both short and long answers. The model needs to tell whether to generate short or long answers based on the questions. We use it for the TAT-QA dataset where questions do not need the arithmetic calculation, since we observe the lengths of the corresponding answers can be both short and long.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS2.p8">
<p class="ltx_p" id="A1.SS2.p8.2">For the stage-1 SFT data used in stage-2 instruction tuning, we keep the same format template as in stage-1. In other words, the <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p8.2.1">{Context for Latest Question}</span> and <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="A1.SS2.p8.2.2">{Instruction}</span> are empty.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Prompts for Synthetic Data Generation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS3.p1.1.1">We use a two-step prompting on GPT-3.5-turbo for obtaining a high-quality multi-turn QA conversation.</span> In step-1, we prompt GPT-3.5-turbo to generate a multi-turn QA conversation based on a given document. However, we find that the generated conversation lacks pronouns or common nouns to refer back to words or phrases in dialogue history. Hence, in step-2, we prompt GPT-3.5-turbo to revise the generated conversation by adding pronouns or common nouns. For both stages, we give GPT-3.5-turbo few-shot examples to ensure a high-quality generation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS3.p2">
<p class="ltx_p" id="A1.SS3.p2.1">The in-context few-shot prompt in step-1 is as follows (aims to generate multi-turn QA conversation):</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS3.p2.2">You are a helpful, pattern-following assistant.

role-user:
PersonA has access to information in the following passage:
###There are certain important aspects of college preparation. The student preparing
for college should define their goals and interests. The student should also plan
financially and academically. We have a checklist that helps students get ready
financially and academically.###
Generate a conversation of around four turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
PersonU: I need to know how to prepare for college.
PersonA: You should first define your goals and interest and be sure to know the
costs and how to plan financially and academically for college.
PersonU: Is there something I can use as a guide to get ready for it?
PersonA: you can use our checklist to get ready financially and academically.
PersonU: Any other tip?
PersonA: Be sure to check out the reasons to attend a College or Career schools.
PersonU: Ok, that sounds good. Can you give me more tips?
PersonA: Another thing to take into consideration is to explore your career options.

role-user:
PersonA has access to information in the following passage:
###There are several disability compensation benefits. One of the benefits is the
annual clothing allowance. This allowance covers the cost of purchasing new clothes
up to a limit of 200 dollars annually.###
Generate a conversation of around two turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
PersonU: I heard about a disability benefit for clothing. What is it about?
PersonA: This is a disability compensation benefit known as an annual clothing
allowance.
PersonU: I am quite interested about such benefits.
PersonA: Would you like to find out if you are eligible?

role-user:
PersonA has access to information in the following passage:
###Vascepa (icosapent ethyl) is a prescription fish oil medication approved for
treating adults with extremely high triglycerides, defined as a triglyceride level
of 500 mg/dL or higher. While using this medicine, you may need frequent blood tests
at your doctor’s office. Vascepa is only part of a treatment program that may also
include diet, exercise, and weight control. Follow your doctor’s instructions very
closely. You should not use Vascepa if you are allergic to icosapent. To make sure
Vascepa is safe for you, tell your doctor if you have: 1 liver disease; 2 diabetes;
3 a thyroid disorder; 4 problems with your pancreas; 5 a bleeding or blood-clotting
disorder; 6 if you are allergic to fish or shellfish; or. 7 if you drink large
amounts of alcohol, Vascepa (icosapent) works in the liver and bloodstream to reduce
very low-density triglycerides.###
Generate a conversation of around three turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
PersonU: I heard about a medication called Vascepa. What is it used for?
PersonA: Vascepa is a prescription fish oil medication used for treating adults with
extremely high triglyceride levels.
PersonU: Are there any side effects or precautions I should be aware of?
PersonA: While using Vascepa, you may need frequent blood tests, and it’s important
to follow your doctor’s instructions closely. It’s important to talk to your doctor
about any allergies or existing conditions you have.
PersonU: Is it effective in lowering cholesterol levels?
PersonA: Vascepa is used together with a low-fat diet and other treatments to lower
total cholesterol in people with severe high cholesterol.

role-user:
PersonA has access to information in the following passage:

{Document}

Generate a conversation of around three turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:

role-assistant:
</pre>
</div>
<div class="ltx_para" id="A1.SS3.p3">
<p class="ltx_p" id="A1.SS3.p3.1">The in-context few-shot prompt in step-2 is as follows (aims to revise the multi-turn QA conversation by adding pronouns and common nouns):</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A1.SS3.p4">
<pre class="ltx_verbatim ltx_font_typewriter" id="A1.SS3.p4.1">You are a helpful, pattern-following assistant.

role-user:
Given the following conversation between PersonU and PersonA:
###PersonU: I’ve heard about adobo sauce in Mexican cuisine. Can you tell me more
about it?
PersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and
vinegar. It’s used as a marinade and cooking sauce in Mexican and Southwestern
cuisines.
PersonU: What are the main ingredients in adobo sauce?
PersonA: The main ingredients are typically chile peppers, garlic, and vinegar.
However, there can be variations in the recipe.
PersonU: How can I make adobo sauce at home?
PersonA: One way to make adobo sauce at home is by using smoked jalapeno peppers
called chipotles. You can stew the chipotles in a sauce with tomatoes, garlic,
vinegar, salt, and spices. It’s also possible to find canned adobo sauce in the
Hispanic foods section of many grocery stores.
PersonU: Are there any other uses for adobo sauce?
PersonA: Adobo sauce can be used as a condiment or as a marinade for various dishes.
It enhances the flavor of meats and can be used as a cooking and serving sauce.
Additionally, adobo is also used in a stew-like presentation of meat prepared in
this sauce.
PersonU: Is adobo sauce easy to preserve?
PersonA: Yes, you can preserve adobo sauce by freezing or canning it when you’re
done making it. This allows you to store it for up to a year and enjoy its flavors
whenever you want.###
Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.

role-assistant:
PersonU: I’ve heard about adobo sauce in Mexican cuisine. Can you provide more
information about it?
PersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and vinegar.
It’s used as a marinade and cooking sauce in Mexican and Southwestern cuisines.
PersonU: What are the main ingredients in the sauce?
PersonA: The main ingredients are typically chile peppers, garlic, and vinegar.
However, there can be variations in the recipe.
PersonU: How can I make the sauce at home?
PersonA: One way to make it at home is by using smoked jalapeno peppers called
chipotles. You can stew them in a sauce with tomatoes, garlic, vinegar, salt, and
spices. It’s also possible to find canned adobo sauce in the Hispanic foods section
of many grocery stores.
PersonU: Are there any other uses for it?
PersonA: It can be used as a condiment or as a marinade for various dishes. It
enhances the flavor of meats and can be used as a cooking and serving sauce.
Additionally, it is also used in a stew-like presentation of meat prepared in
this sauce.
PersonU: Is it easy to preserve?
PersonA: Yes, you can preserve it by freezing or canning it when you’re done
making it. This allows you to store it for up to a year and enjoy its flavors
whenever you want.

role-user:
Given the following conversation between PersonU and PersonA:
###PersonU: I’ve heard about a test called urinalysis. What is it used for?
PersonA: Urinalysis is a test that evaluates a sample of your urine. It is used to
detect and assess a wide range of disorders, such as urinary tract infection,
kidney disease, and diabetes. It examines the appearance, concentration, and content
of urine.
PersonU: Can it help diagnose bladder cancer?
PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for
bladder cancer. One way to test for bladder cancer is to check for blood in the
urine, which is called hematuria. This can be done during a urinalysis by examining
the urine cells under a special microscope.
PersonU: What other conditions or diseases can be detected through urinalysis?
PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,
evaluate causes of kidney failure, screen for the progression of chronic conditions
like diabetes mellitus and high blood pressure, and it can be used in combination
with other tests to diagnose certain diseases. It is a versatile test that helps
detect abnormalities in the urinary tract.
PersonU: Where can I get a urinalysis done?
PersonA: Urinalysis can be performed in various healthcare settings, including
doctors’ offices, urgent care facilities, laboratories, and hospitals. You can
provide a urine sample in a specimen cup, and the sample can be analyzed on-site
or sent to a laboratory for testing.
PersonU: How much urine is typically needed for a urinalysis?
PersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are required
for urinalysis testing. It’s a non-invasive and relatively simple test that provides
valuable information about your health.###
Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.

role-assistant:
PersonU: I’ve heard about a test called urinalysis. What is it used for?
PersonA: Urinalysis is a test that evaluates a sample of your urine. It’s used to
detect and assess a wide range of disorders, such as urinary tract infection,
kidney disease, and diabetes. It examines the appearance, concentration, and
content of urine.
PersonU: Can it help diagnose bladder cancer?
PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for bladder
cancer. One way to test for bladder cancer is to check for blood in the urine, which
is called hematuria. This can be done during a urinalysis by examining the urine
cells under a special microscope.
PersonU: What other conditions or diseases can be detected through this test?
PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,
evaluate causes of kidney failure, screen for the progression of chronic conditions
like diabetes mellitus and high blood pressure, and it can be used in combination
with other tests to diagnose certain diseases. It is a versatile test that helps
detect abnormalities in the urinary tract.
PersonU: Where can I go to get this test done?
PersonA: Urinalysis can be performed in various healthcare settings, including
doctors’ offices, urgent care facilities, laboratories, and hospitals. You can
provide a urine sample in a specimen cup, and the sample can be analyzed on-site
or sent to a laboratory for testing.
PersonU: How much urine is typically needed for the test?
PersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are
required for urinalysis testing. It’s a non-invasive and relatively simple test
that provides valuable information about your health.

role-user:
Given the following conversation between PersonU and PersonA:

{multi-turn QA conversation}

Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.

role-assistant:


</pre>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>More Details and Results for Retrieval in Conversational QA</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Query Rewriting Prompts for GPT-3.5-turbo</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">The in-context few-shot query rewriting prompt we use for GPT-3.5-turbo is as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A2.SS1.p2.1">You are a helpful, pattern-following assistant.

role-user:
Given the following conversation between PersonU and PersonA:
PersonU: Hello, I would like to know what to do if I do not agree with any decision.
PersonA: disagree with our decision about your monthly income adjustment amounts?
PersonU: no. Where can I find my SHIP contact information?
PersonA: You can find your local SHIP contact information in the back of your
Medicare &amp; You 2020 Handbook online.
PersonU: and how do they calculate the adjustments?
Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.

role-assistant:
How is the calculation for adjustments made by SHIP determined?

role-user:
Given the following conversation between PersonU and PersonA:
PersonU: I need to know how to prepare for college.
PersonA: You should first define your goals and interest and be sure to know the
costs and how to plan financially and academically for college.
PersonU: Is there something I can use as a guide to get ready for it?
Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.

role-assistant:
What resources or guides can I use to help me prepare for college?

role-user:
Given the following conversation between PersonU and PersonA:

{Dialogue History + Latest Question}

Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.

role-assistant:

</pre>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>More Results for Retrieval in Conversational QA</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A2.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T9.1" style="width:526.9pt;height:127pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A2.T9.1.1"><span class="ltx_text" id="A2.T9.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="A2.T9.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="A2.T9.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2" id="A2.T9.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.1.1" style="color:#000000;">Models</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.2.1" style="color:#000000;">Average</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.3.1" style="color:#000000;">Doc2Dial</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.4.1" style="color:#000000;">QuAC</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.5.1" style="color:#000000;">QReCC</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.6.1" style="color:#000000;">TopiOCQA</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A2.T9.1.1.1.1.1.1.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.1.1.7.1" style="color:#000000;">INSCIT</span></span></span>
<span class="ltx_tr" id="A2.T9.1.1.1.1.2.2">
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.1.1" style="color:#000000;">top-1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.2.1" style="color:#000000;">top-5</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.3.1" style="color:#000000;">top-1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.4.1" style="color:#000000;">top-5</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.5.1" style="color:#000000;">top-1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.6.1" style="color:#000000;">top-5</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.7.1" style="color:#000000;">top-1</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.8.1" style="color:#000000;">top-5</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.9.1" style="color:#000000;">top-5*</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.10.1" style="color:#000000;">top-20*</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.11.1" style="color:#000000;">top-5*</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.2.2.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.2.2.12.1" style="color:#000000;">top-20*</span></span></span>
<span class="ltx_tr" id="A2.T9.1.1.1.1.3.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T9.1.1.1.1.3.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.1.1" style="color:#000000;">Dragon (w/ dialog history)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.2.1" style="color:#000000;">46.29</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.3.1" style="color:#000000;">73.09</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.4.1" style="color:#000000;">43.33</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.5.1" style="color:#000000;">75.61</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.6.1" style="color:#000000;">56.8</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.7.1" style="color:#000000;">82.86</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.8.1" style="color:#000000;">46.17</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.9.1" style="color:#000000;">81.96</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.10.1" style="color:#000000;">57.68</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.11.1" style="color:#000000;">78.80</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.12.1" style="color:#000000;">27.49</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.3.3.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.3.3.13.1" style="color:#000000;">46.22</span></span></span>
<span class="ltx_tr" id="A2.T9.1.1.1.1.4.4">
<span class="ltx_td ltx_align_left" id="A2.T9.1.1.1.1.4.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.1.1" style="color:#000000;">Dragon + Rewrite (w/ dialog history)</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.2.1" style="color:#000000;">47.57</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.3.1" style="color:#000000;">74.12</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.4.1" style="color:#000000;">44.54</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.5.1" style="color:#000000;">76.98</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.6.1" style="color:#000000;">57.23</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.7.1" style="color:#000000;">83.04</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.8.1" style="color:#000000;">46.45</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.9.1" style="color:#000000;">82.60</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.10.1" style="color:#000000;">60.94</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.11.1" style="color:#000000;">81.74</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.12.1" style="color:#000000;">28.69</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.4.4.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.4.4.13.1" style="color:#000000;">46.22</span></span></span>
<span class="ltx_tr" id="A2.T9.1.1.1.1.5.5">
<span class="ltx_td ltx_align_left" id="A2.T9.1.1.1.1.5.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.1.1" style="color:#000000;">Dragon + Rewrite (w/ single query only)</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.5.5.2.1" style="color:#000000;">54.46</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.5.5.3.1" style="color:#000000;">80.13</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.4.1" style="color:#000000;">47.60</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.5.1" style="color:#000000;">80.60</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.6.1" style="color:#000000;">47.10</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.7.1" style="color:#000000;">77.15</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.8.1" style="color:#000000;">51.73</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.9.1" style="color:#000000;">85.78</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.10.1" style="color:#000000;">73.07</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.11.1" style="color:#000000;">88.19</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.12.1" style="color:#000000;">52.79</span></span>
<span class="ltx_td ltx_align_center" id="A2.T9.1.1.1.1.5.5.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.5.5.13.1" style="color:#000000;">68.92</span></span></span>
<span class="ltx_tr" id="A2.T9.1.1.1.1.6.6">
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T9.1.1.1.1.6.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.1.1" style="color:#000000;">Dragon + Fine-tune (w/ dialog history)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.2.1" style="color:#000000;">52.72</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.3.1" style="color:#000000;">80.67</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.4.1" style="color:#000000;">48.94</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.5.1" style="color:#000000;">83.01</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.6.1" style="color:#000000;">52.64</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.7.1" style="color:#000000;">81.95</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.8.1" style="color:#000000;">50.73</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.9.1" style="color:#000000;">87.17</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.10.1" style="color:#000000;">67.86</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.11.1" style="color:#000000;">86.28</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.12.1" style="color:#000000;">43.43</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A2.T9.1.1.1.1.6.6.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.6.6.13.1" style="color:#000000;">64.94</span></span></span>
<span class="ltx_tr" id="A2.T9.1.1.1.1.7.7">
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T9.1.1.1.1.7.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.1.1" style="color:#000000;">Dragon + Fine-tune + Rewrite (w/ dialog hisotry)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.7.7.2.1" style="color:#000000;">53.17</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A2.T9.1.1.1.1.7.7.3.1" style="color:#000000;">80.84</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.4.1" style="color:#000000;">49.30</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.5" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.5.1" style="color:#000000;">84.64</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.6" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.6.1" style="color:#000000;">55.04</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.7" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.7.1" style="color:#000000;">83.23</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.8" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.8.1" style="color:#000000;">51.23</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.9" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.9.1" style="color:#000000;">87.99</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.10" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.10.1" style="color:#000000;">60.50</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.11" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.11.1" style="color:#000000;">81.03</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.12" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.12.1" style="color:#000000;">49.80</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A2.T9.1.1.1.1.7.7.13" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A2.T9.1.1.1.1.7.7.13.1" style="color:#000000;">67.33</span></span></span>
</span>
</span><span class="ltx_text" id="A2.T9.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Comprehensive multi-turn retrieval results across five datasets.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">In Table&nbsp;<a class="ltx_ref" href="#A2.T9" title="Table 9 ‣ B.2 More Results for Retrieval in Conversational QA ‣ Appendix B More Details and Results for Retrieval in Conversational QA ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">9</span></a>, we show comprehensive comparisons between fine-tuning and rewriting methods.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS2.p2">
<p class="ltx_p" id="A2.SS2.p2.1">Interestingly, we find that compared to only using the rewritten query as input (Dragon + Rewrite (w/ single query only)), giving additional dialog history (Dragon + Rewrite (w/ dialog history)) makes the average scores significantly drop. This is because Dragon is originally pretrained on single-turn queries, it will naturally have better generalization ability when a single-turn rewritten query instead of a multi-turn conversation is provided. And the rewritten query already contains sufficient information from the dialog history.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A2.SS2.p3">
<p class="ltx_p" id="A2.SS2.p3.1">In addition, we observe that “Dragon + Fine-tune” performs on par with “Dragon + Fine-tune + Rewrite”. In other words, for the multi-turn fine-tuning method, replacing original query with rewritten query as inputs yields comparable results. This is because rewritten query will not provide much additional information for the model since the dialog history has already been provided, and it might even cause negative effects (e.g., results on the TopiOCQA dataset) since it makes the whole conversation input not natural. This once again demonstrates how effective the fine-tuning method is in equipping the model with the capability to comprehend the multi-turn context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Conversational QA Benchmarks</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Data Statistics</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Doc2Dial</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px1.p1.1">We use the test set of Doc2Dial for the evaluation. It consists of 719 dialogs with 3939 user-agent turns.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">QuAC</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px2.p1.1">We use the validation set of QuAC for the evaluation since its test set cannot be directly obtained. Its validation set consists of 1000 dialogs with 7354 user-agent turns. Among these 7354 user-agent turns, there are 1486 (around 20.2%) unanswerable questions. For the combined evaluation of answerable and unanswerable questions, we set the ground truth response for the unanswerable question as “<span class="ltx_text ltx_font_typewriter" id="A3.SS1.SSS0.Px2.p1.1.1">Sorry. I cannot find the answer based on the context.</span>”, same as the one we set in our stage-2 tuning. For the fair comparison, we replace the unanswerable responses in the baseline models (i.e., Llama2-SFT/Chat, GPT-3.5-turbo, GPT-4) with the same sentence (Details can be found in Appendix&nbsp;<a class="ltx_ref" href="#A5" title="Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">E</span></a>).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">QReCC</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px3.p1.1">We use the test set of QReCC for the evaluation. Its test set includes some dialogue samples source from the QuAC dataset. To avoid overlap with other benchmark datasets, we remove those QuAC-sourced samples, resulting in 2805 user-agent turns.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">TopiOCQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px4.p1.1">We use the validation set of TopiOCQA since its test set is not available yet. Its validation set consists of 205 dialogs with 2514 user-agent turns. Each question has a total of four human-written answers. It has unanswerable cases. However, we do not include this dataset in unanswerable case evaluation because we only find 46 questions with no answer being found, which is not sufficient enough for the unanswerable case evaluation. Given that this dataset use the whole Wikipedia as the retrieval corpus, we leverage its topic information for multi-turn retrieval since we focused more on conversational QA over documents which typically narrow down to a specific topic or domain.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">INSCIT</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px5.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px5.p1.1">We use the validation set of TopiOCQA since its test set is not available yet. Its validation set consists of 86 dialogues with 502 user-agent turns. Each question has an average of 1.9 human-written answers. It has unanswerable response for a few questions. However, we do not include this dataset in unanswerable case evaluation because every question always has at least one answerable response, which means every question is answerable. INSCIT also use the whole Wikipedia as the retrieval corpus. Hence, we also leverage its topic information for multi-turn retrieval for the same reason mentioned in TopiOCQA.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px6">
<h5 class="ltx_title ltx_title_paragraph">CoQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px6.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px6.p1.1">We use the validation set of CoQA since its test set cannot be directly obtained. Its validation set consists of 500 dialogues with 7983 user-agent turns. Each question has a total of four human-written answers. It has unanswerable cases. However, we do not include this dataset in unanswerable case evaluation for the same reason as in the TopiOCQA dataset. We only find 13 questions with no answer being found, which is not sufficient enough for the unanswerable case evaluation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px7">
<h5 class="ltx_title ltx_title_paragraph">DoQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px7.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px7.p1.1">We use the test set of DoQA for the evaluation. Its test set consists of 1200 dialogues with 5394 user-agent turns across Cooking, Travel and Movie domains. Among 5394 user-agent turns, there are 1479 (around 27.4%) unanswerable questions. We use the same strategy as the one mentioned in QuAC dataset to incorporate unanswerable samples into the evaluation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px8">
<h5 class="ltx_title ltx_title_paragraph">ConvFinQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px8.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px8.p1.1">We use the validation set of CoQA since its test set cannot be directly obtained. Its validation set consists of 421 dialogues with 1490 user-agent turns.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px9">
<h5 class="ltx_title ltx_title_paragraph">SQA</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px9.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px9.p1.1">We use the test set of SQA for the evaluation. Its test set consists of 1025 dialogues with 3100 user-agent turns.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS1.SSS0.Px10">
<h5 class="ltx_title ltx_title_paragraph">HybridDial</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS1.SSS0.Px10.p1">
<p class="ltx_p" id="A3.SS1.SSS0.Px10.p1.1">We use the test set of HybridDial for the evaluation. Its test set consists of 243 dialogues with 1111 user-agent turns.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Prompts for the Benchmarks</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">We use the prompt format mentioned in §<a class="ltx_ref" href="#A1.SS2" title="A.2 Stage-2: Context-Enhanced Instruction Tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">A.2</span></a> for benchmarks in ChatQA, Llama2-Chat, GPT-3.5-turbo and GPT-4. We keep the <span class="ltx_text ltx_font_typewriter" id="A3.SS2.p1.1.1">{Context for Latest Question}</span> the same for all models, while we adjust the <span class="ltx_text ltx_font_typewriter" id="A3.SS2.p1.1.2">{Instruction}</span> for the baselines (i.e., Llama2-Chat, GPT-3.5-turbo and GPT-4) to ensure they do not get sub-optimal results.
We use different instructions for test benchmarks with different answer types (e.g., long answer, short answer, arithmetic calculation). The <span class="ltx_text ltx_font_typewriter" id="A3.SS2.p1.1.3">{Instruction}</span> for all models on the benchmarks are as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="A3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.1 </span>ChatQA</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.SSS1.p1">
<p class="ltx_p" id="A3.SS2.SSS1.p1.1">We keep the <span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p1.1.1">{Instruction}</span> consistent between the data blends in stage-2 and test benchmarks based on different answer types.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS1.p2">
<p class="ltx_p" id="A3.SS2.SSS1.p2.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p2.1.1">Please give a full and complete answer for the question.</span>” for DoQA, INSCIT, HybridDial, Doc2Dial, QuAC, and QReCC, since these datasets generally have long answers for the questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS1.p3">
<p class="ltx_p" id="A3.SS2.SSS1.p3.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p3.1.1">Answer the following question with a short span, or a full and complete answer.</span>” for SQA and TopiOCQA, since these datasets have both short and long answers based on the questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS1.p4">
<p class="ltx_p" id="A3.SS2.SSS1.p4.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p4.1.1">Answer the following question with a short span. The answer needs to be just in a few words.</span>” for CoQA, since it generally has short answers for the questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS1.p5">
<p class="ltx_p" id="A3.SS2.SSS1.p5.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS1.p5.1.1">Answer the following question with a number from context or the math arithmetic using +, -, *, or /.</span>” for ConvFinQA, since this dataset requires the model to either extract numbers from the context or do arithmetic calculation. We will calculate the number based on the arithmetic formula the model generates and compare it with the gold answer.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="A3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.2 </span>Llama2-Chat</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.SSS2.p1">
<p class="ltx_p" id="A3.SS2.SSS2.p1.1">Note that we tried original Llama2-Chat prompt template<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gpus.llm-utils.org/llama-2-prompt-template/" title="">https://gpus.llm-utils.org/llama-2-prompt-template/</a></span></span></span>, which gives slightly worse results compared to using the one in §<a class="ltx_ref" href="#A1.SS2" title="A.2 Stage-2: Context-Enhanced Instruction Tuning ‣ Appendix A ChatQA Instruction Tuning ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">A.2</span></a>. We have tried several <span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p1.1.1">{Instruction}</span> for Llama2-Chat. We find the ones below works the best.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p2">
<p class="ltx_p" id="A3.SS2.SSS2.p2.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p2.1.1">Please give an answer in just one sentence.</span>” for DoQA, INSCIT, HybridDial, Doc2Dial, QuAC, and QReCC, since these datasets generally have long answers but within one sentence. We notice that it is important to give Llama2-Chat specific instruction like “one sentence” instead of “full and complete answer” to prevent the model from generating very long answers.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p3">
<p class="ltx_p" id="A3.SS2.SSS2.p3.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p3.1.1">Answer the following question with a short span, or one sentence.</span>” for TopiOCQA, since this dataset has both short and long answers based on the questions, and the long answers are generally within one sentence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p4">
<p class="ltx_p" id="A3.SS2.SSS2.p4.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p4.1.1">Answer the following questions with one or a list of entities.</span>” for SQA, since the answer for this dataset always consists of one or a list of entities from the context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p5">
<p class="ltx_p" id="A3.SS2.SSS2.p5.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p5.1.1">Answer the following question with a short span. The answer needs to be just in a few words.</span>” for CoQA, since it generally has short answers for the questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS2.p6">
<p class="ltx_p" id="A3.SS2.SSS2.p6.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS2.p6.1.1">Answer the following question with just a number from context or just the math arithmetic using +, -, *, or /.</span>” for ConvFinQA, since this dataset requires the model to either extract numbers from the context or do arithmetic calculation. We extract the arithmetic formula generated by the model and use a calculator to get its final result.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="A3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">C.2.3 </span>GPT-3.5-turbo &amp; GPT-4</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A3.SS2.SSS3.p1">
<p class="ltx_p" id="A3.SS2.SSS3.p1.1">We have tried several <span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p1.1.1">{Instruction}</span> for GPT-3.5-turbo and GPT-4, we find the ones below works the best (applied for both GPT-3.5-turbo and GPT-4).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p2">
<p class="ltx_p" id="A3.SS2.SSS3.p2.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p2.1.1">Please give an answer in just one sentence.</span>” for DoQA, INSCIT, HybridDial, Doc2Dial, QuAC, and QReCC, since these datasets generally have long answers but within one sentence. Similar to Llama2-Chat, we also find OpenAI models tend to generate quite long answers given the instruction of “full and complete answer”. Hence, we make the instruction more specific (i.e., “one sentence”) to prevent the model from generating very long answers.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p3">
<p class="ltx_p" id="A3.SS2.SSS3.p3.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p3.1.1">Answer the following questions in JUST a few words or one sentence.</span>” for TopiOCQA, since this dataset has both short and long answers based on the questions, and the long answers are generally within one sentence.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p4">
<p class="ltx_p" id="A3.SS2.SSS3.p4.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p4.1.1">Answer the following questions with one or a list of entities. Do not give a detailed explanation. Answer needs to be as short as possible.</span>” for SQA. We find that OpenAI models often give detailed explanation for SQA dataset unless specifically instructed not to.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p5">
<p class="ltx_p" id="A3.SS2.SSS3.p5.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p5.1.1">Answer the following question with a short span. The answer needs to be just in a few words.</span>” for CoQA, since it generally has short answers for the questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A3.SS2.SSS3.p6">
<p class="ltx_p" id="A3.SS2.SSS3.p6.1">We use “<span class="ltx_text ltx_font_typewriter" id="A3.SS2.SSS3.p6.1.1">Answer the following questions with just a number from context or just the math arithmetic using +, -, *, or /.</span>” for ConvFinQA, since this dataset requires the model to either extract numbers from the context or do arithmetic calculation. We extract the arithmetic formula generated by the model and use a calculator to get its final result.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Human Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We conduct human evaluation across the ten test benchmark datasets. We randomly select 60 samples for each dataset, and each sample is labelled by three annotators, which results in a total of 1800 annotations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">We ask the annotators to verify the facts in models’ outputs and determine which model provides a more accurate response to the question. We use Amazon Mechanical Turk platform to conduct the human evaluation. The details of our human evaluation instructions and the interface shown to annotators are illustrated in Figure&nbsp;<a class="ltx_ref" href="#A4.F3" title="Figure 3 ‣ Appendix D Human Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">3</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="A4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="547" id="A4.F3.g1" src="x3.png" width="829">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Human evaluation instructions and the interface for annotators.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Unanswerable Case Evaluation</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">For unanswerable case, we consider the model indicating that the question cannot be answered as correct. For answerable cases, we consider the model not indicating the question is unanswerable as correct (i.e., the model giving an answer).
We find that ChatQA always generates “<span class="ltx_text ltx_font_typewriter" id="A5.p1.1.1">Sorry. I cannot find the answer based on the context</span>” when it finds the question is unanswerable, since we use this sentence as a response for unanswerable questions in the stage-2 tuning. As for Llama2-Chat, GPT-3.5-turbo, and GPT-4, their outputs for unanswerable questions usually follow some specific patterns. Below, we list the heuristic matching patterns derived from all generated samples in DoQA and QuAC datasets to determine if the model suggests that the question is unanswerable. When we find the generated output contains the pattern listed below, we interpret it as an indication that the question is unanswerable, and we will replace its answer with “<span class="ltx_text ltx_font_typewriter" id="A5.p1.1.2">Sorry. I cannot find the answer based on the context.</span>” for the F1 evaluation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A5.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="A5.p2.1">i’m not sure, cannot find, does not provide, cannot provide, cannot answer,
cannot be found, cannot be determined, don’t have information, do not have
information, couldn’t find, no information in the context, does not mention,
not explicitly mentioned, i don’t have any, i do not have any, does not
specify, doesn’t provide, not able to, unable to, doesn’t specify, there is
no information, there is no mention, not mentioned, i don’t have enough
information, there is no specific information, there is no specific mention,
no information found, I don’t have that information
</pre>
</div>
<figure class="ltx_table" id="A5.tab1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.tab1.1" style="width:551.7pt;height:964pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A5.tab1.1.1"><span class="ltx_text" id="A5.tab1.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.tab1.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.tab1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.1.1.1.1" style="color:#000000;">Context</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.tab1.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.1.1.2.1" style="color:#000000;">Conversation</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.tab1.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.1.1.3.1" style="color:#000000;">Assistant Responses</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.2.1.1.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.2.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.2.1.1.1.1.1.1" style="color:#000000;">Pos</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.2.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.2.1.1.1.1.2.1" style="color:#000000;">Rider</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.2.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.2.1.1.1.1.3.1" style="color:#000000;">Time/Retired</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.2.1.1" style="color:#000000;">1</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.1.1.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.2.2.1" style="color:#000000;">Loris Capirossi</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.1.1.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.2.3.1" style="color:#000000;">38:04.730</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.3.1.1" style="color:#000000;">2</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.3.2.1" style="color:#000000;">Valentino Rossi</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.3.3.1" style="color:#000000;">+0.180</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.4.1.1" style="color:#000000;">3</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.4.2.1" style="color:#000000;">Jeremy McWilliams</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.4.3.1" style="color:#000000;">+0.534</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.5.1.1" style="color:#000000;">4</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.5.2.1" style="color:#000000;">Tohru Ukawa</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.5.3.1" style="color:#000000;">+0.537</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.6">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.6.1.1" style="color:#000000;">5</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.6.2.1" style="color:#000000;">Shinya Nakano</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.6.3.1" style="color:#000000;">+0.742</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.7">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.7.1.1" style="color:#000000;">6</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.7.2.1" style="color:#000000;">Ralf Waldmann</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.7.3.1" style="color:#000000;">+7.019</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.8">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.8.1.1" style="color:#000000;">7</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.8.2.1" style="color:#000000;">Franco Battaini</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.8.3.1" style="color:#000000;">+20.889</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.9">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.9.1.1" style="color:#000000;">8</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.9.2.1" style="color:#000000;">Stefano Perugini</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.9.3.1" style="color:#000000;">+20.891</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.10">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.10.1.1" style="color:#000000;">9</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.10.2.1" style="color:#000000;">Jason Vincent</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.10.3.1" style="color:#000000;">+21.310</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.11">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.11.1.1" style="color:#000000;">10</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.11.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.11.2.1" style="color:#000000;">Anthony West</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.11.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.11.3.1" style="color:#000000;">+26.816</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.12">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.12.1.1" style="color:#000000;">11</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.12.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.12.2.1" style="color:#000000;">Alex Hofmann</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.12.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.12.3.1" style="color:#000000;">+26.933</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.13">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.13.1.1" style="color:#000000;">…</span></span>
<span class="ltx_td" id="A5.tab1.1.1.1.1.2.1.1.1.13.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td" id="A5.tab1.1.1.1.1.2.1.1.1.13.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.14">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.14.1.1" style="color:#000000;">23</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.14.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.14.2.1" style="color:#000000;">Arno Visscher</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.14.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.14.3.1" style="color:#000000;">+1:40.635</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.15">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.15.1.1" style="color:#000000;">24</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.15.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.15.2.1" style="color:#000000;">Henk Van De Lagemaat</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.15.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.15.3.1" style="color:#0000FF;">+1 Lap</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.16">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.16.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.16.1.1" style="color:#000000;">Ret</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.16.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.16.2.1" style="color:#000000;">Maurice Bolwerk</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.16.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.16.3.1" style="color:#000000;">Retirement</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.17">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.17.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.17.1.1" style="color:#000000;">Ret</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.17.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.17.2.1" style="color:#000000;">Andre Romein</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.17.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.17.3.1" style="color:#000000;">Retirement</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.18">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.18.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.18.1.1" style="color:#000000;">Ret</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.18.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.18.2.1" style="color:#000000;">Roberto Rolfo</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.1.1.18.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.18.3.1" style="color:#000000;">Retirement</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.1.1.19">
<span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.2.1.1.1.19.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.19.1.1" style="color:#000000;">Ret</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.2.1.1.1.19.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.19.2.1" style="color:#000000;">Marcellino Lucchi</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.2.1.1.1.19.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.1.1.19.3.1" style="color:#000000;">Retirement</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.2.1.2.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.1">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.1.1.1" style="color:#000000;">User: what racers are shown?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.2">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.2.1.1" style="color:#000000;">Assistant: Loris Capirossi, Valentino Rossi,</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.3.1.1" style="color:#000000;">Jeremy McWilliams, Tohru Ukawa, … ,</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.4.1.1" style="color:#000000;">Henk Van De Lagemaat, Maurice Bolwerk,</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.5.1.1" style="color:#000000;">Roberto Rolfo, Marcellino Lucchi.</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.6">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.6.1.1" style="color:#000000;">User: of those, what times were recorded?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.7">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.7.1.1" style="color:#000000;">Assistant: 38:04.730, +0.180, +0.534,</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.8">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.8.1.1" style="color:#000000;">+0.537, … , +1:40.635, +1 Lap, Retirement.</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.9">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.9.1.1" style="color:#000000;">User: of those times, which were not</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.10">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.10.1.1" style="color:#000000;">actual times?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.11">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.11.1.1" style="color:#000000;">Assistant: +1 Lap, Retirement</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.12">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.12.1.1" style="color:#000000;">User: which of was an actual finish?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.2.1.13">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.2.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.2.1.13.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.2.1.3.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.1">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.1.1.1" style="color:#000000;">ChatQA-13B: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.1.1.2" style="color:#FF0000;">Loris Capirossi</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.2">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.2.1.1" style="color:#000000;">ChatQA-70B: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.2.1.2" style="color:#41B729;">+1 Lap</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.3.1.1" style="color:#000000;">GPT-3.5-turbo: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.3.1.2" style="color:#FF0000;">38:04.730</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.4.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.4.1.2" style="color:#FF0000;">38:04.730</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.2.1.3.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.2.1.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.2.1.3.1.5.1.1" style="color:#000000;">Ground Truth: +1 Lap</span></span></span>
</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.1.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1">
<span class="ltx_td ltx_align_left ltx_colspan ltx_colspan_3" id="A5.tab1.1.1.1.1.3.2.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.1.1.1" style="color:#000000;">2014 compared to 2013 mst 2019s net sales</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.2.1.1" style="color:#000000;">decreased $ 305 million, or 3% in 2014 as</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.3.1.1" style="color:#000000;">compared to 2013 … space systems 2019</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.4.1.1" style="color:#000000;">operating results included the following</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.1.5.1.1" style="color:#000000;">(in millions):</span></span></span>
</span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.1.1.2" style="color:#000000;"></span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.2">
<span class="ltx_td ltx_border_tt" id="A5.tab1.1.1.1.1.3.2.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.3.2.1.1.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.3.2.1.1.2.2.1" style="color:#000000;">net sales</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.3.2.1.1.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.3.2.1.1.2.3.1" style="color:#000000;">backlog at year-end</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.3.1.1" style="color:#000000;">2015</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.1.1.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.3.2.1" style="color:#000000;">$ 9105</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.1.1.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.3.3.1" style="color:#0000FF;">$ 17400</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.4.1.1" style="color:#000000;">2014</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.4.2.1" style="color:#000000;">$ 9202</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.4.3.1" style="color:#0000FF;">$ 20300</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.5.1.1" style="color:#000000;">2013</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.5.2.1" style="color:#000000;">$ 9288</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.5.3.1" style="color:#0000FF;">$ 21400</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6">
<span class="ltx_td ltx_align_left ltx_border_tt ltx_colspan ltx_colspan_3" id="A5.tab1.1.1.1.1.3.2.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.1">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.1.1.1" style="color:#000000;">2015 compared to 2014 space systems 2019</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.2">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.2.1.1" style="color:#000000;">net sales in 2015 decreased $ 97 million, or</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.3.1.1" style="color:#000000;">1%, compared to 2014.
these decreases were</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.4.1.1" style="color:#000000;">partially offset by higher net sales of</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.5.1.1" style="color:#000000;">approximately $ 235 million for businesses</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.6">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.6.1.1" style="color:#000000;">acquired in 2014; and approximately $ 75</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.7">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.7.1.1" style="color:#000000;">million for the orion program due to</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.8">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.1.8.1.1" style="color:#000000;">increased volume.</span></span></span>
</span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.1.1.6.1.2" style="color:#000000;"></span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.2.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.1">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.1.1.1" style="color:#000000;">User: what was the average backlog in 2015?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.2">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.2.1.1" style="color:#000000;">Assistant: 17400</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.3.1.1" style="color:#000000;">User: what was it in 2014?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.4.1.1" style="color:#000000;">Assistant: 20300</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.5.1.1" style="color:#000000;">User: what is the sum of those years?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.6">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.6.1.1" style="color:#000000;">Assistant: 17400 + 20300</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.7">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.7.1.1" style="color:#000000;">User: what was the average backlog in 2013?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.8">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.8.1.1" style="color:#000000;">Assistant: 21400</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.9">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.9.1.1" style="color:#000000;">User: what is the total sum for all 3 years?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.2.1.10">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.2.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.2.1.10.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.3.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.3.2.3.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.1">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.1.1.1" style="color:#000000;">ChatQA-13B: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.1.1.2" style="color:#41B729;">17400 + 20300 + 21400</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.2">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.2.1.1" style="color:#000000;">ChatQA-70B: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.2.1.2" style="color:#41B729;">17400 + 20300 + 21400</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.3.1.1" style="color:#000000;">GPT-3.5-turbo: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.3.1.2" style="color:#FF0000;">9288 + 9202 + 9105</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.4.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.4.1.2" style="color:#41B729;">17400 + 20300 + 21400</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.3.2.3.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.3.2.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.3.2.3.1.5.1.1" style="color:#000000;">Ground Truth: 17400 + 20300 + 21400</span></span></span>
</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.4.3.1.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.1">
<span class="ltx_td ltx_align_left ltx_colspan ltx_colspan_4" id="A5.tab1.1.1.1.1.4.3.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.1.1.1" style="color:#000000;">The XVI World Rhythmic Gymnastics</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.2.1.1" style="color:#000000;">Championships were held in Brussels,</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.1.3.1.1" style="color:#000000;">Belgium, on November 20-22, 1992.</span></span></span>
</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.1.1.2" style="color:#000000;"></span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.4.3.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.4.3.1.1.2.1.1" style="color:#000000;">Rank</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.4.3.1.1.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.4.3.1.1.2.2.1" style="color:#000000;">Gymnast</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.4.3.1.1.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.4.3.1.1.2.3.1" style="color:#000000;">Country</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.tab1.1.1.1.1.4.3.1.1.2.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.tab1.1.1.1.1.4.3.1.1.2.4.1" style="color:#000000;">Point</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.3.1.1" style="color:#000000;">1</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1.1.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.3.2.1" style="color:#000000;">Oxana Kostina</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1.1.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.3.3.1" style="color:#000000;">Russia</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.tab1.1.1.1.1.4.3.1.1.3.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.3.4.1" style="color:#000000;">9.775</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.4.1.1" style="color:#000000;">2</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.4.2.1" style="color:#000000;">Maria Petrova</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.4.3.1" style="color:#000000;">Bulgaria</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.4.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.4.4.1" style="color:#000000;">9.700</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.5.1.1" style="color:#000000;">3</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.5.2.1" style="color:#000000;">Diana Popova</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.5.3.1" style="color:#000000;">Bulgaria</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.5.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.5.4.1" style="color:#000000;">9.625</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.6">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.6.1.1" style="color:#000000;">4</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.6.2.1" style="color:#000000;">Carmen Acedo</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.6.3.1" style="color:#000000;">Spain</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.6.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.6.4.1" style="color:#000000;">9.625</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.7">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.7.1.1" style="color:#000000;">5</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.7.2.1" style="color:#000000;">Irina Deleanu</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.7.3.1" style="color:#000000;">Romania</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.7.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.7.4.1" style="color:#000000;">9.550</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.8">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.8.1.1" style="color:#000000;">6</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.8.2.1" style="color:#000000;">Ekaterina S.</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.8.3.1" style="color:#000000;">Ukraine</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.8.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.8.4.1" style="color:#000000;">9.525</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.9">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.9.1.1" style="color:#000000;">7</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.9.2.1" style="color:#000000;">Viktoriya Yani</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.9.3.1" style="color:#000000;">Ukraine</span></span>
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.1.1.9.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.9.4.1" style="color:#000000;">9.475</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.1.1.10">
<span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.4.3.1.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.10.1.1" style="color:#0000FF;">8</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.4.3.1.1.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.10.2.1" style="color:#000000;">Maria Sansaridou</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.4.3.1.1.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.10.3.1" style="color:#000000;">Greece</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A5.tab1.1.1.1.1.4.3.1.1.10.4" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.1.1.10.4.1" style="color:#000000;">9.425</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.tab1.1.1.1.1.4.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.4.3.2.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.1">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.1.1.1" style="color:#000000;">User: What is the XVI World Rhythmic</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.2">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.2.1.1" style="color:#000000;">Gymnastics Championships?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.3.1.1" style="color:#000000;">Assistant: The XVI World Rhythmic</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.4.1.1" style="color:#000000;">Gymnastics Championships were held in</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.5.1.1" style="color:#000000;">Brussels, Belgium, on November</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.6">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.6.1.1" style="color:#000000;">20-22, 1992</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.7">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.7.1.1" style="color:#000000;">User: How many </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.7.1.2" style="color:#FF8000;">Gymnastics</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.7.1.3" style="color:#000000;"> participated</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.8">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.8.1.1" style="color:#000000;">this championship?</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.2.1.9">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.2.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.2.1.9.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.tab1.1.1.1.1.4.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.tab1.1.1.1.1.4.3.3.1">
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.1">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.1.1.1" style="color:#000000;">ChatQA-13B: There are </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.1.1.2" style="color:#41B729;">8</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.1.1.3" style="color:#000000;"> Gymnastics</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.2">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.2.1.1" style="color:#000000;">participated in the championship.</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.3">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.3.1.1" style="color:#000000;">ChatQA-70B: There are </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.3.1.2" style="color:#41B729;">8</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.3.1.3" style="color:#000000;"> </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.3.1.4" style="color:#FF8000;">gymnasts*</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.4">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.4.1.1" style="color:#000000;">participated in the championship.</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.5">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.5.1.1" style="color:#000000;">GPT-3.5-turbo: There were a total of </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.5.1.2" style="color:#41B729;">8</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.6">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.6.1.1" style="color:#FF8000;">gymnasts*</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.6.1.2" style="color:#000000;"> who participated in</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.7">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.7.1.1" style="color:#000000;">this championship.</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.8">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.8.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.8.1.2" style="color:#FF0000;">The context does not provide</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.9">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.9.1.1" style="color:#FF0000;">the total number of gymnasts</span><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.9.1.2" style="color:#000000;"> who</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.10">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.10.1.1" style="color:#000000;">participated in the XVI World</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.11">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.11.1.1" style="color:#000000;">Rhythmic Gymnastics Championships.</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.12">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.12.1.1" style="color:#000000;">Ground Truth: There are 8 gymnasts</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.13">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.13.1.1" style="color:#000000;">participated this championship.</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.14">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.14.1.1" style="color:#FF8000;">*Both models correct the User’s typo of</span></span></span>
<span class="ltx_tr" id="A5.tab1.1.1.1.1.4.3.3.1.15">
<span class="ltx_td ltx_align_left" id="A5.tab1.1.1.1.1.4.3.3.1.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.tab1.1.1.1.1.4.3.3.1.15.1.1" style="color:#FF8000;">“Gymnastics” to “gymnasts”</span></span></span>
</span></span></span>
</span>
</span><span class="ltx_text" id="A5.tab1.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="A5.T10">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T10.1" style="width:730.8pt;height:1084.6pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A5.T10.1.1"><span class="ltx_text" id="A5.T10.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.T10.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="A5.T10.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.T10.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.1.1.1.1.1.1.1" style="color:#000000;">Context</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.T10.1.1.1.1.1.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.1.1.1.1.1.2.1" style="color:#000000;">Conversation</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A5.T10.1.1.1.1.1.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.1.1.1.1.1.3.1" style="color:#000000;">Assistant Responses</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.2.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.2.1.1.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_colspan ltx_colspan_3" id="A5.T10.1.1.1.1.2.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.1.1.1" style="color:#000000;">The 3000 metres steeplechase or 3000-meter</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.2">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.2.1.1" style="color:#000000;">steeplechase is the most common distance for the</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.3">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.3.1.1" style="color:#000000;">steeplechase in track and field. It is an obstacle</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.4.1.1" style="color:#000000;">race over the distance of the 3000 metres, which</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.1.1.1.5.1.1" style="color:#000000;">derives its name from the horse racing steeplechase.</span></span></span>
</span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.1.1.2" style="color:#000000;"></span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.T10.1.1.1.1.2.1.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.1.1.1.2.1.1.1.2.1.1" style="color:#000000;">Year</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.T10.1.1.1.1.2.1.1.1.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.1.1.1.2.1.1.1.2.2.1" style="color:#000000;">Athlete</span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A5.T10.1.1.1.1.2.1.1.1.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text ltx_font_bold" id="A5.T10.1.1.1.1.2.1.1.1.2.3.1" style="color:#000000;">Location</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.2.1.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.3.1.1" style="color:#000000;">1968</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.2.1.1.1.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.3.2.1" style="color:#000000;">Jouko Kuha (FIN)</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.2.1.1.1.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.3.3.1" style="color:#000000;">Stockholm</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.4.1.1" style="color:#000000;">1969</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.4.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.4.2.1" style="color:#000000;">Vladimiras Dudinas (URS)</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.4.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.4.3.1" style="color:#000000;">Kiev</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.5.1.1" style="color:#000000;">1970</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.5.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.5.2.1" style="color:#000000;">Kerry O’Brien (AUS)</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.5.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.5.3.1" style="color:#0000FF;">Berlin</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.6">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.6.1.1" style="color:#000000;">1971</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.6.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.6.2.1" style="color:#000000;">Kerry O’Brien (AUS)</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.6.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.6.3.1" style="color:#0000FF;">Adelaide</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.7">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.7.1.1" style="color:#000000;">1972</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.7.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.7.2.1" style="color:#000000;">Anders Gärderud (SWE)</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.7.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.7.3.1" style="color:#000000;">Helsinki</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.8">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.8.1.1" style="color:#000000;">1973</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.8.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.8.2.1" style="color:#000000;">Ben Jipcho (KEN)</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.8.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.8.3.1" style="color:#000000;">Helsinki</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.9">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.9.1.1" style="color:#000000;">1974</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.9.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.9.2.1" style="color:#000000;">Anders Gärderud (SWE)</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.9.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.9.3.1" style="color:#000000;">Helsinki</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.10">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.10.1.1" style="color:#000000;">1975</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.10.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.10.2.1" style="color:#000000;">Anders Gärderud (SWE)</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.10.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.10.3.1" style="color:#000000;">Stockholm</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.11">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.11.1.1" style="color:#000000;">1976</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.11.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.11.2.1" style="color:#000000;">Anders Gärderud (SWE)</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.11.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.11.3.1" style="color:#000000;">Montréal</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.12">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.12.1.1" style="color:#000000;">1977</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.12.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.12.2.1" style="color:#000000;">Michael Karst (FRG)</span></span>
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.1.1.12.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.12.3.1" style="color:#000000;">Stockholm</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.1.1.13">
<span class="ltx_td ltx_align_left ltx_border_bb" id="A5.T10.1.1.1.1.2.1.1.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.1.1.13.1.1" style="color:#000000;">…</span></span>
<span class="ltx_td ltx_border_bb" id="A5.T10.1.1.1.1.2.1.1.1.13.2" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span>
<span class="ltx_td ltx_border_bb" id="A5.T10.1.1.1.1.2.1.1.1.13.3" style="padding-top:0.15pt;padding-bottom:0.15pt;"></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.2.1.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.2.1.2.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.1">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.1.1.1" style="color:#000000;">User: Hello, can you tell me about the 3000</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.2">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.2.1.1" style="color:#000000;">metres steeplechase?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.3">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.3.1.1" style="color:#000000;">Assistant: The 3000 metres steeplechase is the</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.4.1.1" style="color:#000000;">most common distance for the steeplechase in</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.5.1.1" style="color:#000000;">track and field that goes over the distance of</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.6">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.6.1.1" style="color:#000000;">the 3000 metres, which derives its name from</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.7">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.7.1.1" style="color:#000000;">the horse racing steeplechase.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.8">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.8.1.1" style="color:#000000;">User: Can you tell me the men that are part of it?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.9">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.9.1.1" style="color:#000000;">Assistant: Some of the men are Henry Rono and</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.10">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.10.1.1" style="color:#000000;">Michael Karst.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.11">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.11.1.1" style="color:#000000;">User: Which locations did Kerry O’Brien ran for?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.2.1.12">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.2.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.2.1.12.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.2.1.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.2.1.3.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.1">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.1.1.1" style="color:#000000;">ChatQA-13B: He ran in </span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.1.1.2" style="color:#41B729;">Berlin, Adelaide</span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.1.1.3" style="color:#000000;">, </span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.1.1.4" style="color:#FF0000;">and Stockholm.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.2">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.2.1.1" style="color:#000000;">ChatQA-70B: Kerry O’Brien ran in </span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.2.1.2" style="color:#41B729;">Adelaide and Berlin.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.3">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.3.1.1" style="color:#000000;">GPT-3.5-turbo: Kerry O’Brien ran in </span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.3.1.2" style="color:#41B729;">Berlin, Adelaide</span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.3.1.3" style="color:#000000;">,</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.4.1.1" style="color:#FF0000;">and Champaign</span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.4.1.2" style="color:#000000;">.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.5.1.1" style="color:#000000;">GPT-4: Kerry O’Brien ran in </span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.5.1.2" style="color:#41B729;">Berlin and Adelaide</span><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.5.1.3" style="color:#000000;">.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.2.1.3.1.6">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.2.1.3.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.2.1.3.1.6.1.1" style="color:#000000;">Ground Truth: He ran for both Berlin and Adelaide.</span></span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.3.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.3.2.1.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.1">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.1.1.1" style="color:#000000;">title: Non-EU family member traveling to Schengen</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.2">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.2.1.1" style="color:#000000;">countries. source: You can travel visa-free only if you</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.3">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.3.1.1" style="color:#000000;">have a “residence” card of a family member of a union</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.4.1.1" style="color:#000000;">citizen” issued under article 10 of the free movement</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.5.1.1" style="color:#000000;">directive (or if you are a citizen of a country whose</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.6">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.6.1.1" style="color:#000000;">nationals don’t require visas in the Schengen area). A</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.7">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.7.1.1" style="color:#000000;">residence card issued by Romania to the spouse of a</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.8">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.8.1.1" style="color:#000000;">Romanian citizen is not necessarily an article 10 card.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.9">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.9.1.1" style="color:#000000;">If you do not have an article 10 card, EU countries</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.10">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.10.1.1" style="color:#000000;">other than Romania must issue you a visa free of charge</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.11">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.11.1.1" style="color:#000000;">and on the basis of an expedited procedure. The grounds</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.12">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.12.1.1" style="color:#000000;">for refusal of that visa are strictly limited. These</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.13">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.13.1.1" style="color:#000000;">provisions are also dependent on your traveling with or</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.14">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.14.1.1" style="color:#000000;">joining your spouse. In case you are not, you’ll need a</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.15">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.15.1.1" style="color:#000000;">regular Schengen visa. As Relaxed pointed out in a</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.16">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.16.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.16.1.1" style="color:#000000;">comment, it appears that your concern stems in part</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.17">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.17.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.17.1.1" style="color:#000000;">from the fact that Romania is not yet a full participant</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.18">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.18.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.18.1.1" style="color:#000000;">in the Schengen agreement. These rules apply to all</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.19">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.19.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.19.1.1" style="color:#000000;">EU countries, not just Schengen countries, because</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.1.1.20">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.1.1.20.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.1.1.20.1.1" style="color:#000000;">they are based on the EU right of freedom of movement.</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.3.2.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.3.2.2.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.1">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.1.1.1" style="color:#000000;">User: Can I travel from Romania to the Schengen</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.2">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.2.1.1" style="color:#000000;">area visa-free?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.3">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.3.1.1" style="color:#000000;">Assistant: You can travel visa-free only if you</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.4.1.1" style="color:#000000;">have a “residence card of a family member of a</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.5.1.1" style="color:#000000;">union citizen” issued under article 10 of the free</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.6">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.6.1.1" style="color:#000000;">movement directive</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.7">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.7.1.1" style="color:#000000;">User: I have a temporary residence card also, can</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.8">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.8.1.1" style="color:#000000;">we travel together visa-free?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.9">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.9.1.1" style="color:#000000;">Assistant: If you do not have an article 10 card, EU</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.10">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.10.1.1" style="color:#000000;">countries other than Romania must issue you a visa</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.11">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.11.1.1" style="color:#000000;">free of charge and on the basis of an expedited</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.12">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.12.1.1" style="color:#000000;">procedure.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.13">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.13.1.1" style="color:#000000;">User: How do I get a article 10 card?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.2.1.14">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.2.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.2.1.14.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A5.T10.1.1.1.1.3.2.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.3.2.3.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.1">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.1.1.1" style="color:#000000;">ChatQA-13B: </span><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.1.1.2" style="color:#FF0000;">The grounds for refusal of that visa are</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.2">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.2.1.1" style="color:#FF0000;">strictly limited. These provisions are also dependent</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.3">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.3.1.1" style="color:#FF0000;">on your traveling with or joining your spouse. In</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.4.1.1" style="color:#FF0000;">case you are not, you’ll need a regular Schengen visa.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.5.1.1" style="color:#000000;">ChatQA-70B: </span><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.5.1.2" style="color:#41B729;">Sorry. I cannot find the answer based on</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.6">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.6.1.1" style="color:#41B729;">the context.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.7">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.7.1.1" style="color:#000000;">GPT-3.5-turbo: </span><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.7.1.2" style="color:#FF0000;">To obtain an article 10 card, you would</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.8">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.8.1.1" style="color:#FF0000;">need to meet the requirements set out in the free</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.9">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.9.1.1" style="color:#FF0000;">movement directive and apply for it through the</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.10">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.10.1.1" style="color:#FF0000;">appropriate channels in Romania.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.11">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.11.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.11.1.2" style="color:#41B729;">The text does not provide information on how</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.12">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.12.1.1" style="color:#41B729;">to obtain an article 10 card.<span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.12.1.1.1" style="color:#FF8000;">*</span></span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.13">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.13.1.1" style="color:#000000;">Ground Truth: Sorry. I cannot find the answer based</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.14">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.14.1.1" style="color:#000000;">on the context.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.15">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.15.1.1" style="color:#FF8000;">*In the evaluation, we will detect this unanswerable</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.16">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.16.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.16.1.1" style="color:#FF8000;">pattern and replace it with “Sorry. I cannot find the</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.17">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.17.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.17.1.1" style="color:#FF8000;">answer based on the context” to match with the</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.3.2.3.1.18">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.3.2.3.1.18.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.3.2.3.1.18.1.1" style="color:#FF8000;">ground truth.</span></span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3">
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.T10.1.1.1.1.4.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.4.3.1.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.1">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.1.1.1" style="color:#0000FF;">title: Bengal fox. “Vulpes bengalensis” is a relatively</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.2">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.2.1.1" style="color:#0000FF;">small fox with an elongated muzzle, long, pointed</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.3">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.3.1.1" style="color:#0000FF;">ears, and a bushy tail about 50 to 60% of the length</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.4.1.1" style="color:#0000FF;">of the head and body.</span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.4.1.2" style="color:#000000;"> Its dorsal pelage is very variable,</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.5.1.1" style="color:#000000;">but mostly grayish and paler ventrally; its legs tend</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.6">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.6.1.1" style="color:#000000;">to be brownish or rufous. It is more daintily built than</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.7">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.7.1.1" style="color:#000000;">“Vulpes vulpes”. The tail is bushy with a prominent</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.8">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.8.1.1" style="color:#000000;">black tip which distinguishes it from ”V. vulpes”. Back</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.9">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.9.1.1" style="color:#000000;">of ears are dark brown with black margin. Its rhinarium</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.10">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.10.1.1" style="color:#000000;">is naked and the lips are black, with small black hair</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.11">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.11.1.1" style="color:#000000;">patches on upper part of nuzzle (shaft) in front of eyes.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.12">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.12.1.1" style="color:#000000;">title: Red fox. The red fox is considered a more</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.13">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.13.1.1" style="color:#000000;">specialised form of ”Vulpes” than the Afghan, corsac</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.14">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.14.1.1" style="color:#000000;">and Bengal foxes in the direction of size and adaptation</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.15">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.15.1.1" style="color:#000000;">to carnivory; the skull displays far fewer neotenous</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.16">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.16.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.16.1.1" style="color:#000000;">traits than in other species …</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.17">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.17.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.17.1.1" style="color:#0000FF;">title: Bengal fox. The ears have the same colour as the</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.18">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.18.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.18.1.1" style="color:#0000FF;">nape or maybe darker, but not having a dark patch as in</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.19">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.19.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.19.1.1" style="color:#0000FF;">“V. vulpes”. Extensive variation in coat colour exists</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.20">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.20.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.20.1.1" style="color:#0000FF;">across populations and seasonally within populations,</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.1.1.21">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.1.1.21.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.21.1.1" style="color:#0000FF;">but generally varies from grey to pale brown</span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.1.1.21.1.2" style="color:#000000;"> …</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.T10.1.1.1.1.4.3.2" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.4.3.2.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.1">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.1.1.1" style="color:#000000;">User: what is red fox?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.2">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.2.1.1" style="color:#000000;">Assistant:
It is the largest of the true foxes and</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.3">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.3.1.1" style="color:#000000;">one of the most widely distributed members of</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.4.1.1" style="color:#000000;">the order Carnivora.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.5.1.1" style="color:#000000;">User: where did it originate?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.6">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.6.1.1" style="color:#000000;">Assistant: The species is Eurasian in origin.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.7">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.7.1.1" style="color:#000000;">User: how did it evolve?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.8">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.8.1.1" style="color:#000000;">Assistant:
It is considered a more specialised form</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.9">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.9.1.1" style="color:#000000;">of “Vulpes” than the Afghan, corsac and </span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.9.1.2" style="color:#0000FF;">Bengal</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.10">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.10.1.1" style="color:#0000FF;">foxes</span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.10.1.2" style="color:#000000;"> in the direction of size and adaptation</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.11">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.11.1.1" style="color:#000000;">to carnivory.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.12">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.12.1.1" style="color:#000000;">User: what is the appearance of </span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.12.1.2" style="color:#0000FF;">this third type</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.13">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.13.1.1" style="color:#0000FF;">of fox</span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.13.1.2" style="color:#000000;">?</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.2.1.14">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.2.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.2.1.14.1.1" style="color:#000000;">Assistant:</span></span></span>
</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A5.T10.1.1.1.1.4.3.3" style="padding-top:0.15pt;padding-bottom:0.15pt;">
<span class="ltx_tabular ltx_align_middle" id="A5.T10.1.1.1.1.4.3.3.1">
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.1">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.1.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.1.1.1" style="color:#000000;">ChatQA-13B:
</span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.1.1.2" style="color:#41B729;">The ears have the same colour as the</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.2">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.2.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.2.1.1" style="color:#41B729;">nape or maybe darker, but not having a dark patch</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.3">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.3.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.3.1.1" style="color:#41B729;">as in “V. vulpes”. Extensive variation in coat colour</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.4">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.4.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.4.1.1" style="color:#41B729;">exists across populations and seasonally within</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.5">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.5.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.5.1.1" style="color:#41B729;">populations, but generally varies from grey to pale</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.6">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.6.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.6.1.1" style="color:#41B729;">brown.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.7">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.7.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.7.1.1" style="color:#000000;">ChatQA-70B:
</span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.7.1.2" style="color:#41B729;">“Vulpes bengalensis” is a relatively</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.8">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.8.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.8.1.1" style="color:#41B729;">small fox with an elongated muzzle, long, pointed</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.9">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.9.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.9.1.1" style="color:#41B729;">ears, and a bushy tail about 50 to 60% of the length</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.10">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.10.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.10.1.1" style="color:#41B729;">of the head and body.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.11">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.11.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.11.1.1" style="color:#000000;">GPT-3.5-turbo: </span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.11.1.2" style="color:#FF0000;">I’m sorry, but I couldn’t find</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.12">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.12.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.12.1.1" style="color:#FF0000;">any information about a third type of fox in the</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.13">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.13.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.13.1.1" style="color:#FF0000;">given context.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.14">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.14.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.14.1.1" style="color:#000000;">GPT-4: </span><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.14.1.2" style="color:#FF0000;">The context does not provide information on</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.15">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.15.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.15.1.1" style="color:#FF0000;">a third type of fox.</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.16">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.16.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.16.1.1" style="color:#000000;">Ground Truth: It is a relatively small fox with an</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.17">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.17.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.17.1.1" style="color:#000000;">elongated muzzle, long, pointed ears, and a bushy tail</span></span></span>
<span class="ltx_tr" id="A5.T10.1.1.1.1.4.3.3.1.18">
<span class="ltx_td ltx_align_left" id="A5.T10.1.1.1.1.4.3.3.1.18.1" style="padding-top:0.15pt;padding-bottom:0.15pt;"><span class="ltx_text" id="A5.T10.1.1.1.1.4.3.3.1.18.1.1" style="color:#000000;">about 50 to 60% of the length of the head and body.</span></span></span>
</span></span></span>
</span>
</span><span class="ltx_text" id="A5.T10.1.1.1.2" style="color:#000000;"></span></span></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Case study for ChatQA-13B, ChatQA-70B, GPT-3.5-turbo, and GPT-4. We use <span class="ltx_text" id="A5.T10.5.1" style="color:#0000FF;">blue</span> color to highlight the relevant context for the last user question. And we use <span class="ltx_text" id="A5.T10.6.2" style="color:#41B729;">green</span> and <span class="ltx_text" id="A5.T10.7.3" style="color:#FF0000;">red</span> color to highlight the correct and incorrect parts in the assistant responses.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Case Study</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">More examples on ChatQA-13B, ChatQA-70B, GPT-3.5-turbo, and GPT-4 can be found in Table&nbsp;<a class="ltx_ref" href="#A5.T10" title="Table 10 ‣ Appendix E Unanswerable Case Evaluation ‣ ChatQA: Building GPT-4 Level Conversational QA Models"><span class="ltx_text ltx_ref_tag">10</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Guidelines for Conversational QA Data Collection</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">In this section, we demonstrate the guidelines we provide for the conversational QA data collection. We request the annotators to act as both user and agent to construct the conversational QA sample based on the given document, which makes the annotation more efficient.
The guidelines consists of three parts: 1) What does conversational QA samples look like; 2) What kinds of conversational QA samples we need; and 3) What we need to annotate.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="A7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.1 </span>What does conversational QA samples look like</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS1.p1">
<p class="ltx_p" id="A7.SS1.p1.1">One conversational QA sample is based on a document that we provide. The user and agent behaviors are as follows:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para" id="A7.SS1.p2">
<ul class="ltx_itemize" id="A7.I1">
<li class="ltx_item" id="A7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i1.p1.1">User behavior: 1) Ask questions to agent based on the given document; 2) Answer questions from agent when agent wants to clarify something.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i2.p1.1">Agent behavior: 1) Answer questions from user based on the document; 2) Ask questions to the user when the user’s question is not clear, or too general/broad.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.2 </span>What kinds of multi-turn QA samples we need</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS2.p1">
<p class="ltx_p" id="A7.SS2.p1.1">We list the requirements for user’s questions and agent’s responses below.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="A7.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">User’s Questions</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS2.SSS0.Px1.p1">
<ul class="ltx_itemize" id="A7.I2">
<li class="ltx_item" id="A7.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i1.p1">
<p class="ltx_p" id="A7.I2.i1.p1.1">User’s questions can refer to his/her previous (or multiple turns before) question.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i2.p1">
<p class="ltx_p" id="A7.I2.i2.p1.1">User’s questions can also refer to the previous (or multiple turns before) answer from the agent.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i3.p1">
<p class="ltx_p" id="A7.I2.i3.p1.1">Try to use pronouns or common nouns to replace the entities mentioned before.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i4.p1">
<p class="ltx_p" id="A7.I2.i4.p1.1">Try to make the user’s questions diverse. For the same type of questions, try to use different ways to express it in different dialogue annotations.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I2.i5.p1">
<p class="ltx_p" id="A7.I2.i5.p1.1">Other user question types we need</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A7.I2.i5.I1">
<li class="ltx_item" id="A7.I2.i5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A7.I2.i5.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="A7.I2.i5.I1.i1.p1">
<p class="ltx_p" id="A7.I2.i5.I1.i1.p1.1">Given the answer from agent, ask for extra information (e.g., What else …; any other …; is there more …).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I2.i5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A7.I2.i5.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="A7.I2.i5.I1.i2.p1">
<p class="ltx_p" id="A7.I2.i5.I1.i2.p1.1">Switch the topic and starting a new thread in conversation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I2.i5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A7.I2.i5.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="A7.I2.i5.I1.i3.p1">
<p class="ltx_p" id="A7.I2.i5.I1.i3.p1.1">Ask two questions at the same time.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A7.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Agent’s Response</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS2.SSS0.Px2.p1">
<ul class="ltx_itemize" id="A7.I3">
<li class="ltx_item" id="A7.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I3.i1.p1">
<p class="ltx_p" id="A7.I3.i1.p1.1">Try to make the agent’s answer within 1-2 sentences. If the answer has to be long, try to make it concise.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I3.i2.p1">
<p class="ltx_p" id="A7.I3.i2.p1.1">Try not to directly copy the whole relevant context from the document as the answer. Instead, try to construct the answer by paraphrasing the chosen context.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I3.i3.p1">
<p class="ltx_p" id="A7.I3.i3.p1.1">Try to come up with a small proportion of cases where agent asks questions to user to clarify something. Specifically, when the user’s question is too broad or not clear, the agent needs to narrow down the scope by asking clarification questions to see which specific aspects that the user cares more about.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="A7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.3 </span>What we need to annotate</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="A7.SS3.p1">
<p class="ltx_p" id="A7.SS3.p1.1">We list what we need to annotate for each conversation below.
</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A7.I4">
<li class="ltx_item" id="A7.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I4.i1.p1">
<p class="ltx_p" id="A7.I4.i1.p1.1">For each document, we need to annotate the user’s questions and corresponding agent’s responses. The average number of user-agent turns per conversation needs to be around five.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A7.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I4.i2.p1">
<p class="ltx_p" id="A7.I4.i2.p1.1">For each user’s question, we need to annotate all the relevant context within the document.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>

</div>


<div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated on Wed Dec 14 18:01:44 2022 by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer><button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed;">Report Issue for Selection</button></body></html>
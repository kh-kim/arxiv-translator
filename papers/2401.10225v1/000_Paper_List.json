{
    "2401.10225v1": {
        "paper_id": "2401.10225v1",
        "abs_url": "https://arxiv.org/abs/2401.10225v1",
        "pdf_url": "https://arxiv.org/pdf/2401.10225v1.pdf",
        "supp_url": null,
        "src_website": "ArXiv",
        "download_name": "2401.10225v1_ChatQA_Building_GPT-4_Level_Conversational_QA_Models.pdf",
        "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
        "year": null,
        "paper_venue": null,
        "authors": [
            "Zihan Liu",
            "Wei Ping",
            "Rajarshi Roy",
            "Peng Xu",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "abstract": "In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.",
        "comments": "",
        "official_code_urls": [],
        "pwc_page_url": "https://paperswithcode.com/paper/chatqa-building-gpt-4-level-conversational-qa",
        "bibtex": "@misc{liu2024chatqa,\n      title={ChatQA: Building GPT-4 Level Conversational QA Models}, \n      author={Zihan Liu and Wei Ping and Rajarshi Roy and Peng Xu and Mohammad Shoeybi and Bryan Catanzaro},\n      year={2024},\n      eprint={2401.10225},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
    }
}
# 언어 모델의 과학 가속

Dirk Groeneveld\({}^{\alpha}\) Iz Beltagy\({}^{\alpha}\)

Pete Walsh\({}^{\alpha}\) Akshita Bhagia\({}^{\alpha}\) Rodney Kinney\({}^{\alpha}\) Oyvind Tafjord\({}^{\alpha}\)

Ananya Harsh Jha\({}^{\alpha}\) Hamish Ivison\({}^{\alpha\beta}\) Ian Magnusson\({}^{\alpha}\) Yizhong Wang\({}^{\alpha\beta}\)

Shane Arora\({}^{\alpha}\) David Atkinson\({}^{\alpha}\) Russell Authur\({}^{\alpha}\) Khyathi Raghavi Chandu\({}^{\alpha}\)

Arman Cohan\({}^{\gamma\alpha}\) Jennifer Dumas\({}^{\alpha}\) Yanai Elazar\({}^{\alpha\beta}\) Yuling Gu\({}^{\alpha}\)

Jack Hessel\({}^{\alpha}\) Tushar Khot\({}^{\alpha}\) William Merrill\({}^{\delta}\) Jacob Morrison\({}^{\alpha}\)

Niklas Muennighoff Aakanksha Naik\({}^{\alpha}\) Crystal Nam\({}^{\alpha}\) Matthew E. Peters\({}^{\alpha}\)

Valentina Pyatkin\({}^{\alpha\beta}\) Abhilasha Ravichander\({}^{\alpha}\) Dustin Schwenk\({}^{\alpha}\) Saurabh Shah\({}^{\alpha}\)

Will Smith\({}^{\alpha}\) Emma Strubel\({}^{\alpha\mu}\) Nishant Subramani\({}^{\alpha}\) Mitchell Wortsman\({}^{\beta}\)

Pradeep Dasigi\({}^{\alpha}\) Nathan Lambert\({}^{\alpha}\) Kyle Richardson\({}^{\alpha}\)

Luke Zettlemoyer\({}^{\beta}\) Jesse Dodge\({}^{\alpha}\) Kyle Lo\({}^{\alpha}\) Luca Soldaini\({}^{\alpha}\)

Noah A. Smith\({}^{\alpha\beta}\) Hannaneh Hajishirzi\({}^{\alpha\beta}\)

({}^{\alpha}\)Allen 인공지능연구소

\({}^{\beta}\)University of Washington \({}^{\gamma}\)Yale University

({}^{\delta}\)뉴욕대학교 \({}^{\mu}\)카네기멜론대학교

olmo@allenai.org

###### Abstract

언어 모델(LMs)은 NLP 연구 및 상업적 제품 제공 모두에서 편재하게 되었다. 상업적 중요성이 급증함에 따라 가장 강력한 모델은 훈련 데이터, 아키텍처 및 개발에 대한 중요한 세부 정보가 공개되지 않은 독점 인터페이스 뒤에 갇혀 폐쇄되었다. 편향과 잠재적 위험을 포함하여 이러한 모델을 과학적으로 연구할 때 이러한 세부 정보의 중요성을 감안할 때 연구 커뮤니티가 강력하고 진정으로 개방된 LMs에 접근할 수 있는 것이 필수적이라고 믿는다. 이를 위해 이 기술 보고서는 최신 언어 모델인 OLMo의 첫 번째 릴리스와 언어 모델링 과학을 구축하고 연구하기 위한 최신 언어 모델인 OLMo의 프레임워크에 대해 자세히 설명한다. 모델 가중치 및 추론 코드만 공개했던 대부분의 기존 노력과 달리, 우리는 학습 데이터 및 학습 및 평가 코드를 포함한 OLMo 및 전체 프레임워크를 공개한다. 이 출시가 개방형 연구 커뮤니티에 힘을 실어주고 강화하고 혁신의 새로운 물결을 불러일으키기를 바랍니다.

\begin{tabular}{c c}
**가중치** & [https://huggingface.co/allenai/OLMo-7B](https://huggingface.co/allenai/OLMo-7B) \\
**코드** & [https://github.com/allenai/OLMo](https://github.com/allenai/OLMo) \\
**데이터** & [https://huggingface.co/datasets/allenai/dolma](https://huggingface.co/datasets/allenai/dolma) \\
**평가** & [https://github.com/allenai/OLMo-Eval](https://github.com/allenai/OLMo-Eval) \\
**Adaptation** & [https://github.com/allenai/open-instruct](https://github.com/allenai/open-instruct) \\ \end{tabular}

Introduction

언어 모델은 수년 동안 NLP 기술의 중심에 있었다(Rosenfeld, 2000; Bengio et al., 2003; Mikolov et al., 2013; Peters et al., 2018; Brown et al., 2020). 최근에는 정렬을 위한 대규모 사전 훈련 및 인간 주석으로 인해 상업적으로 가치가 있게 되었다(OpenAI, 2023). 그러나 상업적 가치가 증가함에 따라 가장 큰 모델은 독점 인터페이스 뒤에 문이 열렸고 중요한 세부 사항은 공개되지 않았다.

우리는 연구 커뮤니티를 위한 개방형 언어 모델에 대한 완전한 접근이 이러한 모델의 과학적 연구, 장단점, 편향 및 위험에 중요하다고 믿는다. 따라서 학습 데이터, 학습 및 평가 코드, 중간 모델 검사점 및 학습 로그와 함께 LM을 빌드, 연구 및 발전시키기 위한 최첨단 진정한 개방형 언어 모델 및 프레임워크인 **OLMo** 를 소개합니다.

최근 LM 방출은 개방 정도가 다양했다. 예를 들어, Mistral 8x7B는 모델 가중치와 간략한 보고서를 제공한 반면(Jiang et al., 2024), LLaMA는 심층 적응 훈련 명령어를 제공하였고(Touvron et al., 2023), Mosaic Pretrained Transformer는 데이터 자체는 아니지만 데이터세트 분포를 포함한 많은 세부 사항을 제공하였다(MosaicML NLP Team, 2023). 팔콘의 사전 훈련 데이터는 부분적으로 공개되었으며(Almazrouei et al., 2023), 가장 개방적인 모델인 피티아 제품군(Biderman et al., 2023) 및 BLOOM(BigScience et al., 2022)은 훈련 코드, 모델 체크포인트, 훈련 데이터 등을 공개했다.

OLMo는 허용 라이선스를 사용 하 여 여러 하드웨어 유형에 걸쳐 여러 교육 검사점, 교육 로그 및 사용 된 정확한 데이터 집합에 대 한 데이터부터 교육 평가 도구까지 전체 프레임워크를 릴리스 합니다. 우리는 이것을 하는 유일한 팀이 아니다; LLM360의 최근 작업은 유사한 목표를 목표로 한다(Liu et al., 2023). OLMo는 LLaMA2와 같은 최신 모델의 능력으로 모델 간의 격차를 좁힌다. 이 프로젝트는 다양한 개방도를 가진 이러한 이전의 모든 노력에서 얻은 교훈으로부터 이익을 얻었으며, 우리는 개방형 모델의 크고 다양한 모집단이 언어 모델을 이해하는 과학적 진보와 유용성 향상에 대한 엔지니어링 진보에 대한 최고의 희망이라고 믿는다.

OLMo 프레임워크는 언어 모델을 구축하고 연구하는 데 필요한 도구와 자원을 포괄한다. 훈련 및 모델링의 경우 전체 모델 가중치, 훈련 코드, 훈련 로그, 절제, 가중치 및 편향 로그 형태의 훈련 메트릭 및 추론 코드를 포함한다. 이 첫 번째 릴리스에는 서로 다른 아키텍처, 최적화기 및 훈련 하드웨어에 해당하는 7B 스케일의 언어 모델의 4가지 변형과 1B 스케일의 하나의 모델이 포함되며, 모두 최소 2T 토큰으로 훈련된다. 또한 포옹페이스에서 수정 가능한 수백 개의 중간 검문소를 공개합니다. 데이터셋 구축 및 분석을 위해 AI2의 Dolma(Soldaini et al., 2024), 사전 훈련 데이터를 분석하기 위한 WIMBD(Elazar et al., 2023)로부터 훈련 데이터를 생성하는 코드를 포함하여 이들 모델에 사용되는 전체 훈련 데이터를 포함하고, 평가를 위해 다운스트림 평가를 위한 AI2의 Catwalk(Groeneveld et al., 2023), Perplexity 기반 평가를 위한 Paloma(Magnusson et al., 2023)를 포함한다. 명령어-조정을 위해, 우리는 Open Instruct(Ivison et al., 2023; Wang et al., 2023)를 출시했으며, 현재 이를 사용하여 OLMo의 적응된(명령어-조정된 및 RLHFed) 버전을 생산하고 있으며, 이는 곧 출시될 것이다. 마지막으로 모든 코드 및 가중치는 Apache 2.0 라이선스.1에서 릴리스됩니다.

각주 1: [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)

이것은 더 큰 모델, 지시 조정된 모델, 더 많은 양식 및 변형으로 이어지는 긴 일련의 계획된 릴리스의 첫 번째 단계이다. 따라서 우리는 사전 훈련 데이터와 모델 능력 사이의 관계, 설계 및 하이퍼파라미터 선택의 영향, 다양한 최적화 방법과 모델 훈련에 미치는 영향과 같은 이러한 모델의 아직 잘 이해되지 않은 측면에 대한 연구를 촉진하기를 바란다. 또한, 이 규모에서 언어 모델을 성공적으로 훈련하는 데 필요한 교훈과 중요한 세부 사항에 대해 보고한다.

## 2 OLMo Framework

이 섹션에서는 OLMo 모델(섹션 2.1), 사전 훈련 데이터 세트인 돌마(섹션 2.2) 및 평가 프레임워크(섹션 2.3)로 구성된 OLMo 프레임워크를 설명한다.

### OLMo 모델 및 아키텍처

우리는 Vaswani 등(2017)에 기초한 디코더 전용 트랜스포머 아키텍처를 채택하고, 표 1에 기술된 바와 같이 1B 및 7B 변형을 전달하며, 65B 버전이 곧 온다. 우리의 특정 아키텍처는 PaLM(Chowdhery et al., 2022), LLaMA 패밀리(Touvron et al., 2023a,b), OpenLM(Gururangan et al., 2023) 및 Falcon(Almazrouei et al., 2023)과 같은 다른 최근의 대형 언어 모델에 후속하여 Vaswani et al.(2017)로부터의 바닐라 변압기에 대한 몇 가지 개선을 포함한다. 표 2는 7B 아키텍처를 이러한 다른 패밀리의 유사한 크기의 모델과 종합적으로 비교한다.

우리는 일반적으로 손실 스파이크와 느린 발산의 위험을 최소화하면서 하드웨어에서 처리량을 훈련하기 위해 최적화하여 하이퍼파라미터를 선택한다. 우리는 사용 가능한 계산 소스가 주어진 루프 내 평가 설정을 통해 선택을 줄인다(섹션 2.3). 표 2는 우리의 디자인 선택을 최근의 최신 개방형 언어 모델과 비교한다. 바닐라 변압기 구조에 대한 주요 변경 사항은 다음과 같이 요약할 수 있다.

1. **편향 없음** LLaMA, PaLM 및 기타에 따라 훈련 안정성을 개선하기 위해 아키텍처에서 모든 편향 용어를 제외합니다.
2. **비모수 계층 norm.** 계층 norm의 비모수 공식 (Ba et al., 2016)을 사용 합니다. 여기서 norm 내에 아핀 변환, 즉 "적응적 이득"(또는 편향)이 없습니다. 우리는 이것이 가장 안전한 옵션이며 우리가 고려했던 다른 변형들, 즉 파라메트릭 계층 규범과 RMSNorm(Zhang and Sennrich, 2019)에 비해 가장 빠르다고 믿는다.
3. **SwiGLU 활성화 함수.** LLaMA, PaLM 및 기타와 마찬가지로 ReLU 대신 SwiGLU 활성화 함수(Shazeer, 2020)를 사용합니다. LLaMA에 따라 활성화 숨겨진 크기는 대략 \(\frac{8}{3}d\)이지만 처리량을 개선하기 위해 128(예: 7B 모델의 경우 11,008)의 가장 가까운 배수로 증가합니다. 2 각주 2: SwiGLU는 "게이트" 활성화 함수이므로 출력은 입력의 절반 크기입니다. 따라서 기술적으로 SwiGLU에 대한 우리의 입력은 우리의 7B 모델에 대해 2 \(\times\) 11,008 = 22,016의 차원을 갖는다.
4. **RoPE(회전 위치 임베딩)** LLaMA, PaLM 및 기타와 마찬가지로 절대 위치 임베딩을 회전 위치 임베딩으로 대체합니다 (RoPE; Su 등, 2021).
5. **어휘.** 개인 식별 정보(PII)를 마스킹하기 위한 추가 토큰과 함께 GPT-NeoX-20B(Black 등, 2022)의 수정된 버전의 BPE 기반 토큰화기를 사용합니다. 최종 어휘 크기는 50,280이다. 그러나 학습 처리량을 최대화하기 위해 모델 내 해당 임베딩 행렬의 크기를 50,304로 증가시켜 128의 배수가 되도록 한다.

### Pretraining Data: Dolma

모델 매개변수에 대한 액세스의 진행에도 불구하고 사전 훈련 데이터 세트는 여전히 열려 있지 않다. 사전 학습 데이터는 종종 열린 모델(닫힌 모델뿐만 아니라)과 함께 공개되지 않으며 이러한 데이터에 대한 문서화는 작업을 재현하거나 완전히 이해하는 데 필요한 세부 사항이 부족한 경우가 많다. 이것은 트레이닝 데이터가 모델 능력 및 한계에 어떻게 영향을 미치는지 이해하는 것과 같은 언어 모델 연구의 특정 스레드를 지원하는 것을 어렵게 만들었다. 언어 모델 사전 학습에 대한 공개 연구를 용이하게 하기 위해 7가지 다른 데이터에서 얻은 5B 문서에 걸쳐 3T 토큰의 다양한 다중 소스 코퍼스인 사전 학습 데이터 세트 돌마를 구축하고 공개했다.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline
**Size** & **Layers** & **Hidden Size** & **Attention Heads** & **Tokens Trained** \\ \hline
1B & 16 & 2048 & 16 & 2T \\
7B & 32 & 4086 & 32 & 2.46T \\
65B* & 80 & 8192 & 64 & \\ \hline \hline \end{tabular}
\end{table}
표 1: OLMo 모델 크기 및 훈련된 토큰의 최대 개수.

* _65B 모델을 작성할 때 아직 학습 중입니다._ (1) 대규모 언어 모델 사전 트레이닝에서 흔히 볼 수 있고 (2) 일반 대중이 액세스할 수 있는 소스들(Soldaini 등, 2024). 표 3은 각 소스의 데이터 양에 대한 상위 수준의 개요를 제공한다.

돌마는 (1) 언어 필터링, (2) 품질 필터링, (3) 콘텐츠 필터링, (4) 중복 제거, (5) 다중 소스 혼합 및 (6) 토큰화의 파이프라인을 사용하여 구축된다. 독자는 돌마 보고서(Soldaini et al., 2024)를 참조하여 그것의 설계 원리, 그것의 구성에 대한 세부 사항 및 그것의 내용에 대한 보다 상세한 요약을 참조한다. 이 보고서는 돌마의 중간 상태에 대한 훈련 언어 모델의 추가 분석 및 실험 결과를 제공하여 콘텐츠 또는 품질 필터의 역할, 중복 제거 및 여러 소스의 데이터 혼합을 포함하여 중요한 데이터 큐레이션 관행에 대해 배운 내용을 공유한다. 큐레이션과 최종 릴리스 모두에서 각 소스의 문서를 별도로 보관합니다. 우리는 고성능 데이터 큐레이션 도구를 오픈소싱했습니다. 이 툴킷은 돌마에 대한 추가 실험, 작업 재현, 사전 훈련 말뭉치를 빠르고 쉽게 큐레이션하는 데 사용할 수 있습니다. 마지막으로, 데이터셋 분석을 돕기 위해 WIMBD 도구(Elazar et al., 2023)를 오픈소싱했다.

\begin{table}
\begin{tabular}{l|l l l l l} \hline  & **OLMo-7B** & **LLLaMA2-7B** & **OpenLM-7B** & **Falcon-7B** & **PaLM-8B** \\ \hline Dimension & 4096 & 4096 & 4096 & 4544 & 4096 \\ Num heads & 32 & 32 & 32 & 71 & 16 \\ Num layers & 32 & 32 & 32 & 32 & 32 \\ MLP ratio & \(\sim\)8/3 & \(\sim\)8/3 & \(\sim\)8/3 & 4 & 4 \\ Layer norm type & non-parametric & RMSNorm & parametric & parametric & parametric \\ Positional embeddings & RoPE & RoPE & RoPE & RoPE & RoPE \\ Attention variant & full & GQA & full & MQA & MQA \\ Biases & none & none & in LN only & in LN only & none \\ Block type & sequential & sequential & sequential & parallel & parallel \\ Activation & SwiGLU & SwiGLU & SwiGLU & GeLU & SwiGLU \\ Sequence length & 2048 & 4096 & 2048 & 2048 & 2048 \\ Batch size (instances) & 2160 & 1024 & 2048 & 2304 & 512 \\ Batch size (tokens) & \(\sim\)4M & \(\sim\)4M & \(\sim\)4M & \(\sim\)4M & \(\sim\)1M \\ Weight tying & no & no & no & no & yes \\ \hline \end{tabular}
\end{table}
표 2: 7-8B 규모에서 LM 아키텍처 비교. "계층 규범 유형" 행에서, "모수" 및 "비모수"는 각각 적응 이득 및 바이어스가 있거나 없는 통상적인 계층 규범 구현을 지칭한다.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{**Source**} & \multirow{2}{*}{**Doc Type**} & **UTF-8** & **Documents** & **GPT-NeoX** \\  & & **bytes** & _(millions)_ & _tokens_ \\  & & _(GB)_ & _(billions)_ & _(billions)_ \\ \hline Common Crawl & web pages & 9,022 & 3,370 & 2,006 \\ The Stack & code & 1,043 & 210 & 342 \\ C4 & web pages & 790 & 364 & 174 \\ Reddit & social media & 339 & 377 & 80 \\ peS2o & STEM papers & 268 & 38.8 & 57 \\ Project Gutenberg & books & 20.4 & 0.056 & 5.2 \\ Wikipedia, Wikibooks & encyclopedic & 16.2 & 6.2 & 3.7 \\ \hline
**Total** & & **11,519** & **4,367** & **2,668** \\ \hline \hline \end{tabular}
\end{table}
표 3: 돌마의 구성.

### Evaluation

모델 설계를 위한 결정을 내리는 _online_ 평가와 모델 체크포인트를 평가하는 _offline_ 평가의 두 단계로 모델 평가를 수행한다. 오프라인 평가를 위해, 우리는 광범위한 데이터 세트 및 작업 포맷에 액세스할 수 있는 공개적으로 이용 가능한 평가 도구인 Catwalk 프레임워크(Groeneveld 등, 2023)를 사용한다. Catwalk를 이용하여, 우리는 우리의 새로운 perplexity 벤치마크인 Paloma(Magnusson et al., 2023)에 대한 내재적 언어 모델링 평가뿐만 아니라 다운스트림 평가를 수행한다.

하류 및 복잡성 평가를 위해 고정 평가 파이프라인을 사용하여 공개적으로 사용 가능한 여러 모델과 결과를 비교한다.

모델 학습 전반에 걸쳐 다운스트림 평가를 수행 하 여 모델 아키텍처, 초기화, 최적화기, 학습 속도 일정 및 데이터 혼합물을 중심으로 결정 합니다. 1000개의 학습 단계(또는 \(\sim\)4B 학습 토큰)마다 인루프를 실행하고 학습 중인 모델의 품질에 대한 초기 및 지속적인 신호를 제공하기 때문에 이를 _온라인_ 평가라고 합니다. 이러한 평가는 섹션 4.1에 자세히 설명된 _offline_ 평가에 사용되는 많은 핵심 작업 및 실험 설정에 의존하며, EleutherAI 평가 하니스의 작업 및 평가 구조(Gao 등, 2023)도 반영합니다.

다운스트림 평가 많은 이전 작업(Brown et al., 2020; Black et al., 2022; Touvron et al., 2023,b, _inter alia_)에 이어, 우리는 다운스트림 작업 세트에 대한 제로 샷 성능을 보고한다. 우리의 평가 세트는 Touvron 등(2023) 및 Touvron 등(2023)에 의해 보고된 상식 추론 태스크 세트에 밀접하게 대응하는 9개의 핵심 태스크들로 구성된다(태스크들의 목록은 표 6 참조). 평가되는 모델의 스케일을 감안할 때, 그러한 태스크는 자연스러움(예를 들어, 모두 텍스트 완성 채점 태스크로서 공식화될 수 있음) 및 트레이닝 전반에 걸쳐 의미 있는 신호를 제공하는 능력으로 인해 모델 개발 초기에 선택되었다(도 1 참조).

내재적 언어 모델링 평가 OLMo-7B가 홀드 아웃 트레이닝 데이터를 넘어 언어의 분포들을 어떻게 적합시키는지를 측정하기 위해, 우리는 585개의 상이한 텍스트 도메인을 포함하는 새로운 복잡성 벤치마크인 팔로마(Magnusson et al., 2023)를 사용한다. 도메인은 nytimes.com에서 Reddit에 대한 r/우울증에 이르기까지 다양하며 계층화된 샘플에서 C4(Raffel 등, 2020)와 같은 18개의 개별 데이터 소스에서 추출된다. 이를 통해 소스 말뭉치에 과소 표현된 텍스트 도메인을 보다 동등하게 포함할 수 있다.

우리는 최상의 성능을 위해 OLMo-7B를 다른 모델과 비교하는 것뿐만 아니라 그것이 더 풍부하고 통제된 과학적 평가를 가능하게 하는 방법을 입증하는 것을 목표로 한다. OLMo-7B는 복잡성 평가를 위한 명시적 오염 제거가 있는 가장 큰 LM이다. 팔로마에서 설명한 접근법에 따라 팔로마 평가 데이터에서 단락 누출이 있는 사전 훈련 문서를 제거한다. 오염 제거 없이 다른 모델은 복잡성을 과소평가할 위험이 있다(즉, 모델의 표본 외 적합도를 과대평가함). 우리는 또한 중간 체크포인트를 방출하여, 체크포인트를 방출하는 두 개의 다른 모델인 Pythia-6.9B(Biderman et al., 2023) 및 RPJ-INCITE-7B(Together Computer, 2023)와 보다 풍부한 비교를 허용한다(도 2 참조).

## 3 Training OLMo

이 섹션에서는 분산 훈련 프레임워크(섹션 3.1), 최적화기 설정(섹션 3.2), 데이터 준비(섹션 3.3) 및 하드웨어(섹션 3.4)를 포함하여 사전 훈련 설정을 설명한다.

### 분산 학습 프레임워크

우리는 PyTorch의 FSDP 프레임워크(Zhao et al., 2023)를 통해 _ZeRO_ 옵티마이저 전략(Rajbhandari et al., 2019)을 사용하여 모델을 훈련시키며, 이는 GPU에 걸쳐 모델 가중치 및 대응하는 옵티마이저 상태를 공유함으로써 메모리 소비를 감소시킨다. 7B 규모에서는 GPU당 4096 토큰의 마이크로 배치 크기로 하드웨어에서 학습할 수 있습니다 (섹션 3.4 참조). OLMo-1B 및 -7B 모델의 경우 약 4M 토큰의 일정한 전역 배치 크기(각각 2048 토큰의 시퀀스 길이가 있는 2048 인스턴스)를 사용합니다. OLMo-65B 모델 (현재 훈련)의 경우 약 2M 토큰 (1024 인스턴스)에서 시작 된 배치 크기 워밍업을 사용 하 고 약 16M 토큰 (8192 인스턴스)에 도달할 때까지 100B 토큰 마다 두 배로 증가 합니다.

처리량을 향상시키기 위해 FSDP의 내장 설정과 PyTorch의 앰프 모듈을 통해 혼합 정밀 훈련(Micikevicius et al., 2017)을 사용했다. 후자는 소프트맥스와 같은 특정 작업이 안정성을 향상시키기 위해 항상 완전한 정밀도로 실행되는 반면 다른 모든 작업은 bfloat16 형식으로 반 정밀도로 실행되도록 한다. 특정 설정에서 각 GPU에 로컬로 샤드된 모델 가중치 및 최적화기 상태는 완전한 정밀도로 유지됩니다. 각 변압기 블록 내의 가중치는 순방향 및 역방향 통과 동안 전체 크기의 파라미터가 각 GPU 상에서 구체화될 때 bfloat16에만 캐스팅된다. 그라디언트는 GPU 전체에서 완전 정밀도로 감소됩니다.

### Optimizer

본 논문에서는 AdamW 최적화기(Loshchilov and Hutter, 2019)를 이용하여 표 4와 같은 하이퍼파라미터들을 사용한다. 모든 모델 크기에 대해 학습률을 5000 단계(\(\sim\)21B 토큰) 이상 워밍업한 후, 나머지 학습률에서 최대 학습률의 10분의 1까지 선형적으로 감쇠한다.

예열 기간 후, 파라미터 gradients3의 총 \(l^{2}\)-norm이 \(1.0\)을 초과하지 않도록 gradient를 clip한다. 표 5는 7B 규모에서 우리의 최적화기 설정을 AdamW를 사용한 다른 최근 LM의 설정과 비교했다.

각주 3: 기울기 클리핑 동안 모델의 모든 매개변수는 하나의 큰 벡터로 처리되며(모든 매개변수가 평평하고 함께 연결된 것처럼), 해당 단일 기울기 벡터보다 \(\ell_{2}\)-norm을 취합니다. 이것은 PyTorch에서 구배를 자르는 표준 방법입니다.

### Data

2.2절에서 설명하는 돌마(Soldaini et al., 2024)의 오픈 데이터셋에서 2T-토큰 샘플로 학습 데이터셋을 구축했다. 모든 문서의 토큰은 각 문서의 끝에 특별한 EOS 토큰을 추가한 후 함께 연결되며, 2048개의 토큰으로 구성된 연속된 청크를 그룹화하여 학습 인스턴스를 구성한다. 각 교육 실행에 대해 동일한 방식으로 교육 인스턴스를 섞습니다. 각 훈련 배치의 데이터 순서와 정확한 구성은 우리가 방출하는 아티팩트로부터 재구성될 수 있다.

발표된 모든 모델은 최소 2T 토큰(학습 데이터 위의 단일 에포크)으로 훈련되었으며 일부는 다른 셔플링 순서로 데이터 위의 두 번째 에포크를 시작하여 그 이상으로 훈련되었다. 이러한 적은 양의 데이터를 반복하는 것의 영향은 이전 작업에 따라 무시할 수 있어야 한다(Muennighoff et al., 2023).

### Hardware

성능의 손실 없이 코드베이스가 NVIDIA 및 AMD GPU 모두에서 사용될 수 있는지 확인하기 위해 두 개의 다른 클러스터에서 모델을 훈련했다.

* **LUMI:** LUMI 슈퍼컴퓨터가 제공한 이 클러스터에서 최대 256개의 노드를 사용했으며, 각 노드는 128GB의 메모리 5와 800Gbps의 상호 연결이 있는 4x AMD MI250X GPU로 구성됩니다.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline
**Size** & **Peak LR** & **Betas** & **Epsilon** & **Weight Decay** & **Batch Size (tokens)** \\ \hline
1B & 4.0E-4 & (0.9, 0.95) & 1.0E-5 & 0.1 & \(\sim\)4M \\
7B & 3.0E-4 & (0.9, 0.95) & 1.0E-5 & 0.1 & \(\sim\)4M \\
65B* & 1.5E-4 & (0.9, 0.95) & 1.0E-5 & 0.1 & \(\sim\)2M \(\rightarrow\)\(\sim\)4M \(\rightarrow\)\(\sim\)8M \(\rightarrow\)\(\sim\)16M \\ \hline \hline \end{tabular}
\end{table}
표 4: OLMo 모델에 대한 AdamW 사전 트레이닝 하이퍼파라미터.

* _65B 모델을 작성할 때 아직 학습 중입니다._* **MosaicML:** MosaicML6(Databricks)에서 제공한 이 클러스터에서 27개의 노드를 사용했으며, 각 노드는 40GB의 메모리와 800Gbps의 상호 연결이 있는 8x NVIDIA A100 GPU로 구성됩니다.

각주 6: [https://www.mosaicml.com](https://www.mosaicml.com)

훈련 처리량을 최적화하기 위한 배치 크기의 사소한 차이에도 불구하고 두 실행 모두 2T 토큰에 의한 평가 제품군에서 거의 동일한 성능을 보였다.

## 4 Results

OLMo-7B 평가에 사용되는 체크포인트는 섹션 3.2에서 언급된 선형 학습 레이트 감쇠 스케쥴을 갖는 Dolma(Soldaini et al., 2024) 데이터세트 상에서 2.46T 토큰까지 트레이닝된다. 본 실험에서, 학습 레이트가 0으로 선형적으로 감쇠된 돌ma 데이터세트에 대해 이 체크포인트를 튜닝하는 것이 섹션 2.3에서 설명된 퍼플렉시티 및 엔드-태스크 평가 스위트들에 대한 모델 성능을 부스트한다. OLMo를 LLaMA-7B(Touvron et al., 2023a), LLaMA2-7B(Touvron et al., 2023b), MPT-7B(MosaicML NLP Team, 2023), Pythia-6.9B(Biderman et al., 2023), Falcon-7B(Almazrouei et al., 2023) 및 RPJ-INCITE-7B(Together Computer, 2023)를 포함하는 다른 공개적으로 이용가능한 모델들과 비교한다.

### Downstream evaluation

Setup우리의 핵심 **다운스트림 평가 제품군** (표 6 참조)은 다음과 같습니다. * arc_easy 및 arc_challenge* (Clark et al., 2018), boolq (Clark et al., 2019), openbookqa (Mihaylov et al., 2018), sciq (Welbl et al., 2017), hellaswag (Zellers et al., 2019), piqa (Bisk et al., 2020), copa (Roemmele et al., 2011) 및 winogrande (Sakaguchi et al., 2021). 부록 A에서 우리는 또한 우리의 핵심 평가 집합 이외의 추가 보조 작업 집합에 대한 결과를 보고하는데, 이는 우리가 덜 안정적인 성능 경향을 갖는다는 것을 발견했다(그림 4 참조). 우리는 다운스트림 평가 제품군이 아직 개발 중이며 추가 결과 및 분석이 향후 버전에서 보고될 것이라는 점에 주목한다.

모든 경우에, Brown 등(2020)에 의해 대중화된 순위 분류 접근법을 사용하여 제로 샷 평가를 수행한다. 이러한 접근법 하에서, 후보 텍스트 완성들(예를 들어, 상이한 객관식 옵션들)은 우도(보통 일부 정규화 인자에 의해 정규화됨)에 의해 순위화되고, 예측 정확도가 보고된다. Catwalk은 토큰 수(토큰당 정규화)로 정규화하는 것(Brown et al., 2020; Liang et al., 2022), 글자 수(문자당 정규화)로 정규화하는 것(Gao et al., 2023), 답변의 무조건적 가능성(Brown et al., 2020)을 포함하는 몇 가지 공통 가능성 정규화 전략을 구현하는 반면, 각 데이터 세트에 대한 정규화 전략을 별도로 선택했다. 구체적으로, arc와 openbookqa에 대한 unconditional normalization, hellaswag, piqa, winogrande에 대한 per-token normalization, boolq, copa, sciq에 대한 no normalization(즉, 단일 토큰 예측 태스크로 공식화된 태스크)를 사용하였다.

\begin{table}
\begin{tabular}{l|l l l l} \hline \hline  & **OLMo-7B** & **LLaMA2-7B** & **OpenLM-7B** & **Falcon-7B** \\ \hline warmup steps & 5000 & 2000 & 2000 & 1000 \\ peak LR & 3.0E-04 & 3.0E-04 & 3.0E-04 & 6.0E-04 \\ minimum LR & 3.0E-05 & 3.0E-05 & 3.0E-05 & 1.2E-05 \\ weight decay & 0.1 & 0.1 & 0.1 & 0.1 \\ beta1 & 0.9 & 0.9 & 0.9 & 0.99 \\ beta2 & 0.95 & 0.95 & 0.95 & 0.999 \\ epsilon & 1.0E-05 & 1.0E-05 & 1.0E-05 & 1.0E-05 \\ LR schedule & linear & cosine & cosine & cosine \\ gradient clipping & global 1.0 & global 1.0 & global 1.0 \\ gradient reduce dtype & FP32 & FP32 & FP32 & BF16 \\ optimizer state dtype & FP32 & most likely FP32 & FP32 & FP32 \\ \hline \hline \end{tabular}
\end{table}
표 5: 7B 스케일에서의 프리트레이닝 최적화기 설정의 비교. 이 표의 각 모델은 AdamW를 최적화기로 사용했다.

결과 표 6은 OLMo-7B의 제로 샷 평가 결과를 요약하고 유사한 크기의 다른 공개적으로 이용 가능한 6가지 모델과 비교한다. 섹션 2.3에 설명된 평가 집합의 9개 핵심 작업에 대한 결과를 보고합니다. OLMo-7B 검사점은 2개의 엔드 태스크에서 다른 모든 공개적으로 사용 가능한 모델보다 성능이 우수하고 평가 집합의 8/9 엔드 태스크에서 상위 3위에 있습니다. 종합적으로 OLMo-7B는 비교 표에서 공개적으로 사용 가능한 6개 모델 체크포인트 모두에 대해 경쟁력이 있다.

그림 1에서 우리는 9개의 핵심 엔드 태스크의 정확도 점수 진행을 표시한다. OBQA를 제외한 모든 태스크는 OLMo-7B가 더 많은 토큰에 대해 훈련됨에 따라 정확도 수의 상승 추세를 보인다. 마지막 단계와 두 번째 단계부터 마지막 단계 사이의 많은 작업의 정확도에서 날카로운 상향 눈금은 최종 1000개의 훈련 단계에 걸쳐 LR을 0으로 선형적으로 감소시키는 이점을 보여준다. 추가 평가 결과 및 논의는 부록 A의 표 8을 참조한다.

### 내부 언어 모델링 평가

내재적 평가를 위해 팔로마는 각 도메인의 성능 검사부터 도메인 조합에 대한 보다 요약된 결과까지 다양한 분석을 제안한다. 우리

\begin{table}
\begin{tabular}{l|c c c c c c c c c|c} \hline \hline
**7B Models** & \begin{tabular}{c} arc \\ challenge \\ \end{tabular} & \begin{tabular}{c} arc \\ easy \\ \end{tabular} & boolq & copa & \begin{tabular}{c} hella- \\ swag \\ \end{tabular} & \begin{tabular}{c} open \\ bookqa \\ \end{tabular} & piqa & sciq &
\begin{tabular}{c} wino- \\ grande \\ \end{tabular} & avg. \\ \hline
**Falcon** & 47.5 & 70.4 & 74.6 & 86.0 & 75.9 & 53.0 & 78.5 & 93.9 & 68.9 & 72.1 \\
**LLaMA** & 44.5 & 57.0 & 73.1 & 85.0 & 74.5 & 49.8 & 76.3 & 89.5 & 68.2 & 68.7 \\
**LLaMA2** & 39.8 & 57.7 & 73.5 & 87.0 & 74.5 & 48.4 & 76.4 & 90.8 & 67.3 & 68.4 \\
**MPT** & 46.5 & 70.5 & 74.2 & 85.0 & 77.6 & 48.6 & 77.3 & 93.7 & 69.9 & 71.5 \\
**피티아** & 44.2 & 61.9 & 61.1 & 84.0 & 63.8 & 45.0 & 75.1 & 91.1 & 62.0 & 65.4 \\
**RPJ-INCITE** & 42.8 & 68.4 & 68.6 & 88.0 & 70.3 & 49.4 & 76.0 & 92.9 & 64.7 & 69.0 \\ \hline
**OLMo-7B** & 48.5 & 65.4 & 73.4 & 90.0 & 76.4 & 50.4 & 78.4 & 93.8 & 67.9 & 71.6 \\ \hline \hline \end{tabular}
\end{table}
표 6: 섹션 2.3에 설명된 다운스트림 평가 스위트로부터의 9개의 코어 태스크에 대한 OLMo-7B 및 6개의 다른 공개적으로 이용 가능한 비교 가능한 모델 체크포인트의 제로샷 평가. OLMo-7B의 경우, 2.46T 토큰 체크포인트에 대한 결과를 보고한다.

도 1: 섹션 2.3에 설명된 Catwalk 평가 스위트로부터의 9개의 코어 엔드-태스크 스코어에 대한 OLMo-7B의 정확도 스코어 진행. 7/9 엔드-태스크에 대한 트레이닝의 최종 1000 단계에서 LR을 0으로 감소시키는 이점을 볼 수 있다.

매그누손 등(2023)에서와 같이 팔로마에서 18개 소스 중 11개에 대한 집계 성능과 이러한 소스 각각에 대해 개별적으로 더 세밀한 결과의 두 가지 입도 수준에서 결과를 보고한다. 팔로마의 11개 소스의 이 특정 하위 집합은 공개적으로 사용할 수 없거나 프린지 또는 독성 텍스트를 포함하거나 팔로마의 오염 제거 접근법에서 지원되지 않는 코드 데이터로 구성된 소스를 제외한다. 이는 C4 (Raffel et al., 2020), mC4-en (Chung et al., 2023), Wikitext 103 (Merity et al., 2016), Penn Treebank (Marcus et al., 1999; Nunes, 2020), RedPajama (Together Computer, 2023), Falcon-RefinedWeb (Penedo et al., 2023), Dolma (Soldaini et al., 2024), M2D2 S2ORC (Reid et al., 2022), M2D2 Wikipedia (Reid et al., 2022), C4 100 도메인 (Chronopoulou et al., 2022), 및 Dolma 100 Subreddits (Soldaini et al., 2024). 서로 다른 어휘를 갖는 모델들 간의 공정한 비교를 허용하기 위해, 우리는 이러한 소스들의 테스트 세트들에 대해 Gao 등(2020)에 의해 정의된 바이트당 비트들을 보고한다.

결과 그림 2의 _소스 결합_ 하위 도표에서 팔로마의 11개 데이터 소스 조합에 대해 6개의 비교 크기 언어 모델에 대한 OLMo-7B의 성능을 보여준다. 전반적으로 OLMo의 훈련 데이터가 팔로마에 대해 명시적으로 오염 제거되었기 때문에 경쟁적 적합성을 갖는다는 것을 발견했다. 최종 모델(형상 참조) 및 중간 체크포인트(점선 참조)의 비교를 통해 알 수 있듯이, OLMo 결과는 다른 모델의 유사한 스케일링 경향을 따른다. 중간 체크포인트의 성능은 학습 속도 스케줄에서 그 체크포인트가 어디에서 발생하는지에 의해 영향을 받는다. 따라서 더 적은 단계에 대해 훈련된 모델은 훈련 기간이 모든 모델에 걸쳐 고정된 경우 반드시 더 샘플 효율적이지 않고 더 가파른 훈련 곡선을 갖는 경향이 있다. 그럼에도 불구하고 MPT-7B는 이 서브플롯에서 다른 모델보다 앞서 개선되는 것으로 두드러진다. 이는 사전 트레이닝 데이터 구성 및 팔로마 내의 도메인과의 매칭(예를 들어, LLaMA의 경우 18%, RedPajama의 경우 12.2%, OLMo의 경우 11.2%가 아닌 27% 비공통 크롤 데이터에 대한 MPT 트레이닝)뿐만 아니라 다양한 데이터 전처리 결정(예를 들어, Abbas 등, 2023, on C4에 의한 MPT의 의미 중복제거 사용)을 포함하는 다수의 인자 때문일 수 있다.

그림 2: 팔로마와 그 조합(매그누슨 등, 2023)의 11개 평가 데이터 소스에 대한 바이트당 비트, OLMo의 사전 훈련 데이터에서 오염 제거. 모형은 일반적인 데이터 크기 조정 추세를 따르지만 표본 효율성은 분포 내 데이터에 가장 유리합니다. 예를 들어, OLMo-7B는 아마도 88.8% 커먼 크롤 사전 훈련 데이터를 갖는 것으로부터 C4의 다른 모든 모델을 능가한다.

그림 2의 나머지 서브플롯은 집계된 팔로마 메트릭에서 결합된 11개의 데이터 소스 각각에 대해 바이트당 비트를 개별적으로 보고함으로써 보다 세밀한 분석을 제공한다. 이로부터 우리는 주로 훈련 및 평가 분포의 유사성에 의해 주도되는 샘플 효율의 더 큰 변동을 본다. 특히, OLMo-7B는 C4와 같은 Common Crawl이 우세한 평가에서 잘 계산되지만, Common Crawl의 후처리 방법은 Falcon RefinedWeb의 Falcon-7B와 같은 특정 데이터로 훈련된 모델에 의해 가장 적합하다. 한편, OLMo-7B는 WikiText-103, M2D2 S2ORC, M2D2 Wikipedia와 같이 스크래핑된 웹 텍스트와 관련이 적은 소스에서 다른 모델에 비해 샘플 효율이 낮다. 레드파자마 평가는 아마도 7개 도메인 중 2개만이 공통 크롤에서 유래하고 팔로마 가중치 도메인은 각 소스 내에서 동일하게 제공되기 때문에 유사한 패턴을 보여준다. 위키피디아 및 ArXiv 논문과 같은 큐레이팅된 소스의 이질적인 데이터는 스크래핑된 웹 텍스트보다 훨씬 덜 풍부하기 때문에 사전 훈련 말뭉치가 크기 때문에 이러한 언어 분포에 적합하도록 샘플 효율성을 유지하는 것은 어려울 것이다.

### 전력 소비 및 탄소 발자국

이전 문헌(Strubell et al., 2019; Patterson et al., 2021; Wu et al., 2022; Dodge et al., 2022)에 따라 훈련에 필요한 총 전력 소비량을 계산한 다음 모델이 훈련된 전력망의 탄소 배출 강도를 곱하여 모델을 사전 훈련하면서 총 에너지 소비량과 방출된 탄소를 추정한다. 이러한 운영 배출을 보고하는 것은 표준 관행이지만 하드웨어 및 데이터 센터 인프라의 제조, 운송 및 폐기로 인한 구체화된 배출, 사용으로 인한 평생 운영 배출, 반등 효과 또는 물 소비 또는 채굴과 같은 기타 환경 영향과 같은 기타 배출원을 설명하지 않는다. 따라서 우리의 추정치는 하한으로 간주되어야 한다.

우리는 25ms마다 단일 노드의 전력 소비를 측정하고 전체 훈련 실행에 대한 평균을 계산하고 총 노드 수를 곱하여 모델에 대한 총 전력 소비를 계산한다. 그런 다음 이전 총계에 전력 사용 효율 (PUE) 계수를 곱하여 데이터 센터의 에너지 효율성을 설명 합니다. 1.1로 설정 하 여 에너지 효율적인 데이터 센터의 일반적인 보존적 10% 에너지 소비 오버 헤드를 나타냅니다. 78 7B 모델을 사전 훈련 하면 **239 MWh** 의 에너지가 소비 된 것으로 추정 합니다.

각주 7: [https://www.nrel.gov/computational-science/measuring-efficiency-pue.html](https://www.nrel.gov/computational-science/measuring-efficiency-pue.html)

각주 8: [https://www.google.com/about/datacenters/efficiency/](https://www.google.com/about/datacenters/efficiency/)

탄소 배출량을 계산하기 위해 각 모델이 훈련된 데이터 센터의 물리적 위치를 기반으로 KWh당 배출되는 kg CO\({}_{2}\)으로 측정된 탄소 강도 계수에 총 전력 소비량을 곱한다. A100-40GB GPU에서 훈련된 모델은 호주에서 훈련되었기 때문에 우리는 2022년 호주에 대한 전국 평균인 0.610의 탄소 강도 계수를 가정한다. MI250X GPU에서 훈련된 모델은 100% 재생 가능한 탄소 중립 에너지로 실행되는 LUMI 슈퍼컴퓨터에서 훈련되었기 때문에 우리는 0의 탄소 강도 계수를 가정한다. LUMI는 전적으로 수력에 의해 전력을 공급받고 일부 소스(Ubierna et al., 2022)는 수력의 탄소 강도 계수를 0.024로 측정하므로 총 탄소 배출량은 3.54 tCO\({}_{2}\)eq.10임을 의미한다. 그러나 계산을 위해 공식 LUMI 데이터에 의존하므로 총 사전 훈련 배출량을 **69.78 tCO\({}_{2}\)eq.**11로 추정한다.

각주 9: [https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/december-quarter-2022/Emissions-Reduction.aspx](https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/december-quarter-2022/Emissions-Reduction.aspx)

각주 10: [https://www.lumi-supercomputer.eu](https://www.lumi-supercomputer.eu)

각주 11: 이러한 메트릭은 부분적으로 카르보나라의 AI 에이전트와 모니터링 플랫폼을 사용하여 수집되었다. [https://trycarbonara.com](https://trycarbonara.com)에서 자세히 알아보세요.

우리는 모델을 공개적으로 공개함으로써 다른 사람들이 처음부터 모델을 사전 훈련할 필요성을 피할 수 있도록 하여 미래의 배출량을 줄이고 최첨단 모델 개발의 실제 비용에 대한 통찰력을 제공할 수 있기를 바란다. 또한 디버깅, 하이퍼파라미터 조정 및 다운타임과 같은 다른 중요한 개발 조각을 포함하지 않기 때문에 추정치가 하한이라는 점을 강조한다.

## 5 Artifacts 릴리스

모든 파이프라인 단계의 아티팩트를 공유함으로써 개방형 연구를 장려하고 학계와 실무자가 중복되고 종종 비용이 많이 드는 노력을 줄이는 것을 목표로 한다. 다음 내용을 공개합니다.

1. 학습 및 모델링 코드.12. 각주 12: [https://github.com/allenai/OLMo](https://github.com/allenai/OLMo)
2. 7B 모델13, 7B-twin-2T14 및 1B 모델15에 대해 훈련된 모델 가중치. 모든 모델에 대해 최종 모델 가중치뿐만 아니라 1000단계 간격으로 500+ 중간 체크포인트를 릴리스합니다. 각주 13: [https://huggingface.co/allenai/OLMo-7B](https://huggingface.co/allenai/OLMo-7B)
3. 학습 데이터 Dolma(Soldaini 등, 2024).16 각주 14: [https://huggingface.co/allenai/OLMo-7B-Twin-2T](https://huggingface.co/allenai/OLMo-7B-Twin-2T)
4. Dolma's toolkit to construct new datasets 17, and WIMBD (Elazar et al., 2023) for dataset analysis18. 각주 15: [https://huggingface.co/allenai/OLMo-1B](https://huggingface.co/allenai/OLMo-1B)
5. Downstream 평가를 위한 Catwalk20(Groeneveld et al., 2023) 및 Perplexity 기반 평가를 위한 Paloma21(Magnusson et al., 2023)을 이용한 평가 코드19.

각주 16: [https://huggingface.co/datasets/allenai/dolma](https://huggingface.co/datasets/allenai/dolma)

각주 17: [https://github.com/allenai/dolma](https://github.com/allenai/dolma)

각주 18: [https://github.com/allenai/wimbd](https://github.com/allenai/wimbd)

각주 19: [https://github.com/allenai/OLMo-Eval](https://github.com/allenai/OLMo-Eval)

각주 20: [https://github.com/allenai/catwalk](https://github.com/allenai/catwalk)

각주 21: [https://paloma.allen.ai](https://paloma.allen.ai)

우리는 다음 사항을 포함하는 다른 릴리스와 함께 이 릴리스에 대한 후속 조치를 취할 계획입니다.

1. 훈련 로그, 절제 및 결과.
2. 교육 실행을 위한 가중치 및 편향 로그입니다.
3. Adapted OLMo with instruction-tuning and RLHF, including its training and evaluation code and data using our Open Instruct22 library (Wang et al., 2023; Ivison et al., 2023).

각주 22: [https://github.com/allenai/open-instruct](https://github.com/allenai/open-instruct)

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline  & \multicolumn{2}{c}{GPU Power} & \multicolumn{1}{c}{Power} & \multicolumn{1}{c}{Carbon} & \multicolumn{1}{c}{Carbon} \\  & GPU Type & Consumption & Usage & Intensity & Emissions \\  & & (MWh) & Effectiveness & (kg CO\({}_{2}\)e/KWh) & (tCO\({}_{2}\)eq) \\ \hline
**Gopher-280B** & TPU v3 & 1,066 & 1.08 & 0.330 & 380 \\
**BLOOM-176B** & A100-80GB & 433 & 1.2 & 0.057 & 30 \\
**OPT-175B** & A100-80GB & 324 & 1.1 & 0.231 & 82 \\
**T5-11B** & TPU v3 & 77 & 1.12 & 0.545 & 47 \\
**LLaMA-7B** & A100-80GB & 33 & 1.1 & 0.385 & 14 \\
**LLaMA-7B** & A100-80GB & 74 & 1.1 & 0.385 & 31 \\ \hline
**OLMo-7B** & **7B250X** & 185 & 1.1 & 0.000* & 0* \\
**OLMo-7B** & **A100-40GB** & 104 & 1.1 & 0.610 & 70 \\ \hline \hline \end{tabular}
\end{table}
표 7: 사전 훈련 중 CO\({}_{2}\) 배출량. PUE, 지역 전력망의 탄소 강도 및 보고된 전력 소비에 대한 공개적으로 이용 가능한 데이터를 사용하여 다양한 모델에 대한 총 탄소 배출량을 추정한다. Gopher-280B(Rae et al., 2022), BLOOM-176B(Luccioni et al., 2022), OPT-175B(Zhang et al., 2022), T5-11B(Patterson et al., 2021), LLaMA(Touvron et al., 2023), 및 LLaMA2(Touvron et al., 2023)에 대한 번호는 각각의 논문으로부터 취해진다. tCO2eq를 계산하는 방법에 대한 자세한 내용은 섹션 4.3을 참조한다.

## 6 License

우리의 목표는 과학적 개발을 촉진하고 과학 커뮤니티에 권한을 부여하는 것이므로 사용자에게 자원과 인공물을 사용할 수 있는 유연성을 제공하는 허용 라이선스를 선호한다. 따라서 모든 코드 및 가중치는 Apache 2.0 라이선스.23 최근 모델 릴리즈에 대해 다른 조직에서 사용 하는 일부 라이선스는 인공 지능 또는 기계 학습 시스템을 훈련 하기 위해 모델의 출력을 사용 하는 것을 금지 하는 반면 사용자가 그렇게 할 수 있도록 명시적으로 허용 합니다. 상업적 사용도 제한하지 않습니다. 저희 모델이 다른 모델을 더 좋게 만들 수 있기를 바랍니다. 우리는 챗봇으로 채택되지 않은 언어 모델이 주로 공개 채택이 광범위한 제품이 아닌 과학적 인공물로 사용되었기 때문에 모델의 오남용 위험이 상대적으로 낮음을 인식한다. 또한 지난 1년 동안 매우 허용적인 라이선스로 유사한 모델이 많이 출시되었으므로 작업에 더 엄격한 라이선스를 사용하면 현장의 전반적인 위험이 제거되지 않는다. 우리는 더 개방적인 측면에서 이 절충안이 최선의 선택이라고 믿습니다.

각주 23: [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)

## 7 결론 및 향후 작업

이 기술 보고서는 언어 모델링의 과학을 구축하고 연구하기 위한 최첨단 진정한 개방형 언어 모델인 OLMo의 첫 번째 릴리스와 그 프레임워크를 제시한다. 모델 가중치 및 추론 코드만 공개했던 대부분의 기존 노력과 달리, 우리는 학습 데이터 및 학습 및 평가 코드를 포함한 OLMo 및 전체 프레임워크를 공개한다. 곧 교육 로그, 절제, 결과 및 Weight & Biases 로그도 공개할 예정입니다. 또한 명령어 튜닝과 다양한 맛의 RLHF를 사용하여 OLMo의 적응을 탐색하고 있다. 적응된 모델과 모델 적응 코드 및 데이터를 모두 공개할 예정입니다.

우리는 OLMo와 그 프레임워크를 지속적으로 지원하고 확장하며, 개방형 연구 커뮤니티에 힘을 실어주기 위해 개방형 LM의 경계를 계속 밀고 나가고자 한다. 이를 위해 OLMo 계열에 다양한 모델 크기, 양식, 데이터 세트, 안전 조치 및 평가를 가져오기를 기대한다. 이번 발표와 향후 발표가 개방형 연구 커뮤니티에 힘을 실어주고 강화하고 새로운 혁신의 물결을 불러일으키기를 바랍니다.

## Author Contributions

OLMo는 많은 팀원들과 협력자들의 도움이 없었다면 불가능했을 것입니다. 아래에 작성자 기여(알파벳 순서)를 나열합니다.

**데이터 세트 구성 및 도구화 사전 훈련** (돌마)의 기여자에는 러셀 오투르, 이즈 벨타기, 아크시타 Bhagia, Khyathi Chandu, 제시 도지, 야나이 엘라자르, 더크 그로네벨드, 로드니 키니, 카일 로, 아칸샤 나이크, 압힐라샤 라비칸더, 더스틴 슈웬크, 루카 솔다이니 및 니샨트 서브라마니가 포함됩니다.

**모델 교육 및 아키텍처** 에 대 한 기여자에는 Shane Arora, Iz Beltagy, Akshita Bhagia, Matthew E. Peters, Dirk Groeneveld, Ananya Harsh Jha, William Merrill, Jacob Morrison, Niklas Muennighoff, Dustin Schwenk, Saurabh Shah, Pete Walsh 및 Mitchell Wortsman이 포함 됩니다.

**평가 제품군 및 도구** 에 대한 기여자에는 Akshita Bhagia, Arman Cohan, Pradeep Dasigi, Jesse Dodge, Dirk Groeneveld, Yuling Gu, Tushar Khot, Kyle Richardson, Oyvind Tajford 및 Pete Walsh가 포함됩니다.

**모델 적응** 기여자에는 Iz Beltagy, Pradeep Dasigi, Jack Hessel, Hamish Ivison, Nathan Lambert, Valentina Pyatkin, Pete Walsh 및 Yizhong Wang이 포함됩니다.

**라이선스 생성 및 위험 평가** 에 대 한 기여자에는 David Atkinson, Jesse Dodge, Jennifer Dumas, Crystal Nam 및 Will Smith가 포함 됩니다.

올모 프로젝트는 한나네 하지시르지와 노아 A. 스미스가 주도했다.

## Acknowledgements

OLMo는 많은 개인과 기관의 지원이 없었다면 불가능했을 것이다. 이 작업의 실험 구성 요소는 AMD 및 CSC와의 파트너십을 통해 가능해져 LUMI 슈퍼컴퓨터 및 하버드 대학의 켐프너 연구소의 사용이 가능했다. 우리는 FSDP와 그들의 경험을 공유하고 OLMo가 기반으로 하는 코드 기반을 구축한 조나단 프랭클과 모자이크ML(현재 Databricks)의 팀에 감사드린다. 우리는 팀 동료인 타이라 앤더슨, 미셸 베네딕트, 존 보르차르트, 에비 쳉, 아르나비 체다, 요한 달, 맷 라츠케, 켈시 맥밀란, 애런 서랫, 캐리사 쇼닉, 샘 스콘스버그, 마이클 슈미츠, 마이클 윌슨, 케이틀린 위틀리프 및 IT 팀 전체에 대해 웹사이트, 디자인, 내부 및 외부 커뮤니케이션, 예산 책정 및 이 프로젝트의 원활한 진행을 지원하는 기타 활동에 대한 도움에 감사드린다. 마지막으로, 우리는 또한 프리트비라지(라지) 암마나브로루, 피터 클라크, 니콜 드카리오, 더그 다우니, 알리 파르하디, 이안 페레이라, 바이노 하탄파아, 샴 M을 포함한 AI2의 팀원들과 긴밀한 협력자들의 유익한 토론과 피드백에 감사를 표한다. Kakade, Julien Launay, Sydney Levine, Pekka Manninen, Franzi Roessner, Maarten Sap, Ludwig Schmidt, and Yulia Tsvetkov.

## References

* Abbas 등(2023) Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: semantic deduplication을 통한 웹 스케일에서의 데이터 효율적인 학습. _ arXiv preprint arXiv:2303.09540_, 2023. URL [https://arxiv.org/abs/2303.09540](https://arxiv.org/abs/2303.09540).
* Almazrouei 등(2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra-Aimee Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon 시리즈의 개방형 언어 모델입니다. _ ArXiv_, abs/2311.16867, 2023. URL [https://api.semanticscholar.org/CorpusID:265466629](https://api.semanticscholar.org/CorpusID:265466629).
* Ba 등(2016) Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 레이어 정규화_ ArXiv_, abs/1607.06450, 2016. URL [https://api.semanticscholar.org/CorpusID:8236317](https://api.semanticscholar.org/CorpusID:8236317).
* Bengio 등(2003) Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Janvin. 신경 확률 언어 모델. _ 제이 마케터 배워 Res._ , 3:1137-1155, 2003. URL [https://api.semanticscholar.org/CorpusID:221275765](https://api.semanticscholar.org/CorpusID:221275765).
* Biderman 등(2023) Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. 피티아: 훈련 및 스케일링 전반에 걸쳐 대규모 언어 모델을 분석하기 위한 제품군입니다. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, Volume 202 of _Proceedings of Machine Learning Research_, pages 2397-2430. PMLR, 23-29 Jul 2023. URL [https://proceedings.mlr.press/v202/biderman23a.html](https://proceedings.mlr.press/v202/biderman23a.html)
* BigScience 등(2022) BigScience, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, 등 블룸: A 176b-parameter open-access 다국어 언어 모델. _ arXiv preprint arXiv:2211.05100_, 2022.
* Bisk 등(2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. _인공지능에 대한 AAAI 회의 진행률_에서 볼륨 34, 페이지 7432-7439, 2020. URL [https://ojs.aaai.org/index.php/AAAI/article/view/6239](https://ojs.aaai.org/index.php/AAAI/article/view/6239)입니다.
* Black 등(2022) Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: 오픈소스 자기회귀 언어 모델. _대용량 언어 모델 만들기의 도전 및 전망에 대 한 ACL 워크샵 진행률_, 2022. URL [https://arxiv.org/abs/2204.06745](https://arxiv.org/abs/2204.06745)입니다.
* Black 등(2016)Su Lin Blodgett, Lisa Green, and Brendan O'Connor. 소셜 미디어의 인구학적 변증: 아프리카계 미국인 영어 사례 연구 _Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing_, pages 1119-1130, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1120. URL [https://aclanthology.org/D16-1120](https://aclanthology.org/D16-1120).
* Brown 등(2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. 지글러, 제프 우, 클레멘스 윈터, 크리스토퍼 헤세, 마크 첸, 에릭 시글러, 마테우스 리트윈, 스콧 그레이, 벤자민 체스, 잭 클라크, 크리스토퍼 베르너, 샘 맥캔들리시, 알렉 래드포드, 일야 서츠케버, 다리오 아모데이. 언어 모델은 적은 수의 학습자입니다. _ ArXiv_, abs/2005.14165, 2020. URL [https://api.semanticscholar.org/CorpusID:218971783](https://api.semanticscholar.org/CorpusID:218971783).
* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dee, Henryk Michalewski, Sunipa Dee, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito 다이, 타누말라얀 산카라나리아나 필라이, 마리 펠랏, 아이토르 르코위츠, 에리카 모레이라, 르원 차일드, 올렉산드르 폴로조프, 캐서린 리, 종웨이저우, 수에지 왕, 브레넌 사에타, 마크 디아즈, 오르한 피라트, 미셸 카타스타, 제이슨 웨이, 캐시 마이어-헬스턴, 더글러스 엑, 제프 딘, 슬라브 페트로프, 노아 피델 등이다. Palm: 경로를 사용한 언어 모델링, 2022. URL [https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311)
* Chronopoulou et al.(2022) Alexandra Chronopoulou, Matthew Peters, and Jesse Dodge. 사전 훈련된 언어 모델을 위한 효율적인 계층적 도메인 적응 _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 1336-1351, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.96. URL [https://aclanthology.org/2022.naacl-main.96](https://aclanthology.org/2022.naacl-main.96)
* Chung 등(2023) Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. _ ArXiv_, abs/2304.09151, 2023. URL [https://api.semanticscholar.org/CorpusID:258187051](https://api.semanticscholar.org/CorpusID:258187051).
* Clark 등(2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다. _ arXiv preprint arXiv:1905.10044_, 2019.
* Clark 등(2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 질문에 대한 답을 풀었다고 생각해? try arc, the ai2 reasoning challenge. _ preprint arXiv:1803.05457_, 2018. URL [https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457)
* Dodge 등(2022) Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. 클라우드 인스턴스에서 ai의 탄소 강도 측정, 2022. URL [https://dl.acm.org/doi/10.1145/3531146.3533234](https://dl.acm.org/doi/10.1145/3531146.3533234)
* Dolan and Brockett (2005) William B. Dolan and Chris Brockett. 센센셜 패러프레이즈 코퍼스를 자동으로 구성합니다. _자연어 처리에 대 한 국제 공동 회의_ 2005. URL [https://www.microsoft.com/en-us/research/publication/automatically-constructing-a-corpus-of-sentential-paraphrases/](https://www.microsoft.com/en-us/research/publication/automatically-constructing-a-corpus-of-sentential-paraphrases/)입니다.
*Elazar 등(2023) Yanai Elazar, Akshita Bhagia, Ian H. Magnusson, Abhilsa Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dir Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, and Jesse Dodge. 내 빅데이터에 뭐가 있지? _ ArXiv_, abs/2310.20707, 2023. URL [https://api.semanticscholar.org/CorpusID:264803575](https://api.semanticscholar.org/CorpusID:264803575).
* Elazar et al. (2022)Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. _ arXiv preprint arXiv:2101.00027_, 2020. URL [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)
* Gao 등(2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. few-shot 언어 모델 평가를 위한 프레임워크, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
* Greenbaum and Nelson (1996) Sidney Greenbaum and Gerald Nelson. 국제 영어 코퍼스(ICE) 프로젝트입니다. _ World Englishes_, 15(1):3-15, mar 1996. doi: 10.1111/j.1467-971x.1996.tb00088.x. URL [https://doi.org/10.1111/2Fj.1467-971x.1996.tb00088.x](https://doi.org/10.1111/2Fj.1467-971x.1996.tb00088.x)
* Groeneveld 등(2023) Dirk Groeneveld, Anas Awadalla, Iz Beltagy, Akshita Bhagia, Ian Magnusson, Hao Peng, Oyvind Tafjord, Pete Walsh, Kyle Richardson, and Jesse Dodge. Catwalk: 많은 데이터 세트에 대한 통합 언어 모델 평가 프레임워크입니다. _ arXiv preprint arXiv:2312.10253_, 2023. URL [https://arxiv.org/abs/2312.10253](https://arxiv.org/abs/2312.10253).
* Gururangan 등(2023) Suchin Gururangan, Mitchell Wortsman, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and Ludwig Schmidt. OpenLM: 최소이지만 수행적 언어 모델링 (lm) 리포지토리, 2023. URL [https://github.com/mlfoundations/open_lm/](https://github.com/mlfoundations/open_lm/)입니다. GitHub 리포지토리입니다.
* Ivison 등(2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 변화하는 기후의 Camels: tulu 2, 2023. URL [https://arxiv.org/abs/2311.10702](https://arxiv.org/abs/2311.10702)
* Jiang et al.(2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. _ arXiv preprint arXiv:2401.04088_, 2024. URL [https://arxiv.org/abs/2401.04088](https://arxiv.org/abs/2401.04088).
* Liang et al.(2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. _ arXiv preprint arXiv:2211.09110_, 2022. URL [https://arxiv.org/abs/2211.09110](https://arxiv.org/abs/2211.09110).
* Liu 등(2020) Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: 논리적 추론을 사용하여 기계 읽기 이해를 위한 챌린지 데이터 세트입니다. _ CoRR_, abs/2007.08124, 2020. URL [https://arxiv.org/abs/2007.08124](https://arxiv.org/abs/2007.08124).
* Liu et al.(2023) Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, et al. Llm360: Towards fully transparent open-source llms. _ arXiv preprint arXiv:2312.06550_, 2023. URL [https://arxiv.org/abs/2312.06550](https://arxiv.org/abs/2312.06550).
* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. 비결합 중량 감소 규칙화. _학습 표현에 대 한 국제 회의_ 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)입니다.
* Luccioni 등(2022) Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 176b 매개 변수 언어 모델인 블룸의 탄소 발자국 추정, 2022. URL [https://arxiv.org/abs/2211.02001](https://arxiv.org/abs/2211.02001)
* Magnusson et al.(2023) Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, et al. Paloma: A benchmark for evaluating language model fit. _ arXiv preprint arXiv:2312.10523_, 2023.
* Marcus 등(1999) Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. Treebank-3, 1999. URL [https://catalog.ldc.upenn.edu/LDC99T42](https://catalog.ldc.upenn.edu/LDC99T42).
* Mens 등(2019)Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 포인터 센티넬 혼합물 모델입니다. _ ArXiv_, abs/1609.07843, 2016. URL [https://api.semanticscholar.org/CorpusID:16299141](https://api.semanticscholar.org/CorpusID:16299141).
* Micikevicius 등(2017) Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Frederick Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. 혼합 정밀도 트레이닝. _ ArXiv_, abs/1710.03740, 2017. URL [https://api.semanticscholar.org/CorpusID:3297437](https://api.semanticscholar.org/CorpusID:3297437).
* Mihaylov et al.(2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 갑옷이 전기를 전도할 수 있나요? 오픈 북 질문 답변을 위한 새 데이터 세트입니다. _ arXiv preprint arXiv:1809.02789_, 2018. URL [https://arxiv.org/abs/1809.02789](https://arxiv.org/abs/1809.02789).
* Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. 코라도, 제프리 딘 단어들 및 구절들의 분포된 표현들 및 이들의 구성성. _신경 정보 처리 시스템_, 2013. URL [https://api.semanticscholar.org/CorpusID:16447573](https://api.semanticscholar.org/CorpusID:16447573).
* Tear(2023) MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially useable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.
* Muennighoff 등(2023) Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 데이터 제약 언어 모델 크기 조정 _ arXiv preprint arXiv:2305.16264_, 2023.
* Nunes (2020) Davide Nunes. Preprocessed penn tree bank, 2020. URL [https://zenodo.org/record/3910021](https://zenodo.org/record/3910021).
* OpenAI(2023) OpenAI. Gpt-4 기술 보고서. _ ArXiv_, abs/2303.08774, 2023. URL [https://api.semanticscholar.org/CorpusID:257532815](https://api.semanticscholar.org/CorpusID:257532815).
* Papasavva 등(2020) Antonis Papasavva, Savvas Zannettou, Emiliano De Cristofaro, Gianluca Stringhini, and Jeremy Blackburn. 잃어버린 컨테이너의 라이더들: 정치적으로 잘못된 게시판에서 3.5년 동안 4chan 게시물을 올렸습니다. _ Proceedings of International AAAI Conference on Web and Social Media_, 14:885-894, may 2020. doi: 10.1609/icwsm.v14i1.7354. URL [https://doi.org/10.1609%2Fciwsm.v14i1.7354](https://doi.org/10.1609%2Fciwsm.v14i1.7354)
* Patterson 등(2021) David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 탄소 배출 및 대규모 신경망 훈련, 2021. URL [https://arxiv.org/abs/2104.10350](https://arxiv.org/abs/2104.10350).
* Penedo 등(2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aimee Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. falcon llm에 대한 정제된 웹 데이터 세트: 웹 데이터 및 웹 데이터만으로 큐레이트된 말뭉치를 성능 저하시킵니다. _ ArXiv_, abs/2306.01116, 2023. URL [https://api.semanticscholar.org/CorpusID:259063761](https://api.semanticscholar.org/CorpusID:259063761).
* Peters 등(2018) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 심층 문맥화된 단어 표현입니다. _ ArXiv_, abs/1802.05365, 2018. URL [https://api.semanticscholar.org/CorpusID:3626819](https://api.semanticscholar.org/CorpusID:3626819).
* Pilehvar and Camacho-Collados (2018) Mohammad Taher Pilehvar and Jose Camacho-Collados. Wic: 문맥-민감 표현들을 평가하기 위한 10,000개의 예시 쌍들. _ CoRR_, abs/1808.09121, 2018. URL [http://arxiv.org/abs/1808.09121](http://arxiv.org/abs/1808.09121).
* Rae 등(2019) Jack W. 래, 세바스티안 보헤우, 트레보 카제이, 케이티 밀리칸, 조던 호프만, 프란시스 송, 존 아슬란데스, 로만 링, 수잔 아슬란데스 영, 엘라이자 러더포드, 톰 헤니건, 제이콥 메닉스, 요하네스 웰블, 수만스 다타트리, 사프론 황, 조나단 우에사토, 존 멜로, 이리나 히긴스, 안토니아 크레웰, 나트 맥앨리스, 에이미 우, 에리히 엘센, 싯트 자야쿠마, 나트 맥앨리스, 에이미 우, 에리히 엘센, 시안 시모니아, 미켈라 파가니니, 로랑 시프르, 라다 마타케, 엘레나 마르텐스, 니콜라이 그라니케, 니코레니, 미켈라 파가니니, 로랑 시프르, 아이다 마타케, 장-밥티스트 레스피어, 니코레네 리, 아델리키 라자르, 아데나 마타케, 니코레네 리, 아델리키 라자르, 아 언어 모델 크기 조정: Methods, analysis & insights from training gopher, 2022. URL [https://arxiv.org/abs/2112.11446](https://arxiv.org/abs/2112.11446).
* Raffel 등(2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계를 탐구합니다. _ 제이 마케터 배워 Res._ , 21(1), jan 2020. ISSN 1532-4435.
* Rajbhandari 등(2019) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 0: 메모리 최적화로 조 단위 매개 변수 모델을 학습합니다. _ SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16, 2019. URL [https://api.semanticscholar.org/CorpusID:203736482](https://api.semanticscholar.org/CorpusID:203736482).
* Reid 등(2022) Machel Reid, Victor Zhong, Suchin Gururangan, and Luke Zettlemoyer. M2D2: 대규모 다중 도메인 언어 모델링 데이터세트. _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 964-975, page 964-975, United Arab Emirates, December 2022. Association for Computational Linguistics. URL [https://aclanthology.org/2022.ennlp-main.63](https://aclanthology.org/2022.ennlp-main.63)
* Ribeiro 등(2021) Manoel Horta Ribeiro, Jeremy Blackburn, Barry Bradlyn, Emiliano De Cristofaro, Gianluca Stringhini, Summer Long, Stephanie Greenberg, and Savvas Zannettou. 웹을 가로지르는 맨스피어의 진화. _ Proceedings of International AAAI Conference on Web and Social Media_, 15:196-207, may 2021. doi: 10.1609/icwsm.v15i1.18053. URL [https://doi.org/10.1609/2Ficwsm.v15i1.18053](https://doi.org/10.1609/2Ficwsm.v15i1.18053)
* Roemmele 등(2011) Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 그럴듯한 대안들의 선택: 상식적인 인과 추론에 대한 평가 _2011 AAAI Spring Symposium Series_에서 2011. URL [https://aaai.org/papers/02418-2418-choice-of-plausible-alternatives-an-evaluation-of-commonsense-causal-reasoning/](https://aaai.org/papers/02418-2418-choice-of-plausible-alternatives-an-evaluation-of-commonsense-causal-reasoning/)입니다.
* Rosenfeld (2000) Ronald Rosenfeld. 20년간의 통계적 언어 모델링: 이제 어떻게 해야 할까요? _ Proceedings of the IEEE_, 88(8):1270-1278, 2000.
* Sakaguchi 등(2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: 스케일에서의 적대적인 winograd 스키마 챌린지. _ ACSM_, 64(9):99-106, 2021. URL [https://dl.acm.org/doi/abs/10.1145/3474381](https://dl.acm.org/doi/abs/10.1145/3474381).
* Shazeer(2020) Noam M. 셰이저 Glu 변형은 변압기를 개선 합니다. _ ArXiv_, abs/2002.05202, 2020. URL [https://api.semanticscholar.org/CorpusID:211096588](https://api.semanticscholar.org/CorpusID:211096588).
* Soldaini 등(2024) Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Ian Magnusson, Jacob Morrison, Niklas Muenninghoff, Aakakkansha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Gyvind Tafjord, Evan Pete Walsh, Hannanehajishirzi, Noah A. Smith, Luke Zettlemoyer, Iz Beltagy, Dirk Groeneveld, Jess Dodge, and Kyle Lo. 돌마: 언어 모델 사전 훈련 연구를 위한 3조 토큰의 열린 말뭉치 arXiv preprint_, 2024.
* Strubell 등(2019) Emma Strubell, Ananya Ganesh, and Andrew McCallum. NLP에서 딥러닝을 위한 에너지 및 정책 고려 사항 Anna Korhonen, David Traum, and Lluis Marquez, editors, _Proceedings of the 57 Annual Meeting of the Association of Computational Linguistics_, pages 3645-3650, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1355. URL [https://aclanthology.org/P19-1355](https://aclanthology.org/P19-1355).
* Su et al.(2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu. 로포르머: 회전 위치 매립을 갖는 향상된 트랜스포머. _ ArXiv_, abs/2104.09864, 2021. URL [https://api.semanticscholar.org/CorpusID:233307138](https://api.semanticscholar.org/CorpusID:233307138).
* Su 등(2020)Together Computer. RedPajama: LLaMA 학습 데이터 세트를 재현하는 오픈 소스 레시피, 2023년 4월. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).
* Touvron 등(2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: 오픈하고 효율적인 기초 언어 모델입니다. _ ArXiv_, abs/2302.13971, 2023a. URL [https://api.semanticscholar.org/CorpusID:257219404](https://api.semanticscholar.org/CorpusID:257219404).
* Touvron 등(2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Anthony Hartshorn, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi 라마 2: 오픈 파운데이션과 미세 조정된 채팅 모델, 2023b. URL [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).
* Ubierna 등(2022) Maria Ubierna, Cristina Diez Santos, and Sara Mercier-Blais. _ 물 보안 및 기후 변화: 수력 저장소 온실 가스 배출_, 69-94 페이지 싱가포르, 싱가포르, 2022. ISBN 978-981-16-5493-0. doi: 10.1007/978-981-16-5493-0_5. URL [https://doi.org/10.1007/978-981-16-5493-0_5](https://doi.org/10.1007/978-981-16-5493-0_5)
* Vaswani 등(2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. 관심만 있으면 됩니다. 인구연 본룩스부르크 벵지오 퍼거스 Vishwanathan, R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL [https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
* Vilares and Gomez-Rodriguez (2019) David Vilares and Carlos Gomez-Rodriguez. HEAD-QA: 복잡한 추론을 위한 헬스케어 데이터세트. Anna Korhonen, David Traum, and Lluis Marquez, editors, _Proceedings of the 57 Annual Meeting of the Association of Computational Linguistics_, pages 960-966, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1092. URL [https://aclanthology.org/P19-1092](https://aclanthology.org/P19-1092)
* Wang 등(2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. 보우먼 글루: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼. _ ArXiv_, abs/1804.07461, 2018. URL [https://arxiv.org/abs/1804.07461](https://arxiv.org/abs/1804.07461).
* Wang 등(2023) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 낙타는 얼마나 멀리 갈 수 있나요? 오픈 리소스의 명령 조정 상태 탐색, 2023. URL [https://arxiv.org/abs/2306.04751](https://arxiv.org/abs/2306.04751).
* Welbl 등(2017) Johannes Welbl, Nelson F Liu, and Matt Gardner. 크라우드소싱 객관식 과학 질문입니다. _ arXiv preprint arXiv:1707.06209_, 2017. URL [https://arxiv.org/abs/1707.06209](https://arxiv.org/abs/1707.06209)
* Wu 등(2022) Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S. 리, 부그라 아킬디즈, 막시밀리안 발란다트, 조 스피삭, 라비 자인, 마이크 랍바트, 김 헤이즐우드 등이다. Sustainable ai: Environmental implications, challenges and opportunities, 2022. URL [https://arxiv.org/abs/2111.00364](https://arxiv.org/abs/2111.00364)
* Zannettou 등(2018) Savvas Zannettou, Barry Bradlyn, Emiliano De Cristofaro, Haewoon Kwak, Michael Sirivianos, Gianluca Stringini, and Jeremy Blackburn. 개브란 무엇인가: 자유로운 발언의 보루나 정통한 반향실. _Companion Proceedings of the Web Conference 2018_, WWW'18,page 1007-1014, Republic and Canton of Geneva, CHE, 2018. International World Wide Web Conferences 운영위원회. ISBN 9781450356404. doi: 10.1145/3184558.3191531. URL [https://doi.org/10.1145/3184558.3191531](https://doi.org/10.1145/3184558.3191531)
* Zellers 등(2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 헬라스와그: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _ arXiv preprint arXiv:1905.07830_, 2019. URL [https://arxiv.org/abs/1905.07830](https://arxiv.org/abs/1905.07830).
* Zhang and Sennrich (2019) Biao Zhang and Rico Sennrich. Root mean square layer normalization. _ ArXiv_, abs/1910.07467, 2019. URL [https://api.semanticscholar.org/CorpusID:113405151](https://api.semanticscholar.org/CorpusID:113405151).
* Zhang 등(2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: 사전 훈련된 변압기 언어 모델 열기, 2022. URL [https://arxiv.org/abs/2205.01068](https://arxiv.org/abs/2205.01068).
* Zhao 등(2023) Yanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li. Pytorch fsdp: 완전히 샤드된 데이터를 병렬로 스케일링하는 경험 _ Proc. VLDB Endow._ , 16:3848-3860, 2023. URL [https://api.semanticscholar.org/CorpusID:258297871](https://api.semanticscholar.org/CorpusID:258297871).

추가 복잡성 결과

도 3에서 우리는 도 2에서 결합된 메트릭에서 제외되는 팔로마(Magnusson et al., 2023)의 7개의 데이터 소스 각각에 대한 결과를 제공한다. Pile(Gao et al., 2020) 및 ICE(Greenbaum and Nelson, 1996)와 같은 이러한 소스 중 일부는 현재 공개적으로 이용 가능하지 않다. 돌마 100 프로그래밍 언어(Soldaini 등, 2024)는 팔로마에서 사용되는 오염 제거 접근법에 의해 지원되지 않는 코드 데이터로 구성된다. 트위터AAE(Blodgett et al., 2016)는 ICE와 함께 서로 다른 방언 간의 성능 차이를 대상으로 분석하기 위한 데이터 세트이므로 별도로 평가해야 한다. 그리고 마지막으로, Manosphere, Gab, and 4chan corpora (Ribeiro et al., 2021; Zannettou et al., 2018; Papasavva et al., 2020)는 널리 퍼진 혐오 발언 및 독성에 대해 연구된 프린지 온라인 커뮤니티로부터 언어에 대한 모델 적합성을 조사하기 위한 것이다. 따라서 이러한 프린지 말뭉치에서 복잡성을 최소화하는 것이 항상 바람직한 것은 아니다.

여기서 주목할 만한 결과 중 하나는 OLMo-7B가 돌마 100 프로그래밍 언어(100 PL)의 다른 모델보다 훨씬 앞서 있다는 것이다. 이 효과는 부분적으로 오염 제거 코드 데이터가 팔로마에서 방법의 범위를 벗어났기 때문에 오염으로 인한 과소평가 때문일 수 있다. 동시에 RPJ-INCITE-7B와 같은 GitHub의 코드 데이터에 대해 훈련된 다른 모델은 오염이 있을 가능성이 훨씬 더 나쁘다. 또 다른 요인은 OLMo-7B가 100 PL의 코드 데이터와 정확히 동일한 후처리를 통해 코드 데이터를 학습하는 반면 다른 모델의 코드 데이터는 다르게 처리된다는 것이다. 유사하게, Pile 평가는 OLMo-7B보다 거의 10배 적은 토큰으로 훈련되었음에도 불구하고 피티아-6.9B가 최고 성능을 달성함에 따라 이러한 분포 내 및 잠재적인 오염 효과를 보여준다.

팔로마는 종종 이러한 소스에 대한 당혹감이 이러한 연설 커뮤니티의 구성체에 실제로 두드러질 것 보다는 낮은 평균 문서 길이와 같은 표면적 특징에 의해 지배된다는 것을 발견하기 때문에 나머지 5개의 표적 소스에 대한 결과는 주의 깊게 해석되어야 한다. 트위터AAE와 Gab는 팔로마에서 가장 짧은 문서 중 이 그림에서 바이트당 비정상적으로 높은 비트에 기여하는 문서를 가지고 있다. 이 두 가지 외에도 모델은 ICE, Manosphere 및 4chan의 데이터 스케일링 경향으로 특히 매우 밀접하게 그룹화된다.

추가 최종 작업 결과 다음으로, 표 8에서 핵심 평가 제품군의 9개 외에 6개의 추가 최종 작업에 대한 OLMo-7B의 제로 샷 평가 결과를 제공한다. 이러한 작업은 headqa_en(Vilares and Gomez-Rodriguez, 2019), logiqa(Liu et al., 2020), mrpc(Dolan and Brockett, 2005), qnli(Wang et al., 2018), wic(Pilehvar and Camacho-Collados, 2018), wnli(Wang et al., 2018)입니다.

그림 3: 그림 2에서 집계되지 않은 나머지 7개의 팔로마 데이터 소스 각각에 대한 바이트당 비트.

[MISSING_PAGE_FAIL:21]

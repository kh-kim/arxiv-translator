<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.01889] Ring Attention with Blockwise Transformers for Near-Infinite Context</title><meta property="og:description" content="Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformer…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ring Attention with Blockwise Transformers for Near-Infinite Context">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Ring Attention with Blockwise Transformers for Near-Infinite Context">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.01889">

<!--Generated on Sun Nov  5 17:25:43 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Ring Attention with Blockwise 
<br class="ltx_break">Transformers for Near-Infinite Context</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Liu, Matei Zaharia, Pieter Abbeel

<br class="ltx_break">
UC Berkeley
<br class="ltx_break">
<span id="id1.1.id1" class="ltx_text ltx_font_typewriter">hao.liu@cs.berkeley.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while overlapping the communication of key-value blocks with the computation of blockwise attention. Ring Attention enables training and inference of sequences that are up to device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices.
Extensive experiments on language modeling tasks demonstrate the effectiveness of Ring Attention in allowing large sequence input size and improving performance.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Transformers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite> have become the backbone of many state-of-the-art AI systems that have demonstrated impressive performance across a wide range of AI problems.
Transformers achieve this success through their architecture design that uses self-attention and position-wise feedforward mechanisms.</p>
</div>
<figure id="S1.F1" class="ltx_figure ltx_align_floatright"><img src="/html/2310.01889/assets/figures/context_len.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Maximum context length under end-to-end large-scale training on TPUv4-1024.
Baselines are vanilla transformers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite>, memory efficient transformers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, and memory efficient attention and feedforward (blockwise parallel transformers)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>.
Our proposed approach Ring Attention allows training up to device count times longer sequence than baselines and enables the training of sequences that exceed millions in length without making approximations to attention.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">These components facilitate the efficient capture of long-range dependencies between input tokens, and enable scalability through highly parallel computations.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, scaling up the context length of Transformers is a challenge&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>, since the inherited architecture design of Transformers, <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">i</span>.<span id="S1.p3.1.2" class="ltx_text ltx_font_italic">e</span>.&nbsp;the self-attention has memory cost quadratic in the input sequence length,
which makes it challenging to scale to longer input sequences.
Large context Transformers are essential for
tackling a diverse array of AI challenges, ranging from processing books and high-resolution images to analyzing long videos and complex codebases.
They excel at extracting information from the interconnected web and hyperlinked content, and are crucial for handling complex scientific experiment data.
There have been emerging use cases of language models with significantly expanded context than before: GPT-3.5&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Schulman et&nbsp;al., <a href="#bib.bib33" title="" class="ltx_ref">2022</a>)</cite> with context length 16K, GPT-4&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite> with context length 32k, MosaicML’s MPT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(MosaicML, <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> with context length 65k, and Anthropic’s Claude&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> with context length 100k.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Driven by the significance, there has been surging research interests in reducing memory cost. One line of research leverages the observation that the softmax matrix in self-attention can be computed without materializing the full matrix&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Milakov and Gimelshein, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite> which has led to the development of blockwise computation of self-attention and feedforward&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Dao et&nbsp;al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>; Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite> without making approximations.
Despite the reduced memory, a significant challenge still arises from storing the output of each layer.
This necessity arises from self-attention’s inherent nature, involving interactions among all elements (n to n interactions). The subsequent layer’s self-attention relies on accessing all of the prior layer’s outputs. Failing to do so would increase computational costs cubically, as every output must be recomputed for each sequence element, rendering it impractical for longer sequences.
To put the memory demand in perspective, even when dealing with a batch size of 1, processing 100 million tokens requires over 1000GB of memory for a modest model with a hidden size of 1024. This is much greater than the capacity of contemporary GPUs and TPUs, which typically have less than 100GB of high-bandwidth memory (HBM).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To tackle this challenge, we make a key observation: by performing self-attention and feedforward network computations in a blockwise fashion&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>, we can distribute sequence dimensions across multiple devices, allowing concurrent computation and communication.
This insight stems from the fact that when we compute the attention on a block-by-block basis, the results are invariant to the ordering of these blockwise computations.
Our method distributes the outer loop of computing blockwise attention among hosts, with each device managing its respective input block. For the inner loop, every device computes blockwise attention and feedforward operations specific to its designated input block.
Host devices form a conceptual ring, where during the inner loop, each device sends a copy of its key-value blocks being used for blockwise computation to the next device in the ring, while simultaneously receiving key-value blocks from the previous one.
Because block computations take longer than block transfers, overlapping these processes results in no added overhead compared to standard transformers.
By doing so, each device requires memory only proportional to the block size, which is independent of the original input sequence length. This effectively eliminates the memory constraints imposed by individual devices.
Since our approach overlaps the communication of key-value blocks between hosts in a ring with blockwise computation, we name it Ring Attention.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We evaluate the effectiveness of our approach on language modeling benchmarks. Our experiments show that Ring Attention can reduce the memory requirements of Transformers, enabling us to train more than 500 times longer sequence than prior memory efficient state-of-the-arts and enables the training of sequences that exceed 100 million in length without making approximations to attention. Importantly, Ring Attention eliminates the memory constraints imposed by individual devices, empowering the training and inference of sequences with lengths that scale in proportion to the number of devices, essentially achieving near-infinite context size.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our contributions are twofold: (a) proposing a memory efficient transformers architecture that allows the context length to scale linearly with the number of devices while maintaining performance, eliminating the memory bottleneck imposed by individual devices, and (b) demonstrating the effectiveness of our approach through extensive experiments.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Large Context Memory Constraint</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.3" class="ltx_p">Given input sequences <math id="S2.p1.1.m1.3" class="ltx_Math" alttext="Q,K,V\in\mathbb{R}^{s\times d}" display="inline"><semantics id="S2.p1.1.m1.3a"><mrow id="S2.p1.1.m1.3.4" xref="S2.p1.1.m1.3.4.cmml"><mrow id="S2.p1.1.m1.3.4.2.2" xref="S2.p1.1.m1.3.4.2.1.cmml"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">Q</mi><mo id="S2.p1.1.m1.3.4.2.2.1" xref="S2.p1.1.m1.3.4.2.1.cmml">,</mo><mi id="S2.p1.1.m1.2.2" xref="S2.p1.1.m1.2.2.cmml">K</mi><mo id="S2.p1.1.m1.3.4.2.2.2" xref="S2.p1.1.m1.3.4.2.1.cmml">,</mo><mi id="S2.p1.1.m1.3.3" xref="S2.p1.1.m1.3.3.cmml">V</mi></mrow><mo id="S2.p1.1.m1.3.4.1" xref="S2.p1.1.m1.3.4.1.cmml">∈</mo><msup id="S2.p1.1.m1.3.4.3" xref="S2.p1.1.m1.3.4.3.cmml"><mi id="S2.p1.1.m1.3.4.3.2" xref="S2.p1.1.m1.3.4.3.2.cmml">ℝ</mi><mrow id="S2.p1.1.m1.3.4.3.3" xref="S2.p1.1.m1.3.4.3.3.cmml"><mi id="S2.p1.1.m1.3.4.3.3.2" xref="S2.p1.1.m1.3.4.3.3.2.cmml">s</mi><mo lspace="0.222em" rspace="0.222em" id="S2.p1.1.m1.3.4.3.3.1" xref="S2.p1.1.m1.3.4.3.3.1.cmml">×</mo><mi id="S2.p1.1.m1.3.4.3.3.3" xref="S2.p1.1.m1.3.4.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.3b"><apply id="S2.p1.1.m1.3.4.cmml" xref="S2.p1.1.m1.3.4"><in id="S2.p1.1.m1.3.4.1.cmml" xref="S2.p1.1.m1.3.4.1"></in><list id="S2.p1.1.m1.3.4.2.1.cmml" xref="S2.p1.1.m1.3.4.2.2"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝑄</ci><ci id="S2.p1.1.m1.2.2.cmml" xref="S2.p1.1.m1.2.2">𝐾</ci><ci id="S2.p1.1.m1.3.3.cmml" xref="S2.p1.1.m1.3.3">𝑉</ci></list><apply id="S2.p1.1.m1.3.4.3.cmml" xref="S2.p1.1.m1.3.4.3"><csymbol cd="ambiguous" id="S2.p1.1.m1.3.4.3.1.cmml" xref="S2.p1.1.m1.3.4.3">superscript</csymbol><ci id="S2.p1.1.m1.3.4.3.2.cmml" xref="S2.p1.1.m1.3.4.3.2">ℝ</ci><apply id="S2.p1.1.m1.3.4.3.3.cmml" xref="S2.p1.1.m1.3.4.3.3"><times id="S2.p1.1.m1.3.4.3.3.1.cmml" xref="S2.p1.1.m1.3.4.3.3.1"></times><ci id="S2.p1.1.m1.3.4.3.3.2.cmml" xref="S2.p1.1.m1.3.4.3.3.2">𝑠</ci><ci id="S2.p1.1.m1.3.4.3.3.3.cmml" xref="S2.p1.1.m1.3.4.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.3c">Q,K,V\in\mathbb{R}^{s\times d}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.3d">italic_Q , italic_K , italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_s × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> where <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">italic_s</annotation></semantics></math> is the sequence length and
<math id="S2.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.1d">italic_d</annotation></semantics></math> is the head dimension.
We compute the matrix of outputs as:</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.5" class="ltx_Math" alttext="\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d}})V," display="block"><semantics id="S2.Ex1.m1.5a"><mrow id="S2.Ex1.m1.5.5.1" xref="S2.Ex1.m1.5.5.1.1.cmml"><mrow id="S2.Ex1.m1.5.5.1.1" xref="S2.Ex1.m1.5.5.1.1.cmml"><mrow id="S2.Ex1.m1.5.5.1.1.2" xref="S2.Ex1.m1.5.5.1.1.2.cmml"><mi id="S2.Ex1.m1.5.5.1.1.2.2" xref="S2.Ex1.m1.5.5.1.1.2.2.cmml">Attention</mi><mo id="S2.Ex1.m1.5.5.1.1.2.1" xref="S2.Ex1.m1.5.5.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mrow id="S2.Ex1.m1.5.5.1.1.2.3.2" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.2.3.2.1" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml">(</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">Q</mi><mo id="S2.Ex1.m1.5.5.1.1.2.3.2.2" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml">K</mi><mo id="S2.Ex1.m1.5.5.1.1.2.3.2.3" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S2.Ex1.m1.3.3" xref="S2.Ex1.m1.3.3.cmml">V</mi><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.2.3.2.4" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.5.5.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S2.Ex1.m1.5.5.1.1.3" xref="S2.Ex1.m1.5.5.1.1.3.cmml"><mi id="S2.Ex1.m1.5.5.1.1.3.2" xref="S2.Ex1.m1.5.5.1.1.3.2.cmml">softmax</mi><mo id="S2.Ex1.m1.5.5.1.1.3.1" xref="S2.Ex1.m1.5.5.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mrow id="S2.Ex1.m1.5.5.1.1.3.3.2" xref="S2.Ex1.m1.4.4.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.3.3.2.1" xref="S2.Ex1.m1.4.4.cmml">(</mo><mfrac id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.4.cmml"><mrow id="S2.Ex1.m1.4.4.2" xref="S2.Ex1.m1.4.4.2.cmml"><mi id="S2.Ex1.m1.4.4.2.2" xref="S2.Ex1.m1.4.4.2.2.cmml">Q</mi><mo id="S2.Ex1.m1.4.4.2.1" xref="S2.Ex1.m1.4.4.2.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S2.Ex1.m1.4.4.2.3" xref="S2.Ex1.m1.4.4.2.3.cmml"><mi id="S2.Ex1.m1.4.4.2.3.2" xref="S2.Ex1.m1.4.4.2.3.2.cmml">K</mi><mi id="S2.Ex1.m1.4.4.2.3.3" xref="S2.Ex1.m1.4.4.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S2.Ex1.m1.4.4.3" xref="S2.Ex1.m1.4.4.3.cmml"><mi id="S2.Ex1.m1.4.4.3.2" xref="S2.Ex1.m1.4.4.3.2.cmml">d</mi></msqrt></mfrac><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.3.3.2.2" xref="S2.Ex1.m1.4.4.cmml">)</mo></mrow><mo id="S2.Ex1.m1.5.5.1.1.3.1a" xref="S2.Ex1.m1.5.5.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.Ex1.m1.5.5.1.1.3.4" xref="S2.Ex1.m1.5.5.1.1.3.4.cmml">V</mi></mrow></mrow><mo id="S2.Ex1.m1.5.5.1.2" xref="S2.Ex1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.5b"><apply id="S2.Ex1.m1.5.5.1.1.cmml" xref="S2.Ex1.m1.5.5.1"><eq id="S2.Ex1.m1.5.5.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1"></eq><apply id="S2.Ex1.m1.5.5.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2"><times id="S2.Ex1.m1.5.5.1.1.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2.1"></times><ci id="S2.Ex1.m1.5.5.1.1.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2">Attention</ci><vector id="S2.Ex1.m1.5.5.1.1.2.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2.3.2"><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">𝑄</ci><ci id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2">𝐾</ci><ci id="S2.Ex1.m1.3.3.cmml" xref="S2.Ex1.m1.3.3">𝑉</ci></vector></apply><apply id="S2.Ex1.m1.5.5.1.1.3.cmml" xref="S2.Ex1.m1.5.5.1.1.3"><times id="S2.Ex1.m1.5.5.1.1.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.3.1"></times><ci id="S2.Ex1.m1.5.5.1.1.3.2.cmml" xref="S2.Ex1.m1.5.5.1.1.3.2">softmax</ci><apply id="S2.Ex1.m1.4.4.cmml" xref="S2.Ex1.m1.5.5.1.1.3.3.2"><divide id="S2.Ex1.m1.4.4.1.cmml" xref="S2.Ex1.m1.5.5.1.1.3.3.2"></divide><apply id="S2.Ex1.m1.4.4.2.cmml" xref="S2.Ex1.m1.4.4.2"><times id="S2.Ex1.m1.4.4.2.1.cmml" xref="S2.Ex1.m1.4.4.2.1"></times><ci id="S2.Ex1.m1.4.4.2.2.cmml" xref="S2.Ex1.m1.4.4.2.2">𝑄</ci><apply id="S2.Ex1.m1.4.4.2.3.cmml" xref="S2.Ex1.m1.4.4.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.4.4.2.3.1.cmml" xref="S2.Ex1.m1.4.4.2.3">superscript</csymbol><ci id="S2.Ex1.m1.4.4.2.3.2.cmml" xref="S2.Ex1.m1.4.4.2.3.2">𝐾</ci><ci id="S2.Ex1.m1.4.4.2.3.3.cmml" xref="S2.Ex1.m1.4.4.2.3.3">𝑇</ci></apply></apply><apply id="S2.Ex1.m1.4.4.3.cmml" xref="S2.Ex1.m1.4.4.3"><root id="S2.Ex1.m1.4.4.3a.cmml" xref="S2.Ex1.m1.4.4.3"></root><ci id="S2.Ex1.m1.4.4.3.2.cmml" xref="S2.Ex1.m1.4.4.3.2">𝑑</ci></apply></apply><ci id="S2.Ex1.m1.5.5.1.1.3.4.cmml" xref="S2.Ex1.m1.5.5.1.1.3.4">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.5c">\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d}})V,</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.5d">roman_Attention ( italic_Q , italic_K , italic_V ) = roman_softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d end_ARG end_ARG ) italic_V ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.p1.4" class="ltx_p">where <math id="S2.p1.4.m1.1" class="ltx_Math" alttext="\mathrm{softmax}" display="inline"><semantics id="S2.p1.4.m1.1a"><mi id="S2.p1.4.m1.1.1" xref="S2.p1.4.m1.1.1.cmml">softmax</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m1.1b"><ci id="S2.p1.4.m1.1.1.cmml" xref="S2.p1.4.m1.1.1">softmax</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m1.1c">\mathrm{softmax}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m1.1d">roman_softmax</annotation></semantics></math> is applied row-wise.
Each self-attention sub-layer is accompanied with a feedforward network, which is applied to each position separately and identically.
This consists of two linear transformations with a ReLU activation in between.</p>
<table id="S2.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex2.m1.4" class="ltx_Math" alttext="\mathrm{FFN}(x)=\max(0,xW_{1}+b_{1})W_{2}+b_{2}." display="block"><semantics id="S2.Ex2.m1.4a"><mrow id="S2.Ex2.m1.4.4.1" xref="S2.Ex2.m1.4.4.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1" xref="S2.Ex2.m1.4.4.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1.3" xref="S2.Ex2.m1.4.4.1.1.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.3.2" xref="S2.Ex2.m1.4.4.1.1.3.2.cmml">FFN</mi><mo id="S2.Ex2.m1.4.4.1.1.3.1" xref="S2.Ex2.m1.4.4.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mrow id="S2.Ex2.m1.4.4.1.1.3.3.2" xref="S2.Ex2.m1.4.4.1.1.3.cmml"><mo stretchy="false" id="S2.Ex2.m1.4.4.1.1.3.3.2.1" xref="S2.Ex2.m1.4.4.1.1.3.cmml">(</mo><mi id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml">x</mi><mo stretchy="false" id="S2.Ex2.m1.4.4.1.1.3.3.2.2" xref="S2.Ex2.m1.4.4.1.1.3.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.4.4.1.1.2" xref="S2.Ex2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S2.Ex2.m1.4.4.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml"><mi id="S2.Ex2.m1.2.2" xref="S2.Ex2.m1.2.2.cmml">max</mi><mo id="S2.Ex2.m1.4.4.1.1.1.1.1.1a" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml">(</mo><mn id="S2.Ex2.m1.3.3" xref="S2.Ex2.m1.3.3.cmml">0</mn><mo id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml">,</mo><mrow id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><msub id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.2.cmml">W</mi><mn id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></msub></mrow><mo id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml">b</mi><mn id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.4" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.4.4.1.1.1.1.2" xref="S2.Ex2.m1.4.4.1.1.1.1.2.cmml" lspace="0px" rspace="0px"></mo><msub id="S2.Ex2.m1.4.4.1.1.1.1.3" xref="S2.Ex2.m1.4.4.1.1.1.1.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.1.3.2" xref="S2.Ex2.m1.4.4.1.1.1.1.3.2.cmml">W</mi><mn id="S2.Ex2.m1.4.4.1.1.1.1.3.3" xref="S2.Ex2.m1.4.4.1.1.1.1.3.3.cmml">2</mn></msub></mrow><mo id="S2.Ex2.m1.4.4.1.1.1.2" xref="S2.Ex2.m1.4.4.1.1.1.2.cmml">+</mo><msub id="S2.Ex2.m1.4.4.1.1.1.3" xref="S2.Ex2.m1.4.4.1.1.1.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.3.2" xref="S2.Ex2.m1.4.4.1.1.1.3.2.cmml">b</mi><mn id="S2.Ex2.m1.4.4.1.1.1.3.3" xref="S2.Ex2.m1.4.4.1.1.1.3.3.cmml">2</mn></msub></mrow></mrow><mo lspace="0em" id="S2.Ex2.m1.4.4.1.2" xref="S2.Ex2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.4b"><apply id="S2.Ex2.m1.4.4.1.1.cmml" xref="S2.Ex2.m1.4.4.1"><eq id="S2.Ex2.m1.4.4.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.2"></eq><apply id="S2.Ex2.m1.4.4.1.1.3.cmml" xref="S2.Ex2.m1.4.4.1.1.3"><times id="S2.Ex2.m1.4.4.1.1.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.3.1"></times><ci id="S2.Ex2.m1.4.4.1.1.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.3.2">FFN</ci><ci id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1">𝑥</ci></apply><apply id="S2.Ex2.m1.4.4.1.1.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1"><plus id="S2.Ex2.m1.4.4.1.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.2"></plus><apply id="S2.Ex2.m1.4.4.1.1.1.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1"><times id="S2.Ex2.m1.4.4.1.1.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.2"></times><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1"><max id="S2.Ex2.m1.2.2.cmml" xref="S2.Ex2.m1.2.2"></max><cn type="integer" id="S2.Ex2.m1.3.3.cmml" xref="S2.Ex2.m1.3.3">0</cn><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1"><plus id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2"><times id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.1"></times><ci id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.2">𝑥</ci><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.2">𝑊</ci><cn type="integer" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.2">𝑏</ci><cn type="integer" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply><apply id="S2.Ex2.m1.4.4.1.1.1.1.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.4.4.1.1.1.1.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex2.m1.4.4.1.1.1.1.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.3.2">𝑊</ci><cn type="integer" id="S2.Ex2.m1.4.4.1.1.1.1.3.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.3.3">2</cn></apply></apply><apply id="S2.Ex2.m1.4.4.1.1.1.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.4.4.1.1.1.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.3">subscript</csymbol><ci id="S2.Ex2.m1.4.4.1.1.1.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.3.2">𝑏</ci><cn type="integer" id="S2.Ex2.m1.4.4.1.1.1.3.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.4c">\mathrm{FFN}(x)=\max(0,xW_{1}+b_{1})W_{2}+b_{2}.</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.4d">roman_FFN ( italic_x ) = roman_max ( 0 , italic_x italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.7" class="ltx_p"><span id="S2.p2.7.1" class="ltx_text ltx_font_bold">Blockwise Parallel Transformers.</span>
Prior state-of-the-arts have led to substantial reductions in memory utilization, achieved through innovative techniques that enable attention computation without full materialization by computing attention in a block by block manner&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Dao et&nbsp;al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>; Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>.
These advancements lowered the memory overhead of attention to <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="2bsh" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mn id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">2</mn><mo id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">b</mi><mo id="S2.p2.1.m1.1.1.1a" xref="S2.p2.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.1.m1.1.1.4" xref="S2.p2.1.m1.1.1.4.cmml">s</mi><mo id="S2.p2.1.m1.1.1.1b" xref="S2.p2.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.1.m1.1.1.5" xref="S2.p2.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><times id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"></times><cn type="integer" id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">2</cn><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">𝑏</ci><ci id="S2.p2.1.m1.1.1.4.cmml" xref="S2.p2.1.m1.1.1.4">𝑠</ci><ci id="S2.p2.1.m1.1.1.5.cmml" xref="S2.p2.1.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">2 italic_b italic_s italic_h</annotation></semantics></math> bytes per layer, where <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">b</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">italic_b</annotation></semantics></math> represents the batch size, <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">italic_s</annotation></semantics></math> denotes the sequence length, and <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S2.p2.4.m4.1a"><mi id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><ci id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.p2.4.m4.1d">italic_h</annotation></semantics></math> stands for the hidden size of the model.
To further reduce memory usage, blockwise parallel transformer (BPT)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite> introduced a strategy where the feedforward network associated with each self-attention sub-layer is computed in a block-wise fashion.
This approach effectively limits the maximum activation size of feedforward network from <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="8bsh" display="inline"><semantics id="S2.p2.5.m5.1a"><mrow id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><mn id="S2.p2.5.m5.1.1.2" xref="S2.p2.5.m5.1.1.2.cmml">8</mn><mo id="S2.p2.5.m5.1.1.1" xref="S2.p2.5.m5.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.5.m5.1.1.3" xref="S2.p2.5.m5.1.1.3.cmml">b</mi><mo id="S2.p2.5.m5.1.1.1a" xref="S2.p2.5.m5.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.5.m5.1.1.4" xref="S2.p2.5.m5.1.1.4.cmml">s</mi><mo id="S2.p2.5.m5.1.1.1b" xref="S2.p2.5.m5.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.5.m5.1.1.5" xref="S2.p2.5.m5.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><times id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1.1"></times><cn type="integer" id="S2.p2.5.m5.1.1.2.cmml" xref="S2.p2.5.m5.1.1.2">8</cn><ci id="S2.p2.5.m5.1.1.3.cmml" xref="S2.p2.5.m5.1.1.3">𝑏</ci><ci id="S2.p2.5.m5.1.1.4.cmml" xref="S2.p2.5.m5.1.1.4">𝑠</ci><ci id="S2.p2.5.m5.1.1.5.cmml" xref="S2.p2.5.m5.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">8bsh</annotation><annotation encoding="application/x-llamapun" id="S2.p2.5.m5.1d">8 italic_b italic_s italic_h</annotation></semantics></math> to <math id="S2.p2.6.m6.1" class="ltx_Math" alttext="2bsh" display="inline"><semantics id="S2.p2.6.m6.1a"><mrow id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml"><mn id="S2.p2.6.m6.1.1.2" xref="S2.p2.6.m6.1.1.2.cmml">2</mn><mo id="S2.p2.6.m6.1.1.1" xref="S2.p2.6.m6.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.6.m6.1.1.3" xref="S2.p2.6.m6.1.1.3.cmml">b</mi><mo id="S2.p2.6.m6.1.1.1a" xref="S2.p2.6.m6.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.6.m6.1.1.4" xref="S2.p2.6.m6.1.1.4.cmml">s</mi><mo id="S2.p2.6.m6.1.1.1b" xref="S2.p2.6.m6.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.6.m6.1.1.5" xref="S2.p2.6.m6.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><apply id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1"><times id="S2.p2.6.m6.1.1.1.cmml" xref="S2.p2.6.m6.1.1.1"></times><cn type="integer" id="S2.p2.6.m6.1.1.2.cmml" xref="S2.p2.6.m6.1.1.2">2</cn><ci id="S2.p2.6.m6.1.1.3.cmml" xref="S2.p2.6.m6.1.1.3">𝑏</ci><ci id="S2.p2.6.m6.1.1.4.cmml" xref="S2.p2.6.m6.1.1.4">𝑠</ci><ci id="S2.p2.6.m6.1.1.5.cmml" xref="S2.p2.6.m6.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S2.p2.6.m6.1d">2 italic_b italic_s italic_h</annotation></semantics></math>.
For a more detailed analysis of memory efficiency, please refer to the discussion provided therein.
In summary, the state-of-the-art transformer layer’s memory cost of activation is <math id="S2.p2.7.m7.1" class="ltx_Math" alttext="2bsh" display="inline"><semantics id="S2.p2.7.m7.1a"><mrow id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml"><mn id="S2.p2.7.m7.1.1.2" xref="S2.p2.7.m7.1.1.2.cmml">2</mn><mo id="S2.p2.7.m7.1.1.1" xref="S2.p2.7.m7.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.7.m7.1.1.3" xref="S2.p2.7.m7.1.1.3.cmml">b</mi><mo id="S2.p2.7.m7.1.1.1a" xref="S2.p2.7.m7.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.7.m7.1.1.4" xref="S2.p2.7.m7.1.1.4.cmml">s</mi><mo id="S2.p2.7.m7.1.1.1b" xref="S2.p2.7.m7.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.p2.7.m7.1.1.5" xref="S2.p2.7.m7.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><apply id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1"><times id="S2.p2.7.m7.1.1.1.cmml" xref="S2.p2.7.m7.1.1.1"></times><cn type="integer" id="S2.p2.7.m7.1.1.2.cmml" xref="S2.p2.7.m7.1.1.2">2</cn><ci id="S2.p2.7.m7.1.1.3.cmml" xref="S2.p2.7.m7.1.1.3">𝑏</ci><ci id="S2.p2.7.m7.1.1.4.cmml" xref="S2.p2.7.m7.1.1.4">𝑠</ci><ci id="S2.p2.7.m7.1.1.5.cmml" xref="S2.p2.7.m7.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S2.p2.7.m7.1d">2 italic_b italic_s italic_h</annotation></semantics></math>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Large Output of Each Layer.</span>
While BPT significantly reduces memory demand in Transformers, it still presents a major challenge for scaling up context length because it requires storing the output of each layer. This storage is crucial due to the inherent nature of self-attention, which involves interactions among all elements (n to n interactions). Without these stored outputs, the subsequent layer’s self-attention becomes computationally impractical, necessitating recomputation for each sequence element. To put it simply, processing 100 million tokens with a batch size of 1 requires over 1000GB of memory even for a modest model with a hidden size of 1024. In contrast, modern GPUs and TPUs typically provide less than 100GB of high-bandwidth memory (HBM), and the prospects for significant HBM expansion are hindered by physical limitations and high manufacturing costs.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Ring Attention</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our primary objective is to eliminates the memory constraints imposed by individual devices by efficiently distribute long sequences across multiple hosts without adding overhead. To achieve this goal, we propose an enhancement to the blockwise parallel transformers (BPT) framework&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2310.01889/assets/figures/merged.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.3.1" class="ltx_text ltx_font_bold">Top (a):</span> We use the same model architecture as the original Transformer but reorganize the compute.
In the diagram, we explain this by showing that in a ring of hosts, each host holds one query block, and key-value blocks traverse through a ring of hosts for attention and feedforward computations in a block-by-block fashion.
As we compute attention, each host sends key-value blocks to the next host while receives key-value blocks from the preceding host. The communication is overlapped with the computation of blockwise attention and feedforward.
<span id="S3.F2.4.2" class="ltx_text ltx_font_bold">Bottom (b):</span> We compute the original Transformer block-by-block.
Each host is responsible for one iteration of the query’s outer loop, while the key-value blocks rotate among the hosts. As visualized, a device starts with the first query block on the left; then we iterate over the key-value blocks sequence positioned horizontally.
The query block, combined with the key-value blocks, are used to compute self-attention (yellow box), whose output is pass to feedforward network (cyan box).
</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">When distributing an input sequence across different hosts, each host is responsible for running one element of the outer loop of blockwise attention corresponding to its designated block, as well as the feedforward network specific to that block. These operations do not necessitate communication with other hosts. However, a challenge arises in the inner loop, which involves key-value block interactions that require fetching blocks from other hosts. Since each host possesses only one key-value block, the naive approach of fetching blocks from other hosts results in two significant issues. Firstly, it introduces a computation delay as the system waits to receive the necessary key-value blocks. Secondly, the accumulation of key-value blocks leads to increased memory usage, which defeats the purpose of reducing memory cost.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.6" class="ltx_p"><span id="S3.p3.6.1" class="ltx_text ltx_font_bold">Ring-Based Blockwise Attention.</span> To tackle the aforementioned challenges, we leverage the permutation invariance property of the inner loop’s key-value block operations. This property stems from the fact that the self-attention between a query block and a group of key-value blocks can be computed in any order, as long as the statistics of each block are combined correctly for rescaling.
We leverage this property by conceptualizing all hosts as forming a ring structure: host-<math id="S3.p3.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.p3.1.m1.1a"><mn id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><cn type="integer" id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">1</annotation></semantics></math>, host-<math id="S3.p3.2.m2.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.p3.2.m2.1a"><mn id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><cn type="integer" id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">2</annotation></semantics></math>, …, host-<math id="S3.p3.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_N</annotation></semantics></math>. As we compute blockwise attention and feedforward, each host efficiently coordinates by concurrently sending key-value blocks being used for attention computation to the next host while receiving key-value blocks from the preceding host, effectively overlapping transferring of blocks with blockwise computation.
Concretely, for any host-<math id="S3.p3.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">italic_i</annotation></semantics></math>, during the computation of attention between its query block and a key-value block,
it concurrently sends key-value blocks to the next host-<math id="S3.p3.5.m5.1" class="ltx_Math" alttext="(i+1)" display="inline"><semantics id="S3.p3.5.m5.1a"><mrow id="S3.p3.5.m5.1.1.1" xref="S3.p3.5.m5.1.1.1.1.cmml"><mo stretchy="false" id="S3.p3.5.m5.1.1.1.2" xref="S3.p3.5.m5.1.1.1.1.cmml">(</mo><mrow id="S3.p3.5.m5.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.cmml"><mi id="S3.p3.5.m5.1.1.1.1.2" xref="S3.p3.5.m5.1.1.1.1.2.cmml">i</mi><mo id="S3.p3.5.m5.1.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.1.cmml">+</mo><mn id="S3.p3.5.m5.1.1.1.1.3" xref="S3.p3.5.m5.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.p3.5.m5.1.1.1.3" xref="S3.p3.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><apply id="S3.p3.5.m5.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1"><plus id="S3.p3.5.m5.1.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1.1.1"></plus><ci id="S3.p3.5.m5.1.1.1.1.2.cmml" xref="S3.p3.5.m5.1.1.1.1.2">𝑖</ci><cn type="integer" id="S3.p3.5.m5.1.1.1.1.3.cmml" xref="S3.p3.5.m5.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">(i+1)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m5.1d">( italic_i + 1 )</annotation></semantics></math> while receiving key-value blocks from the preceding host-<math id="S3.p3.6.m6.1" class="ltx_Math" alttext="(i-1)" display="inline"><semantics id="S3.p3.6.m6.1a"><mrow id="S3.p3.6.m6.1.1.1" xref="S3.p3.6.m6.1.1.1.1.cmml"><mo stretchy="false" id="S3.p3.6.m6.1.1.1.2" xref="S3.p3.6.m6.1.1.1.1.cmml">(</mo><mrow id="S3.p3.6.m6.1.1.1.1" xref="S3.p3.6.m6.1.1.1.1.cmml"><mi id="S3.p3.6.m6.1.1.1.1.2" xref="S3.p3.6.m6.1.1.1.1.2.cmml">i</mi><mo id="S3.p3.6.m6.1.1.1.1.1" xref="S3.p3.6.m6.1.1.1.1.1.cmml">−</mo><mn id="S3.p3.6.m6.1.1.1.1.3" xref="S3.p3.6.m6.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.p3.6.m6.1.1.1.3" xref="S3.p3.6.m6.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><apply id="S3.p3.6.m6.1.1.1.1.cmml" xref="S3.p3.6.m6.1.1.1"><minus id="S3.p3.6.m6.1.1.1.1.1.cmml" xref="S3.p3.6.m6.1.1.1.1.1"></minus><ci id="S3.p3.6.m6.1.1.1.1.2.cmml" xref="S3.p3.6.m6.1.1.1.1.2">𝑖</ci><cn type="integer" id="S3.p3.6.m6.1.1.1.1.3.cmml" xref="S3.p3.6.m6.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">(i-1)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.6.m6.1d">( italic_i - 1 )</annotation></semantics></math>.
If the computation time exceeds the time required for transferring key-value blocks, this results in no additional communication cost.
This overlapping mechanism applies to both forward and backward passes of our approach since the same operations and techniques can be used.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of maximum activation sizes among different Transformer architectures.
Here, <math id="S3.T1.8.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.T1.8.m1.1b"><mi id="S3.T1.8.m1.1.1" xref="S3.T1.8.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.T1.8.m1.1c"><ci id="S3.T1.8.m1.1.1.cmml" xref="S3.T1.8.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.m1.1d">b</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.m1.1e">italic_b</annotation></semantics></math> is batch size, <math id="S3.T1.9.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.T1.9.m2.1b"><mi id="S3.T1.9.m2.1.1" xref="S3.T1.9.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.T1.9.m2.1c"><ci id="S3.T1.9.m2.1.1.cmml" xref="S3.T1.9.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.m2.1d">h</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.m2.1e">italic_h</annotation></semantics></math> is hidden dimension, <math id="S3.T1.10.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.T1.10.m3.1b"><mi id="S3.T1.10.m3.1.1" xref="S3.T1.10.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.T1.10.m3.1c"><ci id="S3.T1.10.m3.1.1.cmml" xref="S3.T1.10.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.m3.1d">n</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.m3.1e">italic_n</annotation></semantics></math> is number of head, <math id="S3.T1.11.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.T1.11.m4.1b"><mi id="S3.T1.11.m4.1.1" xref="S3.T1.11.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.T1.11.m4.1c"><ci id="S3.T1.11.m4.1.1.cmml" xref="S3.T1.11.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.m4.1d">s</annotation><annotation encoding="application/x-llamapun" id="S3.T1.11.m4.1e">italic_s</annotation></semantics></math> is sequence length, <math id="S3.T1.12.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.T1.12.m5.1b"><mi id="S3.T1.12.m5.1.1" xref="S3.T1.12.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.T1.12.m5.1c"><ci id="S3.T1.12.m5.1.1.cmml" xref="S3.T1.12.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.m5.1d">c</annotation><annotation encoding="application/x-llamapun" id="S3.T1.12.m5.1e">italic_c</annotation></semantics></math> is block size, the block size (<math id="S3.T1.13.m6.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.T1.13.m6.1b"><mi id="S3.T1.13.m6.1.1" xref="S3.T1.13.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.T1.13.m6.1c"><ci id="S3.T1.13.m6.1.1.cmml" xref="S3.T1.13.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.m6.1d">c</annotation><annotation encoding="application/x-llamapun" id="S3.T1.13.m6.1e">italic_c</annotation></semantics></math>) is independent of the input sequence length (<math id="S3.T1.14.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.T1.14.m7.1b"><mi id="S3.T1.14.m7.1.1" xref="S3.T1.14.m7.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.T1.14.m7.1c"><ci id="S3.T1.14.m7.1.1.cmml" xref="S3.T1.14.m7.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.m7.1d">s</annotation><annotation encoding="application/x-llamapun" id="S3.T1.14.m7.1e">italic_s</annotation></semantics></math>).
The comparison is between vanilla Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite>, memory efficient attention&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, memory efficient attention and feedforward&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>, and our proposed approach Ring Attention.
Numbers are shown in bytes per layer, assuming <span id="S3.T1.28.1" class="ltx_text ltx_font_italic">bfloat16</span> precision.
</figcaption>
<div id="S3.T1.26" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:266.8pt;height:113.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.1pt,0.1pt) scale(0.999,0.999) ;">
<table id="S3.T1.26.12" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T1.26.12.13" class="ltx_tr">
<td id="S3.T1.26.12.13.1" class="ltx_td ltx_align_left ltx_border_tt">Layer Type</td>
<td id="S3.T1.26.12.13.2" class="ltx_td ltx_align_right ltx_border_tt">Self-Attention</td>
<td id="S3.T1.26.12.13.3" class="ltx_td ltx_align_right ltx_border_tt">FeedForward</td>
<td id="S3.T1.26.12.13.4" class="ltx_td ltx_align_right ltx_border_tt">Total</td>
</tr>
<tr id="S3.T1.17.3.3" class="ltx_tr">
<td id="S3.T1.17.3.3.4" class="ltx_td ltx_align_left ltx_border_t">Vanilla</td>
<td id="S3.T1.15.1.1.1" class="ltx_td ltx_align_right ltx_border_t"><math id="S3.T1.15.1.1.1.m1.1" class="ltx_Math" alttext="2bns^{2}" display="inline"><semantics id="S3.T1.15.1.1.1.m1.1a"><mrow id="S3.T1.15.1.1.1.m1.1.1" xref="S3.T1.15.1.1.1.m1.1.1.cmml"><mn id="S3.T1.15.1.1.1.m1.1.1.2" xref="S3.T1.15.1.1.1.m1.1.1.2.cmml">2</mn><mo id="S3.T1.15.1.1.1.m1.1.1.1" xref="S3.T1.15.1.1.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.15.1.1.1.m1.1.1.3" xref="S3.T1.15.1.1.1.m1.1.1.3.cmml">b</mi><mo id="S3.T1.15.1.1.1.m1.1.1.1a" xref="S3.T1.15.1.1.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.15.1.1.1.m1.1.1.4" xref="S3.T1.15.1.1.1.m1.1.1.4.cmml">n</mi><mo id="S3.T1.15.1.1.1.m1.1.1.1b" xref="S3.T1.15.1.1.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S3.T1.15.1.1.1.m1.1.1.5" xref="S3.T1.15.1.1.1.m1.1.1.5.cmml"><mi id="S3.T1.15.1.1.1.m1.1.1.5.2" xref="S3.T1.15.1.1.1.m1.1.1.5.2.cmml">s</mi><mn id="S3.T1.15.1.1.1.m1.1.1.5.3" xref="S3.T1.15.1.1.1.m1.1.1.5.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.15.1.1.1.m1.1b"><apply id="S3.T1.15.1.1.1.m1.1.1.cmml" xref="S3.T1.15.1.1.1.m1.1.1"><times id="S3.T1.15.1.1.1.m1.1.1.1.cmml" xref="S3.T1.15.1.1.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.15.1.1.1.m1.1.1.2.cmml" xref="S3.T1.15.1.1.1.m1.1.1.2">2</cn><ci id="S3.T1.15.1.1.1.m1.1.1.3.cmml" xref="S3.T1.15.1.1.1.m1.1.1.3">𝑏</ci><ci id="S3.T1.15.1.1.1.m1.1.1.4.cmml" xref="S3.T1.15.1.1.1.m1.1.1.4">𝑛</ci><apply id="S3.T1.15.1.1.1.m1.1.1.5.cmml" xref="S3.T1.15.1.1.1.m1.1.1.5"><csymbol cd="ambiguous" id="S3.T1.15.1.1.1.m1.1.1.5.1.cmml" xref="S3.T1.15.1.1.1.m1.1.1.5">superscript</csymbol><ci id="S3.T1.15.1.1.1.m1.1.1.5.2.cmml" xref="S3.T1.15.1.1.1.m1.1.1.5.2">𝑠</ci><cn type="integer" id="S3.T1.15.1.1.1.m1.1.1.5.3.cmml" xref="S3.T1.15.1.1.1.m1.1.1.5.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.1.1.1.m1.1c">2bns^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.15.1.1.1.m1.1d">2 italic_b italic_n italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td id="S3.T1.16.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><math id="S3.T1.16.2.2.2.m1.1" class="ltx_Math" alttext="8bsh" display="inline"><semantics id="S3.T1.16.2.2.2.m1.1a"><mrow id="S3.T1.16.2.2.2.m1.1.1" xref="S3.T1.16.2.2.2.m1.1.1.cmml"><mn id="S3.T1.16.2.2.2.m1.1.1.2" xref="S3.T1.16.2.2.2.m1.1.1.2.cmml">8</mn><mo id="S3.T1.16.2.2.2.m1.1.1.1" xref="S3.T1.16.2.2.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.16.2.2.2.m1.1.1.3" xref="S3.T1.16.2.2.2.m1.1.1.3.cmml">b</mi><mo id="S3.T1.16.2.2.2.m1.1.1.1a" xref="S3.T1.16.2.2.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.16.2.2.2.m1.1.1.4" xref="S3.T1.16.2.2.2.m1.1.1.4.cmml">s</mi><mo id="S3.T1.16.2.2.2.m1.1.1.1b" xref="S3.T1.16.2.2.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.16.2.2.2.m1.1.1.5" xref="S3.T1.16.2.2.2.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.16.2.2.2.m1.1b"><apply id="S3.T1.16.2.2.2.m1.1.1.cmml" xref="S3.T1.16.2.2.2.m1.1.1"><times id="S3.T1.16.2.2.2.m1.1.1.1.cmml" xref="S3.T1.16.2.2.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.16.2.2.2.m1.1.1.2.cmml" xref="S3.T1.16.2.2.2.m1.1.1.2">8</cn><ci id="S3.T1.16.2.2.2.m1.1.1.3.cmml" xref="S3.T1.16.2.2.2.m1.1.1.3">𝑏</ci><ci id="S3.T1.16.2.2.2.m1.1.1.4.cmml" xref="S3.T1.16.2.2.2.m1.1.1.4">𝑠</ci><ci id="S3.T1.16.2.2.2.m1.1.1.5.cmml" xref="S3.T1.16.2.2.2.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.2.2.2.m1.1c">8bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.16.2.2.2.m1.1d">8 italic_b italic_s italic_h</annotation></semantics></math></td>
<td id="S3.T1.17.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><math id="S3.T1.17.3.3.3.m1.1" class="ltx_Math" alttext="2bhs^{2}" display="inline"><semantics id="S3.T1.17.3.3.3.m1.1a"><mrow id="S3.T1.17.3.3.3.m1.1.1" xref="S3.T1.17.3.3.3.m1.1.1.cmml"><mn id="S3.T1.17.3.3.3.m1.1.1.2" xref="S3.T1.17.3.3.3.m1.1.1.2.cmml">2</mn><mo id="S3.T1.17.3.3.3.m1.1.1.1" xref="S3.T1.17.3.3.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.17.3.3.3.m1.1.1.3" xref="S3.T1.17.3.3.3.m1.1.1.3.cmml">b</mi><mo id="S3.T1.17.3.3.3.m1.1.1.1a" xref="S3.T1.17.3.3.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.17.3.3.3.m1.1.1.4" xref="S3.T1.17.3.3.3.m1.1.1.4.cmml">h</mi><mo id="S3.T1.17.3.3.3.m1.1.1.1b" xref="S3.T1.17.3.3.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S3.T1.17.3.3.3.m1.1.1.5" xref="S3.T1.17.3.3.3.m1.1.1.5.cmml"><mi id="S3.T1.17.3.3.3.m1.1.1.5.2" xref="S3.T1.17.3.3.3.m1.1.1.5.2.cmml">s</mi><mn id="S3.T1.17.3.3.3.m1.1.1.5.3" xref="S3.T1.17.3.3.3.m1.1.1.5.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.17.3.3.3.m1.1b"><apply id="S3.T1.17.3.3.3.m1.1.1.cmml" xref="S3.T1.17.3.3.3.m1.1.1"><times id="S3.T1.17.3.3.3.m1.1.1.1.cmml" xref="S3.T1.17.3.3.3.m1.1.1.1"></times><cn type="integer" id="S3.T1.17.3.3.3.m1.1.1.2.cmml" xref="S3.T1.17.3.3.3.m1.1.1.2">2</cn><ci id="S3.T1.17.3.3.3.m1.1.1.3.cmml" xref="S3.T1.17.3.3.3.m1.1.1.3">𝑏</ci><ci id="S3.T1.17.3.3.3.m1.1.1.4.cmml" xref="S3.T1.17.3.3.3.m1.1.1.4">ℎ</ci><apply id="S3.T1.17.3.3.3.m1.1.1.5.cmml" xref="S3.T1.17.3.3.3.m1.1.1.5"><csymbol cd="ambiguous" id="S3.T1.17.3.3.3.m1.1.1.5.1.cmml" xref="S3.T1.17.3.3.3.m1.1.1.5">superscript</csymbol><ci id="S3.T1.17.3.3.3.m1.1.1.5.2.cmml" xref="S3.T1.17.3.3.3.m1.1.1.5.2">𝑠</ci><cn type="integer" id="S3.T1.17.3.3.3.m1.1.1.5.3.cmml" xref="S3.T1.17.3.3.3.m1.1.1.5.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.3.3.3.m1.1c">2bhs^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.17.3.3.3.m1.1d">2 italic_b italic_h italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.20.6.6" class="ltx_tr">
<td id="S3.T1.20.6.6.4" class="ltx_td ltx_align_left">Memory efficient attention</td>
<td id="S3.T1.18.4.4.1" class="ltx_td ltx_align_right"><math id="S3.T1.18.4.4.1.m1.1" class="ltx_Math" alttext="2bsh+4bch" display="inline"><semantics id="S3.T1.18.4.4.1.m1.1a"><mrow id="S3.T1.18.4.4.1.m1.1.1" xref="S3.T1.18.4.4.1.m1.1.1.cmml"><mrow id="S3.T1.18.4.4.1.m1.1.1.2" xref="S3.T1.18.4.4.1.m1.1.1.2.cmml"><mn id="S3.T1.18.4.4.1.m1.1.1.2.2" xref="S3.T1.18.4.4.1.m1.1.1.2.2.cmml">2</mn><mo id="S3.T1.18.4.4.1.m1.1.1.2.1" xref="S3.T1.18.4.4.1.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.2.3" xref="S3.T1.18.4.4.1.m1.1.1.2.3.cmml">b</mi><mo id="S3.T1.18.4.4.1.m1.1.1.2.1a" xref="S3.T1.18.4.4.1.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.2.4" xref="S3.T1.18.4.4.1.m1.1.1.2.4.cmml">s</mi><mo id="S3.T1.18.4.4.1.m1.1.1.2.1b" xref="S3.T1.18.4.4.1.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.2.5" xref="S3.T1.18.4.4.1.m1.1.1.2.5.cmml">h</mi></mrow><mo id="S3.T1.18.4.4.1.m1.1.1.1" xref="S3.T1.18.4.4.1.m1.1.1.1.cmml">+</mo><mrow id="S3.T1.18.4.4.1.m1.1.1.3" xref="S3.T1.18.4.4.1.m1.1.1.3.cmml"><mn id="S3.T1.18.4.4.1.m1.1.1.3.2" xref="S3.T1.18.4.4.1.m1.1.1.3.2.cmml">4</mn><mo id="S3.T1.18.4.4.1.m1.1.1.3.1" xref="S3.T1.18.4.4.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.3.3" xref="S3.T1.18.4.4.1.m1.1.1.3.3.cmml">b</mi><mo id="S3.T1.18.4.4.1.m1.1.1.3.1a" xref="S3.T1.18.4.4.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.3.4" xref="S3.T1.18.4.4.1.m1.1.1.3.4.cmml">c</mi><mo id="S3.T1.18.4.4.1.m1.1.1.3.1b" xref="S3.T1.18.4.4.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.3.5" xref="S3.T1.18.4.4.1.m1.1.1.3.5.cmml">h</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.18.4.4.1.m1.1b"><apply id="S3.T1.18.4.4.1.m1.1.1.cmml" xref="S3.T1.18.4.4.1.m1.1.1"><plus id="S3.T1.18.4.4.1.m1.1.1.1.cmml" xref="S3.T1.18.4.4.1.m1.1.1.1"></plus><apply id="S3.T1.18.4.4.1.m1.1.1.2.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2"><times id="S3.T1.18.4.4.1.m1.1.1.2.1.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.1"></times><cn type="integer" id="S3.T1.18.4.4.1.m1.1.1.2.2.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.2">2</cn><ci id="S3.T1.18.4.4.1.m1.1.1.2.3.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.3">𝑏</ci><ci id="S3.T1.18.4.4.1.m1.1.1.2.4.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.4">𝑠</ci><ci id="S3.T1.18.4.4.1.m1.1.1.2.5.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.5">ℎ</ci></apply><apply id="S3.T1.18.4.4.1.m1.1.1.3.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3"><times id="S3.T1.18.4.4.1.m1.1.1.3.1.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.1"></times><cn type="integer" id="S3.T1.18.4.4.1.m1.1.1.3.2.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.2">4</cn><ci id="S3.T1.18.4.4.1.m1.1.1.3.3.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.3">𝑏</ci><ci id="S3.T1.18.4.4.1.m1.1.1.3.4.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.4">𝑐</ci><ci id="S3.T1.18.4.4.1.m1.1.1.3.5.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.5">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.18.4.4.1.m1.1c">2bsh+4bch</annotation><annotation encoding="application/x-llamapun" id="S3.T1.18.4.4.1.m1.1d">2 italic_b italic_s italic_h + 4 italic_b italic_c italic_h</annotation></semantics></math></td>
<td id="S3.T1.19.5.5.2" class="ltx_td ltx_align_right"><math id="S3.T1.19.5.5.2.m1.1" class="ltx_Math" alttext="8bsh" display="inline"><semantics id="S3.T1.19.5.5.2.m1.1a"><mrow id="S3.T1.19.5.5.2.m1.1.1" xref="S3.T1.19.5.5.2.m1.1.1.cmml"><mn id="S3.T1.19.5.5.2.m1.1.1.2" xref="S3.T1.19.5.5.2.m1.1.1.2.cmml">8</mn><mo id="S3.T1.19.5.5.2.m1.1.1.1" xref="S3.T1.19.5.5.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.19.5.5.2.m1.1.1.3" xref="S3.T1.19.5.5.2.m1.1.1.3.cmml">b</mi><mo id="S3.T1.19.5.5.2.m1.1.1.1a" xref="S3.T1.19.5.5.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.19.5.5.2.m1.1.1.4" xref="S3.T1.19.5.5.2.m1.1.1.4.cmml">s</mi><mo id="S3.T1.19.5.5.2.m1.1.1.1b" xref="S3.T1.19.5.5.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.19.5.5.2.m1.1.1.5" xref="S3.T1.19.5.5.2.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.19.5.5.2.m1.1b"><apply id="S3.T1.19.5.5.2.m1.1.1.cmml" xref="S3.T1.19.5.5.2.m1.1.1"><times id="S3.T1.19.5.5.2.m1.1.1.1.cmml" xref="S3.T1.19.5.5.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.19.5.5.2.m1.1.1.2.cmml" xref="S3.T1.19.5.5.2.m1.1.1.2">8</cn><ci id="S3.T1.19.5.5.2.m1.1.1.3.cmml" xref="S3.T1.19.5.5.2.m1.1.1.3">𝑏</ci><ci id="S3.T1.19.5.5.2.m1.1.1.4.cmml" xref="S3.T1.19.5.5.2.m1.1.1.4">𝑠</ci><ci id="S3.T1.19.5.5.2.m1.1.1.5.cmml" xref="S3.T1.19.5.5.2.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.19.5.5.2.m1.1c">8bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.19.5.5.2.m1.1d">8 italic_b italic_s italic_h</annotation></semantics></math></td>
<td id="S3.T1.20.6.6.3" class="ltx_td ltx_align_right"><math id="S3.T1.20.6.6.3.m1.1" class="ltx_Math" alttext="8bsh" display="inline"><semantics id="S3.T1.20.6.6.3.m1.1a"><mrow id="S3.T1.20.6.6.3.m1.1.1" xref="S3.T1.20.6.6.3.m1.1.1.cmml"><mn id="S3.T1.20.6.6.3.m1.1.1.2" xref="S3.T1.20.6.6.3.m1.1.1.2.cmml">8</mn><mo id="S3.T1.20.6.6.3.m1.1.1.1" xref="S3.T1.20.6.6.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.20.6.6.3.m1.1.1.3" xref="S3.T1.20.6.6.3.m1.1.1.3.cmml">b</mi><mo id="S3.T1.20.6.6.3.m1.1.1.1a" xref="S3.T1.20.6.6.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.20.6.6.3.m1.1.1.4" xref="S3.T1.20.6.6.3.m1.1.1.4.cmml">s</mi><mo id="S3.T1.20.6.6.3.m1.1.1.1b" xref="S3.T1.20.6.6.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.20.6.6.3.m1.1.1.5" xref="S3.T1.20.6.6.3.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.20.6.6.3.m1.1b"><apply id="S3.T1.20.6.6.3.m1.1.1.cmml" xref="S3.T1.20.6.6.3.m1.1.1"><times id="S3.T1.20.6.6.3.m1.1.1.1.cmml" xref="S3.T1.20.6.6.3.m1.1.1.1"></times><cn type="integer" id="S3.T1.20.6.6.3.m1.1.1.2.cmml" xref="S3.T1.20.6.6.3.m1.1.1.2">8</cn><ci id="S3.T1.20.6.6.3.m1.1.1.3.cmml" xref="S3.T1.20.6.6.3.m1.1.1.3">𝑏</ci><ci id="S3.T1.20.6.6.3.m1.1.1.4.cmml" xref="S3.T1.20.6.6.3.m1.1.1.4">𝑠</ci><ci id="S3.T1.20.6.6.3.m1.1.1.5.cmml" xref="S3.T1.20.6.6.3.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.20.6.6.3.m1.1c">8bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.20.6.6.3.m1.1d">8 italic_b italic_s italic_h</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.23.9.9" class="ltx_tr">
<td id="S3.T1.23.9.9.4" class="ltx_td ltx_align_left">
<span id="S3.T1.23.9.9.4.1" class="ltx_text"></span><span id="S3.T1.23.9.9.4.2" class="ltx_text">
<span id="S3.T1.23.9.9.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.23.9.9.4.2.1.1" class="ltx_tr">
<span id="S3.T1.23.9.9.4.2.1.1.1" class="ltx_td ltx_align_left">Memory efficient attention</span></span>
<span id="S3.T1.23.9.9.4.2.1.2" class="ltx_tr">
<span id="S3.T1.23.9.9.4.2.1.2.1" class="ltx_td ltx_align_left">and feedforward</span></span>
</span></span> <span id="S3.T1.23.9.9.4.3" class="ltx_text"></span>
</td>
<td id="S3.T1.21.7.7.1" class="ltx_td ltx_align_right"><math id="S3.T1.21.7.7.1.m1.1" class="ltx_Math" alttext="2bsh" display="inline"><semantics id="S3.T1.21.7.7.1.m1.1a"><mrow id="S3.T1.21.7.7.1.m1.1.1" xref="S3.T1.21.7.7.1.m1.1.1.cmml"><mn id="S3.T1.21.7.7.1.m1.1.1.2" xref="S3.T1.21.7.7.1.m1.1.1.2.cmml">2</mn><mo id="S3.T1.21.7.7.1.m1.1.1.1" xref="S3.T1.21.7.7.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.21.7.7.1.m1.1.1.3" xref="S3.T1.21.7.7.1.m1.1.1.3.cmml">b</mi><mo id="S3.T1.21.7.7.1.m1.1.1.1a" xref="S3.T1.21.7.7.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.21.7.7.1.m1.1.1.4" xref="S3.T1.21.7.7.1.m1.1.1.4.cmml">s</mi><mo id="S3.T1.21.7.7.1.m1.1.1.1b" xref="S3.T1.21.7.7.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.21.7.7.1.m1.1.1.5" xref="S3.T1.21.7.7.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.21.7.7.1.m1.1b"><apply id="S3.T1.21.7.7.1.m1.1.1.cmml" xref="S3.T1.21.7.7.1.m1.1.1"><times id="S3.T1.21.7.7.1.m1.1.1.1.cmml" xref="S3.T1.21.7.7.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.21.7.7.1.m1.1.1.2.cmml" xref="S3.T1.21.7.7.1.m1.1.1.2">2</cn><ci id="S3.T1.21.7.7.1.m1.1.1.3.cmml" xref="S3.T1.21.7.7.1.m1.1.1.3">𝑏</ci><ci id="S3.T1.21.7.7.1.m1.1.1.4.cmml" xref="S3.T1.21.7.7.1.m1.1.1.4">𝑠</ci><ci id="S3.T1.21.7.7.1.m1.1.1.5.cmml" xref="S3.T1.21.7.7.1.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.21.7.7.1.m1.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.21.7.7.1.m1.1d">2 italic_b italic_s italic_h</annotation></semantics></math></td>
<td id="S3.T1.22.8.8.2" class="ltx_td ltx_align_right"><math id="S3.T1.22.8.8.2.m1.1" class="ltx_Math" alttext="2bsh" display="inline"><semantics id="S3.T1.22.8.8.2.m1.1a"><mrow id="S3.T1.22.8.8.2.m1.1.1" xref="S3.T1.22.8.8.2.m1.1.1.cmml"><mn id="S3.T1.22.8.8.2.m1.1.1.2" xref="S3.T1.22.8.8.2.m1.1.1.2.cmml">2</mn><mo id="S3.T1.22.8.8.2.m1.1.1.1" xref="S3.T1.22.8.8.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.22.8.8.2.m1.1.1.3" xref="S3.T1.22.8.8.2.m1.1.1.3.cmml">b</mi><mo id="S3.T1.22.8.8.2.m1.1.1.1a" xref="S3.T1.22.8.8.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.22.8.8.2.m1.1.1.4" xref="S3.T1.22.8.8.2.m1.1.1.4.cmml">s</mi><mo id="S3.T1.22.8.8.2.m1.1.1.1b" xref="S3.T1.22.8.8.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.22.8.8.2.m1.1.1.5" xref="S3.T1.22.8.8.2.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.22.8.8.2.m1.1b"><apply id="S3.T1.22.8.8.2.m1.1.1.cmml" xref="S3.T1.22.8.8.2.m1.1.1"><times id="S3.T1.22.8.8.2.m1.1.1.1.cmml" xref="S3.T1.22.8.8.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.22.8.8.2.m1.1.1.2.cmml" xref="S3.T1.22.8.8.2.m1.1.1.2">2</cn><ci id="S3.T1.22.8.8.2.m1.1.1.3.cmml" xref="S3.T1.22.8.8.2.m1.1.1.3">𝑏</ci><ci id="S3.T1.22.8.8.2.m1.1.1.4.cmml" xref="S3.T1.22.8.8.2.m1.1.1.4">𝑠</ci><ci id="S3.T1.22.8.8.2.m1.1.1.5.cmml" xref="S3.T1.22.8.8.2.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.22.8.8.2.m1.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.22.8.8.2.m1.1d">2 italic_b italic_s italic_h</annotation></semantics></math></td>
<td id="S3.T1.23.9.9.3" class="ltx_td ltx_align_right"><math id="S3.T1.23.9.9.3.m1.1" class="ltx_Math" alttext="2bsh" display="inline"><semantics id="S3.T1.23.9.9.3.m1.1a"><mrow id="S3.T1.23.9.9.3.m1.1.1" xref="S3.T1.23.9.9.3.m1.1.1.cmml"><mn id="S3.T1.23.9.9.3.m1.1.1.2" xref="S3.T1.23.9.9.3.m1.1.1.2.cmml">2</mn><mo id="S3.T1.23.9.9.3.m1.1.1.1" xref="S3.T1.23.9.9.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.23.9.9.3.m1.1.1.3" xref="S3.T1.23.9.9.3.m1.1.1.3.cmml">b</mi><mo id="S3.T1.23.9.9.3.m1.1.1.1a" xref="S3.T1.23.9.9.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.23.9.9.3.m1.1.1.4" xref="S3.T1.23.9.9.3.m1.1.1.4.cmml">s</mi><mo id="S3.T1.23.9.9.3.m1.1.1.1b" xref="S3.T1.23.9.9.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.23.9.9.3.m1.1.1.5" xref="S3.T1.23.9.9.3.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.23.9.9.3.m1.1b"><apply id="S3.T1.23.9.9.3.m1.1.1.cmml" xref="S3.T1.23.9.9.3.m1.1.1"><times id="S3.T1.23.9.9.3.m1.1.1.1.cmml" xref="S3.T1.23.9.9.3.m1.1.1.1"></times><cn type="integer" id="S3.T1.23.9.9.3.m1.1.1.2.cmml" xref="S3.T1.23.9.9.3.m1.1.1.2">2</cn><ci id="S3.T1.23.9.9.3.m1.1.1.3.cmml" xref="S3.T1.23.9.9.3.m1.1.1.3">𝑏</ci><ci id="S3.T1.23.9.9.3.m1.1.1.4.cmml" xref="S3.T1.23.9.9.3.m1.1.1.4">𝑠</ci><ci id="S3.T1.23.9.9.3.m1.1.1.5.cmml" xref="S3.T1.23.9.9.3.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.23.9.9.3.m1.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.23.9.9.3.m1.1d">2 italic_b italic_s italic_h</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.26.12.12" class="ltx_tr">
<td id="S3.T1.26.12.12.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Ring Attention</td>
<td id="S3.T1.24.10.10.1" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><math id="S3.T1.24.10.10.1.m1.1" class="ltx_Math" alttext="6bch" display="inline"><semantics id="S3.T1.24.10.10.1.m1.1a"><mrow id="S3.T1.24.10.10.1.m1.1.1" xref="S3.T1.24.10.10.1.m1.1.1.cmml"><mn id="S3.T1.24.10.10.1.m1.1.1.2" xref="S3.T1.24.10.10.1.m1.1.1.2.cmml">6</mn><mo id="S3.T1.24.10.10.1.m1.1.1.1" xref="S3.T1.24.10.10.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.24.10.10.1.m1.1.1.3" xref="S3.T1.24.10.10.1.m1.1.1.3.cmml">b</mi><mo id="S3.T1.24.10.10.1.m1.1.1.1a" xref="S3.T1.24.10.10.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.24.10.10.1.m1.1.1.4" xref="S3.T1.24.10.10.1.m1.1.1.4.cmml">c</mi><mo id="S3.T1.24.10.10.1.m1.1.1.1b" xref="S3.T1.24.10.10.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.24.10.10.1.m1.1.1.5" xref="S3.T1.24.10.10.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.24.10.10.1.m1.1b"><apply id="S3.T1.24.10.10.1.m1.1.1.cmml" xref="S3.T1.24.10.10.1.m1.1.1"><times id="S3.T1.24.10.10.1.m1.1.1.1.cmml" xref="S3.T1.24.10.10.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.24.10.10.1.m1.1.1.2.cmml" xref="S3.T1.24.10.10.1.m1.1.1.2">6</cn><ci id="S3.T1.24.10.10.1.m1.1.1.3.cmml" xref="S3.T1.24.10.10.1.m1.1.1.3">𝑏</ci><ci id="S3.T1.24.10.10.1.m1.1.1.4.cmml" xref="S3.T1.24.10.10.1.m1.1.1.4">𝑐</ci><ci id="S3.T1.24.10.10.1.m1.1.1.5.cmml" xref="S3.T1.24.10.10.1.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.24.10.10.1.m1.1c">6bch</annotation><annotation encoding="application/x-llamapun" id="S3.T1.24.10.10.1.m1.1d">6 italic_b italic_c italic_h</annotation></semantics></math></td>
<td id="S3.T1.25.11.11.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><math id="S3.T1.25.11.11.2.m1.1" class="ltx_Math" alttext="2bch" display="inline"><semantics id="S3.T1.25.11.11.2.m1.1a"><mrow id="S3.T1.25.11.11.2.m1.1.1" xref="S3.T1.25.11.11.2.m1.1.1.cmml"><mn id="S3.T1.25.11.11.2.m1.1.1.2" xref="S3.T1.25.11.11.2.m1.1.1.2.cmml">2</mn><mo id="S3.T1.25.11.11.2.m1.1.1.1" xref="S3.T1.25.11.11.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.25.11.11.2.m1.1.1.3" xref="S3.T1.25.11.11.2.m1.1.1.3.cmml">b</mi><mo id="S3.T1.25.11.11.2.m1.1.1.1a" xref="S3.T1.25.11.11.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.25.11.11.2.m1.1.1.4" xref="S3.T1.25.11.11.2.m1.1.1.4.cmml">c</mi><mo id="S3.T1.25.11.11.2.m1.1.1.1b" xref="S3.T1.25.11.11.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.25.11.11.2.m1.1.1.5" xref="S3.T1.25.11.11.2.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.25.11.11.2.m1.1b"><apply id="S3.T1.25.11.11.2.m1.1.1.cmml" xref="S3.T1.25.11.11.2.m1.1.1"><times id="S3.T1.25.11.11.2.m1.1.1.1.cmml" xref="S3.T1.25.11.11.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.25.11.11.2.m1.1.1.2.cmml" xref="S3.T1.25.11.11.2.m1.1.1.2">2</cn><ci id="S3.T1.25.11.11.2.m1.1.1.3.cmml" xref="S3.T1.25.11.11.2.m1.1.1.3">𝑏</ci><ci id="S3.T1.25.11.11.2.m1.1.1.4.cmml" xref="S3.T1.25.11.11.2.m1.1.1.4">𝑐</ci><ci id="S3.T1.25.11.11.2.m1.1.1.5.cmml" xref="S3.T1.25.11.11.2.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.25.11.11.2.m1.1c">2bch</annotation><annotation encoding="application/x-llamapun" id="S3.T1.25.11.11.2.m1.1d">2 italic_b italic_c italic_h</annotation></semantics></math></td>
<td id="S3.T1.26.12.12.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><math id="S3.T1.26.12.12.3.m1.1" class="ltx_Math" alttext="6bch" display="inline"><semantics id="S3.T1.26.12.12.3.m1.1a"><mrow id="S3.T1.26.12.12.3.m1.1.1" xref="S3.T1.26.12.12.3.m1.1.1.cmml"><mn id="S3.T1.26.12.12.3.m1.1.1.2" xref="S3.T1.26.12.12.3.m1.1.1.2.cmml">6</mn><mo id="S3.T1.26.12.12.3.m1.1.1.1" xref="S3.T1.26.12.12.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.26.12.12.3.m1.1.1.3" xref="S3.T1.26.12.12.3.m1.1.1.3.cmml">b</mi><mo id="S3.T1.26.12.12.3.m1.1.1.1a" xref="S3.T1.26.12.12.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.26.12.12.3.m1.1.1.4" xref="S3.T1.26.12.12.3.m1.1.1.4.cmml">c</mi><mo id="S3.T1.26.12.12.3.m1.1.1.1b" xref="S3.T1.26.12.12.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.26.12.12.3.m1.1.1.5" xref="S3.T1.26.12.12.3.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.26.12.12.3.m1.1b"><apply id="S3.T1.26.12.12.3.m1.1.1.cmml" xref="S3.T1.26.12.12.3.m1.1.1"><times id="S3.T1.26.12.12.3.m1.1.1.1.cmml" xref="S3.T1.26.12.12.3.m1.1.1.1"></times><cn type="integer" id="S3.T1.26.12.12.3.m1.1.1.2.cmml" xref="S3.T1.26.12.12.3.m1.1.1.2">6</cn><ci id="S3.T1.26.12.12.3.m1.1.1.3.cmml" xref="S3.T1.26.12.12.3.m1.1.1.3">𝑏</ci><ci id="S3.T1.26.12.12.3.m1.1.1.4.cmml" xref="S3.T1.26.12.12.3.m1.1.1.4">𝑐</ci><ci id="S3.T1.26.12.12.3.m1.1.1.5.cmml" xref="S3.T1.26.12.12.3.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.26.12.12.3.m1.1c">6bch</annotation><annotation encoding="application/x-llamapun" id="S3.T1.26.12.12.3.m1.1d">6 italic_b italic_c italic_h</annotation></semantics></math></td>
</tr>
</tbody></table>
</span></div>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.12" class="ltx_p"><span id="S3.p4.12.1" class="ltx_text ltx_font_bold">Arithmetic Intensity Between Hosts.</span>
In order to determine the minimal required block size to overlap transferring with computation, assume that each host has <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.p4.1.m1.1a"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.p4.1.m1.1d">italic_F</annotation></semantics></math> FLOPS and that the bandwidth between hosts is denoted as <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.p4.2.m2.1a"><mi id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><ci id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.p4.2.m2.1d">italic_B</annotation></semantics></math>.
It’s worth noting that our approach involves interactions only with the immediately previous and next hosts in a circular configuration, thus our analysis applies to both GPU all-to-all topology and TPU torus topology.
Let’s consider the variables: block size denoted as <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.p4.3.m3.1a"><mi id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><ci id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.p4.3.m3.1d">italic_c</annotation></semantics></math> and hidden size as <math id="S3.p4.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.p4.4.m4.1a"><mi id="S3.p4.4.m4.1.1" xref="S3.p4.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.1b"><ci id="S3.p4.4.m4.1.1.cmml" xref="S3.p4.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.p4.4.m4.1d">italic_d</annotation></semantics></math>. When computing blockwise self-attention, we require <math id="S3.p4.5.m5.1" class="ltx_Math" alttext="2dc^{2}" display="inline"><semantics id="S3.p4.5.m5.1a"><mrow id="S3.p4.5.m5.1.1" xref="S3.p4.5.m5.1.1.cmml"><mn id="S3.p4.5.m5.1.1.2" xref="S3.p4.5.m5.1.1.2.cmml">2</mn><mo id="S3.p4.5.m5.1.1.1" xref="S3.p4.5.m5.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.5.m5.1.1.3" xref="S3.p4.5.m5.1.1.3.cmml">d</mi><mo id="S3.p4.5.m5.1.1.1a" xref="S3.p4.5.m5.1.1.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S3.p4.5.m5.1.1.4" xref="S3.p4.5.m5.1.1.4.cmml"><mi id="S3.p4.5.m5.1.1.4.2" xref="S3.p4.5.m5.1.1.4.2.cmml">c</mi><mn id="S3.p4.5.m5.1.1.4.3" xref="S3.p4.5.m5.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.5.m5.1b"><apply id="S3.p4.5.m5.1.1.cmml" xref="S3.p4.5.m5.1.1"><times id="S3.p4.5.m5.1.1.1.cmml" xref="S3.p4.5.m5.1.1.1"></times><cn type="integer" id="S3.p4.5.m5.1.1.2.cmml" xref="S3.p4.5.m5.1.1.2">2</cn><ci id="S3.p4.5.m5.1.1.3.cmml" xref="S3.p4.5.m5.1.1.3">𝑑</ci><apply id="S3.p4.5.m5.1.1.4.cmml" xref="S3.p4.5.m5.1.1.4"><csymbol cd="ambiguous" id="S3.p4.5.m5.1.1.4.1.cmml" xref="S3.p4.5.m5.1.1.4">superscript</csymbol><ci id="S3.p4.5.m5.1.1.4.2.cmml" xref="S3.p4.5.m5.1.1.4.2">𝑐</ci><cn type="integer" id="S3.p4.5.m5.1.1.4.3.cmml" xref="S3.p4.5.m5.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.5.m5.1c">2dc^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.5.m5.1d">2 italic_d italic_c start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOPs for calculating attention scores using queries and keys, and an additional <math id="S3.p4.6.m6.1" class="ltx_Math" alttext="2dc^{2}" display="inline"><semantics id="S3.p4.6.m6.1a"><mrow id="S3.p4.6.m6.1.1" xref="S3.p4.6.m6.1.1.cmml"><mn id="S3.p4.6.m6.1.1.2" xref="S3.p4.6.m6.1.1.2.cmml">2</mn><mo id="S3.p4.6.m6.1.1.1" xref="S3.p4.6.m6.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.6.m6.1.1.3" xref="S3.p4.6.m6.1.1.3.cmml">d</mi><mo id="S3.p4.6.m6.1.1.1a" xref="S3.p4.6.m6.1.1.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S3.p4.6.m6.1.1.4" xref="S3.p4.6.m6.1.1.4.cmml"><mi id="S3.p4.6.m6.1.1.4.2" xref="S3.p4.6.m6.1.1.4.2.cmml">c</mi><mn id="S3.p4.6.m6.1.1.4.3" xref="S3.p4.6.m6.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.6.m6.1b"><apply id="S3.p4.6.m6.1.1.cmml" xref="S3.p4.6.m6.1.1"><times id="S3.p4.6.m6.1.1.1.cmml" xref="S3.p4.6.m6.1.1.1"></times><cn type="integer" id="S3.p4.6.m6.1.1.2.cmml" xref="S3.p4.6.m6.1.1.2">2</cn><ci id="S3.p4.6.m6.1.1.3.cmml" xref="S3.p4.6.m6.1.1.3">𝑑</ci><apply id="S3.p4.6.m6.1.1.4.cmml" xref="S3.p4.6.m6.1.1.4"><csymbol cd="ambiguous" id="S3.p4.6.m6.1.1.4.1.cmml" xref="S3.p4.6.m6.1.1.4">superscript</csymbol><ci id="S3.p4.6.m6.1.1.4.2.cmml" xref="S3.p4.6.m6.1.1.4.2">𝑐</ci><cn type="integer" id="S3.p4.6.m6.1.1.4.3.cmml" xref="S3.p4.6.m6.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.6.m6.1c">2dc^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.6.m6.1d">2 italic_d italic_c start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOPs for multiplying these attention scores by values. In total, the computation demands amount to <math id="S3.p4.7.m7.1" class="ltx_Math" alttext="4dc^{2}" display="inline"><semantics id="S3.p4.7.m7.1a"><mrow id="S3.p4.7.m7.1.1" xref="S3.p4.7.m7.1.1.cmml"><mn id="S3.p4.7.m7.1.1.2" xref="S3.p4.7.m7.1.1.2.cmml">4</mn><mo id="S3.p4.7.m7.1.1.1" xref="S3.p4.7.m7.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.7.m7.1.1.3" xref="S3.p4.7.m7.1.1.3.cmml">d</mi><mo id="S3.p4.7.m7.1.1.1a" xref="S3.p4.7.m7.1.1.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S3.p4.7.m7.1.1.4" xref="S3.p4.7.m7.1.1.4.cmml"><mi id="S3.p4.7.m7.1.1.4.2" xref="S3.p4.7.m7.1.1.4.2.cmml">c</mi><mn id="S3.p4.7.m7.1.1.4.3" xref="S3.p4.7.m7.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.7.m7.1b"><apply id="S3.p4.7.m7.1.1.cmml" xref="S3.p4.7.m7.1.1"><times id="S3.p4.7.m7.1.1.1.cmml" xref="S3.p4.7.m7.1.1.1"></times><cn type="integer" id="S3.p4.7.m7.1.1.2.cmml" xref="S3.p4.7.m7.1.1.2">4</cn><ci id="S3.p4.7.m7.1.1.3.cmml" xref="S3.p4.7.m7.1.1.3">𝑑</ci><apply id="S3.p4.7.m7.1.1.4.cmml" xref="S3.p4.7.m7.1.1.4"><csymbol cd="ambiguous" id="S3.p4.7.m7.1.1.4.1.cmml" xref="S3.p4.7.m7.1.1.4">superscript</csymbol><ci id="S3.p4.7.m7.1.1.4.2.cmml" xref="S3.p4.7.m7.1.1.4.2">𝑐</ci><cn type="integer" id="S3.p4.7.m7.1.1.4.3.cmml" xref="S3.p4.7.m7.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.7.m7.1c">4dc^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.7.m7.1d">4 italic_d italic_c start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOPs.
We exclude the projection of queries, keys, and values, as well as blockwise feedforward operations, since they only add compute complexity without any communication costs between hosts. This simplification leads to more stringent condition and does not compromise the validity of our approach.
On the communication front, both key and value blocks require a total of <math id="S3.p4.8.m8.1" class="ltx_Math" alttext="2cd" display="inline"><semantics id="S3.p4.8.m8.1a"><mrow id="S3.p4.8.m8.1.1" xref="S3.p4.8.m8.1.1.cmml"><mn id="S3.p4.8.m8.1.1.2" xref="S3.p4.8.m8.1.1.2.cmml">2</mn><mo id="S3.p4.8.m8.1.1.1" xref="S3.p4.8.m8.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.8.m8.1.1.3" xref="S3.p4.8.m8.1.1.3.cmml">c</mi><mo id="S3.p4.8.m8.1.1.1a" xref="S3.p4.8.m8.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.8.m8.1.1.4" xref="S3.p4.8.m8.1.1.4.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.8.m8.1b"><apply id="S3.p4.8.m8.1.1.cmml" xref="S3.p4.8.m8.1.1"><times id="S3.p4.8.m8.1.1.1.cmml" xref="S3.p4.8.m8.1.1.1"></times><cn type="integer" id="S3.p4.8.m8.1.1.2.cmml" xref="S3.p4.8.m8.1.1.2">2</cn><ci id="S3.p4.8.m8.1.1.3.cmml" xref="S3.p4.8.m8.1.1.3">𝑐</ci><ci id="S3.p4.8.m8.1.1.4.cmml" xref="S3.p4.8.m8.1.1.4">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.8.m8.1c">2cd</annotation><annotation encoding="application/x-llamapun" id="S3.p4.8.m8.1d">2 italic_c italic_d</annotation></semantics></math> bytes. Thus, the combined communication demand is <math id="S3.p4.9.m9.1" class="ltx_Math" alttext="4cd" display="inline"><semantics id="S3.p4.9.m9.1a"><mrow id="S3.p4.9.m9.1.1" xref="S3.p4.9.m9.1.1.cmml"><mn id="S3.p4.9.m9.1.1.2" xref="S3.p4.9.m9.1.1.2.cmml">4</mn><mo id="S3.p4.9.m9.1.1.1" xref="S3.p4.9.m9.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.9.m9.1.1.3" xref="S3.p4.9.m9.1.1.3.cmml">c</mi><mo id="S3.p4.9.m9.1.1.1a" xref="S3.p4.9.m9.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.9.m9.1.1.4" xref="S3.p4.9.m9.1.1.4.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.9.m9.1b"><apply id="S3.p4.9.m9.1.1.cmml" xref="S3.p4.9.m9.1.1"><times id="S3.p4.9.m9.1.1.1.cmml" xref="S3.p4.9.m9.1.1.1"></times><cn type="integer" id="S3.p4.9.m9.1.1.2.cmml" xref="S3.p4.9.m9.1.1.2">4</cn><ci id="S3.p4.9.m9.1.1.3.cmml" xref="S3.p4.9.m9.1.1.3">𝑐</ci><ci id="S3.p4.9.m9.1.1.4.cmml" xref="S3.p4.9.m9.1.1.4">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.9.m9.1c">4cd</annotation><annotation encoding="application/x-llamapun" id="S3.p4.9.m9.1d">4 italic_c italic_d</annotation></semantics></math> bytes.
To achieve an overlap between communication and computation, the following condition must hold: <math id="S3.p4.10.m10.1" class="ltx_Math" alttext="4dc^{2}/F\geq 4cd/B" display="inline"><semantics id="S3.p4.10.m10.1a"><mrow id="S3.p4.10.m10.1.1" xref="S3.p4.10.m10.1.1.cmml"><mrow id="S3.p4.10.m10.1.1.2" xref="S3.p4.10.m10.1.1.2.cmml"><mrow id="S3.p4.10.m10.1.1.2.2" xref="S3.p4.10.m10.1.1.2.2.cmml"><mn id="S3.p4.10.m10.1.1.2.2.2" xref="S3.p4.10.m10.1.1.2.2.2.cmml">4</mn><mo id="S3.p4.10.m10.1.1.2.2.1" xref="S3.p4.10.m10.1.1.2.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.10.m10.1.1.2.2.3" xref="S3.p4.10.m10.1.1.2.2.3.cmml">d</mi><mo id="S3.p4.10.m10.1.1.2.2.1a" xref="S3.p4.10.m10.1.1.2.2.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S3.p4.10.m10.1.1.2.2.4" xref="S3.p4.10.m10.1.1.2.2.4.cmml"><mi id="S3.p4.10.m10.1.1.2.2.4.2" xref="S3.p4.10.m10.1.1.2.2.4.2.cmml">c</mi><mn id="S3.p4.10.m10.1.1.2.2.4.3" xref="S3.p4.10.m10.1.1.2.2.4.3.cmml">2</mn></msup></mrow><mo id="S3.p4.10.m10.1.1.2.1" xref="S3.p4.10.m10.1.1.2.1.cmml">/</mo><mi id="S3.p4.10.m10.1.1.2.3" xref="S3.p4.10.m10.1.1.2.3.cmml">F</mi></mrow><mo id="S3.p4.10.m10.1.1.1" xref="S3.p4.10.m10.1.1.1.cmml">≥</mo><mrow id="S3.p4.10.m10.1.1.3" xref="S3.p4.10.m10.1.1.3.cmml"><mrow id="S3.p4.10.m10.1.1.3.2" xref="S3.p4.10.m10.1.1.3.2.cmml"><mn id="S3.p4.10.m10.1.1.3.2.2" xref="S3.p4.10.m10.1.1.3.2.2.cmml">4</mn><mo id="S3.p4.10.m10.1.1.3.2.1" xref="S3.p4.10.m10.1.1.3.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.10.m10.1.1.3.2.3" xref="S3.p4.10.m10.1.1.3.2.3.cmml">c</mi><mo id="S3.p4.10.m10.1.1.3.2.1a" xref="S3.p4.10.m10.1.1.3.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p4.10.m10.1.1.3.2.4" xref="S3.p4.10.m10.1.1.3.2.4.cmml">d</mi></mrow><mo id="S3.p4.10.m10.1.1.3.1" xref="S3.p4.10.m10.1.1.3.1.cmml">/</mo><mi id="S3.p4.10.m10.1.1.3.3" xref="S3.p4.10.m10.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.10.m10.1b"><apply id="S3.p4.10.m10.1.1.cmml" xref="S3.p4.10.m10.1.1"><geq id="S3.p4.10.m10.1.1.1.cmml" xref="S3.p4.10.m10.1.1.1"></geq><apply id="S3.p4.10.m10.1.1.2.cmml" xref="S3.p4.10.m10.1.1.2"><divide id="S3.p4.10.m10.1.1.2.1.cmml" xref="S3.p4.10.m10.1.1.2.1"></divide><apply id="S3.p4.10.m10.1.1.2.2.cmml" xref="S3.p4.10.m10.1.1.2.2"><times id="S3.p4.10.m10.1.1.2.2.1.cmml" xref="S3.p4.10.m10.1.1.2.2.1"></times><cn type="integer" id="S3.p4.10.m10.1.1.2.2.2.cmml" xref="S3.p4.10.m10.1.1.2.2.2">4</cn><ci id="S3.p4.10.m10.1.1.2.2.3.cmml" xref="S3.p4.10.m10.1.1.2.2.3">𝑑</ci><apply id="S3.p4.10.m10.1.1.2.2.4.cmml" xref="S3.p4.10.m10.1.1.2.2.4"><csymbol cd="ambiguous" id="S3.p4.10.m10.1.1.2.2.4.1.cmml" xref="S3.p4.10.m10.1.1.2.2.4">superscript</csymbol><ci id="S3.p4.10.m10.1.1.2.2.4.2.cmml" xref="S3.p4.10.m10.1.1.2.2.4.2">𝑐</ci><cn type="integer" id="S3.p4.10.m10.1.1.2.2.4.3.cmml" xref="S3.p4.10.m10.1.1.2.2.4.3">2</cn></apply></apply><ci id="S3.p4.10.m10.1.1.2.3.cmml" xref="S3.p4.10.m10.1.1.2.3">𝐹</ci></apply><apply id="S3.p4.10.m10.1.1.3.cmml" xref="S3.p4.10.m10.1.1.3"><divide id="S3.p4.10.m10.1.1.3.1.cmml" xref="S3.p4.10.m10.1.1.3.1"></divide><apply id="S3.p4.10.m10.1.1.3.2.cmml" xref="S3.p4.10.m10.1.1.3.2"><times id="S3.p4.10.m10.1.1.3.2.1.cmml" xref="S3.p4.10.m10.1.1.3.2.1"></times><cn type="integer" id="S3.p4.10.m10.1.1.3.2.2.cmml" xref="S3.p4.10.m10.1.1.3.2.2">4</cn><ci id="S3.p4.10.m10.1.1.3.2.3.cmml" xref="S3.p4.10.m10.1.1.3.2.3">𝑐</ci><ci id="S3.p4.10.m10.1.1.3.2.4.cmml" xref="S3.p4.10.m10.1.1.3.2.4">𝑑</ci></apply><ci id="S3.p4.10.m10.1.1.3.3.cmml" xref="S3.p4.10.m10.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.10.m10.1c">4dc^{2}/F\geq 4cd/B</annotation><annotation encoding="application/x-llamapun" id="S3.p4.10.m10.1d">4 italic_d italic_c start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_F ≥ 4 italic_c italic_d / italic_B</annotation></semantics></math>.
This implies that the block size, denoted as <math id="S3.p4.11.m11.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.p4.11.m11.1a"><mi id="S3.p4.11.m11.1.1" xref="S3.p4.11.m11.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.p4.11.m11.1b"><ci id="S3.p4.11.m11.1.1.cmml" xref="S3.p4.11.m11.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.11.m11.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.p4.11.m11.1d">italic_c</annotation></semantics></math>, should be greater than or equal to <math id="S3.p4.12.m12.1" class="ltx_Math" alttext="F/B" display="inline"><semantics id="S3.p4.12.m12.1a"><mrow id="S3.p4.12.m12.1.1" xref="S3.p4.12.m12.1.1.cmml"><mi id="S3.p4.12.m12.1.1.2" xref="S3.p4.12.m12.1.1.2.cmml">F</mi><mo id="S3.p4.12.m12.1.1.1" xref="S3.p4.12.m12.1.1.1.cmml">/</mo><mi id="S3.p4.12.m12.1.1.3" xref="S3.p4.12.m12.1.1.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.12.m12.1b"><apply id="S3.p4.12.m12.1.1.cmml" xref="S3.p4.12.m12.1.1"><divide id="S3.p4.12.m12.1.1.1.cmml" xref="S3.p4.12.m12.1.1.1"></divide><ci id="S3.p4.12.m12.1.1.2.cmml" xref="S3.p4.12.m12.1.1.2">𝐹</ci><ci id="S3.p4.12.m12.1.1.3.cmml" xref="S3.p4.12.m12.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.12.m12.1c">F/B</annotation><annotation encoding="application/x-llamapun" id="S3.p4.12.m12.1d">italic_F / italic_B</annotation></semantics></math>. Effectively, this means that the block size needs to be larger than the ratio of FLOPs over bandwidth.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.5" class="ltx_p"><span id="S3.p5.5.1" class="ltx_text ltx_font_bold">Memory Requirement.</span>
A host needs to store multiple blocks, including one block size to store the current query block, two block sizes for the current key and value blocks, and two block sizes for receiving key and value blocks. Furthermore, storing the output of blockwise attention and feedforward necessitates one block size, as the output retains the shape of the query block. Therefore, a total of six blocks are required, which translates to <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="6bch" display="inline"><semantics id="S3.p5.1.m1.1a"><mrow id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mn id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">6</mn><mo id="S3.p5.1.m1.1.1.1" xref="S3.p5.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">b</mi><mo id="S3.p5.1.m1.1.1.1a" xref="S3.p5.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p5.1.m1.1.1.4" xref="S3.p5.1.m1.1.1.4.cmml">c</mi><mo id="S3.p5.1.m1.1.1.1b" xref="S3.p5.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p5.1.m1.1.1.5" xref="S3.p5.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><times id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1"></times><cn type="integer" id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2">6</cn><ci id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">𝑏</ci><ci id="S3.p5.1.m1.1.1.4.cmml" xref="S3.p5.1.m1.1.1.4">𝑐</ci><ci id="S3.p5.1.m1.1.1.5.cmml" xref="S3.p5.1.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">6bch</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.1d">6 italic_b italic_c italic_h</annotation></semantics></math> bytes of memory.
It’s worth noting that the blockwise feedforward network has a maximum activation size of <math id="S3.p5.2.m2.1" class="ltx_Math" alttext="2bch" display="inline"><semantics id="S3.p5.2.m2.1a"><mrow id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml"><mn id="S3.p5.2.m2.1.1.2" xref="S3.p5.2.m2.1.1.2.cmml">2</mn><mo id="S3.p5.2.m2.1.1.1" xref="S3.p5.2.m2.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p5.2.m2.1.1.3" xref="S3.p5.2.m2.1.1.3.cmml">b</mi><mo id="S3.p5.2.m2.1.1.1a" xref="S3.p5.2.m2.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p5.2.m2.1.1.4" xref="S3.p5.2.m2.1.1.4.cmml">c</mi><mo id="S3.p5.2.m2.1.1.1b" xref="S3.p5.2.m2.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p5.2.m2.1.1.5" xref="S3.p5.2.m2.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><apply id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1"><times id="S3.p5.2.m2.1.1.1.cmml" xref="S3.p5.2.m2.1.1.1"></times><cn type="integer" id="S3.p5.2.m2.1.1.2.cmml" xref="S3.p5.2.m2.1.1.2">2</cn><ci id="S3.p5.2.m2.1.1.3.cmml" xref="S3.p5.2.m2.1.1.3">𝑏</ci><ci id="S3.p5.2.m2.1.1.4.cmml" xref="S3.p5.2.m2.1.1.4">𝑐</ci><ci id="S3.p5.2.m2.1.1.5.cmml" xref="S3.p5.2.m2.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">2bch</annotation><annotation encoding="application/x-llamapun" id="S3.p5.2.m2.1d">2 italic_b italic_c italic_h</annotation></semantics></math>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>. Consequently, the total maximum activation size remains at <math id="S3.p5.3.m3.1" class="ltx_Math" alttext="6bch" display="inline"><semantics id="S3.p5.3.m3.1a"><mrow id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml"><mn id="S3.p5.3.m3.1.1.2" xref="S3.p5.3.m3.1.1.2.cmml">6</mn><mo id="S3.p5.3.m3.1.1.1" xref="S3.p5.3.m3.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p5.3.m3.1.1.3" xref="S3.p5.3.m3.1.1.3.cmml">b</mi><mo id="S3.p5.3.m3.1.1.1a" xref="S3.p5.3.m3.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p5.3.m3.1.1.4" xref="S3.p5.3.m3.1.1.4.cmml">c</mi><mo id="S3.p5.3.m3.1.1.1b" xref="S3.p5.3.m3.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p5.3.m3.1.1.5" xref="S3.p5.3.m3.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><apply id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1"><times id="S3.p5.3.m3.1.1.1.cmml" xref="S3.p5.3.m3.1.1.1"></times><cn type="integer" id="S3.p5.3.m3.1.1.2.cmml" xref="S3.p5.3.m3.1.1.2">6</cn><ci id="S3.p5.3.m3.1.1.3.cmml" xref="S3.p5.3.m3.1.1.3">𝑏</ci><ci id="S3.p5.3.m3.1.1.4.cmml" xref="S3.p5.3.m3.1.1.4">𝑐</ci><ci id="S3.p5.3.m3.1.1.5.cmml" xref="S3.p5.3.m3.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">6bch</annotation><annotation encoding="application/x-llamapun" id="S3.p5.3.m3.1d">6 italic_b italic_c italic_h</annotation></semantics></math> bytes.
Table&nbsp;<a href="#S3.T1" title="Table 1 ‣ 3 Ring Attention ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides a detailed comparison of the memory costs between our method and other approaches. Notably, our method exhibits the advantage of linear memory scaling with respect to the block size <math id="S3.p5.4.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.p5.4.m4.1a"><mi id="S3.p5.4.m4.1.1" xref="S3.p5.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.p5.4.m4.1b"><ci id="S3.p5.4.m4.1.1.cmml" xref="S3.p5.4.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.4.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.p5.4.m4.1d">italic_c</annotation></semantics></math>, and is independent of the input sequence length <math id="S3.p5.5.m5.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.p5.5.m5.1a"><mi id="S3.p5.5.m5.1.1" xref="S3.p5.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.p5.5.m5.1b"><ci id="S3.p5.5.m5.1.1.cmml" xref="S3.p5.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.5.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.p5.5.m5.1d">italic_s</annotation></semantics></math>.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">Our analysis shows that the model needs to have a sequence length of <math id="S3.p6.1.m1.1" class="ltx_Math" alttext="s=6c" display="inline"><semantics id="S3.p6.1.m1.1a"><mrow id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml"><mi id="S3.p6.1.m1.1.1.2" xref="S3.p6.1.m1.1.1.2.cmml">s</mi><mo id="S3.p6.1.m1.1.1.1" xref="S3.p6.1.m1.1.1.1.cmml">=</mo><mrow id="S3.p6.1.m1.1.1.3" xref="S3.p6.1.m1.1.1.3.cmml"><mn id="S3.p6.1.m1.1.1.3.2" xref="S3.p6.1.m1.1.1.3.2.cmml">6</mn><mo id="S3.p6.1.m1.1.1.3.1" xref="S3.p6.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.p6.1.m1.1.1.3.3" xref="S3.p6.1.m1.1.1.3.3.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><apply id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1"><eq id="S3.p6.1.m1.1.1.1.cmml" xref="S3.p6.1.m1.1.1.1"></eq><ci id="S3.p6.1.m1.1.1.2.cmml" xref="S3.p6.1.m1.1.1.2">𝑠</ci><apply id="S3.p6.1.m1.1.1.3.cmml" xref="S3.p6.1.m1.1.1.3"><times id="S3.p6.1.m1.1.1.3.1.cmml" xref="S3.p6.1.m1.1.1.3.1"></times><cn type="integer" id="S3.p6.1.m1.1.1.3.2.cmml" xref="S3.p6.1.m1.1.1.3.2">6</cn><ci id="S3.p6.1.m1.1.1.3.3.cmml" xref="S3.p6.1.m1.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">s=6c</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.1d">italic_s = 6 italic_c</annotation></semantics></math>, which is six times the minimal block size. Requirements for popular computing servers are shown in Table&nbsp;<a href="#S3.T2" title="Table 2 ‣ 3 Ring Attention ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The required minimal sequence length (rightmost column) for each host varies between 6K and 10K, and the minimal block size (second-to-rightmost column) for each host is around 1K for TPUs and GPUs with high bandwidth interconnect.
For GPUs connected via InfiniBand, which offers lower bandwidth, the requirements are more strict. In this case, we can consider extending Ring Attention to harness the GPU all-to-all topology, which we leave for future work.
These requirements are easy to meet with parallelism and memory efficient blockwise attention and feedforward&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Dao et&nbsp;al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>; Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>, which we will show in experiment Section&nbsp;<a href="#S5" title="5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Minimal sequence length needed on each device. Interconnect Bandwidth is the unidirectional bandwidth between hosts, <span id="S3.T2.9.1" class="ltx_text ltx_font_italic">i</span>.<span id="S3.T2.10.2" class="ltx_text ltx_font_italic">e</span>., NVLink / InfiniBand bandwidth between GPUs and ICI bandwidth between TPUs.
The minimal block size required <math id="S3.T2.3.m1.1" class="ltx_Math" alttext="c=\text{FLOPS}/\text{Bandwidth}" display="inline"><semantics id="S3.T2.3.m1.1b"><mrow id="S3.T2.3.m1.1.1" xref="S3.T2.3.m1.1.1.cmml"><mi id="S3.T2.3.m1.1.1.2" xref="S3.T2.3.m1.1.1.2.cmml">c</mi><mo id="S3.T2.3.m1.1.1.1" xref="S3.T2.3.m1.1.1.1.cmml">=</mo><mrow id="S3.T2.3.m1.1.1.3" xref="S3.T2.3.m1.1.1.3.cmml"><mtext id="S3.T2.3.m1.1.1.3.2" xref="S3.T2.3.m1.1.1.3.2a.cmml">FLOPS</mtext><mo id="S3.T2.3.m1.1.1.3.1" xref="S3.T2.3.m1.1.1.3.1.cmml">/</mo><mtext id="S3.T2.3.m1.1.1.3.3" xref="S3.T2.3.m1.1.1.3.3a.cmml">Bandwidth</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.3.m1.1c"><apply id="S3.T2.3.m1.1.1.cmml" xref="S3.T2.3.m1.1.1"><eq id="S3.T2.3.m1.1.1.1.cmml" xref="S3.T2.3.m1.1.1.1"></eq><ci id="S3.T2.3.m1.1.1.2.cmml" xref="S3.T2.3.m1.1.1.2">𝑐</ci><apply id="S3.T2.3.m1.1.1.3.cmml" xref="S3.T2.3.m1.1.1.3"><divide id="S3.T2.3.m1.1.1.3.1.cmml" xref="S3.T2.3.m1.1.1.3.1"></divide><ci id="S3.T2.3.m1.1.1.3.2a.cmml" xref="S3.T2.3.m1.1.1.3.2"><mtext id="S3.T2.3.m1.1.1.3.2.cmml" xref="S3.T2.3.m1.1.1.3.2">FLOPS</mtext></ci><ci id="S3.T2.3.m1.1.1.3.3a.cmml" xref="S3.T2.3.m1.1.1.3.3"><mtext id="S3.T2.3.m1.1.1.3.3.cmml" xref="S3.T2.3.m1.1.1.3.3">Bandwidth</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.m1.1d">c=\text{FLOPS}/\text{Bandwidth}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.m1.1e">italic_c = FLOPS / Bandwidth</annotation></semantics></math>, and minimal sequence length <math id="S3.T2.4.m2.1" class="ltx_Math" alttext="s=6c" display="inline"><semantics id="S3.T2.4.m2.1b"><mrow id="S3.T2.4.m2.1.1" xref="S3.T2.4.m2.1.1.cmml"><mi id="S3.T2.4.m2.1.1.2" xref="S3.T2.4.m2.1.1.2.cmml">s</mi><mo id="S3.T2.4.m2.1.1.1" xref="S3.T2.4.m2.1.1.1.cmml">=</mo><mrow id="S3.T2.4.m2.1.1.3" xref="S3.T2.4.m2.1.1.3.cmml"><mn id="S3.T2.4.m2.1.1.3.2" xref="S3.T2.4.m2.1.1.3.2.cmml">6</mn><mo id="S3.T2.4.m2.1.1.3.1" xref="S3.T2.4.m2.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T2.4.m2.1.1.3.3" xref="S3.T2.4.m2.1.1.3.3.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.4.m2.1c"><apply id="S3.T2.4.m2.1.1.cmml" xref="S3.T2.4.m2.1.1"><eq id="S3.T2.4.m2.1.1.1.cmml" xref="S3.T2.4.m2.1.1.1"></eq><ci id="S3.T2.4.m2.1.1.2.cmml" xref="S3.T2.4.m2.1.1.2">𝑠</ci><apply id="S3.T2.4.m2.1.1.3.cmml" xref="S3.T2.4.m2.1.1.3"><times id="S3.T2.4.m2.1.1.3.1.cmml" xref="S3.T2.4.m2.1.1.3.1"></times><cn type="integer" id="S3.T2.4.m2.1.1.3.2.cmml" xref="S3.T2.4.m2.1.1.3.2">6</cn><ci id="S3.T2.4.m2.1.1.3.3.cmml" xref="S3.T2.4.m2.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.m2.1d">s=6c</annotation><annotation encoding="application/x-llamapun" id="S3.T2.4.m2.1e">italic_s = 6 italic_c</annotation></semantics></math>.
</figcaption>
<div id="S3.T2.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:309.4pt;height:149.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.2pt,0.1pt) scale(0.999,0.999) ;">
<table id="S3.T2.6.2" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T2.6.2.3" class="ltx_tr">
<td id="S3.T2.6.2.3.1" class="ltx_td ltx_align_left ltx_border_tt">Spec Per Host</td>
<td id="S3.T2.6.2.3.2" class="ltx_td ltx_align_right ltx_border_tt">FLOPS</td>
<td id="S3.T2.6.2.3.3" class="ltx_td ltx_align_right ltx_border_tt">HBM</td>
<td id="S3.T2.6.2.3.4" class="ltx_td ltx_align_right ltx_border_tt">
<span id="S3.T2.6.2.3.4.1" class="ltx_text"></span> <span id="S3.T2.6.2.3.4.2" class="ltx_text">
<span id="S3.T2.6.2.3.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.6.2.3.4.2.1.1" class="ltx_tr">
<span id="S3.T2.6.2.3.4.2.1.1.1" class="ltx_td ltx_align_center">Interconnect</span></span>
<span id="S3.T2.6.2.3.4.2.1.2" class="ltx_tr">
<span id="S3.T2.6.2.3.4.2.1.2.1" class="ltx_td ltx_align_center">Bandwidth</span></span>
</span></span> <span id="S3.T2.6.2.3.4.3" class="ltx_text"></span>
</td>
<td id="S3.T2.6.2.3.5" class="ltx_td ltx_align_right ltx_border_tt">
<span id="S3.T2.6.2.3.5.1" class="ltx_text"></span> <span id="S3.T2.6.2.3.5.2" class="ltx_text">
<span id="S3.T2.6.2.3.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.6.2.3.5.2.1.1" class="ltx_tr">
<span id="S3.T2.6.2.3.5.2.1.1.1" class="ltx_td ltx_align_center">Minimal</span></span>
<span id="S3.T2.6.2.3.5.2.1.2" class="ltx_tr">
<span id="S3.T2.6.2.3.5.2.1.2.1" class="ltx_td ltx_align_center">Blocksize</span></span>
</span></span> <span id="S3.T2.6.2.3.5.3" class="ltx_text"></span>
</td>
<td id="S3.T2.6.2.3.6" class="ltx_td ltx_align_right ltx_border_tt">
<span id="S3.T2.6.2.3.6.1" class="ltx_text"></span> <span id="S3.T2.6.2.3.6.2" class="ltx_text">
<span id="S3.T2.6.2.3.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.6.2.3.6.2.1.1" class="ltx_tr">
<span id="S3.T2.6.2.3.6.2.1.1.1" class="ltx_td ltx_align_center">Minimal</span></span>
<span id="S3.T2.6.2.3.6.2.1.2" class="ltx_tr">
<span id="S3.T2.6.2.3.6.2.1.2.1" class="ltx_td ltx_align_center">Sequence Len</span></span>
</span></span> <span id="S3.T2.6.2.3.6.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S3.T2.6.2.2" class="ltx_tr">
<td id="S3.T2.6.2.2.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.6.2.2.4" class="ltx_td ltx_align_right ltx_border_t">(TF)</td>
<td id="S3.T2.6.2.2.5" class="ltx_td ltx_align_right ltx_border_t">(GB)</td>
<td id="S3.T2.6.2.2.6" class="ltx_td ltx_align_right ltx_border_t">(GB/s)</td>
<td id="S3.T2.5.1.1.1" class="ltx_td ltx_align_right ltx_border_t">(<math id="S3.T2.5.1.1.1.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S3.T2.5.1.1.1.m1.1a"><mrow id="S3.T2.5.1.1.1.m1.1.1" xref="S3.T2.5.1.1.1.m1.1.1.cmml"><mi id="S3.T2.5.1.1.1.m1.1.1.2" xref="S3.T2.5.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.T2.5.1.1.1.m1.1.1.1" xref="S3.T2.5.1.1.1.m1.1.1.1.cmml">×</mo><mrow id="S3.T2.5.1.1.1.m1.1.1.3" xref="S3.T2.5.1.1.1.m1.1.1.3.cmml"><mn id="S3.T2.5.1.1.1.m1.1.1.3.2" xref="S3.T2.5.1.1.1.m1.1.1.3.2.cmml">1</mn><mo id="S3.T2.5.1.1.1.m1.1.1.3.1" xref="S3.T2.5.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S3.T2.5.1.1.1.m1.1.1.3.3" xref="S3.T2.5.1.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.T2.5.1.1.1.m1.1.1.3.1a" xref="S3.T2.5.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn id="S3.T2.5.1.1.1.m1.1.1.3.4" xref="S3.T2.5.1.1.1.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.5.1.1.1.m1.1b"><apply id="S3.T2.5.1.1.1.m1.1.1.cmml" xref="S3.T2.5.1.1.1.m1.1.1"><times id="S3.T2.5.1.1.1.m1.1.1.1.cmml" xref="S3.T2.5.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S3.T2.5.1.1.1.m1.1.1.2.cmml" xref="S3.T2.5.1.1.1.m1.1.1.2">absent</csymbol><apply id="S3.T2.5.1.1.1.m1.1.1.3.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3"><times id="S3.T2.5.1.1.1.m1.1.1.3.1.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3.1"></times><cn type="integer" id="S3.T2.5.1.1.1.m1.1.1.3.2.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3.2">1</cn><ci id="S3.T2.5.1.1.1.m1.1.1.3.3.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3.3">e</ci><cn type="integer" id="S3.T2.5.1.1.1.m1.1.1.3.4.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.1.1.1.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.1.1.1.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</td>
<td id="S3.T2.6.2.2.2" class="ltx_td ltx_align_right ltx_border_t">(<math id="S3.T2.6.2.2.2.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S3.T2.6.2.2.2.m1.1a"><mrow id="S3.T2.6.2.2.2.m1.1.1" xref="S3.T2.6.2.2.2.m1.1.1.cmml"><mi id="S3.T2.6.2.2.2.m1.1.1.2" xref="S3.T2.6.2.2.2.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.T2.6.2.2.2.m1.1.1.1" xref="S3.T2.6.2.2.2.m1.1.1.1.cmml">×</mo><mrow id="S3.T2.6.2.2.2.m1.1.1.3" xref="S3.T2.6.2.2.2.m1.1.1.3.cmml"><mn id="S3.T2.6.2.2.2.m1.1.1.3.2" xref="S3.T2.6.2.2.2.m1.1.1.3.2.cmml">1</mn><mo id="S3.T2.6.2.2.2.m1.1.1.3.1" xref="S3.T2.6.2.2.2.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S3.T2.6.2.2.2.m1.1.1.3.3" xref="S3.T2.6.2.2.2.m1.1.1.3.3.cmml">e</mi><mo id="S3.T2.6.2.2.2.m1.1.1.3.1a" xref="S3.T2.6.2.2.2.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn id="S3.T2.6.2.2.2.m1.1.1.3.4" xref="S3.T2.6.2.2.2.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.6.2.2.2.m1.1b"><apply id="S3.T2.6.2.2.2.m1.1.1.cmml" xref="S3.T2.6.2.2.2.m1.1.1"><times id="S3.T2.6.2.2.2.m1.1.1.1.cmml" xref="S3.T2.6.2.2.2.m1.1.1.1"></times><csymbol cd="latexml" id="S3.T2.6.2.2.2.m1.1.1.2.cmml" xref="S3.T2.6.2.2.2.m1.1.1.2">absent</csymbol><apply id="S3.T2.6.2.2.2.m1.1.1.3.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3"><times id="S3.T2.6.2.2.2.m1.1.1.3.1.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3.1"></times><cn type="integer" id="S3.T2.6.2.2.2.m1.1.1.3.2.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3.2">1</cn><ci id="S3.T2.6.2.2.2.m1.1.1.3.3.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3.3">e</ci><cn type="integer" id="S3.T2.6.2.2.2.m1.1.1.3.4.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.2.2.2.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S3.T2.6.2.2.2.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</td>
</tr>
<tr id="S3.T2.6.2.4" class="ltx_tr">
<td id="S3.T2.6.2.4.1" class="ltx_td ltx_align_left ltx_border_t">A100 NVLink</td>
<td id="S3.T2.6.2.4.2" class="ltx_td ltx_align_right ltx_border_t">312</td>
<td id="S3.T2.6.2.4.3" class="ltx_td ltx_align_right ltx_border_t">80</td>
<td id="S3.T2.6.2.4.4" class="ltx_td ltx_align_right ltx_border_t">300</td>
<td id="S3.T2.6.2.4.5" class="ltx_td ltx_align_right ltx_border_t">1.0</td>
<td id="S3.T2.6.2.4.6" class="ltx_td ltx_align_right ltx_border_t">6.2</td>
</tr>
<tr id="S3.T2.6.2.5" class="ltx_tr">
<td id="S3.T2.6.2.5.1" class="ltx_td ltx_align_left">A100 InfiniBand</td>
<td id="S3.T2.6.2.5.2" class="ltx_td ltx_align_right">312</td>
<td id="S3.T2.6.2.5.3" class="ltx_td ltx_align_right">80</td>
<td id="S3.T2.6.2.5.4" class="ltx_td ltx_align_right">12.5</td>
<td id="S3.T2.6.2.5.5" class="ltx_td ltx_align_right">24.5</td>
<td id="S3.T2.6.2.5.6" class="ltx_td ltx_align_right">149.5</td>
</tr>
<tr id="S3.T2.6.2.6" class="ltx_tr">
<td id="S3.T2.6.2.6.1" class="ltx_td ltx_align_left">TPU v3</td>
<td id="S3.T2.6.2.6.2" class="ltx_td ltx_align_right">123</td>
<td id="S3.T2.6.2.6.3" class="ltx_td ltx_align_right">16</td>
<td id="S3.T2.6.2.6.4" class="ltx_td ltx_align_right">112</td>
<td id="S3.T2.6.2.6.5" class="ltx_td ltx_align_right">1.1</td>
<td id="S3.T2.6.2.6.6" class="ltx_td ltx_align_right">6.6</td>
</tr>
<tr id="S3.T2.6.2.7" class="ltx_tr">
<td id="S3.T2.6.2.7.1" class="ltx_td ltx_align_left">TPU v4</td>
<td id="S3.T2.6.2.7.2" class="ltx_td ltx_align_right">275</td>
<td id="S3.T2.6.2.7.3" class="ltx_td ltx_align_right">32</td>
<td id="S3.T2.6.2.7.4" class="ltx_td ltx_align_right">268</td>
<td id="S3.T2.6.2.7.5" class="ltx_td ltx_align_right">1.0</td>
<td id="S3.T2.6.2.7.6" class="ltx_td ltx_align_right">6.2</td>
</tr>
<tr id="S3.T2.6.2.8" class="ltx_tr">
<td id="S3.T2.6.2.8.1" class="ltx_td ltx_align_left ltx_border_bb">TPU v5e</td>
<td id="S3.T2.6.2.8.2" class="ltx_td ltx_align_right ltx_border_bb">196</td>
<td id="S3.T2.6.2.8.3" class="ltx_td ltx_align_right ltx_border_bb">16</td>
<td id="S3.T2.6.2.8.4" class="ltx_td ltx_align_right ltx_border_bb">186</td>
<td id="S3.T2.6.2.8.5" class="ltx_td ltx_align_right ltx_border_bb">1.1</td>
<td id="S3.T2.6.2.8.6" class="ltx_td ltx_align_right ltx_border_bb">6.3</td>
</tr>
</tbody></table>
</span></div>
</figure>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p"><span id="S3.p7.1.1" class="ltx_text ltx_font_bold">Algorithm and Implementation.</span>
Algorithm&nbsp;<a href="#alg1" title="Algorithm 1 ‣ 3 Ring Attention ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides the pseudocode of the algorithm.
Ring Attention is compatible with existing code for memory efficient transformers:
Ring Attention just needs to call whatever available memory efficient computation locally on each host, and overlap the communication of key-value blocks between hosts with blockwise computation. We use collective operation <span id="S3.p7.1.2" class="ltx_text ltx_font_typewriter">jax.lax.ppermute</span> to send and receive key value blocks between nearby hosts.
A Jax implementation is provided in Appendix&nbsp;<a href="#A1" title="Appendix A Code ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Reducing Transformers Memory Cost with Ring Attention.</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">&nbsp;&nbsp;<span id="alg1.l1.1" class="ltx_text ltx_font_bold">Required:</span> Input sequence <math id="alg1.l1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_x</annotation></semantics></math>. Number of hosts <math id="alg1.l1.m2.1" class="ltx_Math" alttext="N_{h}" display="inline"><semantics id="alg1.l1.m2.1a"><msub id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mi id="alg1.l1.m2.1.1.2" xref="alg1.l1.m2.1.1.2.cmml">N</mi><mi id="alg1.l1.m2.1.1.3" xref="alg1.l1.m2.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><csymbol cd="ambiguous" id="alg1.l1.m2.1.1.1.cmml" xref="alg1.l1.m2.1.1">subscript</csymbol><ci id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1.2">𝑁</ci><ci id="alg1.l1.m2.1.1.3.cmml" xref="alg1.l1.m2.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">N_{h}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math>.

</div>
<div id="alg1.l2" class="ltx_listingline">&nbsp;&nbsp;Initialize

</div>
<div id="alg1.l3" class="ltx_listingline">&nbsp;&nbsp;Split input sequence into <math id="alg1.l3.m1.1" class="ltx_Math" alttext="N_{h}" display="inline"><semantics id="alg1.l3.m1.1a"><msub id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">N</mi><mi id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝑁</ci><ci id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">N_{h}</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> blocks that each host has one input block.

</div>
<div id="alg1.l4" class="ltx_listingline">&nbsp;&nbsp;Compute query, key, and value for its input block on each host.

</div>
<div id="alg1.l5" class="ltx_listingline">&nbsp;&nbsp;<span id="alg1.l5.1" class="ltx_text ltx_font_bold">for</span>&nbsp;Each transformer layer&nbsp;<span id="alg1.l5.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l6" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l6.1" class="ltx_text ltx_font_bold">for</span>&nbsp;<math id="alg1.l6.m1.1" class="ltx_Math" alttext="count=1" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mrow id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml"><mi id="alg1.l6.m1.1.1.2.2" xref="alg1.l6.m1.1.1.2.2.cmml">c</mi><mo id="alg1.l6.m1.1.1.2.1" xref="alg1.l6.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="alg1.l6.m1.1.1.2.3" xref="alg1.l6.m1.1.1.2.3.cmml">o</mi><mo id="alg1.l6.m1.1.1.2.1a" xref="alg1.l6.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="alg1.l6.m1.1.1.2.4" xref="alg1.l6.m1.1.1.2.4.cmml">u</mi><mo id="alg1.l6.m1.1.1.2.1b" xref="alg1.l6.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="alg1.l6.m1.1.1.2.5" xref="alg1.l6.m1.1.1.2.5.cmml">n</mi><mo id="alg1.l6.m1.1.1.2.1c" xref="alg1.l6.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="alg1.l6.m1.1.1.2.6" xref="alg1.l6.m1.1.1.2.6.cmml">t</mi></mrow><mo id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">=</mo><mn id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><eq id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"></eq><apply id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2"><times id="alg1.l6.m1.1.1.2.1.cmml" xref="alg1.l6.m1.1.1.2.1"></times><ci id="alg1.l6.m1.1.1.2.2.cmml" xref="alg1.l6.m1.1.1.2.2">𝑐</ci><ci id="alg1.l6.m1.1.1.2.3.cmml" xref="alg1.l6.m1.1.1.2.3">𝑜</ci><ci id="alg1.l6.m1.1.1.2.4.cmml" xref="alg1.l6.m1.1.1.2.4">𝑢</ci><ci id="alg1.l6.m1.1.1.2.5.cmml" xref="alg1.l6.m1.1.1.2.5">𝑛</ci><ci id="alg1.l6.m1.1.1.2.6.cmml" xref="alg1.l6.m1.1.1.2.6">𝑡</ci></apply><cn type="integer" id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">count=1</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">italic_c italic_o italic_u italic_n italic_t = 1</annotation></semantics></math> <span id="alg1.l6.2" class="ltx_text ltx_font_bold">to</span> <math id="alg1.l6.m2.1" class="ltx_Math" alttext="N_{h}-1" display="inline"><semantics id="alg1.l6.m2.1a"><mrow id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml"><msub id="alg1.l6.m2.1.1.2" xref="alg1.l6.m2.1.1.2.cmml"><mi id="alg1.l6.m2.1.1.2.2" xref="alg1.l6.m2.1.1.2.2.cmml">N</mi><mi id="alg1.l6.m2.1.1.2.3" xref="alg1.l6.m2.1.1.2.3.cmml">h</mi></msub><mo id="alg1.l6.m2.1.1.1" xref="alg1.l6.m2.1.1.1.cmml">−</mo><mn id="alg1.l6.m2.1.1.3" xref="alg1.l6.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><apply id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1"><minus id="alg1.l6.m2.1.1.1.cmml" xref="alg1.l6.m2.1.1.1"></minus><apply id="alg1.l6.m2.1.1.2.cmml" xref="alg1.l6.m2.1.1.2"><csymbol cd="ambiguous" id="alg1.l6.m2.1.1.2.1.cmml" xref="alg1.l6.m2.1.1.2">subscript</csymbol><ci id="alg1.l6.m2.1.1.2.2.cmml" xref="alg1.l6.m2.1.1.2.2">𝑁</ci><ci id="alg1.l6.m2.1.1.2.3.cmml" xref="alg1.l6.m2.1.1.2.3">ℎ</ci></apply><cn type="integer" id="alg1.l6.m2.1.1.3.cmml" xref="alg1.l6.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">N_{h}-1</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m2.1d">italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT - 1</annotation></semantics></math>&nbsp;<span id="alg1.l6.3" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l7" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l7.1" class="ltx_text ltx_font_bold">for</span>&nbsp;For each host concurrently.&nbsp;<span id="alg1.l7.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l8" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute memory efficient attention incrementally using local query, key, value blocks.

</div>
<div id="alg1.l9" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Send key and value blocks to next host and receive key and value blocks from previous host.

</div>
<div id="alg1.l10" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l10.1" class="ltx_text ltx_font_bold">end</span>&nbsp;<span id="alg1.l10.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l11" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l11.1" class="ltx_text ltx_font_bold">end</span>&nbsp;<span id="alg1.l11.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l12" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l12.1" class="ltx_text ltx_font_bold">for</span>&nbsp;For each host concurrently.&nbsp;<span id="alg1.l12.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l13" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute memory efficient feedforward using local attention output.

</div>
<div id="alg1.l14" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l14.1" class="ltx_text ltx_font_bold">end</span>&nbsp;<span id="alg1.l14.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l15" class="ltx_listingline">&nbsp;&nbsp;<span id="alg1.l15.1" class="ltx_text ltx_font_bold">end</span>&nbsp;<span id="alg1.l15.2" class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Setting</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We evaluate the impact of using Ring Attention in improving Transformer models by benchmarking maximum sequence length and model flops utilization.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Model Configuration.</span>
Our study is built upon the LLaMA architecture, we consider 3B, 7B, 13B, and 30B model sizes in our experiments.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Baselines.</span>
We evaluate our method by comparing it with vanilla transformers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite> which computes self-attention by materializing the attention matrix and computes the feedforward network normally, transformers with memory efficient attention&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> and its efficient CUDA implementation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dao et&nbsp;al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>, and transformers with both memory efficient attention and feedforward&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Training Configuration.</span>
For all methods, we apply full gradient checkpointing&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chen et&nbsp;al., <a href="#bib.bib5" title="" class="ltx_ref">2016</a>)</cite> to both attention and feedforward, following prior works&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>.
The experiments are on both GPUs and TPUs. For GPUs, we consider both single DGX A100 server with 8 GPUs and distributed 32 A100 GPUs.
We also experiment with TPUs, from older generations TPUv3 to newer generations of TPUv4 and TPUv5e.
We note that all of our results are obtained using full precision instead of mixed precision.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In our experiments, our primary objective is to comprehensively evaluate the performance of Ring Attention across multiple key metrics, including maximum supported sequence length within accelerator memory, model flops utilization, and throughput.
We compare Ring Attention’s performance with several baseline models
, including the vanilla transformers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite>, transformers with memory efficient attention&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, and transformers with both memory efficient attention and feedforward&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>,
across different model sizes and accelerator configurations.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluating Max Context Size</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.5" class="ltx_p">We evaluate maximum supported context length using fully sharded tensor parallelsim (FSDP)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Facebook, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> which is widely used in prior end-to-end training&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>; Geng and Liu, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>.
We note that no tensor parallelism is considered in our evaluations since our approach is independent of tensor parallelism.
Practitioners can combine our method with tensor parallelism, which we will show in Section&nbsp;<a href="#S5.SS2" title="5.2 Evaluating Model Flops Utilization ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
Using FSDP allows us to set the same batch size in tokens for baselines and our approach, ensuring a fair comparison.
Concretely, on <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_n</annotation></semantics></math> devices, FSDP is used to shard the model for baselines, which gives a sequence length of <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">italic_l</annotation></semantics></math>. The total batch size in tokens is <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="nl" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mrow id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mi id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml">n</mi><mo id="S5.SS1.p1.3.m3.1.1.1" xref="S5.SS1.p1.3.m3.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><times id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1.1"></times><ci id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">𝑛</ci><ci id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">nl</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.3.m3.1d">italic_n italic_l</annotation></semantics></math>. We utilize FSDP along with Ring Attention to extend the sequence length to <math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="\frac{nl}{m}" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><mfrac id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><mrow id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2.cmml"><mi id="S5.SS1.p1.4.m4.1.1.2.2" xref="S5.SS1.p1.4.m4.1.1.2.2.cmml">n</mi><mo id="S5.SS1.p1.4.m4.1.1.2.1" xref="S5.SS1.p1.4.m4.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S5.SS1.p1.4.m4.1.1.2.3" xref="S5.SS1.p1.4.m4.1.1.2.3.cmml">l</mi></mrow><mi id="S5.SS1.p1.4.m4.1.1.3" xref="S5.SS1.p1.4.m4.1.1.3.cmml">m</mi></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><divide id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"></divide><apply id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2"><times id="S5.SS1.p1.4.m4.1.1.2.1.cmml" xref="S5.SS1.p1.4.m4.1.1.2.1"></times><ci id="S5.SS1.p1.4.m4.1.1.2.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2.2">𝑛</ci><ci id="S5.SS1.p1.4.m4.1.1.2.3.cmml" xref="S5.SS1.p1.4.m4.1.1.2.3">𝑙</ci></apply><ci id="S5.SS1.p1.4.m4.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">\frac{nl}{m}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.4.m4.1d">divide start_ARG italic_n italic_l end_ARG start_ARG italic_m end_ARG</annotation></semantics></math> and <math id="S5.SS1.p1.5.m5.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.SS1.p1.5.m5.1a"><mi id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><ci id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.5.m5.1d">italic_m</annotation></semantics></math> sequences. This means that the total batch size in tokens remains the same, but Ring Attention enables a significantly larger context size.
Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Evaluating Max Context Size ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes the results of our experiments.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.3" class="ltx_p">Our Ring Attention model consistently surpasses baselines, delivering superior scalability across diverse hardware setups. For example, with 32 A100 GPUs, we achieve over 1 million tokens in context size for 7B model, a 32 times improvement over previous best.
Furthermore, when utilizing larger accelerators like TPUv4-512, Ring Attention enables a 256 times increase in context size, allows training sequences of over 30 million tokens.
Furthermore, our Ring Attention model scales linearly with the number of devices, as demonstrated by the 8x improvement over previous best on 8 A100 and the 256x improvement on TPUv3-512.
If a model can be trained with context size <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">italic_s</annotation></semantics></math> on <math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mi id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><ci id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_n</annotation></semantics></math> GPUs using the blockwise attention and feedforward, with our Ring Attention approach, it becomes possible to train a model with a context size of <math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="ns" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><mrow id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mi id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml">n</mi><mo id="S5.SS1.p2.3.m3.1.1.1" xref="S5.SS1.p2.3.m3.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><times id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1.1"></times><ci id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">𝑛</ci><ci id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">ns</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">italic_n italic_s</annotation></semantics></math>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The maximum context length supported in end-to-end training using fully sharded data parallelism and various transformers architectures.
We show different model sizes and accelerators.
Baselines are vanilla transformer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite>, transformer with memory efficient attention&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, and transformer with memory efficient attention and feedforward&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>.
The context size is reported in tokens (1e3).
Our Ring Attention substantially outperforms baselines and enables training sequences that are up to device count times longer than prior state-of-the-arts.
</figcaption>
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:360.8pt;height:429.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.2pt,0.2pt) scale(0.999,0.999) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S5.T3.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Max context size supported (<math id="S5.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S5.T3.1.1.1.1.1.m1.1a"><mrow id="S5.T3.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" mathvariant="normal" rspace="0.222em" id="S5.T3.1.1.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mrow id="S5.T3.1.1.1.1.1.m1.1.1.3" xref="S5.T3.1.1.1.1.1.m1.1.1.3.cmml"><mn mathvariant="normal" id="S5.T3.1.1.1.1.1.m1.1.1.3.2" xref="S5.T3.1.1.1.1.1.m1.1.1.3.2.cmml">1</mn><mo mathvariant="bold" id="S5.T3.1.1.1.1.1.m1.1.1.3.1" xref="S5.T3.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S5.T3.1.1.1.1.1.m1.1.1.3.3" xref="S5.T3.1.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo mathvariant="bold" id="S5.T3.1.1.1.1.1.m1.1.1.3.1a" xref="S5.T3.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn mathvariant="normal" id="S5.T3.1.1.1.1.1.m1.1.1.3.4" xref="S5.T3.1.1.1.1.1.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1"><times id="S5.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S5.T3.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S5.T3.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3"><times id="S5.T3.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3.1"></times><cn type="integer" id="S5.T3.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3.2">1</cn><ci id="S5.T3.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3.3">normal-e</ci><cn type="integer" id="S5.T3.1.1.1.1.1.m1.1.1.3.4.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</span></td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T3.1.1.2" class="ltx_tr">
<td id="S5.T3.1.1.2.1" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.1.2.2" class="ltx_td ltx_align_right ltx_border_t">Vanilla</td>
<td id="S5.T3.1.1.2.3" class="ltx_td ltx_align_right ltx_border_t">
<span id="S5.T3.1.1.2.3.1" class="ltx_text"></span> <span id="S5.T3.1.1.2.3.2" class="ltx_text">
<span id="S5.T3.1.1.2.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.2.3.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.2.3.2.1.1.1" class="ltx_td ltx_align_center">Memory</span></span>
<span id="S5.T3.1.1.2.3.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.2.3.2.1.2.1" class="ltx_td ltx_align_center">Efficient Attn</span></span>
</span></span> <span id="S5.T3.1.1.2.3.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.2.4" class="ltx_td ltx_align_right ltx_border_t">
<span id="S5.T3.1.1.2.4.1" class="ltx_text"></span> <span id="S5.T3.1.1.2.4.2" class="ltx_text">
<span id="S5.T3.1.1.2.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.2.4.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.2.4.2.1.1.1" class="ltx_td ltx_align_center">Memory Efficient</span></span>
<span id="S5.T3.1.1.2.4.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.2.4.2.1.2.1" class="ltx_td ltx_align_center">Attn and FFN</span></span>
</span></span> <span id="S5.T3.1.1.2.4.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">
<span id="S5.T3.1.1.2.5.1" class="ltx_text"></span> <span id="S5.T3.1.1.2.5.2" class="ltx_text">
<span id="S5.T3.1.1.2.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.2.5.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.2.5.2.1.1.1" class="ltx_td ltx_align_center">Ring Attention</span></span>
<span id="S5.T3.1.1.2.5.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.2.5.2.1.2.1" class="ltx_td ltx_align_center">(Ours)</span></span>
</span></span> <span id="S5.T3.1.1.2.5.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.2.6" class="ltx_td ltx_align_right">
<span id="S5.T3.1.1.2.6.1" class="ltx_text"></span> <span id="S5.T3.1.1.2.6.2" class="ltx_text">
<span id="S5.T3.1.1.2.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.2.6.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.2.6.2.1.1.1" class="ltx_td ltx_align_center">Ours</span></span>
<span id="S5.T3.1.1.2.6.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.2.6.2.1.2.1" class="ltx_td ltx_align_center">vs SOTA</span></span>
</span></span> <span id="S5.T3.1.1.2.6.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S5.T3.1.1.3" class="ltx_tr">
<td id="S5.T3.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S5.T3.1.1.3.1.1" class="ltx_text"></span><span id="S5.T3.1.1.3.1.2" class="ltx_text">
<span id="S5.T3.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.3.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.3.1.2.1.1.1" class="ltx_td ltx_align_left">8x A100</span></span>
<span id="S5.T3.1.1.3.1.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.3.1.2.1.2.1" class="ltx_td ltx_align_left">NVLink</span></span>
</span></span> <span id="S5.T3.1.1.3.1.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.3.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.3.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.3.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.3.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.3.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.4" class="ltx_tr">
<td id="S5.T3.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r">3B</td>
<td id="S5.T3.1.1.4.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.4.3" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.4.4" class="ltx_td ltx_align_right">64</td>
<td id="S5.T3.1.1.4.5" class="ltx_td ltx_align_right ltx_border_r">512</td>
<td id="S5.T3.1.1.4.6" class="ltx_td ltx_align_right">8x</td>
</tr>
<tr id="S5.T3.1.1.5" class="ltx_tr">
<td id="S5.T3.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">7B</td>
<td id="S5.T3.1.1.5.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T3.1.1.5.3" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.5.4" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.5.5" class="ltx_td ltx_align_right ltx_border_r">256</td>
<td id="S5.T3.1.1.5.6" class="ltx_td ltx_align_right">8x</td>
</tr>
<tr id="S5.T3.1.1.6" class="ltx_tr">
<td id="S5.T3.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r">13B</td>
<td id="S5.T3.1.1.6.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T3.1.1.6.3" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.6.4" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.6.5" class="ltx_td ltx_align_right ltx_border_r">128</td>
<td id="S5.T3.1.1.6.6" class="ltx_td ltx_align_right">8x</td>
</tr>
<tr id="S5.T3.1.1.7" class="ltx_tr">
<td id="S5.T3.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S5.T3.1.1.7.1.1" class="ltx_text"></span><span id="S5.T3.1.1.7.1.2" class="ltx_text">
<span id="S5.T3.1.1.7.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.7.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.7.1.2.1.1.1" class="ltx_td ltx_align_left">32x A100</span></span>
<span id="S5.T3.1.1.7.1.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.7.1.2.1.2.1" class="ltx_td ltx_align_left">InfiniBand</span></span>
</span></span> <span id="S5.T3.1.1.7.1.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.7.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.7.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.7.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.7.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.7.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.8" class="ltx_tr">
<td id="S5.T3.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r">7B</td>
<td id="S5.T3.1.1.8.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.8.3" class="ltx_td ltx_align_right">64</td>
<td id="S5.T3.1.1.8.4" class="ltx_td ltx_align_right">128</td>
<td id="S5.T3.1.1.8.5" class="ltx_td ltx_align_right ltx_border_r">4096</td>
<td id="S5.T3.1.1.8.6" class="ltx_td ltx_align_right">32x</td>
</tr>
<tr id="S5.T3.1.1.9" class="ltx_tr">
<td id="S5.T3.1.1.9.1" class="ltx_td ltx_align_left ltx_border_r">13B</td>
<td id="S5.T3.1.1.9.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.9.3" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.9.4" class="ltx_td ltx_align_right">64</td>
<td id="S5.T3.1.1.9.5" class="ltx_td ltx_align_right ltx_border_r">2048</td>
<td id="S5.T3.1.1.9.6" class="ltx_td ltx_align_right">32x</td>
</tr>
<tr id="S5.T3.1.1.10" class="ltx_tr">
<td id="S5.T3.1.1.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">TPUv3-512</td>
<td id="S5.T3.1.1.10.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.10.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.10.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.10.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.10.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.11" class="ltx_tr">
<td id="S5.T3.1.1.11.1" class="ltx_td ltx_align_left ltx_border_r">7B</td>
<td id="S5.T3.1.1.11.2" class="ltx_td ltx_align_right">1</td>
<td id="S5.T3.1.1.11.3" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.11.4" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.11.5" class="ltx_td ltx_align_right ltx_border_r">2048</td>
<td id="S5.T3.1.1.11.6" class="ltx_td ltx_align_right">256x</td>
</tr>
<tr id="S5.T3.1.1.12" class="ltx_tr">
<td id="S5.T3.1.1.12.1" class="ltx_td ltx_align_left ltx_border_r">13B</td>
<td id="S5.T3.1.1.12.2" class="ltx_td ltx_align_right">1</td>
<td id="S5.T3.1.1.12.3" class="ltx_td ltx_align_right">2</td>
<td id="S5.T3.1.1.12.4" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.12.5" class="ltx_td ltx_align_right ltx_border_r">1024</td>
<td id="S5.T3.1.1.12.6" class="ltx_td ltx_align_right">128x</td>
</tr>
<tr id="S5.T3.1.1.13" class="ltx_tr">
<td id="S5.T3.1.1.13.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">TPUv4-1024</td>
<td id="S5.T3.1.1.13.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.13.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.13.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.13.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.13.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.14" class="ltx_tr">
<td id="S5.T3.1.1.14.1" class="ltx_td ltx_align_left ltx_border_r">3B</td>
<td id="S5.T3.1.1.14.2" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.14.3" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.14.4" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.14.5" class="ltx_td ltx_align_right ltx_border_r">16384</td>
<td id="S5.T3.1.1.14.6" class="ltx_td ltx_align_right">512x</td>
</tr>
<tr id="S5.T3.1.1.15" class="ltx_tr">
<td id="S5.T3.1.1.15.1" class="ltx_td ltx_align_left ltx_border_r">7B</td>
<td id="S5.T3.1.1.15.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.15.3" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.15.4" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.15.5" class="ltx_td ltx_align_right ltx_border_r">8192</td>
<td id="S5.T3.1.1.15.6" class="ltx_td ltx_align_right">512x</td>
</tr>
<tr id="S5.T3.1.1.16" class="ltx_tr">
<td id="S5.T3.1.1.16.1" class="ltx_td ltx_align_left ltx_border_r">13B</td>
<td id="S5.T3.1.1.16.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.16.3" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.16.4" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.16.5" class="ltx_td ltx_align_right ltx_border_r">4096</td>
<td id="S5.T3.1.1.16.6" class="ltx_td ltx_align_right">256x</td>
</tr>
<tr id="S5.T3.1.1.17" class="ltx_tr">
<td id="S5.T3.1.1.17.1" class="ltx_td ltx_align_left ltx_border_r">30B</td>
<td id="S5.T3.1.1.17.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T3.1.1.17.3" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.17.4" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.17.5" class="ltx_td ltx_align_right ltx_border_r">2048</td>
<td id="S5.T3.1.1.17.6" class="ltx_td ltx_align_right">256x</td>
</tr>
<tr id="S5.T3.1.1.18" class="ltx_tr">
<td id="S5.T3.1.1.18.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">TPUv5e-256</td>
<td id="S5.T3.1.1.18.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.18.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.18.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.18.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.18.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.19" class="ltx_tr">
<td id="S5.T3.1.1.19.1" class="ltx_td ltx_align_left ltx_border_r">3B</td>
<td id="S5.T3.1.1.19.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.19.3" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.19.4" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.19.5" class="ltx_td ltx_align_right ltx_border_r">4096</td>
<td id="S5.T3.1.1.19.6" class="ltx_td ltx_align_right">128x</td>
</tr>
<tr id="S5.T3.1.1.20" class="ltx_tr">
<td id="S5.T3.1.1.20.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">7B</td>
<td id="S5.T3.1.1.20.2" class="ltx_td ltx_align_right ltx_border_bb">2</td>
<td id="S5.T3.1.1.20.3" class="ltx_td ltx_align_right ltx_border_bb">8</td>
<td id="S5.T3.1.1.20.4" class="ltx_td ltx_align_right ltx_border_bb">16</td>
<td id="S5.T3.1.1.20.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">2048</td>
<td id="S5.T3.1.1.20.6" class="ltx_td ltx_align_right ltx_border_bb">128x</td>
</tr>
</tbody></table>
</span></div>
</figure>
<figure id="S5.T4" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Model flops utilization (MFU) with different training configurations: model sizes, compute, and context lengths.
Ring Attention enables training
<span id="S5.T4.4.1" class="ltx_text ltx_framed ltx_framed_underline">large models (7B-65B) on large input context sizes (over 4M) with negligible overheads.</span>
</figcaption><div class="ltx_flex_figure ltx_flex_table">

<div class="ltx_flex_cell 
                  ltx_flex_size_1"><img src="/html/2310.01889/assets/figures/mfu_trend.png" id="S5.T4.g1" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" width="598" height="251" alt="[Uncaptioned image]"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell 
                  ltx_flex_size_1">
<div id="S5.T4.2" class="ltx_inline-block ltx_flex_size_1 ltx_align_center ltx_transformed_outer" style="width:350.5pt;height:112.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.2pt,3.0pt) scale(0.95,0.95) ;">
<table id="S5.T4.2.2" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S5.T4.2.2.3" class="ltx_tr">
<td id="S5.T4.2.2.3.1" class="ltx_td ltx_border_tt"></td>
<td id="S5.T4.2.2.3.2" class="ltx_td ltx_align_right ltx_border_tt">Model size</td>
<td id="S5.T4.2.2.3.3" class="ltx_td ltx_align_right ltx_border_tt">7B</td>
<td id="S5.T4.2.2.3.4" class="ltx_td ltx_align_right ltx_border_tt">13B</td>
<td id="S5.T4.2.2.3.5" class="ltx_td ltx_align_right ltx_border_tt">13B</td>
<td id="S5.T4.2.2.3.6" class="ltx_td ltx_align_right ltx_border_tt">30B</td>
<td id="S5.T4.2.2.3.7" class="ltx_td ltx_align_right ltx_border_tt">65B</td>
</tr>
<tr id="S5.T4.2.2.4" class="ltx_tr">
<td id="S5.T4.2.2.4.1" class="ltx_td"></td>
<td id="S5.T4.2.2.4.2" class="ltx_td ltx_align_right ltx_border_t">Compute</td>
<td id="S5.T4.2.2.4.3" class="ltx_td ltx_align_right ltx_border_t">8x A100</td>
<td id="S5.T4.2.2.4.4" class="ltx_td ltx_align_right ltx_border_t">8x A100</td>
<td id="S5.T4.2.2.4.5" class="ltx_td ltx_align_right ltx_border_t">32x A100</td>
<td id="S5.T4.2.2.4.6" class="ltx_td ltx_align_right ltx_border_t">TPUv4-1024</td>
<td id="S5.T4.2.2.4.7" class="ltx_td ltx_align_right ltx_border_t">TPUv4-1024</td>
</tr>
<tr id="S5.T4.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T4.1.1.1.2.1" class="ltx_text"></span><span id="S5.T4.1.1.1.2.2" class="ltx_text">
<span id="S5.T4.1.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.1.1.1.2.2.1.1" class="ltx_tr">
<span id="S5.T4.1.1.1.2.2.1.1.1" class="ltx_td ltx_align_left">Memory efficient</span></span>
<span id="S5.T4.1.1.1.2.2.1.2" class="ltx_tr">
<span id="S5.T4.1.1.1.2.2.1.2.1" class="ltx_td ltx_align_left">attention &amp; FFN</span></span>
</span></span> <span id="S5.T4.1.1.1.2.3" class="ltx_text"></span>
</td>
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_right ltx_border_t">
<span id="S5.T4.1.1.1.1.2" class="ltx_text"></span> <span id="S5.T4.1.1.1.1.1" class="ltx_text">
<span id="S5.T4.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S5.T4.1.1.1.1.1.1.2.1" class="ltx_td ltx_align_center">Context size</span></span>
<span id="S5.T4.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T4.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center">(<math id="S5.T4.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S5.T4.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mrow id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mn id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">1</mn><mo id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1a" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.4" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1"><times id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3"><times id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1"></times><cn type="integer" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.2">1</cn><ci id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.3">e</ci><cn type="integer" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.4.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.1.1.1.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.1.1.1.1.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</span></span>
</span></span> <span id="S5.T4.1.1.1.1.3" class="ltx_text"></span>
</td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_right ltx_border_t">32</td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_right ltx_border_t">16</td>
<td id="S5.T4.1.1.1.5" class="ltx_td ltx_align_right ltx_border_t">64</td>
<td id="S5.T4.1.1.1.6" class="ltx_td ltx_align_right ltx_border_t">16</td>
<td id="S5.T4.1.1.1.7" class="ltx_td ltx_align_right ltx_border_t">8</td>
</tr>
<tr id="S5.T4.2.2.2" class="ltx_tr">
<td id="S5.T4.2.2.2.2" class="ltx_td ltx_align_left ltx_border_bb">Ring Attention</td>
<td id="S5.T4.2.2.2.1" class="ltx_td ltx_align_right ltx_border_bb">
<span id="S5.T4.2.2.2.1.2" class="ltx_text"></span> <span id="S5.T4.2.2.2.1.1" class="ltx_text">
<span id="S5.T4.2.2.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.2.2.2.1.1.1.2" class="ltx_tr">
<span id="S5.T4.2.2.2.1.1.1.2.1" class="ltx_td ltx_align_center">Context size</span></span>
<span id="S5.T4.2.2.2.1.1.1.1" class="ltx_tr">
<span id="S5.T4.2.2.2.1.1.1.1.1" class="ltx_td ltx_align_center">(<math id="S5.T4.2.2.2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S5.T4.2.2.2.1.1.1.1.1.m1.1a"><mrow id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.2" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.1" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mrow id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.cmml"><mn id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.2" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.2.cmml">1</mn><mo id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.3" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1a" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.4" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.1.1.1.1.m1.1b"><apply id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1"><times id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3"><times id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1"></times><cn type="integer" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.2">1</cn><ci id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.3">e</ci><cn type="integer" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.4.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.1.1.1.1.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.1.1.1.1.1.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</span></span>
</span></span> <span id="S5.T4.2.2.2.1.3" class="ltx_text"></span>
</td>
<td id="S5.T4.2.2.2.3" class="ltx_td ltx_align_right ltx_border_bb">256</td>
<td id="S5.T4.2.2.2.4" class="ltx_td ltx_align_right ltx_border_bb">128</td>
<td id="S5.T4.2.2.2.5" class="ltx_td ltx_align_right ltx_border_bb">2048</td>
<td id="S5.T4.2.2.2.6" class="ltx_td ltx_align_right ltx_border_bb">2048</td>
<td id="S5.T4.2.2.2.7" class="ltx_td ltx_align_right ltx_border_bb">1024</td>
</tr>
</tbody></table>
</span></div>
</div>
</div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluating Model Flops Utilization</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We evaluate the model flops utilization (MFU) of Ring Attention in standard training settings using fully sharded data parallelism(FSDP)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Facebook, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> and tensor parallelism following LLaMA and OpenLLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Touvron et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>; Geng and Liu, <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> with Jax SPMD.
The batch size in tokens are 2M on 8/32x A100 and 4M on TPUv4-256.
Our goal is investigating the impact of model size and context length on MFU, a critical performance metrics while highlighting the benefits of our approach.
Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Evaluating Max Context Size ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the results of our experiments on MFU for different model sizes and context lengths. We present the achieved MFU using state-of-the-art memory efficient transformers BPT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>, compare it to our anticipated MFU based on these results, and demonstrate the actual MFU obtained with our approach (Ring Attention).
For fair comparison, both BPT and our approach are based on the same BPT implementation on both GPUs and TPUs.
It’s worth noting that on GPUs our approach Ring Attention can be also integrated with the more compute efficient Triton code&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(kernel team, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> or CUDA code&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dao et&nbsp;al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> of memory efficient attention&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, similarly on TPUs it is also compatible with Pallas&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(team, <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. Combing these low level kernels implementations with our approach can maximize MFU, we leave that to future work.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Ring Attention trains much longer context sizes for self-attention, resulting in higher self-attention FLOPs compared to baseline models. Since self-attention has a lower MFU than feedforward, Ring Attention is expected to have a lower MFU than the baseline models.
Our method offers a clear advantage in terms of maintaining MFU while enabling training with significantly longer context lengths. As shown in Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Evaluating Max Context Size ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, when comparing our approach to prior state-of-the-arts, it is evident that we can train very large context models without compromising MFU or throughput.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2310.01889/assets/figures/context_acc.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="202" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of different models on the long-range line retrieval task.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Impact on LLM Performance</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We evaluate Ring Attention by applying our method to finetune LLaMA model to longer context.
In this experiment, while our approach enables training with millions of context tokens,
we conducted finetuning on the LLaMA-13B model, limiting the context length to 512K tokens due to constraints on our cloud compute budget.
This finetuning was carried out on 32 A100 GPUs, using the ShareGPT dataset, following methodologies as outlined in prior works&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Chiang et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2023</a>; Geng et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>.
We then evaluated our finetuned model on the line retrieval test&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. In this test, the model needs to precisely retrieve a number from a long document, the task can effectively capture the abilities of text generation, retrieval, and information association at long context, reflected by the retrieving accuracy.
Figure&nbsp;<a href="#S5.F3" title="Figure 3 ‣ 5.2 Evaluating Model Flops Utilization ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the accuracy results for different models across varying context lengths (measured in tokens). Notably, our model, Ring Attention-13B-512K, stands out as it maintains high accuracy levels even with long contexts.
GPT3.5-turbo-16K, Vicuna-16B-16K, and Claude-2-100K demonstrate competitive accuracy within short context lengths. However, they cannot handle extended context lengths.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Transformers have garnered significant attention in the field of AI and have become the backbone for numerous state-of-the-art models. Several works have explored memory-efficient techniques to address the memory limitations of Transformers and enable their application to a wider range of problems.
Computing exact self-attention in a blockwise manner using the tiling technique&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Milakov and Gimelshein, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite> has led to the development of memory efficient attention mechanisms&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite> and its efficient CUDA implementation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dao et&nbsp;al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>, and blockwise parallel transformer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite> that proposes computing both feedforward and self-attention block-by-block, resulting in a significant reduction in memory requirements.
In line with these advancements, our work falls into the category of memory efficient computation for Transformers.
Other works have investigated the approximation of attention mechanisms, yet these efforts have often yielded sub-optimal results or encountered challenges during scaling up. For an in-depth review of these techniques, we recommend referring to the surveys&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Narang et&nbsp;al., <a href="#bib.bib27" title="" class="ltx_ref">2021</a>; Tay et&nbsp;al., <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>.
Another avenue of research explores various parallelism methods, including data parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dean et&nbsp;al., <a href="#bib.bib10" title="" class="ltx_ref">2012</a>)</cite>, tensor parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shoeybi et&nbsp;al., <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>, pipeline parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Narayanan et&nbsp;al., <a href="#bib.bib28" title="" class="ltx_ref">2019</a>; Huang et&nbsp;al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Narayanan et&nbsp;al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>,
sequence parallelism&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Korthikanti et&nbsp;al., <a href="#bib.bib19" title="" class="ltx_ref">2022</a>; Jacobs et&nbsp;al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>, and FSDP&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Facebook, <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Rajbhandari et&nbsp;al., <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>.
The activations of self-attention take a substantial amount of memory for large context models. Tensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation.
Our work leverages on blockwise parallel transformers to distribute blockwise attention and feedforward across devices and concurrently overlaps the communication of key-value blocks in a circular of hosts with the computation of query-key-value blocks and feedforward, allowing number of devices times longer sequences with negligible overheads.
Overlapping communication with computation has been studied in high performance computing literature&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Danalis et&nbsp;al., <a href="#bib.bib7" title="" class="ltx_ref">2005</a>; Wang et&nbsp;al., <a href="#bib.bib40" title="" class="ltx_ref">2022</a>; Danalis et&nbsp;al., <a href="#bib.bib8" title="" class="ltx_ref">2009</a>, <span id="S6.p1.1.1.1" class="ltx_text ltx_font_italic">inter alia</span>)</cite>.
While ring communication has found applications in other parallel computing scenarios&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Bischof, <a href="#bib.bib2" title="" class="ltx_ref">2008</a>; Hursey and Graham, <a href="#bib.bib16" title="" class="ltx_ref">2011</a>; Gibiansky, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>; Sergeev and Del&nbsp;Balso, <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>, our work stands out as the first work to show that it can be applied to self-attention as used in Transformers and to make it fit efficiently into Transformer training and inference without adding significant overhead by overlapping blockwise computation and communication.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In conclusion, we propose a memory efficient approach to reduce the memory requirements of Transformers, the backbone of state-of-the-art AI models.
Our approach allows the context length to scale linearly with the number of devices while maintaining performance, eliminating the memory bottleneck imposed by individual devices.
Through extensive experiments on language modeling and reinforcement learning, we demonstrate its effectiveness, enabling training sequences that are up to device count times longer than those of prior memory-efficient Transformers, exceeding a context length of 100 million without making approximations to attention.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Limitations and Future Work.</span>
Although our method achieves state-of-the-art context length for Transformer models, it does have some limitations that need to be addressed:</p>
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p"><span id="S7.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Scaled up training</span>: due to compute budget constraint, our experiments focus on evaluation the effectiveness of the proposed approach without large scale training models.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p"><span id="S7.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Optimal compute performance</span>: While Ring Attention scales context length with device count while maintaining performance, optimizing low-level operations is required for achieving optimal compute performance. We suggest considering porting our method to CUDA, OpenAI Triton, or Jax Pallas in the future, for both maximum sequence length and maximum compute performance.</p>
</div>
</li>
</ul>
<p id="S7.p2.2" class="ltx_p">In terms of future prospects, the possibility of near-infinite context introduces a vast array of exciting opportunities, such as large video-audio-language models, learning from extended feedback and trial-and-errors, understanding and generating codebase, and adapting AI models to understand scientific data such as gene sequences.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This project is supported in part by Office of Naval Research grant N00014-21-1-2769.
We express our gratitude to the BAIR and RLL communities for their insightful discussions and feedback.
We are also thankful to David Patterson for addressing our questions about TPUs and giving insightful feedback on early versions of this work. Our appreciation goes out to Yash Katariya and Sharad Vikram from the Jax developers’ team for assisting with our Jax related questions.
We also thank Tri Dao for the valuable feedback on this work.
We thank Google TPU Research Cloud for granting us access to TPUs.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing claude, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.anthropic.com/index/introducing-claude" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.anthropic.com/index/introducing-claude</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bischof (2008)</span>
<span class="ltx_bibblock">
Christian Bischof.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Parallel computing: Architectures, algorithms, and
applications</em>, volume&nbsp;15.

</span>
<span class="ltx_bibblock">IOS Press, 2008.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.

</span>
<span class="ltx_bibblock">Decision transformer: Reinforcement learning via sequence modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
34:15084–15097, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.

</span>
<span class="ltx_bibblock">Training deep nets with sublinear memory cost.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1604.06174</em>, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E Gonzalez, et&nbsp;al.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">See https://vicuna.lmsys.org</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Danalis et&nbsp;al. (2005)</span>
<span class="ltx_bibblock">
Anthony Danalis, Ki-Yong Kim, Lori Pollock, and Martin Swany.

</span>
<span class="ltx_bibblock">Transformations to parallel codes for communication-computation
overlap.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">SC’05: Proceedings of the 2005 ACM/IEEE conference on
Supercomputing</em>, pages 58–58. IEEE, 2005.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Danalis et&nbsp;al. (2009)</span>
<span class="ltx_bibblock">
Anthony Danalis, Lori Pollock, Martin Swany, and John Cavazos.

</span>
<span class="ltx_bibblock">Mpi-aware compiler optimizations for improving
communication-computation overlap.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd international conference on
Supercomputing</em>, pages 316–325, 2009.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with
io-awareness.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:16344–16359, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dean et&nbsp;al. (2012)</span>
<span class="ltx_bibblock">
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke&nbsp;Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Large scale distributed deep networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 25, 2012.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Facebook (2023)</span>
<span class="ltx_bibblock">
Facebook.

</span>
<span class="ltx_bibblock">Fully Sharded Data Parallel: faster AI training with
fewer GPUs — engineering.fb.com.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://engineering.fb.com/2021/07/15/open-source/fsdp/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://engineering.fb.com/2021/07/15/open-source/fsdp/</a>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng and Liu (2023)</span>
<span class="ltx_bibblock">
Xinyang Geng and Hao Liu.

</span>
<span class="ltx_bibblock">Openllama: An open reproduction of llama, may 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">URL https://github. com/openlm-research/open_llama</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey
Levine, and Dawn Song.

</span>
<span class="ltx_bibblock">Koala: A dialogue model for academic research.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Blog post, April</em>, 1, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gibiansky (2017)</span>
<span class="ltx_bibblock">
Andrew Gibiansky.

</span>
<span class="ltx_bibblock">Bringing hpc techniques to deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Baidu Research, Tech. Rep.</em>, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
HyoukJoong Lee, Jiquan Ngiam, Quoc&nbsp;V Le, Yonghui Wu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpipe: Efficient training of giant neural networks using pipeline
parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hursey and Graham (2011)</span>
<span class="ltx_bibblock">
Joshua Hursey and Richard&nbsp;L Graham.

</span>
<span class="ltx_bibblock">Building a fault tolerant mpi application: A ring communication
example.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2011 IEEE International Symposium on Parallel and
Distributed Processing Workshops and Phd Forum</em>, pages 1549–1556. IEEE,
2011.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Sam&nbsp;Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song,
Samyam Rajbhandari, and Yuxiong He.

</span>
<span class="ltx_bibblock">Deepspeed ulysses: System optimizations for enabling training of
extreme long sequence transformer models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.14509</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">kernel team (2023)</span>
<span class="ltx_bibblock">
OpenAI kernel team.

</span>
<span class="ltx_bibblock">Openai triton fused attention, 2023.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korthikanti et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael
Andersch, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Reducing activation recomputation in large transformer models.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.05198</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laskin et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu,
Catherine Cang, Lerrel Pinto, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Urlb: Unsupervised reinforcement learning benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.15191</em>, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph
E.&nbsp;Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.

</span>
<span class="ltx_bibblock">How long can open-source llms truly promise on context length?, June
2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://lmsys.org/blog/2023-06-29-longchat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lmsys.org/blog/2023-06-29-longchat</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You.

</span>
<span class="ltx_bibblock">Sequence parallelism: Making 4d parallelism possible.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.13120</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Abbeel (2023a)</span>
<span class="ltx_bibblock">
Hao Liu and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Emergent agentic transformer from chain of hindsight experience.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>,
2023a.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Abbeel (2023b)</span>
<span class="ltx_bibblock">
Hao Liu and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Blockwise parallel transformer for large context models.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
2023b.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milakov and Gimelshein (2018)</span>
<span class="ltx_bibblock">
Maxim Milakov and Natalia Gimelshein.

</span>
<span class="ltx_bibblock">Online normalizer calculation for softmax.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.02867</em>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MosaicML (2023)</span>
<span class="ltx_bibblock">
MosaicML.

</span>
<span class="ltx_bibblock">Introducing mpt-7b: A new standard for open-source, commercially
usable llms, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.mosaicml.com/blog/mpt-7b" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mosaicml.com/blog/mpt-7b</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narang et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Sharan Narang, Hyung&nbsp;Won Chung, Yi&nbsp;Tay, William Fedus, Thibault Fevry, Michael
Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et&nbsp;al.

</span>
<span class="ltx_bibblock">Do transformer modifications transfer across implementations and
applications?

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.11972</em>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil&nbsp;R
Devanur, Gregory&nbsp;R Ganger, Phillip&nbsp;B Gibbons, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Pipedream: Generalized pipeline parallelism for dnn training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM Symposium on Operating Systems
Principles</em>, pages 1–15, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Memory-efficient pipeline-parallel dnn training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
7937–7947. PMLR, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rabe and Staats (2021)</span>
<span class="ltx_bibblock">
Markus&nbsp;N Rabe and Charles Staats.

</span>
<span class="ltx_bibblock">Self-attention does not need o(n2) memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.05682</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">Zero: Memory optimizations toward training trillion parameter models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis</em>, pages 1–16. IEEE, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
J.&nbsp;Schulman, B.&nbsp;Zoph, C.&nbsp;Kim, J.&nbsp;Hilton, J.&nbsp;Menick, J.&nbsp;Weng, J.&nbsp;F.&nbsp;C. Uribe,
L.&nbsp;Fedus, L.&nbsp;Metz, M.&nbsp;Pokorny, R.&nbsp;G. Lopes, S.&nbsp;Zhao, A.&nbsp;Vijayvergiya,
E.&nbsp;Sigler, A.&nbsp;Perelman, C.&nbsp;Voss, M.&nbsp;Heaton, J.&nbsp;Parish, D.&nbsp;Cummings, R.&nbsp;Nayak,
V.&nbsp;Balcom, D.&nbsp;Schnurr, T.&nbsp;Kaftan, C.&nbsp;Hallacy, N.&nbsp;Turley, N.&nbsp;Deutsch, and
V.&nbsp;Goel.

</span>
<span class="ltx_bibblock">Chatgpt: Optimizing language models for dialogue.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sergeev and Del&nbsp;Balso (2018)</span>
<span class="ltx_bibblock">
Alexander Sergeev and Mike Del&nbsp;Balso.

</span>
<span class="ltx_bibblock">Horovod: fast and easy distributed deep learning in tensorflow.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.05799</em>, 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-lm: Training multi-billion parameter language models using
model parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Mostafa Dehghani, Samira Abnar, Hyung&nbsp;Won Chung, William Fedus, Jinfeng
Rao, Sharan Narang, Vinh&nbsp;Q Tran, Dani Yogatama, and Donald Metzler.

</span>
<span class="ltx_bibblock">Scaling laws vs model architectures: How does inductive bias
influence scaling?

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.10551</em>, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">team (2023)</span>
<span class="ltx_bibblock">
Jax team.

</span>
<span class="ltx_bibblock">Jax pallas fused attention, 2023.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake
Hechtman, Dehao Chen, Karthik&nbsp;Srinivasa Murthy, Marcello Maggioni, Qiao
Zhang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Overlap communication with dependent computation via decomposition in
large deep learning models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on
Architectural Support for Programming Languages and Operating Systems, Volume
1</em>, pages 93–106, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yarats et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel,
Alessandro Lazaric, and Lerrel Pinto.

</span>
<span class="ltx_bibblock">Don’t change the algorithm, change the data: Exploratory data for
offline reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.13425</em>, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Code</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The implementation of Ring Attention in Jax is provided in Figure&nbsp;<a href="#A1.F4" title="Figure 4 ‣ Appendix A Code ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We use <span id="A1.p1.1.1" class="ltx_text ltx_font_typewriter">defvjp</span> function to define both the forward and backward passes, and use collective operation <span id="A1.p1.1.2" class="ltx_text ltx_font_typewriter">jax.lax.ppermute</span> to facilitate the exchange of key-value blocks among a ring of hosts.
The provided code snippet highlights essential components of Ring Attention.
We provide the complete code of our Ring Attention at <a target="_blank" href="https://github.com/lhao499/llm_large_context" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/lhao499/llm_large_context</a>.
It is worth mentioning that, for maximum MFU, Ring Attention can be integrated with exiting kernel-level fused-attention implementations, such as Ring Attention can be integrated with Triton / CUDA / Pallas based code&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[kernel team, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>, Dao et&nbsp;al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>, team, <a href="#bib.bib37" title="" class="ltx_ref">2023</a>]</cite>.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_font_bold">Practitioner Guide.</span>
For large scale end-to-end training on TPU or on GPU cluster with high bandwidth inter connection, we recommend using FSDP to shard large models and using Ring Attention to achieve large context. If total batch size is too large, add tensor parallelism to reduce the global batch size. The degree of parallelism can be adjusted using the <span id="A1.p2.1.2" class="ltx_text ltx_font_typewriter">mesh_dim</span> parameter within the codebase.
To illustrate, consider a setup with 512 devices, such as 512x A100. If the model size is 30B, you can shard it across 8 devices and allocate the remaining 32 devices for Ring Attention. This setup allows the context size to be expanded 32 times more than if you didn’t use Ring Attention. Conversely, for models sized 7B or 3B, there is no need for FSDP. This means you can utilize all 512 devices exclusively to expand the context using Ring Attention by 512 times. Building upon the result that our approach allows for a 256K context size when using 8x A100 GPUs, it suggests that by employing 512 A100 GPUs, the potential context size can be expanded to 16 million.</p>
</div>
<figure id="A1.F4" class="ltx_figure">
<div id="A1.F4.1" class="ltx_listing ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,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" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span id="lstnumberx2.1" class="ltx_text ltx_lst_identifier">def</span><span id="lstnumberx2.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.3" class="ltx_text ltx_lst_identifier">_ring_attention_fwd</span>(<span id="lstnumberx2.4" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx2.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.6" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx2.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.8" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx2.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.10" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx2.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.12" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx2.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.14" class="ltx_text ltx_lst_identifier">float32_logits</span>,<span id="lstnumberx2.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.16" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>):
</div>
<div id="lstnumberx3" class="ltx_listingline">
<span id="lstnumberx3.1" class="ltx_text ltx_lst_identifier">if</span><span id="lstnumberx3.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.3" class="ltx_text ltx_lst_identifier">float32_logits</span>:
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span id="lstnumberx4.1" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx4.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.3" class="ltx_text ltx_lst_identifier">k</span><span id="lstnumberx4.4" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx4.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.6" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx4.7" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx4.8" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx4.9" class="ltx_text ltx_lst_identifier">float32</span>),<span id="lstnumberx4.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.11" class="ltx_text ltx_lst_identifier">k</span>.<span id="lstnumberx4.12" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx4.13" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx4.14" class="ltx_text ltx_lst_identifier">float32</span>)
</div>
<div id="lstnumberx5" class="ltx_listingline">
<span id="lstnumberx5.1" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx5.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx5.3" class="ltx_text ltx_lst_identifier">q_len</span>,<span id="lstnumberx5.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx5.5" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx5.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx5.7" class="ltx_text ltx_lst_identifier">dim_per_head</span><span id="lstnumberx5.8" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx5.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx5.10" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx5.11" class="ltx_text ltx_lst_identifier">shape</span>
</div>
<div id="lstnumberx6" class="ltx_listingline">
<span id="lstnumberx6.1" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx6.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.3" class="ltx_text ltx_lst_identifier">kv_len</span>,<span id="lstnumberx6.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.5" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx6.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.7" class="ltx_text ltx_lst_identifier">dim_per_head</span><span id="lstnumberx6.8" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx6.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.10" class="ltx_text ltx_lst_identifier">k</span>.<span id="lstnumberx6.11" class="ltx_text ltx_lst_identifier">shape</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span id="lstnumberx7.1" class="ltx_text ltx_lst_identifier">numerator</span><span id="lstnumberx7.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx7.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx7.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx7.5" class="ltx_text ltx_lst_identifier">zeros</span>((<span id="lstnumberx7.6" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx7.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx7.8" class="ltx_text ltx_lst_identifier">q_len</span>,<span id="lstnumberx7.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx7.10" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx7.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx7.12" class="ltx_text ltx_lst_identifier">dim_per_head</span>)).<span id="lstnumberx7.13" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx7.14" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx7.15" class="ltx_text ltx_lst_identifier">dtype</span>)
</div>
<div id="lstnumberx8" class="ltx_listingline">
<span id="lstnumberx8.1" class="ltx_text ltx_lst_identifier">denominator</span><span id="lstnumberx8.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx8.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx8.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx8.5" class="ltx_text ltx_lst_identifier">zeros</span>((<span id="lstnumberx8.6" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx8.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx8.8" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx8.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx8.10" class="ltx_text ltx_lst_identifier">q_len</span>)).<span id="lstnumberx8.11" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx8.12" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx8.13" class="ltx_text ltx_lst_identifier">dtype</span>)
</div>
<div id="lstnumberx9" class="ltx_listingline">
<span id="lstnumberx9.1" class="ltx_text ltx_lst_identifier">axis_size</span><span id="lstnumberx9.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx9.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx9.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx9.5" class="ltx_text ltx_lst_identifier">psum</span>(1,<span id="lstnumberx9.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx9.7" class="ltx_text ltx_lst_identifier">axis_name</span>)
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span id="lstnumberx10.1" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx10.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx10.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.4" class="ltx_text ltx_lst_identifier">q_len</span><span id="lstnumberx10.5" class="ltx_text ltx_lst_space"> </span>#<span id="lstnumberx10.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.7" class="ltx_text ltx_lst_identifier">assumes</span><span id="lstnumberx10.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.9" class="ltx_text ltx_lst_identifier">this</span><span id="lstnumberx10.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.11" class="ltx_text ltx_lst_identifier">function</span><span id="lstnumberx10.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.13" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx10.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.15" class="ltx_text ltx_lst_identifier">pre</span>-<span id="lstnumberx10.16" class="ltx_text ltx_lst_identifier">sharded</span><span id="lstnumberx10.17" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.18" class="ltx_text ltx_lst_identifier">inside</span><span id="lstnumberx10.19" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.20" class="ltx_text ltx_lst_identifier">shard_map</span>
</div>
<div id="lstnumberx11" class="ltx_listingline">
<span id="lstnumberx11.1" class="ltx_text ltx_lst_identifier">query_chunk_size</span><span id="lstnumberx11.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx11.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx11.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>["<span id="lstnumberx11.5" class="ltx_text ltx_lst_identifier">query_chunk_size</span>"]
</div>
<div id="lstnumberx12" class="ltx_listingline">
<span id="lstnumberx12.1" class="ltx_text ltx_lst_identifier">key_chunk_size</span><span id="lstnumberx12.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx12.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx12.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>["<span id="lstnumberx12.5" class="ltx_text ltx_lst_identifier">key_chunk_size</span>"]
</div>
<div id="lstnumberx13" class="ltx_listingline">
<span id="lstnumberx13.1" class="ltx_text ltx_lst_identifier">def</span><span id="lstnumberx13.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx13.3" class="ltx_text ltx_lst_identifier">scan_kv_block</span>(<span id="lstnumberx13.4" class="ltx_text ltx_lst_identifier">carry</span>,<span id="lstnumberx13.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx13.6" class="ltx_text ltx_lst_identifier">idx</span>):
</div>
<div id="lstnumberx14" class="ltx_listingline">
<span id="lstnumberx14.1" class="ltx_text ltx_lst_identifier">prev_max_score</span>,<span id="lstnumberx14.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.3" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx14.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.5" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx14.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.7" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx14.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.9" class="ltx_text ltx_lst_identifier">v</span><span id="lstnumberx14.10" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx14.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.12" class="ltx_text ltx_lst_identifier">carry</span>
</div>
<div id="lstnumberx15" class="ltx_listingline">
<span id="lstnumberx15.1" class="ltx_text ltx_lst_identifier">attn_bias_slice</span><span id="lstnumberx15.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx15.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx15.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx15.5" class="ltx_text ltx_lst_identifier">dynamic_slice_in_dim</span>(<span id="lstnumberx15.6" class="ltx_text ltx_lst_identifier">attn_bias</span>,
</div>
<div id="lstnumberx16" class="ltx_listingline">(<span id="lstnumberx16.1" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx16.2" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx16.3" class="ltx_text ltx_lst_identifier">axis_name</span>)<span id="lstnumberx16.4" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx16.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx16.6" class="ltx_text ltx_lst_identifier">idx</span>)<span id="lstnumberx16.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx16.8" class="ltx_text ltx_lst_identifier">q_block_idx</span><span id="lstnumberx16.9" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx16.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx16.11" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx16.12" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx16.13" class="ltx_text ltx_lst_identifier">axis_name</span>)
</div>
<div id="lstnumberx17" class="ltx_listingline">
<span id="lstnumberx17.1" class="ltx_text ltx_lst_identifier">k_block_idx</span><span id="lstnumberx17.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx17.3" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx17.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx17.5" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx17.6" class="ltx_text ltx_lst_identifier">axis_name</span>)<span id="lstnumberx17.7" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx17.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx17.9" class="ltx_text ltx_lst_identifier">idx</span>)<span id="lstnumberx17.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx17.11" class="ltx_text ltx_lst_identifier">q_chunk_idx_start</span><span id="lstnumberx17.12" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx17.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx17.14" class="ltx_text ltx_lst_identifier">q_block_idx</span><span id="lstnumberx17.15" class="ltx_text ltx_lst_space"> </span>*<span id="lstnumberx17.16" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx17.17" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx17.18" class="ltx_text ltx_lst_space"> </span>//<span id="lstnumberx17.19" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx17.20" class="ltx_text ltx_lst_identifier">query_chunk_size</span>)
</div>
<div id="lstnumberx18" class="ltx_listingline">
<span id="lstnumberx18.1" class="ltx_text ltx_lst_identifier">k_chunk_idx_start</span><span id="lstnumberx18.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx18.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx18.4" class="ltx_text ltx_lst_identifier">k_block_idx</span><span id="lstnumberx18.5" class="ltx_text ltx_lst_space"> </span>*<span id="lstnumberx18.6" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx18.7" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx18.8" class="ltx_text ltx_lst_space"> </span>//<span id="lstnumberx18.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx18.10" class="ltx_text ltx_lst_identifier">key_chunk_size</span>)
</div>
<div id="lstnumberx19" class="ltx_listingline">
<span id="lstnumberx19.1" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx19.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.3" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx19.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.5" class="ltx_text ltx_lst_identifier">max_score</span><span id="lstnumberx19.6" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx19.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.8" class="ltx_text ltx_lst_identifier">_blockwise_attention_fwd</span>(<span id="lstnumberx19.9" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx19.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.11" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx19.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.13" class="ltx_text ltx_lst_identifier">v</span>,
</div>
<div id="lstnumberx20" class="ltx_listingline">(<span id="lstnumberx20.1" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx20.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.3" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx20.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.5" class="ltx_text ltx_lst_identifier">prev_max_score</span>),<span id="lstnumberx20.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.7" class="ltx_text ltx_lst_identifier">q_chunk_idx_start</span>,<span id="lstnumberx20.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.9" class="ltx_text ltx_lst_identifier">k_chunk_idx_start</span>,
</div>
<div id="lstnumberx21" class="ltx_listingline">
<span id="lstnumberx21.1" class="ltx_text ltx_lst_identifier">bias</span>=<span id="lstnumberx21.2" class="ltx_text ltx_lst_identifier">attn_bias_slice</span>,<span id="lstnumberx21.3" class="ltx_text ltx_lst_space"> </span>**<span id="lstnumberx21.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>)
</div>
<div id="lstnumberx22" class="ltx_listingline">
<span id="lstnumberx22.1" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx22.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.3" class="ltx_text ltx_lst_identifier">v</span><span id="lstnumberx22.4" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx22.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.6" class="ltx_text ltx_lst_identifier">map</span>(<span id="lstnumberx22.7" class="ltx_text ltx_lst_identifier">lambda</span><span id="lstnumberx22.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.9" class="ltx_text ltx_lst_identifier">x</span>:<span id="lstnumberx22.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.11" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx22.12" class="ltx_text ltx_lst_identifier">ppermute</span>(<span id="lstnumberx22.13" class="ltx_text ltx_lst_identifier">x</span>,<span id="lstnumberx22.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.15" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx22.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.17" class="ltx_text ltx_lst_identifier">perm</span>=[(<span id="lstnumberx22.18" class="ltx_text ltx_lst_identifier">i</span>,<span id="lstnumberx22.19" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx22.20" class="ltx_text ltx_lst_identifier">i</span><span id="lstnumberx22.21" class="ltx_text ltx_lst_space"> </span>+<span id="lstnumberx22.22" class="ltx_text ltx_lst_space"> </span>1)<span id="lstnumberx22.23" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.24" class="ltx_text ltx_lst_identifier">for</span><span id="lstnumberx22.25" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.26" class="ltx_text ltx_lst_identifier">i</span><span id="lstnumberx22.27" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.28" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx22.29" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.30" class="ltx_text ltx_lst_identifier">range</span>(<span id="lstnumberx22.31" class="ltx_text ltx_lst_identifier">axis_size</span>)]),<span id="lstnumberx22.32" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx22.33" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx22.34" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.35" class="ltx_text ltx_lst_identifier">v</span>))
</div>
<div id="lstnumberx23" class="ltx_listingline">
<span id="lstnumberx23.1" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx23.2" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx23.3" class="ltx_text ltx_lst_identifier">max_score</span>,<span id="lstnumberx23.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.5" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx23.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.7" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx23.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.9" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx23.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.11" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx23.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.13" class="ltx_text ltx_lst_identifier">None</span>
</div>
<div id="lstnumberx24" class="ltx_listingline">
<span id="lstnumberx24.1" class="ltx_text ltx_lst_identifier">prev_max_score</span><span id="lstnumberx24.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx24.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx24.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx24.5" class="ltx_text ltx_lst_identifier">full</span>((<span id="lstnumberx24.6" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx24.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx24.8" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx24.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx24.10" class="ltx_text ltx_lst_identifier">q_len</span>),<span id="lstnumberx24.11" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx24.12" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx24.13" class="ltx_text ltx_lst_identifier">inf</span>).<span id="lstnumberx24.14" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx24.15" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx24.16" class="ltx_text ltx_lst_identifier">dtype</span>)
</div>
<div id="lstnumberx25" class="ltx_listingline">(<span id="lstnumberx25.1" class="ltx_text ltx_lst_identifier">max_score</span>,<span id="lstnumberx25.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.3" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx25.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.5" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx25.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.7" class="ltx_text ltx_lst_identifier">_</span>,<span id="lstnumberx25.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.9" class="ltx_text ltx_lst_identifier">_</span>),<span id="lstnumberx25.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.11" class="ltx_text ltx_lst_identifier">_</span><span id="lstnumberx25.12" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx25.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.14" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx25.15" class="ltx_text ltx_lst_identifier">scan</span>(<span id="lstnumberx25.16" class="ltx_text ltx_lst_identifier">scan_kv_block</span>,
</div>
<div id="lstnumberx26" class="ltx_listingline">
<span id="lstnumberx26.1" class="ltx_text ltx_lst_identifier">init</span>=(<span id="lstnumberx26.2" class="ltx_text ltx_lst_identifier">prev_max_score</span>,<span id="lstnumberx26.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.4" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx26.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.6" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx26.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.8" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx26.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.10" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx26.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.12" class="ltx_text ltx_lst_identifier">xs</span>=<span id="lstnumberx26.13" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx26.14" class="ltx_text ltx_lst_identifier">arange</span>(0,<span id="lstnumberx26.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.16" class="ltx_text ltx_lst_identifier">axis_size</span>))
</div>
<div id="lstnumberx27" class="ltx_listingline">
<span id="lstnumberx27.1" class="ltx_text ltx_lst_identifier">output</span><span id="lstnumberx27.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx27.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.4" class="ltx_text ltx_lst_identifier">numerator</span><span id="lstnumberx27.5" class="ltx_text ltx_lst_space"> </span>/<span id="lstnumberx27.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.7" class="ltx_text ltx_lst_identifier">rearrange</span>(<span id="lstnumberx27.8" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx27.9" class="ltx_text ltx_lst_space"> </span>’<span id="lstnumberx27.10" class="ltx_text ltx_lst_identifier">b</span><span id="lstnumberx27.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.12" class="ltx_text ltx_lst_identifier">h</span><span id="lstnumberx27.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.14" class="ltx_text ltx_lst_identifier">q</span><span id="lstnumberx27.15" class="ltx_text ltx_lst_space"> </span>-&gt;<span id="lstnumberx27.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.17" class="ltx_text ltx_lst_identifier">b</span><span id="lstnumberx27.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.19" class="ltx_text ltx_lst_identifier">q</span><span id="lstnumberx27.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.21" class="ltx_text ltx_lst_identifier">h</span>’)[…,<span id="lstnumberx27.22" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.23" class="ltx_text ltx_lst_identifier">None</span>]
</div>
<div id="lstnumberx28" class="ltx_listingline">
<span id="lstnumberx28.1" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx28.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.3" class="ltx_text ltx_lst_identifier">output</span>.<span id="lstnumberx28.4" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx28.5" class="ltx_text ltx_lst_identifier">v</span>.<span id="lstnumberx28.6" class="ltx_text ltx_lst_identifier">dtype</span>),<span id="lstnumberx28.7" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx28.8" class="ltx_text ltx_lst_identifier">output</span>,<span id="lstnumberx28.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.10" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx28.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.12" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx28.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.14" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx28.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.16" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx28.17" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.18" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx28.19" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.20" class="ltx_text ltx_lst_identifier">max_score</span>)
</div>
<div id="lstnumberx29" class="ltx_listingline">\<span id="lstnumberx29.1" class="ltx_text ltx_lst_identifier">pardef</span><span id="lstnumberx29.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.3" class="ltx_text ltx_lst_identifier">_ring_attention_bwd</span>(<span id="lstnumberx29.4" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx29.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.6" class="ltx_text ltx_lst_identifier">float32_logits</span>,<span id="lstnumberx29.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.8" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>,<span id="lstnumberx29.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.10" class="ltx_text ltx_lst_identifier">res</span>,<span id="lstnumberx29.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.12" class="ltx_text ltx_lst_identifier">g</span>):
</div>
<div id="lstnumberx30" class="ltx_listingline">
<span id="lstnumberx30.1" class="ltx_text ltx_lst_identifier">output</span>,<span id="lstnumberx30.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.3" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx30.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.5" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx30.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.7" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx30.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.9" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx30.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.11" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx30.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.13" class="ltx_text ltx_lst_identifier">max_score</span><span id="lstnumberx30.14" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx30.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.16" class="ltx_text ltx_lst_identifier">res</span>
</div>
<div id="lstnumberx31" class="ltx_listingline">
<span id="lstnumberx31.1" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx31.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx31.3" class="ltx_text ltx_lst_identifier">kv_len</span>,<span id="lstnumberx31.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx31.5" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx31.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx31.7" class="ltx_text ltx_lst_identifier">dim_per_head</span><span id="lstnumberx31.8" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx31.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx31.10" class="ltx_text ltx_lst_identifier">k</span>.<span id="lstnumberx31.11" class="ltx_text ltx_lst_identifier">shape</span>
</div>
<div id="lstnumberx32" class="ltx_listingline">
<span id="lstnumberx32.1" class="ltx_text ltx_lst_identifier">axis_size</span><span id="lstnumberx32.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx32.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx32.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx32.5" class="ltx_text ltx_lst_identifier">psum</span>(1,<span id="lstnumberx32.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx32.7" class="ltx_text ltx_lst_identifier">axis_name</span>)
</div>
<div id="lstnumberx33" class="ltx_listingline">
<span id="lstnumberx33.1" class="ltx_text ltx_lst_identifier">dq</span><span id="lstnumberx33.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx33.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx33.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx33.5" class="ltx_text ltx_lst_identifier">zeros_like</span>(<span id="lstnumberx33.6" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx33.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx33.8" class="ltx_text ltx_lst_identifier">dtype</span>=<span id="lstnumberx33.9" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx33.10" class="ltx_text ltx_lst_identifier">float32</span>)
</div>
<div id="lstnumberx34" class="ltx_listingline">
<span id="lstnumberx34.1" class="ltx_text ltx_lst_identifier">dk</span><span id="lstnumberx34.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx34.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx34.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx34.5" class="ltx_text ltx_lst_identifier">zeros_like</span>(<span id="lstnumberx34.6" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx34.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx34.8" class="ltx_text ltx_lst_identifier">dtype</span>=<span id="lstnumberx34.9" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx34.10" class="ltx_text ltx_lst_identifier">float32</span>)
</div>
<div id="lstnumberx35" class="ltx_listingline">
<span id="lstnumberx35.1" class="ltx_text ltx_lst_identifier">dv</span><span id="lstnumberx35.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx35.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx35.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx35.5" class="ltx_text ltx_lst_identifier">zeros_like</span>(<span id="lstnumberx35.6" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx35.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx35.8" class="ltx_text ltx_lst_identifier">dtype</span>=<span id="lstnumberx35.9" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx35.10" class="ltx_text ltx_lst_identifier">float32</span>)
</div>
<div id="lstnumberx36" class="ltx_listingline">
<span id="lstnumberx36.1" class="ltx_text ltx_lst_identifier">query_chunk_size</span><span id="lstnumberx36.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx36.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx36.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>["<span id="lstnumberx36.5" class="ltx_text ltx_lst_identifier">query_chunk_size</span>"]
</div>
<div id="lstnumberx37" class="ltx_listingline">
<span id="lstnumberx37.1" class="ltx_text ltx_lst_identifier">key_chunk_size</span><span id="lstnumberx37.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx37.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx37.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>["<span id="lstnumberx37.5" class="ltx_text ltx_lst_identifier">key_chunk_size</span>"]
</div>
<div id="lstnumberx38" class="ltx_listingline">
<span id="lstnumberx38.1" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx38.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx38.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.4" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx38.5" class="ltx_text ltx_lst_identifier">shape</span>[1]<span id="lstnumberx38.6" class="ltx_text ltx_lst_space"> </span>#<span id="lstnumberx38.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.8" class="ltx_text ltx_lst_identifier">assumes</span><span id="lstnumberx38.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.10" class="ltx_text ltx_lst_identifier">this</span><span id="lstnumberx38.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.12" class="ltx_text ltx_lst_identifier">function</span><span id="lstnumberx38.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.14" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx38.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.16" class="ltx_text ltx_lst_identifier">pre</span>-<span id="lstnumberx38.17" class="ltx_text ltx_lst_identifier">sharded</span><span id="lstnumberx38.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.19" class="ltx_text ltx_lst_identifier">inside</span><span id="lstnumberx38.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.21" class="ltx_text ltx_lst_identifier">shard_map</span>
</div>
<div id="lstnumberx39" class="ltx_listingline">
<span id="lstnumberx39.1" class="ltx_text ltx_lst_identifier">def</span><span id="lstnumberx39.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx39.3" class="ltx_text ltx_lst_identifier">scan_kv_block</span>(<span id="lstnumberx39.4" class="ltx_text ltx_lst_identifier">carry</span>,<span id="lstnumberx39.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx39.6" class="ltx_text ltx_lst_identifier">idx</span>):
</div>
<div id="lstnumberx40" class="ltx_listingline">
<span id="lstnumberx40.1" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx40.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.3" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx40.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.5" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx40.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.7" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx40.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.9" class="ltx_text ltx_lst_identifier">v</span><span id="lstnumberx40.10" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx40.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.12" class="ltx_text ltx_lst_identifier">carry</span>
</div>
<div id="lstnumberx41" class="ltx_listingline">
<span id="lstnumberx41.1" class="ltx_text ltx_lst_identifier">attn_bias_slice</span><span id="lstnumberx41.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx41.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx41.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx41.5" class="ltx_text ltx_lst_identifier">dynamic_slice_in_dim</span>(<span id="lstnumberx41.6" class="ltx_text ltx_lst_identifier">attn_bias</span>,
</div>
<div id="lstnumberx42" class="ltx_listingline">(<span id="lstnumberx42.1" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx42.2" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx42.3" class="ltx_text ltx_lst_identifier">axis_name</span>)<span id="lstnumberx42.4" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx42.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.6" class="ltx_text ltx_lst_identifier">idx</span>)<span id="lstnumberx42.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.8" class="ltx_text ltx_lst_identifier">q_block_idx</span><span id="lstnumberx42.9" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx42.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.11" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx42.12" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx42.13" class="ltx_text ltx_lst_identifier">axis_name</span>)
</div>
<div id="lstnumberx43" class="ltx_listingline">
<span id="lstnumberx43.1" class="ltx_text ltx_lst_identifier">k_block_idx</span><span id="lstnumberx43.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx43.3" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx43.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx43.5" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx43.6" class="ltx_text ltx_lst_identifier">axis_name</span>)<span id="lstnumberx43.7" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx43.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx43.9" class="ltx_text ltx_lst_identifier">idx</span>)<span id="lstnumberx43.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx43.11" class="ltx_text ltx_lst_identifier">q_chunk_idx_start</span><span id="lstnumberx43.12" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx43.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx43.14" class="ltx_text ltx_lst_identifier">q_block_idx</span><span id="lstnumberx43.15" class="ltx_text ltx_lst_space"> </span>*<span id="lstnumberx43.16" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx43.17" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx43.18" class="ltx_text ltx_lst_space"> </span>//<span id="lstnumberx43.19" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx43.20" class="ltx_text ltx_lst_identifier">query_chunk_size</span>)
</div>
<div id="lstnumberx44" class="ltx_listingline">
<span id="lstnumberx44.1" class="ltx_text ltx_lst_identifier">k_chunk_idx_start</span><span id="lstnumberx44.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx44.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx44.4" class="ltx_text ltx_lst_identifier">k_block_idx</span><span id="lstnumberx44.5" class="ltx_text ltx_lst_space"> </span>*<span id="lstnumberx44.6" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx44.7" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx44.8" class="ltx_text ltx_lst_space"> </span>//<span id="lstnumberx44.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx44.10" class="ltx_text ltx_lst_identifier">key_chunk_size</span>)
</div>
<div id="lstnumberx45" class="ltx_listingline">
<span id="lstnumberx45.1" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx45.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.3" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx45.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.5" class="ltx_text ltx_lst_identifier">dv</span><span id="lstnumberx45.6" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx45.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.8" class="ltx_text ltx_lst_identifier">_blockwise_attention_bwd</span>(<span id="lstnumberx45.9" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx45.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.11" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx45.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.13" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx45.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.15" class="ltx_text ltx_lst_identifier">g</span>,<span id="lstnumberx45.16" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx45.17" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx45.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.19" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx45.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.21" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx45.22" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.23" class="ltx_text ltx_lst_identifier">output</span>,<span id="lstnumberx45.24" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.25" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx45.26" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.27" class="ltx_text ltx_lst_identifier">max_score</span>),
</div>
<div id="lstnumberx46" class="ltx_listingline">
<span id="lstnumberx46.1" class="ltx_text ltx_lst_identifier">q_chunk_idx_start</span>,<span id="lstnumberx46.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx46.3" class="ltx_text ltx_lst_identifier">k_chunk_idx_start</span>,<span id="lstnumberx46.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx46.5" class="ltx_text ltx_lst_identifier">bias</span>=<span id="lstnumberx46.6" class="ltx_text ltx_lst_identifier">attn_bias_slice</span>,<span id="lstnumberx46.7" class="ltx_text ltx_lst_space"> </span>**<span id="lstnumberx46.8" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>)
</div>
<div id="lstnumberx47" class="ltx_listingline">
<span id="lstnumberx47.1" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx47.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.3" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx47.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.5" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx47.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.7" class="ltx_text ltx_lst_identifier">dv</span><span id="lstnumberx47.8" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx47.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.10" class="ltx_text ltx_lst_identifier">map</span>(<span id="lstnumberx47.11" class="ltx_text ltx_lst_identifier">lambda</span><span id="lstnumberx47.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.13" class="ltx_text ltx_lst_identifier">x</span>:<span id="lstnumberx47.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.15" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx47.16" class="ltx_text ltx_lst_identifier">ppermute</span>(<span id="lstnumberx47.17" class="ltx_text ltx_lst_identifier">x</span>,<span id="lstnumberx47.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.19" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx47.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.21" class="ltx_text ltx_lst_identifier">perm</span>=[(<span id="lstnumberx47.22" class="ltx_text ltx_lst_identifier">i</span>,
</div>
<div id="lstnumberx48" class="ltx_listingline">(<span id="lstnumberx48.1" class="ltx_text ltx_lst_identifier">i</span><span id="lstnumberx48.2" class="ltx_text ltx_lst_space"> </span>+<span id="lstnumberx48.3" class="ltx_text ltx_lst_space"> </span>1)<span id="lstnumberx48.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.5" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx48.6" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx48.7" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx48.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.9" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx48.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.11" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx48.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.13" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx48.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.15" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx48.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.17" class="ltx_text ltx_lst_identifier">None</span>
</div>
<div id="lstnumberx49" class="ltx_listingline">(<span id="lstnumberx49.1" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx49.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.3" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx49.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.5" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx49.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.7" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx49.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.9" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx49.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.11" class="ltx_text ltx_lst_identifier">_</span><span id="lstnumberx49.12" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx49.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.14" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx49.15" class="ltx_text ltx_lst_identifier">scan</span>(<span id="lstnumberx49.16" class="ltx_text ltx_lst_identifier">scan_kv_block</span>,<span id="lstnumberx49.17" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.18" class="ltx_text ltx_lst_identifier">init</span>=(<span id="lstnumberx49.19" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx49.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.21" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx49.22" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.23" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx49.24" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.25" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx49.26" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.27" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx49.28" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.29" class="ltx_text ltx_lst_identifier">xs</span>=<span id="lstnumberx49.30" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx49.31" class="ltx_text ltx_lst_identifier">arange</span>(0,<span id="lstnumberx49.32" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.33" class="ltx_text ltx_lst_identifier">axis_size</span>))
</div>
<div id="lstnumberx50" class="ltx_listingline">
<span id="lstnumberx50.1" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx50.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.3" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx50.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.5" class="ltx_text ltx_lst_identifier">dv</span><span id="lstnumberx50.6" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx50.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.8" class="ltx_text ltx_lst_identifier">dq</span>.<span id="lstnumberx50.9" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx50.10" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx50.11" class="ltx_text ltx_lst_identifier">dtype</span>),<span id="lstnumberx50.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.13" class="ltx_text ltx_lst_identifier">dk</span>.<span id="lstnumberx50.14" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx50.15" class="ltx_text ltx_lst_identifier">k</span>.<span id="lstnumberx50.16" class="ltx_text ltx_lst_identifier">dtype</span>),<span id="lstnumberx50.17" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.18" class="ltx_text ltx_lst_identifier">dv</span>.<span id="lstnumberx50.19" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx50.20" class="ltx_text ltx_lst_identifier">v</span>.<span id="lstnumberx50.21" class="ltx_text ltx_lst_identifier">dtype</span>)
</div>
<div id="lstnumberx51" class="ltx_listingline">
<span id="lstnumberx51.1" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx51.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx51.3" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx51.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx51.5" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx51.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx51.7" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx51.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx51.9" class="ltx_text ltx_lst_identifier">None</span>
</div>
<div id="lstnumberx52" class="ltx_listingline">\<span id="lstnumberx52.1" class="ltx_text ltx_lst_identifier">par@partial</span>(<span id="lstnumberx52.2" class="ltx_text ltx_lst_identifier">jax</span>.<span id="lstnumberx52.3" class="ltx_text ltx_lst_identifier">custom_vjp</span>,<span id="lstnumberx52.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx52.5" class="ltx_text ltx_lst_identifier">nondiff_argnums</span>=[4,<span id="lstnumberx52.6" class="ltx_text ltx_lst_space"> </span>5,<span id="lstnumberx52.7" class="ltx_text ltx_lst_space"> </span>6])
</div>
<div id="lstnumberx53" class="ltx_listingline">
<span id="lstnumberx53.1" class="ltx_text ltx_lst_identifier">def</span><span id="lstnumberx53.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.3" class="ltx_text ltx_lst_identifier">ring_attention</span>(<span id="lstnumberx53.4" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx53.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.6" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx53.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.8" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx53.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.10" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx53.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.12" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx53.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.14" class="ltx_text ltx_lst_identifier">float32_logits</span>,<span id="lstnumberx53.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.16" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>):
</div>
<div id="lstnumberx54" class="ltx_listingline">
<span id="lstnumberx54.1" class="ltx_text ltx_lst_identifier">y</span>,<span id="lstnumberx54.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.3" class="ltx_text ltx_lst_identifier">_</span><span id="lstnumberx54.4" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx54.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.6" class="ltx_text ltx_lst_identifier">_ring_attention_fwd</span>(<span id="lstnumberx54.7" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx54.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.9" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx54.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.11" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx54.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.13" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx54.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.15" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx54.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.17" class="ltx_text ltx_lst_identifier">float32_logits</span>,<span id="lstnumberx54.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.19" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>)
</div>
<div id="lstnumberx55" class="ltx_listingline">
<span id="lstnumberx55.1" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx55.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx55.3" class="ltx_text ltx_lst_identifier">y</span>
</div>
<div id="lstnumberx56" class="ltx_listingline">\<span id="lstnumberx56.1" class="ltx_text ltx_lst_identifier">parring_attention</span>.<span id="lstnumberx56.2" class="ltx_text ltx_lst_identifier">defvjp</span>(<span id="lstnumberx56.3" class="ltx_text ltx_lst_identifier">_ring_attention_fwd</span>,<span id="lstnumberx56.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx56.5" class="ltx_text ltx_lst_identifier">_ring_attention_bwd</span>)
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Key parts of the implementation of Ring Attention in Jax. We use collective operation <span id="A1.F4.3.1" class="ltx_text ltx_font_typewriter">lax.ppermute</span> to send and receive key value blocks between previous and next hosts.</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Release Note</h2>

<div id="A2.p1" class="ltx_para">
<ol id="A2.I1" class="ltx_enumerate">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">v1</span> (3rd October 2023): Release of Ring Attention.</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">v2</span> (5th October 2023):</p>
<ul id="A2.I1.i2.I1" class="ltx_itemize">
<li id="A2.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i2.I1.i1.p1.1" class="ltx_p">Fixed several typos.</p>
</div>
</li>
<li id="A2.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.I1.i2.p1.1" class="ltx_p">Clarified interconnect bandwidth.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">v3</span> (11th October 2023):</p>
<ul id="A2.I1.i3.I1" class="ltx_itemize">
<li id="A2.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i3.I1.i1.p1.1" class="ltx_p">Released the code of Ring Attention.</p>
</div>
</li>
<li id="A2.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i3.I1.i2.p1.1" class="ltx_p">Updated maximum context length experiments to use large-scale end-to-end FSDP training setting.</p>
</div>
</li>
<li id="A2.I1.i3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.I1.i3.p1.1" class="ltx_p">Updated MFU evaluation experiments to use large-scale end-to-end FSDP training setting.</p>
</div>
</li>
<li id="A2.I1.i3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i3.I1.i4.p1.1" class="ltx_p">Added a practitioner guide for using Ring Attention.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Experiment Details</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Evaluation of context length</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">In the experimental results presented in Section&nbsp;<a href="#S5.SS1" title="5.1 Evaluating Max Context Size ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>, we used fully sharded tensor parallelism (FSDP) to partition the model across GPUs or TPU devices.
Our evaluation focused on determining the maximum achievable sequence length in commonly used FSDP training scenarios.
For TPUs, we utilized its default training configuration, which involved performing matmul operations in <span id="A3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">bfloat16</span> format with weight accumulation in <span id="A3.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">float32</span>. On the other hand, for GPUs, we adopted the default setup, where all operations were performed in <span id="A3.SS1.p1.1.3" class="ltx_text ltx_font_typewriter">float32</span>.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Evaluation of MFU</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">In the evaluation presented in Section&nbsp;<a href="#S5.SS2" title="5.2 Evaluating Model Flops Utilization ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
The batch size in tokens is 2 million per batch on GPU and 4 million per batch on TPU. The training was conducted using FSDP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Facebook [<a href="#bib.bib11" title="" class="ltx_ref">2023</a>]</cite> with Jax SPMD.
For gradient checkpointing&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Chen et&nbsp;al., <a href="#bib.bib5" title="" class="ltx_ref">2016</a>]</cite>, we used <span id="A3.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">nothing_saveable</span> as checkpointing policies for attention and feedforward network (FFN). For more details, please refer to Jax documentation.</p>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Evaluation on line retrieval</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p id="A3.SS3.p1.1" class="ltx_p">In the evaluation presented in Section&nbsp;<a href="#S5.SS3" title="5.3 Impact on LLM Performance ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>, we finetuned the LLaMA-13B model&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Touvron et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>]</cite>, limiting context length to 512K tokens due to constraints on our cloud compute budget, the training was conducted on 32x A100 80GB Cloud GPUs.
We use user-shared conversations gathered from ShareGPT.com with its public APIs for finetuning, following methodologies as outlined in prior works&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Chiang et&nbsp;al., <a href="#bib.bib6" title="" class="ltx_ref">2023</a>, Geng et&nbsp;al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>]</cite>. ShareGPT is a website where users can share their ChatGPT conversations. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples, which results in 125K conversations after data cleaning.</p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Training FLOPs Scaling of Context Size</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.8" class="ltx_p">Given that our proposed approach unlocks the possibility of training with a context size exceeding 100 million tokens and allows for linear scaling of the context size based on the number of devices, it is essential to understand how the training FLOPs per dataset scale with the context size. While a larger context size results in a higher number of FLOPs, the increased ratio does not scale quadratically because the number of tokens remains fixed. We present these results in Figure&nbsp;<a href="#A4.F5" title="Figure 5 ‣ Appendix D Training FLOPs Scaling of Context Size ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, which showcases various model sizes and context lengths, representing different computational budgets. The figure illustrates the ratio of FLOPs for larger context lengths compared to the same model with a shorter 4K context size.
We calculated the per sequence FLOPs using <math id="A4.p1.1.m1.1" class="ltx_Math" alttext="(24bsh^{2}+4bs^{2}h)n" display="inline"><semantics id="A4.p1.1.m1.1a"><mrow id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml"><mrow id="A4.p1.1.m1.1.1.1.1" xref="A4.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="A4.p1.1.m1.1.1.1.1.2" xref="A4.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="A4.p1.1.m1.1.1.1.1.1" xref="A4.p1.1.m1.1.1.1.1.1.cmml"><mrow id="A4.p1.1.m1.1.1.1.1.1.2" xref="A4.p1.1.m1.1.1.1.1.1.2.cmml"><mn id="A4.p1.1.m1.1.1.1.1.1.2.2" xref="A4.p1.1.m1.1.1.1.1.1.2.2.cmml">24</mn><mo id="A4.p1.1.m1.1.1.1.1.1.2.1" xref="A4.p1.1.m1.1.1.1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.1.m1.1.1.1.1.1.2.3" xref="A4.p1.1.m1.1.1.1.1.1.2.3.cmml">b</mi><mo id="A4.p1.1.m1.1.1.1.1.1.2.1a" xref="A4.p1.1.m1.1.1.1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.1.m1.1.1.1.1.1.2.4" xref="A4.p1.1.m1.1.1.1.1.1.2.4.cmml">s</mi><mo id="A4.p1.1.m1.1.1.1.1.1.2.1b" xref="A4.p1.1.m1.1.1.1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><msup id="A4.p1.1.m1.1.1.1.1.1.2.5" xref="A4.p1.1.m1.1.1.1.1.1.2.5.cmml"><mi id="A4.p1.1.m1.1.1.1.1.1.2.5.2" xref="A4.p1.1.m1.1.1.1.1.1.2.5.2.cmml">h</mi><mn id="A4.p1.1.m1.1.1.1.1.1.2.5.3" xref="A4.p1.1.m1.1.1.1.1.1.2.5.3.cmml">2</mn></msup></mrow><mo id="A4.p1.1.m1.1.1.1.1.1.1" xref="A4.p1.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="A4.p1.1.m1.1.1.1.1.1.3" xref="A4.p1.1.m1.1.1.1.1.1.3.cmml"><mn id="A4.p1.1.m1.1.1.1.1.1.3.2" xref="A4.p1.1.m1.1.1.1.1.1.3.2.cmml">4</mn><mo id="A4.p1.1.m1.1.1.1.1.1.3.1" xref="A4.p1.1.m1.1.1.1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.1.m1.1.1.1.1.1.3.3" xref="A4.p1.1.m1.1.1.1.1.1.3.3.cmml">b</mi><mo id="A4.p1.1.m1.1.1.1.1.1.3.1a" xref="A4.p1.1.m1.1.1.1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><msup id="A4.p1.1.m1.1.1.1.1.1.3.4" xref="A4.p1.1.m1.1.1.1.1.1.3.4.cmml"><mi id="A4.p1.1.m1.1.1.1.1.1.3.4.2" xref="A4.p1.1.m1.1.1.1.1.1.3.4.2.cmml">s</mi><mn id="A4.p1.1.m1.1.1.1.1.1.3.4.3" xref="A4.p1.1.m1.1.1.1.1.1.3.4.3.cmml">2</mn></msup><mo id="A4.p1.1.m1.1.1.1.1.1.3.1b" xref="A4.p1.1.m1.1.1.1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.1.m1.1.1.1.1.1.3.5" xref="A4.p1.1.m1.1.1.1.1.1.3.5.cmml">h</mi></mrow></mrow><mo stretchy="false" id="A4.p1.1.m1.1.1.1.1.3" xref="A4.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A4.p1.1.m1.1.1.2" xref="A4.p1.1.m1.1.1.2.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.1.m1.1.1.3" xref="A4.p1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><apply id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1"><times id="A4.p1.1.m1.1.1.2.cmml" xref="A4.p1.1.m1.1.1.2"></times><apply id="A4.p1.1.m1.1.1.1.1.1.cmml" xref="A4.p1.1.m1.1.1.1.1"><plus id="A4.p1.1.m1.1.1.1.1.1.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.1"></plus><apply id="A4.p1.1.m1.1.1.1.1.1.2.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2"><times id="A4.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.1"></times><cn type="integer" id="A4.p1.1.m1.1.1.1.1.1.2.2.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.2">24</cn><ci id="A4.p1.1.m1.1.1.1.1.1.2.3.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.3">𝑏</ci><ci id="A4.p1.1.m1.1.1.1.1.1.2.4.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.4">𝑠</ci><apply id="A4.p1.1.m1.1.1.1.1.1.2.5.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.5"><csymbol cd="ambiguous" id="A4.p1.1.m1.1.1.1.1.1.2.5.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.5">superscript</csymbol><ci id="A4.p1.1.m1.1.1.1.1.1.2.5.2.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.5.2">ℎ</ci><cn type="integer" id="A4.p1.1.m1.1.1.1.1.1.2.5.3.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.5.3">2</cn></apply></apply><apply id="A4.p1.1.m1.1.1.1.1.1.3.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3"><times id="A4.p1.1.m1.1.1.1.1.1.3.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.1"></times><cn type="integer" id="A4.p1.1.m1.1.1.1.1.1.3.2.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.2">4</cn><ci id="A4.p1.1.m1.1.1.1.1.1.3.3.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.3">𝑏</ci><apply id="A4.p1.1.m1.1.1.1.1.1.3.4.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.1.m1.1.1.1.1.1.3.4.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.4">superscript</csymbol><ci id="A4.p1.1.m1.1.1.1.1.1.3.4.2.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.4.2">𝑠</ci><cn type="integer" id="A4.p1.1.m1.1.1.1.1.1.3.4.3.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.4.3">2</cn></apply><ci id="A4.p1.1.m1.1.1.1.1.1.3.5.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.5">ℎ</ci></apply></apply><ci id="A4.p1.1.m1.1.1.3.cmml" xref="A4.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">(24bsh^{2}+4bs^{2}h)n</annotation><annotation encoding="application/x-llamapun" id="A4.p1.1.m1.1d">( 24 italic_b italic_s italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 4 italic_b italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_h ) italic_n</annotation></semantics></math> where <math id="A4.p1.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="A4.p1.2.m2.1a"><mi id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><ci id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">h</annotation><annotation encoding="application/x-llamapun" id="A4.p1.2.m2.1d">italic_h</annotation></semantics></math> is model hidden dimension, <math id="A4.p1.3.m3.1" class="ltx_Math" alttext="b" display="inline"><semantics id="A4.p1.3.m3.1a"><mi id="A4.p1.3.m3.1.1" xref="A4.p1.3.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="A4.p1.3.m3.1b"><ci id="A4.p1.3.m3.1.1.cmml" xref="A4.p1.3.m3.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m3.1c">b</annotation><annotation encoding="application/x-llamapun" id="A4.p1.3.m3.1d">italic_b</annotation></semantics></math> is batch size, <math id="A4.p1.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="A4.p1.4.m4.1a"><mi id="A4.p1.4.m4.1.1" xref="A4.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="A4.p1.4.m4.1b"><ci id="A4.p1.4.m4.1.1.cmml" xref="A4.p1.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="A4.p1.4.m4.1d">italic_s</annotation></semantics></math> is total sequence length, and <math id="A4.p1.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A4.p1.5.m5.1a"><mi id="A4.p1.5.m5.1.1" xref="A4.p1.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A4.p1.5.m5.1b"><ci id="A4.p1.5.m5.1.1.cmml" xref="A4.p1.5.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.5.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="A4.p1.5.m5.1d">italic_n</annotation></semantics></math> is number of layers.
The per dataset FLOPs ratio is then given by <math id="A4.p1.6.m6.4" class="ltx_Math" alttext="((24bs_{2}h^{2}+4b{s_{2}}^{2}h)/(24bs_{1}h^{2}+4b{s_{1}}^{2}h))/(s_{2}/s_{1})=(6h+s_{2})/(6h+s_{1})" display="inline"><semantics id="A4.p1.6.m6.4a"><mrow id="A4.p1.6.m6.4.4" xref="A4.p1.6.m6.4.4.cmml"><mrow id="A4.p1.6.m6.2.2.2" xref="A4.p1.6.m6.2.2.2.cmml"><mrow id="A4.p1.6.m6.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A4.p1.6.m6.1.1.1.1.1.2" xref="A4.p1.6.m6.1.1.1.1.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.cmml"><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml"><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.cmml"><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.2.cmml">24</mn><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.3.cmml">b</mi><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1a" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><msub id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.2.cmml">s</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.3.cmml">2</mn></msub><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1b" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><msup id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.2.cmml">h</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.3.cmml">2</mn></msup></mrow><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.2.cmml">4</mn><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.3.cmml">b</mi><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1a" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mmultiscripts id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.2.cmml">s</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.3.cmml">2</mn><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4a" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.cmml"></mrow><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4b" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.cmml"></mrow><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.3.cmml">2</mn></mmultiscripts><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1b" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.5" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.5.cmml">h</mi></mrow></mrow><mo stretchy="false" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A4.p1.6.m6.1.1.1.1.1.1.3" xref="A4.p1.6.m6.1.1.1.1.1.1.3.cmml">/</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml"><mo stretchy="false" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml"><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.cmml"><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.2.cmml">24</mn><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.3.cmml">b</mi><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1a" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><msub id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.2.cmml">s</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.3.cmml">1</mn></msub><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1b" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><msup id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.2.cmml">h</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.3.cmml">2</mn></msup></mrow><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.1.cmml">+</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.cmml"><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.2.cmml">4</mn><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.3.cmml">b</mi><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1a" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mmultiscripts id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.2.cmml">s</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.3.cmml">1</mn><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4a" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.cmml"></mrow><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4b" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.cmml"></mrow><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.3.cmml">2</mn></mmultiscripts><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1b" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.5" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.5.cmml">h</mi></mrow></mrow><mo stretchy="false" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="A4.p1.6.m6.1.1.1.1.1.3" xref="A4.p1.6.m6.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A4.p1.6.m6.2.2.2.3" xref="A4.p1.6.m6.2.2.2.3.cmml">/</mo><mrow id="A4.p1.6.m6.2.2.2.2.1" xref="A4.p1.6.m6.2.2.2.2.1.1.cmml"><mo stretchy="false" id="A4.p1.6.m6.2.2.2.2.1.2" xref="A4.p1.6.m6.2.2.2.2.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.2.2.2.2.1.1" xref="A4.p1.6.m6.2.2.2.2.1.1.cmml"><msub id="A4.p1.6.m6.2.2.2.2.1.1.2" xref="A4.p1.6.m6.2.2.2.2.1.1.2.cmml"><mi id="A4.p1.6.m6.2.2.2.2.1.1.2.2" xref="A4.p1.6.m6.2.2.2.2.1.1.2.2.cmml">s</mi><mn id="A4.p1.6.m6.2.2.2.2.1.1.2.3" xref="A4.p1.6.m6.2.2.2.2.1.1.2.3.cmml">2</mn></msub><mo id="A4.p1.6.m6.2.2.2.2.1.1.1" xref="A4.p1.6.m6.2.2.2.2.1.1.1.cmml">/</mo><msub id="A4.p1.6.m6.2.2.2.2.1.1.3" xref="A4.p1.6.m6.2.2.2.2.1.1.3.cmml"><mi id="A4.p1.6.m6.2.2.2.2.1.1.3.2" xref="A4.p1.6.m6.2.2.2.2.1.1.3.2.cmml">s</mi><mn id="A4.p1.6.m6.2.2.2.2.1.1.3.3" xref="A4.p1.6.m6.2.2.2.2.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="A4.p1.6.m6.2.2.2.2.1.3" xref="A4.p1.6.m6.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="A4.p1.6.m6.4.4.5" xref="A4.p1.6.m6.4.4.5.cmml">=</mo><mrow id="A4.p1.6.m6.4.4.4" xref="A4.p1.6.m6.4.4.4.cmml"><mrow id="A4.p1.6.m6.3.3.3.1.1" xref="A4.p1.6.m6.3.3.3.1.1.1.cmml"><mo stretchy="false" id="A4.p1.6.m6.3.3.3.1.1.2" xref="A4.p1.6.m6.3.3.3.1.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.3.3.3.1.1.1" xref="A4.p1.6.m6.3.3.3.1.1.1.cmml"><mrow id="A4.p1.6.m6.3.3.3.1.1.1.2" xref="A4.p1.6.m6.3.3.3.1.1.1.2.cmml"><mn id="A4.p1.6.m6.3.3.3.1.1.1.2.2" xref="A4.p1.6.m6.3.3.3.1.1.1.2.2.cmml">6</mn><mo id="A4.p1.6.m6.3.3.3.1.1.1.2.1" xref="A4.p1.6.m6.3.3.3.1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.6.m6.3.3.3.1.1.1.2.3" xref="A4.p1.6.m6.3.3.3.1.1.1.2.3.cmml">h</mi></mrow><mo id="A4.p1.6.m6.3.3.3.1.1.1.1" xref="A4.p1.6.m6.3.3.3.1.1.1.1.cmml">+</mo><msub id="A4.p1.6.m6.3.3.3.1.1.1.3" xref="A4.p1.6.m6.3.3.3.1.1.1.3.cmml"><mi id="A4.p1.6.m6.3.3.3.1.1.1.3.2" xref="A4.p1.6.m6.3.3.3.1.1.1.3.2.cmml">s</mi><mn id="A4.p1.6.m6.3.3.3.1.1.1.3.3" xref="A4.p1.6.m6.3.3.3.1.1.1.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="A4.p1.6.m6.3.3.3.1.1.3" xref="A4.p1.6.m6.3.3.3.1.1.1.cmml">)</mo></mrow><mo id="A4.p1.6.m6.4.4.4.3" xref="A4.p1.6.m6.4.4.4.3.cmml">/</mo><mrow id="A4.p1.6.m6.4.4.4.2.1" xref="A4.p1.6.m6.4.4.4.2.1.1.cmml"><mo stretchy="false" id="A4.p1.6.m6.4.4.4.2.1.2" xref="A4.p1.6.m6.4.4.4.2.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.4.4.4.2.1.1" xref="A4.p1.6.m6.4.4.4.2.1.1.cmml"><mrow id="A4.p1.6.m6.4.4.4.2.1.1.2" xref="A4.p1.6.m6.4.4.4.2.1.1.2.cmml"><mn id="A4.p1.6.m6.4.4.4.2.1.1.2.2" xref="A4.p1.6.m6.4.4.4.2.1.1.2.2.cmml">6</mn><mo id="A4.p1.6.m6.4.4.4.2.1.1.2.1" xref="A4.p1.6.m6.4.4.4.2.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="A4.p1.6.m6.4.4.4.2.1.1.2.3" xref="A4.p1.6.m6.4.4.4.2.1.1.2.3.cmml">h</mi></mrow><mo id="A4.p1.6.m6.4.4.4.2.1.1.1" xref="A4.p1.6.m6.4.4.4.2.1.1.1.cmml">+</mo><msub id="A4.p1.6.m6.4.4.4.2.1.1.3" xref="A4.p1.6.m6.4.4.4.2.1.1.3.cmml"><mi id="A4.p1.6.m6.4.4.4.2.1.1.3.2" xref="A4.p1.6.m6.4.4.4.2.1.1.3.2.cmml">s</mi><mn id="A4.p1.6.m6.4.4.4.2.1.1.3.3" xref="A4.p1.6.m6.4.4.4.2.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="A4.p1.6.m6.4.4.4.2.1.3" xref="A4.p1.6.m6.4.4.4.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.6.m6.4b"><apply id="A4.p1.6.m6.4.4.cmml" xref="A4.p1.6.m6.4.4"><eq id="A4.p1.6.m6.4.4.5.cmml" xref="A4.p1.6.m6.4.4.5"></eq><apply id="A4.p1.6.m6.2.2.2.cmml" xref="A4.p1.6.m6.2.2.2"><divide id="A4.p1.6.m6.2.2.2.3.cmml" xref="A4.p1.6.m6.2.2.2.3"></divide><apply id="A4.p1.6.m6.1.1.1.1.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1"><divide id="A4.p1.6.m6.1.1.1.1.1.1.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.3"></divide><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1"><plus id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.1"></plus><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2"><times id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1"></times><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.2">24</cn><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.3">𝑏</ci><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.2">𝑠</ci><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.3">2</cn></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5">superscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.2">ℎ</ci><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.3">2</cn></apply></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3"><times id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1"></times><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.2">4</cn><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.3">𝑏</ci><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4">superscript</csymbol><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4">subscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.2">𝑠</ci><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.3">2</cn></apply><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.3">2</cn></apply><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.5.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.5">ℎ</ci></apply></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1"><plus id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.1"></plus><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2"><times id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1"></times><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.2">24</cn><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.3">𝑏</ci><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4">subscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.2">𝑠</ci><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.3">1</cn></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5">superscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.2">ℎ</ci><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.3">2</cn></apply></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3"><times id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1"></times><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.2">4</cn><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.3">𝑏</ci><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4">superscript</csymbol><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4">subscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.2">𝑠</ci><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.3">1</cn></apply><cn type="integer" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.3">2</cn></apply><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.5.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.5">ℎ</ci></apply></apply></apply><apply id="A4.p1.6.m6.2.2.2.2.1.1.cmml" xref="A4.p1.6.m6.2.2.2.2.1"><divide id="A4.p1.6.m6.2.2.2.2.1.1.1.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.1"></divide><apply id="A4.p1.6.m6.2.2.2.2.1.1.2.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="A4.p1.6.m6.2.2.2.2.1.1.2.1.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.2">subscript</csymbol><ci id="A4.p1.6.m6.2.2.2.2.1.1.2.2.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.2.2">𝑠</ci><cn type="integer" id="A4.p1.6.m6.2.2.2.2.1.1.2.3.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.2.3">2</cn></apply><apply id="A4.p1.6.m6.2.2.2.2.1.1.3.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="A4.p1.6.m6.2.2.2.2.1.1.3.1.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.3">subscript</csymbol><ci id="A4.p1.6.m6.2.2.2.2.1.1.3.2.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.3.2">𝑠</ci><cn type="integer" id="A4.p1.6.m6.2.2.2.2.1.1.3.3.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.3.3">1</cn></apply></apply></apply><apply id="A4.p1.6.m6.4.4.4.cmml" xref="A4.p1.6.m6.4.4.4"><divide id="A4.p1.6.m6.4.4.4.3.cmml" xref="A4.p1.6.m6.4.4.4.3"></divide><apply id="A4.p1.6.m6.3.3.3.1.1.1.cmml" xref="A4.p1.6.m6.3.3.3.1.1"><plus id="A4.p1.6.m6.3.3.3.1.1.1.1.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.1"></plus><apply id="A4.p1.6.m6.3.3.3.1.1.1.2.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.2"><times id="A4.p1.6.m6.3.3.3.1.1.1.2.1.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.2.1"></times><cn type="integer" id="A4.p1.6.m6.3.3.3.1.1.1.2.2.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.2.2">6</cn><ci id="A4.p1.6.m6.3.3.3.1.1.1.2.3.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.2.3">ℎ</ci></apply><apply id="A4.p1.6.m6.3.3.3.1.1.1.3.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="A4.p1.6.m6.3.3.3.1.1.1.3.1.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.3">subscript</csymbol><ci id="A4.p1.6.m6.3.3.3.1.1.1.3.2.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.3.2">𝑠</ci><cn type="integer" id="A4.p1.6.m6.3.3.3.1.1.1.3.3.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.3.3">2</cn></apply></apply><apply id="A4.p1.6.m6.4.4.4.2.1.1.cmml" xref="A4.p1.6.m6.4.4.4.2.1"><plus id="A4.p1.6.m6.4.4.4.2.1.1.1.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.1"></plus><apply id="A4.p1.6.m6.4.4.4.2.1.1.2.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.2"><times id="A4.p1.6.m6.4.4.4.2.1.1.2.1.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.2.1"></times><cn type="integer" id="A4.p1.6.m6.4.4.4.2.1.1.2.2.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.2.2">6</cn><ci id="A4.p1.6.m6.4.4.4.2.1.1.2.3.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.2.3">ℎ</ci></apply><apply id="A4.p1.6.m6.4.4.4.2.1.1.3.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.3"><csymbol cd="ambiguous" id="A4.p1.6.m6.4.4.4.2.1.1.3.1.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.3">subscript</csymbol><ci id="A4.p1.6.m6.4.4.4.2.1.1.3.2.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.3.2">𝑠</ci><cn type="integer" id="A4.p1.6.m6.4.4.4.2.1.1.3.3.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.6.m6.4c">((24bs_{2}h^{2}+4b{s_{2}}^{2}h)/(24bs_{1}h^{2}+4b{s_{1}}^{2}h))/(s_{2}/s_{1})=(6h+s_{2})/(6h+s_{1})</annotation><annotation encoding="application/x-llamapun" id="A4.p1.6.m6.4d">( ( 24 italic_b italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 4 italic_b italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_h ) / ( 24 italic_b italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 4 italic_b italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_h ) ) / ( italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT / italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = ( 6 italic_h + italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) / ( 6 italic_h + italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math id="A4.p1.7.m7.1" class="ltx_Math" alttext="s_{2}" display="inline"><semantics id="A4.p1.7.m7.1a"><msub id="A4.p1.7.m7.1.1" xref="A4.p1.7.m7.1.1.cmml"><mi id="A4.p1.7.m7.1.1.2" xref="A4.p1.7.m7.1.1.2.cmml">s</mi><mn id="A4.p1.7.m7.1.1.3" xref="A4.p1.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p1.7.m7.1b"><apply id="A4.p1.7.m7.1.1.cmml" xref="A4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="A4.p1.7.m7.1.1.1.cmml" xref="A4.p1.7.m7.1.1">subscript</csymbol><ci id="A4.p1.7.m7.1.1.2.cmml" xref="A4.p1.7.m7.1.1.2">𝑠</ci><cn type="integer" id="A4.p1.7.m7.1.1.3.cmml" xref="A4.p1.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.7.m7.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.7.m7.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> and <math id="A4.p1.8.m8.1" class="ltx_Math" alttext="s_{1}" display="inline"><semantics id="A4.p1.8.m8.1a"><msub id="A4.p1.8.m8.1.1" xref="A4.p1.8.m8.1.1.cmml"><mi id="A4.p1.8.m8.1.1.2" xref="A4.p1.8.m8.1.1.2.cmml">s</mi><mn id="A4.p1.8.m8.1.1.3" xref="A4.p1.8.m8.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p1.8.m8.1b"><apply id="A4.p1.8.m8.1.1.cmml" xref="A4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="A4.p1.8.m8.1.1.1.cmml" xref="A4.p1.8.m8.1.1">subscript</csymbol><ci id="A4.p1.8.m8.1.1.2.cmml" xref="A4.p1.8.m8.1.1.2">𝑠</ci><cn type="integer" id="A4.p1.8.m8.1.1.3.cmml" xref="A4.p1.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.8.m8.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.8.m8.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> are new and old context lengths.
Model sizes and their hidden dimensions are as follows: LLaMA-7B (4096), LLaMA-13B (5140), LLaMA-33B (7168), LLaMA-65B (8192), GPT3-175B (12288), and 1TB (36864). These model configurations are from LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Touvron et&nbsp;al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>]</cite> and GPT-3&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Brown et&nbsp;al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>]</cite> papers, except the 1TB model size and dimension were defined by us.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p">As depicted in Figure&nbsp;<a href="#A4.F5" title="Figure 5 ‣ Appendix D Training FLOPs Scaling of Context Size ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, scaling up small models to a 1M context size results in approximately 20-40 times more FLOPs, and even more for 10M and 100M token context sizes. However, as the model sizes increase, the cost ratio decreases. For instance, scaling up the 170B model from 4K to 10M incurs 162.6x higher per dataset FLOPs, despite the context size being 3072 times longer.</p>
</div>
<figure id="A4.F5" class="ltx_figure"><img src="/html/2310.01889/assets/figures/cost_heatmap.png" id="A4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="595" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The per dataset trainig FLOPs cost ratio relative to a 4k context size, considering different model dimensions. On the x-axis, you’ll find the context length, where, for example, 32x(128k) denotes a context length of 128k, 32x the size of the same model’s 4k context length.</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Impact on In Context RL Performance</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.3" class="ltx_p">In addition to show the application of Ring Attention to finetune LLM in Section&nbsp;<a href="#S5.SS3" title="5.3 Impact on LLM Performance ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>, we present additional results of applying Ring Attention for learning trial-and-error RL experience using Transformers.
We report our results in Table <a href="#A5.T5" title="Table 5 ‣ Appendix E Impact on In Context RL Performance ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where we evaluate our proposed model on the ExoRL benchmark across six different tasks.
On ExoRL, we report the cumulative return, as per ExoRL&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Yarats et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2022</a>]</cite>.
We compare BC, DT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Chen et&nbsp;al., <a href="#bib.bib4" title="" class="ltx_ref">2021</a>]</cite>, AT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Liu and Abbeel, <a href="#bib.bib23" title="" class="ltx_ref">2023a</a>]</cite>, and AT with memory efficient attention&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>]</cite> (AT+ME), AT with blockwise parallel transformers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>]</cite> (AT+BPT), and AT with our Ring Attention (AT+Ring Attention).
The numbers of BC, DT, AT are from the ExoRL and AT paper.
AT + Ring Attention numbers are run by ourselves.
Since the ExoRL data is highly diverse, having been collected using unsupervised RL&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Laskin et&nbsp;al., <a href="#bib.bib20" title="" class="ltx_ref">2021</a>]</cite>, it has been found that TD learning performs best, while behavior cloning struggles&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Yarats et&nbsp;al., <a href="#bib.bib41" title="" class="ltx_ref">2022</a>]</cite>.
AT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[Liu and Abbeel, <a href="#bib.bib23" title="" class="ltx_ref">2023a</a>]</cite> shows that conditioning Transformer on multiple trajectories with relabeled target return can achieve competitive results with TD learning. For more details, please refer to their papers.
We are interested in applying Ring Attention to improve the performance of AT by conditioning on a larger number of trajectories rather than 32 trajectories in prior works.
It is worth noting that each trajectory has <math id="A5.p1.1.m1.1" class="ltx_Math" alttext="1000\times 4" display="inline"><semantics id="A5.p1.1.m1.1a"><mrow id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><mn id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2.cmml">1000</mn><mo lspace="0.222em" rspace="0.222em" id="A5.p1.1.m1.1.1.1" xref="A5.p1.1.m1.1.1.1.cmml">×</mo><mn id="A5.p1.1.m1.1.1.3" xref="A5.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><times id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1"></times><cn type="integer" id="A5.p1.1.m1.1.1.2.cmml" xref="A5.p1.1.m1.1.1.2">1000</cn><cn type="integer" id="A5.p1.1.m1.1.1.3.cmml" xref="A5.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">1000\times 4</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.1d">1000 × 4</annotation></semantics></math> length where <math id="A5.p1.2.m2.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="A5.p1.2.m2.1a"><mn id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.1b"><cn type="integer" id="A5.p1.2.m2.1.1.cmml" xref="A5.p1.2.m2.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.1c">1000</annotation><annotation encoding="application/x-llamapun" id="A5.p1.2.m2.1d">1000</annotation></semantics></math> is sequence length while <math id="A5.p1.3.m3.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A5.p1.3.m3.1a"><mn id="A5.p1.3.m3.1.1" xref="A5.p1.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A5.p1.3.m3.1b"><cn type="integer" id="A5.p1.3.m3.1.1.cmml" xref="A5.p1.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.3.m3.1c">4</annotation><annotation encoding="application/x-llamapun" id="A5.p1.3.m3.1d">4</annotation></semantics></math> is return-state-action-reward, making training 128 trajectories with modest 350M size model infeasible for prior state-of-the-art blockwise parallel transformers.
Results in Table <a href="#A5.T5" title="Table 5 ‣ Appendix E Impact on In Context RL Performance ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show that, by scaling up the sequence length (number of trajectories), AT + Ring Attention consistently outperforms oringal AT with BPT across all six tasks, achieving a total average return of 113.66 compared to the AT with BPT model’s total average return of 111.13.
The results show that the advantage of Ring Attention for training and inference with long sequences.</p>
</div>
<figure id="A5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Application of Ring Attention on improving Transformer in RL. BC and DT use vanilla attention. AT + ME denotes using memory efficient attention, AT + BPT denotes using blockwise parallel transformer. AT + RA denotes using Ring Attention.</figcaption>
<div id="A5.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:328.6pt;height:137.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.0pt,12.1pt) scale(0.85,0.85) ;">
<table id="A5.T5.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="A5.T5.1.1.1" class="ltx_tr">
<td id="A5.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A5.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">ExoRL</span></td>
<td id="A5.T5.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">BC-10%</span></td>
<td id="A5.T5.1.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt"><span id="A5.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">DT</span></td>
<td id="A5.T5.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.4.1" class="ltx_text ltx_font_bold">AT + ME</span></td>
<td id="A5.T5.1.1.1.5" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.5.1" class="ltx_text ltx_font_bold">AT + BPT</span></td>
<td id="A5.T5.1.1.1.6" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.6.1" class="ltx_text ltx_font_bold">AT + BPT</span></td>
<td id="A5.T5.1.1.1.7" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.7.1" class="ltx_text ltx_font_bold">AT + RA</span></td>
</tr>
<tr id="A5.T5.1.1.2" class="ltx_tr">
<td id="A5.T5.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T5.1.1.2.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="A5.T5.1.1.2.2" class="ltx_td ltx_border_t"></td>
<td id="A5.T5.1.1.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="A5.T5.1.1.2.4" class="ltx_td ltx_align_right ltx_border_t">N Trajs = 32</td>
<td id="A5.T5.1.1.2.5" class="ltx_td ltx_align_right ltx_border_t">N Trajs = 32</td>
<td id="A5.T5.1.1.2.6" class="ltx_td ltx_align_right ltx_border_t">N Trajs = 128</td>
<td id="A5.T5.1.1.2.7" class="ltx_td ltx_align_right ltx_border_t">N Trajs = 128</td>
</tr>
<tr id="A5.T5.1.1.3" class="ltx_tr">
<td id="A5.T5.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Walker Stand</td>
<td id="A5.T5.1.1.3.2" class="ltx_td ltx_align_right ltx_border_t">52.91</td>
<td id="A5.T5.1.1.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">34.54</td>
<td id="A5.T5.1.1.3.4" class="ltx_td ltx_align_right ltx_border_t">oom</td>
<td id="A5.T5.1.1.3.5" class="ltx_td ltx_align_right ltx_border_t">95.45</td>
<td id="A5.T5.1.1.3.6" class="ltx_td ltx_align_right ltx_border_t">oom</td>
<td id="A5.T5.1.1.3.7" class="ltx_td ltx_align_right ltx_border_t">98.23</td>
</tr>
<tr id="A5.T5.1.1.4" class="ltx_tr">
<td id="A5.T5.1.1.4.1" class="ltx_td ltx_align_left">Walker Run</td>
<td id="A5.T5.1.1.4.2" class="ltx_td ltx_align_right">34.81</td>
<td id="A5.T5.1.1.4.3" class="ltx_td ltx_align_right ltx_border_r">49.82</td>
<td id="A5.T5.1.1.4.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.4.5" class="ltx_td ltx_align_right">105.88</td>
<td id="A5.T5.1.1.4.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.4.7" class="ltx_td ltx_align_right">110.45</td>
</tr>
<tr id="A5.T5.1.1.5" class="ltx_tr">
<td id="A5.T5.1.1.5.1" class="ltx_td ltx_align_left">Walker Walk</td>
<td id="A5.T5.1.1.5.2" class="ltx_td ltx_align_right">13.53</td>
<td id="A5.T5.1.1.5.3" class="ltx_td ltx_align_right ltx_border_r">34.94</td>
<td id="A5.T5.1.1.5.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.5.5" class="ltx_td ltx_align_right">78.56</td>
<td id="A5.T5.1.1.5.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.5.7" class="ltx_td ltx_align_right">78.95</td>
</tr>
<tr id="A5.T5.1.1.6" class="ltx_tr">
<td id="A5.T5.1.1.6.1" class="ltx_td ltx_align_left">Cheetah Run</td>
<td id="A5.T5.1.1.6.2" class="ltx_td ltx_align_right">34.66</td>
<td id="A5.T5.1.1.6.3" class="ltx_td ltx_align_right ltx_border_r">67.53</td>
<td id="A5.T5.1.1.6.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.6.5" class="ltx_td ltx_align_right">178.75</td>
<td id="A5.T5.1.1.6.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.6.7" class="ltx_td ltx_align_right">181.34</td>
</tr>
<tr id="A5.T5.1.1.7" class="ltx_tr">
<td id="A5.T5.1.1.7.1" class="ltx_td ltx_align_left">Jaco Reach</td>
<td id="A5.T5.1.1.7.2" class="ltx_td ltx_align_right">23.95</td>
<td id="A5.T5.1.1.7.3" class="ltx_td ltx_align_right ltx_border_r">18.64</td>
<td id="A5.T5.1.1.7.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.7.5" class="ltx_td ltx_align_right">87.56</td>
<td id="A5.T5.1.1.7.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.7.7" class="ltx_td ltx_align_right">89.51</td>
</tr>
<tr id="A5.T5.1.1.8" class="ltx_tr">
<td id="A5.T5.1.1.8.1" class="ltx_td ltx_align_left">Cartpole Swingup</td>
<td id="A5.T5.1.1.8.2" class="ltx_td ltx_align_right">56.82</td>
<td id="A5.T5.1.1.8.3" class="ltx_td ltx_align_right ltx_border_r">67.56</td>
<td id="A5.T5.1.1.8.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.8.5" class="ltx_td ltx_align_right">120.56</td>
<td id="A5.T5.1.1.8.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.8.7" class="ltx_td ltx_align_right">123.45</td>
</tr>
<tr id="A5.T5.1.1.9" class="ltx_tr">
<td id="A5.T5.1.1.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="A5.T5.1.1.9.1.1" class="ltx_text ltx_font_bold">Total Average</span></td>
<td id="A5.T5.1.1.9.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">36.11</td>
<td id="A5.T5.1.1.9.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t">45.51</td>
<td id="A5.T5.1.1.9.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">oom</td>
<td id="A5.T5.1.1.9.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">111.13</td>
<td id="A5.T5.1.1.9.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">oom</td>
<td id="A5.T5.1.1.9.7" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">113.66</td>
</tr>
</tbody></table>
</span></div>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.01888" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.01889" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2310.01889">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.01889" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.01890" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Nov  5 17:25:43 2023 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.01889] Ring Attention with Blockwise Transformers for Near-Infinite Context</title><meta property="og:description" content="Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformer…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ring Attention with Blockwise Transformers for Near-Infinite Context">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Ring Attention with Blockwise Transformers for Near-Infinite Context">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.01889">

<!--Generated on Sun Nov  5 17:25:43 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv.0.7.7.min.css"><link media="all" rel="stylesheet" href="https://ar5iv.labs.arxiv.org/assets/ar5iv-site.0.2.1.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Ring Attention with Blockwise 
<br class="ltx_break">Transformers for Near-Infinite Context</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Liu, Matei Zaharia, Pieter Abbeel

<br class="ltx_break">
UC Berkeley
<br class="ltx_break">
<span id="id1.1.id1" class="ltx_text ltx_font_typewriter">hao.liu@cs.berkeley.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">트랜스포머는 광범위한 AI 애플리케이션에서 탁월한 성능을 보여주면서 많은 최첨단 AI 모델의 선택 아키텍처로 부상했다. 그러나, 트랜스포머에 의해 부과된 메모리 요구는 긴 시퀀스들을 처리하는 그들의 능력을 제한함으로써, 확장된 시퀀스들 또는 장기 종속성을 수반하는 태스크들에 대한 도전들을 생성한다. 본 논문에서는 키-값 블록의 통신과 블록별 주의의 계산을 중첩시키면서 자기 주의의 블록별 계산을 활용하여 여러 장치에 긴 시퀀스를 분산시키는 링 주의(Ring Attention) 기법을 제안한다. 링 어텐션(Ring Attention)은 이전의 메모리 효율적인 트랜스포머보다 디바이스 카운트 시간까지 긴 시퀀스들의 트레이닝 및 추론을 가능하게 하여, 개별 디바이스들에 의해 부과되는 메모리 제약들을 효과적으로 제거한다. 언어 모델링 작업에 대한 광범위한 실험은 큰 시퀀스 입력 크기를 허용하고 성능을 향상시키는 링 어텐션의 효과를 보여준다.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p" id="S1.p1.1">트랜스포머 <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="#bib.bib39" title="">2017</a>)</cite>는 광범위한 AI 문제에 걸쳐 인상적인 성능을 보여준 많은 최첨단 AI 시스템의 백본이 되었습니다. 트랜스포머는 자기 주의 및 위치별 피드포워드 메커니즘을 사용하는 아키텍처 설계를 통해 이러한 성공을 달성한다.</p>
</div>
<figure id="S1.F1" class="ltx_figure ltx_align_floatright"><img src="https://ar5iv.labs.arxiv.org/html/2310.01889/assets/figures/context_len.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">도 1:</span></figcaption>
Maximum context length under end-to-end large-scale training on TPUv4-1024.
Baselines are vanilla transformers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite>, memory efficient transformers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, and memory efficient attention and feedforward (blockwise parallel transformers)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>.
Our proposed approach Ring Attention allows training up to device count times longer sequence than baselines and enables the training of sequences that exceed millions in length without making approximations to attention.
</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p" id="S1.p2.1">이러한 구성 요소는 입력 토큰 간의 장거리 종속성을 효율적으로 캡처하고, 높은 병렬 계산을 통해 확장성을 가능하게 한다.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p" id="S1.p3.1">그러나 Transformers의 컨텍스트 길이를 확장하는 것은 Transformers의 상속 아키텍처 설계인 <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">i</span>이기 때문에 챌린지 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib30" title="">2023</a>)</cite>입니다. <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">e</span>. 자기 주의는 입력 시퀀스의 길이에서 메모리 비용이 2차이므로 더 긴 입력 시퀀스로 확장하기가 어렵다. 대용량 컨텍스트 트랜스포머는 서적 및 고해상도 이미지 처리부터 긴 비디오 및 복잡한 코드베이스 분석에 이르기까지 다양한 AI 문제를 해결하는 데 필수적이다. 그들은 상호 연결된 웹과 하이퍼링크된 콘텐츠에서 정보를 추출하는 데 탁월하며 복잡한 과학 실험 데이터를 처리하는 데 중요하다. 문맥 길이가 16K인 GPT-3.5 <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al., <a class="ltx_ref" href="#bib.bib33" title="">2022</a>)</cite>, 문맥 길이가 32k인 GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="#bib.bib30" title="">2023</a>)</cite>, 문맥 길이가 65k인 MosaicML의 MPT <cite class="ltx_cite ltx_citemacro_citep">(MosaicML, <a class="ltx_ref" href="#bib.bib26" title="">2023</a>)</cite>, 문맥 길이가 100k인 Anthropic의 Claude <cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="#bib.bib1" title="">2023</a>)</cite> 등 문맥이 크게 확장된 언어 모델의 사용 사례가 등장하고 있다.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p" id="S1.p4.1">그 중요성에 힘입어 메모리 비용 절감에 대한 연구 관심이 급증하고 있다. 한 연구 라인은 전체 행렬 <cite class="ltx_cite ltx_citemacro_citep">(Milakov and Gimelshein, <a class="ltx_ref" href="#bib.bib25" title="">2018</a>)</cite>를 구체화하지 않고도 자기 주의에서 소프트맥스 행렬을 계산할 수 있다는 관찰을 활용하며, 이는 근사화를 하지 않고도 자기 주의와 피드포워드 <cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a class="ltx_ref" href="#bib.bib31" title="">2021</a>; Dao et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>; Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>를 블록 단위로 계산하게 만들었다. 감소된 메모리에도 불구하고, 각 층의 출력을 저장하는 것으로부터 여전히 상당한 도전이 발생한다. 이러한 필요성은 모든 요소 간의 상호 작용(n~n개의 상호 작용)을 포함하는 자기 주의의 고유한 특성에서 발생한다. 후속 레이어의 자체 주의는 이전 레이어의 모든 출력에 액세스하는 데 의존한다. 이렇게 하지 않으면 각 시퀀스 요소에 대해 모든 출력을 다시 계산해야 하므로 계산 비용이 3차적으로 증가하여 더 긴 시퀀스에 대해서는 비실용적이다. 메모리 수요를 관점으로 볼 때, 1의 배치 크기를 다루는 경우에도 1억 개의 토큰을 처리하려면 1024의 숨겨진 크기를 가진 적당한 모델의 경우 1000GB 이상의 메모리가 필요하다. 이는 일반적으로 100GB 미만의 고대역폭 메모리(HBM)를 갖는 현대 GPU 및 TPU의 용량보다 훨씬 크다.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p" id="S1.p5.1">이 문제를 해결하기 위해 우리는 주요 관찰을 한다: 블록 방식으로 자기 주의 및 피드포워드 네트워크 계산을 수행하면 <cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite> 여러 장치에 시퀀스 차원을 분산하여 동시 계산 및 통신을 할 수 있다. 이러한 통찰은 우리가 블록 단위로 주의력을 계산할 때 결과가 이러한 블록 단위의 계산 순서에 불변한다는 사실에서 비롯된다. 제안하는 방법은 호스트들 사이에 블록 단위의 집중도를 계산하는 외부 루프를 분배하고, 각 디바이스들은 각각의 입력 블록을 관리한다. 내부 루프의 경우 모든 장치는 지정된 입력 블록에 따라 블록별 주의 및 피드포워드 동작을 계산합니다. 호스트 디바이스들은 개념적 링을 형성하고, 여기서 내부 루프 동안, 각각의 디바이스는 블록 단위 계산을 위해 사용되고 있는 자신의 키-값 블록들의 복사본을 링 내의 다음 디바이스로 전송하는 한편, 동시에 이전 디바이스로부터 키-값 블록들을 수신한다. 블록 계산은 블록 전송보다 더 오래 걸리기 때문에 이러한 프로세스를 중첩하면 표준 변압기에 비해 추가 오버헤드가 발생하지 않는다. 이렇게 함으로써, 각 장치는 원래의 입력 시퀀스 길이와 무관한 블록 크기에만 비례하는 메모리를 필요로 한다. 이는 개별 디바이스에 의해 부과되는 메모리 제약을 효과적으로 제거한다. 제안하는 방법은 블록 단위의 연산으로 링 내의 호스트들 사이의 키-값 블록들의 통신을 중복하기 때문에, 이를 링 어텐션이라고 명명한다.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p class="ltx_p" id="S1.p6.1">우리는 언어 모델링 벤치마크에 대한 접근법의 효율성을 평가한다. 실험 결과, Ring Attention은 Transformers의 메모리 요구량을 줄일 수 있으며, 기존의 메모리 효율적인 최신 기술보다 500배 이상의 긴 시퀀스를 학습시킬 수 있으며, 1억 개를 초과하는 시퀀스를 어림셈 없이 학습시킬 수 있음을 보였다. 중요하게도, 링 어텐션은 개별 디바이스들에 의해 부과되는 메모리 제약들을 제거하여, 디바이스들의 수에 비례하여 스케일링되는 길이들을 갖는 시퀀스들의 트레이닝 및 추론에 권한을 부여함으로써, 본질적으로 거의 무한한 컨텍스트 크기를 달성한다.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p class="ltx_p" id="S1.p7.1">본 논문의 기여도는 (a) 성능을 유지하면서 컨텍스트 길이가 디바이스 수에 따라 선형적으로 확장될 수 있도록 하는 메모리 효율적인 트랜스포머 아키텍처를 제안하고, (b) 개별 디바이스에 의해 부과되는 메모리 병목 현상을 제거하며, 그리고 (b) 광범위한 실험을 통해 본 논문의 접근법의 유효성을 입증한다.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Large Context Memory Constraint</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p" id="S2.p1.3">주어진 입력 시퀀스 <math alttext="Q,K,V\in\mathbb{R}^{s\times d}" class="ltx_Math" display="inline" id="S2.p1.1.m1.3"><semantics id="S2.p1.1.m1.3a"><mrow id="S2.p1.1.m1.3.4" xref="S2.p1.1.m1.3.4.cmml"><mrow id="S2.p1.1.m1.3.4.2.2" xref="S2.p1.1.m1.3.4.2.1.cmml"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">Q</mi><mo id="S2.p1.1.m1.3.4.2.2.1" xref="S2.p1.1.m1.3.4.2.1.cmml">,</mo><mi id="S2.p1.1.m1.2.2" xref="S2.p1.1.m1.2.2.cmml">K</mi><mo id="S2.p1.1.m1.3.4.2.2.2" xref="S2.p1.1.m1.3.4.2.1.cmml">,</mo><mi id="S2.p1.1.m1.3.3" xref="S2.p1.1.m1.3.3.cmml">V</mi></mrow><mo id="S2.p1.1.m1.3.4.1" xref="S2.p1.1.m1.3.4.1.cmml">∈</mo><msup id="S2.p1.1.m1.3.4.3" xref="S2.p1.1.m1.3.4.3.cmml"><mi id="S2.p1.1.m1.3.4.3.2" xref="S2.p1.1.m1.3.4.3.2.cmml">ℝ</mi><mrow id="S2.p1.1.m1.3.4.3.3" xref="S2.p1.1.m1.3.4.3.3.cmml"><mi id="S2.p1.1.m1.3.4.3.3.2" xref="S2.p1.1.m1.3.4.3.3.2.cmml">s</mi><mo id="S2.p1.1.m1.3.4.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.p1.1.m1.3.4.3.3.1.cmml">×</mo><mi id="S2.p1.1.m1.3.4.3.3.3" xref="S2.p1.1.m1.3.4.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.3b"><apply id="S2.p1.1.m1.3.4.cmml" xref="S2.p1.1.m1.3.4"><in id="S2.p1.1.m1.3.4.1.cmml" xref="S2.p1.1.m1.3.4.1"></in><list id="S2.p1.1.m1.3.4.2.1.cmml" xref="S2.p1.1.m1.3.4.2.2"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝑄</ci><ci id="S2.p1.1.m1.2.2.cmml" xref="S2.p1.1.m1.2.2">𝐾</ci><ci id="S2.p1.1.m1.3.3.cmml" xref="S2.p1.1.m1.3.3">𝑉</ci></list><apply id="S2.p1.1.m1.3.4.3.cmml" xref="S2.p1.1.m1.3.4.3"><csymbol cd="ambiguous" id="S2.p1.1.m1.3.4.3.1.cmml" xref="S2.p1.1.m1.3.4.3">superscript</csymbol><ci id="S2.p1.1.m1.3.4.3.2.cmml" xref="S2.p1.1.m1.3.4.3.2">ℝ</ci><apply id="S2.p1.1.m1.3.4.3.3.cmml" xref="S2.p1.1.m1.3.4.3.3"><times id="S2.p1.1.m1.3.4.3.3.1.cmml" xref="S2.p1.1.m1.3.4.3.3.1"></times><ci id="S2.p1.1.m1.3.4.3.3.2.cmml" xref="S2.p1.1.m1.3.4.3.3.2">𝑠</ci><ci id="S2.p1.1.m1.3.4.3.3.3.cmml" xref="S2.p1.1.m1.3.4.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.3c">Q,K,V\in\mathbb{R}^{s\times d}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.3d">italic_Q , italic_K , italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_s × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> 여기서 <math alttext="s" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">italic_s</annotation></semantics></math>는 시퀀스 길이이고 <math alttext="d" class="ltx_Math" display="inline" id="S2.p1.3.m3.1"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.1d">italic_d</annotation></semantics></math>는 헤드 차원이다. 우리는 출력의 행렬을 다음과 같이 계산한다:</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.5" class="ltx_Math" alttext="\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d}})V," display="block"><semantics id="S2.Ex1.m1.5a"><mrow id="S2.Ex1.m1.5.5.1" xref="S2.Ex1.m1.5.5.1.1.cmml"><mrow id="S2.Ex1.m1.5.5.1.1" xref="S2.Ex1.m1.5.5.1.1.cmml"><mrow id="S2.Ex1.m1.5.5.1.1.2" xref="S2.Ex1.m1.5.5.1.1.2.cmml"><mi id="S2.Ex1.m1.5.5.1.1.2.2" xref="S2.Ex1.m1.5.5.1.1.2.2.cmml">Attention</mi><mo id="S2.Ex1.m1.5.5.1.1.2.1" xref="S2.Ex1.m1.5.5.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mrow id="S2.Ex1.m1.5.5.1.1.2.3.2" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.2.3.2.1" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml">(</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">Q</mi><mo id="S2.Ex1.m1.5.5.1.1.2.3.2.2" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml">K</mi><mo id="S2.Ex1.m1.5.5.1.1.2.3.2.3" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml">,</mo><mi id="S2.Ex1.m1.3.3" xref="S2.Ex1.m1.3.3.cmml">V</mi><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.2.3.2.4" xref="S2.Ex1.m1.5.5.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.5.5.1.1.1" xref="S2.Ex1.m1.5.5.1.1.1.cmml">=</mo><mrow id="S2.Ex1.m1.5.5.1.1.3" xref="S2.Ex1.m1.5.5.1.1.3.cmml"><mi id="S2.Ex1.m1.5.5.1.1.3.2" xref="S2.Ex1.m1.5.5.1.1.3.2.cmml">softmax</mi><mo id="S2.Ex1.m1.5.5.1.1.3.1" xref="S2.Ex1.m1.5.5.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mrow id="S2.Ex1.m1.5.5.1.1.3.3.2" xref="S2.Ex1.m1.4.4.cmml"><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.3.3.2.1" xref="S2.Ex1.m1.4.4.cmml">(</mo><mfrac id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.4.cmml"><mrow id="S2.Ex1.m1.4.4.2" xref="S2.Ex1.m1.4.4.2.cmml"><mi id="S2.Ex1.m1.4.4.2.2" xref="S2.Ex1.m1.4.4.2.2.cmml">Q</mi><mo id="S2.Ex1.m1.4.4.2.1" xref="S2.Ex1.m1.4.4.2.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S2.Ex1.m1.4.4.2.3" xref="S2.Ex1.m1.4.4.2.3.cmml"><mi id="S2.Ex1.m1.4.4.2.3.2" xref="S2.Ex1.m1.4.4.2.3.2.cmml">K</mi><mi id="S2.Ex1.m1.4.4.2.3.3" xref="S2.Ex1.m1.4.4.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S2.Ex1.m1.4.4.3" xref="S2.Ex1.m1.4.4.3.cmml"><mi id="S2.Ex1.m1.4.4.3.2" xref="S2.Ex1.m1.4.4.3.2.cmml">d</mi></msqrt></mfrac><mo stretchy="false" id="S2.Ex1.m1.5.5.1.1.3.3.2.2" xref="S2.Ex1.m1.4.4.cmml">)</mo></mrow><mo id="S2.Ex1.m1.5.5.1.1.3.1a" xref="S2.Ex1.m1.5.5.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S2.Ex1.m1.5.5.1.1.3.4" xref="S2.Ex1.m1.5.5.1.1.3.4.cmml">V</mi></mrow></mrow><mo id="S2.Ex1.m1.5.5.1.2" xref="S2.Ex1.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.5b"><apply id="S2.Ex1.m1.5.5.1.1.cmml" xref="S2.Ex1.m1.5.5.1"><eq id="S2.Ex1.m1.5.5.1.1.1.cmml" xref="S2.Ex1.m1.5.5.1.1.1"></eq><apply id="S2.Ex1.m1.5.5.1.1.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2"><times id="S2.Ex1.m1.5.5.1.1.2.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2.1"></times><ci id="S2.Ex1.m1.5.5.1.1.2.2.cmml" xref="S2.Ex1.m1.5.5.1.1.2.2">Attention</ci><vector id="S2.Ex1.m1.5.5.1.1.2.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.2.3.2"><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">𝑄</ci><ci id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2">𝐾</ci><ci id="S2.Ex1.m1.3.3.cmml" xref="S2.Ex1.m1.3.3">𝑉</ci></vector></apply><apply id="S2.Ex1.m1.5.5.1.1.3.cmml" xref="S2.Ex1.m1.5.5.1.1.3"><times id="S2.Ex1.m1.5.5.1.1.3.1.cmml" xref="S2.Ex1.m1.5.5.1.1.3.1"></times><ci id="S2.Ex1.m1.5.5.1.1.3.2.cmml" xref="S2.Ex1.m1.5.5.1.1.3.2">softmax</ci><apply id="S2.Ex1.m1.4.4.cmml" xref="S2.Ex1.m1.5.5.1.1.3.3.2"><divide id="S2.Ex1.m1.4.4.1.cmml" xref="S2.Ex1.m1.5.5.1.1.3.3.2"></divide><apply id="S2.Ex1.m1.4.4.2.cmml" xref="S2.Ex1.m1.4.4.2"><times id="S2.Ex1.m1.4.4.2.1.cmml" xref="S2.Ex1.m1.4.4.2.1"></times><ci id="S2.Ex1.m1.4.4.2.2.cmml" xref="S2.Ex1.m1.4.4.2.2">𝑄</ci><apply id="S2.Ex1.m1.4.4.2.3.cmml" xref="S2.Ex1.m1.4.4.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.4.4.2.3.1.cmml" xref="S2.Ex1.m1.4.4.2.3">superscript</csymbol><ci id="S2.Ex1.m1.4.4.2.3.2.cmml" xref="S2.Ex1.m1.4.4.2.3.2">𝐾</ci><ci id="S2.Ex1.m1.4.4.2.3.3.cmml" xref="S2.Ex1.m1.4.4.2.3.3">𝑇</ci></apply></apply><apply id="S2.Ex1.m1.4.4.3.cmml" xref="S2.Ex1.m1.4.4.3"><root id="S2.Ex1.m1.4.4.3a.cmml" xref="S2.Ex1.m1.4.4.3"></root><ci id="S2.Ex1.m1.4.4.3.2.cmml" xref="S2.Ex1.m1.4.4.3.2">𝑑</ci></apply></apply><ci id="S2.Ex1.m1.5.5.1.1.3.4.cmml" xref="S2.Ex1.m1.5.5.1.1.3.4">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.5c">\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d}})V,</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.5d">roman_Attention ( italic_Q , italic_K , italic_V ) = roman_softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d end_ARG end_ARG ) italic_V ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p1.4">여기서 <math alttext="\mathrm{softmax}" class="ltx_Math" display="inline" id="S2.p1.4.m1.1"><semantics id="S2.p1.4.m1.1a"><mi id="S2.p1.4.m1.1.1" xref="S2.p1.4.m1.1.1.cmml">softmax</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m1.1b"><ci id="S2.p1.4.m1.1.1.cmml" xref="S2.p1.4.m1.1.1">softmax</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m1.1c">\mathrm{softmax}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m1.1d">roman_softmax</annotation></semantics></math>는 행별로 적용된다. 각각의 셀프-어텐션 서브-레이어는 피드포워드 네트워크를 동반하며, 이는 각각의 포지션에 개별적으로 그리고 동일하게 적용된다. 이것은 ReLU 활성화를 사이에 두고 두 개의 선형 변환으로 구성된다.</p>
<table id="S2.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex2.m1.4" class="ltx_Math" alttext="\mathrm{FFN}(x)=\max(0,xW_{1}+b_{1})W_{2}+b_{2}." display="block"><semantics id="S2.Ex2.m1.4a"><mrow id="S2.Ex2.m1.4.4.1" xref="S2.Ex2.m1.4.4.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1" xref="S2.Ex2.m1.4.4.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1.3" xref="S2.Ex2.m1.4.4.1.1.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.3.2" xref="S2.Ex2.m1.4.4.1.1.3.2.cmml">FFN</mi><mo id="S2.Ex2.m1.4.4.1.1.3.1" xref="S2.Ex2.m1.4.4.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mrow id="S2.Ex2.m1.4.4.1.1.3.3.2" xref="S2.Ex2.m1.4.4.1.1.3.cmml"><mo stretchy="false" id="S2.Ex2.m1.4.4.1.1.3.3.2.1" xref="S2.Ex2.m1.4.4.1.1.3.cmml">(</mo><mi id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml">x</mi><mo stretchy="false" id="S2.Ex2.m1.4.4.1.1.3.3.2.2" xref="S2.Ex2.m1.4.4.1.1.3.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.4.4.1.1.2" xref="S2.Ex2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S2.Ex2.m1.4.4.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml"><mi id="S2.Ex2.m1.2.2" xref="S2.Ex2.m1.2.2.cmml">max</mi><mo id="S2.Ex2.m1.4.4.1.1.1.1.1.1a" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml">(</mo><mn id="S2.Ex2.m1.3.3" xref="S2.Ex2.m1.3.3.cmml">0</mn><mo id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml">,</mo><mrow id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><msub id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.2.cmml">W</mi><mn id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></msub></mrow><mo id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.2" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml">b</mi><mn id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.3" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.4" xref="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.4.4.1.1.1.1.2" xref="S2.Ex2.m1.4.4.1.1.1.1.2.cmml" lspace="0px" rspace="0px"></mo><msub id="S2.Ex2.m1.4.4.1.1.1.1.3" xref="S2.Ex2.m1.4.4.1.1.1.1.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.1.3.2" xref="S2.Ex2.m1.4.4.1.1.1.1.3.2.cmml">W</mi><mn id="S2.Ex2.m1.4.4.1.1.1.1.3.3" xref="S2.Ex2.m1.4.4.1.1.1.1.3.3.cmml">2</mn></msub></mrow><mo id="S2.Ex2.m1.4.4.1.1.1.2" xref="S2.Ex2.m1.4.4.1.1.1.2.cmml">+</mo><msub id="S2.Ex2.m1.4.4.1.1.1.3" xref="S2.Ex2.m1.4.4.1.1.1.3.cmml"><mi id="S2.Ex2.m1.4.4.1.1.1.3.2" xref="S2.Ex2.m1.4.4.1.1.1.3.2.cmml">b</mi><mn id="S2.Ex2.m1.4.4.1.1.1.3.3" xref="S2.Ex2.m1.4.4.1.1.1.3.3.cmml">2</mn></msub></mrow></mrow><mo lspace="0em" id="S2.Ex2.m1.4.4.1.2" xref="S2.Ex2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.4b"><apply id="S2.Ex2.m1.4.4.1.1.cmml" xref="S2.Ex2.m1.4.4.1"><eq id="S2.Ex2.m1.4.4.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.2"></eq><apply id="S2.Ex2.m1.4.4.1.1.3.cmml" xref="S2.Ex2.m1.4.4.1.1.3"><times id="S2.Ex2.m1.4.4.1.1.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.3.1"></times><ci id="S2.Ex2.m1.4.4.1.1.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.3.2">FFN</ci><ci id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1">𝑥</ci></apply><apply id="S2.Ex2.m1.4.4.1.1.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1"><plus id="S2.Ex2.m1.4.4.1.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.2"></plus><apply id="S2.Ex2.m1.4.4.1.1.1.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1"><times id="S2.Ex2.m1.4.4.1.1.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.2"></times><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1"><max id="S2.Ex2.m1.2.2.cmml" xref="S2.Ex2.m1.2.2"></max><cn type="integer" id="S2.Ex2.m1.3.3.cmml" xref="S2.Ex2.m1.3.3">0</cn><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1"><plus id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2"><times id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.1"></times><ci id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.2">𝑥</ci><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.2">𝑊</ci><cn type="integer" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.2">𝑏</ci><cn type="integer" id="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply><apply id="S2.Ex2.m1.4.4.1.1.1.1.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.4.4.1.1.1.1.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.3">subscript</csymbol><ci id="S2.Ex2.m1.4.4.1.1.1.1.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.3.2">𝑊</ci><cn type="integer" id="S2.Ex2.m1.4.4.1.1.1.1.3.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.1.3.3">2</cn></apply></apply><apply id="S2.Ex2.m1.4.4.1.1.1.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.4.4.1.1.1.3.1.cmml" xref="S2.Ex2.m1.4.4.1.1.1.3">subscript</csymbol><ci id="S2.Ex2.m1.4.4.1.1.1.3.2.cmml" xref="S2.Ex2.m1.4.4.1.1.1.3.2">𝑏</ci><cn type="integer" id="S2.Ex2.m1.4.4.1.1.1.3.3.cmml" xref="S2.Ex2.m1.4.4.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.4c">\mathrm{FFN}(x)=\max(0,xW_{1}+b_{1})W_{2}+b_{2}.</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.4d">roman_FFN ( italic_x ) = roman_max ( 0 , italic_x italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p" id="S2.p2.7"><span class="ltx_text ltx_font_bold" id="S2.p2.7.1">Blockwise Parallel Transformers. </span> 이전의 최첨단 기술은 블록 단위로 주의력을 계산함으로써 완전한 실체화 없이 주의력 계산을 가능하게 하는 혁신적인 기술을 통해 달성된 메모리 활용도의 실질적인 감소를 가져왔다. 이러한 발전은 주의의 메모리 오버헤드를 계층당 <math alttext="2bsh" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mn id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">2</mn><mo id="S2.p2.1.m1.1.1.1" lspace="0px" rspace="0px" xref="S2.p2.1.m1.1.1.1.cmml"></mo><mi id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">b</mi><mo id="S2.p2.1.m1.1.1.1a" lspace="0px" rspace="0px" xref="S2.p2.1.m1.1.1.1.cmml"></mo><mi id="S2.p2.1.m1.1.1.4" xref="S2.p2.1.m1.1.1.4.cmml">s</mi><mo id="S2.p2.1.m1.1.1.1b" lspace="0px" rspace="0px" xref="S2.p2.1.m1.1.1.1.cmml"></mo><mi id="S2.p2.1.m1.1.1.5" xref="S2.p2.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><times id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"></times><cn id="S2.p2.1.m1.1.1.2.cmml" type="integer" xref="S2.p2.1.m1.1.1.2">2</cn><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">𝑏</ci><ci id="S2.p2.1.m1.1.1.4.cmml" xref="S2.p2.1.m1.1.1.4">𝑠</ci><ci id="S2.p2.1.m1.1.1.5.cmml" xref="S2.p2.1.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">2 italic_b italic_s italic_h</annotation></semantics></math> 바이트로 낮추었으며, 여기서 <math alttext="b" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">b</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">italic_b</annotation></semantics></math>는 배치 크기를 나타내고, <math alttext="s" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">italic_s</annotation></semantics></math>는 시퀀스 길이를 나타내며, <math alttext="h" class="ltx_Math" display="inline" id="S2.p2.4.m4.1"><semantics id="S2.p2.4.m4.1a"><mi id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><ci id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.p2.4.m4.1d">italic_h</annotation></semantics></math>는 모델의 숨겨진 크기를 나타낸다. 메모리 사용을 더욱 줄이기 위해 블록별 병렬 트랜스포머 (BPT) <cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>는 각 자기 주의 하위 계층과 연결된 피드포워드 네트워크를 블록 단위로 계산하는 전략을 도입했다. 이 방법은 피드포워드 네트워크의 최대 활성화 크기를 <math alttext="8bsh" class="ltx_Math" display="inline" id="S2.p2.5.m5.1"><semantics id="S2.p2.5.m5.1a"><mrow id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><mn id="S2.p2.5.m5.1.1.2" xref="S2.p2.5.m5.1.1.2.cmml">8</mn><mo id="S2.p2.5.m5.1.1.1" lspace="0px" rspace="0px" xref="S2.p2.5.m5.1.1.1.cmml"></mo><mi id="S2.p2.5.m5.1.1.3" xref="S2.p2.5.m5.1.1.3.cmml">b</mi><mo id="S2.p2.5.m5.1.1.1a" lspace="0px" rspace="0px" xref="S2.p2.5.m5.1.1.1.cmml"></mo><mi id="S2.p2.5.m5.1.1.4" xref="S2.p2.5.m5.1.1.4.cmml">s</mi><mo id="S2.p2.5.m5.1.1.1b" lspace="0px" rspace="0px" xref="S2.p2.5.m5.1.1.1.cmml"></mo><mi id="S2.p2.5.m5.1.1.5" xref="S2.p2.5.m5.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><times id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1.1"></times><cn id="S2.p2.5.m5.1.1.2.cmml" type="integer" xref="S2.p2.5.m5.1.1.2">8</cn><ci id="S2.p2.5.m5.1.1.3.cmml" xref="S2.p2.5.m5.1.1.3">𝑏</ci><ci id="S2.p2.5.m5.1.1.4.cmml" xref="S2.p2.5.m5.1.1.4">𝑠</ci><ci id="S2.p2.5.m5.1.1.5.cmml" xref="S2.p2.5.m5.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">8bsh</annotation><annotation encoding="application/x-llamapun" id="S2.p2.5.m5.1d">8 italic_b italic_s italic_h</annotation></semantics></math>에서 <math alttext="2bsh" class="ltx_Math" display="inline" id="S2.p2.6.m6.1"><semantics id="S2.p2.6.m6.1a"><mrow id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml"><mn id="S2.p2.6.m6.1.1.2" xref="S2.p2.6.m6.1.1.2.cmml">2</mn><mo id="S2.p2.6.m6.1.1.1" lspace="0px" rspace="0px" xref="S2.p2.6.m6.1.1.1.cmml"></mo><mi id="S2.p2.6.m6.1.1.3" xref="S2.p2.6.m6.1.1.3.cmml">b</mi><mo id="S2.p2.6.m6.1.1.1a" lspace="0px" rspace="0px" xref="S2.p2.6.m6.1.1.1.cmml"></mo><mi id="S2.p2.6.m6.1.1.4" xref="S2.p2.6.m6.1.1.4.cmml">s</mi><mo id="S2.p2.6.m6.1.1.1b" lspace="0px" rspace="0px" xref="S2.p2.6.m6.1.1.1.cmml"></mo><mi id="S2.p2.6.m6.1.1.5" xref="S2.p2.6.m6.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><apply id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1"><times id="S2.p2.6.m6.1.1.1.cmml" xref="S2.p2.6.m6.1.1.1"></times><cn id="S2.p2.6.m6.1.1.2.cmml" type="integer" xref="S2.p2.6.m6.1.1.2">2</cn><ci id="S2.p2.6.m6.1.1.3.cmml" xref="S2.p2.6.m6.1.1.3">𝑏</ci><ci id="S2.p2.6.m6.1.1.4.cmml" xref="S2.p2.6.m6.1.1.4">𝑠</ci><ci id="S2.p2.6.m6.1.1.5.cmml" xref="S2.p2.6.m6.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S2.p2.6.m6.1d">2 italic_b italic_s italic_h</annotation></semantics></math>로 효과적으로 제한한다. 메모리 효율성에 대한 보다 상세한 분석을 위해, 이에 제공된 논의를 참조하기 바란다. 요약하면, 최신 트랜스포머 계층의 활성화 메모리 비용은 <math alttext="2bsh" class="ltx_Math" display="inline" id="S2.p2.7.m7.1"><semantics id="S2.p2.7.m7.1a"><mrow id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml"><mn id="S2.p2.7.m7.1.1.2" xref="S2.p2.7.m7.1.1.2.cmml">2</mn><mo id="S2.p2.7.m7.1.1.1" lspace="0px" rspace="0px" xref="S2.p2.7.m7.1.1.1.cmml"></mo><mi id="S2.p2.7.m7.1.1.3" xref="S2.p2.7.m7.1.1.3.cmml">b</mi><mo id="S2.p2.7.m7.1.1.1a" lspace="0px" rspace="0px" xref="S2.p2.7.m7.1.1.1.cmml"></mo><mi id="S2.p2.7.m7.1.1.4" xref="S2.p2.7.m7.1.1.4.cmml">s</mi><mo id="S2.p2.7.m7.1.1.1b" lspace="0px" rspace="0px" xref="S2.p2.7.m7.1.1.1.cmml"></mo><mi id="S2.p2.7.m7.1.1.5" xref="S2.p2.7.m7.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><apply id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1"><times id="S2.p2.7.m7.1.1.1.cmml" xref="S2.p2.7.m7.1.1.1"></times><cn id="S2.p2.7.m7.1.1.2.cmml" type="integer" xref="S2.p2.7.m7.1.1.2">2</cn><ci id="S2.p2.7.m7.1.1.3.cmml" xref="S2.p2.7.m7.1.1.3">𝑏</ci><ci id="S2.p2.7.m7.1.1.4.cmml" xref="S2.p2.7.m7.1.1.4">𝑠</ci><ci id="S2.p2.7.m7.1.1.5.cmml" xref="S2.p2.7.m7.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S2.p2.7.m7.1d">2 italic_b italic_s italic_h</annotation></semantics></math>이다.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Large Output of Each Layer. </span> BPT는 Transformers에서 메모리 수요를 상당히 감소시키지만, 각 레이어의 출력을 저장해야 하기 때문에 컨텍스트 길이를 확장하기 위한 주요 과제를 여전히 제시한다. 이 저장소는 모든 요소 간의 상호 작용(n~n개의 상호 작용)을 포함하는 자기 주의의 고유한 특성으로 인해 중요하다. 이러한 저장된 출력이 없으면 후속 레이어의 자체 주의가 계산적으로 비실용적이 되어 각 시퀀스 요소에 대한 재계산이 필요하다. 간단히 말해서, 배치 크기가 1인 1억 개의 토큰을 처리하려면 숨겨진 크기가 1024인 적당한 모델의 경우에도 1000GB 이상의 메모리가 필요하다. 대조적으로, 현대의 GPU와 TPU는 일반적으로 100GB 미만의 고대역폭 메모리(HBM)를 제공하며, 상당한 HBM 확장에 대한 전망은 물리적 한계와 높은 제조 비용에 의해 방해를 받는다.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Ring Attention</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p" id="S3.p1.1">우리의 주요 목표는 오버헤드를 추가하지 않고 여러 호스트에 긴 시퀀스를 효율적으로 분산함으로써 개별 장치에 의해 부과되는 메모리 제약을 제거하는 것이다. 이러한 목적을 달성하기 위해, 우리는 블록 병렬 트랜스포머(BPT) 프레임워크 <cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>에 대한 향상을 제안한다.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2310.01889/assets/figures/merged.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 2:</span><span class="ltx_text ltx_font_bold" id="S3.F2.3.1">Top(a):</span> 원본 Transformer와 동일한 모델 아키텍처를 사용하지만 compute를 재구성합니다.</figcaption>
In the diagram, we explain this by showing that in a ring of hosts, each host holds one query block, and key-value blocks traverse through a ring of hosts for attention and feedforward computations in a block-by-block fashion.
As we compute attention, each host sends key-value blocks to the next host while receives key-value blocks from the preceding host. The communication is overlapped with the computation of blockwise attention and feedforward.
<span id="S3.F2.4.2" class="ltx_text ltx_font_bold">Bottom (b):</span> We compute the original Transformer block-by-block.
Each host is responsible for one iteration of the query’s outer loop, while the key-value blocks rotate among the hosts. As visualized, a device starts with the first query block on the left; then we iterate over the key-value blocks sequence positioned horizontally.
The query block, combined with the key-value blocks, are used to compute self-attention (yellow box), whose output is pass to feedforward network (cyan box).
</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p" id="S3.p2.1">상이한 호스트들에 걸쳐 입력 시퀀스를 분배할 때, 각각의 호스트는 그 지정된 블록에 대응하는 블록별 어텐션의 외부 루프의 하나의 엘리먼트, 뿐만 아니라 그 블록에 특정한 피드포워드 네트워크를 실행할 책임이 있다. 이러한 작업은 다른 호스트와의 통신을 필요로 하지 않습니다. 그러나 내부 루프에는 다른 호스트에서 블록을 가져와야 하는 키 값 블록 상호 작용이 수반되는 문제가 발생한다. 각 호스트는 하나의 키 값 블록만 가지고 있기 때문에 다른 호스트에서 블록을 가져오는 순진한 접근법은 두 가지 중요한 문제를 초래한다. 먼저, 시스템이 필요한 키-값 블록들을 수신하기 위해 대기함에 따라 계산 지연을 도입한다. 둘째, 키-밸류 블록들의 축적은 메모리 사용을 증가시키며, 이는 메모리 비용 감소의 목적을 무효화한다.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p" id="S3.p3.6"><span class="ltx_text ltx_font_bold" id="S3.p3.6.1">Ring-Based Blockwise Attention. </span> 앞에서 언급한 문제를 해결하기 위해 내부 루프의 키 값 블록 연산의 순열 불변 특성을 활용합니다. 이 속성은 각 블록의 통계가 리스케일링을 위해 올바르게 조합되는 한 쿼리 블록과 키-값 블록 그룹 사이의 자체 주의가 임의의 순서로 계산될 수 있다는 사실에서 비롯된다. 우리는 모든 호스트를 링 구조를 형성하는 것으로 개념화함으로써 이 속성을 활용한다: 호스트-<math alttext="1" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mn id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><cn id="S3.p3.1.m1.1.1.cmml" type="integer" xref="S3.p3.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">1</annotation></semantics></math>, 호스트-<math alttext="2" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mn id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><cn id="S3.p3.2.m2.1.1.cmml" type="integer" xref="S3.p3.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">2</annotation></semantics></math>, …, 호스트-<math alttext="N" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">italic_N</annotation></semantics></math>. 블록 단위의 어텐션 및 피드포워드를 계산함에 따라, 각 호스트는 어텐션 계산에 사용되는 키-값 블록들을 이전 호스트로부터 키-값 블록들을 수신하는 동안 다음 호스트로 동시에 전송함으로써 효율적으로 좌표화하고 블록 단위의 계산과 블록의 전송을 효과적으로 중첩시킨다. 구체적으로, 임의의 호스트-<math alttext="i" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><mi id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><ci id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">italic_i</annotation></semantics></math>에 대해, 자신의 쿼리 블록과 키-값 블록 사이의 주의력 계산 동안, 선행 호스트-<math alttext="(i-1)" class="ltx_Math" display="inline" id="S3.p3.6.m6.1"><semantics id="S3.p3.6.m6.1a"><mrow id="S3.p3.6.m6.1.1.1" xref="S3.p3.6.m6.1.1.1.1.cmml"><mo id="S3.p3.6.m6.1.1.1.2" stretchy="false" xref="S3.p3.6.m6.1.1.1.1.cmml">(</mo><mrow id="S3.p3.6.m6.1.1.1.1" xref="S3.p3.6.m6.1.1.1.1.cmml"><mi id="S3.p3.6.m6.1.1.1.1.2" xref="S3.p3.6.m6.1.1.1.1.2.cmml">i</mi><mo id="S3.p3.6.m6.1.1.1.1.1" xref="S3.p3.6.m6.1.1.1.1.1.cmml">−</mo><mn id="S3.p3.6.m6.1.1.1.1.3" xref="S3.p3.6.m6.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.p3.6.m6.1.1.1.3" stretchy="false" xref="S3.p3.6.m6.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.1b"><apply id="S3.p3.6.m6.1.1.1.1.cmml" xref="S3.p3.6.m6.1.1.1"><minus id="S3.p3.6.m6.1.1.1.1.1.cmml" xref="S3.p3.6.m6.1.1.1.1.1"></minus><ci id="S3.p3.6.m6.1.1.1.1.2.cmml" xref="S3.p3.6.m6.1.1.1.1.2">𝑖</ci><cn id="S3.p3.6.m6.1.1.1.1.3.cmml" type="integer" xref="S3.p3.6.m6.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.1c">(i-1)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.6.m6.1d">( italic_i - 1 )</annotation></semantics></math>로부터 키-값 블록을 수신하면서 동시에 키-값 블록을 다음 호스트-<math alttext="(i+1)" class="ltx_Math" display="inline" id="S3.p3.5.m5.1"><semantics id="S3.p3.5.m5.1a"><mrow id="S3.p3.5.m5.1.1.1" xref="S3.p3.5.m5.1.1.1.1.cmml"><mo id="S3.p3.5.m5.1.1.1.2" stretchy="false" xref="S3.p3.5.m5.1.1.1.1.cmml">(</mo><mrow id="S3.p3.5.m5.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.cmml"><mi id="S3.p3.5.m5.1.1.1.1.2" xref="S3.p3.5.m5.1.1.1.1.2.cmml">i</mi><mo id="S3.p3.5.m5.1.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.1.cmml">+</mo><mn id="S3.p3.5.m5.1.1.1.1.3" xref="S3.p3.5.m5.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.p3.5.m5.1.1.1.3" stretchy="false" xref="S3.p3.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><apply id="S3.p3.5.m5.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1"><plus id="S3.p3.5.m5.1.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1.1.1"></plus><ci id="S3.p3.5.m5.1.1.1.1.2.cmml" xref="S3.p3.5.m5.1.1.1.1.2">𝑖</ci><cn id="S3.p3.5.m5.1.1.1.1.3.cmml" type="integer" xref="S3.p3.5.m5.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">(i+1)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m5.1d">( italic_i + 1 )</annotation></semantics></math>로 전송한다. 계산 시간이 키-값 블록을 전송하는 데 필요한 시간을 초과하는 경우, 이는 추가적인 통신 비용을 초래하지 않는다. 이 중첩 메커니즘은 동일한 연산과 기술이 사용될 수 있기 때문에 접근 방식의 전진 및 후진 패스에 모두 적용된다.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 1:</span>서로 다른 Transformer 아키텍처 간의 최대 활성화 크기 비교.</figcaption>
Here, <math id="S3.T1.8.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S3.T1.8.m1.1b"><mi id="S3.T1.8.m1.1.1" xref="S3.T1.8.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S3.T1.8.m1.1c"><ci id="S3.T1.8.m1.1.1.cmml" xref="S3.T1.8.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.m1.1d">b</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.m1.1e">italic_b</annotation></semantics></math> is batch size, <math id="S3.T1.9.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.T1.9.m2.1b"><mi id="S3.T1.9.m2.1.1" xref="S3.T1.9.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.T1.9.m2.1c"><ci id="S3.T1.9.m2.1.1.cmml" xref="S3.T1.9.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.m2.1d">h</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.m2.1e">italic_h</annotation></semantics></math> is hidden dimension, <math id="S3.T1.10.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.T1.10.m3.1b"><mi id="S3.T1.10.m3.1.1" xref="S3.T1.10.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.T1.10.m3.1c"><ci id="S3.T1.10.m3.1.1.cmml" xref="S3.T1.10.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.m3.1d">n</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.m3.1e">italic_n</annotation></semantics></math> is number of head, <math id="S3.T1.11.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.T1.11.m4.1b"><mi id="S3.T1.11.m4.1.1" xref="S3.T1.11.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.T1.11.m4.1c"><ci id="S3.T1.11.m4.1.1.cmml" xref="S3.T1.11.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.m4.1d">s</annotation><annotation encoding="application/x-llamapun" id="S3.T1.11.m4.1e">italic_s</annotation></semantics></math> is sequence length, <math id="S3.T1.12.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.T1.12.m5.1b"><mi id="S3.T1.12.m5.1.1" xref="S3.T1.12.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.T1.12.m5.1c"><ci id="S3.T1.12.m5.1.1.cmml" xref="S3.T1.12.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.m5.1d">c</annotation><annotation encoding="application/x-llamapun" id="S3.T1.12.m5.1e">italic_c</annotation></semantics></math> is block size, the block size (<math id="S3.T1.13.m6.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.T1.13.m6.1b"><mi id="S3.T1.13.m6.1.1" xref="S3.T1.13.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.T1.13.m6.1c"><ci id="S3.T1.13.m6.1.1.cmml" xref="S3.T1.13.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.m6.1d">c</annotation><annotation encoding="application/x-llamapun" id="S3.T1.13.m6.1e">italic_c</annotation></semantics></math>) is independent of the input sequence length (<math id="S3.T1.14.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.T1.14.m7.1b"><mi id="S3.T1.14.m7.1.1" xref="S3.T1.14.m7.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.T1.14.m7.1c"><ci id="S3.T1.14.m7.1.1.cmml" xref="S3.T1.14.m7.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.m7.1d">s</annotation><annotation encoding="application/x-llamapun" id="S3.T1.14.m7.1e">italic_s</annotation></semantics></math>).
The comparison is between vanilla Transformer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">Vaswani et&nbsp;al. (<a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite>, memory efficient attention&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, memory efficient attention and feedforward&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>, and our proposed approach Ring Attention.
Numbers are shown in bytes per layer, assuming <span id="S3.T1.28.1" class="ltx_text ltx_font_italic">bfloat16</span> precision.
</figcaption>
<div id="S3.T1.26" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:266.8pt;height:113.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.1pt,0.1pt) scale(0.999,0.999) ;">
<table id="S3.T1.26.12" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T1.26.12.13" class="ltx_tr">
<td id="S3.T1.26.12.13.1" class="ltx_td ltx_align_left ltx_border_tt">Layer Type</td>
<td id="S3.T1.26.12.13.2" class="ltx_td ltx_align_right ltx_border_tt">Self-Attention</td>
<td id="S3.T1.26.12.13.3" class="ltx_td ltx_align_right ltx_border_tt">FeedForward</td>
<td id="S3.T1.26.12.13.4" class="ltx_td ltx_align_right ltx_border_tt">Total</td>
</tr>
<tr id="S3.T1.17.3.3" class="ltx_tr">
<td id="S3.T1.17.3.3.4" class="ltx_td ltx_align_left ltx_border_t">Vanilla</td>
<td id="S3.T1.15.1.1.1" class="ltx_td ltx_align_right ltx_border_t"><math id="S3.T1.15.1.1.1.m1.1" class="ltx_Math" alttext="2bns^{2}" display="inline"><semantics id="S3.T1.15.1.1.1.m1.1a"><mrow id="S3.T1.15.1.1.1.m1.1.1" xref="S3.T1.15.1.1.1.m1.1.1.cmml"><mn id="S3.T1.15.1.1.1.m1.1.1.2" xref="S3.T1.15.1.1.1.m1.1.1.2.cmml">2</mn><mo id="S3.T1.15.1.1.1.m1.1.1.1" xref="S3.T1.15.1.1.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.15.1.1.1.m1.1.1.3" xref="S3.T1.15.1.1.1.m1.1.1.3.cmml">b</mi><mo id="S3.T1.15.1.1.1.m1.1.1.1a" xref="S3.T1.15.1.1.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.15.1.1.1.m1.1.1.4" xref="S3.T1.15.1.1.1.m1.1.1.4.cmml">n</mi><mo id="S3.T1.15.1.1.1.m1.1.1.1b" xref="S3.T1.15.1.1.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S3.T1.15.1.1.1.m1.1.1.5" xref="S3.T1.15.1.1.1.m1.1.1.5.cmml"><mi id="S3.T1.15.1.1.1.m1.1.1.5.2" xref="S3.T1.15.1.1.1.m1.1.1.5.2.cmml">s</mi><mn id="S3.T1.15.1.1.1.m1.1.1.5.3" xref="S3.T1.15.1.1.1.m1.1.1.5.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.15.1.1.1.m1.1b"><apply id="S3.T1.15.1.1.1.m1.1.1.cmml" xref="S3.T1.15.1.1.1.m1.1.1"><times id="S3.T1.15.1.1.1.m1.1.1.1.cmml" xref="S3.T1.15.1.1.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.15.1.1.1.m1.1.1.2.cmml" xref="S3.T1.15.1.1.1.m1.1.1.2">2</cn><ci id="S3.T1.15.1.1.1.m1.1.1.3.cmml" xref="S3.T1.15.1.1.1.m1.1.1.3">𝑏</ci><ci id="S3.T1.15.1.1.1.m1.1.1.4.cmml" xref="S3.T1.15.1.1.1.m1.1.1.4">𝑛</ci><apply id="S3.T1.15.1.1.1.m1.1.1.5.cmml" xref="S3.T1.15.1.1.1.m1.1.1.5"><csymbol cd="ambiguous" id="S3.T1.15.1.1.1.m1.1.1.5.1.cmml" xref="S3.T1.15.1.1.1.m1.1.1.5">superscript</csymbol><ci id="S3.T1.15.1.1.1.m1.1.1.5.2.cmml" xref="S3.T1.15.1.1.1.m1.1.1.5.2">𝑠</ci><cn type="integer" id="S3.T1.15.1.1.1.m1.1.1.5.3.cmml" xref="S3.T1.15.1.1.1.m1.1.1.5.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.1.1.1.m1.1c">2bns^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.15.1.1.1.m1.1d">2 italic_b italic_n italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td id="S3.T1.16.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><math id="S3.T1.16.2.2.2.m1.1" class="ltx_Math" alttext="8bsh" display="inline"><semantics id="S3.T1.16.2.2.2.m1.1a"><mrow id="S3.T1.16.2.2.2.m1.1.1" xref="S3.T1.16.2.2.2.m1.1.1.cmml"><mn id="S3.T1.16.2.2.2.m1.1.1.2" xref="S3.T1.16.2.2.2.m1.1.1.2.cmml">8</mn><mo id="S3.T1.16.2.2.2.m1.1.1.1" xref="S3.T1.16.2.2.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.16.2.2.2.m1.1.1.3" xref="S3.T1.16.2.2.2.m1.1.1.3.cmml">b</mi><mo id="S3.T1.16.2.2.2.m1.1.1.1a" xref="S3.T1.16.2.2.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.16.2.2.2.m1.1.1.4" xref="S3.T1.16.2.2.2.m1.1.1.4.cmml">s</mi><mo id="S3.T1.16.2.2.2.m1.1.1.1b" xref="S3.T1.16.2.2.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.16.2.2.2.m1.1.1.5" xref="S3.T1.16.2.2.2.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.16.2.2.2.m1.1b"><apply id="S3.T1.16.2.2.2.m1.1.1.cmml" xref="S3.T1.16.2.2.2.m1.1.1"><times id="S3.T1.16.2.2.2.m1.1.1.1.cmml" xref="S3.T1.16.2.2.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.16.2.2.2.m1.1.1.2.cmml" xref="S3.T1.16.2.2.2.m1.1.1.2">8</cn><ci id="S3.T1.16.2.2.2.m1.1.1.3.cmml" xref="S3.T1.16.2.2.2.m1.1.1.3">𝑏</ci><ci id="S3.T1.16.2.2.2.m1.1.1.4.cmml" xref="S3.T1.16.2.2.2.m1.1.1.4">𝑠</ci><ci id="S3.T1.16.2.2.2.m1.1.1.5.cmml" xref="S3.T1.16.2.2.2.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.2.2.2.m1.1c">8bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.16.2.2.2.m1.1d">8 italic_b italic_s italic_h</annotation></semantics></math></td>
<td id="S3.T1.17.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><math id="S3.T1.17.3.3.3.m1.1" class="ltx_Math" alttext="2bhs^{2}" display="inline"><semantics id="S3.T1.17.3.3.3.m1.1a"><mrow id="S3.T1.17.3.3.3.m1.1.1" xref="S3.T1.17.3.3.3.m1.1.1.cmml"><mn id="S3.T1.17.3.3.3.m1.1.1.2" xref="S3.T1.17.3.3.3.m1.1.1.2.cmml">2</mn><mo id="S3.T1.17.3.3.3.m1.1.1.1" xref="S3.T1.17.3.3.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.17.3.3.3.m1.1.1.3" xref="S3.T1.17.3.3.3.m1.1.1.3.cmml">b</mi><mo id="S3.T1.17.3.3.3.m1.1.1.1a" xref="S3.T1.17.3.3.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.17.3.3.3.m1.1.1.4" xref="S3.T1.17.3.3.3.m1.1.1.4.cmml">h</mi><mo id="S3.T1.17.3.3.3.m1.1.1.1b" xref="S3.T1.17.3.3.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><msup id="S3.T1.17.3.3.3.m1.1.1.5" xref="S3.T1.17.3.3.3.m1.1.1.5.cmml"><mi id="S3.T1.17.3.3.3.m1.1.1.5.2" xref="S3.T1.17.3.3.3.m1.1.1.5.2.cmml">s</mi><mn id="S3.T1.17.3.3.3.m1.1.1.5.3" xref="S3.T1.17.3.3.3.m1.1.1.5.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.17.3.3.3.m1.1b"><apply id="S3.T1.17.3.3.3.m1.1.1.cmml" xref="S3.T1.17.3.3.3.m1.1.1"><times id="S3.T1.17.3.3.3.m1.1.1.1.cmml" xref="S3.T1.17.3.3.3.m1.1.1.1"></times><cn type="integer" id="S3.T1.17.3.3.3.m1.1.1.2.cmml" xref="S3.T1.17.3.3.3.m1.1.1.2">2</cn><ci id="S3.T1.17.3.3.3.m1.1.1.3.cmml" xref="S3.T1.17.3.3.3.m1.1.1.3">𝑏</ci><ci id="S3.T1.17.3.3.3.m1.1.1.4.cmml" xref="S3.T1.17.3.3.3.m1.1.1.4">ℎ</ci><apply id="S3.T1.17.3.3.3.m1.1.1.5.cmml" xref="S3.T1.17.3.3.3.m1.1.1.5"><csymbol cd="ambiguous" id="S3.T1.17.3.3.3.m1.1.1.5.1.cmml" xref="S3.T1.17.3.3.3.m1.1.1.5">superscript</csymbol><ci id="S3.T1.17.3.3.3.m1.1.1.5.2.cmml" xref="S3.T1.17.3.3.3.m1.1.1.5.2">𝑠</ci><cn type="integer" id="S3.T1.17.3.3.3.m1.1.1.5.3.cmml" xref="S3.T1.17.3.3.3.m1.1.1.5.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.3.3.3.m1.1c">2bhs^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.17.3.3.3.m1.1d">2 italic_b italic_h italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.20.6.6" class="ltx_tr">
<td id="S3.T1.20.6.6.4" class="ltx_td ltx_align_left">Memory efficient attention</td>
<td id="S3.T1.18.4.4.1" class="ltx_td ltx_align_right"><math id="S3.T1.18.4.4.1.m1.1" class="ltx_Math" alttext="2bsh+4bch" display="inline"><semantics id="S3.T1.18.4.4.1.m1.1a"><mrow id="S3.T1.18.4.4.1.m1.1.1" xref="S3.T1.18.4.4.1.m1.1.1.cmml"><mrow id="S3.T1.18.4.4.1.m1.1.1.2" xref="S3.T1.18.4.4.1.m1.1.1.2.cmml"><mn id="S3.T1.18.4.4.1.m1.1.1.2.2" xref="S3.T1.18.4.4.1.m1.1.1.2.2.cmml">2</mn><mo id="S3.T1.18.4.4.1.m1.1.1.2.1" xref="S3.T1.18.4.4.1.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.2.3" xref="S3.T1.18.4.4.1.m1.1.1.2.3.cmml">b</mi><mo id="S3.T1.18.4.4.1.m1.1.1.2.1a" xref="S3.T1.18.4.4.1.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.2.4" xref="S3.T1.18.4.4.1.m1.1.1.2.4.cmml">s</mi><mo id="S3.T1.18.4.4.1.m1.1.1.2.1b" xref="S3.T1.18.4.4.1.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.2.5" xref="S3.T1.18.4.4.1.m1.1.1.2.5.cmml">h</mi></mrow><mo id="S3.T1.18.4.4.1.m1.1.1.1" xref="S3.T1.18.4.4.1.m1.1.1.1.cmml">+</mo><mrow id="S3.T1.18.4.4.1.m1.1.1.3" xref="S3.T1.18.4.4.1.m1.1.1.3.cmml"><mn id="S3.T1.18.4.4.1.m1.1.1.3.2" xref="S3.T1.18.4.4.1.m1.1.1.3.2.cmml">4</mn><mo id="S3.T1.18.4.4.1.m1.1.1.3.1" xref="S3.T1.18.4.4.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.3.3" xref="S3.T1.18.4.4.1.m1.1.1.3.3.cmml">b</mi><mo id="S3.T1.18.4.4.1.m1.1.1.3.1a" xref="S3.T1.18.4.4.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.3.4" xref="S3.T1.18.4.4.1.m1.1.1.3.4.cmml">c</mi><mo id="S3.T1.18.4.4.1.m1.1.1.3.1b" xref="S3.T1.18.4.4.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.18.4.4.1.m1.1.1.3.5" xref="S3.T1.18.4.4.1.m1.1.1.3.5.cmml">h</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.18.4.4.1.m1.1b"><apply id="S3.T1.18.4.4.1.m1.1.1.cmml" xref="S3.T1.18.4.4.1.m1.1.1"><plus id="S3.T1.18.4.4.1.m1.1.1.1.cmml" xref="S3.T1.18.4.4.1.m1.1.1.1"></plus><apply id="S3.T1.18.4.4.1.m1.1.1.2.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2"><times id="S3.T1.18.4.4.1.m1.1.1.2.1.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.1"></times><cn type="integer" id="S3.T1.18.4.4.1.m1.1.1.2.2.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.2">2</cn><ci id="S3.T1.18.4.4.1.m1.1.1.2.3.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.3">𝑏</ci><ci id="S3.T1.18.4.4.1.m1.1.1.2.4.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.4">𝑠</ci><ci id="S3.T1.18.4.4.1.m1.1.1.2.5.cmml" xref="S3.T1.18.4.4.1.m1.1.1.2.5">ℎ</ci></apply><apply id="S3.T1.18.4.4.1.m1.1.1.3.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3"><times id="S3.T1.18.4.4.1.m1.1.1.3.1.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.1"></times><cn type="integer" id="S3.T1.18.4.4.1.m1.1.1.3.2.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.2">4</cn><ci id="S3.T1.18.4.4.1.m1.1.1.3.3.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.3">𝑏</ci><ci id="S3.T1.18.4.4.1.m1.1.1.3.4.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.4">𝑐</ci><ci id="S3.T1.18.4.4.1.m1.1.1.3.5.cmml" xref="S3.T1.18.4.4.1.m1.1.1.3.5">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.18.4.4.1.m1.1c">2bsh+4bch</annotation><annotation encoding="application/x-llamapun" id="S3.T1.18.4.4.1.m1.1d">2 italic_b italic_s italic_h + 4 italic_b italic_c italic_h</annotation></semantics></math></td>
<td id="S3.T1.19.5.5.2" class="ltx_td ltx_align_right"><math id="S3.T1.19.5.5.2.m1.1" class="ltx_Math" alttext="8bsh" display="inline"><semantics id="S3.T1.19.5.5.2.m1.1a"><mrow id="S3.T1.19.5.5.2.m1.1.1" xref="S3.T1.19.5.5.2.m1.1.1.cmml"><mn id="S3.T1.19.5.5.2.m1.1.1.2" xref="S3.T1.19.5.5.2.m1.1.1.2.cmml">8</mn><mo id="S3.T1.19.5.5.2.m1.1.1.1" xref="S3.T1.19.5.5.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.19.5.5.2.m1.1.1.3" xref="S3.T1.19.5.5.2.m1.1.1.3.cmml">b</mi><mo id="S3.T1.19.5.5.2.m1.1.1.1a" xref="S3.T1.19.5.5.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.19.5.5.2.m1.1.1.4" xref="S3.T1.19.5.5.2.m1.1.1.4.cmml">s</mi><mo id="S3.T1.19.5.5.2.m1.1.1.1b" xref="S3.T1.19.5.5.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.19.5.5.2.m1.1.1.5" xref="S3.T1.19.5.5.2.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.19.5.5.2.m1.1b"><apply id="S3.T1.19.5.5.2.m1.1.1.cmml" xref="S3.T1.19.5.5.2.m1.1.1"><times id="S3.T1.19.5.5.2.m1.1.1.1.cmml" xref="S3.T1.19.5.5.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.19.5.5.2.m1.1.1.2.cmml" xref="S3.T1.19.5.5.2.m1.1.1.2">8</cn><ci id="S3.T1.19.5.5.2.m1.1.1.3.cmml" xref="S3.T1.19.5.5.2.m1.1.1.3">𝑏</ci><ci id="S3.T1.19.5.5.2.m1.1.1.4.cmml" xref="S3.T1.19.5.5.2.m1.1.1.4">𝑠</ci><ci id="S3.T1.19.5.5.2.m1.1.1.5.cmml" xref="S3.T1.19.5.5.2.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.19.5.5.2.m1.1c">8bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.19.5.5.2.m1.1d">8 italic_b italic_s italic_h</annotation></semantics></math></td>
<td id="S3.T1.20.6.6.3" class="ltx_td ltx_align_right"><math id="S3.T1.20.6.6.3.m1.1" class="ltx_Math" alttext="8bsh" display="inline"><semantics id="S3.T1.20.6.6.3.m1.1a"><mrow id="S3.T1.20.6.6.3.m1.1.1" xref="S3.T1.20.6.6.3.m1.1.1.cmml"><mn id="S3.T1.20.6.6.3.m1.1.1.2" xref="S3.T1.20.6.6.3.m1.1.1.2.cmml">8</mn><mo id="S3.T1.20.6.6.3.m1.1.1.1" xref="S3.T1.20.6.6.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.20.6.6.3.m1.1.1.3" xref="S3.T1.20.6.6.3.m1.1.1.3.cmml">b</mi><mo id="S3.T1.20.6.6.3.m1.1.1.1a" xref="S3.T1.20.6.6.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.20.6.6.3.m1.1.1.4" xref="S3.T1.20.6.6.3.m1.1.1.4.cmml">s</mi><mo id="S3.T1.20.6.6.3.m1.1.1.1b" xref="S3.T1.20.6.6.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.20.6.6.3.m1.1.1.5" xref="S3.T1.20.6.6.3.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.20.6.6.3.m1.1b"><apply id="S3.T1.20.6.6.3.m1.1.1.cmml" xref="S3.T1.20.6.6.3.m1.1.1"><times id="S3.T1.20.6.6.3.m1.1.1.1.cmml" xref="S3.T1.20.6.6.3.m1.1.1.1"></times><cn type="integer" id="S3.T1.20.6.6.3.m1.1.1.2.cmml" xref="S3.T1.20.6.6.3.m1.1.1.2">8</cn><ci id="S3.T1.20.6.6.3.m1.1.1.3.cmml" xref="S3.T1.20.6.6.3.m1.1.1.3">𝑏</ci><ci id="S3.T1.20.6.6.3.m1.1.1.4.cmml" xref="S3.T1.20.6.6.3.m1.1.1.4">𝑠</ci><ci id="S3.T1.20.6.6.3.m1.1.1.5.cmml" xref="S3.T1.20.6.6.3.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.20.6.6.3.m1.1c">8bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.20.6.6.3.m1.1d">8 italic_b italic_s italic_h</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.23.9.9" class="ltx_tr">
<td id="S3.T1.23.9.9.4" class="ltx_td ltx_align_left">
<span id="S3.T1.23.9.9.4.1" class="ltx_text"></span><span id="S3.T1.23.9.9.4.2" class="ltx_text">
<span id="S3.T1.23.9.9.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T1.23.9.9.4.2.1.1" class="ltx_tr">
<span id="S3.T1.23.9.9.4.2.1.1.1" class="ltx_td ltx_align_left">Memory efficient attention</span></span>
<span id="S3.T1.23.9.9.4.2.1.2" class="ltx_tr">
<span id="S3.T1.23.9.9.4.2.1.2.1" class="ltx_td ltx_align_left">and feedforward</span></span>
</span></span> <span id="S3.T1.23.9.9.4.3" class="ltx_text"></span>
</td>
<td id="S3.T1.21.7.7.1" class="ltx_td ltx_align_right"><math id="S3.T1.21.7.7.1.m1.1" class="ltx_Math" alttext="2bsh" display="inline"><semantics id="S3.T1.21.7.7.1.m1.1a"><mrow id="S3.T1.21.7.7.1.m1.1.1" xref="S3.T1.21.7.7.1.m1.1.1.cmml"><mn id="S3.T1.21.7.7.1.m1.1.1.2" xref="S3.T1.21.7.7.1.m1.1.1.2.cmml">2</mn><mo id="S3.T1.21.7.7.1.m1.1.1.1" xref="S3.T1.21.7.7.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.21.7.7.1.m1.1.1.3" xref="S3.T1.21.7.7.1.m1.1.1.3.cmml">b</mi><mo id="S3.T1.21.7.7.1.m1.1.1.1a" xref="S3.T1.21.7.7.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.21.7.7.1.m1.1.1.4" xref="S3.T1.21.7.7.1.m1.1.1.4.cmml">s</mi><mo id="S3.T1.21.7.7.1.m1.1.1.1b" xref="S3.T1.21.7.7.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.21.7.7.1.m1.1.1.5" xref="S3.T1.21.7.7.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.21.7.7.1.m1.1b"><apply id="S3.T1.21.7.7.1.m1.1.1.cmml" xref="S3.T1.21.7.7.1.m1.1.1"><times id="S3.T1.21.7.7.1.m1.1.1.1.cmml" xref="S3.T1.21.7.7.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.21.7.7.1.m1.1.1.2.cmml" xref="S3.T1.21.7.7.1.m1.1.1.2">2</cn><ci id="S3.T1.21.7.7.1.m1.1.1.3.cmml" xref="S3.T1.21.7.7.1.m1.1.1.3">𝑏</ci><ci id="S3.T1.21.7.7.1.m1.1.1.4.cmml" xref="S3.T1.21.7.7.1.m1.1.1.4">𝑠</ci><ci id="S3.T1.21.7.7.1.m1.1.1.5.cmml" xref="S3.T1.21.7.7.1.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.21.7.7.1.m1.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.21.7.7.1.m1.1d">2 italic_b italic_s italic_h</annotation></semantics></math></td>
<td id="S3.T1.22.8.8.2" class="ltx_td ltx_align_right"><math id="S3.T1.22.8.8.2.m1.1" class="ltx_Math" alttext="2bsh" display="inline"><semantics id="S3.T1.22.8.8.2.m1.1a"><mrow id="S3.T1.22.8.8.2.m1.1.1" xref="S3.T1.22.8.8.2.m1.1.1.cmml"><mn id="S3.T1.22.8.8.2.m1.1.1.2" xref="S3.T1.22.8.8.2.m1.1.1.2.cmml">2</mn><mo id="S3.T1.22.8.8.2.m1.1.1.1" xref="S3.T1.22.8.8.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.22.8.8.2.m1.1.1.3" xref="S3.T1.22.8.8.2.m1.1.1.3.cmml">b</mi><mo id="S3.T1.22.8.8.2.m1.1.1.1a" xref="S3.T1.22.8.8.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.22.8.8.2.m1.1.1.4" xref="S3.T1.22.8.8.2.m1.1.1.4.cmml">s</mi><mo id="S3.T1.22.8.8.2.m1.1.1.1b" xref="S3.T1.22.8.8.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.22.8.8.2.m1.1.1.5" xref="S3.T1.22.8.8.2.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.22.8.8.2.m1.1b"><apply id="S3.T1.22.8.8.2.m1.1.1.cmml" xref="S3.T1.22.8.8.2.m1.1.1"><times id="S3.T1.22.8.8.2.m1.1.1.1.cmml" xref="S3.T1.22.8.8.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.22.8.8.2.m1.1.1.2.cmml" xref="S3.T1.22.8.8.2.m1.1.1.2">2</cn><ci id="S3.T1.22.8.8.2.m1.1.1.3.cmml" xref="S3.T1.22.8.8.2.m1.1.1.3">𝑏</ci><ci id="S3.T1.22.8.8.2.m1.1.1.4.cmml" xref="S3.T1.22.8.8.2.m1.1.1.4">𝑠</ci><ci id="S3.T1.22.8.8.2.m1.1.1.5.cmml" xref="S3.T1.22.8.8.2.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.22.8.8.2.m1.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.22.8.8.2.m1.1d">2 italic_b italic_s italic_h</annotation></semantics></math></td>
<td id="S3.T1.23.9.9.3" class="ltx_td ltx_align_right"><math id="S3.T1.23.9.9.3.m1.1" class="ltx_Math" alttext="2bsh" display="inline"><semantics id="S3.T1.23.9.9.3.m1.1a"><mrow id="S3.T1.23.9.9.3.m1.1.1" xref="S3.T1.23.9.9.3.m1.1.1.cmml"><mn id="S3.T1.23.9.9.3.m1.1.1.2" xref="S3.T1.23.9.9.3.m1.1.1.2.cmml">2</mn><mo id="S3.T1.23.9.9.3.m1.1.1.1" xref="S3.T1.23.9.9.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.23.9.9.3.m1.1.1.3" xref="S3.T1.23.9.9.3.m1.1.1.3.cmml">b</mi><mo id="S3.T1.23.9.9.3.m1.1.1.1a" xref="S3.T1.23.9.9.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.23.9.9.3.m1.1.1.4" xref="S3.T1.23.9.9.3.m1.1.1.4.cmml">s</mi><mo id="S3.T1.23.9.9.3.m1.1.1.1b" xref="S3.T1.23.9.9.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.23.9.9.3.m1.1.1.5" xref="S3.T1.23.9.9.3.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.23.9.9.3.m1.1b"><apply id="S3.T1.23.9.9.3.m1.1.1.cmml" xref="S3.T1.23.9.9.3.m1.1.1"><times id="S3.T1.23.9.9.3.m1.1.1.1.cmml" xref="S3.T1.23.9.9.3.m1.1.1.1"></times><cn type="integer" id="S3.T1.23.9.9.3.m1.1.1.2.cmml" xref="S3.T1.23.9.9.3.m1.1.1.2">2</cn><ci id="S3.T1.23.9.9.3.m1.1.1.3.cmml" xref="S3.T1.23.9.9.3.m1.1.1.3">𝑏</ci><ci id="S3.T1.23.9.9.3.m1.1.1.4.cmml" xref="S3.T1.23.9.9.3.m1.1.1.4">𝑠</ci><ci id="S3.T1.23.9.9.3.m1.1.1.5.cmml" xref="S3.T1.23.9.9.3.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.23.9.9.3.m1.1c">2bsh</annotation><annotation encoding="application/x-llamapun" id="S3.T1.23.9.9.3.m1.1d">2 italic_b italic_s italic_h</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.26.12.12" class="ltx_tr">
<td id="S3.T1.26.12.12.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Ring Attention</td>
<td id="S3.T1.24.10.10.1" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><math id="S3.T1.24.10.10.1.m1.1" class="ltx_Math" alttext="6bch" display="inline"><semantics id="S3.T1.24.10.10.1.m1.1a"><mrow id="S3.T1.24.10.10.1.m1.1.1" xref="S3.T1.24.10.10.1.m1.1.1.cmml"><mn id="S3.T1.24.10.10.1.m1.1.1.2" xref="S3.T1.24.10.10.1.m1.1.1.2.cmml">6</mn><mo id="S3.T1.24.10.10.1.m1.1.1.1" xref="S3.T1.24.10.10.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.24.10.10.1.m1.1.1.3" xref="S3.T1.24.10.10.1.m1.1.1.3.cmml">b</mi><mo id="S3.T1.24.10.10.1.m1.1.1.1a" xref="S3.T1.24.10.10.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.24.10.10.1.m1.1.1.4" xref="S3.T1.24.10.10.1.m1.1.1.4.cmml">c</mi><mo id="S3.T1.24.10.10.1.m1.1.1.1b" xref="S3.T1.24.10.10.1.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.24.10.10.1.m1.1.1.5" xref="S3.T1.24.10.10.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.24.10.10.1.m1.1b"><apply id="S3.T1.24.10.10.1.m1.1.1.cmml" xref="S3.T1.24.10.10.1.m1.1.1"><times id="S3.T1.24.10.10.1.m1.1.1.1.cmml" xref="S3.T1.24.10.10.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.24.10.10.1.m1.1.1.2.cmml" xref="S3.T1.24.10.10.1.m1.1.1.2">6</cn><ci id="S3.T1.24.10.10.1.m1.1.1.3.cmml" xref="S3.T1.24.10.10.1.m1.1.1.3">𝑏</ci><ci id="S3.T1.24.10.10.1.m1.1.1.4.cmml" xref="S3.T1.24.10.10.1.m1.1.1.4">𝑐</ci><ci id="S3.T1.24.10.10.1.m1.1.1.5.cmml" xref="S3.T1.24.10.10.1.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.24.10.10.1.m1.1c">6bch</annotation><annotation encoding="application/x-llamapun" id="S3.T1.24.10.10.1.m1.1d">6 italic_b italic_c italic_h</annotation></semantics></math></td>
<td id="S3.T1.25.11.11.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><math id="S3.T1.25.11.11.2.m1.1" class="ltx_Math" alttext="2bch" display="inline"><semantics id="S3.T1.25.11.11.2.m1.1a"><mrow id="S3.T1.25.11.11.2.m1.1.1" xref="S3.T1.25.11.11.2.m1.1.1.cmml"><mn id="S3.T1.25.11.11.2.m1.1.1.2" xref="S3.T1.25.11.11.2.m1.1.1.2.cmml">2</mn><mo id="S3.T1.25.11.11.2.m1.1.1.1" xref="S3.T1.25.11.11.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.25.11.11.2.m1.1.1.3" xref="S3.T1.25.11.11.2.m1.1.1.3.cmml">b</mi><mo id="S3.T1.25.11.11.2.m1.1.1.1a" xref="S3.T1.25.11.11.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.25.11.11.2.m1.1.1.4" xref="S3.T1.25.11.11.2.m1.1.1.4.cmml">c</mi><mo id="S3.T1.25.11.11.2.m1.1.1.1b" xref="S3.T1.25.11.11.2.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.25.11.11.2.m1.1.1.5" xref="S3.T1.25.11.11.2.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.25.11.11.2.m1.1b"><apply id="S3.T1.25.11.11.2.m1.1.1.cmml" xref="S3.T1.25.11.11.2.m1.1.1"><times id="S3.T1.25.11.11.2.m1.1.1.1.cmml" xref="S3.T1.25.11.11.2.m1.1.1.1"></times><cn type="integer" id="S3.T1.25.11.11.2.m1.1.1.2.cmml" xref="S3.T1.25.11.11.2.m1.1.1.2">2</cn><ci id="S3.T1.25.11.11.2.m1.1.1.3.cmml" xref="S3.T1.25.11.11.2.m1.1.1.3">𝑏</ci><ci id="S3.T1.25.11.11.2.m1.1.1.4.cmml" xref="S3.T1.25.11.11.2.m1.1.1.4">𝑐</ci><ci id="S3.T1.25.11.11.2.m1.1.1.5.cmml" xref="S3.T1.25.11.11.2.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.25.11.11.2.m1.1c">2bch</annotation><annotation encoding="application/x-llamapun" id="S3.T1.25.11.11.2.m1.1d">2 italic_b italic_c italic_h</annotation></semantics></math></td>
<td id="S3.T1.26.12.12.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><math id="S3.T1.26.12.12.3.m1.1" class="ltx_Math" alttext="6bch" display="inline"><semantics id="S3.T1.26.12.12.3.m1.1a"><mrow id="S3.T1.26.12.12.3.m1.1.1" xref="S3.T1.26.12.12.3.m1.1.1.cmml"><mn id="S3.T1.26.12.12.3.m1.1.1.2" xref="S3.T1.26.12.12.3.m1.1.1.2.cmml">6</mn><mo id="S3.T1.26.12.12.3.m1.1.1.1" xref="S3.T1.26.12.12.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.26.12.12.3.m1.1.1.3" xref="S3.T1.26.12.12.3.m1.1.1.3.cmml">b</mi><mo id="S3.T1.26.12.12.3.m1.1.1.1a" xref="S3.T1.26.12.12.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.26.12.12.3.m1.1.1.4" xref="S3.T1.26.12.12.3.m1.1.1.4.cmml">c</mi><mo id="S3.T1.26.12.12.3.m1.1.1.1b" xref="S3.T1.26.12.12.3.m1.1.1.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T1.26.12.12.3.m1.1.1.5" xref="S3.T1.26.12.12.3.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.26.12.12.3.m1.1b"><apply id="S3.T1.26.12.12.3.m1.1.1.cmml" xref="S3.T1.26.12.12.3.m1.1.1"><times id="S3.T1.26.12.12.3.m1.1.1.1.cmml" xref="S3.T1.26.12.12.3.m1.1.1.1"></times><cn type="integer" id="S3.T1.26.12.12.3.m1.1.1.2.cmml" xref="S3.T1.26.12.12.3.m1.1.1.2">6</cn><ci id="S3.T1.26.12.12.3.m1.1.1.3.cmml" xref="S3.T1.26.12.12.3.m1.1.1.3">𝑏</ci><ci id="S3.T1.26.12.12.3.m1.1.1.4.cmml" xref="S3.T1.26.12.12.3.m1.1.1.4">𝑐</ci><ci id="S3.T1.26.12.12.3.m1.1.1.5.cmml" xref="S3.T1.26.12.12.3.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.26.12.12.3.m1.1c">6bch</annotation><annotation encoding="application/x-llamapun" id="S3.T1.26.12.12.3.m1.1d">6 italic_b italic_c italic_h</annotation></semantics></math></td>
</tr>
</tbody></table>
</span></div>
</figure>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p" id="S3.p4.12"><span class="ltx_text ltx_font_bold" id="S3.p4.12.1">Arithmetic Intensity Between Hosts. </span> 계산과 중복 전송할 최소 요구 블록 크기를 결정하기 위해, 각 호스트가 <math alttext="F" class="ltx_Math" display="inline" id="S3.p4.1.m1.1"><semantics id="S3.p4.1.m1.1a"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="S3.p4.1.m1.1d">italic_F</annotation></semantics></math> FLOPS를 갖고 호스트 간의 대역폭이 <math alttext="B" class="ltx_Math" display="inline" id="S3.p4.2.m2.1"><semantics id="S3.p4.2.m2.1a"><mi id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><ci id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.p4.2.m2.1d">italic_B</annotation></semantics></math>로 표기된다고 가정하자. 우리의 접근법은 원형 구성에서 바로 이전 및 다음 호스트와의 상호작용만을 포함하므로, 우리의 분석은 GPU 전체 토폴로지 및 TPU 토러스 토폴로지 모두에 적용된다는 점에 주목할 필요가 있다. 변수를 고려하자: 블록 크기는 <math alttext="c" class="ltx_Math" display="inline" id="S3.p4.3.m3.1"><semantics id="S3.p4.3.m3.1a"><mi id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><ci id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.p4.3.m3.1d">italic_c</annotation></semantics></math>로 표시되고 숨겨진 크기는 <math alttext="d" class="ltx_Math" display="inline" id="S3.p4.4.m4.1"><semantics id="S3.p4.4.m4.1a"><mi id="S3.p4.4.m4.1.1" xref="S3.p4.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.1b"><ci id="S3.p4.4.m4.1.1.cmml" xref="S3.p4.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.p4.4.m4.1d">italic_d</annotation></semantics></math>로 표시된다. 블록 단위로 셀프 어텐션을 계산할 때, 쿼리와 키를 이용하여 어텐션 스코어를 계산하기 위한 <math alttext="2dc^{2}" class="ltx_Math" display="inline" id="S3.p4.5.m5.1"><semantics id="S3.p4.5.m5.1a"><mrow id="S3.p4.5.m5.1.1" xref="S3.p4.5.m5.1.1.cmml"><mn id="S3.p4.5.m5.1.1.2" xref="S3.p4.5.m5.1.1.2.cmml">2</mn><mo id="S3.p4.5.m5.1.1.1" lspace="0px" rspace="0px" xref="S3.p4.5.m5.1.1.1.cmml"></mo><mi id="S3.p4.5.m5.1.1.3" xref="S3.p4.5.m5.1.1.3.cmml">d</mi><mo id="S3.p4.5.m5.1.1.1a" lspace="0px" rspace="0px" xref="S3.p4.5.m5.1.1.1.cmml"></mo><msup id="S3.p4.5.m5.1.1.4" xref="S3.p4.5.m5.1.1.4.cmml"><mi id="S3.p4.5.m5.1.1.4.2" xref="S3.p4.5.m5.1.1.4.2.cmml">c</mi><mn id="S3.p4.5.m5.1.1.4.3" xref="S3.p4.5.m5.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.5.m5.1b"><apply id="S3.p4.5.m5.1.1.cmml" xref="S3.p4.5.m5.1.1"><times id="S3.p4.5.m5.1.1.1.cmml" xref="S3.p4.5.m5.1.1.1"></times><cn id="S3.p4.5.m5.1.1.2.cmml" type="integer" xref="S3.p4.5.m5.1.1.2">2</cn><ci id="S3.p4.5.m5.1.1.3.cmml" xref="S3.p4.5.m5.1.1.3">𝑑</ci><apply id="S3.p4.5.m5.1.1.4.cmml" xref="S3.p4.5.m5.1.1.4"><csymbol cd="ambiguous" id="S3.p4.5.m5.1.1.4.1.cmml" xref="S3.p4.5.m5.1.1.4">superscript</csymbol><ci id="S3.p4.5.m5.1.1.4.2.cmml" xref="S3.p4.5.m5.1.1.4.2">𝑐</ci><cn id="S3.p4.5.m5.1.1.4.3.cmml" type="integer" xref="S3.p4.5.m5.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.5.m5.1c">2dc^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.5.m5.1d">2 italic_d italic_c start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOP와, 이러한 어텐션 스코어에 값을 곱하기 위한 추가적인 <math alttext="2dc^{2}" class="ltx_Math" display="inline" id="S3.p4.6.m6.1"><semantics id="S3.p4.6.m6.1a"><mrow id="S3.p4.6.m6.1.1" xref="S3.p4.6.m6.1.1.cmml"><mn id="S3.p4.6.m6.1.1.2" xref="S3.p4.6.m6.1.1.2.cmml">2</mn><mo id="S3.p4.6.m6.1.1.1" lspace="0px" rspace="0px" xref="S3.p4.6.m6.1.1.1.cmml"></mo><mi id="S3.p4.6.m6.1.1.3" xref="S3.p4.6.m6.1.1.3.cmml">d</mi><mo id="S3.p4.6.m6.1.1.1a" lspace="0px" rspace="0px" xref="S3.p4.6.m6.1.1.1.cmml"></mo><msup id="S3.p4.6.m6.1.1.4" xref="S3.p4.6.m6.1.1.4.cmml"><mi id="S3.p4.6.m6.1.1.4.2" xref="S3.p4.6.m6.1.1.4.2.cmml">c</mi><mn id="S3.p4.6.m6.1.1.4.3" xref="S3.p4.6.m6.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.6.m6.1b"><apply id="S3.p4.6.m6.1.1.cmml" xref="S3.p4.6.m6.1.1"><times id="S3.p4.6.m6.1.1.1.cmml" xref="S3.p4.6.m6.1.1.1"></times><cn id="S3.p4.6.m6.1.1.2.cmml" type="integer" xref="S3.p4.6.m6.1.1.2">2</cn><ci id="S3.p4.6.m6.1.1.3.cmml" xref="S3.p4.6.m6.1.1.3">𝑑</ci><apply id="S3.p4.6.m6.1.1.4.cmml" xref="S3.p4.6.m6.1.1.4"><csymbol cd="ambiguous" id="S3.p4.6.m6.1.1.4.1.cmml" xref="S3.p4.6.m6.1.1.4">superscript</csymbol><ci id="S3.p4.6.m6.1.1.4.2.cmml" xref="S3.p4.6.m6.1.1.4.2">𝑐</ci><cn id="S3.p4.6.m6.1.1.4.3.cmml" type="integer" xref="S3.p4.6.m6.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.6.m6.1c">2dc^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.6.m6.1d">2 italic_d italic_c start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOP가 필요하다. 전체적으로, 연산 요구량은 <math alttext="4dc^{2}" class="ltx_Math" display="inline" id="S3.p4.7.m7.1"><semantics id="S3.p4.7.m7.1a"><mrow id="S3.p4.7.m7.1.1" xref="S3.p4.7.m7.1.1.cmml"><mn id="S3.p4.7.m7.1.1.2" xref="S3.p4.7.m7.1.1.2.cmml">4</mn><mo id="S3.p4.7.m7.1.1.1" lspace="0px" rspace="0px" xref="S3.p4.7.m7.1.1.1.cmml"></mo><mi id="S3.p4.7.m7.1.1.3" xref="S3.p4.7.m7.1.1.3.cmml">d</mi><mo id="S3.p4.7.m7.1.1.1a" lspace="0px" rspace="0px" xref="S3.p4.7.m7.1.1.1.cmml"></mo><msup id="S3.p4.7.m7.1.1.4" xref="S3.p4.7.m7.1.1.4.cmml"><mi id="S3.p4.7.m7.1.1.4.2" xref="S3.p4.7.m7.1.1.4.2.cmml">c</mi><mn id="S3.p4.7.m7.1.1.4.3" xref="S3.p4.7.m7.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.7.m7.1b"><apply id="S3.p4.7.m7.1.1.cmml" xref="S3.p4.7.m7.1.1"><times id="S3.p4.7.m7.1.1.1.cmml" xref="S3.p4.7.m7.1.1.1"></times><cn id="S3.p4.7.m7.1.1.2.cmml" type="integer" xref="S3.p4.7.m7.1.1.2">4</cn><ci id="S3.p4.7.m7.1.1.3.cmml" xref="S3.p4.7.m7.1.1.3">𝑑</ci><apply id="S3.p4.7.m7.1.1.4.cmml" xref="S3.p4.7.m7.1.1.4"><csymbol cd="ambiguous" id="S3.p4.7.m7.1.1.4.1.cmml" xref="S3.p4.7.m7.1.1.4">superscript</csymbol><ci id="S3.p4.7.m7.1.1.4.2.cmml" xref="S3.p4.7.m7.1.1.4.2">𝑐</ci><cn id="S3.p4.7.m7.1.1.4.3.cmml" type="integer" xref="S3.p4.7.m7.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.7.m7.1c">4dc^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p4.7.m7.1d">4 italic_d italic_c start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> FLOPs이다. 쿼리, 키 및 값의 투영과 블록 방향 피드포워드 연산은 호스트 간의 통신 비용 없이 계산 복잡도만 추가하기 때문에 제외한다. 이러한 단순화는 더 엄격한 조건으로 이어지며 접근법의 유효성을 손상시키지 않는다. 통신 전면에서 키 및 값 블록 모두 총 <math alttext="2cd" class="ltx_Math" display="inline" id="S3.p4.8.m8.1"><semantics id="S3.p4.8.m8.1a"><mrow id="S3.p4.8.m8.1.1" xref="S3.p4.8.m8.1.1.cmml"><mn id="S3.p4.8.m8.1.1.2" xref="S3.p4.8.m8.1.1.2.cmml">2</mn><mo id="S3.p4.8.m8.1.1.1" lspace="0px" rspace="0px" xref="S3.p4.8.m8.1.1.1.cmml"></mo><mi id="S3.p4.8.m8.1.1.3" xref="S3.p4.8.m8.1.1.3.cmml">c</mi><mo id="S3.p4.8.m8.1.1.1a" lspace="0px" rspace="0px" xref="S3.p4.8.m8.1.1.1.cmml"></mo><mi id="S3.p4.8.m8.1.1.4" xref="S3.p4.8.m8.1.1.4.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.8.m8.1b"><apply id="S3.p4.8.m8.1.1.cmml" xref="S3.p4.8.m8.1.1"><times id="S3.p4.8.m8.1.1.1.cmml" xref="S3.p4.8.m8.1.1.1"></times><cn id="S3.p4.8.m8.1.1.2.cmml" type="integer" xref="S3.p4.8.m8.1.1.2">2</cn><ci id="S3.p4.8.m8.1.1.3.cmml" xref="S3.p4.8.m8.1.1.3">𝑐</ci><ci id="S3.p4.8.m8.1.1.4.cmml" xref="S3.p4.8.m8.1.1.4">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.8.m8.1c">2cd</annotation><annotation encoding="application/x-llamapun" id="S3.p4.8.m8.1d">2 italic_c italic_d</annotation></semantics></math> 바이트를 필요로 한다. 따라서, 결합된 통신 요구는 <math alttext="4cd" class="ltx_Math" display="inline" id="S3.p4.9.m9.1"><semantics id="S3.p4.9.m9.1a"><mrow id="S3.p4.9.m9.1.1" xref="S3.p4.9.m9.1.1.cmml"><mn id="S3.p4.9.m9.1.1.2" xref="S3.p4.9.m9.1.1.2.cmml">4</mn><mo id="S3.p4.9.m9.1.1.1" lspace="0px" rspace="0px" xref="S3.p4.9.m9.1.1.1.cmml"></mo><mi id="S3.p4.9.m9.1.1.3" xref="S3.p4.9.m9.1.1.3.cmml">c</mi><mo id="S3.p4.9.m9.1.1.1a" lspace="0px" rspace="0px" xref="S3.p4.9.m9.1.1.1.cmml"></mo><mi id="S3.p4.9.m9.1.1.4" xref="S3.p4.9.m9.1.1.4.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.9.m9.1b"><apply id="S3.p4.9.m9.1.1.cmml" xref="S3.p4.9.m9.1.1"><times id="S3.p4.9.m9.1.1.1.cmml" xref="S3.p4.9.m9.1.1.1"></times><cn id="S3.p4.9.m9.1.1.2.cmml" type="integer" xref="S3.p4.9.m9.1.1.2">4</cn><ci id="S3.p4.9.m9.1.1.3.cmml" xref="S3.p4.9.m9.1.1.3">𝑐</ci><ci id="S3.p4.9.m9.1.1.4.cmml" xref="S3.p4.9.m9.1.1.4">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.9.m9.1c">4cd</annotation><annotation encoding="application/x-llamapun" id="S3.p4.9.m9.1d">4 italic_c italic_d</annotation></semantics></math> 바이트이다. 통신과 계산 사이의 중첩을 달성하기 위해, 다음 조건이 유지되어야 한다 : <math alttext="4dc^{2}/F\geq 4cd/B" class="ltx_Math" display="inline" id="S3.p4.10.m10.1"><semantics id="S3.p4.10.m10.1a"><mrow id="S3.p4.10.m10.1.1" xref="S3.p4.10.m10.1.1.cmml"><mrow id="S3.p4.10.m10.1.1.2" xref="S3.p4.10.m10.1.1.2.cmml"><mrow id="S3.p4.10.m10.1.1.2.2" xref="S3.p4.10.m10.1.1.2.2.cmml"><mn id="S3.p4.10.m10.1.1.2.2.2" xref="S3.p4.10.m10.1.1.2.2.2.cmml">4</mn><mo id="S3.p4.10.m10.1.1.2.2.1" lspace="0px" rspace="0px" xref="S3.p4.10.m10.1.1.2.2.1.cmml"></mo><mi id="S3.p4.10.m10.1.1.2.2.3" xref="S3.p4.10.m10.1.1.2.2.3.cmml">d</mi><mo id="S3.p4.10.m10.1.1.2.2.1a" lspace="0px" rspace="0px" xref="S3.p4.10.m10.1.1.2.2.1.cmml"></mo><msup id="S3.p4.10.m10.1.1.2.2.4" xref="S3.p4.10.m10.1.1.2.2.4.cmml"><mi id="S3.p4.10.m10.1.1.2.2.4.2" xref="S3.p4.10.m10.1.1.2.2.4.2.cmml">c</mi><mn id="S3.p4.10.m10.1.1.2.2.4.3" xref="S3.p4.10.m10.1.1.2.2.4.3.cmml">2</mn></msup></mrow><mo id="S3.p4.10.m10.1.1.2.1" xref="S3.p4.10.m10.1.1.2.1.cmml">/</mo><mi id="S3.p4.10.m10.1.1.2.3" xref="S3.p4.10.m10.1.1.2.3.cmml">F</mi></mrow><mo id="S3.p4.10.m10.1.1.1" xref="S3.p4.10.m10.1.1.1.cmml">≥</mo><mrow id="S3.p4.10.m10.1.1.3" xref="S3.p4.10.m10.1.1.3.cmml"><mrow id="S3.p4.10.m10.1.1.3.2" xref="S3.p4.10.m10.1.1.3.2.cmml"><mn id="S3.p4.10.m10.1.1.3.2.2" xref="S3.p4.10.m10.1.1.3.2.2.cmml">4</mn><mo id="S3.p4.10.m10.1.1.3.2.1" lspace="0px" rspace="0px" xref="S3.p4.10.m10.1.1.3.2.1.cmml"></mo><mi id="S3.p4.10.m10.1.1.3.2.3" xref="S3.p4.10.m10.1.1.3.2.3.cmml">c</mi><mo id="S3.p4.10.m10.1.1.3.2.1a" lspace="0px" rspace="0px" xref="S3.p4.10.m10.1.1.3.2.1.cmml"></mo><mi id="S3.p4.10.m10.1.1.3.2.4" xref="S3.p4.10.m10.1.1.3.2.4.cmml">d</mi></mrow><mo id="S3.p4.10.m10.1.1.3.1" xref="S3.p4.10.m10.1.1.3.1.cmml">/</mo><mi id="S3.p4.10.m10.1.1.3.3" xref="S3.p4.10.m10.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.10.m10.1b"><apply id="S3.p4.10.m10.1.1.cmml" xref="S3.p4.10.m10.1.1"><geq id="S3.p4.10.m10.1.1.1.cmml" xref="S3.p4.10.m10.1.1.1"></geq><apply id="S3.p4.10.m10.1.1.2.cmml" xref="S3.p4.10.m10.1.1.2"><divide id="S3.p4.10.m10.1.1.2.1.cmml" xref="S3.p4.10.m10.1.1.2.1"></divide><apply id="S3.p4.10.m10.1.1.2.2.cmml" xref="S3.p4.10.m10.1.1.2.2"><times id="S3.p4.10.m10.1.1.2.2.1.cmml" xref="S3.p4.10.m10.1.1.2.2.1"></times><cn id="S3.p4.10.m10.1.1.2.2.2.cmml" type="integer" xref="S3.p4.10.m10.1.1.2.2.2">4</cn><ci id="S3.p4.10.m10.1.1.2.2.3.cmml" xref="S3.p4.10.m10.1.1.2.2.3">𝑑</ci><apply id="S3.p4.10.m10.1.1.2.2.4.cmml" xref="S3.p4.10.m10.1.1.2.2.4"><csymbol cd="ambiguous" id="S3.p4.10.m10.1.1.2.2.4.1.cmml" xref="S3.p4.10.m10.1.1.2.2.4">superscript</csymbol><ci id="S3.p4.10.m10.1.1.2.2.4.2.cmml" xref="S3.p4.10.m10.1.1.2.2.4.2">𝑐</ci><cn id="S3.p4.10.m10.1.1.2.2.4.3.cmml" type="integer" xref="S3.p4.10.m10.1.1.2.2.4.3">2</cn></apply></apply><ci id="S3.p4.10.m10.1.1.2.3.cmml" xref="S3.p4.10.m10.1.1.2.3">𝐹</ci></apply><apply id="S3.p4.10.m10.1.1.3.cmml" xref="S3.p4.10.m10.1.1.3"><divide id="S3.p4.10.m10.1.1.3.1.cmml" xref="S3.p4.10.m10.1.1.3.1"></divide><apply id="S3.p4.10.m10.1.1.3.2.cmml" xref="S3.p4.10.m10.1.1.3.2"><times id="S3.p4.10.m10.1.1.3.2.1.cmml" xref="S3.p4.10.m10.1.1.3.2.1"></times><cn id="S3.p4.10.m10.1.1.3.2.2.cmml" type="integer" xref="S3.p4.10.m10.1.1.3.2.2">4</cn><ci id="S3.p4.10.m10.1.1.3.2.3.cmml" xref="S3.p4.10.m10.1.1.3.2.3">𝑐</ci><ci id="S3.p4.10.m10.1.1.3.2.4.cmml" xref="S3.p4.10.m10.1.1.3.2.4">𝑑</ci></apply><ci id="S3.p4.10.m10.1.1.3.3.cmml" xref="S3.p4.10.m10.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.10.m10.1c">4dc^{2}/F\geq 4cd/B</annotation><annotation encoding="application/x-llamapun" id="S3.p4.10.m10.1d">4 italic_d italic_c start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_F ≥ 4 italic_c italic_d / italic_B</annotation></semantics></math>. 이는 <math alttext="c" class="ltx_Math" display="inline" id="S3.p4.11.m11.1"><semantics id="S3.p4.11.m11.1a"><mi id="S3.p4.11.m11.1.1" xref="S3.p4.11.m11.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.p4.11.m11.1b"><ci id="S3.p4.11.m11.1.1.cmml" xref="S3.p4.11.m11.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.11.m11.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.p4.11.m11.1d">italic_c</annotation></semantics></math>로 표시된 블록 크기가 <math alttext="F/B" class="ltx_Math" display="inline" id="S3.p4.12.m12.1"><semantics id="S3.p4.12.m12.1a"><mrow id="S3.p4.12.m12.1.1" xref="S3.p4.12.m12.1.1.cmml"><mi id="S3.p4.12.m12.1.1.2" xref="S3.p4.12.m12.1.1.2.cmml">F</mi><mo id="S3.p4.12.m12.1.1.1" xref="S3.p4.12.m12.1.1.1.cmml">/</mo><mi id="S3.p4.12.m12.1.1.3" xref="S3.p4.12.m12.1.1.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.12.m12.1b"><apply id="S3.p4.12.m12.1.1.cmml" xref="S3.p4.12.m12.1.1"><divide id="S3.p4.12.m12.1.1.1.cmml" xref="S3.p4.12.m12.1.1.1"></divide><ci id="S3.p4.12.m12.1.1.2.cmml" xref="S3.p4.12.m12.1.1.2">𝐹</ci><ci id="S3.p4.12.m12.1.1.3.cmml" xref="S3.p4.12.m12.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.12.m12.1c">F/B</annotation><annotation encoding="application/x-llamapun" id="S3.p4.12.m12.1d">italic_F / italic_B</annotation></semantics></math>보다 크거나 같아야 함을 의미한다. 효과적으로, 이것은 블록 크기가 대역폭에 대한 FLOP들의 비율보다 클 필요가 있다는 것을 의미한다.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p" id="S3.p5.5"><span class="ltx_text ltx_font_bold" id="S3.p5.5.1">Memory Requirement. </span> 호스트는 현재 쿼리 블록을 저장하기 위한 하나의 블록 크기, 현재 키 및 값 블록에 대한 두 개의 블록 크기, 키 및 값 블록을 수신하기 위한 두 개의 블록 크기를 포함하여 여러 개의 블록을 저장해야 합니다. 또한, 블록 단위의 어텐션 및 피드포워드의 출력을 저장하는 것은 출력이 쿼리 블록의 형태를 유지하기 때문에 하나의 블록 크기를 필요로 한다. 따라서, 총 6개의 블록이 필요하며, 이는 <math alttext="6bch" class="ltx_Math" display="inline" id="S3.p5.1.m1.1"><semantics id="S3.p5.1.m1.1a"><mrow id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mn id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">6</mn><mo id="S3.p5.1.m1.1.1.1" lspace="0px" rspace="0px" xref="S3.p5.1.m1.1.1.1.cmml"></mo><mi id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">b</mi><mo id="S3.p5.1.m1.1.1.1a" lspace="0px" rspace="0px" xref="S3.p5.1.m1.1.1.1.cmml"></mo><mi id="S3.p5.1.m1.1.1.4" xref="S3.p5.1.m1.1.1.4.cmml">c</mi><mo id="S3.p5.1.m1.1.1.1b" lspace="0px" rspace="0px" xref="S3.p5.1.m1.1.1.1.cmml"></mo><mi id="S3.p5.1.m1.1.1.5" xref="S3.p5.1.m1.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><times id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1"></times><cn id="S3.p5.1.m1.1.1.2.cmml" type="integer" xref="S3.p5.1.m1.1.1.2">6</cn><ci id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">𝑏</ci><ci id="S3.p5.1.m1.1.1.4.cmml" xref="S3.p5.1.m1.1.1.4">𝑐</ci><ci id="S3.p5.1.m1.1.1.5.cmml" xref="S3.p5.1.m1.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">6bch</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.1d">6 italic_b italic_c italic_h</annotation></semantics></math>바이트의 메모리로 번역된다. 블록별 피드포워드 네트워크가 <math alttext="2bch" class="ltx_Math" display="inline" id="S3.p5.2.m2.1"><semantics id="S3.p5.2.m2.1a"><mrow id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml"><mn id="S3.p5.2.m2.1.1.2" xref="S3.p5.2.m2.1.1.2.cmml">2</mn><mo id="S3.p5.2.m2.1.1.1" lspace="0px" rspace="0px" xref="S3.p5.2.m2.1.1.1.cmml"></mo><mi id="S3.p5.2.m2.1.1.3" xref="S3.p5.2.m2.1.1.3.cmml">b</mi><mo id="S3.p5.2.m2.1.1.1a" lspace="0px" rspace="0px" xref="S3.p5.2.m2.1.1.1.cmml"></mo><mi id="S3.p5.2.m2.1.1.4" xref="S3.p5.2.m2.1.1.4.cmml">c</mi><mo id="S3.p5.2.m2.1.1.1b" lspace="0px" rspace="0px" xref="S3.p5.2.m2.1.1.1.cmml"></mo><mi id="S3.p5.2.m2.1.1.5" xref="S3.p5.2.m2.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><apply id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1"><times id="S3.p5.2.m2.1.1.1.cmml" xref="S3.p5.2.m2.1.1.1"></times><cn id="S3.p5.2.m2.1.1.2.cmml" type="integer" xref="S3.p5.2.m2.1.1.2">2</cn><ci id="S3.p5.2.m2.1.1.3.cmml" xref="S3.p5.2.m2.1.1.3">𝑏</ci><ci id="S3.p5.2.m2.1.1.4.cmml" xref="S3.p5.2.m2.1.1.4">𝑐</ci><ci id="S3.p5.2.m2.1.1.5.cmml" xref="S3.p5.2.m2.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">2bch</annotation><annotation encoding="application/x-llamapun" id="S3.p5.2.m2.1d">2 italic_b italic_c italic_h</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>의 최대 활성화 크기를 갖는다는 점에 주목할 필요가 있다. 결과적으로, 총 최대 활성화 크기는 <math alttext="6bch" class="ltx_Math" display="inline" id="S3.p5.3.m3.1"><semantics id="S3.p5.3.m3.1a"><mrow id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml"><mn id="S3.p5.3.m3.1.1.2" xref="S3.p5.3.m3.1.1.2.cmml">6</mn><mo id="S3.p5.3.m3.1.1.1" lspace="0px" rspace="0px" xref="S3.p5.3.m3.1.1.1.cmml"></mo><mi id="S3.p5.3.m3.1.1.3" xref="S3.p5.3.m3.1.1.3.cmml">b</mi><mo id="S3.p5.3.m3.1.1.1a" lspace="0px" rspace="0px" xref="S3.p5.3.m3.1.1.1.cmml"></mo><mi id="S3.p5.3.m3.1.1.4" xref="S3.p5.3.m3.1.1.4.cmml">c</mi><mo id="S3.p5.3.m3.1.1.1b" lspace="0px" rspace="0px" xref="S3.p5.3.m3.1.1.1.cmml"></mo><mi id="S3.p5.3.m3.1.1.5" xref="S3.p5.3.m3.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><apply id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1"><times id="S3.p5.3.m3.1.1.1.cmml" xref="S3.p5.3.m3.1.1.1"></times><cn id="S3.p5.3.m3.1.1.2.cmml" type="integer" xref="S3.p5.3.m3.1.1.2">6</cn><ci id="S3.p5.3.m3.1.1.3.cmml" xref="S3.p5.3.m3.1.1.3">𝑏</ci><ci id="S3.p5.3.m3.1.1.4.cmml" xref="S3.p5.3.m3.1.1.4">𝑐</ci><ci id="S3.p5.3.m3.1.1.5.cmml" xref="S3.p5.3.m3.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">6bch</annotation><annotation encoding="application/x-llamapun" id="S3.p5.3.m3.1d">6 italic_b italic_c italic_h</annotation></semantics></math> 바이트로 유지된다. 표 <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3 Ring Attention ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">1</span></a>는 우리의 방법과 다른 접근법 사이의 메모리 비용에 대한 상세한 비교를 제공한다. 특히, 제안된 방법은 블록 크기 <math alttext="c" class="ltx_Math" display="inline" id="S3.p5.4.m4.1"><semantics id="S3.p5.4.m4.1a"><mi id="S3.p5.4.m4.1.1" xref="S3.p5.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.p5.4.m4.1b"><ci id="S3.p5.4.m4.1.1.cmml" xref="S3.p5.4.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.4.m4.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.p5.4.m4.1d">italic_c</annotation></semantics></math>에 대한 선형 메모리 스케일링의 장점을 나타내며, 입력 시퀀스 길이 <math alttext="s" class="ltx_Math" display="inline" id="S3.p5.5.m5.1"><semantics id="S3.p5.5.m5.1a"><mi id="S3.p5.5.m5.1.1" xref="S3.p5.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.p5.5.m5.1b"><ci id="S3.p5.5.m5.1.1.cmml" xref="S3.p5.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.5.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.p5.5.m5.1d">italic_s</annotation></semantics></math>와 무관하다.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p class="ltx_p" id="S3.p6.1">분석 결과, 모델은 최소 블록 크기의 6배인 <math alttext="s=6c" class="ltx_Math" display="inline" id="S3.p6.1.m1.1"><semantics id="S3.p6.1.m1.1a"><mrow id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml"><mi id="S3.p6.1.m1.1.1.2" xref="S3.p6.1.m1.1.1.2.cmml">s</mi><mo id="S3.p6.1.m1.1.1.1" xref="S3.p6.1.m1.1.1.1.cmml">=</mo><mrow id="S3.p6.1.m1.1.1.3" xref="S3.p6.1.m1.1.1.3.cmml"><mn id="S3.p6.1.m1.1.1.3.2" xref="S3.p6.1.m1.1.1.3.2.cmml">6</mn><mo id="S3.p6.1.m1.1.1.3.1" lspace="0px" rspace="0px" xref="S3.p6.1.m1.1.1.3.1.cmml"></mo><mi id="S3.p6.1.m1.1.1.3.3" xref="S3.p6.1.m1.1.1.3.3.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><apply id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1"><eq id="S3.p6.1.m1.1.1.1.cmml" xref="S3.p6.1.m1.1.1.1"></eq><ci id="S3.p6.1.m1.1.1.2.cmml" xref="S3.p6.1.m1.1.1.2">𝑠</ci><apply id="S3.p6.1.m1.1.1.3.cmml" xref="S3.p6.1.m1.1.1.3"><times id="S3.p6.1.m1.1.1.3.1.cmml" xref="S3.p6.1.m1.1.1.3.1"></times><cn id="S3.p6.1.m1.1.1.3.2.cmml" type="integer" xref="S3.p6.1.m1.1.1.3.2">6</cn><ci id="S3.p6.1.m1.1.1.3.3.cmml" xref="S3.p6.1.m1.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">s=6c</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.1d">italic_s = 6 italic_c</annotation></semantics></math>의 시퀀스 길이를 가져야 한다. 인기 컴퓨팅 서버에 대한 요구 사항은 표<a class="ltx_ref" href="#S3.T2" title="Table 2 ‣ 3 Ring Attention ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">2</span></a>에 나와 있다. 각 호스트에 필요한 최소 시퀀스 길이(최우측 열)는 6K에서 10K 사이에서 변하며, 각 호스트에 필요한 최소 블록 크기(최우측 열)는 높은 대역폭 상호연결을 갖는 TPU 및 GPU의 경우 약 1K이다. 낮은 대역폭을 제공하는 InfiniBand를 통해 연결된 GPU의 경우 요구 사항이 더 엄격합니다. 이 경우 향후 작업을 위해 남겨두는 GPU 전체 토폴로지를 활용하기 위해 링 어텐션을 확장하는 것을 고려할 수 있다. 이러한 요구 사항은 병렬성과 메모리 효율적인 블록 주의와 피드포워드 <cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a class="ltx_ref" href="#bib.bib31" title="">2021</a>; Dao et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>; Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>를 충족하기 쉬우며, 실험 섹션 <a class="ltx_ref" href="#S5" title="5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5</span></a>에서 보여 줄 것이다.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 2:</span>각 디바이스에서 필요한 최소 시퀀스 길이. Interconnect Bandwidth는 호스트 간의 단방향 대역폭으로 <span class="ltx_text ltx_font_italic" id="S3.T2.9.1">i</span>입니다. <span class="ltx_text ltx_font_italic" id="S3.T2.10.2">e</span>., NVLink/InfiniBand bandwidth between GPUs and ICI bandwidth between TPUs.</figcaption>
The minimal block size required <math id="S3.T2.3.m1.1" class="ltx_Math" alttext="c=\text{FLOPS}/\text{Bandwidth}" display="inline"><semantics id="S3.T2.3.m1.1b"><mrow id="S3.T2.3.m1.1.1" xref="S3.T2.3.m1.1.1.cmml"><mi id="S3.T2.3.m1.1.1.2" xref="S3.T2.3.m1.1.1.2.cmml">c</mi><mo id="S3.T2.3.m1.1.1.1" xref="S3.T2.3.m1.1.1.1.cmml">=</mo><mrow id="S3.T2.3.m1.1.1.3" xref="S3.T2.3.m1.1.1.3.cmml"><mtext id="S3.T2.3.m1.1.1.3.2" xref="S3.T2.3.m1.1.1.3.2a.cmml">FLOPS</mtext><mo id="S3.T2.3.m1.1.1.3.1" xref="S3.T2.3.m1.1.1.3.1.cmml">/</mo><mtext id="S3.T2.3.m1.1.1.3.3" xref="S3.T2.3.m1.1.1.3.3a.cmml">Bandwidth</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.3.m1.1c"><apply id="S3.T2.3.m1.1.1.cmml" xref="S3.T2.3.m1.1.1"><eq id="S3.T2.3.m1.1.1.1.cmml" xref="S3.T2.3.m1.1.1.1"></eq><ci id="S3.T2.3.m1.1.1.2.cmml" xref="S3.T2.3.m1.1.1.2">𝑐</ci><apply id="S3.T2.3.m1.1.1.3.cmml" xref="S3.T2.3.m1.1.1.3"><divide id="S3.T2.3.m1.1.1.3.1.cmml" xref="S3.T2.3.m1.1.1.3.1"></divide><ci id="S3.T2.3.m1.1.1.3.2a.cmml" xref="S3.T2.3.m1.1.1.3.2"><mtext id="S3.T2.3.m1.1.1.3.2.cmml" xref="S3.T2.3.m1.1.1.3.2">FLOPS</mtext></ci><ci id="S3.T2.3.m1.1.1.3.3a.cmml" xref="S3.T2.3.m1.1.1.3.3"><mtext id="S3.T2.3.m1.1.1.3.3.cmml" xref="S3.T2.3.m1.1.1.3.3">Bandwidth</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.m1.1d">c=\text{FLOPS}/\text{Bandwidth}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.m1.1e">italic_c = FLOPS / Bandwidth</annotation></semantics></math>, and minimal sequence length <math id="S3.T2.4.m2.1" class="ltx_Math" alttext="s=6c" display="inline"><semantics id="S3.T2.4.m2.1b"><mrow id="S3.T2.4.m2.1.1" xref="S3.T2.4.m2.1.1.cmml"><mi id="S3.T2.4.m2.1.1.2" xref="S3.T2.4.m2.1.1.2.cmml">s</mi><mo id="S3.T2.4.m2.1.1.1" xref="S3.T2.4.m2.1.1.1.cmml">=</mo><mrow id="S3.T2.4.m2.1.1.3" xref="S3.T2.4.m2.1.1.3.cmml"><mn id="S3.T2.4.m2.1.1.3.2" xref="S3.T2.4.m2.1.1.3.2.cmml">6</mn><mo id="S3.T2.4.m2.1.1.3.1" xref="S3.T2.4.m2.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi id="S3.T2.4.m2.1.1.3.3" xref="S3.T2.4.m2.1.1.3.3.cmml">c</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.4.m2.1c"><apply id="S3.T2.4.m2.1.1.cmml" xref="S3.T2.4.m2.1.1"><eq id="S3.T2.4.m2.1.1.1.cmml" xref="S3.T2.4.m2.1.1.1"></eq><ci id="S3.T2.4.m2.1.1.2.cmml" xref="S3.T2.4.m2.1.1.2">𝑠</ci><apply id="S3.T2.4.m2.1.1.3.cmml" xref="S3.T2.4.m2.1.1.3"><times id="S3.T2.4.m2.1.1.3.1.cmml" xref="S3.T2.4.m2.1.1.3.1"></times><cn type="integer" id="S3.T2.4.m2.1.1.3.2.cmml" xref="S3.T2.4.m2.1.1.3.2">6</cn><ci id="S3.T2.4.m2.1.1.3.3.cmml" xref="S3.T2.4.m2.1.1.3.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.m2.1d">s=6c</annotation><annotation encoding="application/x-llamapun" id="S3.T2.4.m2.1e">italic_s = 6 italic_c</annotation></semantics></math>.
</figcaption>
<div id="S3.T2.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:309.4pt;height:149.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.2pt,0.1pt) scale(0.999,0.999) ;">
<table id="S3.T2.6.2" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S3.T2.6.2.3" class="ltx_tr">
<td id="S3.T2.6.2.3.1" class="ltx_td ltx_align_left ltx_border_tt">Spec Per Host</td>
<td id="S3.T2.6.2.3.2" class="ltx_td ltx_align_right ltx_border_tt">FLOPS</td>
<td id="S3.T2.6.2.3.3" class="ltx_td ltx_align_right ltx_border_tt">HBM</td>
<td id="S3.T2.6.2.3.4" class="ltx_td ltx_align_right ltx_border_tt">
<span id="S3.T2.6.2.3.4.1" class="ltx_text"></span> <span id="S3.T2.6.2.3.4.2" class="ltx_text">
<span id="S3.T2.6.2.3.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.6.2.3.4.2.1.1" class="ltx_tr">
<span id="S3.T2.6.2.3.4.2.1.1.1" class="ltx_td ltx_align_center">Interconnect</span></span>
<span id="S3.T2.6.2.3.4.2.1.2" class="ltx_tr">
<span id="S3.T2.6.2.3.4.2.1.2.1" class="ltx_td ltx_align_center">Bandwidth</span></span>
</span></span> <span id="S3.T2.6.2.3.4.3" class="ltx_text"></span>
</td>
<td id="S3.T2.6.2.3.5" class="ltx_td ltx_align_right ltx_border_tt">
<span id="S3.T2.6.2.3.5.1" class="ltx_text"></span> <span id="S3.T2.6.2.3.5.2" class="ltx_text">
<span id="S3.T2.6.2.3.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.6.2.3.5.2.1.1" class="ltx_tr">
<span id="S3.T2.6.2.3.5.2.1.1.1" class="ltx_td ltx_align_center">Minimal</span></span>
<span id="S3.T2.6.2.3.5.2.1.2" class="ltx_tr">
<span id="S3.T2.6.2.3.5.2.1.2.1" class="ltx_td ltx_align_center">Blocksize</span></span>
</span></span> <span id="S3.T2.6.2.3.5.3" class="ltx_text"></span>
</td>
<td id="S3.T2.6.2.3.6" class="ltx_td ltx_align_right ltx_border_tt">
<span id="S3.T2.6.2.3.6.1" class="ltx_text"></span> <span id="S3.T2.6.2.3.6.2" class="ltx_text">
<span id="S3.T2.6.2.3.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.6.2.3.6.2.1.1" class="ltx_tr">
<span id="S3.T2.6.2.3.6.2.1.1.1" class="ltx_td ltx_align_center">Minimal</span></span>
<span id="S3.T2.6.2.3.6.2.1.2" class="ltx_tr">
<span id="S3.T2.6.2.3.6.2.1.2.1" class="ltx_td ltx_align_center">Sequence Len</span></span>
</span></span> <span id="S3.T2.6.2.3.6.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S3.T2.6.2.2" class="ltx_tr">
<td id="S3.T2.6.2.2.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.6.2.2.4" class="ltx_td ltx_align_right ltx_border_t">(TF)</td>
<td id="S3.T2.6.2.2.5" class="ltx_td ltx_align_right ltx_border_t">(GB)</td>
<td id="S3.T2.6.2.2.6" class="ltx_td ltx_align_right ltx_border_t">(GB/s)</td>
<td id="S3.T2.5.1.1.1" class="ltx_td ltx_align_right ltx_border_t">(<math id="S3.T2.5.1.1.1.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S3.T2.5.1.1.1.m1.1a"><mrow id="S3.T2.5.1.1.1.m1.1.1" xref="S3.T2.5.1.1.1.m1.1.1.cmml"><mi id="S3.T2.5.1.1.1.m1.1.1.2" xref="S3.T2.5.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.T2.5.1.1.1.m1.1.1.1" xref="S3.T2.5.1.1.1.m1.1.1.1.cmml">×</mo><mrow id="S3.T2.5.1.1.1.m1.1.1.3" xref="S3.T2.5.1.1.1.m1.1.1.3.cmml"><mn id="S3.T2.5.1.1.1.m1.1.1.3.2" xref="S3.T2.5.1.1.1.m1.1.1.3.2.cmml">1</mn><mo id="S3.T2.5.1.1.1.m1.1.1.3.1" xref="S3.T2.5.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S3.T2.5.1.1.1.m1.1.1.3.3" xref="S3.T2.5.1.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.T2.5.1.1.1.m1.1.1.3.1a" xref="S3.T2.5.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn id="S3.T2.5.1.1.1.m1.1.1.3.4" xref="S3.T2.5.1.1.1.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.5.1.1.1.m1.1b"><apply id="S3.T2.5.1.1.1.m1.1.1.cmml" xref="S3.T2.5.1.1.1.m1.1.1"><times id="S3.T2.5.1.1.1.m1.1.1.1.cmml" xref="S3.T2.5.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S3.T2.5.1.1.1.m1.1.1.2.cmml" xref="S3.T2.5.1.1.1.m1.1.1.2">absent</csymbol><apply id="S3.T2.5.1.1.1.m1.1.1.3.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3"><times id="S3.T2.5.1.1.1.m1.1.1.3.1.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3.1"></times><cn type="integer" id="S3.T2.5.1.1.1.m1.1.1.3.2.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3.2">1</cn><ci id="S3.T2.5.1.1.1.m1.1.1.3.3.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3.3">e</ci><cn type="integer" id="S3.T2.5.1.1.1.m1.1.1.3.4.cmml" xref="S3.T2.5.1.1.1.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.1.1.1.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.1.1.1.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</td>
<td id="S3.T2.6.2.2.2" class="ltx_td ltx_align_right ltx_border_t">(<math id="S3.T2.6.2.2.2.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S3.T2.6.2.2.2.m1.1a"><mrow id="S3.T2.6.2.2.2.m1.1.1" xref="S3.T2.6.2.2.2.m1.1.1.cmml"><mi id="S3.T2.6.2.2.2.m1.1.1.2" xref="S3.T2.6.2.2.2.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S3.T2.6.2.2.2.m1.1.1.1" xref="S3.T2.6.2.2.2.m1.1.1.1.cmml">×</mo><mrow id="S3.T2.6.2.2.2.m1.1.1.3" xref="S3.T2.6.2.2.2.m1.1.1.3.cmml"><mn id="S3.T2.6.2.2.2.m1.1.1.3.2" xref="S3.T2.6.2.2.2.m1.1.1.3.2.cmml">1</mn><mo id="S3.T2.6.2.2.2.m1.1.1.3.1" xref="S3.T2.6.2.2.2.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S3.T2.6.2.2.2.m1.1.1.3.3" xref="S3.T2.6.2.2.2.m1.1.1.3.3.cmml">e</mi><mo id="S3.T2.6.2.2.2.m1.1.1.3.1a" xref="S3.T2.6.2.2.2.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn id="S3.T2.6.2.2.2.m1.1.1.3.4" xref="S3.T2.6.2.2.2.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.6.2.2.2.m1.1b"><apply id="S3.T2.6.2.2.2.m1.1.1.cmml" xref="S3.T2.6.2.2.2.m1.1.1"><times id="S3.T2.6.2.2.2.m1.1.1.1.cmml" xref="S3.T2.6.2.2.2.m1.1.1.1"></times><csymbol cd="latexml" id="S3.T2.6.2.2.2.m1.1.1.2.cmml" xref="S3.T2.6.2.2.2.m1.1.1.2">absent</csymbol><apply id="S3.T2.6.2.2.2.m1.1.1.3.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3"><times id="S3.T2.6.2.2.2.m1.1.1.3.1.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3.1"></times><cn type="integer" id="S3.T2.6.2.2.2.m1.1.1.3.2.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3.2">1</cn><ci id="S3.T2.6.2.2.2.m1.1.1.3.3.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3.3">e</ci><cn type="integer" id="S3.T2.6.2.2.2.m1.1.1.3.4.cmml" xref="S3.T2.6.2.2.2.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.2.2.2.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S3.T2.6.2.2.2.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</td>
</tr>
<tr id="S3.T2.6.2.4" class="ltx_tr">
<td id="S3.T2.6.2.4.1" class="ltx_td ltx_align_left ltx_border_t">A100 NVLink</td>
<td id="S3.T2.6.2.4.2" class="ltx_td ltx_align_right ltx_border_t">312</td>
<td id="S3.T2.6.2.4.3" class="ltx_td ltx_align_right ltx_border_t">80</td>
<td id="S3.T2.6.2.4.4" class="ltx_td ltx_align_right ltx_border_t">300</td>
<td id="S3.T2.6.2.4.5" class="ltx_td ltx_align_right ltx_border_t">1.0</td>
<td id="S3.T2.6.2.4.6" class="ltx_td ltx_align_right ltx_border_t">6.2</td>
</tr>
<tr id="S3.T2.6.2.5" class="ltx_tr">
<td id="S3.T2.6.2.5.1" class="ltx_td ltx_align_left">A100 InfiniBand</td>
<td id="S3.T2.6.2.5.2" class="ltx_td ltx_align_right">312</td>
<td id="S3.T2.6.2.5.3" class="ltx_td ltx_align_right">80</td>
<td id="S3.T2.6.2.5.4" class="ltx_td ltx_align_right">12.5</td>
<td id="S3.T2.6.2.5.5" class="ltx_td ltx_align_right">24.5</td>
<td id="S3.T2.6.2.5.6" class="ltx_td ltx_align_right">149.5</td>
</tr>
<tr id="S3.T2.6.2.6" class="ltx_tr">
<td id="S3.T2.6.2.6.1" class="ltx_td ltx_align_left">TPU v3</td>
<td id="S3.T2.6.2.6.2" class="ltx_td ltx_align_right">123</td>
<td id="S3.T2.6.2.6.3" class="ltx_td ltx_align_right">16</td>
<td id="S3.T2.6.2.6.4" class="ltx_td ltx_align_right">112</td>
<td id="S3.T2.6.2.6.5" class="ltx_td ltx_align_right">1.1</td>
<td id="S3.T2.6.2.6.6" class="ltx_td ltx_align_right">6.6</td>
</tr>
<tr id="S3.T2.6.2.7" class="ltx_tr">
<td id="S3.T2.6.2.7.1" class="ltx_td ltx_align_left">TPU v4</td>
<td id="S3.T2.6.2.7.2" class="ltx_td ltx_align_right">275</td>
<td id="S3.T2.6.2.7.3" class="ltx_td ltx_align_right">32</td>
<td id="S3.T2.6.2.7.4" class="ltx_td ltx_align_right">268</td>
<td id="S3.T2.6.2.7.5" class="ltx_td ltx_align_right">1.0</td>
<td id="S3.T2.6.2.7.6" class="ltx_td ltx_align_right">6.2</td>
</tr>
<tr id="S3.T2.6.2.8" class="ltx_tr">
<td id="S3.T2.6.2.8.1" class="ltx_td ltx_align_left ltx_border_bb">TPU v5e</td>
<td id="S3.T2.6.2.8.2" class="ltx_td ltx_align_right ltx_border_bb">196</td>
<td id="S3.T2.6.2.8.3" class="ltx_td ltx_align_right ltx_border_bb">16</td>
<td id="S3.T2.6.2.8.4" class="ltx_td ltx_align_right ltx_border_bb">186</td>
<td id="S3.T2.6.2.8.5" class="ltx_td ltx_align_right ltx_border_bb">1.1</td>
<td id="S3.T2.6.2.8.6" class="ltx_td ltx_align_right ltx_border_bb">6.3</td>
</tr>
</tbody></table>
</span></div>
</figure>
<div id="S3.p7" class="ltx_para">
<p class="ltx_p" id="S3.p7.1"><span class="ltx_text ltx_font_bold" id="S3.p7.1.1">Algorithm and Implementation. </span> Algorithm <a class="ltx_ref" href="#alg1" title="Algorithm 1 ‣ 3 Ring Attention ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">1</span></a>는 알고리즘의 pseudocode를 제공한다. 링 어텐션은 메모리 효율적인 변압기를 위한 기존 코드와 호환됩니다. 링 어텐션은 각 호스트에서 로컬로 사용 가능한 메모리 효율적인 계산을 호출하고 호스트 간의 키 값 블록 통신을 블록 단위로 중첩하면 됩니다. 집단 연산 <span class="ltx_text ltx_font_typewriter" id="S3.p7.1.2">jax.lax.ppermute</span>을 사용하여 가까운 호스트 간에 키 값 블록을 주고 받습니다. Jax 구현은 부록<a class="ltx_ref" href="#A1" title="Appendix A Code ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">A</span></a>에서 제공된다.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span></span> Reducing Transformers Memory Cost with Ring Attention.</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">&nbsp;&nbsp;<span id="alg1.l1.1" class="ltx_text ltx_font_bold">Required:</span> Input sequence <math id="alg1.l1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_x</annotation></semantics></math>. Number of hosts <math id="alg1.l1.m2.1" class="ltx_Math" alttext="N_{h}" display="inline"><semantics id="alg1.l1.m2.1a"><msub id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mi id="alg1.l1.m2.1.1.2" xref="alg1.l1.m2.1.1.2.cmml">N</mi><mi id="alg1.l1.m2.1.1.3" xref="alg1.l1.m2.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><csymbol cd="ambiguous" id="alg1.l1.m2.1.1.1.cmml" xref="alg1.l1.m2.1.1">subscript</csymbol><ci id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1.2">𝑁</ci><ci id="alg1.l1.m2.1.1.3.cmml" xref="alg1.l1.m2.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">N_{h}</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math>.

</div>
<div id="alg1.l2" class="ltx_listingline">&nbsp;&nbsp;Initialize

</div>
<div id="alg1.l3" class="ltx_listingline">&nbsp;&nbsp;Split input sequence into <math id="alg1.l3.m1.1" class="ltx_Math" alttext="N_{h}" display="inline"><semantics id="alg1.l3.m1.1a"><msub id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">N</mi><mi id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝑁</ci><ci id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">N_{h}</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> blocks that each host has one input block.

</div>
<div id="alg1.l4" class="ltx_listingline">&nbsp;&nbsp;Compute query, key, and value for its input block on each host.

</div>
<div id="alg1.l5" class="ltx_listingline">&nbsp;&nbsp;<span id="alg1.l5.1" class="ltx_text ltx_font_bold">for</span>&nbsp;Each transformer layer&nbsp;<span id="alg1.l5.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l6" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l6.1" class="ltx_text ltx_font_bold">for</span>&nbsp;<math id="alg1.l6.m1.1" class="ltx_Math" alttext="count=1" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mrow id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml"><mi id="alg1.l6.m1.1.1.2.2" xref="alg1.l6.m1.1.1.2.2.cmml">c</mi><mo id="alg1.l6.m1.1.1.2.1" xref="alg1.l6.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="alg1.l6.m1.1.1.2.3" xref="alg1.l6.m1.1.1.2.3.cmml">o</mi><mo id="alg1.l6.m1.1.1.2.1a" xref="alg1.l6.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="alg1.l6.m1.1.1.2.4" xref="alg1.l6.m1.1.1.2.4.cmml">u</mi><mo id="alg1.l6.m1.1.1.2.1b" xref="alg1.l6.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="alg1.l6.m1.1.1.2.5" xref="alg1.l6.m1.1.1.2.5.cmml">n</mi><mo id="alg1.l6.m1.1.1.2.1c" xref="alg1.l6.m1.1.1.2.1.cmml" lspace="0px" rspace="0px"></mo><mi id="alg1.l6.m1.1.1.2.6" xref="alg1.l6.m1.1.1.2.6.cmml">t</mi></mrow><mo id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">=</mo><mn id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><eq id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"></eq><apply id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2"><times id="alg1.l6.m1.1.1.2.1.cmml" xref="alg1.l6.m1.1.1.2.1"></times><ci id="alg1.l6.m1.1.1.2.2.cmml" xref="alg1.l6.m1.1.1.2.2">𝑐</ci><ci id="alg1.l6.m1.1.1.2.3.cmml" xref="alg1.l6.m1.1.1.2.3">𝑜</ci><ci id="alg1.l6.m1.1.1.2.4.cmml" xref="alg1.l6.m1.1.1.2.4">𝑢</ci><ci id="alg1.l6.m1.1.1.2.5.cmml" xref="alg1.l6.m1.1.1.2.5">𝑛</ci><ci id="alg1.l6.m1.1.1.2.6.cmml" xref="alg1.l6.m1.1.1.2.6">𝑡</ci></apply><cn type="integer" id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">count=1</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m1.1d">italic_c italic_o italic_u italic_n italic_t = 1</annotation></semantics></math> <span id="alg1.l6.2" class="ltx_text ltx_font_bold">to</span> <math id="alg1.l6.m2.1" class="ltx_Math" alttext="N_{h}-1" display="inline"><semantics id="alg1.l6.m2.1a"><mrow id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml"><msub id="alg1.l6.m2.1.1.2" xref="alg1.l6.m2.1.1.2.cmml"><mi id="alg1.l6.m2.1.1.2.2" xref="alg1.l6.m2.1.1.2.2.cmml">N</mi><mi id="alg1.l6.m2.1.1.2.3" xref="alg1.l6.m2.1.1.2.3.cmml">h</mi></msub><mo id="alg1.l6.m2.1.1.1" xref="alg1.l6.m2.1.1.1.cmml">−</mo><mn id="alg1.l6.m2.1.1.3" xref="alg1.l6.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><apply id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1"><minus id="alg1.l6.m2.1.1.1.cmml" xref="alg1.l6.m2.1.1.1"></minus><apply id="alg1.l6.m2.1.1.2.cmml" xref="alg1.l6.m2.1.1.2"><csymbol cd="ambiguous" id="alg1.l6.m2.1.1.2.1.cmml" xref="alg1.l6.m2.1.1.2">subscript</csymbol><ci id="alg1.l6.m2.1.1.2.2.cmml" xref="alg1.l6.m2.1.1.2.2">𝑁</ci><ci id="alg1.l6.m2.1.1.2.3.cmml" xref="alg1.l6.m2.1.1.2.3">ℎ</ci></apply><cn type="integer" id="alg1.l6.m2.1.1.3.cmml" xref="alg1.l6.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">N_{h}-1</annotation><annotation encoding="application/x-llamapun" id="alg1.l6.m2.1d">italic_N start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT - 1</annotation></semantics></math>&nbsp;<span id="alg1.l6.3" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l7" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l7.1" class="ltx_text ltx_font_bold">for</span>&nbsp;For each host concurrently.&nbsp;<span id="alg1.l7.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l8" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute memory efficient attention incrementally using local query, key, value blocks.

</div>
<div id="alg1.l9" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Send key and value blocks to next host and receive key and value blocks from previous host.

</div>
<div id="alg1.l10" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l10.1" class="ltx_text ltx_font_bold">end</span>&nbsp;<span id="alg1.l10.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l11" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l11.1" class="ltx_text ltx_font_bold">end</span>&nbsp;<span id="alg1.l11.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l12" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l12.1" class="ltx_text ltx_font_bold">for</span>&nbsp;For each host concurrently.&nbsp;<span id="alg1.l12.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l13" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Compute memory efficient feedforward using local attention output.

</div>
<div id="alg1.l14" class="ltx_listingline">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span id="alg1.l14.1" class="ltx_text ltx_font_bold">end</span>&nbsp;<span id="alg1.l14.2" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l15" class="ltx_listingline">&nbsp;&nbsp;<span id="alg1.l15.1" class="ltx_text ltx_font_bold">end</span>&nbsp;<span id="alg1.l15.2" class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Setting</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p" id="S4.p1.1">우리는 최대 시퀀스 길이와 모델 플롭 활용도를 벤치마킹하여 Transformer 모델 개선에 Ring Attention을 사용하는 것이 미치는 영향을 평가한다.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Model Configuration. </span> 우리의 연구는 LLaMA 아키텍처를 기반으로 하며, 우리는 실험에서 3B, 7B, 13B 및 30B 모델 크기를 고려한다.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Baselines. </span> 어텐션 매트릭스를 구체화하여 자기 어텐션을 계산하고 피드포워드 네트워크를 정상적으로 계산하는 바닐라 트랜스포머 <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="#bib.bib39" title="">2017</a>)</cite>와 비교하여 우리의 방법을 평가한다. 메모리 효율적인 어텐션을 가진 트랜스포머 <cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a class="ltx_ref" href="#bib.bib31" title="">2021</a>)</cite>와 그것의 효율적인 CUDA 구현 <cite class="ltx_cite ltx_citemacro_citep">(Dao et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>와 메모리 효율적인 어텐션과 피드포워드 <cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>를 모두 가진 트랜스포머이다.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Training Configuration. </span> 모든 방법에 대해 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a class="ltx_ref" href="#bib.bib31" title="">2021</a>; Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>에 이어 주의 및 피드포워드 모두에 풀 그래디언트 체크포인팅 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="#bib.bib5" title="">2016</a>)</cite>를 적용합니다. 실험은 GPU와 TPU 모두에 대해 이루어집니다. GPU의 경우 8개의 GPU가 있는 단일 DGX A100 서버와 32개의 A100 GPU가 분산된 서버를 모두 고려한다. 또한 오래된 세대 TPUv3에서 새로운 세대 TPUv4 및 TPUv5e에 이르기까지 TPU를 실험한다. 우리는 우리의 모든 결과가 혼합 정밀도 대신 완전 정밀도를 사용하여 얻어진다는 점에 주목한다.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p" id="S5.p1.1">실험에서 우리의 주요 목표는 가속기 메모리 내에서 지원되는 최대 시퀀스 길이, 모델 플롭 활용률 및 처리량을 포함하여 여러 주요 메트릭에 걸쳐 링 어텐션의 성능을 종합적으로 평가하는 것이다. 링 어텐션의 성능을 바닐라 트랜스포머 <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="#bib.bib39" title="">2017</a>)</cite>, 메모리 효율적인 어텐션을 가진 트랜스포머 <cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a class="ltx_ref" href="#bib.bib31" title="">2021</a>)</cite>, 메모리 효율적인 어텐션과 피드포워드 <cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>를 포함한 여러 기본 모델과 모델 크기와 가속기 구성에 따라 비교한다.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Evaluating Max Context Size</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p1.5">사전 종단 간 학습에서 널리 사용되는 완전 샤딩된 텐서 병렬심(FSDP) <cite class="ltx_cite ltx_citemacro_citep">(Facebook, <a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite>를 사용하여 최대 지원 컨텍스트 길이를 평가한다. 우리의 접근법은 텐서 병렬성과 무관하기 때문에 우리의 평가에서는 텐서 병렬성이 고려되지 않는다는 점에 주목한다. 연습자들은 우리의 방법을 텐서 병렬성과 결합할 수 있는데, 우리는 Section <a class="ltx_ref" href="#S5.SS2" title="5.2 Evaluating Model Flops Utilization ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5.2</span></a>에서 보여줄 것이다. FSDP를 사용하면 기준선과 접근 방식에 대해 토큰에서 동일한 배치 크기를 설정할 수 있으므로 공정한 비교를 보장합니다. 구체적으로, <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_n</annotation></semantics></math> 장치에서, FSDP는 베이스라인에 대한 모델을 샤드하는 데 사용되며, 이는 <math alttext="l" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">italic_l</annotation></semantics></math>의 시퀀스 길이를 제공한다. 토큰의 총 배치 크기는 <math alttext="nl" class="ltx_Math" display="inline" id="S5.SS1.p1.3.m3.1"><semantics id="S5.SS1.p1.3.m3.1a"><mrow id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mi id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml">n</mi><mo id="S5.SS1.p1.3.m3.1.1.1" lspace="0px" rspace="0px" xref="S5.SS1.p1.3.m3.1.1.1.cmml"></mo><mi id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><times id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1.1"></times><ci id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">𝑛</ci><ci id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">nl</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.3.m3.1d">italic_n italic_l</annotation></semantics></math>입니다. 링 어텐션과 함께 FSDP를 활용하여 시퀀스 길이를 <math alttext="\frac{nl}{m}" class="ltx_Math" display="inline" id="S5.SS1.p1.4.m4.1"><semantics id="S5.SS1.p1.4.m4.1a"><mfrac id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><mrow id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2.cmml"><mi id="S5.SS1.p1.4.m4.1.1.2.2" xref="S5.SS1.p1.4.m4.1.1.2.2.cmml">n</mi><mo id="S5.SS1.p1.4.m4.1.1.2.1" lspace="0px" rspace="0px" xref="S5.SS1.p1.4.m4.1.1.2.1.cmml"></mo><mi id="S5.SS1.p1.4.m4.1.1.2.3" xref="S5.SS1.p1.4.m4.1.1.2.3.cmml">l</mi></mrow><mi id="S5.SS1.p1.4.m4.1.1.3" xref="S5.SS1.p1.4.m4.1.1.3.cmml">m</mi></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><divide id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"></divide><apply id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2"><times id="S5.SS1.p1.4.m4.1.1.2.1.cmml" xref="S5.SS1.p1.4.m4.1.1.2.1"></times><ci id="S5.SS1.p1.4.m4.1.1.2.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2.2">𝑛</ci><ci id="S5.SS1.p1.4.m4.1.1.2.3.cmml" xref="S5.SS1.p1.4.m4.1.1.2.3">𝑙</ci></apply><ci id="S5.SS1.p1.4.m4.1.1.3.cmml" xref="S5.SS1.p1.4.m4.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">\frac{nl}{m}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.4.m4.1d">divide start_ARG italic_n italic_l end_ARG start_ARG italic_m end_ARG</annotation></semantics></math> 및 <math alttext="m" class="ltx_Math" display="inline" id="S5.SS1.p1.5.m5.1"><semantics id="S5.SS1.p1.5.m5.1a"><mi id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><ci id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">m</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.5.m5.1d">italic_m</annotation></semantics></math> 시퀀스로 확장한다. 이는 토큰의 총 배치 크기가 동일하게 유지되지만 Ring Attention을 사용하면 훨씬 더 큰 컨텍스트 크기를 사용할 수 있습니다. 표 <a class="ltx_ref" href="#S5.T3" title="Table 3 ‣ 5.1 Evaluating Max Context Size ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">3</span></a>는 우리의 실험 결과를 요약한 것이다.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS1.p2.3">당사의 링 어텐션 모델은 지속적으로 기준선을 능가하여 다양한 하드웨어 설정에서 뛰어난 확장성을 제공합니다. 예를 들어 32개의 A100 GPU를 사용하여 7B 모델에 대한 컨텍스트 크기에서 100만 개 이상의 토큰을 달성했으며, 이는 이전 베스트보다 32배 개선되었다. 또한 TPUv4-512와 같은 더 큰 가속기를 사용할 때 Ring Attention은 컨텍스트 크기를 256배 증가시킬 수 있으며 3천만 개 이상의 토큰의 훈련 시퀀스를 허용한다. 또한, 본 논문에서 제안한 링 어텐션 모델은 8 A100에서 8배 향상, TPUv3-512에서 256배 향상으로 장치의 수에 따라 선형적으로 확장된다. 블록 주의 및 피드포워드를 사용하여 컨텍스트 크기 <math alttext="s" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">italic_s</annotation></semantics></math> <math alttext="n" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mi id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><ci id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_n</annotation></semantics></math> GPU에서 모델을 학습시킬 수 있다면, 본 논문에서 제안한 링 어텐션 접근법으로 컨텍스트 크기 <math alttext="ns" class="ltx_Math" display="inline" id="S5.SS1.p2.3.m3.1"><semantics id="S5.SS1.p2.3.m3.1a"><mrow id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml"><mi id="S5.SS1.p2.3.m3.1.1.2" xref="S5.SS1.p2.3.m3.1.1.2.cmml">n</mi><mo id="S5.SS1.p2.3.m3.1.1.1" lspace="0px" rspace="0px" xref="S5.SS1.p2.3.m3.1.1.1.cmml"></mo><mi id="S5.SS1.p2.3.m3.1.1.3" xref="S5.SS1.p2.3.m3.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><apply id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"><times id="S5.SS1.p2.3.m3.1.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1.1"></times><ci id="S5.SS1.p2.3.m3.1.1.2.cmml" xref="S5.SS1.p2.3.m3.1.1.2">𝑛</ci><ci id="S5.SS1.p2.3.m3.1.1.3.cmml" xref="S5.SS1.p2.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">ns</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">italic_n italic_s</annotation></semantics></math>의 모델을 학습시킬 수 있게 된다.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 3:</span>완전 샤딩된 데이터 병렬성 및 다양한 트랜스포머 아키텍처를 사용하는 엔드 투 엔드 트레이닝에서 지원되는 최대 컨텍스트 길이.</figcaption>
We show different model sizes and accelerators.
Baselines are vanilla transformer&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Vaswani et&nbsp;al., <a href="#bib.bib39" title="" class="ltx_ref">2017</a>)</cite>, transformer with memory efficient attention&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, and transformer with memory efficient attention and feedforward&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a href="#bib.bib24" title="" class="ltx_ref">2023b</a>)</cite>.
The context size is reported in tokens (1e3).
Our Ring Attention substantially outperforms baselines and enables training sequences that are up to device count times longer than prior state-of-the-arts.
</figcaption>
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:360.8pt;height:429.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.2pt,0.2pt) scale(0.999,0.999) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S5.T3.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Max context size supported (<math id="S5.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S5.T3.1.1.1.1.1.m1.1a"><mrow id="S5.T3.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" mathvariant="normal" rspace="0.222em" id="S5.T3.1.1.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mrow id="S5.T3.1.1.1.1.1.m1.1.1.3" xref="S5.T3.1.1.1.1.1.m1.1.1.3.cmml"><mn mathvariant="normal" id="S5.T3.1.1.1.1.1.m1.1.1.3.2" xref="S5.T3.1.1.1.1.1.m1.1.1.3.2.cmml">1</mn><mo mathvariant="bold" id="S5.T3.1.1.1.1.1.m1.1.1.3.1" xref="S5.T3.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S5.T3.1.1.1.1.1.m1.1.1.3.3" xref="S5.T3.1.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo mathvariant="bold" id="S5.T3.1.1.1.1.1.m1.1.1.3.1a" xref="S5.T3.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn mathvariant="normal" id="S5.T3.1.1.1.1.1.m1.1.1.3.4" xref="S5.T3.1.1.1.1.1.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1"><times id="S5.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S5.T3.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S5.T3.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3"><times id="S5.T3.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3.1"></times><cn type="integer" id="S5.T3.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3.2">1</cn><ci id="S5.T3.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3.3">normal-e</ci><cn type="integer" id="S5.T3.1.1.1.1.1.m1.1.1.3.4.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</span></td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S5.T3.1.1.2" class="ltx_tr">
<td id="S5.T3.1.1.2.1" class="ltx_td ltx_border_r"></td>
<td id="S5.T3.1.1.2.2" class="ltx_td ltx_align_right ltx_border_t">Vanilla</td>
<td id="S5.T3.1.1.2.3" class="ltx_td ltx_align_right ltx_border_t">
<span id="S5.T3.1.1.2.3.1" class="ltx_text"></span> <span id="S5.T3.1.1.2.3.2" class="ltx_text">
<span id="S5.T3.1.1.2.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.2.3.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.2.3.2.1.1.1" class="ltx_td ltx_align_center">Memory</span></span>
<span id="S5.T3.1.1.2.3.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.2.3.2.1.2.1" class="ltx_td ltx_align_center">Efficient Attn</span></span>
</span></span> <span id="S5.T3.1.1.2.3.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.2.4" class="ltx_td ltx_align_right ltx_border_t">
<span id="S5.T3.1.1.2.4.1" class="ltx_text"></span> <span id="S5.T3.1.1.2.4.2" class="ltx_text">
<span id="S5.T3.1.1.2.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.2.4.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.2.4.2.1.1.1" class="ltx_td ltx_align_center">Memory Efficient</span></span>
<span id="S5.T3.1.1.2.4.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.2.4.2.1.2.1" class="ltx_td ltx_align_center">Attn and FFN</span></span>
</span></span> <span id="S5.T3.1.1.2.4.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">
<span id="S5.T3.1.1.2.5.1" class="ltx_text"></span> <span id="S5.T3.1.1.2.5.2" class="ltx_text">
<span id="S5.T3.1.1.2.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.2.5.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.2.5.2.1.1.1" class="ltx_td ltx_align_center">Ring Attention</span></span>
<span id="S5.T3.1.1.2.5.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.2.5.2.1.2.1" class="ltx_td ltx_align_center">(Ours)</span></span>
</span></span> <span id="S5.T3.1.1.2.5.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.2.6" class="ltx_td ltx_align_right">
<span id="S5.T3.1.1.2.6.1" class="ltx_text"></span> <span id="S5.T3.1.1.2.6.2" class="ltx_text">
<span id="S5.T3.1.1.2.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.2.6.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.2.6.2.1.1.1" class="ltx_td ltx_align_center">Ours</span></span>
<span id="S5.T3.1.1.2.6.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.2.6.2.1.2.1" class="ltx_td ltx_align_center">vs SOTA</span></span>
</span></span> <span id="S5.T3.1.1.2.6.3" class="ltx_text"></span>
</td>
</tr>
<tr id="S5.T3.1.1.3" class="ltx_tr">
<td id="S5.T3.1.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S5.T3.1.1.3.1.1" class="ltx_text"></span><span id="S5.T3.1.1.3.1.2" class="ltx_text">
<span id="S5.T3.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.3.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.3.1.2.1.1.1" class="ltx_td ltx_align_left">8x A100</span></span>
<span id="S5.T3.1.1.3.1.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.3.1.2.1.2.1" class="ltx_td ltx_align_left">NVLink</span></span>
</span></span> <span id="S5.T3.1.1.3.1.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.3.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.3.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.3.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.3.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.3.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.4" class="ltx_tr">
<td id="S5.T3.1.1.4.1" class="ltx_td ltx_align_left ltx_border_r">3B</td>
<td id="S5.T3.1.1.4.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.4.3" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.4.4" class="ltx_td ltx_align_right">64</td>
<td id="S5.T3.1.1.4.5" class="ltx_td ltx_align_right ltx_border_r">512</td>
<td id="S5.T3.1.1.4.6" class="ltx_td ltx_align_right">8x</td>
</tr>
<tr id="S5.T3.1.1.5" class="ltx_tr">
<td id="S5.T3.1.1.5.1" class="ltx_td ltx_align_left ltx_border_r">7B</td>
<td id="S5.T3.1.1.5.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T3.1.1.5.3" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.5.4" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.5.5" class="ltx_td ltx_align_right ltx_border_r">256</td>
<td id="S5.T3.1.1.5.6" class="ltx_td ltx_align_right">8x</td>
</tr>
<tr id="S5.T3.1.1.6" class="ltx_tr">
<td id="S5.T3.1.1.6.1" class="ltx_td ltx_align_left ltx_border_r">13B</td>
<td id="S5.T3.1.1.6.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T3.1.1.6.3" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.6.4" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.6.5" class="ltx_td ltx_align_right ltx_border_r">128</td>
<td id="S5.T3.1.1.6.6" class="ltx_td ltx_align_right">8x</td>
</tr>
<tr id="S5.T3.1.1.7" class="ltx_tr">
<td id="S5.T3.1.1.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S5.T3.1.1.7.1.1" class="ltx_text"></span><span id="S5.T3.1.1.7.1.2" class="ltx_text">
<span id="S5.T3.1.1.7.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.7.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.7.1.2.1.1.1" class="ltx_td ltx_align_left">32x A100</span></span>
<span id="S5.T3.1.1.7.1.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.7.1.2.1.2.1" class="ltx_td ltx_align_left">InfiniBand</span></span>
</span></span> <span id="S5.T3.1.1.7.1.3" class="ltx_text"></span>
</td>
<td id="S5.T3.1.1.7.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.7.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.7.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.7.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.7.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.8" class="ltx_tr">
<td id="S5.T3.1.1.8.1" class="ltx_td ltx_align_left ltx_border_r">7B</td>
<td id="S5.T3.1.1.8.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.8.3" class="ltx_td ltx_align_right">64</td>
<td id="S5.T3.1.1.8.4" class="ltx_td ltx_align_right">128</td>
<td id="S5.T3.1.1.8.5" class="ltx_td ltx_align_right ltx_border_r">4096</td>
<td id="S5.T3.1.1.8.6" class="ltx_td ltx_align_right">32x</td>
</tr>
<tr id="S5.T3.1.1.9" class="ltx_tr">
<td id="S5.T3.1.1.9.1" class="ltx_td ltx_align_left ltx_border_r">13B</td>
<td id="S5.T3.1.1.9.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.9.3" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.9.4" class="ltx_td ltx_align_right">64</td>
<td id="S5.T3.1.1.9.5" class="ltx_td ltx_align_right ltx_border_r">2048</td>
<td id="S5.T3.1.1.9.6" class="ltx_td ltx_align_right">32x</td>
</tr>
<tr id="S5.T3.1.1.10" class="ltx_tr">
<td id="S5.T3.1.1.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">TPUv3-512</td>
<td id="S5.T3.1.1.10.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.10.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.10.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.10.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.10.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.11" class="ltx_tr">
<td id="S5.T3.1.1.11.1" class="ltx_td ltx_align_left ltx_border_r">7B</td>
<td id="S5.T3.1.1.11.2" class="ltx_td ltx_align_right">1</td>
<td id="S5.T3.1.1.11.3" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.11.4" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.11.5" class="ltx_td ltx_align_right ltx_border_r">2048</td>
<td id="S5.T3.1.1.11.6" class="ltx_td ltx_align_right">256x</td>
</tr>
<tr id="S5.T3.1.1.12" class="ltx_tr">
<td id="S5.T3.1.1.12.1" class="ltx_td ltx_align_left ltx_border_r">13B</td>
<td id="S5.T3.1.1.12.2" class="ltx_td ltx_align_right">1</td>
<td id="S5.T3.1.1.12.3" class="ltx_td ltx_align_right">2</td>
<td id="S5.T3.1.1.12.4" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.12.5" class="ltx_td ltx_align_right ltx_border_r">1024</td>
<td id="S5.T3.1.1.12.6" class="ltx_td ltx_align_right">128x</td>
</tr>
<tr id="S5.T3.1.1.13" class="ltx_tr">
<td id="S5.T3.1.1.13.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">TPUv4-1024</td>
<td id="S5.T3.1.1.13.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.13.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.13.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.13.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.13.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.14" class="ltx_tr">
<td id="S5.T3.1.1.14.1" class="ltx_td ltx_align_left ltx_border_r">3B</td>
<td id="S5.T3.1.1.14.2" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.14.3" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.14.4" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.14.5" class="ltx_td ltx_align_right ltx_border_r">16384</td>
<td id="S5.T3.1.1.14.6" class="ltx_td ltx_align_right">512x</td>
</tr>
<tr id="S5.T3.1.1.15" class="ltx_tr">
<td id="S5.T3.1.1.15.1" class="ltx_td ltx_align_left ltx_border_r">7B</td>
<td id="S5.T3.1.1.15.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.15.3" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.15.4" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.15.5" class="ltx_td ltx_align_right ltx_border_r">8192</td>
<td id="S5.T3.1.1.15.6" class="ltx_td ltx_align_right">512x</td>
</tr>
<tr id="S5.T3.1.1.16" class="ltx_tr">
<td id="S5.T3.1.1.16.1" class="ltx_td ltx_align_left ltx_border_r">13B</td>
<td id="S5.T3.1.1.16.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.16.3" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.16.4" class="ltx_td ltx_align_right">16</td>
<td id="S5.T3.1.1.16.5" class="ltx_td ltx_align_right ltx_border_r">4096</td>
<td id="S5.T3.1.1.16.6" class="ltx_td ltx_align_right">256x</td>
</tr>
<tr id="S5.T3.1.1.17" class="ltx_tr">
<td id="S5.T3.1.1.17.1" class="ltx_td ltx_align_left ltx_border_r">30B</td>
<td id="S5.T3.1.1.17.2" class="ltx_td ltx_align_right">2</td>
<td id="S5.T3.1.1.17.3" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.17.4" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.17.5" class="ltx_td ltx_align_right ltx_border_r">2048</td>
<td id="S5.T3.1.1.17.6" class="ltx_td ltx_align_right">256x</td>
</tr>
<tr id="S5.T3.1.1.18" class="ltx_tr">
<td id="S5.T3.1.1.18.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">TPUv5e-256</td>
<td id="S5.T3.1.1.18.2" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.18.3" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.18.4" class="ltx_td ltx_border_t"></td>
<td id="S5.T3.1.1.18.5" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.18.6" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S5.T3.1.1.19" class="ltx_tr">
<td id="S5.T3.1.1.19.1" class="ltx_td ltx_align_left ltx_border_r">3B</td>
<td id="S5.T3.1.1.19.2" class="ltx_td ltx_align_right">4</td>
<td id="S5.T3.1.1.19.3" class="ltx_td ltx_align_right">8</td>
<td id="S5.T3.1.1.19.4" class="ltx_td ltx_align_right">32</td>
<td id="S5.T3.1.1.19.5" class="ltx_td ltx_align_right ltx_border_r">4096</td>
<td id="S5.T3.1.1.19.6" class="ltx_td ltx_align_right">128x</td>
</tr>
<tr id="S5.T3.1.1.20" class="ltx_tr">
<td id="S5.T3.1.1.20.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">7B</td>
<td id="S5.T3.1.1.20.2" class="ltx_td ltx_align_right ltx_border_bb">2</td>
<td id="S5.T3.1.1.20.3" class="ltx_td ltx_align_right ltx_border_bb">8</td>
<td id="S5.T3.1.1.20.4" class="ltx_td ltx_align_right ltx_border_bb">16</td>
<td id="S5.T3.1.1.20.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">2048</td>
<td id="S5.T3.1.1.20.6" class="ltx_td ltx_align_right ltx_border_bb">128x</td>
</tr>
</tbody></table>
</span></div>
</figure>
<figure id="S5.T4" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 4:</span>모델 플롭스 활용률(MFU)은 서로 다른 훈련 구성: 모델 크기, 계산 및 컨텍스트 길이입니다.</figcaption>
Ring Attention enables training
<span id="S5.T4.4.1" class="ltx_text ltx_framed ltx_framed_underline">large models (7B-65B) on large input context sizes (over 4M) with negligible overheads.</span>
</figcaption><div class="ltx_flex_figure ltx_flex_table">

<div class="ltx_flex_cell 
                  ltx_flex_size_1"><img src="https://ar5iv.labs.arxiv.org/html/2310.01889/assets/figures/mfu_trend.png" id="S5.T4.g1" class="ltx_graphics ltx_centering ltx_flex_size_1 ltx_img_landscape" width="598" height="251" alt="[Uncaptioned image]"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell 
                  ltx_flex_size_1">
<div id="S5.T4.2" class="ltx_inline-block ltx_flex_size_1 ltx_align_center ltx_transformed_outer" style="width:350.5pt;height:112.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.2pt,3.0pt) scale(0.95,0.95) ;">
<table id="S5.T4.2.2" class="ltx_tabular ltx_align_middle">
<tbody><tr id="S5.T4.2.2.3" class="ltx_tr">
<td id="S5.T4.2.2.3.1" class="ltx_td ltx_border_tt"></td>
<td id="S5.T4.2.2.3.2" class="ltx_td ltx_align_right ltx_border_tt">Model size</td>
<td id="S5.T4.2.2.3.3" class="ltx_td ltx_align_right ltx_border_tt">7B</td>
<td id="S5.T4.2.2.3.4" class="ltx_td ltx_align_right ltx_border_tt">13B</td>
<td id="S5.T4.2.2.3.5" class="ltx_td ltx_align_right ltx_border_tt">13B</td>
<td id="S5.T4.2.2.3.6" class="ltx_td ltx_align_right ltx_border_tt">30B</td>
<td id="S5.T4.2.2.3.7" class="ltx_td ltx_align_right ltx_border_tt">65B</td>
</tr>
<tr id="S5.T4.2.2.4" class="ltx_tr">
<td id="S5.T4.2.2.4.1" class="ltx_td"></td>
<td id="S5.T4.2.2.4.2" class="ltx_td ltx_align_right ltx_border_t">Compute</td>
<td id="S5.T4.2.2.4.3" class="ltx_td ltx_align_right ltx_border_t">8x A100</td>
<td id="S5.T4.2.2.4.4" class="ltx_td ltx_align_right ltx_border_t">8x A100</td>
<td id="S5.T4.2.2.4.5" class="ltx_td ltx_align_right ltx_border_t">32x A100</td>
<td id="S5.T4.2.2.4.6" class="ltx_td ltx_align_right ltx_border_t">TPUv4-1024</td>
<td id="S5.T4.2.2.4.7" class="ltx_td ltx_align_right ltx_border_t">TPUv4-1024</td>
</tr>
<tr id="S5.T4.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T4.1.1.1.2.1" class="ltx_text"></span><span id="S5.T4.1.1.1.2.2" class="ltx_text">
<span id="S5.T4.1.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.1.1.1.2.2.1.1" class="ltx_tr">
<span id="S5.T4.1.1.1.2.2.1.1.1" class="ltx_td ltx_align_left">Memory efficient</span></span>
<span id="S5.T4.1.1.1.2.2.1.2" class="ltx_tr">
<span id="S5.T4.1.1.1.2.2.1.2.1" class="ltx_td ltx_align_left">attention &amp; FFN</span></span>
</span></span> <span id="S5.T4.1.1.1.2.3" class="ltx_text"></span>
</td>
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_right ltx_border_t">
<span id="S5.T4.1.1.1.1.2" class="ltx_text"></span> <span id="S5.T4.1.1.1.1.1" class="ltx_text">
<span id="S5.T4.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S5.T4.1.1.1.1.1.1.2.1" class="ltx_td ltx_align_center">Context size</span></span>
<span id="S5.T4.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T4.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center">(<math id="S5.T4.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S5.T4.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.1" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mrow id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mn id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">1</mn><mo id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1a" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.4" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.1.1.1.m1.1b"><apply id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1"><times id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3"><times id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.1"></times><cn type="integer" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.2">1</cn><ci id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.3">e</ci><cn type="integer" id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.4.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.1.1.1.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.1.1.1.1.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</span></span>
</span></span> <span id="S5.T4.1.1.1.1.3" class="ltx_text"></span>
</td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_right ltx_border_t">32</td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_right ltx_border_t">16</td>
<td id="S5.T4.1.1.1.5" class="ltx_td ltx_align_right ltx_border_t">64</td>
<td id="S5.T4.1.1.1.6" class="ltx_td ltx_align_right ltx_border_t">16</td>
<td id="S5.T4.1.1.1.7" class="ltx_td ltx_align_right ltx_border_t">8</td>
</tr>
<tr id="S5.T4.2.2.2" class="ltx_tr">
<td id="S5.T4.2.2.2.2" class="ltx_td ltx_align_left ltx_border_bb">Ring Attention</td>
<td id="S5.T4.2.2.2.1" class="ltx_td ltx_align_right ltx_border_bb">
<span id="S5.T4.2.2.2.1.2" class="ltx_text"></span> <span id="S5.T4.2.2.2.1.1" class="ltx_text">
<span id="S5.T4.2.2.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.2.2.2.1.1.1.2" class="ltx_tr">
<span id="S5.T4.2.2.2.1.1.1.2.1" class="ltx_td ltx_align_center">Context size</span></span>
<span id="S5.T4.2.2.2.1.1.1.1" class="ltx_tr">
<span id="S5.T4.2.2.2.1.1.1.1.1" class="ltx_td ltx_align_center">(<math id="S5.T4.2.2.2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 1\mathrm{e}3" display="inline"><semantics id="S5.T4.2.2.2.1.1.1.1.1.m1.1a"><mrow id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.2" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.1" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mrow id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.cmml"><mn id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.2" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.2.cmml">1</mn><mo id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mi mathvariant="normal" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.3" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1a" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1.cmml" lspace="0px" rspace="0px"></mo><mn id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.4" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.4.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.1.1.1.1.1.m1.1b"><apply id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1"><times id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3"><times id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.1"></times><cn type="integer" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.2">1</cn><ci id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.3">e</ci><cn type="integer" id="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.4.cmml" xref="S5.T4.2.2.2.1.1.1.1.1.m1.1.1.3.4">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.1.1.1.1.1.m1.1c">\times 1\mathrm{e}3</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.1.1.1.1.1.m1.1d">× 1 roman_e 3</annotation></semantics></math>)</span></span>
</span></span> <span id="S5.T4.2.2.2.1.3" class="ltx_text"></span>
</td>
<td id="S5.T4.2.2.2.3" class="ltx_td ltx_align_right ltx_border_bb">256</td>
<td id="S5.T4.2.2.2.4" class="ltx_td ltx_align_right ltx_border_bb">128</td>
<td id="S5.T4.2.2.2.5" class="ltx_td ltx_align_right ltx_border_bb">2048</td>
<td id="S5.T4.2.2.2.6" class="ltx_td ltx_align_right ltx_border_bb">2048</td>
<td id="S5.T4.2.2.2.7" class="ltx_td ltx_align_right ltx_border_bb">1024</td>
</tr>
</tbody></table>
</span></div>
</div>
</div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluating Model Flops Utilization</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p1.1">Jax SPMD를 사용하여 LLaMA와 OpenLLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="#bib.bib38" title="">2023</a>; Geng and Liu, <a class="ltx_ref" href="#bib.bib12" title="">2023</a>)</cite>에 이어 완전 샤드 데이터 병렬성(FSDP) <cite class="ltx_cite ltx_citemacro_citep">(Facebook, <a class="ltx_ref" href="#bib.bib11" title="">2023</a>)</cite> 및 텐서 병렬성을 사용하여 표준 훈련 설정에서 Ring Attention의 모델 플롭 활용도(MFU)를 평가한다. 토큰의 배치 크기는 8/32x A100에서 2M, TPUv4-256에서 4M이다. 본 연구의 목표는 모델 크기와 컨텍스트 길이가 중요한 성능 메트릭인 MFU에 미치는 영향을 조사하는 동시에 접근 방식의 이점을 강조하는 것이다. 표 <a class="ltx_ref" href="#S5.T4" title="Table 4 ‣ 5.1 Evaluating Max Context Size ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">4</span></a>는 다양한 모델 크기와 컨텍스트 길이에 대한 MFU에 대한 실험 결과를 보여준다. 최첨단 메모리 효율 트랜스포머 BPT<cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>를 사용하여 달성된 MFU를 제시하고, 이러한 결과를 바탕으로 우리의 예상 MFU와 비교하고, 우리의 접근법(Ring Attention)으로 얻은 실제 MFU를 입증한다. 공정한 비교를 위해 BPT와 우리의 접근법은 GPU와 TPU 모두에서 동일한 BPT 구현을 기반으로 한다. GPU에서 우리의 접근 방식인 Ring Attention은 더 컴퓨팅 효율적인 트리톤 코드 <cite class="ltx_cite ltx_citemacro_citep">(kernel team, <a class="ltx_ref" href="#bib.bib18" title="">2023</a>)</cite> 또는 CUDA 코드 <cite class="ltx_cite ltx_citemacro_citep">(Dao et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>의 메모리 효율적인 어텐션 <cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a class="ltx_ref" href="#bib.bib31" title="">2021</a>)</cite>와 통합될 수 있습니다. 마찬가지로 TPU에서도 Pallas <cite class="ltx_cite ltx_citemacro_citep">(team, <a class="ltx_ref" href="#bib.bib37" title="">2023</a>)</cite>와 호환됩니다. 이러한 저수준 커널 구현을 우리의 접근법으로 조합하면 MFU를 최대화할 수 있으므로 향후 작업에 맡긴다.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p class="ltx_p" id="S5.SS2.p2.1">링 어텐션은 셀프 어텐션에 대한 컨텍스트 크기를 훨씬 더 길게 트레이닝하여, 베이스라인 모델에 비해 셀프 어텐션 FLOP가 더 높아진다. 자기 주의는 피드포워드보다 MFU가 낮기 때문에 링 주의는 기준 모델보다 MFU가 낮을 것으로 예상된다. 우리의 방법은 MFU를 유지하는 동시에 훨씬 더 긴 컨텍스트 길이로 훈련을 가능하게 한다는 측면에서 분명한 이점을 제공한다. 표 <a class="ltx_ref" href="#S5.T4" title="Table 4 ‣ 5.1 Evaluating Max Context Size ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">4</span></a>에 나타낸 바와 같이, 이전의 최첨단 기술에 대한 우리의 접근법을 비교할 때, MFU 또는 처리량을 손상시키지 않으면서 매우 큰 컨텍스트 모델을 훈련시킬 수 있다는 것이 명백하다.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2310.01889/assets/figures/context_acc.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="202" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 3:</span>장거리 라인 검색 태스크에서 서로 다른 모델의 비교.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Impact on LLM Performance</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p class="ltx_p" id="S5.SS3.p1.1">본 논문에서 제안한 방법을 LLaMA 모델에 적용하여 보다 긴 문맥에 대한 링 어텐션을 평가한다. 이 실험에서는 수백만 개의 컨텍스트 토큰으로 학습을 가능하게 하는 반면, 클라우드 컴퓨팅 예산에 대한 제약으로 인해 컨텍스트 길이를 512K 토큰으로 제한하는 LLaMA-13B 모델에 대한 미세 조정을 수행했다. 이 미세 조정은 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al., <a class="ltx_ref" href="#bib.bib6" title="">2023</a>; Geng et al., <a class="ltx_ref" href="#bib.bib13" title="">2023</a>)</cite>에 설명된 대로 방법론에 따라 ShareGPT 데이터 세트를 사용하여 32개의 A100 GPU에서 수행되었다. 그런 다음 라인 검색 테스트 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="#bib.bib21" title="">2023</a>)</cite>에 대해 미세 조정 모델을 평가했다. 이 테스트에서, 모델은 긴 문서에서 숫자를 정확하게 검색할 필요가 있으며, 태스크는 검색 정확도에 반영되어 긴 문맥에서 텍스트 생성, 검색 및 정보 연관의 능력을 효과적으로 포착할 수 있다. 그림 <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ 5.2 Evaluating Model Flops Utilization ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">3</span></a>는 다양한 컨텍스트 길이에 걸쳐 서로 다른 모델에 대한 정확도 결과를 제시한다(토큰 단위로 측정). 특히, 우리의 모델인 Ring Attention-13B-512K는 긴 컨텍스트에도 높은 정확도 수준을 유지한다는 점에서 두드러진다. GPT3.5-터보-16K, 비쿠나-16B-16K 및 클로드-2-100K는 짧은 컨텍스트 길이 내에서 경쟁적 정확도를 보여준다. 그러나 확장된 컨텍스트 길이를 처리할 수 없습니다.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p" id="S6.p1.1">트랜스포머는 AI 분야에서 상당한 관심을 받았고 수많은 최첨단 모델의 중추가 되었다. 여러 연구에서 트랜스포머의 메모리 한계를 해결하고 더 넓은 범위의 문제에 적용할 수 있도록 메모리 효율적인 기술을 탐구했다. 타일링 기법 <cite class="ltx_cite ltx_citemacro_citep">(Milakov and Gimelshein, <a class="ltx_ref" href="#bib.bib25" title="">2018</a>)</cite>를 사용하여 블록 단위로 정확한 자기 주의력을 계산하는 것은 메모리 효율적인 주의 메커니즘 <cite class="ltx_cite ltx_citemacro_citep">(Rabe and Staats, <a class="ltx_ref" href="#bib.bib31" title="">2021</a>)</cite>와 그것의 효율적인 CUDA 구현 <cite class="ltx_cite ltx_citemacro_citep">(Dao et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>)</cite>의 개발로 이어졌고, 피드포워드 및 자기 주의 블록 단위로 컴퓨팅을 제안하는 블록 단위 병렬 트랜스포머 <cite class="ltx_cite ltx_citemacro_citep">(Liu and Abbeel, <a class="ltx_ref" href="#bib.bib24" title="">2023b</a>)</cite>는 메모리 요구량을 크게 감소시켰다. 이러한 발전에 따라, 우리의 작업은 트랜스포머에 대한 메모리 효율적인 계산 범주에 속한다. 다른 연구에서는 주의 메커니즘의 근사를 조사했지만 이러한 노력은 종종 차선책 결과를 산출하거나 확장 중에 문제에 직면했다. 이러한 기술에 대한 심층적인 검토를 위해 설문 조사 <cite class="ltx_cite ltx_citemacro_citep">(Narang et al., <a class="ltx_ref" href="#bib.bib27" title="">2021</a>; Tay et al., <a class="ltx_ref" href="#bib.bib36" title="">2022</a>)</cite>를 참조하는 것이 좋다. 또 다른 연구 방법은 데이터 병렬성 <cite class="ltx_cite ltx_citemacro_citep">(Dean et al., <a class="ltx_ref" href="#bib.bib10" title="">2012</a>)</cite>, 텐서 병렬성 <cite class="ltx_cite ltx_citemacro_citep">(Shoeybi et al., <a class="ltx_ref" href="#bib.bib35" title="">2019</a>)</cite>, 파이프라인 병렬성 <cite class="ltx_cite ltx_citemacro_citep">(Narayanan et al., <a class="ltx_ref" href="#bib.bib28" title="">2019</a>; Huang et al., <a class="ltx_ref" href="#bib.bib15" title="">2019</a>; Narayanan et al., <a class="ltx_ref" href="#bib.bib29" title="">2021</a>)</cite>, 시퀀스 병렬성 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="#bib.bib22" title="">2021</a>; Korthikanti et al., <a class="ltx_ref" href="#bib.bib19" title="">2022</a>; Jacobs et al., <a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite>, FSDP <cite class="ltx_cite ltx_citemacro_citep">(Facebook, <a class="ltx_ref" href="#bib.bib11" title="">2023</a>; Rajbhandari et al., <a class="ltx_ref" href="#bib.bib32" title="">2020</a>)</cite> 등 다양한 병렬성 방법을 탐구한다. 자기 주의의 활성화는 대용량 컨텍스트 모델에 상당한 양의 메모리를 필요로 한다. 텐서 병렬성은 활성화 메모리의 일부만을 감소시킬 수 있고 시퀀스 병렬성은 계산과 완전히 중첩될 수 없는 상당한 통신 오버헤드를 도입한다. 본 논문에서는 블록 단위의 병렬 트랜스포머를 이용하여 디바이스 간에 블록 단위의 주의와 피드포워드를 분배하고 동시에 호스트의 순환에서 키-값 블록의 통신과 쿼리-키-값 블록 및 피드포워드 계산을 중첩하여 디바이스 수보다 긴 시퀀스의 오버헤드를 무시할 수 있도록 한다. 컴퓨팅과의 중첩 통신은 고성능 컴퓨팅 문헌 <cite class="ltx_cite ltx_citemacro_citep">(Danalis et al., <a class="ltx_ref" href="#bib.bib7" title="">2005</a>; Wang et al., <a class="ltx_ref" href="#bib.bib40" title="">2022</a>; Danalis et al., <a class="ltx_ref" href="#bib.bib8" title="">2009</a>, <span class="ltx_text ltx_font_italic" id="S6.p1.1.1.1">inter alia</span>)</cite>에서 연구되어 왔다. 링 통신은 다른 병렬 컴퓨팅 시나리오 <cite class="ltx_cite ltx_citemacro_citep">(Bischof, <a class="ltx_ref" href="#bib.bib2" title="">2008</a>; Hursey and Graham, <a class="ltx_ref" href="#bib.bib16" title="">2011</a>; Gibiansky, <a class="ltx_ref" href="#bib.bib14" title="">2017</a>; Sergeev and Del Balso, <a class="ltx_ref" href="#bib.bib34" title="">2018</a>)</cite>에서 응용을 찾아냈지만, 본 연구는 트랜스포머에서 사용하는 셀프 어텐션에 적용할 수 있음을 보이고 블록 단위의 연산과 통신을 중복하여 큰 오버헤드를 추가하지 않고 트랜스포머 학습과 추론에 효율적으로 적합하도록 하는 첫 번째 작업으로 눈에 띈다.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p" id="S7.p1.1">결론적으로, 우리는 최신 AI 모델의 백본인 트랜스포머의 메모리 요구 사항을 줄이기 위한 메모리 효율적인 접근법을 제안한다. 제안된 방법은 성능을 유지하면서 컨텍스트 길이가 장치 수에 따라 선형적으로 확장될 수 있도록 하여 개별 장치에 의해 부과되는 메모리 병목 현상을 제거한다. 언어 모델링 및 강화 학습에 대한 광범위한 실험을 통해 그 효과를 입증하여 이전 메모리 효율적인 트랜스포머보다 장치 카운트 시간이 최대인 트레이닝 시퀀스가 주목에 대한 근사치를 만들지 않고도 컨텍스트 길이 1억을 초과할 수 있음을 보여준다.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">Limitations and Future Work. </span> 비록 우리의 방법이 Transformer 모델들에 대한 최첨단 컨텍스트 길이를 달성하지만, 그것은 해결해야 할 몇 가지 제한들을 가지고 있다:</p>
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="S7.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S7.I1.i1.p1.1.1">Scaled up training</span>: due to compute budget constraint, our experiments focus on evaluation the effectiveness of the proposed approach without large scale training models.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="S7.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S7.I1.i2.p1.1.1">Optimal compute performance</span>: While Ring Attention scale context length with device count while maintaining performance, optimizing low-level operations is required to achieve optimal compute performance. 향후 최대 시퀀스 길이와 최대 계산 성능 모두에 대해 CUDA, OpenAI 트리톤 또는 Jax 팔라스에 방법을 이식하는 것을 고려할 것을 제안한다.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S7.p2.2">미래 전망 측면에서, 무한대에 가까운 맥락의 가능성은 큰 비디오-오디오-언어 모델, 확장된 피드백 및 시행착오를 통한 학습, 코드베이스의 이해 및 생성, 유전자 서열과 같은 과학적 데이터를 이해하기 위해 AI 모델을 적용하는 것과 같은 흥미진진한 기회의 방대한 배열을 도입한다.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p" id="Sx1.p1.1">이 프로젝트는 해군 연구소의 보조금 N00014-21-1-2769에 의해 부분적으로 지원됩니다. 통찰력 있는 토론과 피드백에 대한 BAIR 및 RLL 커뮤니티에 감사를 표합니다. 우리는 또한 TPU에 대한 우리의 질문을 해결하고 이 작업의 초기 버전에 대한 통찰력 있는 피드백을 제공한 데이비드 패터슨에게 감사한다. 야시 카타리야와 샤라드 비크람은 잭스와 관련된 질문을 도와준 잭스 개발자 팀의 감사를 표합니다. 또한 이 작업에 대한 귀중한 피드백에 대해 트리 다오에게 감사드립니다. TPU에 대한 액세스 권한을 부여해 주신 구글 TPU 리서치 클라우드에 감사드립니다.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing claude, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.anthropic.com/index/introducing-claude" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.anthropic.com/index/introducing-claude</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bischof (2008)</span>
<span class="ltx_bibblock">
Christian Bischof.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Parallel computing: Architectures, algorithms, and
applications</em>, volume&nbsp;15.

</span>
<span class="ltx_bibblock">IOS Press, 2008.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.

</span>
<span class="ltx_bibblock">Decision transformer: Reinforcement learning via sequence modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
34:15084–15097, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et&nbsp;al. (2016)</span>
<span class="ltx_bibblock">
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.

</span>
<span class="ltx_bibblock">Training deep nets with sublinear memory cost.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1604.06174</em>, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi&nbsp;Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph&nbsp;E Gonzalez, et&nbsp;al.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">See https://vicuna.lmsys.org</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Danalis et&nbsp;al. (2005)</span>
<span class="ltx_bibblock">
Anthony Danalis, Ki-Yong Kim, Lori Pollock, and Martin Swany.

</span>
<span class="ltx_bibblock">Transformations to parallel codes for communication-computation
overlap.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">SC’05: Proceedings of the 2005 ACM/IEEE conference on
Supercomputing</em>, pages 58–58. IEEE, 2005.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Danalis et&nbsp;al. (2009)</span>
<span class="ltx_bibblock">
Anthony Danalis, Lori Pollock, Martin Swany, and John Cavazos.

</span>
<span class="ltx_bibblock">Mpi-aware compiler optimizations for improving
communication-computation overlap.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd international conference on
Supercomputing</em>, pages 316–325, 2009.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with
io-awareness.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:16344–16359, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dean et&nbsp;al. (2012)</span>
<span class="ltx_bibblock">
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke&nbsp;Yang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Large scale distributed deep networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 25, 2012.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Facebook (2023)</span>
<span class="ltx_bibblock">
Facebook.

</span>
<span class="ltx_bibblock">Fully Sharded Data Parallel: faster AI training with
fewer GPUs — engineering.fb.com.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://engineering.fb.com/2021/07/15/open-source/fsdp/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://engineering.fb.com/2021/07/15/open-source/fsdp/</a>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng and Liu (2023)</span>
<span class="ltx_bibblock">
Xinyang Geng and Hao Liu.

</span>
<span class="ltx_bibblock">Openllama: An open reproduction of llama, may 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">URL https://github. com/openlm-research/open_llama</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey
Levine, and Dawn Song.

</span>
<span class="ltx_bibblock">Koala: A dialogue model for academic research.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Blog post, April</em>, 1, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gibiansky (2017)</span>
<span class="ltx_bibblock">
Andrew Gibiansky.

</span>
<span class="ltx_bibblock">Bringing hpc techniques to deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Baidu Research, Tech. Rep.</em>, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
HyoukJoong Lee, Jiquan Ngiam, Quoc&nbsp;V Le, Yonghui Wu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Gpipe: Efficient training of giant neural networks using pipeline
parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hursey and Graham (2011)</span>
<span class="ltx_bibblock">
Joshua Hursey and Richard&nbsp;L Graham.

</span>
<span class="ltx_bibblock">Building a fault tolerant mpi application: A ring communication
example.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2011 IEEE International Symposium on Parallel and
Distributed Processing Workshops and Phd Forum</em>, pages 1549–1556. IEEE,
2011.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Sam&nbsp;Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song,
Samyam Rajbhandari, and Yuxiong He.

</span>
<span class="ltx_bibblock">Deepspeed ulysses: System optimizations for enabling training of
extreme long sequence transformer models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.14509</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">kernel team (2023)</span>
<span class="ltx_bibblock">
OpenAI kernel team.

</span>
<span class="ltx_bibblock">Openai triton fused attention, 2023.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Korthikanti et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael
Andersch, Mohammad Shoeybi, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Reducing activation recomputation in large transformer models.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.05198</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laskin et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu,
Catherine Cang, Lerrel Pinto, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Urlb: Unsupervised reinforcement learning benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.15191</em>, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph
E.&nbsp;Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.

</span>
<span class="ltx_bibblock">How long can open-source llms truly promise on context length?, June
2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://lmsys.org/blog/2023-06-29-longchat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lmsys.org/blog/2023-06-29-longchat</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You.

</span>
<span class="ltx_bibblock">Sequence parallelism: Making 4d parallelism possible.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.13120</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Abbeel (2023a)</span>
<span class="ltx_bibblock">
Hao Liu and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Emergent agentic transformer from chain of hindsight experience.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>,
2023a.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Abbeel (2023b)</span>
<span class="ltx_bibblock">
Hao Liu and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Blockwise parallel transformer for large context models.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
2023b.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milakov and Gimelshein (2018)</span>
<span class="ltx_bibblock">
Maxim Milakov and Natalia Gimelshein.

</span>
<span class="ltx_bibblock">Online normalizer calculation for softmax.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.02867</em>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MosaicML (2023)</span>
<span class="ltx_bibblock">
MosaicML.

</span>
<span class="ltx_bibblock">Introducing mpt-7b: A new standard for open-source, commercially
usable llms, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.mosaicml.com/blog/mpt-7b" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mosaicml.com/blog/mpt-7b</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narang et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Sharan Narang, Hyung&nbsp;Won Chung, Yi&nbsp;Tay, William Fedus, Thibault Fevry, Michael
Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et&nbsp;al.

</span>
<span class="ltx_bibblock">Do transformer modifications transfer across implementations and
applications?

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.11972</em>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil&nbsp;R
Devanur, Gregory&nbsp;R Ganger, Phillip&nbsp;B Gibbons, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Pipedream: Generalized pipeline parallelism for dnn training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM Symposium on Operating Systems
Principles</em>, pages 1–15, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et&nbsp;al. (2021)</span>
<span class="ltx_bibblock">
Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.

</span>
<span class="ltx_bibblock">Memory-efficient pipeline-parallel dnn training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
7937–7947. PMLR, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rabe and Staats (2021)</span>
<span class="ltx_bibblock">
Markus&nbsp;N Rabe and Charles Staats.

</span>
<span class="ltx_bibblock">Self-attention does not need o(n2) memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.05682</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et&nbsp;al. (2020)</span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">Zero: Memory optimizations toward training trillion parameter models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis</em>, pages 1–16. IEEE, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
J.&nbsp;Schulman, B.&nbsp;Zoph, C.&nbsp;Kim, J.&nbsp;Hilton, J.&nbsp;Menick, J.&nbsp;Weng, J.&nbsp;F.&nbsp;C. Uribe,
L.&nbsp;Fedus, L.&nbsp;Metz, M.&nbsp;Pokorny, R.&nbsp;G. Lopes, S.&nbsp;Zhao, A.&nbsp;Vijayvergiya,
E.&nbsp;Sigler, A.&nbsp;Perelman, C.&nbsp;Voss, M.&nbsp;Heaton, J.&nbsp;Parish, D.&nbsp;Cummings, R.&nbsp;Nayak,
V.&nbsp;Balcom, D.&nbsp;Schnurr, T.&nbsp;Kaftan, C.&nbsp;Hallacy, N.&nbsp;Turley, N.&nbsp;Deutsch, and
V.&nbsp;Goel.

</span>
<span class="ltx_bibblock">Chatgpt: Optimizing language models for dialogue.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sergeev and Del&nbsp;Balso (2018)</span>
<span class="ltx_bibblock">
Alexander Sergeev and Mike Del&nbsp;Balso.

</span>
<span class="ltx_bibblock">Horovod: fast and easy distributed deep learning in tensorflow.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.05799</em>, 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et&nbsp;al. (2019)</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-lm: Training multi-billion parameter language models using
model parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Yi&nbsp;Tay, Mostafa Dehghani, Samira Abnar, Hyung&nbsp;Won Chung, William Fedus, Jinfeng
Rao, Sharan Narang, Vinh&nbsp;Q Tran, Dani Yogatama, and Donald Metzler.

</span>
<span class="ltx_bibblock">Scaling laws vs model architectures: How does inductive bias
influence scaling?

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.10551</em>, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">team (2023)</span>
<span class="ltx_bibblock">
Jax team.

</span>
<span class="ltx_bibblock">Jax pallas fused attention, 2023.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/google/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et&nbsp;al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et&nbsp;al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake
Hechtman, Dehao Chen, Karthik&nbsp;Srinivasa Murthy, Marcello Maggioni, Qiao
Zhang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Overlap communication with dependent computation via decomposition in
large deep learning models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on
Architectural Support for Programming Languages and Operating Systems, Volume
1</em>, pages 93–106, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yarats et&nbsp;al. (2022)</span>
<span class="ltx_bibblock">
Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel,
Alessandro Lazaric, and Lerrel Pinto.

</span>
<span class="ltx_bibblock">Don’t change the algorithm, change the data: Exploratory data for
offline reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.13425</em>, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Code</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p" id="A1.p1.1">Jax에서 Ring Attention의 구현은 그림 <a class="ltx_ref" href="#A1.F4" title="Figure 4 ‣ Appendix A Code ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">4</span></a>에 나와 있다. <span class="ltx_text ltx_font_typewriter" id="A1.p1.1.1">defvjp</span> 함수를 사용하여 순방향 및 역방향 패스를 모두 정의하고, 집합 연산 <span class="ltx_text ltx_font_typewriter" id="A1.p1.1.2">jax.lax.ppermute</span>을 사용하여 호스트의 링 간에 키 값 블록의 교환을 용이하게 합니다. 제공된 코드 조각은 링 어텐션의 필수 구성 요소를 강조 표시합니다. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/lhao499/llm_large_context" target="_blank" title="">https://github.com/lhao499/llm_large_context</a>에서 Ring Attention의 전체 코드를 제공합니다. 최대 MFU에 대해, Ring Attention은 Triton/CUDA/Pallas 기반 코드<cite class="ltx_cite ltx_citemacro_citep">[kernel team, <a class="ltx_ref" href="#bib.bib18" title="">2023</a>, Dao et al., <a class="ltx_ref" href="#bib.bib9" title="">2022</a>, team, <a class="ltx_ref" href="#bib.bib37" title="">2023</a>]</cite>와 통합될 수 있는 것과 같이, Ring Attention은 기존 커널 수준의 융합-attention 구현과 통합될 수 있다는 것을 언급할 가치가 있다.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.p2.1.1">Practitioner Guide. </span> TPU 또는 높은 대역폭 인터 연결이 있는 GPU 클러스터에서 대규모 종단 간 훈련을 위해 FSDP를 사용하여 대규모 모델을 샤드하고 링 어텐션을 사용하여 대규모 컨텍스트를 달성하는 것이 좋습니다. 총 배치 크기가 너무 크면 텐서 병렬 처리를 추가하여 전체 배치 크기를 줄입니다. 코드베이스 내에서 <span class="ltx_text ltx_font_typewriter" id="A1.p2.1.2">mesh_dim</span> 매개 변수를 사용하여 병렬 처리 정도를 조정할 수 있습니다. 예를 들어 512x A100과 같은 512 개의 장치를 사용 하는 설정을 고려 합니다. 모델 크기가 30B 인 경우 8 개의 장치에서 분할 하 고 나머지 32 개의 장치를 링 어텐션에 할당할 수 있습니다. 이 설정을 사용하면 링 어텐션을 사용하지 않은 경우보다 컨텍스트 크기를 32배 더 확장할 수 있습니다. 반대로 7B 또는 3B 크기의 모델의 경우 FSDP가 필요하지 않다. 이는 512개의 모든 장치를 독점적으로 활용하여 링 어텐션을 사용하여 컨텍스트를 512배 확장할 수 있음을 의미합니다. 8x A100 GPU를 사용할 때 256K 컨텍스트 크기를 허용하는 결과를 기반으로 512개의 A100 GPU를 사용하여 잠재적인 컨텍스트 크기를 1,600만 개로 확장할 수 있음을 제안한다.</p>
</div>
<figure id="A1.F4" class="ltx_figure">
<div id="A1.F4.1" class="ltx_listing ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,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" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span id="lstnumberx2.1" class="ltx_text ltx_lst_identifier">def</span><span id="lstnumberx2.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.3" class="ltx_text ltx_lst_identifier">_ring_attention_fwd</span>(<span id="lstnumberx2.4" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx2.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.6" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx2.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.8" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx2.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.10" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx2.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.12" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx2.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.14" class="ltx_text ltx_lst_identifier">float32_logits</span>,<span id="lstnumberx2.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.16" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>):
</div>
<div id="lstnumberx3" class="ltx_listingline">
<span id="lstnumberx3.1" class="ltx_text ltx_lst_identifier">if</span><span id="lstnumberx3.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx3.3" class="ltx_text ltx_lst_identifier">float32_logits</span>:
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span id="lstnumberx4.1" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx4.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.3" class="ltx_text ltx_lst_identifier">k</span><span id="lstnumberx4.4" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx4.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.6" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx4.7" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx4.8" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx4.9" class="ltx_text ltx_lst_identifier">float32</span>),<span id="lstnumberx4.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.11" class="ltx_text ltx_lst_identifier">k</span>.<span id="lstnumberx4.12" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx4.13" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx4.14" class="ltx_text ltx_lst_identifier">float32</span>)
</div>
<div id="lstnumberx5" class="ltx_listingline">
<span id="lstnumberx5.1" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx5.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx5.3" class="ltx_text ltx_lst_identifier">q_len</span>,<span id="lstnumberx5.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx5.5" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx5.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx5.7" class="ltx_text ltx_lst_identifier">dim_per_head</span><span id="lstnumberx5.8" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx5.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx5.10" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx5.11" class="ltx_text ltx_lst_identifier">shape</span>
</div>
<div id="lstnumberx6" class="ltx_listingline">
<span id="lstnumberx6.1" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx6.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.3" class="ltx_text ltx_lst_identifier">kv_len</span>,<span id="lstnumberx6.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.5" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx6.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.7" class="ltx_text ltx_lst_identifier">dim_per_head</span><span id="lstnumberx6.8" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx6.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx6.10" class="ltx_text ltx_lst_identifier">k</span>.<span id="lstnumberx6.11" class="ltx_text ltx_lst_identifier">shape</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span id="lstnumberx7.1" class="ltx_text ltx_lst_identifier">numerator</span><span id="lstnumberx7.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx7.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx7.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx7.5" class="ltx_text ltx_lst_identifier">zeros</span>((<span id="lstnumberx7.6" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx7.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx7.8" class="ltx_text ltx_lst_identifier">q_len</span>,<span id="lstnumberx7.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx7.10" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx7.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx7.12" class="ltx_text ltx_lst_identifier">dim_per_head</span>)).<span id="lstnumberx7.13" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx7.14" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx7.15" class="ltx_text ltx_lst_identifier">dtype</span>)
</div>
<div id="lstnumberx8" class="ltx_listingline">
<span id="lstnumberx8.1" class="ltx_text ltx_lst_identifier">denominator</span><span id="lstnumberx8.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx8.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx8.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx8.5" class="ltx_text ltx_lst_identifier">zeros</span>((<span id="lstnumberx8.6" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx8.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx8.8" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx8.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx8.10" class="ltx_text ltx_lst_identifier">q_len</span>)).<span id="lstnumberx8.11" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx8.12" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx8.13" class="ltx_text ltx_lst_identifier">dtype</span>)
</div>
<div id="lstnumberx9" class="ltx_listingline">
<span id="lstnumberx9.1" class="ltx_text ltx_lst_identifier">axis_size</span><span id="lstnumberx9.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx9.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx9.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx9.5" class="ltx_text ltx_lst_identifier">psum</span>(1,<span id="lstnumberx9.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx9.7" class="ltx_text ltx_lst_identifier">axis_name</span>)
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span id="lstnumberx10.1" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx10.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx10.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.4" class="ltx_text ltx_lst_identifier">q_len</span><span id="lstnumberx10.5" class="ltx_text ltx_lst_space"> </span>#<span id="lstnumberx10.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.7" class="ltx_text ltx_lst_identifier">assumes</span><span id="lstnumberx10.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.9" class="ltx_text ltx_lst_identifier">this</span><span id="lstnumberx10.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.11" class="ltx_text ltx_lst_identifier">function</span><span id="lstnumberx10.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.13" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx10.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.15" class="ltx_text ltx_lst_identifier">pre</span>-<span id="lstnumberx10.16" class="ltx_text ltx_lst_identifier">sharded</span><span id="lstnumberx10.17" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.18" class="ltx_text ltx_lst_identifier">inside</span><span id="lstnumberx10.19" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx10.20" class="ltx_text ltx_lst_identifier">shard_map</span>
</div>
<div id="lstnumberx11" class="ltx_listingline">
<span id="lstnumberx11.1" class="ltx_text ltx_lst_identifier">query_chunk_size</span><span id="lstnumberx11.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx11.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx11.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>["<span id="lstnumberx11.5" class="ltx_text ltx_lst_identifier">query_chunk_size</span>"]
</div>
<div id="lstnumberx12" class="ltx_listingline">
<span id="lstnumberx12.1" class="ltx_text ltx_lst_identifier">key_chunk_size</span><span id="lstnumberx12.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx12.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx12.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>["<span id="lstnumberx12.5" class="ltx_text ltx_lst_identifier">key_chunk_size</span>"]
</div>
<div id="lstnumberx13" class="ltx_listingline">
<span id="lstnumberx13.1" class="ltx_text ltx_lst_identifier">def</span><span id="lstnumberx13.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx13.3" class="ltx_text ltx_lst_identifier">scan_kv_block</span>(<span id="lstnumberx13.4" class="ltx_text ltx_lst_identifier">carry</span>,<span id="lstnumberx13.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx13.6" class="ltx_text ltx_lst_identifier">idx</span>):
</div>
<div id="lstnumberx14" class="ltx_listingline">
<span id="lstnumberx14.1" class="ltx_text ltx_lst_identifier">prev_max_score</span>,<span id="lstnumberx14.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.3" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx14.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.5" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx14.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.7" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx14.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.9" class="ltx_text ltx_lst_identifier">v</span><span id="lstnumberx14.10" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx14.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx14.12" class="ltx_text ltx_lst_identifier">carry</span>
</div>
<div id="lstnumberx15" class="ltx_listingline">
<span id="lstnumberx15.1" class="ltx_text ltx_lst_identifier">attn_bias_slice</span><span id="lstnumberx15.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx15.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx15.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx15.5" class="ltx_text ltx_lst_identifier">dynamic_slice_in_dim</span>(<span id="lstnumberx15.6" class="ltx_text ltx_lst_identifier">attn_bias</span>,
</div>
<div id="lstnumberx16" class="ltx_listingline">(<span id="lstnumberx16.1" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx16.2" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx16.3" class="ltx_text ltx_lst_identifier">axis_name</span>)<span id="lstnumberx16.4" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx16.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx16.6" class="ltx_text ltx_lst_identifier">idx</span>)<span id="lstnumberx16.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx16.8" class="ltx_text ltx_lst_identifier">q_block_idx</span><span id="lstnumberx16.9" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx16.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx16.11" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx16.12" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx16.13" class="ltx_text ltx_lst_identifier">axis_name</span>)
</div>
<div id="lstnumberx17" class="ltx_listingline">
<span id="lstnumberx17.1" class="ltx_text ltx_lst_identifier">k_block_idx</span><span id="lstnumberx17.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx17.3" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx17.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx17.5" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx17.6" class="ltx_text ltx_lst_identifier">axis_name</span>)<span id="lstnumberx17.7" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx17.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx17.9" class="ltx_text ltx_lst_identifier">idx</span>)<span id="lstnumberx17.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx17.11" class="ltx_text ltx_lst_identifier">q_chunk_idx_start</span><span id="lstnumberx17.12" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx17.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx17.14" class="ltx_text ltx_lst_identifier">q_block_idx</span><span id="lstnumberx17.15" class="ltx_text ltx_lst_space"> </span>*<span id="lstnumberx17.16" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx17.17" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx17.18" class="ltx_text ltx_lst_space"> </span>//<span id="lstnumberx17.19" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx17.20" class="ltx_text ltx_lst_identifier">query_chunk_size</span>)
</div>
<div id="lstnumberx18" class="ltx_listingline">
<span id="lstnumberx18.1" class="ltx_text ltx_lst_identifier">k_chunk_idx_start</span><span id="lstnumberx18.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx18.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx18.4" class="ltx_text ltx_lst_identifier">k_block_idx</span><span id="lstnumberx18.5" class="ltx_text ltx_lst_space"> </span>*<span id="lstnumberx18.6" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx18.7" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx18.8" class="ltx_text ltx_lst_space"> </span>//<span id="lstnumberx18.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx18.10" class="ltx_text ltx_lst_identifier">key_chunk_size</span>)
</div>
<div id="lstnumberx19" class="ltx_listingline">
<span id="lstnumberx19.1" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx19.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.3" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx19.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.5" class="ltx_text ltx_lst_identifier">max_score</span><span id="lstnumberx19.6" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx19.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.8" class="ltx_text ltx_lst_identifier">_blockwise_attention_fwd</span>(<span id="lstnumberx19.9" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx19.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.11" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx19.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx19.13" class="ltx_text ltx_lst_identifier">v</span>,
</div>
<div id="lstnumberx20" class="ltx_listingline">(<span id="lstnumberx20.1" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx20.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.3" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx20.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.5" class="ltx_text ltx_lst_identifier">prev_max_score</span>),<span id="lstnumberx20.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.7" class="ltx_text ltx_lst_identifier">q_chunk_idx_start</span>,<span id="lstnumberx20.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.9" class="ltx_text ltx_lst_identifier">k_chunk_idx_start</span>,
</div>
<div id="lstnumberx21" class="ltx_listingline">
<span id="lstnumberx21.1" class="ltx_text ltx_lst_identifier">bias</span>=<span id="lstnumberx21.2" class="ltx_text ltx_lst_identifier">attn_bias_slice</span>,<span id="lstnumberx21.3" class="ltx_text ltx_lst_space"> </span>**<span id="lstnumberx21.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>)
</div>
<div id="lstnumberx22" class="ltx_listingline">
<span id="lstnumberx22.1" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx22.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.3" class="ltx_text ltx_lst_identifier">v</span><span id="lstnumberx22.4" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx22.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.6" class="ltx_text ltx_lst_identifier">map</span>(<span id="lstnumberx22.7" class="ltx_text ltx_lst_identifier">lambda</span><span id="lstnumberx22.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.9" class="ltx_text ltx_lst_identifier">x</span>:<span id="lstnumberx22.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.11" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx22.12" class="ltx_text ltx_lst_identifier">ppermute</span>(<span id="lstnumberx22.13" class="ltx_text ltx_lst_identifier">x</span>,<span id="lstnumberx22.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.15" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx22.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.17" class="ltx_text ltx_lst_identifier">perm</span>=[(<span id="lstnumberx22.18" class="ltx_text ltx_lst_identifier">i</span>,<span id="lstnumberx22.19" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx22.20" class="ltx_text ltx_lst_identifier">i</span><span id="lstnumberx22.21" class="ltx_text ltx_lst_space"> </span>+<span id="lstnumberx22.22" class="ltx_text ltx_lst_space"> </span>1)<span id="lstnumberx22.23" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.24" class="ltx_text ltx_lst_identifier">for</span><span id="lstnumberx22.25" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.26" class="ltx_text ltx_lst_identifier">i</span><span id="lstnumberx22.27" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.28" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx22.29" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.30" class="ltx_text ltx_lst_identifier">range</span>(<span id="lstnumberx22.31" class="ltx_text ltx_lst_identifier">axis_size</span>)]),<span id="lstnumberx22.32" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx22.33" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx22.34" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.35" class="ltx_text ltx_lst_identifier">v</span>))
</div>
<div id="lstnumberx23" class="ltx_listingline">
<span id="lstnumberx23.1" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx23.2" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx23.3" class="ltx_text ltx_lst_identifier">max_score</span>,<span id="lstnumberx23.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.5" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx23.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.7" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx23.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.9" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx23.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.11" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx23.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx23.13" class="ltx_text ltx_lst_identifier">None</span>
</div>
<div id="lstnumberx24" class="ltx_listingline">
<span id="lstnumberx24.1" class="ltx_text ltx_lst_identifier">prev_max_score</span><span id="lstnumberx24.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx24.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx24.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx24.5" class="ltx_text ltx_lst_identifier">full</span>((<span id="lstnumberx24.6" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx24.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx24.8" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx24.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx24.10" class="ltx_text ltx_lst_identifier">q_len</span>),<span id="lstnumberx24.11" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx24.12" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx24.13" class="ltx_text ltx_lst_identifier">inf</span>).<span id="lstnumberx24.14" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx24.15" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx24.16" class="ltx_text ltx_lst_identifier">dtype</span>)
</div>
<div id="lstnumberx25" class="ltx_listingline">(<span id="lstnumberx25.1" class="ltx_text ltx_lst_identifier">max_score</span>,<span id="lstnumberx25.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.3" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx25.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.5" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx25.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.7" class="ltx_text ltx_lst_identifier">_</span>,<span id="lstnumberx25.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.9" class="ltx_text ltx_lst_identifier">_</span>),<span id="lstnumberx25.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.11" class="ltx_text ltx_lst_identifier">_</span><span id="lstnumberx25.12" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx25.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.14" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx25.15" class="ltx_text ltx_lst_identifier">scan</span>(<span id="lstnumberx25.16" class="ltx_text ltx_lst_identifier">scan_kv_block</span>,
</div>
<div id="lstnumberx26" class="ltx_listingline">
<span id="lstnumberx26.1" class="ltx_text ltx_lst_identifier">init</span>=(<span id="lstnumberx26.2" class="ltx_text ltx_lst_identifier">prev_max_score</span>,<span id="lstnumberx26.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.4" class="ltx_text ltx_lst_identifier">numerator</span>,<span id="lstnumberx26.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.6" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx26.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.8" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx26.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.10" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx26.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.12" class="ltx_text ltx_lst_identifier">xs</span>=<span id="lstnumberx26.13" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx26.14" class="ltx_text ltx_lst_identifier">arange</span>(0,<span id="lstnumberx26.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx26.16" class="ltx_text ltx_lst_identifier">axis_size</span>))
</div>
<div id="lstnumberx27" class="ltx_listingline">
<span id="lstnumberx27.1" class="ltx_text ltx_lst_identifier">output</span><span id="lstnumberx27.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx27.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.4" class="ltx_text ltx_lst_identifier">numerator</span><span id="lstnumberx27.5" class="ltx_text ltx_lst_space"> </span>/<span id="lstnumberx27.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.7" class="ltx_text ltx_lst_identifier">rearrange</span>(<span id="lstnumberx27.8" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx27.9" class="ltx_text ltx_lst_space"> </span>’<span id="lstnumberx27.10" class="ltx_text ltx_lst_identifier">b</span><span id="lstnumberx27.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.12" class="ltx_text ltx_lst_identifier">h</span><span id="lstnumberx27.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.14" class="ltx_text ltx_lst_identifier">q</span><span id="lstnumberx27.15" class="ltx_text ltx_lst_space"> </span>-&gt;<span id="lstnumberx27.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.17" class="ltx_text ltx_lst_identifier">b</span><span id="lstnumberx27.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.19" class="ltx_text ltx_lst_identifier">q</span><span id="lstnumberx27.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.21" class="ltx_text ltx_lst_identifier">h</span>’)[…,<span id="lstnumberx27.22" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.23" class="ltx_text ltx_lst_identifier">None</span>]
</div>
<div id="lstnumberx28" class="ltx_listingline">
<span id="lstnumberx28.1" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx28.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.3" class="ltx_text ltx_lst_identifier">output</span>.<span id="lstnumberx28.4" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx28.5" class="ltx_text ltx_lst_identifier">v</span>.<span id="lstnumberx28.6" class="ltx_text ltx_lst_identifier">dtype</span>),<span id="lstnumberx28.7" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx28.8" class="ltx_text ltx_lst_identifier">output</span>,<span id="lstnumberx28.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.10" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx28.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.12" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx28.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.14" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx28.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.16" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx28.17" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.18" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx28.19" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.20" class="ltx_text ltx_lst_identifier">max_score</span>)
</div>
<div id="lstnumberx29" class="ltx_listingline">\<span id="lstnumberx29.1" class="ltx_text ltx_lst_identifier">pardef</span><span id="lstnumberx29.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.3" class="ltx_text ltx_lst_identifier">_ring_attention_bwd</span>(<span id="lstnumberx29.4" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx29.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.6" class="ltx_text ltx_lst_identifier">float32_logits</span>,<span id="lstnumberx29.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.8" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>,<span id="lstnumberx29.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.10" class="ltx_text ltx_lst_identifier">res</span>,<span id="lstnumberx29.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.12" class="ltx_text ltx_lst_identifier">g</span>):
</div>
<div id="lstnumberx30" class="ltx_listingline">
<span id="lstnumberx30.1" class="ltx_text ltx_lst_identifier">output</span>,<span id="lstnumberx30.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.3" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx30.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.5" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx30.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.7" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx30.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.9" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx30.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.11" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx30.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.13" class="ltx_text ltx_lst_identifier">max_score</span><span id="lstnumberx30.14" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx30.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.16" class="ltx_text ltx_lst_identifier">res</span>
</div>
<div id="lstnumberx31" class="ltx_listingline">
<span id="lstnumberx31.1" class="ltx_text ltx_lst_identifier">batch</span>,<span id="lstnumberx31.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx31.3" class="ltx_text ltx_lst_identifier">kv_len</span>,<span id="lstnumberx31.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx31.5" class="ltx_text ltx_lst_identifier">num_heads</span>,<span id="lstnumberx31.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx31.7" class="ltx_text ltx_lst_identifier">dim_per_head</span><span id="lstnumberx31.8" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx31.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx31.10" class="ltx_text ltx_lst_identifier">k</span>.<span id="lstnumberx31.11" class="ltx_text ltx_lst_identifier">shape</span>
</div>
<div id="lstnumberx32" class="ltx_listingline">
<span id="lstnumberx32.1" class="ltx_text ltx_lst_identifier">axis_size</span><span id="lstnumberx32.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx32.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx32.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx32.5" class="ltx_text ltx_lst_identifier">psum</span>(1,<span id="lstnumberx32.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx32.7" class="ltx_text ltx_lst_identifier">axis_name</span>)
</div>
<div id="lstnumberx33" class="ltx_listingline">
<span id="lstnumberx33.1" class="ltx_text ltx_lst_identifier">dq</span><span id="lstnumberx33.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx33.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx33.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx33.5" class="ltx_text ltx_lst_identifier">zeros_like</span>(<span id="lstnumberx33.6" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx33.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx33.8" class="ltx_text ltx_lst_identifier">dtype</span>=<span id="lstnumberx33.9" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx33.10" class="ltx_text ltx_lst_identifier">float32</span>)
</div>
<div id="lstnumberx34" class="ltx_listingline">
<span id="lstnumberx34.1" class="ltx_text ltx_lst_identifier">dk</span><span id="lstnumberx34.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx34.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx34.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx34.5" class="ltx_text ltx_lst_identifier">zeros_like</span>(<span id="lstnumberx34.6" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx34.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx34.8" class="ltx_text ltx_lst_identifier">dtype</span>=<span id="lstnumberx34.9" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx34.10" class="ltx_text ltx_lst_identifier">float32</span>)
</div>
<div id="lstnumberx35" class="ltx_listingline">
<span id="lstnumberx35.1" class="ltx_text ltx_lst_identifier">dv</span><span id="lstnumberx35.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx35.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx35.4" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx35.5" class="ltx_text ltx_lst_identifier">zeros_like</span>(<span id="lstnumberx35.6" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx35.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx35.8" class="ltx_text ltx_lst_identifier">dtype</span>=<span id="lstnumberx35.9" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx35.10" class="ltx_text ltx_lst_identifier">float32</span>)
</div>
<div id="lstnumberx36" class="ltx_listingline">
<span id="lstnumberx36.1" class="ltx_text ltx_lst_identifier">query_chunk_size</span><span id="lstnumberx36.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx36.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx36.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>["<span id="lstnumberx36.5" class="ltx_text ltx_lst_identifier">query_chunk_size</span>"]
</div>
<div id="lstnumberx37" class="ltx_listingline">
<span id="lstnumberx37.1" class="ltx_text ltx_lst_identifier">key_chunk_size</span><span id="lstnumberx37.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx37.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx37.4" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>["<span id="lstnumberx37.5" class="ltx_text ltx_lst_identifier">key_chunk_size</span>"]
</div>
<div id="lstnumberx38" class="ltx_listingline">
<span id="lstnumberx38.1" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx38.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx38.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.4" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx38.5" class="ltx_text ltx_lst_identifier">shape</span>[1]<span id="lstnumberx38.6" class="ltx_text ltx_lst_space"> </span>#<span id="lstnumberx38.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.8" class="ltx_text ltx_lst_identifier">assumes</span><span id="lstnumberx38.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.10" class="ltx_text ltx_lst_identifier">this</span><span id="lstnumberx38.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.12" class="ltx_text ltx_lst_identifier">function</span><span id="lstnumberx38.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.14" class="ltx_text ltx_lst_identifier">is</span><span id="lstnumberx38.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.16" class="ltx_text ltx_lst_identifier">pre</span>-<span id="lstnumberx38.17" class="ltx_text ltx_lst_identifier">sharded</span><span id="lstnumberx38.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.19" class="ltx_text ltx_lst_identifier">inside</span><span id="lstnumberx38.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx38.21" class="ltx_text ltx_lst_identifier">shard_map</span>
</div>
<div id="lstnumberx39" class="ltx_listingline">
<span id="lstnumberx39.1" class="ltx_text ltx_lst_identifier">def</span><span id="lstnumberx39.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx39.3" class="ltx_text ltx_lst_identifier">scan_kv_block</span>(<span id="lstnumberx39.4" class="ltx_text ltx_lst_identifier">carry</span>,<span id="lstnumberx39.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx39.6" class="ltx_text ltx_lst_identifier">idx</span>):
</div>
<div id="lstnumberx40" class="ltx_listingline">
<span id="lstnumberx40.1" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx40.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.3" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx40.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.5" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx40.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.7" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx40.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.9" class="ltx_text ltx_lst_identifier">v</span><span id="lstnumberx40.10" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx40.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.12" class="ltx_text ltx_lst_identifier">carry</span>
</div>
<div id="lstnumberx41" class="ltx_listingline">
<span id="lstnumberx41.1" class="ltx_text ltx_lst_identifier">attn_bias_slice</span><span id="lstnumberx41.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx41.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx41.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx41.5" class="ltx_text ltx_lst_identifier">dynamic_slice_in_dim</span>(<span id="lstnumberx41.6" class="ltx_text ltx_lst_identifier">attn_bias</span>,
</div>
<div id="lstnumberx42" class="ltx_listingline">(<span id="lstnumberx42.1" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx42.2" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx42.3" class="ltx_text ltx_lst_identifier">axis_name</span>)<span id="lstnumberx42.4" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx42.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.6" class="ltx_text ltx_lst_identifier">idx</span>)<span id="lstnumberx42.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.8" class="ltx_text ltx_lst_identifier">q_block_idx</span><span id="lstnumberx42.9" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx42.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.11" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx42.12" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx42.13" class="ltx_text ltx_lst_identifier">axis_name</span>)
</div>
<div id="lstnumberx43" class="ltx_listingline">
<span id="lstnumberx43.1" class="ltx_text ltx_lst_identifier">k_block_idx</span><span id="lstnumberx43.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx43.3" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx43.4" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx43.5" class="ltx_text ltx_lst_identifier">axis_index</span>(<span id="lstnumberx43.6" class="ltx_text ltx_lst_identifier">axis_name</span>)<span id="lstnumberx43.7" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx43.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx43.9" class="ltx_text ltx_lst_identifier">idx</span>)<span id="lstnumberx43.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx43.11" class="ltx_text ltx_lst_identifier">q_chunk_idx_start</span><span id="lstnumberx43.12" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx43.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx43.14" class="ltx_text ltx_lst_identifier">q_block_idx</span><span id="lstnumberx43.15" class="ltx_text ltx_lst_space"> </span>*<span id="lstnumberx43.16" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx43.17" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx43.18" class="ltx_text ltx_lst_space"> </span>//<span id="lstnumberx43.19" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx43.20" class="ltx_text ltx_lst_identifier">query_chunk_size</span>)
</div>
<div id="lstnumberx44" class="ltx_listingline">
<span id="lstnumberx44.1" class="ltx_text ltx_lst_identifier">k_chunk_idx_start</span><span id="lstnumberx44.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx44.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx44.4" class="ltx_text ltx_lst_identifier">k_block_idx</span><span id="lstnumberx44.5" class="ltx_text ltx_lst_space"> </span>*<span id="lstnumberx44.6" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx44.7" class="ltx_text ltx_lst_identifier">block_size</span><span id="lstnumberx44.8" class="ltx_text ltx_lst_space"> </span>//<span id="lstnumberx44.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx44.10" class="ltx_text ltx_lst_identifier">key_chunk_size</span>)
</div>
<div id="lstnumberx45" class="ltx_listingline">
<span id="lstnumberx45.1" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx45.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.3" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx45.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.5" class="ltx_text ltx_lst_identifier">dv</span><span id="lstnumberx45.6" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx45.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.8" class="ltx_text ltx_lst_identifier">_blockwise_attention_bwd</span>(<span id="lstnumberx45.9" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx45.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.11" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx45.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.13" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx45.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.15" class="ltx_text ltx_lst_identifier">g</span>,<span id="lstnumberx45.16" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx45.17" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx45.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.19" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx45.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.21" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx45.22" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.23" class="ltx_text ltx_lst_identifier">output</span>,<span id="lstnumberx45.24" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.25" class="ltx_text ltx_lst_identifier">denominator</span>,<span id="lstnumberx45.26" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx45.27" class="ltx_text ltx_lst_identifier">max_score</span>),
</div>
<div id="lstnumberx46" class="ltx_listingline">
<span id="lstnumberx46.1" class="ltx_text ltx_lst_identifier">q_chunk_idx_start</span>,<span id="lstnumberx46.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx46.3" class="ltx_text ltx_lst_identifier">k_chunk_idx_start</span>,<span id="lstnumberx46.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx46.5" class="ltx_text ltx_lst_identifier">bias</span>=<span id="lstnumberx46.6" class="ltx_text ltx_lst_identifier">attn_bias_slice</span>,<span id="lstnumberx46.7" class="ltx_text ltx_lst_space"> </span>**<span id="lstnumberx46.8" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>)
</div>
<div id="lstnumberx47" class="ltx_listingline">
<span id="lstnumberx47.1" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx47.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.3" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx47.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.5" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx47.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.7" class="ltx_text ltx_lst_identifier">dv</span><span id="lstnumberx47.8" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx47.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.10" class="ltx_text ltx_lst_identifier">map</span>(<span id="lstnumberx47.11" class="ltx_text ltx_lst_identifier">lambda</span><span id="lstnumberx47.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.13" class="ltx_text ltx_lst_identifier">x</span>:<span id="lstnumberx47.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.15" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx47.16" class="ltx_text ltx_lst_identifier">ppermute</span>(<span id="lstnumberx47.17" class="ltx_text ltx_lst_identifier">x</span>,<span id="lstnumberx47.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.19" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx47.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx47.21" class="ltx_text ltx_lst_identifier">perm</span>=[(<span id="lstnumberx47.22" class="ltx_text ltx_lst_identifier">i</span>,
</div>
<div id="lstnumberx48" class="ltx_listingline">(<span id="lstnumberx48.1" class="ltx_text ltx_lst_identifier">i</span><span id="lstnumberx48.2" class="ltx_text ltx_lst_space"> </span>+<span id="lstnumberx48.3" class="ltx_text ltx_lst_space"> </span>1)<span id="lstnumberx48.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.5" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx48.6" class="ltx_text ltx_lst_space"> </span>(<span id="lstnumberx48.7" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx48.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.9" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx48.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.11" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx48.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.13" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx48.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.15" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx48.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.17" class="ltx_text ltx_lst_identifier">None</span>
</div>
<div id="lstnumberx49" class="ltx_listingline">(<span id="lstnumberx49.1" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx49.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.3" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx49.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.5" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx49.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.7" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx49.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.9" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx49.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.11" class="ltx_text ltx_lst_identifier">_</span><span id="lstnumberx49.12" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx49.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.14" class="ltx_text ltx_lst_identifier">lax</span>.<span id="lstnumberx49.15" class="ltx_text ltx_lst_identifier">scan</span>(<span id="lstnumberx49.16" class="ltx_text ltx_lst_identifier">scan_kv_block</span>,<span id="lstnumberx49.17" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.18" class="ltx_text ltx_lst_identifier">init</span>=(<span id="lstnumberx49.19" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx49.20" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.21" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx49.22" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.23" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx49.24" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.25" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx49.26" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.27" class="ltx_text ltx_lst_identifier">v</span>),<span id="lstnumberx49.28" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.29" class="ltx_text ltx_lst_identifier">xs</span>=<span id="lstnumberx49.30" class="ltx_text ltx_lst_identifier">jnp</span>.<span id="lstnumberx49.31" class="ltx_text ltx_lst_identifier">arange</span>(0,<span id="lstnumberx49.32" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx49.33" class="ltx_text ltx_lst_identifier">axis_size</span>))
</div>
<div id="lstnumberx50" class="ltx_listingline">
<span id="lstnumberx50.1" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx50.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.3" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx50.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.5" class="ltx_text ltx_lst_identifier">dv</span><span id="lstnumberx50.6" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx50.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.8" class="ltx_text ltx_lst_identifier">dq</span>.<span id="lstnumberx50.9" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx50.10" class="ltx_text ltx_lst_identifier">q</span>.<span id="lstnumberx50.11" class="ltx_text ltx_lst_identifier">dtype</span>),<span id="lstnumberx50.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.13" class="ltx_text ltx_lst_identifier">dk</span>.<span id="lstnumberx50.14" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx50.15" class="ltx_text ltx_lst_identifier">k</span>.<span id="lstnumberx50.16" class="ltx_text ltx_lst_identifier">dtype</span>),<span id="lstnumberx50.17" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx50.18" class="ltx_text ltx_lst_identifier">dv</span>.<span id="lstnumberx50.19" class="ltx_text ltx_lst_identifier">astype</span>(<span id="lstnumberx50.20" class="ltx_text ltx_lst_identifier">v</span>.<span id="lstnumberx50.21" class="ltx_text ltx_lst_identifier">dtype</span>)
</div>
<div id="lstnumberx51" class="ltx_listingline">
<span id="lstnumberx51.1" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx51.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx51.3" class="ltx_text ltx_lst_identifier">dq</span>,<span id="lstnumberx51.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx51.5" class="ltx_text ltx_lst_identifier">dk</span>,<span id="lstnumberx51.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx51.7" class="ltx_text ltx_lst_identifier">dv</span>,<span id="lstnumberx51.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx51.9" class="ltx_text ltx_lst_identifier">None</span>
</div>
<div id="lstnumberx52" class="ltx_listingline">\<span id="lstnumberx52.1" class="ltx_text ltx_lst_identifier">par@partial</span>(<span id="lstnumberx52.2" class="ltx_text ltx_lst_identifier">jax</span>.<span id="lstnumberx52.3" class="ltx_text ltx_lst_identifier">custom_vjp</span>,<span id="lstnumberx52.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx52.5" class="ltx_text ltx_lst_identifier">nondiff_argnums</span>=[4,<span id="lstnumberx52.6" class="ltx_text ltx_lst_space"> </span>5,<span id="lstnumberx52.7" class="ltx_text ltx_lst_space"> </span>6])
</div>
<div id="lstnumberx53" class="ltx_listingline">
<span id="lstnumberx53.1" class="ltx_text ltx_lst_identifier">def</span><span id="lstnumberx53.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.3" class="ltx_text ltx_lst_identifier">ring_attention</span>(<span id="lstnumberx53.4" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx53.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.6" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx53.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.8" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx53.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.10" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx53.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.12" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx53.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.14" class="ltx_text ltx_lst_identifier">float32_logits</span>,<span id="lstnumberx53.15" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx53.16" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>):
</div>
<div id="lstnumberx54" class="ltx_listingline">
<span id="lstnumberx54.1" class="ltx_text ltx_lst_identifier">y</span>,<span id="lstnumberx54.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.3" class="ltx_text ltx_lst_identifier">_</span><span id="lstnumberx54.4" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx54.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.6" class="ltx_text ltx_lst_identifier">_ring_attention_fwd</span>(<span id="lstnumberx54.7" class="ltx_text ltx_lst_identifier">q</span>,<span id="lstnumberx54.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.9" class="ltx_text ltx_lst_identifier">k</span>,<span id="lstnumberx54.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.11" class="ltx_text ltx_lst_identifier">v</span>,<span id="lstnumberx54.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.13" class="ltx_text ltx_lst_identifier">attn_bias</span>,<span id="lstnumberx54.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.15" class="ltx_text ltx_lst_identifier">axis_name</span>,<span id="lstnumberx54.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.17" class="ltx_text ltx_lst_identifier">float32_logits</span>,<span id="lstnumberx54.18" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx54.19" class="ltx_text ltx_lst_identifier">blockwise_kwargs</span>)
</div>
<div id="lstnumberx55" class="ltx_listingline">
<span id="lstnumberx55.1" class="ltx_text ltx_lst_identifier">return</span><span id="lstnumberx55.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx55.3" class="ltx_text ltx_lst_identifier">y</span>
</div>
<div id="lstnumberx56" class="ltx_listingline">\<span id="lstnumberx56.1" class="ltx_text ltx_lst_identifier">parring_attention</span>.<span id="lstnumberx56.2" class="ltx_text ltx_lst_identifier">defvjp</span>(<span id="lstnumberx56.3" class="ltx_text ltx_lst_identifier">_ring_attention_fwd</span>,<span id="lstnumberx56.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx56.5" class="ltx_text ltx_lst_identifier">_ring_attention_bwd</span>)
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4:</span>Key parts of the implementation of Ring Attention in Jax. 집합 연산 <span class="ltx_text ltx_font_typewriter" id="A1.F4.3.1">lax.ppermute</span>을 사용하여 이전 호스트와 다음 호스트 간에 키 값 블록을 주고 받습니다.</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Release Note</h2>

<div id="A2.p1" class="ltx_para">
<ol id="A2.I1" class="ltx_enumerate">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i1.p1.1.1">v1</span> (3rd October 2023): Release of Ring Attention.</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i2.p1.1.1">v2</span> (5th October 2023):</p>
<ul id="A2.I1.i2.I1" class="ltx_itemize">
<li id="A2.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i2.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i2.I1.i1.p1.1">몇 가지 오타를 수정했습니다.</p>
</div>
</li>
<li id="A2.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i2.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i2.I1.i2.p1.1">상호 연결 대역폭이 명확해졌습니다.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A2.I1.i3.p1.1.1">v3</span> (11th October 2023):</p>
<ul id="A2.I1.i3.I1" class="ltx_itemize">
<li id="A2.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.I1.i1.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i3.I1.i1.p1.1">링 어텐션의 코드를 해제했습니다.</p>
</div>
</li>
<li id="A2.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.I1.i2.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i3.I1.i2.p1.1">대규모 종단 간 FSDP 훈련 설정을 사용하도록 최대 컨텍스트 길이 실험을 업데이트했습니다.</p>
</div>
</li>
<li id="A2.I1.i3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.I1.i3.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i3.I1.i3.p1.1">대규모 종단 간 FSDP 훈련 설정을 사용하도록 MFU 평가 실험이 업데이트되었습니다.</p>
</div>
</li>
<li id="A2.I1.i3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.I1.i4.p1" class="ltx_para">
<p class="ltx_p" id="A2.I1.i3.I1.i4.p1.1">링 어텐션 사용에 대한 실무 가이드가 추가되었습니다.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Experiment Details</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Evaluation of context length</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS1.p1.1">섹션 <a class="ltx_ref" href="#S5.SS1" title="5.1 Evaluating Max Context Size ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5.1</span></a>에 제시된 실험 결과에서 우리는 GPU 또는 TPU 장치에 걸쳐 모델을 분할하기 위해 완전히 샤딩된 텐서 병렬 처리(FSDP)를 사용했다. 우리의 평가는 일반적으로 사용되는 FSDP 훈련 시나리오에서 최대 달성 가능한 시퀀스 길이를 결정하는 데 중점을 두었다. TPU의 경우 기본 학습 구성을 활용했으며, 이는 <span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.1.1">bfloat16</span> 형식에서 matmul 작업을 수행하는 것과 함께 <span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.1.2">float32</span>에서 가중치 축적이 있습니다. 반면에 GPU의 경우 기본 설정을 채택했으며 모든 작업은 <span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.1.3">float32</span>에서 수행되었다.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Evaluation of MFU</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS2.p1.1"><a class="ltx_ref" href="#S5.SS2" title="5.2 Evaluating Model Flops Utilization ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5.2</span></a>절에 제시된 평가에서. 토큰의 배치 크기는 GPU의 배치당 200만 개, TPU의 배치당 400만 개입니다. 훈련은 Jax SPMD와 함께 FSDP <cite class="ltx_cite ltx_citemacro_cite">Facebook [<a class="ltx_ref" href="#bib.bib11" title="">2023</a>]</cite>를 사용하여 수행되었다. 그래디언트 체크포인팅 <cite class="ltx_cite ltx_citemacro_citep">[Chen et al., <a class="ltx_ref" href="#bib.bib5" title="">2016</a>]</cite>의 경우, 주의 및 피드포워드 네트워크(FFN)를 위한 체크포인팅 정책으로서 <span class="ltx_text ltx_font_typewriter" id="A3.SS2.p1.1.1">nothing_saveable</span>을 사용하였다. 자세한 내용은 Jax 문서를 참조하십시오.</p>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Evaluation on line retrieval</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p class="ltx_p" id="A3.SS3.p1.1"><a class="ltx_ref" href="#S5.SS3" title="5.3 Impact on LLM Performance ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5.3</span></a> 섹션에 제시된 평가에서 클라우드 컴퓨팅 예산의 제약으로 인해 컨텍스트 길이를 512K 토큰으로 제한하는 LLaMA-13B 모델 <cite class="ltx_cite ltx_citemacro_citep">[Touvron et al., <a class="ltx_ref" href="#bib.bib38" title="">2023</a>]</cite>를 미세 조정했으며, 훈련은 32x A100 80GB 클라우드 GPU에서 수행되었다. Pinetuning을 위해 ShareGPT.com에서 수집한 사용자 공유 대화를 사용 하 여 이전 작업 <cite class="ltx_cite ltx_citemacro_citep">[Chiang et al., <a class="ltx_ref" href="#bib.bib6" title="">2023</a>, Geng et al., <a class="ltx_ref" href="#bib.bib13" title="">2023</a>]</cite>에 요약 된 방법론에 따라 수행 합니다. ShareGPT는 사용자들이 자신의 ChatGPT 대화를 공유할 수 있는 웹사이트이다. 데이터 품질을 보장하기 위해 HTML을 다시 마크다운으로 변환하고 일부 부적절하거나 품질이 낮은 샘플을 필터링하여 데이터 청소 후 125K 대화를 수행합니다.</p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Training FLOPs Scaling of Context Size</h2>

<div id="A4.p1" class="ltx_para">
<p class="ltx_p" id="A4.p1.8">제안된 접근 방식은 1억 토큰을 초과하는 컨텍스트 크기를 가진 훈련의 가능성을 열어주고 장치 수에 따라 컨텍스트 크기를 선형 스케일링할 수 있다는 점을 감안할 때 컨텍스트 크기를 가진 데이터 세트 규모당 훈련 FLOP의 방법을 이해하는 것이 필수적이다. 더 큰 컨텍스트 크기가 더 많은 수의 FLOP를 초래하지만, 증가된 비율은 토큰의 수가 고정된 채로 남아 있기 때문에 2차적으로 확장되지 않는다. 우리는 다른 계산 예산을 나타내는 다양한 모델 크기와 컨텍스트 길이를 보여주는 그림 <a class="ltx_ref" href="#A4.F5" title="Figure 5 ‣ Appendix D Training FLOPs Scaling of Context Size ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5</span></a>에 이러한 결과를 제시한다. 그림은 4K 컨텍스트 크기가 더 짧은 동일한 모델과 비교하여 더 큰 컨텍스트 길이에 대한 FLOP의 비율을 보여준다. <math alttext="(24bsh^{2}+4bs^{2}h)n" class="ltx_Math" display="inline" id="A4.p1.1.m1.1"><semantics id="A4.p1.1.m1.1a"><mrow id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml"><mrow id="A4.p1.1.m1.1.1.1.1" xref="A4.p1.1.m1.1.1.1.1.1.cmml"><mo id="A4.p1.1.m1.1.1.1.1.2" stretchy="false" xref="A4.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="A4.p1.1.m1.1.1.1.1.1" xref="A4.p1.1.m1.1.1.1.1.1.cmml"><mrow id="A4.p1.1.m1.1.1.1.1.1.2" xref="A4.p1.1.m1.1.1.1.1.1.2.cmml"><mn id="A4.p1.1.m1.1.1.1.1.1.2.2" xref="A4.p1.1.m1.1.1.1.1.1.2.2.cmml">24</mn><mo id="A4.p1.1.m1.1.1.1.1.1.2.1" lspace="0px" rspace="0px" xref="A4.p1.1.m1.1.1.1.1.1.2.1.cmml"></mo><mi id="A4.p1.1.m1.1.1.1.1.1.2.3" xref="A4.p1.1.m1.1.1.1.1.1.2.3.cmml">b</mi><mo id="A4.p1.1.m1.1.1.1.1.1.2.1a" lspace="0px" rspace="0px" xref="A4.p1.1.m1.1.1.1.1.1.2.1.cmml"></mo><mi id="A4.p1.1.m1.1.1.1.1.1.2.4" xref="A4.p1.1.m1.1.1.1.1.1.2.4.cmml">s</mi><mo id="A4.p1.1.m1.1.1.1.1.1.2.1b" lspace="0px" rspace="0px" xref="A4.p1.1.m1.1.1.1.1.1.2.1.cmml"></mo><msup id="A4.p1.1.m1.1.1.1.1.1.2.5" xref="A4.p1.1.m1.1.1.1.1.1.2.5.cmml"><mi id="A4.p1.1.m1.1.1.1.1.1.2.5.2" xref="A4.p1.1.m1.1.1.1.1.1.2.5.2.cmml">h</mi><mn id="A4.p1.1.m1.1.1.1.1.1.2.5.3" xref="A4.p1.1.m1.1.1.1.1.1.2.5.3.cmml">2</mn></msup></mrow><mo id="A4.p1.1.m1.1.1.1.1.1.1" xref="A4.p1.1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="A4.p1.1.m1.1.1.1.1.1.3" xref="A4.p1.1.m1.1.1.1.1.1.3.cmml"><mn id="A4.p1.1.m1.1.1.1.1.1.3.2" xref="A4.p1.1.m1.1.1.1.1.1.3.2.cmml">4</mn><mo id="A4.p1.1.m1.1.1.1.1.1.3.1" lspace="0px" rspace="0px" xref="A4.p1.1.m1.1.1.1.1.1.3.1.cmml"></mo><mi id="A4.p1.1.m1.1.1.1.1.1.3.3" xref="A4.p1.1.m1.1.1.1.1.1.3.3.cmml">b</mi><mo id="A4.p1.1.m1.1.1.1.1.1.3.1a" lspace="0px" rspace="0px" xref="A4.p1.1.m1.1.1.1.1.1.3.1.cmml"></mo><msup id="A4.p1.1.m1.1.1.1.1.1.3.4" xref="A4.p1.1.m1.1.1.1.1.1.3.4.cmml"><mi id="A4.p1.1.m1.1.1.1.1.1.3.4.2" xref="A4.p1.1.m1.1.1.1.1.1.3.4.2.cmml">s</mi><mn id="A4.p1.1.m1.1.1.1.1.1.3.4.3" xref="A4.p1.1.m1.1.1.1.1.1.3.4.3.cmml">2</mn></msup><mo id="A4.p1.1.m1.1.1.1.1.1.3.1b" lspace="0px" rspace="0px" xref="A4.p1.1.m1.1.1.1.1.1.3.1.cmml"></mo><mi id="A4.p1.1.m1.1.1.1.1.1.3.5" xref="A4.p1.1.m1.1.1.1.1.1.3.5.cmml">h</mi></mrow></mrow><mo id="A4.p1.1.m1.1.1.1.1.3" stretchy="false" xref="A4.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A4.p1.1.m1.1.1.2" lspace="0px" rspace="0px" xref="A4.p1.1.m1.1.1.2.cmml"></mo><mi id="A4.p1.1.m1.1.1.3" xref="A4.p1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><apply id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1"><times id="A4.p1.1.m1.1.1.2.cmml" xref="A4.p1.1.m1.1.1.2"></times><apply id="A4.p1.1.m1.1.1.1.1.1.cmml" xref="A4.p1.1.m1.1.1.1.1"><plus id="A4.p1.1.m1.1.1.1.1.1.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.1"></plus><apply id="A4.p1.1.m1.1.1.1.1.1.2.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2"><times id="A4.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.1"></times><cn id="A4.p1.1.m1.1.1.1.1.1.2.2.cmml" type="integer" xref="A4.p1.1.m1.1.1.1.1.1.2.2">24</cn><ci id="A4.p1.1.m1.1.1.1.1.1.2.3.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.3">𝑏</ci><ci id="A4.p1.1.m1.1.1.1.1.1.2.4.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.4">𝑠</ci><apply id="A4.p1.1.m1.1.1.1.1.1.2.5.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.5"><csymbol cd="ambiguous" id="A4.p1.1.m1.1.1.1.1.1.2.5.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.5">superscript</csymbol><ci id="A4.p1.1.m1.1.1.1.1.1.2.5.2.cmml" xref="A4.p1.1.m1.1.1.1.1.1.2.5.2">ℎ</ci><cn id="A4.p1.1.m1.1.1.1.1.1.2.5.3.cmml" type="integer" xref="A4.p1.1.m1.1.1.1.1.1.2.5.3">2</cn></apply></apply><apply id="A4.p1.1.m1.1.1.1.1.1.3.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3"><times id="A4.p1.1.m1.1.1.1.1.1.3.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.1"></times><cn id="A4.p1.1.m1.1.1.1.1.1.3.2.cmml" type="integer" xref="A4.p1.1.m1.1.1.1.1.1.3.2">4</cn><ci id="A4.p1.1.m1.1.1.1.1.1.3.3.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.3">𝑏</ci><apply id="A4.p1.1.m1.1.1.1.1.1.3.4.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.1.m1.1.1.1.1.1.3.4.1.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.4">superscript</csymbol><ci id="A4.p1.1.m1.1.1.1.1.1.3.4.2.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.4.2">𝑠</ci><cn id="A4.p1.1.m1.1.1.1.1.1.3.4.3.cmml" type="integer" xref="A4.p1.1.m1.1.1.1.1.1.3.4.3">2</cn></apply><ci id="A4.p1.1.m1.1.1.1.1.1.3.5.cmml" xref="A4.p1.1.m1.1.1.1.1.1.3.5">ℎ</ci></apply></apply><ci id="A4.p1.1.m1.1.1.3.cmml" xref="A4.p1.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">(24bsh^{2}+4bs^{2}h)n</annotation><annotation encoding="application/x-llamapun" id="A4.p1.1.m1.1d">( 24 italic_b italic_s italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 4 italic_b italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_h ) italic_n</annotation></semantics></math>를 사용하여 시퀀스당 FLOP를 계산했으며, 여기서 <math alttext="h" class="ltx_Math" display="inline" id="A4.p1.2.m2.1"><semantics id="A4.p1.2.m2.1a"><mi id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><ci id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">h</annotation><annotation encoding="application/x-llamapun" id="A4.p1.2.m2.1d">italic_h</annotation></semantics></math>는 모델 은닉 차원, <math alttext="b" class="ltx_Math" display="inline" id="A4.p1.3.m3.1"><semantics id="A4.p1.3.m3.1a"><mi id="A4.p1.3.m3.1.1" xref="A4.p1.3.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="A4.p1.3.m3.1b"><ci id="A4.p1.3.m3.1.1.cmml" xref="A4.p1.3.m3.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m3.1c">b</annotation><annotation encoding="application/x-llamapun" id="A4.p1.3.m3.1d">italic_b</annotation></semantics></math>는 배치 크기, <math alttext="s" class="ltx_Math" display="inline" id="A4.p1.4.m4.1"><semantics id="A4.p1.4.m4.1a"><mi id="A4.p1.4.m4.1.1" xref="A4.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="A4.p1.4.m4.1b"><ci id="A4.p1.4.m4.1.1.cmml" xref="A4.p1.4.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="A4.p1.4.m4.1d">italic_s</annotation></semantics></math>는 총 시퀀스 길이, <math alttext="n" class="ltx_Math" display="inline" id="A4.p1.5.m5.1"><semantics id="A4.p1.5.m5.1a"><mi id="A4.p1.5.m5.1.1" xref="A4.p1.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A4.p1.5.m5.1b"><ci id="A4.p1.5.m5.1.1.cmml" xref="A4.p1.5.m5.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.5.m5.1c">n</annotation><annotation encoding="application/x-llamapun" id="A4.p1.5.m5.1d">italic_n</annotation></semantics></math>는 층 수이다. 그런 다음 데이터 세트당 FLOPs 비율은 <math alttext="((24bs_{2}h^{2}+4b{s_{2}}^{2}h)/(24bs_{1}h^{2}+4b{s_{1}}^{2}h))/(s_{2}/s_{1})=(6h+s_{2})/(6h+s_{1})" class="ltx_Math" display="inline" id="A4.p1.6.m6.4"><semantics id="A4.p1.6.m6.4a"><mrow id="A4.p1.6.m6.4.4" xref="A4.p1.6.m6.4.4.cmml"><mrow id="A4.p1.6.m6.2.2.2" xref="A4.p1.6.m6.2.2.2.cmml"><mrow id="A4.p1.6.m6.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.cmml"><mo id="A4.p1.6.m6.1.1.1.1.1.2" stretchy="false" xref="A4.p1.6.m6.1.1.1.1.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.cmml"><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml"><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml"><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.cmml"><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.2.cmml">24</mn><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1.cmml"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.3.cmml">b</mi><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1a" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1.cmml"></mo><msub id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.2.cmml">s</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.3.cmml">2</mn></msub><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1b" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1.cmml"></mo><msup id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.2.cmml">h</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.3.cmml">2</mn></msup></mrow><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.2.cmml">4</mn><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1.cmml"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.3.cmml">b</mi><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1a" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1.cmml"></mo><mmultiscripts id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.2" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.2.cmml">s</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.3.cmml">2</mn><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4a" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.cmml"></mrow><mrow id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4b" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.cmml"></mrow><mn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.3" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.3.cmml">2</mn></mmultiscripts><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1b" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1.cmml"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.5" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.5.cmml">h</mi></mrow></mrow><mo id="A4.p1.6.m6.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A4.p1.6.m6.1.1.1.1.1.1.3" xref="A4.p1.6.m6.1.1.1.1.1.1.3.cmml">/</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml"><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.2" stretchy="false" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml"><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.cmml"><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.2.cmml">24</mn><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1.cmml"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.3.cmml">b</mi><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1a" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1.cmml"></mo><msub id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.2.cmml">s</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.3.cmml">1</mn></msub><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1b" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1.cmml"></mo><msup id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.2.cmml">h</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.3.cmml">2</mn></msup></mrow><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.1" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.1.cmml">+</mo><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.cmml"><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.2.cmml">4</mn><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1.cmml"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.3.cmml">b</mi><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1a" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1.cmml"></mo><mmultiscripts id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.cmml"><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.2" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.2.cmml">s</mi><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.3.cmml">1</mn><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4a" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.cmml"></mrow><mrow id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4b" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.cmml"></mrow><mn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.3" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.3.cmml">2</mn></mmultiscripts><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1b" lspace="0px" rspace="0px" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1.cmml"></mo><mi id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.5" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.5.cmml">h</mi></mrow></mrow><mo id="A4.p1.6.m6.1.1.1.1.1.1.2.1.3" stretchy="false" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo id="A4.p1.6.m6.1.1.1.1.1.3" stretchy="false" xref="A4.p1.6.m6.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A4.p1.6.m6.2.2.2.3" xref="A4.p1.6.m6.2.2.2.3.cmml">/</mo><mrow id="A4.p1.6.m6.2.2.2.2.1" xref="A4.p1.6.m6.2.2.2.2.1.1.cmml"><mo id="A4.p1.6.m6.2.2.2.2.1.2" stretchy="false" xref="A4.p1.6.m6.2.2.2.2.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.2.2.2.2.1.1" xref="A4.p1.6.m6.2.2.2.2.1.1.cmml"><msub id="A4.p1.6.m6.2.2.2.2.1.1.2" xref="A4.p1.6.m6.2.2.2.2.1.1.2.cmml"><mi id="A4.p1.6.m6.2.2.2.2.1.1.2.2" xref="A4.p1.6.m6.2.2.2.2.1.1.2.2.cmml">s</mi><mn id="A4.p1.6.m6.2.2.2.2.1.1.2.3" xref="A4.p1.6.m6.2.2.2.2.1.1.2.3.cmml">2</mn></msub><mo id="A4.p1.6.m6.2.2.2.2.1.1.1" xref="A4.p1.6.m6.2.2.2.2.1.1.1.cmml">/</mo><msub id="A4.p1.6.m6.2.2.2.2.1.1.3" xref="A4.p1.6.m6.2.2.2.2.1.1.3.cmml"><mi id="A4.p1.6.m6.2.2.2.2.1.1.3.2" xref="A4.p1.6.m6.2.2.2.2.1.1.3.2.cmml">s</mi><mn id="A4.p1.6.m6.2.2.2.2.1.1.3.3" xref="A4.p1.6.m6.2.2.2.2.1.1.3.3.cmml">1</mn></msub></mrow><mo id="A4.p1.6.m6.2.2.2.2.1.3" stretchy="false" xref="A4.p1.6.m6.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="A4.p1.6.m6.4.4.5" xref="A4.p1.6.m6.4.4.5.cmml">=</mo><mrow id="A4.p1.6.m6.4.4.4" xref="A4.p1.6.m6.4.4.4.cmml"><mrow id="A4.p1.6.m6.3.3.3.1.1" xref="A4.p1.6.m6.3.3.3.1.1.1.cmml"><mo id="A4.p1.6.m6.3.3.3.1.1.2" stretchy="false" xref="A4.p1.6.m6.3.3.3.1.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.3.3.3.1.1.1" xref="A4.p1.6.m6.3.3.3.1.1.1.cmml"><mrow id="A4.p1.6.m6.3.3.3.1.1.1.2" xref="A4.p1.6.m6.3.3.3.1.1.1.2.cmml"><mn id="A4.p1.6.m6.3.3.3.1.1.1.2.2" xref="A4.p1.6.m6.3.3.3.1.1.1.2.2.cmml">6</mn><mo id="A4.p1.6.m6.3.3.3.1.1.1.2.1" lspace="0px" rspace="0px" xref="A4.p1.6.m6.3.3.3.1.1.1.2.1.cmml"></mo><mi id="A4.p1.6.m6.3.3.3.1.1.1.2.3" xref="A4.p1.6.m6.3.3.3.1.1.1.2.3.cmml">h</mi></mrow><mo id="A4.p1.6.m6.3.3.3.1.1.1.1" xref="A4.p1.6.m6.3.3.3.1.1.1.1.cmml">+</mo><msub id="A4.p1.6.m6.3.3.3.1.1.1.3" xref="A4.p1.6.m6.3.3.3.1.1.1.3.cmml"><mi id="A4.p1.6.m6.3.3.3.1.1.1.3.2" xref="A4.p1.6.m6.3.3.3.1.1.1.3.2.cmml">s</mi><mn id="A4.p1.6.m6.3.3.3.1.1.1.3.3" xref="A4.p1.6.m6.3.3.3.1.1.1.3.3.cmml">2</mn></msub></mrow><mo id="A4.p1.6.m6.3.3.3.1.1.3" stretchy="false" xref="A4.p1.6.m6.3.3.3.1.1.1.cmml">)</mo></mrow><mo id="A4.p1.6.m6.4.4.4.3" xref="A4.p1.6.m6.4.4.4.3.cmml">/</mo><mrow id="A4.p1.6.m6.4.4.4.2.1" xref="A4.p1.6.m6.4.4.4.2.1.1.cmml"><mo id="A4.p1.6.m6.4.4.4.2.1.2" stretchy="false" xref="A4.p1.6.m6.4.4.4.2.1.1.cmml">(</mo><mrow id="A4.p1.6.m6.4.4.4.2.1.1" xref="A4.p1.6.m6.4.4.4.2.1.1.cmml"><mrow id="A4.p1.6.m6.4.4.4.2.1.1.2" xref="A4.p1.6.m6.4.4.4.2.1.1.2.cmml"><mn id="A4.p1.6.m6.4.4.4.2.1.1.2.2" xref="A4.p1.6.m6.4.4.4.2.1.1.2.2.cmml">6</mn><mo id="A4.p1.6.m6.4.4.4.2.1.1.2.1" lspace="0px" rspace="0px" xref="A4.p1.6.m6.4.4.4.2.1.1.2.1.cmml"></mo><mi id="A4.p1.6.m6.4.4.4.2.1.1.2.3" xref="A4.p1.6.m6.4.4.4.2.1.1.2.3.cmml">h</mi></mrow><mo id="A4.p1.6.m6.4.4.4.2.1.1.1" xref="A4.p1.6.m6.4.4.4.2.1.1.1.cmml">+</mo><msub id="A4.p1.6.m6.4.4.4.2.1.1.3" xref="A4.p1.6.m6.4.4.4.2.1.1.3.cmml"><mi id="A4.p1.6.m6.4.4.4.2.1.1.3.2" xref="A4.p1.6.m6.4.4.4.2.1.1.3.2.cmml">s</mi><mn id="A4.p1.6.m6.4.4.4.2.1.1.3.3" xref="A4.p1.6.m6.4.4.4.2.1.1.3.3.cmml">1</mn></msub></mrow><mo id="A4.p1.6.m6.4.4.4.2.1.3" stretchy="false" xref="A4.p1.6.m6.4.4.4.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.6.m6.4b"><apply id="A4.p1.6.m6.4.4.cmml" xref="A4.p1.6.m6.4.4"><eq id="A4.p1.6.m6.4.4.5.cmml" xref="A4.p1.6.m6.4.4.5"></eq><apply id="A4.p1.6.m6.2.2.2.cmml" xref="A4.p1.6.m6.2.2.2"><divide id="A4.p1.6.m6.2.2.2.3.cmml" xref="A4.p1.6.m6.2.2.2.3"></divide><apply id="A4.p1.6.m6.1.1.1.1.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1"><divide id="A4.p1.6.m6.1.1.1.1.1.1.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.3"></divide><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1"><plus id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.1"></plus><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2"><times id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.1"></times><cn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.2.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.2">24</cn><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.3">𝑏</ci><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.2">𝑠</ci><cn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.3.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.4.3">2</cn></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5">superscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.2">ℎ</ci><cn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.3.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.2.5.3">2</cn></apply></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3"><times id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.1"></times><cn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.2">4</cn><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.3">𝑏</ci><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4">superscript</csymbol><apply id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4">subscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.2">𝑠</ci><cn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.3.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.2.3">2</cn></apply><cn id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.3.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.4.3">2</cn></apply><ci id="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.5.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.1.1.1.3.5">ℎ</ci></apply></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1"><plus id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.1"></plus><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2"><times id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.1"></times><cn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.2.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.2">24</cn><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.3">𝑏</ci><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4">subscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.2">𝑠</ci><cn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.3.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.4.3">1</cn></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5">superscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.2">ℎ</ci><cn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.3.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.2.5.3">2</cn></apply></apply><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3"><times id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.1"></times><cn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.2.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.2">4</cn><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.3.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.3">𝑏</ci><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4">superscript</csymbol><apply id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4"><csymbol cd="ambiguous" id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.1.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4">subscript</csymbol><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.2.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.2">𝑠</ci><cn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.3.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.2.3">1</cn></apply><cn id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.3.cmml" type="integer" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.4.3">2</cn></apply><ci id="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.5.cmml" xref="A4.p1.6.m6.1.1.1.1.1.1.2.1.1.3.5">ℎ</ci></apply></apply></apply><apply id="A4.p1.6.m6.2.2.2.2.1.1.cmml" xref="A4.p1.6.m6.2.2.2.2.1"><divide id="A4.p1.6.m6.2.2.2.2.1.1.1.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.1"></divide><apply id="A4.p1.6.m6.2.2.2.2.1.1.2.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="A4.p1.6.m6.2.2.2.2.1.1.2.1.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.2">subscript</csymbol><ci id="A4.p1.6.m6.2.2.2.2.1.1.2.2.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.2.2">𝑠</ci><cn id="A4.p1.6.m6.2.2.2.2.1.1.2.3.cmml" type="integer" xref="A4.p1.6.m6.2.2.2.2.1.1.2.3">2</cn></apply><apply id="A4.p1.6.m6.2.2.2.2.1.1.3.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="A4.p1.6.m6.2.2.2.2.1.1.3.1.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.3">subscript</csymbol><ci id="A4.p1.6.m6.2.2.2.2.1.1.3.2.cmml" xref="A4.p1.6.m6.2.2.2.2.1.1.3.2">𝑠</ci><cn id="A4.p1.6.m6.2.2.2.2.1.1.3.3.cmml" type="integer" xref="A4.p1.6.m6.2.2.2.2.1.1.3.3">1</cn></apply></apply></apply><apply id="A4.p1.6.m6.4.4.4.cmml" xref="A4.p1.6.m6.4.4.4"><divide id="A4.p1.6.m6.4.4.4.3.cmml" xref="A4.p1.6.m6.4.4.4.3"></divide><apply id="A4.p1.6.m6.3.3.3.1.1.1.cmml" xref="A4.p1.6.m6.3.3.3.1.1"><plus id="A4.p1.6.m6.3.3.3.1.1.1.1.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.1"></plus><apply id="A4.p1.6.m6.3.3.3.1.1.1.2.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.2"><times id="A4.p1.6.m6.3.3.3.1.1.1.2.1.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.2.1"></times><cn id="A4.p1.6.m6.3.3.3.1.1.1.2.2.cmml" type="integer" xref="A4.p1.6.m6.3.3.3.1.1.1.2.2">6</cn><ci id="A4.p1.6.m6.3.3.3.1.1.1.2.3.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.2.3">ℎ</ci></apply><apply id="A4.p1.6.m6.3.3.3.1.1.1.3.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="A4.p1.6.m6.3.3.3.1.1.1.3.1.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.3">subscript</csymbol><ci id="A4.p1.6.m6.3.3.3.1.1.1.3.2.cmml" xref="A4.p1.6.m6.3.3.3.1.1.1.3.2">𝑠</ci><cn id="A4.p1.6.m6.3.3.3.1.1.1.3.3.cmml" type="integer" xref="A4.p1.6.m6.3.3.3.1.1.1.3.3">2</cn></apply></apply><apply id="A4.p1.6.m6.4.4.4.2.1.1.cmml" xref="A4.p1.6.m6.4.4.4.2.1"><plus id="A4.p1.6.m6.4.4.4.2.1.1.1.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.1"></plus><apply id="A4.p1.6.m6.4.4.4.2.1.1.2.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.2"><times id="A4.p1.6.m6.4.4.4.2.1.1.2.1.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.2.1"></times><cn id="A4.p1.6.m6.4.4.4.2.1.1.2.2.cmml" type="integer" xref="A4.p1.6.m6.4.4.4.2.1.1.2.2">6</cn><ci id="A4.p1.6.m6.4.4.4.2.1.1.2.3.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.2.3">ℎ</ci></apply><apply id="A4.p1.6.m6.4.4.4.2.1.1.3.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.3"><csymbol cd="ambiguous" id="A4.p1.6.m6.4.4.4.2.1.1.3.1.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.3">subscript</csymbol><ci id="A4.p1.6.m6.4.4.4.2.1.1.3.2.cmml" xref="A4.p1.6.m6.4.4.4.2.1.1.3.2">𝑠</ci><cn id="A4.p1.6.m6.4.4.4.2.1.1.3.3.cmml" type="integer" xref="A4.p1.6.m6.4.4.4.2.1.1.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.6.m6.4c">((24bs_{2}h^{2}+4b{s_{2}}^{2}h)/(24bs_{1}h^{2}+4b{s_{1}}^{2}h))/(s_{2}/s_{1})=(6h+s_{2})/(6h+s_{1})</annotation><annotation encoding="application/x-llamapun" id="A4.p1.6.m6.4d">( ( 24 italic_b italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 4 italic_b italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_h ) / ( 24 italic_b italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 4 italic_b italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_h ) ) / ( italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT / italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = ( 6 italic_h + italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) / ( 6 italic_h + italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )</annotation></semantics></math>에 의해 주어지며, 여기서 <math alttext="s_{2}" class="ltx_Math" display="inline" id="A4.p1.7.m7.1"><semantics id="A4.p1.7.m7.1a"><msub id="A4.p1.7.m7.1.1" xref="A4.p1.7.m7.1.1.cmml"><mi id="A4.p1.7.m7.1.1.2" xref="A4.p1.7.m7.1.1.2.cmml">s</mi><mn id="A4.p1.7.m7.1.1.3" xref="A4.p1.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p1.7.m7.1b"><apply id="A4.p1.7.m7.1.1.cmml" xref="A4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="A4.p1.7.m7.1.1.1.cmml" xref="A4.p1.7.m7.1.1">subscript</csymbol><ci id="A4.p1.7.m7.1.1.2.cmml" xref="A4.p1.7.m7.1.1.2">𝑠</ci><cn id="A4.p1.7.m7.1.1.3.cmml" type="integer" xref="A4.p1.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.7.m7.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.7.m7.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> 및 <math alttext="s_{1}" class="ltx_Math" display="inline" id="A4.p1.8.m8.1"><semantics id="A4.p1.8.m8.1a"><msub id="A4.p1.8.m8.1.1" xref="A4.p1.8.m8.1.1.cmml"><mi id="A4.p1.8.m8.1.1.2" xref="A4.p1.8.m8.1.1.2.cmml">s</mi><mn id="A4.p1.8.m8.1.1.3" xref="A4.p1.8.m8.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A4.p1.8.m8.1b"><apply id="A4.p1.8.m8.1.1.cmml" xref="A4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="A4.p1.8.m8.1.1.1.cmml" xref="A4.p1.8.m8.1.1">subscript</csymbol><ci id="A4.p1.8.m8.1.1.2.cmml" xref="A4.p1.8.m8.1.1.2">𝑠</ci><cn id="A4.p1.8.m8.1.1.3.cmml" type="integer" xref="A4.p1.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.8.m8.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.8.m8.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>는 새롭고 오래된 컨텍스트 길이이다. 모델 크기 및 숨겨진 치수는 다음과 같습니다. LLaMA-7B(4096), LLaMA-13B(5140), LLaMA-33B(7168), LLaMA-65B(8192), GPT3-175B(12288) 및 1TB(36864). 이 모델 구성은 LLaMA <cite class="ltx_cite ltx_citemacro_citep">[Touvron et al., <a class="ltx_ref" href="#bib.bib38" title="">2023</a>]</cite>와 GPT-3 <cite class="ltx_cite ltx_citemacro_citep">[Brown et al., <a class="ltx_ref" href="#bib.bib3" title="">2020</a>]</cite> 논문으로 구성되었으며, 1TB 모델 크기와 차원은 우리가 정의했다.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p class="ltx_p" id="A4.p2.1">그림 <a class="ltx_ref" href="#A4.F5" title="Figure 5 ‣ Appendix D Training FLOPs Scaling of Context Size ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5</span></a>에 묘사된 바와 같이, 작은 모델들을 1M 컨텍스트 사이즈로 스케일업하는 것은 대략 20-40배 더 많은 FLOP들을 야기하고, 10M 및 100M 토큰 컨텍스트 사이즈들에 대해 훨씬 더 많은 FLOP들을 야기한다. 그러나 모형 크기가 커질수록 비용 비율은 감소한다. 예를 들어, 170B 모델을 4K에서 10M으로 확장하면 컨텍스트 크기가 3072배 더 길음에도 불구하고 데이터 세트 FLOP당 162.6배 더 높다.</p>
</div>
<figure id="A4.F5" class="ltx_figure"><img src="https://ar5iv.labs.arxiv.org/html/2310.01889/assets/figures/cost_heatmap.png" id="A4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="595" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">그림 5:</span>데이터 세트당 FLOP는 서로 다른 모델 차원을 고려하여 4k 컨텍스트 크기에 대한 비용 비율을 학습합니다. x축에서 컨텍스트 길이를 찾을 수 있습니다. 예를 들어 32x(128k)는 128k의 컨텍스트 길이를 나타내고 32x는 동일한 모델의 4k 컨텍스트 길이의 크기입니다.</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Impact on In Context RL Performance</h2>

<div id="A5.p1" class="ltx_para">
<p class="ltx_p" id="A5.p1.3"><a class="ltx_ref" href="#S5.SS3" title="5.3 Impact on LLM Performance ‣ 5 Results ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5.3</span></a>절에서 LLM을 미세 조정하기 위한 Ring Attention의 적용을 보여주는 것 외에도 Transformers를 활용한 시행착오 RL 경험 학습을 위해 Ring Attention을 적용한 추가적인 결과를 제시한다. 우리는 우리의 결과를 표 <a class="ltx_ref" href="#A5.T5" title="Table 5 ‣ Appendix E Impact on In Context RL Performance ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5</span></a>에 보고하며, 여기서 우리는 6가지 다른 작업에 걸쳐 ExoRL 벤치마크에서 제안된 모델을 평가한다. ExoRL에서 우리는 ExoRL <cite class="ltx_cite ltx_citemacro_citep">[Yarats et al., <a class="ltx_ref" href="#bib.bib41" title="">2022</a>]</cite>와 같이 누적 수익을 보고한다. BC, DT<cite class="ltx_cite ltx_citemacro_citep">[Chen et al., <a class="ltx_ref" href="#bib.bib4" title="">2021</a>]</cite>, AT<cite class="ltx_cite ltx_citemacro_citep">[Liu and Abbeel, <a class="ltx_ref" href="#bib.bib23" title="">2023a</a>]</cite>, AT with memory efficient attention</cite idx=3></cite> (AT+ME), AT with blockwise parallel transformers</cite idx=4></cite> (AT+BPT), AT with our Ring Attention (AT+Ring Attention)을 비교한다. BC, DT, AT의 숫자는 ExoRL 및 AT 용지에서 가져온 것이다. AT + 링 주의 번호는 자체 실행됩니다. ExoRL 데이터는 매우 다양하기 때문에 감독되지 않은 RL<cite class="ltx_cite ltx_citemacro_citep">[Laskin et al., <a class="ltx_ref" href="#bib.bib20" title="">2021</a>]</cite>를 사용하여 수집되었으며 TD 학습이 가장 잘 수행되는 반면 행동 복제 문제는 <cite class="ltx_cite ltx_citemacro_citep">[Yarats et al., <a class="ltx_ref" href="#bib.bib41" title="">2022</a>]</cite>인 것으로 나타났다. AT<cite class="ltx_cite ltx_citemacro_citep">[Liu and Abbeel, <a class="ltx_ref" href="#bib.bib23" title="">2023a</a>]</cite>는 재표지된 타겟 리턴을 갖는 다중 궤적 상의 컨디셔닝 트랜스포머가 TD 학습으로 경쟁적인 결과를 달성할 수 있음을 보여준다. 자세한 내용은 문서를 참조하십시오. 우리는 이전 작업에서 32개의 궤적이 아닌 더 많은 수의 궤적에 대한 컨디셔닝을 통해 AT의 성능을 향상시키기 위해 링 어텐션을 적용하는 데 관심이 있다. 각각의 궤적은 <math alttext="1000\times 4" class="ltx_Math" display="inline" id="A5.p1.1.m1.1"><semantics id="A5.p1.1.m1.1a"><mrow id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><mn id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2.cmml">1000</mn><mo id="A5.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A5.p1.1.m1.1.1.1.cmml">×</mo><mn id="A5.p1.1.m1.1.1.3" xref="A5.p1.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><times id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1"></times><cn id="A5.p1.1.m1.1.1.2.cmml" type="integer" xref="A5.p1.1.m1.1.1.2">1000</cn><cn id="A5.p1.1.m1.1.1.3.cmml" type="integer" xref="A5.p1.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">1000\times 4</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.1d">1000 × 4</annotation></semantics></math> 길이를 가지며, 여기서 <math alttext="1000" class="ltx_Math" display="inline" id="A5.p1.2.m2.1"><semantics id="A5.p1.2.m2.1a"><mn id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.1b"><cn id="A5.p1.2.m2.1.1.cmml" type="integer" xref="A5.p1.2.m2.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.1c">1000</annotation><annotation encoding="application/x-llamapun" id="A5.p1.2.m2.1d">1000</annotation></semantics></math>는 시퀀스 길이이고, <math alttext="4" class="ltx_Math" display="inline" id="A5.p1.3.m3.1"><semantics id="A5.p1.3.m3.1a"><mn id="A5.p1.3.m3.1.1" xref="A5.p1.3.m3.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A5.p1.3.m3.1b"><cn id="A5.p1.3.m3.1.1.cmml" type="integer" xref="A5.p1.3.m3.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.3.m3.1c">4</annotation><annotation encoding="application/x-llamapun" id="A5.p1.3.m3.1d">4</annotation></semantics></math>는 리턴-스테이트-액션-리워드(return-state-action-reward)이므로, 훈련용 128개의 궤적은 이전의 최신 블록형 병렬 트랜스포머에 대해 적당한 350M 크기 모델을 사용할 수 없게 만든다. 표 <a class="ltx_ref" href="#A5.T5" title="Table 5 ‣ Appendix E Impact on In Context RL Performance ‣ Ring Attention with Blockwise Transformers for Near-Infinite Context"><span class="ltx_text ltx_ref_tag">5</span></a>의 결과는 시퀀스 길이(궤적 수)를 확장함으로써 AT + Ring Attention이 6개의 태스크 모두에서 BPT를 사용한 Oringal AT를 일관되게 능가하여 BPT 모델의 총 평균 수익률 111.13을 가진 AT에 비해 총 평균 수익률 113.66을 달성했음을 보여준다. 결과는 긴 시퀀스를 사용한 훈련 및 추론을 위한 Ring Attention의 이점을 보여준다.</p>
</div>
<figure id="A5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">표 5:</span>Application of Ring Attention on improving Transformer in RL. BC와 DT는 바닐라 주의력을 사용한다. AT + ME는 메모리 효율적인 주의력을 사용하고 AT + BPT는 블록 병렬 변압기를 사용한다. AT + RA는 Ring Attention을 사용함을 나타냅니다.</figcaption>
<div id="A5.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:328.6pt;height:137.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.0pt,12.1pt) scale(0.85,0.85) ;">
<table id="A5.T5.1.1" class="ltx_tabular ltx_align_middle">
<tbody><tr id="A5.T5.1.1.1" class="ltx_tr">
<td id="A5.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A5.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">ExoRL</span></td>
<td id="A5.T5.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">BC-10%</span></td>
<td id="A5.T5.1.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt"><span id="A5.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">DT</span></td>
<td id="A5.T5.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.4.1" class="ltx_text ltx_font_bold">AT + ME</span></td>
<td id="A5.T5.1.1.1.5" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.5.1" class="ltx_text ltx_font_bold">AT + BPT</span></td>
<td id="A5.T5.1.1.1.6" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.6.1" class="ltx_text ltx_font_bold">AT + BPT</span></td>
<td id="A5.T5.1.1.1.7" class="ltx_td ltx_align_right ltx_border_tt"><span id="A5.T5.1.1.1.7.1" class="ltx_text ltx_font_bold">AT + RA</span></td>
</tr>
<tr id="A5.T5.1.1.2" class="ltx_tr">
<td id="A5.T5.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T5.1.1.2.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="A5.T5.1.1.2.2" class="ltx_td ltx_border_t"></td>
<td id="A5.T5.1.1.2.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="A5.T5.1.1.2.4" class="ltx_td ltx_align_right ltx_border_t">N Trajs = 32</td>
<td id="A5.T5.1.1.2.5" class="ltx_td ltx_align_right ltx_border_t">N Trajs = 32</td>
<td id="A5.T5.1.1.2.6" class="ltx_td ltx_align_right ltx_border_t">N Trajs = 128</td>
<td id="A5.T5.1.1.2.7" class="ltx_td ltx_align_right ltx_border_t">N Trajs = 128</td>
</tr>
<tr id="A5.T5.1.1.3" class="ltx_tr">
<td id="A5.T5.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Walker Stand</td>
<td id="A5.T5.1.1.3.2" class="ltx_td ltx_align_right ltx_border_t">52.91</td>
<td id="A5.T5.1.1.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">34.54</td>
<td id="A5.T5.1.1.3.4" class="ltx_td ltx_align_right ltx_border_t">oom</td>
<td id="A5.T5.1.1.3.5" class="ltx_td ltx_align_right ltx_border_t">95.45</td>
<td id="A5.T5.1.1.3.6" class="ltx_td ltx_align_right ltx_border_t">oom</td>
<td id="A5.T5.1.1.3.7" class="ltx_td ltx_align_right ltx_border_t">98.23</td>
</tr>
<tr id="A5.T5.1.1.4" class="ltx_tr">
<td id="A5.T5.1.1.4.1" class="ltx_td ltx_align_left">Walker Run</td>
<td id="A5.T5.1.1.4.2" class="ltx_td ltx_align_right">34.81</td>
<td id="A5.T5.1.1.4.3" class="ltx_td ltx_align_right ltx_border_r">49.82</td>
<td id="A5.T5.1.1.4.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.4.5" class="ltx_td ltx_align_right">105.88</td>
<td id="A5.T5.1.1.4.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.4.7" class="ltx_td ltx_align_right">110.45</td>
</tr>
<tr id="A5.T5.1.1.5" class="ltx_tr">
<td id="A5.T5.1.1.5.1" class="ltx_td ltx_align_left">Walker Walk</td>
<td id="A5.T5.1.1.5.2" class="ltx_td ltx_align_right">13.53</td>
<td id="A5.T5.1.1.5.3" class="ltx_td ltx_align_right ltx_border_r">34.94</td>
<td id="A5.T5.1.1.5.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.5.5" class="ltx_td ltx_align_right">78.56</td>
<td id="A5.T5.1.1.5.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.5.7" class="ltx_td ltx_align_right">78.95</td>
</tr>
<tr id="A5.T5.1.1.6" class="ltx_tr">
<td id="A5.T5.1.1.6.1" class="ltx_td ltx_align_left">Cheetah Run</td>
<td id="A5.T5.1.1.6.2" class="ltx_td ltx_align_right">34.66</td>
<td id="A5.T5.1.1.6.3" class="ltx_td ltx_align_right ltx_border_r">67.53</td>
<td id="A5.T5.1.1.6.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.6.5" class="ltx_td ltx_align_right">178.75</td>
<td id="A5.T5.1.1.6.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.6.7" class="ltx_td ltx_align_right">181.34</td>
</tr>
<tr id="A5.T5.1.1.7" class="ltx_tr">
<td id="A5.T5.1.1.7.1" class="ltx_td ltx_align_left">Jaco Reach</td>
<td id="A5.T5.1.1.7.2" class="ltx_td ltx_align_right">23.95</td>
<td id="A5.T5.1.1.7.3" class="ltx_td ltx_align_right ltx_border_r">18.64</td>
<td id="A5.T5.1.1.7.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.7.5" class="ltx_td ltx_align_right">87.56</td>
<td id="A5.T5.1.1.7.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.7.7" class="ltx_td ltx_align_right">89.51</td>
</tr>
<tr id="A5.T5.1.1.8" class="ltx_tr">
<td id="A5.T5.1.1.8.1" class="ltx_td ltx_align_left">Cartpole Swingup</td>
<td id="A5.T5.1.1.8.2" class="ltx_td ltx_align_right">56.82</td>
<td id="A5.T5.1.1.8.3" class="ltx_td ltx_align_right ltx_border_r">67.56</td>
<td id="A5.T5.1.1.8.4" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.8.5" class="ltx_td ltx_align_right">120.56</td>
<td id="A5.T5.1.1.8.6" class="ltx_td ltx_align_right">oom</td>
<td id="A5.T5.1.1.8.7" class="ltx_td ltx_align_right">123.45</td>
</tr>
<tr id="A5.T5.1.1.9" class="ltx_tr">
<td id="A5.T5.1.1.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="A5.T5.1.1.9.1.1" class="ltx_text ltx_font_bold">Total Average</span></td>
<td id="A5.T5.1.1.9.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">36.11</td>
<td id="A5.T5.1.1.9.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t">45.51</td>
<td id="A5.T5.1.1.9.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">oom</td>
<td id="A5.T5.1.1.9.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">111.13</td>
<td id="A5.T5.1.1.9.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">oom</td>
<td id="A5.T5.1.1.9.7" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">113.66</td>
</tr>
</tbody></table>
</span></div>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2310.01888" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="https://ar5iv.labs.arxiv.org/assets/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/log/2310.01889" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2310.01889">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.01889" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2310.01890" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Nov  5 17:25:43 2023 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
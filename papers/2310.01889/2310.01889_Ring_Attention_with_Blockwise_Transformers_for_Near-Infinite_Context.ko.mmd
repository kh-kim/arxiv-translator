# Near-Infinite Context를 위한 Blockwise Transformer를 이용한 Ring Attention

 하오류, 마테이자하리아, 피에르 아베엘

UC Berkeley

hao.liu@cs.berkeley.edu

###### Abstract

트랜스포머는 광범위한 AI 애플리케이션에서 탁월한 성능을 보여주면서 많은 최첨단 AI 모델의 선택 아키텍처로 부상했다. 그러나 트랜스포머에 의해 부과되는 메모리 요구는 긴 시퀀스를 처리하는 능력을 제한하여 복잡한 환경에서 비디오, 액션 및 기타 긴 형식의 시퀀스 및 모달리티를 활용하는 데 어려움이 있다. 본 논문에서는 키-값 블록의 통신과 블록별 주의의 계산을 완전히 중첩시키면서 자기 주의의 블록별 계산과 피드포워드(feedforward)를 활용하여 여러 장치에 긴 시퀀스를 분산시키는 새로운 접근법인 링 어텐션(Ring Attention with Blockwise Transformers)을 제안한다. 제안된 방법은 기존의 메모리 효율적인 트랜스포머에 의해 달성 가능한 것보다 더 긴 디바이스 카운트 시간까지의 시퀀스들을 근사치들에 의존하거나 추가적인 통신 및 계산 오버헤드를 발생시키지 않으면서 트레이닝 및 추론을 가능하게 한다. 언어 모델링 및 강화 학습 작업에 대한 광범위한 실험은 수백만 토큰 컨텍스트 크기를 허용하고 성능을 개선하는 데 있어 우리의 접근법의 효율성을 보여준다. 1.

각주 1: 코드: [https://github.com/lhao499/lln_large_context](https://github.com/lhao499/lln_large_context)

## 1 Introduction

트랜스포머[37]는 광범위한 AI 문제에 걸쳐 인상적인 성능을 보여준 많은 최첨단 AI 시스템의 중추가 되었다. 트랜스포머는 자기 주의 및 위치별 피드포워드 메커니즘을 사용하는 아키텍처 설계를 통해 이러한 성공을 달성한다. 그러나 트랜스포머의 상속 아키텍처 설계인 _i.e_ 로부터 트랜스포머의 컨텍스트 길이를 확장 하는 것은 과제 [29]입니다. 자기 주의는 입력 시퀀스의 길이에서 메모리 비용이 2차이므로 더 긴 입력 시퀀스로 확장하기가 어렵다. 대용량 컨텍스트 트랜스포머는 서적 및 고해상도 이미지 처리부터 긴 비디오 및 복잡한 코드베이스 분석에 이르기까지 다양한 AI 문제를 해결하는 데 필수적이다. 그들은 상호 연결된 웹과 하이퍼링크된 콘텐츠에서 정보를 추출하는 데 탁월하며 복잡한 과학 실험 데이터를 처리하는 데 중요하다. 문맥 길이가 16K인 GPT-3.5[32], 문맥 길이가 32k인 GPT-4[29], 문맥 길이가 65k인 MosaicML의 MPT[25], 문맥 길이가 100k인 Anthropic의 Claude[1] 등 이전보다 문맥이 크게 확장된 언어 모델의 사용 사례가 등장하고 있다.

그 중요성에 힘입어 메모리 비용 절감에 대한 연구 관심이 급증하고 있다. 한 연구 라인은 자기 주의의 소프트맥스 행렬이 전체 행렬[24]을 구체화하지 않고 계산될 수 있다는 관찰을 활용하며, 이는 근사화를 하지 않고 자기 주의 및 피드포워드[30; 9; 23]의 블록 단위 계산의 개발로 이어졌다. 감소된 메모리에도 불구하고, 각 층의 출력을 저장하는 것으로부터 여전히 상당한 도전이 발생한다. 이러한 필요성은 모든 요소 간의 상호 작용(n~n개의 상호 작용)을 포함하는 자기 주의의 고유한 특성에서 발생한다. 후속 레이어의 자체 주의는 이전 레이어의 모든 출력에 액세스하는 것에 의존한다. 이렇게 하지 않으면 각 시퀀스 요소에 대해 모든 출력을 다시 계산해야 하므로 계산 비용이 3차적으로 증가하여 더 긴 시퀀스에 대해서는 비실용적이다.

이러한 구성 요소는 입력 토큰 간의 장거리 종속성을 효율적으로 캡처하고, 높은 병렬 계산을 통해 확장성을 가능하게 한다. 메모리 수요를 관점으로 볼 때, 1의 배치 크기를 다루는 경우에도 1억 개의 토큰을 처리하려면 1024의 숨겨진 크기를 가진 적당한 모델의 경우 1000GB 이상의 메모리가 필요하다. 이는 일반적으로 100GB 미만의 고대역폭 메모리(HBM)를 갖는 현대 GPU 및 TPU의 용량보다 훨씬 크다.

이 문제를 해결하기 위해, 우리는 주요 관찰을 한다: 블록 방식으로 자기 주의 및 피드포워드 네트워크 계산을 수행함으로써[23], 우리는 동시 계산 및 통신을 허용하면서 여러 장치에 시퀀스 차원을 분산시킬 수 있다. 이러한 통찰은 우리가 블록 단위로 주의력을 계산할 때 결과가 이러한 블록 단위의 계산 순서에 불변한다는 사실에서 비롯된다. 제안하는 방법은 호스트들 사이에 블록 단위의 집중도를 계산하는 외부 루프를 분배하고, 각 디바이스들은 각각의 입력 블록을 관리한다. 내부 루프의 경우 모든 장치는 지정된 입력 블록에 따라 블록별 주의 및 피드포워드 동작을 계산합니다. 호스트 디바이스들은 개념적 링을 형성하고, 여기서 내부 루프 동안, 각각의 디바이스는 블록 단위 계산을 위해 사용되고 있는 자신의 키-값 블록들의 복사본을 링 내의 다음 디바이스로 전송하는 한편, 동시에 이전 디바이스로부터 키-값 블록들을 수신한다. 블록 계산이 블록 전송보다 더 오래 걸리는 한, 이러한 프로세스를 중첩하면 표준 변압기에 비해 추가 오버헤드가 발생하지 않는다. 자기 주의를 계산하기 위한 링 토폴로지의 사용은 이전 작업에서도 연구되었지만[21] 시퀀스 병렬성과 유사한 중복되지 않은 통신 오버헤드를 초래하여 큰 컨텍스트 크기에 대해서는 실행 불가능한다. 우리의 작업은 블록별 병렬 트랜스포머[23]를 사용하여 메모리 비용을 실질적으로 감소시키며, 트레이닝 및 추론 동안 수천만 토큰에 걸쳐 컨텍스트 크기의 제로 오버헤드 스케일링을 가능하게 하고, 임의로 큰 컨텍스트 크기를 사용할 수 있게 한다. 제안하는 방법은 변환기의 블록 단위 연산을 통해 링 내의 호스트들 간의 키-값 블록 간의 통신을 중첩하므로, 이를 블록 단위 병렬 변환기를 이용한 링 어텐션(Ring Attention)이라 명명한다.

우리는 언어 모델링 벤치마크에 대한 접근법의 효율성을 평가한다. 실험 결과, Ring Attention은 Transformers의 메모리 요구량을 줄일 수 있으며, 기존의 메모리 효율적인 최신 기술보다 500배 이상의 긴 시퀀스를 학습시킬 수 있으며, 1억 개를 초과하는 시퀀스를 어림셈 없이 학습시킬 수 있음을 보였다. 중요하게도, 링 어텐션은 개별 디바이스들에 의해 부과되는 메모리 제약들을 제거하여, 디바이스들의 수에 비례하여 스케일링되는 길이들을 갖는 시퀀스들의 트레이닝 및 추론에 권한을 부여함으로써, 본질적으로 거의 무한한 컨텍스트 크기를 달성한다.

본 논문의 기여도는 (a) 성능을 유지하면서 컨텍스트 길이가 디바이스 수에 따라 선형적으로 확장될 수 있도록 하는 메모리 효율적인 트랜스포머 아키텍처를 제안하고, (b) 개별 디바이스에 의해 부과되는 메모리 병목 현상을 제거하며, 그리고 (b) 광범위한 실험을 통해 본 논문의 접근법의 유효성을 입증한다.

## 2 대 컨텍스트 메모리 제약 조건

주어진 입력 시퀀스 \(Q,K,V\in\mathbb{R}^{s\times d}\) 여기서 \(s\)는 시퀀스 길이이고 \(d\)는 헤드 차원이다. 우리는 출력의 행렬을 다음과 같이 계산한다:

\[\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d}})V,\]

여기서 \(\mathrm{softmax}\)는 행 단위로 적용된다. 각각의 셀프-어텐션 서브-레이어는 피드포워드 네트워크를 동반하며, 이는 각각의 포지션에 개별적으로 그리고 동일하게 적용된다. 이것은 두 개의 선형으로 구성되어 있다.

도 1: TPUv4-1024 상의 종단간 대규모 트레이닝 하에서의 최대 컨텍스트 길이. 기준선은 바닐라 변압기[37], 메모리 효율 변압기[30], 및 메모리 효율 주의 및 피드포워드(블록별 병렬 변압기)[23]. 제안하는 링 어텐션은 베이스라인보다 긴 수열을 디바이스 카운트 시간까지 트레이닝할 수 있고, 근사치를 만들거나 통신 및 계산에 오버헤드를 추가하지 않고도 수백만 길이를 초과하는 수열의 트레이닝을 가능하게 한다.

중간에 ReLU 활성화를 갖는 형질전환.

\[\mathrm{FFN}(x)=\max(0,xW_{1}+b_{1})W_{2}+b_{2}.\]

**블록별 병렬 변압기** 이전 최첨단 기술은 블록별로 주의력을 계산하여 주의를 완전히 구체화하지 않고도 주의력 계산을 가능하게 하는 혁신적인 기술을 통해 달성된 메모리 활용률을 상당히 줄였습니다[30; 9; 23]. 이러한 발전은 메모리 오버헤드를 레이어당 \(2bsh\) 바이트로 낮췄으며, 여기서 \(b\)는 배치 크기를 나타내고, \(s\)는 시퀀스 길이를 나타내며, \(h\)는 모델의 숨겨진 크기를 나타낸다. 메모리 사용을 더욱 감소시키기 위해, 블록별 병렬 트랜스포머(BPT) [23]는 각각의 자기-주목 서브-레이어와 연관된 피드포워드 네트워크가 블록별 방식으로 계산되는 전략을 도입하였다. 이 방법은 피드포워드 네트워크의 최대 활성화 크기를 \(8bsh\)에서 \(2bsh\)로 효과적으로 제한한다. 메모리 효율성에 대한 보다 상세한 분석을 위해, 이에 제공된 논의를 참조하기 바란다. 요약하면, 최첨단 변압기 층의 활성화 메모리 비용은 \(2bsh\)이다.

**각 계층의 큰 출력** BPT는 트랜스포머의 메모리 수요를 크게 감소시키지만 각 계층의 출력을 저장 해야 하기 때문에 컨텍스트 길이를 확장 하는 데 여전히 큰 문제가 있습니다. 이 저장소는 모든 요소 간의 상호 작용(n~n개의 상호 작용)을 포함하는 자기 주의의 고유한 특성으로 인해 중요하다. 이러한 저장된 출력이 없으면 후속 레이어의 자체 주의가 계산적으로 비실용적이 되어 각 시퀀스 요소에 대한 재계산이 필요하다. 간단히 말해서, 배치 크기가 1인 1억 개의 토큰을 처리하려면 숨겨진 크기가 1024인 적당한 모델의 경우에도 1000GB 이상의 메모리가 필요하다. 대조적으로, 현대의 GPU와 TPU는 일반적으로 100GB 미만의 고대역폭 메모리(HBM)를 제공하며, 상당한 HBM 확장에 대한 전망은 물리적 한계와 높은 제조 비용에 의해 방해를 받는다.

## 3 Ring Attention with Blockwise Parallel Transformer

우리의 주요 목표는 오버헤드를 추가하지 않고 여러 호스트에 긴 시퀀스를 효율적으로 분산함으로써 개별 장치에 의해 부과되는 메모리 제약을 제거하는 것이다. 이러한 목적을 달성하기 위해, 우리는 블록 병렬 트랜스포머(BPT) 프레임워크에 대한 개선을 제안한다[23]. 상이한 호스트들에 걸쳐 입력 시퀀스를 분배할 때, 각각의 호스트는 그 지정된 블록에 대응하는 블록별 어텐션의 외부 루프의 하나의 엘리먼트, 뿐만 아니라 그 블록에 특정한 피드포워드 네트워크를 실행할 책임이 있다. 이러한 작업은 다른 호스트와의 통신을 필요로 하지 않습니다. 그러나 내부 루프에는 다른 호스트에서 블록을 가져와야 하는 키 값 블록 상호 작용이 수반되는 문제가 발생한다. 각 호스트는 하나의 키 값 블록만 가지고 있기 때문에 다른 호스트에서 블록을 가져오는 순진한 접근법은 두 가지 중요한 문제를 초래한다. 먼저, 시스템이 필요한 키-값 블록들을 수신하기 위해 대기함에 따라 계산 지연을 도입한다. 둘째, 키-밸류 블록들의 축적은 메모리 사용을 증가시키며, 이는 메모리 비용 감소의 목적을 무효화한다.

**링 기반 블록별 주의** 앞에서 언급한 문제를 해결하려면 내부 루프의 키 값 블록 작업의 순열 불변 속성을 활용합니다. 이 속성은 각 블록의 통계가 리스케일링을 위해 올바르게 조합되는 한 쿼리 블록과 키-값 블록 그룹 사이의 자체 주의가 임의의 순서로 계산될 수 있다는 사실에서 비롯된다. 우리는 모든 호스트를 호스트-\(1\), 호스트-\(2\),..., 호스트-\(N\)의 링 구조를 형성하는 것으로 개념화하여 이 속성을 활용한다. 블록 단위의 어텐션 및 피드포워드를 계산함에 따라, 각 호스트는 어텐션 계산에 사용되는 키-값 블록들을 이전 호스트로부터 키-값 블록들을 수신하는 동안 다음 호스트로 동시에 전송함으로써 효율적으로 좌표화하고 블록 단위의 계산과 블록의 전송을 효과적으로 중첩시킨다. 구체적으로, 임의의 호스트-\(i\)에 대해, 질의 블록과 키-값 블록 사이의 주의력 계산 동안, 동시에 이전 호스트-\((i-1)\)로부터 키-값 블록들을 수신하는 동안 키-값 블록들을 다음 호스트-\((i+1)\)로 전송한다. 계산 시간이 키-값 블록을 전송하는 데 필요한 시간을 초과하는 경우, 이는 추가적인 통신 비용을 초래하지 않는다. 이 중첩 메커니즘은 동일한 연산과 기술이 사용될 수 있기 때문에 접근 방식의 전진 및 후진 패스에 모두 적용된다. 또한 이전 연구에서는 통신 비용을 줄이기 위해 링 토폴로지를 활용하여 자기 주의를 계산하는 것을 제안했다[21]. 우리의 작업은 메모리 비용을 실질적으로 줄이기 위해 블록형 병렬 변압기를 활용함으로써 다르다. 다음 섹션에서 보여주듯이, 이것은 훈련 및 추론 동안 컨텍스트 크기의 제로 오버헤드 스케일링을 가능하게 하고 임의로 큰 컨텍스트 크기를 허용한다.

**호스트 간의 산술 강도** 계산과 함께 전송을 중복할 최소 필요 블록 크기를 결정하려면 각 호스트에 \(F\) FLOPS가 있고 호스트 간의 대역폭을 \(B\)로 표시한다고 가정합니다. 우리의 접근법은 원형 구성에서 바로 이전 및 다음 호스트와의 상호작용만을 포함하므로, 우리의 분석은 GPU 전체 토폴로지 및 TPU 토러스 토폴로지 모두에 적용된다는 점에 주목할 필요가 있다. 변수를 고려하자: 블록 크기는 \(c\)로 표시되고 숨겨진 크기는 \(d\)로 표시된다. 블록 단위로 자기주목을 계산할 때, 질의와 키를 이용하여 주의점수를 계산하기 위해서는 \(2dc^{2}\) FLOP가 필요하고, 이러한 주의점수에 값을 곱하기 위해서는 \(2dc^{2}\) FLOP가 추가로 필요하다. 총 계산량은 \(4dc^{2}\) FLOPs이다. 쿼리, 키 및 값의 투영과 블록 방향 피드포워드 연산은 호스트 간의 통신 비용 없이 계산 복잡도만 추가하기 때문에 제외한다. 이러한 단순화는 더 엄격한 조건으로 이어지며 접근법의 유효성을 손상시키지 않는다. 통신 전면에서 키 블록과 값 블록 모두 총 \(2cd\) 바이트가 필요합니다. 따라서, 결합된 통신 요구는 \(4cd\) 바이트이다. 통신과 통신 사이의 중첩을 달성하기 위해

그림 2: **상단 (a):** 원래 트랜스포머와 동일한 모델 아키텍처를 사용하지만 계산을 재구성합니다. 이 다이어그램에서는 호스트의 링에서 각 호스트가 하나의 쿼리 블록을 보유하고 키 값 블록이 블록 단위로 주의 및 피드포워드 계산을 위해 호스트의 링을 횡단한다는 것을 보여줌으로써 이를 설명한다. 주목을 계산할 때 각 호스트는 이전 호스트에서 키 값 블록을 받는 동안 키 값 블록을 다음 호스트로 보냅니다. 통신은 블록 주의 및 피드포워드의 계산과 중첩된다. **아래 (b):** 원본 트랜스포머 블록을 블록 단위로 계산합니다. 각 호스트는 쿼리의 외부 루프를 한 번 반복하고 키 값 블록은 호스트 사이에서 회전합니다. 시각화된 대로 장치는 왼쪽의 첫 번째 쿼리 블록으로 시작하여 수평으로 위치한 키 값 블록 시퀀스를 반복합니다. 키 값 블록과 결합된 쿼리 블록은 출력이 피드포워드 네트워크(cyan box)로 전달되는 자체 주의(yellow box)를 계산하는 데 사용됩니다.

계산, 다음 조건은 유지되어야 합니다. \(4dc^{2}/F\geq 4cd/B\). 이는 \(c\)로 표시된 블록 크기가 \(F/B\)보다 크거나 같아야 함을 의미합니다. 효과적으로, 이것은 블록 크기가 대역폭에 대한 FLOP들의 비율보다 클 필요가 있다는 것을 의미한다.

**메모리 요구 사항** 호스트는 현재 쿼리 블록을 저장하기 위한 하나의 블록 크기, 현재 키 및 값 블록에 대한 두 개의 블록 크기, 키 및 값 블록을 수신하기 위한 두 개의 블록 크기를 포함하여 여러 블록을 저장해야 합니다. 또한, 블록 단위의 어텐션 및 피드포워드의 출력을 저장하는 것은 출력이 쿼리 블록의 형태를 유지하기 때문에 하나의 블록 크기를 필요로 한다. 따라서 총 6개의 블록이 필요하며, 이는 \(6bch\)바이트의 메모리로 변환된다. 블록형 피드포워드 네트워크는 \(2bch\)[23]의 최대 활성화 크기를 갖는다는 점에 주목할 필요가 있다. 결과적으로, 총 최대 활성화 크기는 \(6bch\) 바이트로 유지된다. 표 1은 우리의 방법과 다른 접근법 사이의 메모리 비용에 대한 자세한 비교를 제공한다. 특히, 제안한 방법은 블록 크기 \(c\)에 대한 선형 메모리 스케일링의 장점을 가지며, 입력 시퀀스 길이 \(s\)와 무관하다.

분석 결과, 이 모델은 최소 블록 크기의 6배인 \(s=6c\)의 시퀀스 길이를 가져야 한다. 인기 컴퓨팅 서버에 대한 요구 사항은 표 2에 나타나 있다. 각 호스트에 대해 요구되는 최소 시퀀스 길이(최우측 열)는 6K 내지 10K 사이에서 변하며, 각 호스트에 대한 최소 블록 크기(최우측 열)는 높은 대역폭 상호연결을 갖는 TPU 및 GPU에 대해 약 1K이다. 낮은 대역폭을 제공하는 InfiniBand를 통해 연결된 GPU의 경우 요구 사항이 더 엄격합니다. 이러한 요구 사항은 데이터 및 텐서 병렬과 같은 병렬성과 메모리 효율적인 블록 주의 및 피드포워드[30, 9, 23]를 충족하기 쉬우며, 이는 실험 섹션 5에서 보여질 것이다.

**알고리즘 및 구현** 알고리즘 1은 알고리즘의 의사 코드를 제공합니다. 링 어텐션은 메모리 효율적인 변압기를 위한 기존 코드와 호환됩니다. 링 어텐션은 각 호스트에서 로컬로 사용 가능한 메모리 효율적인 계산을 호출하고 호스트 간의 키 값 블록 통신을 블록 단위로 중첩하면 됩니다. 우리는 집단 작업 jax.lax.ppermute를 사용하여 주변 호스트 간에 키 값 블록을 보내고 받습니다. Jax 구현은 부록 A에 제공된다.

\begin{table}
\begin{tabular}{l r r r} \hline \hline Layer Type & Self-Attention & FeedForward & Total \\ \hline Vanilla & \(2bns^{2}\) & \(8bsh\) & \(2bhs^{2}\) \\ Memory efficient attention & \(2bsh+4bch\) & \(8bsh\) & \(8bsh\) \\ Memory efficient attention & & \(2bsh\) & \(2bsh\) & \(2bsh\) \\ and feedforward & & \(2bsh\) & \(2bsh\) & \(2bsh\) \\ \hline Ring Attention & \(6bch\) & \(2bch\) & \(6bch\) \\ \hline \hline \end{tabular}
\end{table}
표 1: 상이한 트랜스포머 아키텍처들 간의 최대 활성화 크기들의 비교. 여기서, \(b\)는 배치 크기, \(h\)는 은닉 차원, \(n\)는 머리 수, \(s\)는 시퀀스 길이, \(c\)는 블록 크기, 블록 크기(\(c\))는 입력 시퀀스 길이(\(s\)와 무관하다. 비교는 바닐라 트랜스포머 [37], 메모리 효율적인 어텐션 [30], 메모리 효율적인 어텐션 및 피드포워드 [23]와 제안된 접근법 Ring Attention을 비교한다. 숫자는 _bfloat16_ 정밀도를 가정하여 계층당 바이트로 표시됩니다.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline \multirow{2}{*}{Spec Per Host} & \multirow{2}{*}{FLOPS} & \multirow{2}{*}{HBM} & \begin{tabular}{c} Interconnect \\ Bandwidth \\ \end{tabular} & \begin{tabular}{c} Minimal \\ Blocksize \\ \end{tabular} &
\begin{tabular}{c} Minimal \\ Sequence Len \\ \end{tabular} \\ \hline  & (TF) & (GB) & (GB/s) & (\(\times 1\mathrm{e}3\)) & (\(\times 1\mathrm{e}3\)) \\ \hline A100 NVLink & 312 & 80 & 300 & 1.0 & 6.2 \\ A100 InfiniBand & 312 & 80 & 12.5 & 24.5 & 149.5 \\ TPU v3 & 123 & 16 & 112 & 1.1 & 6.6 \\ TPU v4 & 275 & 32 & 268 & 1.0 & 6.2 \\ TPU v5e & 196 & 16 & 186 & 1.1 & 6.3 \\ \hline \hline \end{tabular}
\end{table}
표 2: 각 장치에 필요한 최소 시퀀스 길이. 인터커넥트 대역폭은 호스트 간의 단방향 대역폭, _i.e._, GPU 간의 NVLink/InfiniBand 대역폭 및 TPU 간의 ICI 대역폭이다. 최소 블록 크기는 \(c=\) FLOPS\(/\)Bandwidth 및 최소 시퀀스 길이 \(s=6c\)가 필요합니다.

## 4 Setting

우리는 최대 시퀀스 길이와 모델 플롭 활용도를 벤치마킹하여 Transformer 모델 개선에 Ring Attention을 사용하는 것이 미치는 영향을 평가한다.

**모델 구성** 연구는 LLaMA 아키텍처를 기반으로 하며 실험에서 3B, 7B, 13B 및 30B 모델 크기를 고려 합니다.

**기준선.** 주의 행렬을 구체화하여 자기 주의력을 계산하고 피드포워드 네트워크를 정상적으로 계산하는 바닐라 변압기 [37], 메모리 효율적인 주의력을 사용하는 변압기 [30] 및 효율적인 CUDA 구현 [9], 메모리 효율적인 주의력과 피드포워드 모두를 사용하는 변압기 [23]와 비교하여 방법을 평가한다.

**훈련 구성** 모든 메서드에 대해 이전 작업 [30; 23]에 따라 주의 및 피드포워드 모두에 전체 그래디언트 검사점 [5]를 적용합니다. 실험은 GPU와 TPU 모두에 대해 이루어집니다. GPU의 경우 8개의 GPU가 있는 단일 DGX A100 서버와 32개의 A100 GPU가 분산된 서버를 모두 고려한다. 또한 오래된 세대 TPUv3에서 새로운 세대 TPUv4 및 TPUv5e에 이르기까지 TPU를 실험한다. 우리는 우리의 모든 결과가 혼합 정밀도 대신 완전 정밀도를 사용하여 얻어진다는 점에 주목한다.

## 5 Results

실험에서 우리의 주요 목표는 가속기 메모리 내에서 지원되는 최대 시퀀스 길이, 모델 플롭 활용률 및 처리량을 포함하여 여러 주요 메트릭에 걸쳐 링 어텐션의 성능을 종합적으로 평가하는 것이다. 링 어텐션의 성능을 바닐라 변압기[37], 메모리 효율적인 어텐션이 있는 변압기[30], 메모리 효율적인 어텐션과 피드포워드 모두 있는 변압기[23]를 포함한 여러 기본 모델과 모델 크기와 가속기 구성에 따라 비교한다.

### Max Context Size 평가

우리는 사전 종단 간 훈련에서 널리 사용되는 완전 샤딩 텐서 병렬성(FSDP) [11]을 사용하여 최대 지원 컨텍스트 길이를 평가한다[36; 12]. 우리의 접근법은 텐서 병렬성과 무관하기 때문에 우리의 평가에서는 텐서 병렬성이 고려되지 않는다는 점에 주목한다. 실행자는 5.2절에서 보여 줄 텐서 병렬 처리와 방법을 결합할 수 있습니다. FSDP를 사용하면 기준선과 접근 방식에 대한 토큰에서 동일한 배치 크기를 설정할 수 있으므로 공정한 비교를 보장합니다. 구체적으로, \(n\) 장치에서 FSDP를 사용하여 \(l\)의 시퀀스 길이를 제공하는 기준선의 모델을 샤드한다. 토큰의 총 배치 크기는 \(nl\)입니다. 링 어텐션과 함께 FSDP를 사용하여 시퀀스 길이를 \(\frac{nl}{m}\) 및 \(m\) 시퀀스로 확장한다. 이는 토큰의 총 배치 크기가 동일하게 유지되지만 Ring Attention을 사용하면 훨씬 더 큰 컨텍스트 크기를 사용할 수 있습니다. 표 3은 실험 결과를 요약한 것이다.

당사의 링 어텐션 모델은 지속적으로 기준선을 능가하여 다양한 하드웨어 설정에서 뛰어난 확장성을 제공합니다. 예를 들어 32개의 A100 GPU를 사용하여 7B 모델에 대한 컨텍스트 크기에서 100만 개 이상의 토큰을 달성했으며, 이는 이전 베스트보다 32배 개선되었다. 또한 TPUv4-512와 같은 더 큰 가속기를 사용할 때 Ring Attention은 컨텍스트 크기를 256배 증가시킬 수 있으며 3천만 개 이상의 토큰의 훈련 시퀀스를 허용한다. 또한, 본 논문에서 제안한 링 어텐션 모델은 8 A100에서 8배 향상, TPUv3-512에서 256배 향상으로 장치 수에 따라 선형적으로 확장된다. 블록 주의와 피드포워드를 사용하여 \(n\) GPU에서 컨텍스트 크기 \(s\)로 모델을 학습시킬 수 있다면, 본 논문에서 제안한 링 어텐션 방법을 사용하면 컨텍스트 크기 \(ns\)로 모델을 학습시킬 수 있게 된다.

### 모델 플롭 활용도 평가

Jax SPMD를 사용하여 LLaMA와 OpenLLaMA [36; 12]에 이어 완전 샤디드 데이터 병렬성(FSDP) [11]과 텐서 병렬성을 사용하여 표준 훈련 환경에서 링 어텐션의 모델 플롭 활용도(MFU)를 평가한다. 토큰의 배치 크기는 8/32x A100에서 2M, TPUv4-256에서 4M이다. 본 연구의 목표는 모델 크기와 컨텍스트 길이가 중요한 성능 메트릭인 MFU에 미치는 영향을 조사하는 동시에 접근 방식의 이점을 강조하는 것이다. 표 1은 다양한 모델 크기와 컨텍스트 길이에 대한 MFU에 대한 실험 결과를 보여준다. 최첨단 메모리 효율 트랜스포머 BPT[23]를 사용하여 달성된 MFU를 제시하고, 이러한 결과를 기반으로 예상 MFU와 비교하고, 우리의 접근법(Ring Attention)으로 얻은 실제 MFU를 입증한다. 공정한 비교를 위해 BPT와 우리의 접근법은 GPU와 TPU 모두에서 동일한 BPT 구현을 기반으로 한다.

링 어텐션은 셀프 어텐션에 대한 컨텍스트 크기를 훨씬 더 길게 트레이닝하여, 베이스라인 모델에 비해 셀프 어텐션 FLOP가 더 높아진다. 자기 주의는 피드포워드보다 MFU가 낮기 때문에 링 주의는 기준 모델보다 MFU가 낮을 것으로 예상된다. 우리의 방법은 명확하다.

\begin{table}
\begin{tabular}{l|c c c c|c} \hline \hline  & \multicolumn{4}{c|}{**Max context size supported (\(\times 1\mathrm{e}3\))**} \\ \cline{2-6}  & Vanilla & Memory & Memory Efficient & Ring Attention & Ours \\  & & Efficient Attn & Attn and FFN & (Ours) & vs SOTA \\ \hline
8x A100 & & & & & & & & & & & & & & & &
3B & 4 & 32 & 64 & **512** & 8x \\
7B & 2 & 16 & 32 & **256** & 8x \\
13B & 2 & 4 & 16 & **128** & 8x \\ \hline
32x A100 & & & & & & & & & & & & & & & & & &
7B & 4 & 64 & 128 & **4096 & 32x \\
13B & 4 & 32 & 64 & **2048** & 32x \\ \hline TPUv3-512 & & & & & \\
7B & 1 & 4 & 8 & **2048** & 256x \\
13B & 1 & 2 & 8 & **1024** & 128x \\ \hline TPUv4-1024 & & & & & \\
3B & 8 & 16 & 32 & **16384 & 512x \\
7B & 4 & 8 & 16 & **8192** & 512x \\
13B & 4 & 8 & 16 & **4096** & 256x \\
30B & 2 & 4 & 8 & **2048** & 256x \\ \hline TPUv5e-256 & & & & & \\
3B & 4 & 8 & 32 & **4096** & 128x \\
7B & 2 & 8 & 16 & **2048** & 128x \\ \hline \hline \end{tabular}
\end{table}
표 3: 완전 샤딩된 데이터 병렬성 및 다양한 변압기 아키텍처를 사용하는 종단간 훈련에서 지원되는 최대 컨텍스트 길이. 다양한 모델 크기와 가속기를 보여줍니다. 기준선은 바닐라 변압기[37], 메모리 효율적인 주의력을 갖는 변압기[30], 및 메모리 효율적인 주의력 및 피드포워드[23]를 갖는 변압기이다. 컨텍스트 크기는 토큰(1e3)에 보고됩니다. 우리의 링 어텐션은 기준선을 훨씬 능가하며 이전 최신 기술보다 최대 장치 카운트 시간이 긴 훈련 시퀀스를 가능하게 합니다.

맥락 길이가 상당히 긴 훈련을 가능하게 하면서 MFU를 유지하는 측면에서 이점이 있다. 표 5.1에서 볼 수 있듯이 이전 최신 기술에 대한 접근법을 비교할 때 MFU 또는 처리량을 손상시키지 않고 매우 큰 컨텍스트 모델을 훈련할 수 있음이 분명하다.

### Context RL 성능에 미치는 영향

본 논문에서는 트랜스포머를 이용한 시행착오 RL 경험을 학습하기 위해 Ring Attention을 적용한 결과를 제시한다. 우리는 표 5에 결과를 보고하며, 여기서 6가지 다른 작업에 걸쳐 ExoRL 벤치마크에서 제안된 모델을 평가한다. ExoRL에서 ExoRL [39]에 따라 누적 수익률을 보고합니다. BC, DT [4], AT [22], AT를 메모리 효율 주의 [30] (AT+ME), AT를 블록 병렬 변압기 [23] (AT+BPT), AT를 링 주의 [AT+Ring 주의]와 비교한다. BC, DT, AT의 숫자는 ExoRL 및 AT 용지에서 가져온 것이다. AT + 링 주의 번호는 자체 실행됩니다. ExoRL 데이터는 매우 다양하기 때문에 감독되지 않은 RL을 사용하여 수집되었으며[19], TD 학습이 가장 잘 수행되는 반면 행동 복제는 어려움을 겪는 것으로 밝혀졌다[39]. AT [22]는 재표지된 타겟 리턴을 갖는 다중 궤적 상의 컨디셔닝 트랜스포머가 TD 학습으로 경쟁 결과를 달성할 수 있음을 보여준다. 자세한 내용은 문서를 참조하십시오. 우리는 더 큰 조건에서 컨디셔닝하여 AT의 성능을 향상시키기 위해 Ring Attention을 적용하는 데 관심이 있다.

\begin{table}
\begin{tabular}{l r r|r r r r} \hline \hline
**ExoRL** & **BC-10\%** & **DT** & **AT + ME** & **AT + BPT** & **AT + BPT** & **AT + RA** \\ \hline
**Task** & & N Trajs = 32 & N Trajs = 32 & N Trajs = 128 & N Trajs = 128 \\ \hline Walker Stand & 52.91 & 34.54 & oom & 95.45 & oom & 98.23 \\ Walker Run & 34.81 & 49.82 & oom & 105.88 & oom & 110.45 \\ Walker Walk & 13.53 & 34.94 & oom & 78.56 & oom & 78.95 \\ Cheetah Run & 34.66 & 67.53 & oom & 178.75 & oom & 181.34 \\ Jaco Reach & 23.95 & 18.64 & oom & 87.56 & oom & 89.51 \\ Cartpole Swingup & 56.82 & 67.56 & oom & 120.56 & oom & 123.45 \\ \hline
**Total Average** & 36.11 & 45.51 & oom & 111.13 & oom & **113.66** \\ \hline \hline \end{tabular}
\end{table}
표 5: RL에서 변압기 개선에 대한 링 어텐션의 적용. BC와 DT는 바닐라 주의력을 사용한다. AT + ME는 메모리 효율적인 주의력을 사용하고 AT + BPT는 블록 병렬 변압기를 사용한다. AT + RA는 Ring Attention을 사용함을 나타냅니다.

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline  & Model size & 7B & 13B & 13B & 30B & 65B \\ \cline{2-6}  & Compute & 8x A100 & 8x A100 & 32x A100 & TPUv4-1024 & TPUv4-1024 \\ \hline Memory efficient & Context size & & & & & \\ attention \& FFN & (\(\times 1\)e3) & 32 & 16 & 64 & 16 & 8 \\ Ring Attention &
\begin{tabular}{l} Context size \\ (\(\times 1\)e3) \\ \end{tabular} & 256 & 128 & 2048 & 2048 & 1024 \\ \hline \hline \end{tabular}
\end{table}
표 4: 상이한 트레이닝 구성을 갖는 모델 플롭 활용률(MFU: Model Flops utilization): 모델 크기, 컴퓨팅 및 컨텍스트 길이. 링 주의를 사용하면 오버헤드가 거의 없는 큰 입력 컨텍스트 크기(4M 이상)에서 큰 모델(7B-65B)을 훈련할 수 있습니다.

이전 연구에서는 32개의 궤적보다 궤적의 수가 더 많았다. 각각의 궤적은 \(1000\times4\)의 길이를 가지며, \(1000\)은 시퀀스 길이이고 \(4\)는 리턴-상태-행동-보상을 갖는다는 점에 주목할 필요가 있다. 표 5의 결과는 시퀀스 길이(궤적 수)를 확장함으로써 AT + 링 어텐션이 6개의 태스크 모두에서 BPT를 사용한 오링어텐션 AT를 일관되게 능가하여 BPT 모델의 총 평균 수익률 111.13을 가진 AT에 비해 총 평균 수익률 113.66을 달성했음을 보여준다. 결과는 긴 시퀀스를 사용한 훈련 및 추론을 위한 링 어텐션의 이점을 보여준다.

### LLM 성능에 미치는 영향

본 논문에서 제안한 방법을 LLaMA 모델에 적용하여 보다 긴 문맥에 대한 링 어텐션을 평가한다. 이 실험에서는 수백만 개의 컨텍스트 토큰으로 학습을 가능하게 하는 반면, 클라우드 컴퓨팅 예산에 대한 제약으로 인해 컨텍스트 길이를 512K 토큰으로 제한하는 LLaMA-13B 모델에 대한 미세 조정을 수행했다. 이 미세 조정은 이전 작업 [6; 13]에 설명된 대로 방법론에 따라 ShareGPT 데이터 세트를 사용하여 32개의 A100 GPU에서 수행되었다. 그런 다음 라인 검색 테스트에서 미세 조정된 모델을 평가했다[20]. 이 테스트에서, 모델은 긴 문서에서 숫자를 정확하게 검색할 필요가 있으며, 태스크는 검색 정확도에 반영되어 긴 문맥에서 텍스트 생성, 검색 및 정보 연관의 능력을 효과적으로 포착할 수 있다. 도 3은 다양한 컨텍스트 길이들(토큰들에서 측정됨)에 걸쳐 상이한 모델들에 대한 정확도 결과들을 제시한다. 특히, 우리의 모델인 Ring Attention-13B-512K는 긴 컨텍스트에도 높은 정확도 수준을 유지한다는 점에서 두드러진다. GPT3.5-터보-16K, 비쿠나-16B-16K 및 클로드-2-100K는 짧은 컨텍스트 길이 내에서 경쟁적 정확도를 보여준다. 그러나 확장된 컨텍스트 길이를 처리할 수 없습니다.

## 6 관련 작업

트랜스포머는 AI 분야에서 상당한 관심을 받았고 수많은 최첨단 모델의 중추가 되었다. 여러 연구에서 트랜스포머의 메모리 한계를 해결하고 더 넓은 범위의 문제에 적용할 수 있도록 메모리 효율적인 기술을 탐구했다. 타일링 기법[24]을 사용하여 블록 단위로 정확한 자기 주목(self-attention)을 계산하는 것은 메모리 효율적인 주목 메커니즘[30] 및 그것의 효율적인 CUDA 구현[9]의 개발로 이어졌고, 피드포워드 및 자기 주목 블록 단위 컴퓨팅을 제안하는 블록 병렬 변압기[23]는 메모리 요구 사항을 크게 감소시켰다. 이러한 발전에 따라, 우리의 작업은 트랜스포머에 대한 메모리 효율적인 계산 범주에 속한다. 다른 연구에서는 주의 메커니즘의 근사를 조사했지만 이러한 노력은 종종 차선책 결과를 산출하거나 확장 중에 문제에 직면했다. 이러한 기술에 대한 심층적인 검토를 위해 우리는 설문 조사를 참조하는 것이 좋다[26; 35]. 또 다른 연구 방법은 데이터 병렬성[10], 텐서 병렬성[34], 파이프라인 병렬성[27; 15; 28], 시퀀스 병렬성[21; 18; 17], 및 FSDP[11; 31]을 포함하는 다양한 병렬성 방법을 탐구한다. 자기 주의의 활성화는 대용량 컨텍스트 모델에 상당한 양의 메모리를 필요로 한다. 텐서 병렬성은 활성화 메모리의 일부만을 감소시킬 수 있고 시퀀스 병렬성은 계산과 완전히 중첩될 수 없는 상당한 통신 오버헤드를 도입한다. 이전 연구에서는 시퀀스 및 어텐션 헤드를 따라 샤딩을 연구하고 최적화된 전체 토폴로지를 통해 시퀀스를 수집하여 감소된 통신을 달성했다[17]. 그러나 이 방법은 어텐션 헤드의 수에 의해 제한되며 각 장치에서 전체 시퀀스를 수집해야 한다. 이에 비해, 본 논문에서 제안하는 방법은 블록 단위의 연산과 통신을 완전히 중첩하여 확장성을 높인다. Prior

그림 3: 장거리 선 검색 작업에 대한 서로 다른 모델의 비교.

작업은 링 토폴로지[21]를 사용하여 자기 주의를 계산하기 위한 시퀀스 병렬성을 확장하며, 이는 표준 시퀀스 병렬성에 비해 통신 비용을 감소시킨다. 그러나, 연산과 중복되는 통신은 산술 강도의 제약으로 인해 여전히 어렵다. 통신 오버헤드는 이 접근법을 대 상황 시나리오에서 훈련 및 추론에 사용할 수 없게 만든다. 본 논문에서는 블록 단위의 병렬 트랜스포머를 이용하여 디바이스 간에 블록 단위의 주의와 피드포워드를 분배하고, 질의-키-값 블록 및 피드포워드의 계산과 함께 호스트의 순환에서 키-값 블록의 통신과 동시에 중첩함으로써 메모리 비용을 실질적으로 감소시키고, 오버헤드가 0인 디바이스 카운트 시간보다 더 큰 컨텍스트 크기를 허용한다. 컴퓨팅과의 중첩 통신은 고성능 컴퓨팅 문헌에서 연구되었다[7; 38; 8, _inter alia_]. 링 통신은 다른 병렬 컴퓨팅 시나리오에서 응용 프로그램을 발견했지만 [2; 16; 14; 33] 본 연구는 트랜스포머에서 사용되는 자기 주의에 적용할 수 있음을 보여주고 블록 단위의 계산과 통신을 중첩하여 상당한 오버헤드를 추가하지 않고 트랜스포머 훈련과 추론에 효율적으로 적합하도록 하는 첫 번째 작업으로 눈에 띈다.

## 7 Conclusion

결론적으로, 우리는 최신 AI 모델의 백본인 트랜스포머의 메모리 요구 사항을 줄이기 위한 메모리 효율적인 접근법을 제안한다. 제안된 방법은 성능을 유지하면서 컨텍스트 길이가 장치 수에 따라 선형적으로 확장될 수 있도록 하여 개별 장치에 의해 부과되는 메모리 병목 현상을 제거한다. 언어 모델링 및 강화 학습에 대한 광범위한 실험을 통해 그 효과를 입증하여 이전 메모리 효율적인 트랜스포머보다 장치 카운트 시간이 최대인 트레이닝 시퀀스가 주목에 대한 근사치를 만들지 않고도 컨텍스트 길이 1억을 초과할 수 있음을 보여준다. 미래 전망 측면에서, 무한에 가까운 맥락의 가능성은 큰 비디오-오디오-언어 모델, 확장된 피드백 및 시행착오를 통한 학습, 코드베이스의 이해 및 생성, 유전자 서열과 같은 과학적 데이터를 이해하기 위해 AI 모델을 적응시키고, 데이터를 수집하는 링크에서 강력한 추론을 개발하는 등 흥미진진한 기회를 광범위하게 도입한다.

## Acknowledgments

이 프로젝트는 해군 연구소의 보조금 N00014-21-1-2769에 의해 부분적으로 지원됩니다. 통찰력 있는 토론과 피드백에 대한 BAIR 및 RLL 커뮤니티에 감사를 표합니다. 우리는 또한 TPU에 대한 우리의 질문을 해결하고 이 작업의 초기 버전에 대한 통찰력 있는 피드백을 제공한 데이비드 패터슨에게 감사한다. 잭스 개발팀의 야시 카타리야와 샤라드 비크람에게 감사의 말씀을 전합니다 잭스 관련 질문을 도와주셔서요 또한 이 작업에 대한 귀중한 피드백에 대해 트리 다오에게 감사드립니다. TPU에 대한 액세스 권한을 부여해 주신 구글 TPU 리서치 클라우드에 감사드립니다.

## References

* [1] Anthropic. Introducing claude, 2023. URL [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude).
* [2] Christian Bischof. _Parallel computing: Architectures, algorithms, and applications_, volume 15. IOS Press, 2008.
* [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [4] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [5] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. _arXiv preprint arXiv:1604.06174_, 2016.

* Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. _See [https://vicuna.lmsys.org_](https://vicuna.lmsys.org_), 2023.
* Danalis et al. [2005] Anthony Danalis, Ki-Yong Kim, Lori Pollock, and Martin Swany. Transformations to parallel codes for communication-computation overlap. In _SC'05: Proceedings of the 2005 ACM/IEEE conference on Supercomputing_, pages 58-58. IEEE, 2005.
* Danalis et al. [2009] Anthony Danalis, Lori Pollock, Martin Swany, and John Cavazos. Mpi-aware compiler optimizations for improving communication-computation overlap. In _Proceedings of the 23rd international conference on Supercomputing_, pages 316-325, 2009.
* Dao et al. [2022] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. _Advances in Neural Information Processing Systems_, 35:16344-16359, 2022.
* Dean et al. [2012] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. _Advances in neural information processing systems_, 25, 2012.
* Facebook [2023] Facebook. 완전 샤딩된 데이터 병렬: GPU가 적은 빠른 AI 교육 -- engineering.fb.com [https://engineering.fb.com/2021/07/15/open-source/fsdp/](https://engineering.fb.com/2021/07/15/open-source/fsdp/), 2023.
* Geng and Liu[2023] Xinyang Geng and Hao Liu. Openllama: llama의 개방형 복제본, 2023. _URL https://github. con/openlm-research/open_llama_, 2023.
* Geng et al. [2023] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. Koala: A dialogue model for academic research. _Blog post, April_, 1, 2023.
* Gibiansky [2017] Andrew Gibiansky. hpc 기술을 딥 러닝에 가져옵니다. _ (주)바이두테크 Rep._ , 2017.
* Huang et al. [2019] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukoJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. _Advances in neural information processing systems_, 32, 2019.
* Hursey and Graham [2011] Joshua Hursey and Richard L Graham. 오류 허용 mpi 응용 프로그램 구축: 링 통신 예제. *2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum_, pages 1549-1556. IEEE, 2011.
* Jacobs et al. [2023] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. _arXiv preprint arXiv:2309.14509_, 2023.
* Korthikanti et al. [2022] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. _arXiv preprint arXiv:2205.05198_, 2022.
* Laskin et al. [2021] Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, Lerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. _arXiv preprint arXiv:2110.15191_, 2021.
* Li et al. [2023] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
* Li et al. [2023] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long sequence training from system perspective. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2391-2404, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.134. URL [https://aclanthology.org/2023.acl-long.134](https://aclanthology.org/2023.acl-long.134).

* [22] Hao Liu and Pieter Abbeel. Emergent agentic transformer from chain of hindsight experience. _International Conference on Machine Learning_, 2023.
* [23] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for large context models. _Advances in neural information processing systems_, 2023.
* [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. _arXiv preprint arXiv:1805.02867_, 2018.
* [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL [https://www.mosaicml.com/blog/mpt-7b](https://www.mosaicml.com/blog/mpt-7b).
* [26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? _arXiv preprint arXiv:2102.11972_, 2021.
* [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In _Proceedings of the 27th ACM Symposium on Operating Systems Principles_, pages 1-15, 2019.
* [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient pipeline-parallel dnn training. In _International Conference on Machine Learning_, pages 7937-7947. PMLR, 2021.
* [29] OpenAI. Gpt-4 technical report, 2023.
* [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. _arXiv preprint arXiv:2112.05682_, 2021.
* [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16. IEEE, 2020.
* [32] J. Schulman, B. Zoph, C. Kim, J. Hilton, J. Menick, J. Weng, J. F. C. Uribe, L. Fedus, L. Metz, M. Pokorny, R. G. Lopes, S. Zhao, A. Vijayvergiya, E. Sigler, A. Perelman, C. Voss, M. Heaton, J. Parish, D. Cummings, R. Nayak, V. Balcom, D. Schnurr, T. Kaftan, C. Hallacy, N. Turley, N. Deutsch, and V. Goel. Chatgpt: Optimizing language models for dialogue. _OpenAI Blog_, 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
* [33] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in tensorflow. _arXiv preprint arXiv:1802.05799_, 2018.
* [34] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_, 2019.
* [35] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? _arXiv preprint arXiv:2207.10551_, 2022.
* [36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.

* [38] Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hechtman, Dehao Chen, Karthik Srinivasa Murthy, Marcello Maggioni, Qiao Zhang, et al. Overlap communication with dependent computation via decomposition in large deep learning models. In _Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1_, pages 93-106, 2022.
* [39] Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel, Alessandro Lazaric, and Lerrel Pinto. Don't change the algorithm, change the data: Exploratory data for offline reinforcement learning. _arXiv preprint arXiv:2201.13425_, 2022.

Code

Jax에서 Ring Attention의 구현은 그림 4에 나와 있다. 우리는 defvjp 함수를 사용하여 순방향 및 역방향 패스를 모두 정의하고, 집합 연산 jax.lax.ppermute를 사용하여 호스트의 링 사이에서 키-값 블록의 교환을 용이하게 한다. 제공된 코드 조각은 링 어텐션의 필수 구성 요소를 강조 표시합니다. [https://github.com/lhao49/llm_large_context](https://github.com/lhao49/llm_large_context)에서 Ring Attention의 전체 코드를 제공합니다.

TPU 또는 높은 대역폭 인터 연결이 있는 GPU 클러스터에서 대규모 종단 간 훈련을 위해 FSDP를 하드 대형 모델에 사용하고 링 어텐션을 사용하여 대규모 컨텍스트를 달성하는 것이 좋습니다. 총 배치 크기가 너무 크면 텐서 병렬 처리를 추가하여 전체 배치 크기를 줄입니다. 병렬 정도는 코드베이스 내에서 mesh_dim 파라미터를 이용하여 조절될 수 있다. 예를 들어 512x A100과 같은 512 개의 장치를 사용 하는 설정을 고려 합니다. 모델 크기가 30B 인 경우 8 개의 장치에서 분할 하 고 나머지 32 개의 장치를 링 어텐션에 할당할 수 있습니다. 이 설정을 사용하면 링 어텐션을 사용하지 않은 경우보다 컨텍스트 크기를 32배 더 확장할 수 있습니다. 반대로 7B 또는 3B 크기의 모델의 경우 FSDP가 필요하지 않다. 이는 512개의 모든 장치를 독점적으로 활용하여 링 어텐션을 사용하여 컨텍스트를 512배 확장할 수 있음을 의미합니다. 8x A100 GPU를 사용할 때 256K 컨텍스트 크기를 허용하는 결과를 기반으로 512개의 A100 GPU를 사용하여 잠재적인 컨텍스트 크기를 1,600만 개로 확장할 수 있음을 제안한다.

## 부록 B 실험 세부 정보

### 컨텍스트 길이 평가

섹션 5.1에 제시된 실험 결과에서, 우리는 GPU 또는 TPU 장치에 걸쳐 모델을 분할하기 위해 완전히 샤딩된 텐서 병렬성(FSDP)을 사용했다. 우리의 평가는 일반적으로 사용되는 FSDP 훈련 시나리오에서 최대 달성 가능한 시퀀스 길이를 결정하는 데 중점을 두었다. TPU의 경우, default training configuration을 활용하였으며, matmul 연산을 bfloat16 형식으로 수행하였고, weight accumulation은 float32에서 수행하였다. 반면 GPU의 경우 default setup을 활용하였으며, default setup은 모든 연산이 float32에서 수행되었다.

### MFU 평가

5.2절에 제시된 평가에서 토큰의 배치 크기는 GPU의 배치당 200만 개, TPU의 배치당 400만 개이다. 훈련은 Jax SPMD와 함께 FSDP [11]을 사용하여 수행되었다. 그래디언트 체크포인팅 [5]의 경우, 주의 및 피드포워드 네트워크(FFN)를 위한 체크포인팅 정책으로서 nothing_saveable을 사용하였다. 자세한 내용은 Jax 문서를 참조하십시오.

### 줄 검색 평가

섹션 5.4에 제시된 평가에서 클라우드 컴퓨팅 예산에 대한 제약으로 인해 컨텍스트 길이를 512K 토큰으로 제한하는 LLaMA-13B 모델 [36]을 미세 조정했으며, 교육은 32x A100 80GB 클라우드 GPU에서 수행되었다. 우리는 이전 작업[6; 13]에 요약된 방법론에 따라 미세 조정을 위해 공개 API와 함께 ShareGPT.com에서 수집한 사용자 공유 대화를 사용한다. ShareGPT는 사용자들이 자신의 ChatGPT 대화를 공유할 수 있는 웹사이트이다. 데이터 품질을 보장하기 위해 HTML을 다시 마크다운으로 변환하고 일부 부적절하거나 품질이 낮은 샘플을 필터링하여 데이터 청소 후 125K 대화를 수행합니다.

## 부록 C 추론 요구 사항

표 2에서 훈련 중 계산과 통신을 중첩하기 위해 필요한 최소 시퀀스 길이를 제공한다. 링 어텐션은 장치 수에 따라 선형으로 확장되는 컨텍스트 크기의 쉬운 훈련을 가능하게 한다. 쿼리 토큰의 수가 하나인 자기회귀 추론보다 메모리가 많이 소요되는 훈련에 초점을 맞추지만, 추론에도 Ring Attention을 적용할 수 있다. 예를 들어, 32x TPUv5e 상에서 LLaMa 7B를 서빙하는, 종래의 접근법은 어텐션 헤드 차원을 따라 모델을 분배하는 것이며, 각각의 디바이스는 하나의 어텐션 헤드를 컴퓨팅한다. 배치 크기가 1이라고 가정하면 키 값 캐시 활성화 크기로 인해 최대 256K 컨텍스트 길이를 제공할 수 있습니다. 링 주의는 디바이스의 링 사이에서 키-값 캐시를 순환시킴으로써 32배 더 큰 컨텍스트를 허용할 수 있다. 연산과 통신을 중첩하려면 d2/F >= 2*d2/B가 필요하며, 여기서 B/F >= 2이다. 대역폭은 186GB/s이고 플롭은 196 TFLOPs이며 def_ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)를 가정한다:
2if float32_logits:
3q, k = q.astype(jnp.float32), k.astype(jnp.float32)
4batch, q_len, num_heads, dim_per_head = q.shape
5batch, kv_len, num_heads, dim_per_head = k.shape
6numerator = jnp.zeros((batch, q_len, num_heads, dim_per_head)).astype(q.dtype)
7denominator = jnp.zeros((batch, num_heads, q_len)).astype(q.dtype)
8axis_size = lax.psum(1, axis_name)
9block_size = q_len # assures this function is pre-sharded inside shard_map query_chunk_size = blockwise_kwargs["query_chunk_size"]
10key_chunk_size = blockwise_kwargs["key_chunk_size"]
11defscan_kw_block(carry, idx):
12prev_max_score, 분자, 분모, k, v = 캐리
13attn_bias_slice = lax.dynamic_slice_in_dim(attn_bias,
14(lax.axis_index(axis_name) - idx) % axis_size * kv_len, kv_len, axis=-1)
15q_block_idx = lax.axis_index(axis_name) - idx) % axis_size
16q_chunk_idx_start = q_block_idx * (block_size // query_chunk_size)
17k_chunk_idx_start = k_block_idx * (block_size // key_chunk_size)
18numerator, denominator, max_score = _blockwise_attention_fwd(q, k, v)
19(숫자, 분모, prev_max_score), q_chunk_idx_start, k_chunk_idx_start,
20bias-attn_bias_slice, **blockwise_kwargs**)
21k, v = map(lambda x: lax.permux(x, axis_name, perm=[(i, (i+1) % axis_size)
22for i in range(axis_size)]), (k, v)
23return (max_score, numerator, denominator, k, v), None
24prev_max_score = jnp.full(batch, num_heads, q_len), -jnp.inf).astype(q.dtype)
25(max_score, 분자, 분모, _, _ ), _ = lax.scan(scan_kw_block,
26init=(prev_max_score, 분자, 분모, k, v), xs=jnp.arange(0, axis_size))
27output = 분자/재배치(분모, 'bhq -> bqh')[..., 없음]
28return output.astype(v.dtype), (output, q, k, v, attn_bias, 분모, max_score)
29
30
31def_ring_attention_bwd(axis_name, float32_logits, blockwise_kwargs, res, g):
32output, q, k, v, attn_bias, denominator, max_score = res
33batch, kv_len, num_heads, dim_per_head = k.shape
34axis_size = lax.psum(1, axis_name)
35dq = jnp.zeros_like(q,dtype=jnp.float32)
36dk = jnp.zeros_like(k,dtype=jnp.float32)
37dv = jnp.zeros_like(v,dtype=jnp.float32)
38query_chunk_size = blockwise_kwargs["query_chunk_size"]
39key_chunk_size = blockwise_kwargs["key_chunk_size"]
40block_size = q.shape[1] # assures this function is pre-sharded inside shard_map
41def scan_kv_block(carry, idx):
42dq, dk, dv, kv= 캐리
43attn_bias_slice = lax.dynamic_slice_in_dim(attn_bias,
44(lax.axis_index(axis_name) - idx) % axis_size * kv_len, kv_len, axis=-1)
45q_block_idx = lax.axis_index(axis_name)
46k_block_idx = lax.axis_index(axis_name) - idx) % axis_size
47q_chunk_idx_start = q_block_idx * (block_size // query_chunk_size)
48k_chunk_idx_start = k_block_idx * (block_size // key_chunk_size)
49dq, dk, dv = _blockwise_attention_bwd(q, k, v, g, (dq, dk, dv, output, 분모, max_score),
50q_chunk_idx_start, k_chunk_idx_start, bias=attn_bias_slice, **blockwise_kwargs**
51k, v, dk, dv = map(lambda x: lax.permpermute(x, axis_name, perm=[(i),
53(i+1) % axis_size) for i in range(axis_size)]), (k, v, dk, dv))
52return (dq, dk, dv, k, v), None
53(dq, dk, dv, k, v), - 1ax.scan(scan_kv_block, init-(dq, dk, dv, k, v), xs=jnp.arange(0, axis_size))
54dq, dk, dv = dq.astype(q.dtype), dk.astype(k.dtype), dv.astype(v.dtype)
55return dq, dk, dv, None
56
57
58
59partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
50defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
51y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
52return y
53
54ring_attention.defvjp(_ring_attention_fwd, _ring_attention_bwd)
55
56
57
58
59partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
51defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
52y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
53return y
54ring_attention.defvjp(_ring_attention_fwd, _ring_attention_bwd)
55
56
59partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
56defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
57y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
58return y
59partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
59defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
50y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
510return y
511def(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
52return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
53return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
54return y
556
57
58
59partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
59defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
59y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
59return y
60
610
62
63
64ring_attention.defvjp(_ring_attention_fwd, _ring_attention_bwd)
65
65
66
67
68
69partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
60defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
61y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
62return y
63
64ring_attention.defvjp(_ring_attention_fwd, _ring_attention_bwd)
65
65
67
69partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
6defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
63y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
64return y
651
66
672
683
69partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
6defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
64y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
64return y
652
69partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
66defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
67y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs)
68return y
69partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
60defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
69y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
600return y
610return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
62return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
63y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
64return y
656
66
677
68
69partial(jax.custom_vjp, nondiff_argnums=[4, 5, 6])
60defring_attention(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
67y, _= _ring_attention_fwd(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
68return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
69return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
69return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
69return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
69return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
700return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
71return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
72return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
73return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
73return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
74return(q, k, v, attn_bias, axis_name, float32_logits, blockwise_kwargs):
이 큰 컨텍스트에 대해 40%의 75편 합리적으로 높은 MFU, 그러면 B/F = 2.4이며, 이는 Ring Attention이 오버헤드를 추가하지 않고 추론을 위해 32배 더 큰 컨텍스트를 허용한다는 것을 의미한다.

## 부록 D Training FLOPs Scaling for Context Size

제안된 접근 방식은 1억 토큰을 초과하는 컨텍스트 크기를 가진 훈련의 가능성을 열어주고 장치 수에 따라 컨텍스트 크기를 선형 스케일링할 수 있다는 점을 감안할 때 컨텍스트 크기를 가진 데이터 세트 규모당 훈련 FLOP의 방법을 이해하는 것이 필수적이다. 더 큰 컨텍스트 크기가 더 많은 수의 FLOP를 초래하지만, 증가된 비율은 토큰의 수가 고정된 채로 남아 있기 때문에 2차적으로 확장되지 않는다. 다양한 계산 예산을 나타내는 다양한 모델 크기와 컨텍스트 길이를 보여주는 그림 5에 이러한 결과를 제시한다. 그림은 4K 컨텍스트 크기가 더 짧은 동일한 모델과 비교하여 더 큰 컨텍스트 길이에 대한 FLOP의 비율을 보여준다.

우리는 \((24bsh^{2}+4bs^{2}h)n\)을 사용하여 서열당 FLOP를 계산했으며 여기서 \(h\)는 모델 은닉 차원, \(b\)는 배치 크기, \(s\)는 총 서열 길이, \(n\)는 층 수이다. 데이터 세트당 FLOPs 비율은 \(((24bs_{2}h^{2}+4bs_{2}{}^{2}h)/(24bs_{1}h^{2}+4bs_{1}{}^{2}h))/(s_{2}/s_{1})=(6h+s_{2})/(6h+s_{1})\로 주어지며, 여기서 \(s_{2}\) 및 \(s_{1}\)는 새롭고 오래된 컨텍스트 길이이다. 모델 크기 및 숨겨진 치수는 다음과 같습니다. LLaMA-7B(4096), LLaMA-13B(5140), LLaMA-33B(7168), LLaMA-65B(8192), GPT3-175B(12288) 및 1TB(36864). 이러한 모델 구성은 1TB 모델 크기와 차원이 우리가 정의한 것을 제외하고 LLaMA [36] 및 GPT-3 [3] 논문에서 가져온 것이다.

그림 5에서 볼 수 있듯이 작은 모델을 1M 컨텍스트 크기로 확장하면 FLOP가 약 20~40배, 10M 및 100M 토큰 컨텍스트 크기가 훨씬 더 많아진다. 그러나 모형 크기가 커질수록 비용 비율은 감소한다. 예를 들어, 170B 모델을 4K에서 10M으로 확장하면 컨텍스트 크기가 3072배 더 길음에도 불구하고 데이터 세트 FLOP당 162.6배 더 높다.

그림 5: 데이터 세트당 FLOP는 서로 다른 모델 차원을 고려하여 4k 컨텍스트 크기에 대한 비용 비율을 훈련한다. x축에서 컨텍스트 길이를 찾을 수 있습니다. 예를 들어 32x(128k)는 128k의 컨텍스트 길이를 나타내고 32x는 동일한 모델의 4k 컨텍스트 길이의 크기입니다.

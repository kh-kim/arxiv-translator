<html lang="en" data-theme="light"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.08545] Efficient Continual Pre-training for Building Domain Specific Large Language Models</title><meta property="og:description" content="Large language models (LLMs) have demonstrated remarkable open-domain capabilities. Traditionally, LLMs tailored for a domain are trained from scratch to excel at handling domain-specific tasks. In this work, we explor‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Efficient Continual Pre-training for Building Domain Specific Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Efficient Continual Pre-training for Building Domain Specific Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.08545">

<!--Generated on Tue Feb 27 19:10:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.7.9.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Efficient Continual Pre-training for Building Domain Specific Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yong Xie<sup id="id3.3.id1" class="ltx_sup"><span id="id3.3.id1.1" class="ltx_text ltx_font_italic">‚Ä°</span></sup>
<br class="ltx_break">UIUC 
<br class="ltx_break"><span id="id4.4.id2" class="ltx_text ltx_font_typewriter">yongxie2@illinois.edu</span>
&amp;Karan Aggarwal<sup id="id5.5.id3" class="ltx_sup"><span id="id5.5.id3.1" class="ltx_text ltx_font_italic">‚Ä°</span></sup> 
<br class="ltx_break">Amazon 
<br class="ltx_break"><span id="id6.6.id4" class="ltx_text ltx_font_typewriter">kagg@amazon.com</span>
&amp;Aitzaz Ahmad 
<br class="ltx_break">Amazon 
<br class="ltx_break"><span id="id7.7.id5" class="ltx_text ltx_font_typewriter">aitzaza@amazon.com</span>
</span><span class="ltx_author_notes">This work was done during Yong‚Äôs internship at Amazon</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Large language models (LLMs) have demonstrated remarkable open-domain capabilities. Traditionally, LLMs tailored for a domain are trained from scratch to excel at handling domain-specific tasks. In this work, we explore an alternative strategy of continual pre-training as a means to develop domain-specific LLMs. We introduce <span id="id8.id1.1" class="ltx_text ltx_font_italic">FinPythia-6.9B</span>, developed through domain-adaptive continual pre-training on the financial domain.
Continual pre-trained FinPythia showcases consistent improvements on financial tasks over the original foundational model. We further explore simple but effective data selection strategies for continual pre-training. Our data selection strategies outperforms vanilla continual pre-training‚Äôs performance with just 10% of corpus size and cost, without any degradation on open-domain standard tasks. Our work proposes an alternative solution to building domain-specific LLMs from scratch in a cost-effective manner.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">‚Ä°</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä°</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Large Language Models (LLMs) have exhibited a profound understanding of natural language, improving performance on an array of tasks&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Using open web data has helped in creating general-purpose LLMs with a broad range of capabilities. General-purpose LLMs are however not ‚Äúspecialists‚Äù; for example, while LLMs could write good news articles, it would be hard-pressed to write specialized legal documents.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In order to make a specialist or domain-specific LLM, they need to be trained on domain data. Approaches for building domain-specific LLMs can be categorized into two categories: training domain-specific LLMs from scratch or using continual pre-training existing LLMs with domain data. Most researchers have taken the first approach of building domain-specific LLMs from scratch. Prominent examples are the Med-PaLM family&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> for the medical domain, Galactica for scientific papers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and BloombergGPT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for finance. Little attention has been paid to building domain-specific LLMs using domain-adaptive continual pre-training, despite being a much cheaper alternative. Notably, PMC-LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, a medical LLM was trained through continual pre-training of LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> on medical papers. Continual pre-training can also be used for updating a LLM with the latest knowledge in an evolving environment.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we explore the following: 1) Is domain-adaptive continual pre-training helpful in building domain-specific LLMs?; 2) Can we employ data selection strategies for a more effective domain-adaptive continual pre-training?; and 3) Does domain-adaptive continual pre-training hurt LLM‚Äôs open-domain capabilities? We answer these questions in the confines of finance domain by training a continually pre-trained model, FinPythia, built on top of Pythia&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We report a boost on financial benchmarks&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> after continual pre-training on domain data of size 8% of what Pythia was trained on as an answer to the first question. We also observe an evidence of latest financial domain knowledge acquisition in FinPythia during qualitative analysis. To answer the second question, we propose two simple data selection techniques, task-aware <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">Efficient Task-Similar Domain-Adaptive Continual Pre-training</em> (ETS-DACP) and <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">Efficient Task-Agnostic Domain-Adaptive Continual Pre-training</em> (ETA-DACP). These methods outperform the performance of domain-adaptive continual pre-training with just 10% of selected domain data or 0.8% of Pythia‚Äôs training corpus. We use three metrics for data selection: similarity, perplexity, and token type entropy. While similarity needs task data as seed, the latter two metrics are task-agnostic. To answer the third question, we benchmark these continually pre-trained models on four open-domain standard tasks like MMLU and TruthfulQA. We observe no significant performance change, indicating that LLM retains its general capabilities while adapting to the domain.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main contributions of this paper are threefold. Firstly, we curate a large-scale financial corpus comprising 16 billion words sourced from financial datasets. Secondly, our experiments demonstrate the promise of building domain-specific LLMs through continual pre-training, further validating and extending the findings obtained from smaller language models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This finding provides insights for building domain-specific LLMs with lower costs, as an alternative to expensive pre-training from scratch. Our results indicate that continual pre-training maintains the same open-domain performance as the original foundation model.
Lastly, we propose two Efficient Domain-adaptive Continual Pre-training methods as a more efficient approach to vanilla continual pre-training. Our novel approach deploys data selection strategies that can achieve better performance with a fraction of the cost of the domain-adaptive continual pre-training.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we describe the curation of our financial corpus used for continual pre-training, our domain-adaptive continual pre-training, task-adaptive continual pre-training, and our proposed task-aware domain-adaptive continual pre-training.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Financial Corpus Curation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In our evaluation of data sources, we consider three dimensions: public availability, licensing, and scale. We use two sources of data for the financial corpus: the financial news common crawl and SEC filings. Financial News CommonCrawl is curated by filtering out financial news from the public CommonCrawl data. We follow the de-duplication procedure of Pythia suite&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> to remove duplicate training data. While there is conflicting evidence of duplication hurting the performance&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, there is no evidence of the benefits of duplication in the training data. Hence, for a more efficient training, we use de-duplication following&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Using these two sources, we create a combined dataset of 23.9 billion tokens (16.5 billion words). Details of curation steps can be found in Appendix&nbsp;<a href="#A5" title="Appendix E Financial Dataset Curation ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2311.08545/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="184" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Labeled task data, task-similar domain data and domain corpus in a manifold space.</span></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Domain-adaptive Continual Pre-training (DACP)</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Typically, domain-specific LLMs are built by training the model from scratch using massive amounts of domain data.
This procedure has two drawbacks: it is quite costly and needs much higher amounts of domain data, which is not as feasible in lower data domains like finance with very specialized and confidential data. Domain-adaptive continual pre-training (DACP) is a straightforward alternative to building from scratch; we continually pre-train a general-purpose LLM on a large scale corpus of domain-specific unlabeled data. Domain-adaptive continual pre-training has shown the ability to adapt the language models to better fit the in-domain distribution <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. They also enable large language models to acquire new knowledge as new data appears <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, instead of training the model from scratch. We use DACP in our experiments to benchmark its benefits.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Task-Adaptive Continual Pre-training (TACP)</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Task-adaptive continual pre-training (TACP) refers to continual pre-training aiming to enhance performance on a targeted task. TACP has been studied in the context of smaller language models like BERT by pre-training the language model on labeled and unlabeled data from the task&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> showing improvements over the task. While task data is usually quite limited, TACP shows considerable effects on smaller language models like BERT. We benchmark TACP on our four financial evaluation tasks.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Towards an Efficient Domain-adaptive Continual Pre-training</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The primary limitation of TACP lies in its focus on constructing task-specific LLMs instead of foundation LLMs, owing to the sole use of unlabeled task data for training. While DACP uses a much larger domain corpus, it is prohibitively expensive.
To address these limitations, we propose two approaches: <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">Efficient Task-Similar Domain-Adaptive Continual Pre-training</span> (ETS-DACP) and <span id="S2.SS4.p1.1.2" class="ltx_text ltx_font_italic">Efficient Task-Agnostic Domain-Adaptive Continual Pre-training</span> (ETA-DACP). While ETS-DACP aims to build foundation LLMs for a set of tasks by tailoring the DACP to emphasize the significance of these tasks, ETA-DACP is more general and selects the most informative samples from the domain corpus for continual pre-training.</p>
</div>
<section id="S2.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Formulation</h5>

<div id="S2.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS4.SSS0.Px1.p1.7" class="ltx_p">We first formalize the problem. We are given an unlabeled domain pre-training corpus, <math id="S2.SS4.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\pazocal{U}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.1.m1.1a"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.1.m1.1.1">U</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.1.m1.1c">\pazocal{U}</annotation></semantics></math> represented by green region in Figure&nbsp;<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1 Financial Corpus Curation ‚Ä£ 2 Methodology ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Next, we can take two scenarios: absence or presence of an unlabeled task corpus. The first scenario of the presence of a task corpus, which can be a single or group of tasks, <math id="S2.SS4.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\pazocal{T}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.2.m2.1a"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS4.SSS0.Px1.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.2.m2.1b"><ci id="S2.SS4.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.2.m2.1.1">T</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.2.m2.1c">\pazocal{T}</annotation></semantics></math> is depicted as the red region in Figure&nbsp;<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1 Financial Corpus Curation ‚Ä£ 2 Methodology ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Typically, the task corpus is a subset of the domain corpus, <math id="S2.SS4.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\pazocal{T}\subset\pazocal{U}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.3.m3.1a"><mrow id="S2.SS4.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS4.SSS0.Px1.p1.3.m3.1.1.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.3.m3.1.1.2" xref="S2.SS4.SSS0.Px1.p1.3.m3.1.1.2.cmml">T</mi><mo id="S2.SS4.SSS0.Px1.p1.3.m3.1.1.1" xref="S2.SS4.SSS0.Px1.p1.3.m3.1.1.1.cmml">‚äÇ</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.3.m3.1.1.3" xref="S2.SS4.SSS0.Px1.p1.3.m3.1.1.3.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.3.m3.1b"><apply id="S2.SS4.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.3.m3.1.1"><subset id="S2.SS4.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.3.m3.1.1.1"></subset><ci id="S2.SS4.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.3.m3.1.1.2">T</ci><ci id="S2.SS4.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.3.m3.1.1.3">U</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.3.m3.1c">\pazocal{T}\subset\pazocal{U}</annotation></semantics></math>, with <math id="S2.SS4.SSS0.Px1.p1.4.m4.2" class="ltx_Math" alttext="|\pazocal{U}|>>|\pazocal{T}|" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.4.m4.2a"><mrow id="S2.SS4.SSS0.Px1.p1.4.m4.2.3" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.cmml"><mrow id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.2" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.1.cmml"><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.2.1" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.1.1.cmml">|</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS4.SSS0.Px1.p1.4.m4.1.1.cmml">U</mi><mo rspace="0.111em" stretchy="false" id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.2.2" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.1.1.cmml">|</mo></mrow><mo rspace="0.278em" id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.1" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.1.cmml">&gt;&gt;</mo><mrow id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.2" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.2.1" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.1.cmml">|</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.4.m4.2.2" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.2.cmml">T</mi><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.2.2" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.4.m4.2b"><apply id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.cmml" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3"><csymbol cd="latexml" id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.1">much-greater-than</csymbol><apply id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.1.cmml" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.2"><abs id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.2.2.1"></abs><ci id="S2.SS4.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.4.m4.1.1">U</ci></apply><apply id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.2"><abs id="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.3.3.2.1"></abs><ci id="S2.SS4.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S2.SS4.SSS0.Px1.p1.4.m4.2.2">T</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.4.m4.2c">|\pazocal{U}|&gt;&gt;|\pazocal{T}|</annotation></semantics></math>. The goal of data selection is to select a subset, <math id="S2.SS4.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\pazocal{D}\subset\pazocal{U}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.5.m5.1a"><mrow id="S2.SS4.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS4.SSS0.Px1.p1.5.m5.1.1.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.5.m5.1.1.2" xref="S2.SS4.SSS0.Px1.p1.5.m5.1.1.2.cmml">D</mi><mo id="S2.SS4.SSS0.Px1.p1.5.m5.1.1.1" xref="S2.SS4.SSS0.Px1.p1.5.m5.1.1.1.cmml">‚äÇ</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.5.m5.1.1.3" xref="S2.SS4.SSS0.Px1.p1.5.m5.1.1.3.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.5.m5.1b"><apply id="S2.SS4.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.5.m5.1.1"><subset id="S2.SS4.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.5.m5.1.1.1"></subset><ci id="S2.SS4.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.5.m5.1.1.2">D</ci><ci id="S2.SS4.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.5.m5.1.1.3">U</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.5.m5.1c">\pazocal{D}\subset\pazocal{U}</annotation></semantics></math>, that is most helpful for pre-training the LLM model. We also assume that the selected domain corpus subset is much larger than the task corpus, <math id="S2.SS4.SSS0.Px1.p1.6.m6.2" class="ltx_Math" alttext="|\pazocal{D}|>>|\pazocal{T}|" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.6.m6.2a"><mrow id="S2.SS4.SSS0.Px1.p1.6.m6.2.3" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.cmml"><mrow id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.2" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.1.cmml"><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.2.1" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.1.1.cmml">|</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.6.m6.1.1" xref="S2.SS4.SSS0.Px1.p1.6.m6.1.1.cmml">D</mi><mo rspace="0.111em" stretchy="false" id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.2.2" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.1.1.cmml">|</mo></mrow><mo rspace="0.278em" id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.1" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.1.cmml">&gt;&gt;</mo><mrow id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.2" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.2.1" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.1.1.cmml">|</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.6.m6.2.2" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.2.cmml">T</mi><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.2.2" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.6.m6.2b"><apply id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.cmml" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3"><csymbol cd="latexml" id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.1">much-greater-than</csymbol><apply id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.1.cmml" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.2"><abs id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.2.2.1"></abs><ci id="S2.SS4.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.6.m6.1.1">D</ci></apply><apply id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.2"><abs id="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.3.3.2.1"></abs><ci id="S2.SS4.SSS0.Px1.p1.6.m6.2.2.cmml" xref="S2.SS4.SSS0.Px1.p1.6.m6.2.2">T</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.6.m6.2c">|\pazocal{D}|&gt;&gt;|\pazocal{T}|</annotation></semantics></math>, as is a typical case. The data selection problem can be formally defined as selection of optimal <math id="S2.SS4.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="\pazocal{D}^{*}\subset U" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.7.m7.1a"><mrow id="S2.SS4.SSS0.Px1.p1.7.m7.1.1" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.cmml"><msup id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.2" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.2.cmml">D</mi><mo id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.3" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.3.cmml">‚àó</mo></msup><mo id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.1" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.1.cmml">‚äÇ</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.3" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.3.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.7.m7.1b"><apply id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1"><subset id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.1"></subset><apply id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.1.cmml" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2">superscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.2.cmml" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.2">D</ci><times id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.3.cmml" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.2.3"></times></apply><ci id="S2.SS4.SSS0.Px1.p1.7.m7.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.7.m7.1.1.3">U</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.7.m7.1c">\pazocal{D}^{*}\subset U</annotation></semantics></math>:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.2" class="ltx_Math" alttext="\pazocal{D}^{*}=\underset{\pazocal{D}^{*}\subset\pazocal{U}}{\mathrm{argmin}}~{}\mathbb{E}_{x\in\pazocal{T}}[\pazocal{L}_{t}(y|f(\theta^{*};x))]" display="block"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><msup id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml"><mi mathvariant="normal" id="S2.E1.m1.2.2.3.2" xref="S2.E1.m1.2.2.3.2.cmml">D</mi><mo id="S2.E1.m1.2.2.3.3" xref="S2.E1.m1.2.2.3.3.cmml">‚àó</mo></msup><mo id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml">=</mo><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.cmml"><munder accentunder="true" id="S2.E1.m1.2.2.1.3" xref="S2.E1.m1.2.2.1.3.cmml"><mi id="S2.E1.m1.2.2.1.3.2" xref="S2.E1.m1.2.2.1.3.2.cmml">argmin</mi><mrow id="S2.E1.m1.2.2.1.3.1" xref="S2.E1.m1.2.2.1.3.1.cmml"><msup id="S2.E1.m1.2.2.1.3.1.2" xref="S2.E1.m1.2.2.1.3.1.2.cmml"><mi mathvariant="normal" id="S2.E1.m1.2.2.1.3.1.2.2" xref="S2.E1.m1.2.2.1.3.1.2.2.cmml">D</mi><mo id="S2.E1.m1.2.2.1.3.1.2.3" xref="S2.E1.m1.2.2.1.3.1.2.3.cmml">‚àó</mo></msup><mo id="S2.E1.m1.2.2.1.3.1.1" xref="S2.E1.m1.2.2.1.3.1.1.cmml">‚äÇ</mo><mi mathvariant="normal" id="S2.E1.m1.2.2.1.3.1.3" xref="S2.E1.m1.2.2.1.3.1.3.cmml">U</mi></mrow></munder><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.2.cmml">‚Äã</mo><msub id="S2.E1.m1.2.2.1.4" xref="S2.E1.m1.2.2.1.4.cmml"><mi id="S2.E1.m1.2.2.1.4.2" xref="S2.E1.m1.2.2.1.4.2.cmml">ùîº</mi><mrow id="S2.E1.m1.2.2.1.4.3" xref="S2.E1.m1.2.2.1.4.3.cmml"><mi mathvariant="normal" id="S2.E1.m1.2.2.1.4.3.2" xref="S2.E1.m1.2.2.1.4.3.2.cmml">x</mi><mo id="S2.E1.m1.2.2.1.4.3.1" xref="S2.E1.m1.2.2.1.4.3.1.cmml">‚àà</mo><mi mathvariant="normal" id="S2.E1.m1.2.2.1.4.3.3" xref="S2.E1.m1.2.2.1.4.3.3.cmml">T</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.2a" xref="S2.E1.m1.2.2.1.2.cmml">‚Äã</mo><mrow id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.2" xref="S2.E1.m1.2.2.1.1.2.1.cmml">[</mo><mrow id="S2.E1.m1.2.2.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.cmml"><msub id="S2.E1.m1.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S2.E1.m1.2.2.1.1.1.1.3.2" xref="S2.E1.m1.2.2.1.1.1.1.3.2.cmml">L</mi><mi mathvariant="normal" id="S2.E1.m1.2.2.1.1.1.1.3.3" xref="S2.E1.m1.2.2.1.1.1.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">y</mi><mo fence="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><msup id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">Œ∏</mi><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">‚àó</mo></msup><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi mathvariant="normal" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.4" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.2.2.1.1.1.3" xref="S2.E1.m1.2.2.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></eq><apply id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.3.1.cmml" xref="S2.E1.m1.2.2.3">superscript</csymbol><ci id="S2.E1.m1.2.2.3.2.cmml" xref="S2.E1.m1.2.2.3.2">D</ci><times id="S2.E1.m1.2.2.3.3.cmml" xref="S2.E1.m1.2.2.3.3"></times></apply><apply id="S2.E1.m1.2.2.1.cmml" xref="S2.E1.m1.2.2.1"><times id="S2.E1.m1.2.2.1.2.cmml" xref="S2.E1.m1.2.2.1.2"></times><apply id="S2.E1.m1.2.2.1.3.cmml" xref="S2.E1.m1.2.2.1.3"><apply id="S2.E1.m1.2.2.1.3.1.cmml" xref="S2.E1.m1.2.2.1.3.1"><subset id="S2.E1.m1.2.2.1.3.1.1.cmml" xref="S2.E1.m1.2.2.1.3.1.1"></subset><apply id="S2.E1.m1.2.2.1.3.1.2.cmml" xref="S2.E1.m1.2.2.1.3.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.3.1.2.1.cmml" xref="S2.E1.m1.2.2.1.3.1.2">superscript</csymbol><ci id="S2.E1.m1.2.2.1.3.1.2.2.cmml" xref="S2.E1.m1.2.2.1.3.1.2.2">D</ci><times id="S2.E1.m1.2.2.1.3.1.2.3.cmml" xref="S2.E1.m1.2.2.1.3.1.2.3"></times></apply><ci id="S2.E1.m1.2.2.1.3.1.3.cmml" xref="S2.E1.m1.2.2.1.3.1.3">U</ci></apply><ci id="S2.E1.m1.2.2.1.3.2.cmml" xref="S2.E1.m1.2.2.1.3.2">argmin</ci></apply><apply id="S2.E1.m1.2.2.1.4.cmml" xref="S2.E1.m1.2.2.1.4"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.4.1.cmml" xref="S2.E1.m1.2.2.1.4">subscript</csymbol><ci id="S2.E1.m1.2.2.1.4.2.cmml" xref="S2.E1.m1.2.2.1.4.2">ùîº</ci><apply id="S2.E1.m1.2.2.1.4.3.cmml" xref="S2.E1.m1.2.2.1.4.3"><in id="S2.E1.m1.2.2.1.4.3.1.cmml" xref="S2.E1.m1.2.2.1.4.3.1"></in><ci id="S2.E1.m1.2.2.1.4.3.2.cmml" xref="S2.E1.m1.2.2.1.4.3.2">x</ci><ci id="S2.E1.m1.2.2.1.4.3.3.cmml" xref="S2.E1.m1.2.2.1.4.3.3">T</ci></apply></apply><apply id="S2.E1.m1.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.1.1.2.1.cmml" xref="S2.E1.m1.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S2.E1.m1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2"></times><apply id="S2.E1.m1.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.3.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.3.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3.2">L</ci><ci id="S2.E1.m1.2.2.1.1.1.1.3.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3.3">t</ci></apply><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2">conditional</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3">y</ci><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.3">f</ci><list id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1"><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2">ùúÉ</ci><times id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3"></times></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">x</ci></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\pazocal{D}^{*}=\underset{\pazocal{D}^{*}\subset\pazocal{U}}{\mathrm{argmin}}~{}\mathbb{E}_{x\in\pazocal{T}}[\pazocal{L}_{t}(y|f(\theta^{*};x))]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS0.Px1.p1.17" class="ltx_p">where, <math id="S2.SS4.SSS0.Px1.p1.8.m1.2" class="ltx_Math" alttext="f(\theta;\cdot)" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.8.m1.2a"><mrow id="S2.SS4.SSS0.Px1.p1.8.m1.2.3" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.cmml"><mi id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.2" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.1" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.1.cmml">‚Äã</mo><mrow id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.2" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.2.1" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.1.cmml">(</mo><mi id="S2.SS4.SSS0.Px1.p1.8.m1.1.1" xref="S2.SS4.SSS0.Px1.p1.8.m1.1.1.cmml">Œ∏</mi><mo rspace="0em" id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.2.2" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.1.cmml">;</mo><mo lspace="0em" rspace="0em" id="S2.SS4.SSS0.Px1.p1.8.m1.2.2" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.2.cmml">‚ãÖ</mo><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.2.3" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.8.m1.2b"><apply id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.cmml" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3"><times id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.1"></times><ci id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.2.cmml" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.2">ùëì</ci><list id="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.3.3.2"><ci id="S2.SS4.SSS0.Px1.p1.8.m1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.8.m1.1.1">ùúÉ</ci><ci id="S2.SS4.SSS0.Px1.p1.8.m1.2.2.cmml" xref="S2.SS4.SSS0.Px1.p1.8.m1.2.2">‚ãÖ</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.8.m1.2c">f(\theta;\cdot)</annotation></semantics></math> is a LLM with parameters <math id="S2.SS4.SSS0.Px1.p1.9.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.9.m2.1a"><mi id="S2.SS4.SSS0.Px1.p1.9.m2.1.1" xref="S2.SS4.SSS0.Px1.p1.9.m2.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.9.m2.1b"><ci id="S2.SS4.SSS0.Px1.p1.9.m2.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.9.m2.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.9.m2.1c">\theta</annotation></semantics></math>, <math id="S2.SS4.SSS0.Px1.p1.10.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.10.m3.1a"><mi id="S2.SS4.SSS0.Px1.p1.10.m3.1.1" xref="S2.SS4.SSS0.Px1.p1.10.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.10.m3.1b"><ci id="S2.SS4.SSS0.Px1.p1.10.m3.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.10.m3.1.1">ùë¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.10.m3.1c">y</annotation></semantics></math> is the task output, <math id="S2.SS4.SSS0.Px1.p1.11.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.11.m4.1a"><mi id="S2.SS4.SSS0.Px1.p1.11.m4.1.1" xref="S2.SS4.SSS0.Px1.p1.11.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.11.m4.1b"><ci id="S2.SS4.SSS0.Px1.p1.11.m4.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.11.m4.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.11.m4.1c">x</annotation></semantics></math> is an input in target task data <math id="S2.SS4.SSS0.Px1.p1.12.m5.1" class="ltx_Math" alttext="\pazocal{T}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.12.m5.1a"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.12.m5.1.1" xref="S2.SS4.SSS0.Px1.p1.12.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.12.m5.1b"><ci id="S2.SS4.SSS0.Px1.p1.12.m5.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.12.m5.1.1">T</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.12.m5.1c">\pazocal{T}</annotation></semantics></math>, and <math id="S2.SS4.SSS0.Px1.p1.13.m6.1" class="ltx_Math" alttext="\pazocal{L}_{t}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.13.m6.1a"><msub id="S2.SS4.SSS0.Px1.p1.13.m6.1.1" xref="S2.SS4.SSS0.Px1.p1.13.m6.1.1.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.13.m6.1.1.2" xref="S2.SS4.SSS0.Px1.p1.13.m6.1.1.2.cmml">L</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.13.m6.1.1.3" xref="S2.SS4.SSS0.Px1.p1.13.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.13.m6.1b"><apply id="S2.SS4.SSS0.Px1.p1.13.m6.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.13.m6.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.13.m6.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.13.m6.1.1">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.13.m6.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.13.m6.1.1.2">L</ci><ci id="S2.SS4.SSS0.Px1.p1.13.m6.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.13.m6.1.1.3">t</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.13.m6.1c">\pazocal{L}_{t}</annotation></semantics></math> is the target task loss or metric. <math id="S2.SS4.SSS0.Px1.p1.14.m7.1" class="ltx_Math" alttext="\theta^{*}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.14.m7.1a"><msup id="S2.SS4.SSS0.Px1.p1.14.m7.1.1" xref="S2.SS4.SSS0.Px1.p1.14.m7.1.1.cmml"><mi id="S2.SS4.SSS0.Px1.p1.14.m7.1.1.2" xref="S2.SS4.SSS0.Px1.p1.14.m7.1.1.2.cmml">Œ∏</mi><mo id="S2.SS4.SSS0.Px1.p1.14.m7.1.1.3" xref="S2.SS4.SSS0.Px1.p1.14.m7.1.1.3.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.14.m7.1b"><apply id="S2.SS4.SSS0.Px1.p1.14.m7.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.14.m7.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.14.m7.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.14.m7.1.1">superscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.14.m7.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.14.m7.1.1.2">ùúÉ</ci><times id="S2.SS4.SSS0.Px1.p1.14.m7.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.14.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.14.m7.1c">\theta^{*}</annotation></semantics></math> is computed on pre-training task with <math id="S2.SS4.SSS0.Px1.p1.15.m8.1" class="ltx_Math" alttext="\pazocal{L}_{\mathrm{pre-train}}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.15.m8.1a"><msub id="S2.SS4.SSS0.Px1.p1.15.m8.1.1" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.2" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.2.cmml">L</mi><mrow id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.cmml"><mi id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.2" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.2.cmml">pre</mi><mo id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.1" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.1.cmml">‚àí</mo><mi id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.3" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.3.cmml">train</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.15.m8.1b"><apply id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.2">L</ci><apply id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3"><minus id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.1"></minus><ci id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.2.cmml" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.2">pre</ci><ci id="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.3.cmml" xref="S2.SS4.SSS0.Px1.p1.15.m8.1.1.3.3">train</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.15.m8.1c">\pazocal{L}_{\mathrm{pre-train}}</annotation></semantics></math> as the pre-training loss, and <math id="S2.SS4.SSS0.Px1.p1.16.m9.1" class="ltx_Math" alttext="x_{u}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.16.m9.1a"><msub id="S2.SS4.SSS0.Px1.p1.16.m9.1.1" xref="S2.SS4.SSS0.Px1.p1.16.m9.1.1.cmml"><mi id="S2.SS4.SSS0.Px1.p1.16.m9.1.1.2" xref="S2.SS4.SSS0.Px1.p1.16.m9.1.1.2.cmml">x</mi><mi id="S2.SS4.SSS0.Px1.p1.16.m9.1.1.3" xref="S2.SS4.SSS0.Px1.p1.16.m9.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.16.m9.1b"><apply id="S2.SS4.SSS0.Px1.p1.16.m9.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.16.m9.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.16.m9.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.16.m9.1.1">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.16.m9.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.16.m9.1.1.2">ùë•</ci><ci id="S2.SS4.SSS0.Px1.p1.16.m9.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.16.m9.1.1.3">ùë¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.16.m9.1c">x_{u}</annotation></semantics></math> as the unlabeled sample in <math id="S2.SS4.SSS0.Px1.p1.17.m10.1" class="ltx_Math" alttext="\pazocal{D}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.17.m10.1a"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.17.m10.1.1" xref="S2.SS4.SSS0.Px1.p1.17.m10.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.17.m10.1b"><ci id="S2.SS4.SSS0.Px1.p1.17.m10.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.17.m10.1.1">D</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.17.m10.1c">\pazocal{D}</annotation></semantics></math>:</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.2" class="ltx_Math" alttext="\theta^{*}=\underset{\theta}{\mathrm{argmin}}~{}\mathbb{E}_{x_{u}\in\pazocal{D}}[\pazocal{L}_{\mathrm{pre-train}}(f(\theta;x_{u}))]" display="block"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml"><msup id="S2.E2.m1.2.2.3" xref="S2.E2.m1.2.2.3.cmml"><mi id="S2.E2.m1.2.2.3.2" xref="S2.E2.m1.2.2.3.2.cmml">Œ∏</mi><mo id="S2.E2.m1.2.2.3.3" xref="S2.E2.m1.2.2.3.3.cmml">‚àó</mo></msup><mo id="S2.E2.m1.2.2.2" xref="S2.E2.m1.2.2.2.cmml">=</mo><mrow id="S2.E2.m1.2.2.1" xref="S2.E2.m1.2.2.1.cmml"><munder accentunder="true" id="S2.E2.m1.2.2.1.3" xref="S2.E2.m1.2.2.1.3.cmml"><mi id="S2.E2.m1.2.2.1.3.2" xref="S2.E2.m1.2.2.1.3.2.cmml">argmin</mi><mo id="S2.E2.m1.2.2.1.3.1" xref="S2.E2.m1.2.2.1.3.1.cmml">ùúÉ</mo></munder><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.2" xref="S2.E2.m1.2.2.1.2.cmml">‚Äã</mo><msub id="S2.E2.m1.2.2.1.4" xref="S2.E2.m1.2.2.1.4.cmml"><mi id="S2.E2.m1.2.2.1.4.2" xref="S2.E2.m1.2.2.1.4.2.cmml">ùîº</mi><mrow id="S2.E2.m1.2.2.1.4.3" xref="S2.E2.m1.2.2.1.4.3.cmml"><msub id="S2.E2.m1.2.2.1.4.3.2" xref="S2.E2.m1.2.2.1.4.3.2.cmml"><mi id="S2.E2.m1.2.2.1.4.3.2.2" xref="S2.E2.m1.2.2.1.4.3.2.2.cmml">x</mi><mi id="S2.E2.m1.2.2.1.4.3.2.3" xref="S2.E2.m1.2.2.1.4.3.2.3.cmml">u</mi></msub><mo id="S2.E2.m1.2.2.1.4.3.1" xref="S2.E2.m1.2.2.1.4.3.1.cmml">‚àà</mo><mi mathvariant="normal" id="S2.E2.m1.2.2.1.4.3.3" xref="S2.E2.m1.2.2.1.4.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.2a" xref="S2.E2.m1.2.2.1.2.cmml">‚Äã</mo><mrow id="S2.E2.m1.2.2.1.1.1" xref="S2.E2.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.2" xref="S2.E2.m1.2.2.1.1.2.1.cmml">[</mo><mrow id="S2.E2.m1.2.2.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.cmml"><msub id="S2.E2.m1.2.2.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S2.E2.m1.2.2.1.1.1.1.3.2" xref="S2.E2.m1.2.2.1.1.1.1.3.2.cmml">L</mi><mrow id="S2.E2.m1.2.2.1.1.1.1.3.3" xref="S2.E2.m1.2.2.1.1.1.1.3.3.cmml"><mi id="S2.E2.m1.2.2.1.1.1.1.3.3.2" xref="S2.E2.m1.2.2.1.1.1.1.3.3.2.cmml">pre</mi><mo id="S2.E2.m1.2.2.1.1.1.1.3.3.1" xref="S2.E2.m1.2.2.1.1.1.1.3.3.1.cmml">‚àí</mo><mi id="S2.E2.m1.2.2.1.1.1.1.3.3.3" xref="S2.E2.m1.2.2.1.1.1.1.3.3.3.cmml">train</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">Œ∏</mi><mo id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">;</mo><msub id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi mathvariant="normal" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">u</mi></msub><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.4" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.1.1.1.3" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E2.m1.2.2.1.1.1.3" xref="S2.E2.m1.2.2.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2"><eq id="S2.E2.m1.2.2.2.cmml" xref="S2.E2.m1.2.2.2"></eq><apply id="S2.E2.m1.2.2.3.cmml" xref="S2.E2.m1.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.3.1.cmml" xref="S2.E2.m1.2.2.3">superscript</csymbol><ci id="S2.E2.m1.2.2.3.2.cmml" xref="S2.E2.m1.2.2.3.2">ùúÉ</ci><times id="S2.E2.m1.2.2.3.3.cmml" xref="S2.E2.m1.2.2.3.3"></times></apply><apply id="S2.E2.m1.2.2.1.cmml" xref="S2.E2.m1.2.2.1"><times id="S2.E2.m1.2.2.1.2.cmml" xref="S2.E2.m1.2.2.1.2"></times><apply id="S2.E2.m1.2.2.1.3.cmml" xref="S2.E2.m1.2.2.1.3"><ci id="S2.E2.m1.2.2.1.3.1.cmml" xref="S2.E2.m1.2.2.1.3.1">ùúÉ</ci><ci id="S2.E2.m1.2.2.1.3.2.cmml" xref="S2.E2.m1.2.2.1.3.2">argmin</ci></apply><apply id="S2.E2.m1.2.2.1.4.cmml" xref="S2.E2.m1.2.2.1.4"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.4.1.cmml" xref="S2.E2.m1.2.2.1.4">subscript</csymbol><ci id="S2.E2.m1.2.2.1.4.2.cmml" xref="S2.E2.m1.2.2.1.4.2">ùîº</ci><apply id="S2.E2.m1.2.2.1.4.3.cmml" xref="S2.E2.m1.2.2.1.4.3"><in id="S2.E2.m1.2.2.1.4.3.1.cmml" xref="S2.E2.m1.2.2.1.4.3.1"></in><apply id="S2.E2.m1.2.2.1.4.3.2.cmml" xref="S2.E2.m1.2.2.1.4.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.4.3.2.1.cmml" xref="S2.E2.m1.2.2.1.4.3.2">subscript</csymbol><ci id="S2.E2.m1.2.2.1.4.3.2.2.cmml" xref="S2.E2.m1.2.2.1.4.3.2.2">ùë•</ci><ci id="S2.E2.m1.2.2.1.4.3.2.3.cmml" xref="S2.E2.m1.2.2.1.4.3.2.3">ùë¢</ci></apply><ci id="S2.E2.m1.2.2.1.4.3.3.cmml" xref="S2.E2.m1.2.2.1.4.3.3">D</ci></apply></apply><apply id="S2.E2.m1.2.2.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.2.2.1.1.2.1.cmml" xref="S2.E2.m1.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S2.E2.m1.2.2.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1"><times id="S2.E2.m1.2.2.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.2"></times><apply id="S2.E2.m1.2.2.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.3.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.3.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.3.2">L</ci><apply id="S2.E2.m1.2.2.1.1.1.1.3.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.3.3"><minus id="S2.E2.m1.2.2.1.1.1.1.3.3.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.3.3.1"></minus><ci id="S2.E2.m1.2.2.1.1.1.1.3.3.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.3.3.2">pre</ci><ci id="S2.E2.m1.2.2.1.1.1.1.3.3.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.3.3.3">train</ci></apply></apply><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1"><times id="S2.E2.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.3">f</ci><list id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ùúÉ</ci><apply id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">x</ci><ci id="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">u</ci></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">\theta^{*}=\underset{\theta}{\mathrm{argmin}}~{}\mathbb{E}_{x_{u}\in\pazocal{D}}[\pazocal{L}_{\mathrm{pre-train}}(f(\theta;x_{u}))]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS0.Px1.p1.23" class="ltx_p">Our domain-adaptive continual pre-training can be viewed from the lens of unsupervised domain adaptation&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Our source data is the large unsupervised domain corpus, while the target data is the target task data. With pre-training, we do not have control over the alignment with task training data itself; our idea is that by aligning with the domain during pre-training, we could align the LLM with the task. This intuition is backed by evidence of LLM pre-training helping the performance over open domain tasks. We use the generalization bound from&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> since our problem is similar to unsupervised domain adaptation. Consider a hypothesis space <math id="S2.SS4.SSS0.Px1.p1.18.m1.1" class="ltx_Math" alttext="\pazocal{H}_{p}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.18.m1.1a"><msub id="S2.SS4.SSS0.Px1.p1.18.m1.1.1" xref="S2.SS4.SSS0.Px1.p1.18.m1.1.1.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.18.m1.1.1.2" xref="S2.SS4.SSS0.Px1.p1.18.m1.1.1.2.cmml">H</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.18.m1.1.1.3" xref="S2.SS4.SSS0.Px1.p1.18.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.18.m1.1b"><apply id="S2.SS4.SSS0.Px1.p1.18.m1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.18.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.18.m1.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.18.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.18.m1.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.18.m1.1.1.2">H</ci><ci id="S2.SS4.SSS0.Px1.p1.18.m1.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.18.m1.1.1.3">p</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.18.m1.1c">\pazocal{H}_{p}</annotation></semantics></math> with <math id="S2.SS4.SSS0.Px1.p1.19.m2.1" class="ltx_Math" alttext="f\in\pazocal{H}_{p}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.19.m2.1a"><mrow id="S2.SS4.SSS0.Px1.p1.19.m2.1.1" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.cmml"><mi id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.2" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.2.cmml">f</mi><mo id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.1" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.1.cmml">‚àà</mo><msub id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.2" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.2.cmml">H</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.3" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.3.cmml">p</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.19.m2.1b"><apply id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1"><in id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.1"></in><ci id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.2">ùëì</ci><apply id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.2.cmml" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.2">H</ci><ci id="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.3.cmml" xref="S2.SS4.SSS0.Px1.p1.19.m2.1.1.3.3">p</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.19.m2.1c">f\in\pazocal{H}_{p}</annotation></semantics></math>; generalization errors on source <math id="S2.SS4.SSS0.Px1.p1.20.m3.1" class="ltx_Math" alttext="\pazocal{D}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.20.m3.1a"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.20.m3.1.1" xref="S2.SS4.SSS0.Px1.p1.20.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.20.m3.1b"><ci id="S2.SS4.SSS0.Px1.p1.20.m3.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.20.m3.1.1">D</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.20.m3.1c">\pazocal{D}</annotation></semantics></math> and task data <math id="S2.SS4.SSS0.Px1.p1.21.m4.1" class="ltx_Math" alttext="\pazocal{T}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.21.m4.1a"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.21.m4.1.1" xref="S2.SS4.SSS0.Px1.p1.21.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.21.m4.1b"><ci id="S2.SS4.SSS0.Px1.p1.21.m4.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.21.m4.1.1">T</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.21.m4.1c">\pazocal{T}</annotation></semantics></math> as <math id="S2.SS4.SSS0.Px1.p1.22.m5.1" class="ltx_Math" alttext="\epsilon_{\pazocal{D}}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.22.m5.1a"><msub id="S2.SS4.SSS0.Px1.p1.22.m5.1.1" xref="S2.SS4.SSS0.Px1.p1.22.m5.1.1.cmml"><mi id="S2.SS4.SSS0.Px1.p1.22.m5.1.1.2" xref="S2.SS4.SSS0.Px1.p1.22.m5.1.1.2.cmml">œµ</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.22.m5.1.1.3" xref="S2.SS4.SSS0.Px1.p1.22.m5.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.22.m5.1b"><apply id="S2.SS4.SSS0.Px1.p1.22.m5.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.22.m5.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.22.m5.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.22.m5.1.1">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.22.m5.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.22.m5.1.1.2">italic-œµ</ci><ci id="S2.SS4.SSS0.Px1.p1.22.m5.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.22.m5.1.1.3">D</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.22.m5.1c">\epsilon_{\pazocal{D}}</annotation></semantics></math> and <math id="S2.SS4.SSS0.Px1.p1.23.m6.1" class="ltx_Math" alttext="\epsilon_{\pazocal{T}}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.23.m6.1a"><msub id="S2.SS4.SSS0.Px1.p1.23.m6.1.1" xref="S2.SS4.SSS0.Px1.p1.23.m6.1.1.cmml"><mi id="S2.SS4.SSS0.Px1.p1.23.m6.1.1.2" xref="S2.SS4.SSS0.Px1.p1.23.m6.1.1.2.cmml">œµ</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.23.m6.1.1.3" xref="S2.SS4.SSS0.Px1.p1.23.m6.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.23.m6.1b"><apply id="S2.SS4.SSS0.Px1.p1.23.m6.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.23.m6.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.23.m6.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.23.m6.1.1">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.23.m6.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.23.m6.1.1.2">italic-œµ</ci><ci id="S2.SS4.SSS0.Px1.p1.23.m6.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.23.m6.1.1.3">T</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.23.m6.1c">\epsilon_{\pazocal{T}}</annotation></semantics></math>, respectively. The generalization bound can be given:</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.4" class="ltx_Math" alttext="\epsilon_{\pazocal{T}}(f)\leq\epsilon_{\pazocal{D}}(f)+\frac{1}{2}d_{\pazocal{H}_{p}\Delta\pazocal{H}_{p}}(\pazocal{D},\pazocal{T})+\pazocal{C}" display="block"><semantics id="S2.E3.m1.4a"><mrow id="S2.E3.m1.4.5" xref="S2.E3.m1.4.5.cmml"><mrow id="S2.E3.m1.4.5.2" xref="S2.E3.m1.4.5.2.cmml"><msub id="S2.E3.m1.4.5.2.2" xref="S2.E3.m1.4.5.2.2.cmml"><mi id="S2.E3.m1.4.5.2.2.2" xref="S2.E3.m1.4.5.2.2.2.cmml">œµ</mi><mi mathvariant="normal" id="S2.E3.m1.4.5.2.2.3" xref="S2.E3.m1.4.5.2.2.3.cmml">T</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.5.2.1" xref="S2.E3.m1.4.5.2.1.cmml">‚Äã</mo><mrow id="S2.E3.m1.4.5.2.3.2" xref="S2.E3.m1.4.5.2.cmml"><mo stretchy="false" id="S2.E3.m1.4.5.2.3.2.1" xref="S2.E3.m1.4.5.2.cmml">(</mo><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">f</mi><mo stretchy="false" id="S2.E3.m1.4.5.2.3.2.2" xref="S2.E3.m1.4.5.2.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.4.5.1" xref="S2.E3.m1.4.5.1.cmml">‚â§</mo><mrow id="S2.E3.m1.4.5.3" xref="S2.E3.m1.4.5.3.cmml"><mrow id="S2.E3.m1.4.5.3.2" xref="S2.E3.m1.4.5.3.2.cmml"><msub id="S2.E3.m1.4.5.3.2.2" xref="S2.E3.m1.4.5.3.2.2.cmml"><mi id="S2.E3.m1.4.5.3.2.2.2" xref="S2.E3.m1.4.5.3.2.2.2.cmml">œµ</mi><mi mathvariant="normal" id="S2.E3.m1.4.5.3.2.2.3" xref="S2.E3.m1.4.5.3.2.2.3.cmml">D</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.5.3.2.1" xref="S2.E3.m1.4.5.3.2.1.cmml">‚Äã</mo><mrow id="S2.E3.m1.4.5.3.2.3.2" xref="S2.E3.m1.4.5.3.2.cmml"><mo stretchy="false" id="S2.E3.m1.4.5.3.2.3.2.1" xref="S2.E3.m1.4.5.3.2.cmml">(</mo><mi id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml">f</mi><mo stretchy="false" id="S2.E3.m1.4.5.3.2.3.2.2" xref="S2.E3.m1.4.5.3.2.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.4.5.3.1" xref="S2.E3.m1.4.5.3.1.cmml">+</mo><mrow id="S2.E3.m1.4.5.3.3" xref="S2.E3.m1.4.5.3.3.cmml"><mfrac id="S2.E3.m1.4.5.3.3.2" xref="S2.E3.m1.4.5.3.3.2.cmml"><mn id="S2.E3.m1.4.5.3.3.2.2" xref="S2.E3.m1.4.5.3.3.2.2.cmml">1</mn><mn id="S2.E3.m1.4.5.3.3.2.3" xref="S2.E3.m1.4.5.3.3.2.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.5.3.3.1" xref="S2.E3.m1.4.5.3.3.1.cmml">‚Äã</mo><msub id="S2.E3.m1.4.5.3.3.3" xref="S2.E3.m1.4.5.3.3.3.cmml"><mi id="S2.E3.m1.4.5.3.3.3.2" xref="S2.E3.m1.4.5.3.3.3.2.cmml">d</mi><mrow id="S2.E3.m1.4.5.3.3.3.3" xref="S2.E3.m1.4.5.3.3.3.3.cmml"><msub id="S2.E3.m1.4.5.3.3.3.3.2" xref="S2.E3.m1.4.5.3.3.3.3.2.cmml"><mi mathvariant="normal" id="S2.E3.m1.4.5.3.3.3.3.2.2" xref="S2.E3.m1.4.5.3.3.3.3.2.2.cmml">H</mi><mi mathvariant="normal" id="S2.E3.m1.4.5.3.3.3.3.2.3" xref="S2.E3.m1.4.5.3.3.3.3.2.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.5.3.3.3.3.1" xref="S2.E3.m1.4.5.3.3.3.3.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S2.E3.m1.4.5.3.3.3.3.3" xref="S2.E3.m1.4.5.3.3.3.3.3.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.5.3.3.3.3.1a" xref="S2.E3.m1.4.5.3.3.3.3.1.cmml">‚Äã</mo><msub id="S2.E3.m1.4.5.3.3.3.3.4" xref="S2.E3.m1.4.5.3.3.3.3.4.cmml"><mi mathvariant="normal" id="S2.E3.m1.4.5.3.3.3.3.4.2" xref="S2.E3.m1.4.5.3.3.3.3.4.2.cmml">H</mi><mi mathvariant="normal" id="S2.E3.m1.4.5.3.3.3.3.4.3" xref="S2.E3.m1.4.5.3.3.3.3.4.3.cmml">p</mi></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.4.5.3.3.1a" xref="S2.E3.m1.4.5.3.3.1.cmml">‚Äã</mo><mrow id="S2.E3.m1.4.5.3.3.4.2" xref="S2.E3.m1.4.5.3.3.4.1.cmml"><mo stretchy="false" id="S2.E3.m1.4.5.3.3.4.2.1" xref="S2.E3.m1.4.5.3.3.4.1.cmml">(</mo><mi mathvariant="normal" id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml">D</mi><mo id="S2.E3.m1.4.5.3.3.4.2.2" xref="S2.E3.m1.4.5.3.3.4.1.cmml">,</mo><mi mathvariant="normal" id="S2.E3.m1.4.4" xref="S2.E3.m1.4.4.cmml">T</mi><mo stretchy="false" id="S2.E3.m1.4.5.3.3.4.2.3" xref="S2.E3.m1.4.5.3.3.4.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.4.5.3.1a" xref="S2.E3.m1.4.5.3.1.cmml">+</mo><mi mathvariant="normal" id="S2.E3.m1.4.5.3.4" xref="S2.E3.m1.4.5.3.4.cmml">C</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.4b"><apply id="S2.E3.m1.4.5.cmml" xref="S2.E3.m1.4.5"><leq id="S2.E3.m1.4.5.1.cmml" xref="S2.E3.m1.4.5.1"></leq><apply id="S2.E3.m1.4.5.2.cmml" xref="S2.E3.m1.4.5.2"><times id="S2.E3.m1.4.5.2.1.cmml" xref="S2.E3.m1.4.5.2.1"></times><apply id="S2.E3.m1.4.5.2.2.cmml" xref="S2.E3.m1.4.5.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.5.2.2.1.cmml" xref="S2.E3.m1.4.5.2.2">subscript</csymbol><ci id="S2.E3.m1.4.5.2.2.2.cmml" xref="S2.E3.m1.4.5.2.2.2">italic-œµ</ci><ci id="S2.E3.m1.4.5.2.2.3.cmml" xref="S2.E3.m1.4.5.2.2.3">T</ci></apply><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">ùëì</ci></apply><apply id="S2.E3.m1.4.5.3.cmml" xref="S2.E3.m1.4.5.3"><plus id="S2.E3.m1.4.5.3.1.cmml" xref="S2.E3.m1.4.5.3.1"></plus><apply id="S2.E3.m1.4.5.3.2.cmml" xref="S2.E3.m1.4.5.3.2"><times id="S2.E3.m1.4.5.3.2.1.cmml" xref="S2.E3.m1.4.5.3.2.1"></times><apply id="S2.E3.m1.4.5.3.2.2.cmml" xref="S2.E3.m1.4.5.3.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.5.3.2.2.1.cmml" xref="S2.E3.m1.4.5.3.2.2">subscript</csymbol><ci id="S2.E3.m1.4.5.3.2.2.2.cmml" xref="S2.E3.m1.4.5.3.2.2.2">italic-œµ</ci><ci id="S2.E3.m1.4.5.3.2.2.3.cmml" xref="S2.E3.m1.4.5.3.2.2.3">D</ci></apply><ci id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2">ùëì</ci></apply><apply id="S2.E3.m1.4.5.3.3.cmml" xref="S2.E3.m1.4.5.3.3"><times id="S2.E3.m1.4.5.3.3.1.cmml" xref="S2.E3.m1.4.5.3.3.1"></times><apply id="S2.E3.m1.4.5.3.3.2.cmml" xref="S2.E3.m1.4.5.3.3.2"><divide id="S2.E3.m1.4.5.3.3.2.1.cmml" xref="S2.E3.m1.4.5.3.3.2"></divide><cn type="integer" id="S2.E3.m1.4.5.3.3.2.2.cmml" xref="S2.E3.m1.4.5.3.3.2.2">1</cn><cn type="integer" id="S2.E3.m1.4.5.3.3.2.3.cmml" xref="S2.E3.m1.4.5.3.3.2.3">2</cn></apply><apply id="S2.E3.m1.4.5.3.3.3.cmml" xref="S2.E3.m1.4.5.3.3.3"><csymbol cd="ambiguous" id="S2.E3.m1.4.5.3.3.3.1.cmml" xref="S2.E3.m1.4.5.3.3.3">subscript</csymbol><ci id="S2.E3.m1.4.5.3.3.3.2.cmml" xref="S2.E3.m1.4.5.3.3.3.2">ùëë</ci><apply id="S2.E3.m1.4.5.3.3.3.3.cmml" xref="S2.E3.m1.4.5.3.3.3.3"><times id="S2.E3.m1.4.5.3.3.3.3.1.cmml" xref="S2.E3.m1.4.5.3.3.3.3.1"></times><apply id="S2.E3.m1.4.5.3.3.3.3.2.cmml" xref="S2.E3.m1.4.5.3.3.3.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.5.3.3.3.3.2.1.cmml" xref="S2.E3.m1.4.5.3.3.3.3.2">subscript</csymbol><ci id="S2.E3.m1.4.5.3.3.3.3.2.2.cmml" xref="S2.E3.m1.4.5.3.3.3.3.2.2">H</ci><ci id="S2.E3.m1.4.5.3.3.3.3.2.3.cmml" xref="S2.E3.m1.4.5.3.3.3.3.2.3">p</ci></apply><ci id="S2.E3.m1.4.5.3.3.3.3.3.cmml" xref="S2.E3.m1.4.5.3.3.3.3.3">Œî</ci><apply id="S2.E3.m1.4.5.3.3.3.3.4.cmml" xref="S2.E3.m1.4.5.3.3.3.3.4"><csymbol cd="ambiguous" id="S2.E3.m1.4.5.3.3.3.3.4.1.cmml" xref="S2.E3.m1.4.5.3.3.3.3.4">subscript</csymbol><ci id="S2.E3.m1.4.5.3.3.3.3.4.2.cmml" xref="S2.E3.m1.4.5.3.3.3.3.4.2">H</ci><ci id="S2.E3.m1.4.5.3.3.3.3.4.3.cmml" xref="S2.E3.m1.4.5.3.3.3.3.4.3">p</ci></apply></apply></apply><interval closure="open" id="S2.E3.m1.4.5.3.3.4.1.cmml" xref="S2.E3.m1.4.5.3.3.4.2"><ci id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3">D</ci><ci id="S2.E3.m1.4.4.cmml" xref="S2.E3.m1.4.4">T</ci></interval></apply><ci id="S2.E3.m1.4.5.3.4.cmml" xref="S2.E3.m1.4.5.3.4">C</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.4c">\epsilon_{\pazocal{T}}(f)\leq\epsilon_{\pazocal{D}}(f)+\frac{1}{2}d_{\pazocal{H}_{p}\Delta\pazocal{H}_{p}}(\pazocal{D},\pazocal{T})+\pazocal{C}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS0.Px1.p1.26" class="ltx_p">where, <math id="S2.SS4.SSS0.Px1.p1.24.m1.1" class="ltx_Math" alttext="d_{\pazocal{H}_{p}\Delta\pazocal{H}_{p}}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.24.m1.1a"><msub id="S2.SS4.SSS0.Px1.p1.24.m1.1.1" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.cmml"><mi id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.2" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.2.cmml">d</mi><mrow id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.cmml"><msub id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.2" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.2.cmml">H</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.3" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.1" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.3" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.3.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.1a" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.1.cmml">‚Äã</mo><msub id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.2" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.2.cmml">H</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.3" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.3.cmml">p</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.24.m1.1b"><apply id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.2">ùëë</ci><apply id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3"><times id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.1"></times><apply id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.1.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.2.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.2">H</ci><ci id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.3.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.2.3">p</ci></apply><ci id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.3.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.3">Œî</ci><apply id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.1.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.2.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.2">H</ci><ci id="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.3.cmml" xref="S2.SS4.SSS0.Px1.p1.24.m1.1.1.3.4.3">p</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.24.m1.1c">d_{\pazocal{H}_{p}\Delta\pazocal{H}_{p}}</annotation></semantics></math> is the distribution discrepancy distance between <math id="S2.SS4.SSS0.Px1.p1.25.m2.1" class="ltx_Math" alttext="\pazocal{D}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.25.m2.1a"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.25.m2.1.1" xref="S2.SS4.SSS0.Px1.p1.25.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.25.m2.1b"><ci id="S2.SS4.SSS0.Px1.p1.25.m2.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.25.m2.1.1">D</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.25.m2.1c">\pazocal{D}</annotation></semantics></math> and <math id="S2.SS4.SSS0.Px1.p1.26.m3.1" class="ltx_Math" alttext="\pazocal{T}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.26.m3.1a"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.26.m3.1.1" xref="S2.SS4.SSS0.Px1.p1.26.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.26.m3.1b"><ci id="S2.SS4.SSS0.Px1.p1.26.m3.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.26.m3.1.1">T</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.26.m3.1c">\pazocal{T}</annotation></semantics></math> that is bounded by&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.12" class="ltx_Math" alttext="d_{\pazocal{H}_{p}\Delta\pazocal{H}_{p}}(\pazocal{D},\pazocal{T})=\underset{f,f^{\prime}\in\pazocal{H}_{p}}{\mathrm{sup}}|\mathbb{E}_{x\in\pazocal{D}}[f(x)\neq f^{\prime}(x)]-\mathbb{E}_{x\in\pazocal{T}}[f(x)\neq f^{\prime}(x)]|\leq 2\underset{\alpha(h)\in\pazocal{H}_{d}}{\mathrm{sup}}[\alpha(h)-1]" display="block"><semantics id="S2.E4.m1.12a"><mrow id="S2.E4.m1.12.12" xref="S2.E4.m1.12.12.cmml"><mrow id="S2.E4.m1.12.12.4" xref="S2.E4.m1.12.12.4.cmml"><msub id="S2.E4.m1.12.12.4.2" xref="S2.E4.m1.12.12.4.2.cmml"><mi id="S2.E4.m1.12.12.4.2.2" xref="S2.E4.m1.12.12.4.2.2.cmml">d</mi><mrow id="S2.E4.m1.12.12.4.2.3" xref="S2.E4.m1.12.12.4.2.3.cmml"><msub id="S2.E4.m1.12.12.4.2.3.2" xref="S2.E4.m1.12.12.4.2.3.2.cmml"><mi mathvariant="normal" id="S2.E4.m1.12.12.4.2.3.2.2" xref="S2.E4.m1.12.12.4.2.3.2.2.cmml">H</mi><mi mathvariant="normal" id="S2.E4.m1.12.12.4.2.3.2.3" xref="S2.E4.m1.12.12.4.2.3.2.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.12.12.4.2.3.1" xref="S2.E4.m1.12.12.4.2.3.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S2.E4.m1.12.12.4.2.3.3" xref="S2.E4.m1.12.12.4.2.3.3.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.12.12.4.2.3.1a" xref="S2.E4.m1.12.12.4.2.3.1.cmml">‚Äã</mo><msub id="S2.E4.m1.12.12.4.2.3.4" xref="S2.E4.m1.12.12.4.2.3.4.cmml"><mi mathvariant="normal" id="S2.E4.m1.12.12.4.2.3.4.2" xref="S2.E4.m1.12.12.4.2.3.4.2.cmml">H</mi><mi mathvariant="normal" id="S2.E4.m1.12.12.4.2.3.4.3" xref="S2.E4.m1.12.12.4.2.3.4.3.cmml">p</mi></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.12.12.4.1" xref="S2.E4.m1.12.12.4.1.cmml">‚Äã</mo><mrow id="S2.E4.m1.12.12.4.3.2" xref="S2.E4.m1.12.12.4.3.1.cmml"><mo stretchy="false" id="S2.E4.m1.12.12.4.3.2.1" xref="S2.E4.m1.12.12.4.3.1.cmml">(</mo><mi mathvariant="normal" id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml">D</mi><mo id="S2.E4.m1.12.12.4.3.2.2" xref="S2.E4.m1.12.12.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.E4.m1.5.5" xref="S2.E4.m1.5.5.cmml">T</mi><mo stretchy="false" id="S2.E4.m1.12.12.4.3.2.3" xref="S2.E4.m1.12.12.4.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.12.12.5" xref="S2.E4.m1.12.12.5.cmml">=</mo><mrow id="S2.E4.m1.11.11.1" xref="S2.E4.m1.11.11.1.cmml"><munder accentunder="true" id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml"><mi id="S2.E4.m1.2.2.3" xref="S2.E4.m1.2.2.3.cmml">sup</mi><mrow id="S2.E4.m1.2.2.2" xref="S2.E4.m1.2.2.2.cmml"><mrow id="S2.E4.m1.2.2.2.2.1" xref="S2.E4.m1.2.2.2.2.2.cmml"><mi mathvariant="normal" id="S2.E4.m1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.cmml">f</mi><mo id="S2.E4.m1.2.2.2.2.1.2" xref="S2.E4.m1.2.2.2.2.2.cmml">,</mo><msup id="S2.E4.m1.2.2.2.2.1.1" xref="S2.E4.m1.2.2.2.2.1.1.cmml"><mi mathvariant="normal" id="S2.E4.m1.2.2.2.2.1.1.2" xref="S2.E4.m1.2.2.2.2.1.1.2.cmml">f</mi><mo id="S2.E4.m1.2.2.2.2.1.1.3" xref="S2.E4.m1.2.2.2.2.1.1.3.cmml">‚Ä≤</mo></msup></mrow><mo id="S2.E4.m1.2.2.2.3" xref="S2.E4.m1.2.2.2.3.cmml">‚àà</mo><msub id="S2.E4.m1.2.2.2.4" xref="S2.E4.m1.2.2.2.4.cmml"><mi mathvariant="normal" id="S2.E4.m1.2.2.2.4.2" xref="S2.E4.m1.2.2.2.4.2.cmml">H</mi><mi mathvariant="normal" id="S2.E4.m1.2.2.2.4.3" xref="S2.E4.m1.2.2.2.4.3.cmml">p</mi></msub></mrow></munder><mo lspace="0em" rspace="0em" id="S2.E4.m1.11.11.1.2" xref="S2.E4.m1.11.11.1.2.cmml">‚Äã</mo><mrow id="S2.E4.m1.11.11.1.1.1" xref="S2.E4.m1.11.11.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.2" xref="S2.E4.m1.11.11.1.1.2.1.cmml">|</mo><mrow id="S2.E4.m1.11.11.1.1.1.1" xref="S2.E4.m1.11.11.1.1.1.1.cmml"><mrow id="S2.E4.m1.11.11.1.1.1.1.1" xref="S2.E4.m1.11.11.1.1.1.1.1.cmml"><msub id="S2.E4.m1.11.11.1.1.1.1.1.3" xref="S2.E4.m1.11.11.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.11.11.1.1.1.1.1.3.2" xref="S2.E4.m1.11.11.1.1.1.1.1.3.2.cmml">ùîº</mi><mrow id="S2.E4.m1.11.11.1.1.1.1.1.3.3" xref="S2.E4.m1.11.11.1.1.1.1.1.3.3.cmml"><mi mathvariant="normal" id="S2.E4.m1.11.11.1.1.1.1.1.3.3.2" xref="S2.E4.m1.11.11.1.1.1.1.1.3.3.2.cmml">x</mi><mo id="S2.E4.m1.11.11.1.1.1.1.1.3.3.1" xref="S2.E4.m1.11.11.1.1.1.1.1.3.3.1.cmml">‚àà</mo><mi mathvariant="normal" id="S2.E4.m1.11.11.1.1.1.1.1.3.3.3" xref="S2.E4.m1.11.11.1.1.1.1.1.3.3.3.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.11.11.1.1.1.1.1.2" xref="S2.E4.m1.11.11.1.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.1.1.1" xref="S2.E4.m1.11.11.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.2" xref="S2.E4.m1.11.11.1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.cmml"><mi mathvariant="normal" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.1" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.1.cmml">‚Äã</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.3.2.1" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.cmml">(</mo><mi mathvariant="normal" id="S2.E4.m1.6.6" xref="S2.E4.m1.6.6.cmml">x</mi><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.3.2.2" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.1" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.1.cmml">‚â†</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.cmml"><msup id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.cmml"><mi mathvariant="normal" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.2" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.2.cmml">f</mi><mo id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.3" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.3.cmml">‚Ä≤</mo></msup><mo lspace="0em" rspace="0em" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.1" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.1.cmml">‚Äã</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.3.2" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.3.2.1" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.cmml">(</mo><mi mathvariant="normal" id="S2.E4.m1.7.7" xref="S2.E4.m1.7.7.cmml">x</mi><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.3.2.2" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.3" xref="S2.E4.m1.11.11.1.1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S2.E4.m1.11.11.1.1.1.1.3" xref="S2.E4.m1.11.11.1.1.1.1.3.cmml">‚àí</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.2" xref="S2.E4.m1.11.11.1.1.1.1.2.cmml"><msub id="S2.E4.m1.11.11.1.1.1.1.2.3" xref="S2.E4.m1.11.11.1.1.1.1.2.3.cmml"><mi id="S2.E4.m1.11.11.1.1.1.1.2.3.2" xref="S2.E4.m1.11.11.1.1.1.1.2.3.2.cmml">ùîº</mi><mrow id="S2.E4.m1.11.11.1.1.1.1.2.3.3" xref="S2.E4.m1.11.11.1.1.1.1.2.3.3.cmml"><mi mathvariant="normal" id="S2.E4.m1.11.11.1.1.1.1.2.3.3.2" xref="S2.E4.m1.11.11.1.1.1.1.2.3.3.2.cmml">x</mi><mo id="S2.E4.m1.11.11.1.1.1.1.2.3.3.1" xref="S2.E4.m1.11.11.1.1.1.1.2.3.3.1.cmml">‚àà</mo><mi mathvariant="normal" id="S2.E4.m1.11.11.1.1.1.1.2.3.3.3" xref="S2.E4.m1.11.11.1.1.1.1.2.3.3.3.cmml">T</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.11.11.1.1.1.1.2.2" xref="S2.E4.m1.11.11.1.1.1.1.2.2.cmml">‚Äã</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.2.1.1" xref="S2.E4.m1.11.11.1.1.1.1.2.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.2" xref="S2.E4.m1.11.11.1.1.1.1.2.1.2.1.cmml">[</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.cmml"><mrow id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.cmml"><mi mathvariant="normal" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.2" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.1" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.1.cmml">‚Äã</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.3.2" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.3.2.1" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.cmml">(</mo><mi mathvariant="normal" id="S2.E4.m1.8.8" xref="S2.E4.m1.8.8.cmml">x</mi><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.3.2.2" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.1" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.1.cmml">‚â†</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.cmml"><msup id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.cmml"><mi mathvariant="normal" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.2" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.2.cmml">f</mi><mo id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.3" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.3.cmml">‚Ä≤</mo></msup><mo lspace="0em" rspace="0em" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.1" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.1.cmml">‚Äã</mo><mrow id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.3.2" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.cmml"><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.3.2.1" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.cmml">(</mo><mi mathvariant="normal" id="S2.E4.m1.9.9" xref="S2.E4.m1.9.9.cmml">x</mi><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.3.2.2" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.3" xref="S2.E4.m1.11.11.1.1.1.1.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E4.m1.11.11.1.1.1.3" xref="S2.E4.m1.11.11.1.1.2.1.cmml">|</mo></mrow></mrow><mo id="S2.E4.m1.12.12.6" xref="S2.E4.m1.12.12.6.cmml">‚â§</mo><mrow id="S2.E4.m1.12.12.2" xref="S2.E4.m1.12.12.2.cmml"><mn id="S2.E4.m1.12.12.2.3" xref="S2.E4.m1.12.12.2.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.E4.m1.12.12.2.2" xref="S2.E4.m1.12.12.2.2.cmml">‚Äã</mo><munder accentunder="true" id="S2.E4.m1.3.3" xref="S2.E4.m1.3.3.cmml"><mi id="S2.E4.m1.3.3.2" xref="S2.E4.m1.3.3.2.cmml">sup</mi><mrow id="S2.E4.m1.3.3.1" xref="S2.E4.m1.3.3.1.cmml"><mrow id="S2.E4.m1.3.3.1.3" xref="S2.E4.m1.3.3.1.3.cmml"><mi id="S2.E4.m1.3.3.1.3.2" xref="S2.E4.m1.3.3.1.3.2.cmml">Œ±</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.3.3.1.3.1" xref="S2.E4.m1.3.3.1.3.1.cmml">‚Äã</mo><mrow id="S2.E4.m1.3.3.1.3.3.2" xref="S2.E4.m1.3.3.1.3.cmml"><mo stretchy="false" id="S2.E4.m1.3.3.1.3.3.2.1" xref="S2.E4.m1.3.3.1.3.cmml">(</mo><mi mathvariant="normal" id="S2.E4.m1.3.3.1.1" xref="S2.E4.m1.3.3.1.1.cmml">h</mi><mo stretchy="false" id="S2.E4.m1.3.3.1.3.3.2.2" xref="S2.E4.m1.3.3.1.3.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.3.3.1.2" xref="S2.E4.m1.3.3.1.2.cmml">‚àà</mo><msub id="S2.E4.m1.3.3.1.4" xref="S2.E4.m1.3.3.1.4.cmml"><mi mathvariant="normal" id="S2.E4.m1.3.3.1.4.2" xref="S2.E4.m1.3.3.1.4.2.cmml">H</mi><mi mathvariant="normal" id="S2.E4.m1.3.3.1.4.3" xref="S2.E4.m1.3.3.1.4.3.cmml">d</mi></msub></mrow></munder><mo lspace="0em" rspace="0em" id="S2.E4.m1.12.12.2.2a" xref="S2.E4.m1.12.12.2.2.cmml">‚Äã</mo><mrow id="S2.E4.m1.12.12.2.1.1" xref="S2.E4.m1.12.12.2.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.12.12.2.1.1.2" xref="S2.E4.m1.12.12.2.1.2.1.cmml">[</mo><mrow id="S2.E4.m1.12.12.2.1.1.1" xref="S2.E4.m1.12.12.2.1.1.1.cmml"><mrow id="S2.E4.m1.12.12.2.1.1.1.2" xref="S2.E4.m1.12.12.2.1.1.1.2.cmml"><mi id="S2.E4.m1.12.12.2.1.1.1.2.2" xref="S2.E4.m1.12.12.2.1.1.1.2.2.cmml">Œ±</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.12.12.2.1.1.1.2.1" xref="S2.E4.m1.12.12.2.1.1.1.2.1.cmml">‚Äã</mo><mrow id="S2.E4.m1.12.12.2.1.1.1.2.3.2" xref="S2.E4.m1.12.12.2.1.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.12.12.2.1.1.1.2.3.2.1" xref="S2.E4.m1.12.12.2.1.1.1.2.cmml">(</mo><mi mathvariant="normal" id="S2.E4.m1.10.10" xref="S2.E4.m1.10.10.cmml">h</mi><mo stretchy="false" id="S2.E4.m1.12.12.2.1.1.1.2.3.2.2" xref="S2.E4.m1.12.12.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.12.12.2.1.1.1.1" xref="S2.E4.m1.12.12.2.1.1.1.1.cmml">‚àí</mo><mn id="S2.E4.m1.12.12.2.1.1.1.3" xref="S2.E4.m1.12.12.2.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S2.E4.m1.12.12.2.1.1.3" xref="S2.E4.m1.12.12.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.12b"><apply id="S2.E4.m1.12.12.cmml" xref="S2.E4.m1.12.12"><and id="S2.E4.m1.12.12a.cmml" xref="S2.E4.m1.12.12"></and><apply id="S2.E4.m1.12.12b.cmml" xref="S2.E4.m1.12.12"><eq id="S2.E4.m1.12.12.5.cmml" xref="S2.E4.m1.12.12.5"></eq><apply id="S2.E4.m1.12.12.4.cmml" xref="S2.E4.m1.12.12.4"><times id="S2.E4.m1.12.12.4.1.cmml" xref="S2.E4.m1.12.12.4.1"></times><apply id="S2.E4.m1.12.12.4.2.cmml" xref="S2.E4.m1.12.12.4.2"><csymbol cd="ambiguous" id="S2.E4.m1.12.12.4.2.1.cmml" xref="S2.E4.m1.12.12.4.2">subscript</csymbol><ci id="S2.E4.m1.12.12.4.2.2.cmml" xref="S2.E4.m1.12.12.4.2.2">ùëë</ci><apply id="S2.E4.m1.12.12.4.2.3.cmml" xref="S2.E4.m1.12.12.4.2.3"><times id="S2.E4.m1.12.12.4.2.3.1.cmml" xref="S2.E4.m1.12.12.4.2.3.1"></times><apply id="S2.E4.m1.12.12.4.2.3.2.cmml" xref="S2.E4.m1.12.12.4.2.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.12.12.4.2.3.2.1.cmml" xref="S2.E4.m1.12.12.4.2.3.2">subscript</csymbol><ci id="S2.E4.m1.12.12.4.2.3.2.2.cmml" xref="S2.E4.m1.12.12.4.2.3.2.2">H</ci><ci id="S2.E4.m1.12.12.4.2.3.2.3.cmml" xref="S2.E4.m1.12.12.4.2.3.2.3">p</ci></apply><ci id="S2.E4.m1.12.12.4.2.3.3.cmml" xref="S2.E4.m1.12.12.4.2.3.3">Œî</ci><apply id="S2.E4.m1.12.12.4.2.3.4.cmml" xref="S2.E4.m1.12.12.4.2.3.4"><csymbol cd="ambiguous" id="S2.E4.m1.12.12.4.2.3.4.1.cmml" xref="S2.E4.m1.12.12.4.2.3.4">subscript</csymbol><ci id="S2.E4.m1.12.12.4.2.3.4.2.cmml" xref="S2.E4.m1.12.12.4.2.3.4.2">H</ci><ci id="S2.E4.m1.12.12.4.2.3.4.3.cmml" xref="S2.E4.m1.12.12.4.2.3.4.3">p</ci></apply></apply></apply><interval closure="open" id="S2.E4.m1.12.12.4.3.1.cmml" xref="S2.E4.m1.12.12.4.3.2"><ci id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4">D</ci><ci id="S2.E4.m1.5.5.cmml" xref="S2.E4.m1.5.5">T</ci></interval></apply><apply id="S2.E4.m1.11.11.1.cmml" xref="S2.E4.m1.11.11.1"><times id="S2.E4.m1.11.11.1.2.cmml" xref="S2.E4.m1.11.11.1.2"></times><apply id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2"><apply id="S2.E4.m1.2.2.2.cmml" xref="S2.E4.m1.2.2.2"><in id="S2.E4.m1.2.2.2.3.cmml" xref="S2.E4.m1.2.2.2.3"></in><list id="S2.E4.m1.2.2.2.2.2.cmml" xref="S2.E4.m1.2.2.2.2.1"><ci id="S2.E4.m1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1">f</ci><apply id="S2.E4.m1.2.2.2.2.1.1.cmml" xref="S2.E4.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.2.1.1.1.cmml" xref="S2.E4.m1.2.2.2.2.1.1">superscript</csymbol><ci id="S2.E4.m1.2.2.2.2.1.1.2.cmml" xref="S2.E4.m1.2.2.2.2.1.1.2">f</ci><ci id="S2.E4.m1.2.2.2.2.1.1.3.cmml" xref="S2.E4.m1.2.2.2.2.1.1.3">‚Ä≤</ci></apply></list><apply id="S2.E4.m1.2.2.2.4.cmml" xref="S2.E4.m1.2.2.2.4"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.4.1.cmml" xref="S2.E4.m1.2.2.2.4">subscript</csymbol><ci id="S2.E4.m1.2.2.2.4.2.cmml" xref="S2.E4.m1.2.2.2.4.2">H</ci><ci id="S2.E4.m1.2.2.2.4.3.cmml" xref="S2.E4.m1.2.2.2.4.3">p</ci></apply></apply><ci id="S2.E4.m1.2.2.3.cmml" xref="S2.E4.m1.2.2.3">sup</ci></apply><apply id="S2.E4.m1.11.11.1.1.2.cmml" xref="S2.E4.m1.11.11.1.1.1"><abs id="S2.E4.m1.11.11.1.1.2.1.cmml" xref="S2.E4.m1.11.11.1.1.1.2"></abs><apply id="S2.E4.m1.11.11.1.1.1.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1"><minus id="S2.E4.m1.11.11.1.1.1.1.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.3"></minus><apply id="S2.E4.m1.11.11.1.1.1.1.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1"><times id="S2.E4.m1.11.11.1.1.1.1.1.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.2"></times><apply id="S2.E4.m1.11.11.1.1.1.1.1.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.11.11.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E4.m1.11.11.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.3.2">ùîº</ci><apply id="S2.E4.m1.11.11.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.3.3"><in id="S2.E4.m1.11.11.1.1.1.1.1.3.3.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.3.3.1"></in><ci id="S2.E4.m1.11.11.1.1.1.1.1.3.3.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.3.3.2">x</ci><ci id="S2.E4.m1.11.11.1.1.1.1.1.3.3.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.3.3.3">D</ci></apply></apply><apply id="S2.E4.m1.11.11.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.11.11.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1"><neq id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.1"></neq><apply id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2"><times id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.1"></times><ci id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.2.2">f</ci><ci id="S2.E4.m1.6.6.cmml" xref="S2.E4.m1.6.6">x</ci></apply><apply id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3"><times id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.1"></times><apply id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2">superscript</csymbol><ci id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.2">f</ci><ci id="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.1.1.1.1.3.2.3">‚Ä≤</ci></apply><ci id="S2.E4.m1.7.7.cmml" xref="S2.E4.m1.7.7">x</ci></apply></apply></apply></apply><apply id="S2.E4.m1.11.11.1.1.1.1.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2"><times id="S2.E4.m1.11.11.1.1.1.1.2.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.2"></times><apply id="S2.E4.m1.11.11.1.1.1.1.2.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.11.11.1.1.1.1.2.3.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E4.m1.11.11.1.1.1.1.2.3.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.3.2">ùîº</ci><apply id="S2.E4.m1.11.11.1.1.1.1.2.3.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.3.3"><in id="S2.E4.m1.11.11.1.1.1.1.2.3.3.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.3.3.1"></in><ci id="S2.E4.m1.11.11.1.1.1.1.2.3.3.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.3.3.2">x</ci><ci id="S2.E4.m1.11.11.1.1.1.1.2.3.3.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.3.3.3">T</ci></apply></apply><apply id="S2.E4.m1.11.11.1.1.1.1.2.1.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1"><csymbol cd="latexml" id="S2.E4.m1.11.11.1.1.1.1.2.1.2.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.2">delimited-[]</csymbol><apply id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1"><neq id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.1"></neq><apply id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2"><times id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.1"></times><ci id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.2.2">f</ci><ci id="S2.E4.m1.8.8.cmml" xref="S2.E4.m1.8.8">x</ci></apply><apply id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3"><times id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.1"></times><apply id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.1.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2">superscript</csymbol><ci id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.2.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.2">f</ci><ci id="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.3.cmml" xref="S2.E4.m1.11.11.1.1.1.1.2.1.1.1.3.2.3">‚Ä≤</ci></apply><ci id="S2.E4.m1.9.9.cmml" xref="S2.E4.m1.9.9">x</ci></apply></apply></apply></apply></apply></apply></apply></apply><apply id="S2.E4.m1.12.12c.cmml" xref="S2.E4.m1.12.12"><leq id="S2.E4.m1.12.12.6.cmml" xref="S2.E4.m1.12.12.6"></leq><share href="#S2.E4.m1.11.11.1.cmml" id="S2.E4.m1.12.12d.cmml" xref="S2.E4.m1.12.12"></share><apply id="S2.E4.m1.12.12.2.cmml" xref="S2.E4.m1.12.12.2"><times id="S2.E4.m1.12.12.2.2.cmml" xref="S2.E4.m1.12.12.2.2"></times><cn type="integer" id="S2.E4.m1.12.12.2.3.cmml" xref="S2.E4.m1.12.12.2.3">2</cn><apply id="S2.E4.m1.3.3.cmml" xref="S2.E4.m1.3.3"><apply id="S2.E4.m1.3.3.1.cmml" xref="S2.E4.m1.3.3.1"><in id="S2.E4.m1.3.3.1.2.cmml" xref="S2.E4.m1.3.3.1.2"></in><apply id="S2.E4.m1.3.3.1.3.cmml" xref="S2.E4.m1.3.3.1.3"><times id="S2.E4.m1.3.3.1.3.1.cmml" xref="S2.E4.m1.3.3.1.3.1"></times><ci id="S2.E4.m1.3.3.1.3.2.cmml" xref="S2.E4.m1.3.3.1.3.2">ùõº</ci><ci id="S2.E4.m1.3.3.1.1.cmml" xref="S2.E4.m1.3.3.1.1">h</ci></apply><apply id="S2.E4.m1.3.3.1.4.cmml" xref="S2.E4.m1.3.3.1.4"><csymbol cd="ambiguous" id="S2.E4.m1.3.3.1.4.1.cmml" xref="S2.E4.m1.3.3.1.4">subscript</csymbol><ci id="S2.E4.m1.3.3.1.4.2.cmml" xref="S2.E4.m1.3.3.1.4.2">H</ci><ci id="S2.E4.m1.3.3.1.4.3.cmml" xref="S2.E4.m1.3.3.1.4.3">d</ci></apply></apply><ci id="S2.E4.m1.3.3.2.cmml" xref="S2.E4.m1.3.3.2">sup</ci></apply><apply id="S2.E4.m1.12.12.2.1.2.cmml" xref="S2.E4.m1.12.12.2.1.1"><csymbol cd="latexml" id="S2.E4.m1.12.12.2.1.2.1.cmml" xref="S2.E4.m1.12.12.2.1.1.2">delimited-[]</csymbol><apply id="S2.E4.m1.12.12.2.1.1.1.cmml" xref="S2.E4.m1.12.12.2.1.1.1"><minus id="S2.E4.m1.12.12.2.1.1.1.1.cmml" xref="S2.E4.m1.12.12.2.1.1.1.1"></minus><apply id="S2.E4.m1.12.12.2.1.1.1.2.cmml" xref="S2.E4.m1.12.12.2.1.1.1.2"><times id="S2.E4.m1.12.12.2.1.1.1.2.1.cmml" xref="S2.E4.m1.12.12.2.1.1.1.2.1"></times><ci id="S2.E4.m1.12.12.2.1.1.1.2.2.cmml" xref="S2.E4.m1.12.12.2.1.1.1.2.2">ùõº</ci><ci id="S2.E4.m1.10.10.cmml" xref="S2.E4.m1.10.10">h</ci></apply><cn type="integer" id="S2.E4.m1.12.12.2.1.1.1.3.cmml" xref="S2.E4.m1.12.12.2.1.1.1.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.12c">d_{\pazocal{H}_{p}\Delta\pazocal{H}_{p}}(\pazocal{D},\pazocal{T})=\underset{f,f^{\prime}\in\pazocal{H}_{p}}{\mathrm{sup}}|\mathbb{E}_{x\in\pazocal{D}}[f(x)\neq f^{\prime}(x)]-\mathbb{E}_{x\in\pazocal{T}}[f(x)\neq f^{\prime}(x)]|\leq 2\underset{\alpha(h)\in\pazocal{H}_{d}}{\mathrm{sup}}[\alpha(h)-1]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS0.Px1.p1.30" class="ltx_p">where, <math id="S2.SS4.SSS0.Px1.p1.27.m1.1" class="ltx_Math" alttext="\alpha(h)" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.27.m1.1a"><mrow id="S2.SS4.SSS0.Px1.p1.27.m1.1.2" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.2.cmml"><mi id="S2.SS4.SSS0.Px1.p1.27.m1.1.2.2" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.2.2.cmml">Œ±</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS0.Px1.p1.27.m1.1.2.1" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.2.1.cmml">‚Äã</mo><mrow id="S2.SS4.SSS0.Px1.p1.27.m1.1.2.3.2" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.2.cmml"><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.27.m1.1.2.3.2.1" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.2.cmml">(</mo><mi id="S2.SS4.SSS0.Px1.p1.27.m1.1.1" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.1.cmml">h</mi><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.27.m1.1.2.3.2.2" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.27.m1.1b"><apply id="S2.SS4.SSS0.Px1.p1.27.m1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.2"><times id="S2.SS4.SSS0.Px1.p1.27.m1.1.2.1.cmml" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.2.1"></times><ci id="S2.SS4.SSS0.Px1.p1.27.m1.1.2.2.cmml" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.2.2">ùõº</ci><ci id="S2.SS4.SSS0.Px1.p1.27.m1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.27.m1.1.1">‚Ñé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.27.m1.1c">\alpha(h)</annotation></semantics></math> is optimal domain classifier and <math id="S2.SS4.SSS0.Px1.p1.28.m2.1" class="ltx_Math" alttext="\pazocal{H}_{d}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.28.m2.1a"><msub id="S2.SS4.SSS0.Px1.p1.28.m2.1.1" xref="S2.SS4.SSS0.Px1.p1.28.m2.1.1.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.28.m2.1.1.2" xref="S2.SS4.SSS0.Px1.p1.28.m2.1.1.2.cmml">H</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.28.m2.1.1.3" xref="S2.SS4.SSS0.Px1.p1.28.m2.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.28.m2.1b"><apply id="S2.SS4.SSS0.Px1.p1.28.m2.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.28.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.28.m2.1.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.28.m2.1.1">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.28.m2.1.1.2.cmml" xref="S2.SS4.SSS0.Px1.p1.28.m2.1.1.2">H</ci><ci id="S2.SS4.SSS0.Px1.p1.28.m2.1.1.3.cmml" xref="S2.SS4.SSS0.Px1.p1.28.m2.1.1.3">d</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.28.m2.1c">\pazocal{H}_{d}</annotation></semantics></math> is the hypothesis space of domain classifier. Zhao et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> prove that optimal state of minimum discrepancy distance <math id="S2.SS4.SSS0.Px1.p1.29.m3.2" class="ltx_Math" alttext="d_{\pazocal{H}_{p}\Delta\pazocal{H}_{p}}(\pazocal{D},\pazocal{T})" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.29.m3.2a"><mrow id="S2.SS4.SSS0.Px1.p1.29.m3.2.3" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.cmml"><msub id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.cmml"><mi id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.2" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.2.cmml">d</mi><mrow id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.cmml"><msub id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.2" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.2.cmml">H</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.3" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.3.cmml">p</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.1" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.3" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.3.cmml">Œî</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.1a" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.1.cmml">‚Äã</mo><msub id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.cmml"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.2" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.2.cmml">H</mi><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.3" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.3.cmml">p</mi></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.1" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.1.cmml">‚Äã</mo><mrow id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.2" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.2.1" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.1.cmml">(</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.29.m3.1.1" xref="S2.SS4.SSS0.Px1.p1.29.m3.1.1.cmml">D</mi><mo id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.2.2" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.29.m3.2.2" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.2.cmml">T</mi><mo stretchy="false" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.2.3" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.29.m3.2b"><apply id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3"><times id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.1"></times><apply id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.1.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.2.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.2">ùëë</ci><apply id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3"><times id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.1"></times><apply id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.1.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.2.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.2">H</ci><ci id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.3.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.2.3">p</ci></apply><ci id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.3.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.3">Œî</ci><apply id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4"><csymbol cd="ambiguous" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.1.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4">subscript</csymbol><ci id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.2.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.2">H</ci><ci id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.3.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.2.3.4.3">p</ci></apply></apply></apply><interval closure="open" id="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.1.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.3.3.2"><ci id="S2.SS4.SSS0.Px1.p1.29.m3.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.1.1">D</ci><ci id="S2.SS4.SSS0.Px1.p1.29.m3.2.2.cmml" xref="S2.SS4.SSS0.Px1.p1.29.m3.2.2">T</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.29.m3.2c">d_{\pazocal{H}_{p}\Delta\pazocal{H}_{p}}(\pazocal{D},\pazocal{T})</annotation></semantics></math> is when the domain classifier has random predictions achieving a state of highest entropy. We argue that it is achieved when the representations for samples in two domains are most similar, leading to a random domain classifier that is unable to distinguish between the two dataset distributions. Motivated by this intuition, we can use a strategy based on selecting samples with the most similar representations to our task dataset <math id="S2.SS4.SSS0.Px1.p1.30.m4.1" class="ltx_Math" alttext="\pazocal{T}" display="inline"><semantics id="S2.SS4.SSS0.Px1.p1.30.m4.1a"><mi mathvariant="normal" id="S2.SS4.SSS0.Px1.p1.30.m4.1.1" xref="S2.SS4.SSS0.Px1.p1.30.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px1.p1.30.m4.1b"><ci id="S2.SS4.SSS0.Px1.p1.30.m4.1.1.cmml" xref="S2.SS4.SSS0.Px1.p1.30.m4.1.1">T</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px1.p1.30.m4.1c">\pazocal{T}</annotation></semantics></math>. We use the embedding similarity as a proxy for dataset similarity as getting the optimal representation is challenging in unpractical in the case of large corpus.</p>
</div>
</section>
<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>Efficient Task-Similar Domain-adaptive Continual Pre-training</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">We stipulate that we can form an optimal set <math id="S2.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\pazocal{D}^{*}" display="inline"><semantics id="S2.SS4.SSS1.p1.1.m1.1a"><msup id="S2.SS4.SSS1.p1.1.m1.1.1" xref="S2.SS4.SSS1.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S2.SS4.SSS1.p1.1.m1.1.1.2" xref="S2.SS4.SSS1.p1.1.m1.1.1.2.cmml">D</mi><mo id="S2.SS4.SSS1.p1.1.m1.1.1.3" xref="S2.SS4.SSS1.p1.1.m1.1.1.3.cmml">‚àó</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p1.1.m1.1b"><apply id="S2.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1">superscript</csymbol><ci id="S2.SS4.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1.2">D</ci><times id="S2.SS4.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS4.SSS1.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p1.1.m1.1c">\pazocal{D}^{*}</annotation></semantics></math> by selecting a portion of the domain data that is much closer to the task data (red) given by the blue region based on intuition before. We refer to this as <span id="S2.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_italic">Efficient Task-Similar Domain-adaptive Continual Pre-training</span> (ETS-DACP). Fine-tuning LLMs can take a good amount of instructions, which are quite costly to create. ETS-DACP directly addresses this situation by using the relatively limited unlabeled task data to sample similar samples from the larger pool of pre-training domain corpus.
We are motivated by prior research showing that unsupervised training on tokens that closely align with the target domain and tasks can lead to improved performance <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Therefore, we hypothesize that continual pre-training LLMs on the unlabeled task data can be beneficial for target task performance as it adapts the model to the distribution of task tokens.</p>
</div>
<div id="S2.SS4.SSS1.p2" class="ltx_para">
<p id="S2.SS4.SSS1.p2.1" class="ltx_p">We use similarity between embeddings of task data and domain corpus samples to perform data selection. This allows us to select a subset from the domain corpus that closely resembles the distribution of task data. To quantify document-level task similarity, we employ cosine similarity between the document embedding and task data embedding. Prior works like &nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> calculate embeddings from language model (RoBERTa) for a given unlabeled sample twice, which is not practical for LLMs. It takes a forward pass to compute the embeddings using LLM over entire corpus, or 25% of compute of using to train the pre-train the LLM over the entire corpus. We compute embeddings using the <span id="S2.SS4.SSS1.p2.1.1" class="ltx_text ltx_font_italic">Spacy</span> model <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. This approach allows us to cost-effectively measure the alignment between task-specific information and the financial corpus, enabling more focused and targeted pre-training.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Efficient Task-Agnostic Domain-adaptive Continual Pre-training</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">While the previous case dealt with scenarios where task data is provided to us, in this method we explore scenarios where we do not have task data. This method also overcomes the limitation of ETS-DACP which makes the LLM too tuned to the task data instead of broader domain. We stipulate that two dimensions are important for obtaining domain information from a subset of pre-training domain data: <span id="S2.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_bold">novelty</span> and <span id="S2.SS4.SSS2.p1.1.2" class="ltx_text ltx_font_bold">diversity</span>.</p>
</div>
<div id="S2.SS4.SSS2.p2" class="ltx_para">
<p id="S2.SS4.SSS2.p2.1" class="ltx_p"><span id="S2.SS4.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Novelty</span> refers to the information that was unseen by the LLM before. We gauge the level of novelty in a document based on the <span id="S2.SS4.SSS2.p2.1.2" class="ltx_text ltx_font_bold">perplexity</span> recorded by LLM. Documents with higher perplexity are less represented in the original training corpus, thus being more likely to contain novel knowledge for the model. Such samples are also viewed as more difficult to learn <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Hence, these samples can be valuable in continual pre-training to help models acquire novel information.</p>
</div>
<div id="S2.SS4.SSS2.p3" class="ltx_para">
<p id="S2.SS4.SSS2.p3.1" class="ltx_p">Evaluating perplexity directly on the benchmark model incurs significant costs, as the inference requires approximately 25% of the training compute. To minimize this cost, we employ Pythia-70m as a surrogate model for computing document perplexity. Our preliminary experiment using a sample dataset reveals a strong correlation of 0.97 between the perplexity obtained from Pythia-1B and Pythia-70m. This high correlation justifies the use of a smaller model as a reliable surrogate, enabling more cost-effective sampling based on perplexity.</p>
</div>
<div id="S2.SS4.SSS2.p4" class="ltx_para">
<p id="S2.SS4.SSS2.p4.1" class="ltx_p"><span id="S2.SS4.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Diversity</span> captures the diversity of distributions of token types in the domain corpus. Diversity has been shown to be an effective feature in related research on curriculum learning in language modeling <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. We use part-of-speech (POS) tagging to get token types. Since entropy has been shown to be one of the best measures of diversity&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, we use <span id="S2.SS4.SSS2.p4.1.2" class="ltx_text ltx_font_bold">entropy</span> of POS tags <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> as our diversity measure.</p>
</div>
</section>
<section id="S2.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3 </span>Data Sampling Strategy</h4>

<div id="S2.SS4.SSS3.p1" class="ltx_para">
<p id="S2.SS4.SSS3.p1.1" class="ltx_p">We proposed ETS-DACP and ETA-DACP to enhance vanilla DACP by refining the pre-training data through active selection of relevant samples. We can select the data in two ways:</p>
</div>
<section id="S2.SS4.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Hard Sampling:</h5>

<div id="S2.SS4.SSS3.Px1.p1" class="ltx_para">
<p id="S2.SS4.SSS3.Px1.p1.1" class="ltx_p">We rank the samples in the domain corpus by the measure of choice. We select top-k samples from the domain corpus based on the metric(s), where <math id="S2.SS4.SSS3.Px1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS4.SSS3.Px1.p1.1.m1.1a"><mi id="S2.SS4.SSS3.Px1.p1.1.m1.1.1" xref="S2.SS4.SSS3.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.Px1.p1.1.m1.1b"><ci id="S2.SS4.SSS3.Px1.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS3.Px1.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.Px1.p1.1.m1.1c">k</annotation></semantics></math> is the number of samples needed to hit the pre-decided token budget for continual pre-training.</p>
</div>
</section>
<section id="S2.SS4.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Soft Sampling:</h5>

<div id="S2.SS4.SSS3.Px2.p1" class="ltx_para">
<p id="S2.SS4.SSS3.Px2.p1.1" class="ltx_p">In this case, instead of giving binary weights by leaving out all the other examples in the corpus, we assign soft weights based on the distance metric. This allows for the continual pre-training to see the samples outside the blue region in Figure&nbsp;<a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1 Financial Corpus Curation ‚Ä£ 2 Methodology ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> as well, adding some diversity to the pre-training data.</p>
</div>
<div id="S2.SS4.SSS3.Px2.p2" class="ltx_para">
<p id="S2.SS4.SSS3.Px2.p2.1" class="ltx_p">We use the following three dimensions for selecting samples: similarity to task data (ETS-DACP), perplexity as a proxy for novelty (ETA-DACP), and diversity measured by token type entropy (ETA-DACP). In order to convert metric values into sampling probabilities, we propose a method based on quantile ranges. To achieve this, we first calculate the 0-100 quantiles for each metric within the training data. By dividing the range into 100 intervals using the 100 quantile values, documents are then assigned probabilities corresponding to the interval they fall into. This approach effectively normalizes our metrics, allowing for the aggregation of different metric types.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluation tasks</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We evaluate the models on financial tasks to evalaute the effectiveness of our domain-adaptive continual pre-training. We adopt the <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">FLARE</span> framework <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to evaluate our models. FLARE extends the LLM evaluation framework <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">lm-evaluation-harness<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">‚Ä°</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä°</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_upright">‚Ä°</span></span><span id="footnote1.5" class="ltx_text ltx_font_upright">https://github.com/EleutherAI/lm-evaluation-harness</span></span></span></span></span> by including various financial tasks. We follow their instruction prompt, data split, and metric computation for comparison. We consider following 4 tasks used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>: (1) <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_bold">Financial Phrase Bank</span>. FPB is a sentiment classification task on financial news <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The sentiment reflects whether the news is considered as positive/neutral/negative by investors. (2) <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_bold">FiQA SA.</span> An aspect based sentiment classification task based on financial news and headlines <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. (3) <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_bold">Headline.</span> Binary classification task on whether a headline on a financial entity contains certain information <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Each news article is associated with 9 tags like ‚Äúprice or not‚Äù, ‚Äúprice up‚Äù, ‚Äúprice down‚Äù, ‚Äúprice stable‚Äù, ‚Äúpast price‚Äù, and ‚Äúasset‚Äù. (4) <span id="S3.SS1.p1.1.6" class="ltx_text ltx_font_bold">NER.</span> Financial named entity extraction task is based on credit risk assessment section of SEC reports. Words in this task are annotated with PER, LOC, ORG, and MISC.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Setup and Infrastructure</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For our benchmark pre-trained LLM model, we select 1B and 6.9B parameter models from the Pythia suite <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The Pythia model suite offers a diverse array of model sizes, ranging from 70 million to 12 billion parameters. The continual pre-training configuration is tailored from Pythia‚Äôs training setup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Specifically, we set a learning rate of 1.2e-05 for FinPythia-6.9B and 3e-05 for <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">FinPythia-1B</span>, the smallest learning rates in their original schedules. We use small learning rates to mitigate catastrophic forgetting. We keep them constant throughout the course for efficient pre-training. We use the precision of bf16 rather than fp16 used in Pythia. We half the original batch size to 512.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2311.08545/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Training loss of FinPythia-6.9B. FinPythia-6.9B achieves significant loss drop in financial corpus at mild expense of Pile loss. </span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We run the continual pre-training job on one P4d.24xlarge instance through AWS SageMaker. As the model size is moderate, we only use data parallelism via DeepSpeed ZeRO Stage 2 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> with activation checkpointing enabled. It takes 18 days for FinPythia-6.9B to pre-train and 3 days for FinPythia-1B to pre-train on 24 billion tokens.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Analysis</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Domain-adaptive Continual Pre-training</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To monitor the pre-training process, we randomly sample 0.1% of our financial corpus as a financial test dataset. The model is also evaluated on the Pile test dataset. The loss trajectory for FinPythia-6.9B is reported in Figure <a href="#S3.F2" title="Figure 2 ‚Ä£ 3.2 Training Setup and Infrastructure ‚Ä£ 3 Experimental Setup ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The training loss is smoothed using a moving average of 50 optimization steps. We observe a sharp decrease in Financial test (Fin test) loss during the early stage of continual pre-training, and the progress gradually becomes saturated, similar to the loss trajectory of training from scratch <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The loss log suggests that domain-adaptive continual pre-training succeeds in adopting Pythia to the financial domains at the expense of a mild increase in Pile loss (Pile test).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:105.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-79.0pt,19.2pt) scale(0.732842987139534,0.732842987139534) ;">
<table id="S4.T1.5.5" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.5.5.6.1" class="ltx_tr">
<td id="S4.T1.5.5.6.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T1.5.5.6.1.2" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T1.5.5.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.5.5.6.1.3.1" class="ltx_text ltx_font_bold">BloombergGPT</span></td>
<td id="S4.T1.5.5.6.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.6.1.4.1" class="ltx_text ltx_font_bold">OPT 7B</span></td>
<td id="S4.T1.5.5.6.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.6.1.5.1" class="ltx_text ltx_font_bold">BLOOM 7B</span></td>
<td id="S4.T1.5.5.6.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.5.5.6.1.6.1" class="ltx_text ltx_font_bold">GPT-J-6B</span></td>
<td id="S4.T1.5.5.6.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.6.1.7.1" class="ltx_text ltx_font_bold">Pythia 1B</span></td>
<td id="S4.T1.5.5.6.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.6.1.8.1" class="ltx_text ltx_font_bold">Pythia 7B</span></td>
<td id="S4.T1.5.5.6.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.6.1.9.1" class="ltx_text ltx_font_bold">FinPythia 1B</span></td>
<td id="S4.T1.5.5.6.1.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.5.5.6.1.10.1" class="ltx_text ltx_font_bold">FinPythia 7B</span></td>
</tr>
<tr id="S4.T1.5.5.7.2" class="ltx_tr">
<td id="S4.T1.5.5.7.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.7.2.1.1" class="ltx_text ltx_font_bold">FPB</span></td>
<td id="S4.T1.5.5.7.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Acc</td>
<td id="S4.T1.5.5.7.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.5.5.7.2.4" class="ltx_td ltx_align_center ltx_border_t">57.22</td>
<td id="S4.T1.5.5.7.2.5" class="ltx_td ltx_align_center ltx_border_t">52.68</td>
<td id="S4.T1.5.5.7.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50.21</td>
<td id="S4.T1.5.5.7.2.7" class="ltx_td ltx_align_center ltx_border_t">42.85</td>
<td id="S4.T1.5.5.7.2.8" class="ltx_td ltx_align_center ltx_border_t">54.64</td>
<td id="S4.T1.5.5.7.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.7.2.9.1" class="ltx_text ltx_framed ltx_framed_underline">47.14</span></td>
<td id="S4.T1.5.5.7.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.7.2.10.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">59.90</span></td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.2" class="ltx_td"></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">F1</td>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">51.07<sup id="S4.T1.1.1.1.1.1" class="ltx_sup">‚àó</sup>
</td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">65.77</span></td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center">52.11</td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r">49.31</td>
<td id="S4.T1.1.1.1.7" class="ltx_td ltx_align_center">4.394</td>
<td id="S4.T1.1.1.1.8" class="ltx_td ltx_align_center">55.79</td>
<td id="S4.T1.1.1.1.9" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.1.9.1" class="ltx_text ltx_framed ltx_framed_underline">46.52</span></td>
<td id="S4.T1.1.1.1.10" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.1.10.1" class="ltx_text ltx_framed ltx_framed_underline">64.43</span></td>
</tr>
<tr id="S4.T1.5.5.8.3" class="ltx_tr">
<td id="S4.T1.5.5.8.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.8.3.1.1" class="ltx_text ltx_font_bold">FiQA SA</span></td>
<td id="S4.T1.5.5.8.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Acc</td>
<td id="S4.T1.5.5.8.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.5.5.8.3.4" class="ltx_td ltx_align_center ltx_border_t">40.43</td>
<td id="S4.T1.5.5.8.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.8.3.5.1" class="ltx_text ltx_font_bold">70.21</span></td>
<td id="S4.T1.5.5.8.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.42</td>
<td id="S4.T1.5.5.8.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.8.3.7.1" class="ltx_text ltx_framed ltx_framed_underline">54.51</span></td>
<td id="S4.T1.5.5.8.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.5.5.8.3.8.1" class="ltx_text ltx_framed ltx_framed_underline">60.85</span></td>
<td id="S4.T1.5.5.8.3.9" class="ltx_td ltx_align_center ltx_border_t">46.13</td>
<td id="S4.T1.5.5.8.3.10" class="ltx_td ltx_align_center ltx_border_t">52.34</td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2.2" class="ltx_td"></td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">F1</td>
<td id="S4.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r">75.07<sup id="S4.T1.2.2.2.1.1" class="ltx_sup">‚àó</sup>
</td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center">31.29</td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.2.2.2.5.1" class="ltx_text ltx_font_bold">74.11</span></td>
<td id="S4.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_r">62.14</td>
<td id="S4.T1.2.2.2.7" class="ltx_td ltx_align_center"><span id="S4.T1.2.2.2.7.1" class="ltx_text ltx_framed ltx_framed_underline">56.29</span></td>
<td id="S4.T1.2.2.2.8" class="ltx_td ltx_align_center"><span id="S4.T1.2.2.2.8.1" class="ltx_text ltx_framed ltx_framed_underline">61.33</span></td>
<td id="S4.T1.2.2.2.9" class="ltx_td ltx_align_center">44.53</td>
<td id="S4.T1.2.2.2.10" class="ltx_td ltx_align_center">53.04</td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<td id="S4.T1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.3.2.1" class="ltx_text ltx_font_bold">Headline</span></td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">F1</td>
<td id="S4.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.20<sup id="S4.T1.3.3.3.1.1" class="ltx_sup">‚àó</sup>
</td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.3.4.1" class="ltx_text ltx_font_bold">62.62</span></td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">42.68</td>
<td id="S4.T1.3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">45.54</td>
<td id="S4.T1.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t">44.73</td>
<td id="S4.T1.3.3.3.8" class="ltx_td ltx_align_center ltx_border_t">43.83</td>
<td id="S4.T1.3.3.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.3.9.1" class="ltx_text ltx_framed ltx_framed_underline">53.02</span></td>
<td id="S4.T1.3.3.3.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.3.10.1" class="ltx_text ltx_framed ltx_framed_underline">54.14</span></td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<td id="S4.T1.4.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.4.4.2.1" class="ltx_text ltx_font_bold">NER</span></td>
<td id="S4.T1.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">F1</td>
<td id="S4.T1.4.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">60.82<sup id="S4.T1.4.4.4.1.1" class="ltx_sup">‚àó</sup>
</td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">41.91</td>
<td id="S4.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t">18.97</td>
<td id="S4.T1.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.87</td>
<td id="S4.T1.4.4.4.7" class="ltx_td ltx_align_center ltx_border_t">49.15</td>
<td id="S4.T1.4.4.4.8" class="ltx_td ltx_align_center ltx_border_t">41.60</td>
<td id="S4.T1.4.4.4.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.4.4.9.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline">55.51</span></td>
<td id="S4.T1.4.4.4.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.4.4.4.10.1" class="ltx_text ltx_framed ltx_framed_underline">48.42</span></td>
</tr>
<tr id="S4.T1.5.5.5" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T1.5.5.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.5.5.5.2.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Average</span></td>
<td id="S4.T1.5.5.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.5.5.5.3.1" class="ltx_text" style="background-color:#E6E6E6;">F1</span></td>
<td id="S4.T1.5.5.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.5.5.5.1.1" class="ltx_text" style="background-color:#E6E6E6;">67.29<sup id="S4.T1.5.5.5.1.1.1" class="ltx_sup">‚àó</sup></span></td>
<td id="S4.T1.5.5.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.5.5.5.4.1" class="ltx_text" style="background-color:#E6E6E6;">50.40</span></td>
<td id="S4.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.5.5.5.5.1" class="ltx_text" style="background-color:#E6E6E6;">46.97</span></td>
<td id="S4.T1.5.5.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.5.5.5.6.1" class="ltx_text" style="background-color:#E6E6E6;">48.22</span></td>
<td id="S4.T1.5.5.5.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.5.5.5.7.1" class="ltx_text" style="background-color:#E6E6E6;">48.53</span></td>
<td id="S4.T1.5.5.5.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.5.5.5.8.1" class="ltx_text" style="background-color:#E6E6E6;">50.64</span></td>
<td id="S4.T1.5.5.5.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.5.5.5.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#E6E6E6;">49.90</span></td>
<td id="S4.T1.5.5.5.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.5.5.5.10.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="background-color:#E6E6E6;">54.83</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.11.2.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.7.1" class="ltx_text" style="font-size:90%;"> 5-shot results on financial tasks from domain adaptive continual pre-training. <math id="S4.T1.7.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S4.T1.7.1.m1.1b"><mo id="S4.T1.7.1.m1.1.1" xref="S4.T1.7.1.m1.1.1.cmml">‚àó</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.1.m1.1c"><times id="S4.T1.7.1.m1.1.1.cmml" xref="S4.T1.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.1.m1.1d">*</annotation></semantics></math> indicates that the results are reported from BloombergGPT <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, which are not comparable as they have been evaluated with different prompts and data splits. The values is not directly comparable to others. <span id="S4.T1.7.1.1" class="ltx_text ltx_font_bold">Bold</span> indicates the best results among all the evaluated models except BloombergGPT. <span id="S4.T1.7.1.2" class="ltx_text ltx_framed ltx_framed_underline">Underline</span> indicates the better results between FinPythia and Pythia of the same sizes. </span></figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To evaluate financial domain tasks, we compare FinPythia with Pythia and other open-sourced models of similar size. We include OPT-7B <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, BLOOM-7B <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, and GPT-J-6B <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> as benchmark models. While we report results from open-sourced models, the main insights are obtained from the comparison between Pythia and FinPythia, as their difference reflect the effect of domain-adaptive continual pre-training. Models are evaluated in a 5-shot setting for each task. Shots are randomly sampled from the tasks‚Äô training dataset for each test instance following FLARE&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> benchmark.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Results are reported in Table&nbsp;<a href="#S4.T1" title="Table 1 ‚Ä£ 4.1 Domain-adaptive Continual Pre-training ‚Ä£ 4 Results and Analysis ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. FinPythia-6.9B and FinPythia-1B exhibit superior performance on tasks FPB, Headline, and NER while showing comparatively lower results on the FiQA SA task compared with Pythia counterparts. DACP boosts the average task performance by 2.8% for the 1B model and 8.3% for the 6.9B model. These outcomes directly substantiate the impact of domain-adaptive continual pre-training on enhancing in-domain task performance. Furthermore, Pythia-6.9B outperforms OPT-7B, BLOOM-7B, and GPT-J-6B on average.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><em id="S4.SS1.p4.1.1" class="ltx_emph ltx_font_italic">Comparison with BloombergGPT</em>: results reported on FLARE are not directly comparable with results reported in BloombergGPT&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> on the same tasks, as the data splits used are not public. We could not match the performance of publicly available models like OPT-66B or GPT-NeoX reported by &nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, on all four tasks. See the detailed comparison between the results in Appendix&nbsp;<a href="#A1" title="Appendix A Benchmark BloombergGPT‚Äôs Performance ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. </p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.1.1" class="ltx_tr">
<td id="S4.T2.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T2.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.1.1.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question:</span><span id="S4.T2.2.1.1.1.1.1.2" class="ltx_text" style="font-size:80%;"> What is a SPAC?</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.2.2.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pythia6.9B:</span><span id="S4.T2.2.2.2.1.1.1.2" class="ltx_text" style="font-size:80%;"> Spac is an acronym for Space. It is defined as either (1) above, just beneath, below, above, or through a port in the body of a planet, Moon, or other heavenly body; or (2) beyond a star‚Äôs sphere of influence, usually in interplanetary space or interstellar space; also, (3) outside the limits of the Milky Way Galaxy.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.3.3" class="ltx_tr">
<td id="S4.T2.2.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.3.3.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.3.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">FinPythia-6.9B:</span><span id="S4.T2.2.3.3.1.1.1.2" class="ltx_text" style="font-size:80%;"> A SPAC is a type of merger and acquisition vehicle that is used to raise money from investors, use that money to identify a company that could be valuable to an interested party, then merge with the company and take it public. The use of the SPAC is to give investors a piece of the upside in the acquired company and allows the company to issue shares that may give investors a piece of the upside of the company as well.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.4.4" class="ltx_tr">
<td id="S4.T2.2.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.4.4.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.4.4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question:</span><span id="S4.T2.2.4.4.1.1.1.2" class="ltx_text" style="font-size:80%;"> Who is Cathie Wood?</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.5.5" class="ltx_tr">
<td id="S4.T2.2.5.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.5.5.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.5.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pythia-6.9B:</span><span id="S4.T2.2.5.5.1.1.1.2" class="ltx_text" style="font-size:80%;"> Cathie Wood is the founder of All Things Organic, an organic marketing company that she founded in 2000. In addition to being an organic marketer, Cathie is also a Certified Coach, foodie, cook, a past lover of yoga, a certified teacher, mother and wife to three gorgeous children.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.6.6" class="ltx_tr">
<td id="S4.T2.2.6.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.6.6.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.6.6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">FinPythia-6.9B:</span><span id="S4.T2.2.6.6.1.1.1.2" class="ltx_text" style="font-size:80%;"> Cathie Wood is an American investor and advisor, the founder and chief executive officer of Ark Investment Management.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.7.7" class="ltx_tr">
<td id="S4.T2.2.7.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.7.7.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.7.7.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question:</span><span id="S4.T2.2.7.7.1.1.1.2" class="ltx_text" style="font-size:80%;"> How do you like Fed‚Äôs latest policy to fight inflation?</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.8.8" class="ltx_tr">
<td id="S4.T2.2.8.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.8.8.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.8.8.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pythia6.9B:</span><span id="S4.T2.2.8.8.1.1.1.2" class="ltx_text" style="font-size:80%;"> I like Fed‚Äôs latest policy to fight inflation, which was reported by Tyler. Fed‚Äôs latest policy was reported in a meeting on September 12 and it was named as SOMO, which stood for Standardized Open Market Operations. According to the policy, the U.S. central bank could buy $40 billion worth of financial instruments if the price of the instruments reach 6 percent.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.9.9" class="ltx_tr">
<td id="S4.T2.2.9.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.9.9.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.9.9.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">FinPythia-6.9B:</span><span id="S4.T2.2.9.9.1.1.1.2" class="ltx_text" style="font-size:80%;"> There is a big confusion about Fed‚Äôs new policy to fight inflation. They are following a new policy from September 2022 called the fastest quarter-percentage point rate hikes in two decades to fight the ‚Äùcostly‚Äù inflation which is the 8.5 percent rate of increase in food and energy prices that may aggravate inflation and lead to recession in the near future.</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.10.10" class="ltx_tr">
<td id="S4.T2.2.10.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.10.10.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.10.10.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Question:</span><span id="S4.T2.2.10.10.1.1.1.2" class="ltx_text" style="font-size:80%;"> What is the difference between bond and stock in terms of risk profile?</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.11.11" class="ltx_tr">
<td id="S4.T2.2.11.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T2.2.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.11.11.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.11.11.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pythia-6.9B: </span><span id="S4.T2.2.11.11.1.1.1.2" class="ltx_text" style="font-size:80%;"> Stock is less risky (you can liquidate your shares anytime) but bond is more risky (you can‚Äôt liquidate).</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.2.12.12" class="ltx_tr">
<td id="S4.T2.2.12.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T2.2.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.2.12.12.1.1.1" class="ltx_p" style="width:416.3pt;"><span id="S4.T2.2.12.12.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">FinPythia-6.9B: </span><span id="S4.T2.2.12.12.1.1.1.2" class="ltx_text" style="font-size:80%;"> When you buy a bond, you receive a set of fixed payments from the issuer at a regular interval over a fixed term. You take on the full risk of the coupon payments not being made, so a bond is always considered to be a lower risk debt instrument than a stock. However, a stock is a more liquid investment than a bond. When buying a stock, you are buying shares in the company that the bond represents and you hold on to the stock until you want to cash out, and it can be used as collateral to borrow money.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.5.1.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S4.T2.6.2" class="ltx_text" style="font-size:113%;">Generation examples from Pythia-6.9B and FinPythia-6.9B. We observe FinPythia‚Äôs updated financial events knowledge post 2021, providing factual answers with reasoning as shown in the last example.</span></figcaption>
</figure>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><em id="S4.SS1.p5.1.1" class="ltx_emph ltx_font_italic">Qualitative Evaluation</em>: qualitative examples generated by Pythia-6.9B and FinPythia-6.9B are presented in Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.1 Domain-adaptive Continual Pre-training ‚Ä£ 4 Results and Analysis ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Upon examination, we observe that FinPythia-6.9B generates more relevant and detailed responses for finance-related questions. It acquired the financial events knowledge post 2021 with the continual pre-training. These findings suggest that the DACP helps FinPythia-6.9B acquire in-domain knowledge.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Efficient Domain-adaptive Continual Pre-training</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">FLARE uses 5-shot in-context performance over the entire training data, <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">i.e.,</span> each test example while evaluating each model sees different train examples. This also makes it harder to compare different models, as each test example sees completely different 5 training examples across models. To overcome this randomness and make the comparisons more fair, we set aside a pool of 50 labeled data samples from the training dataset for each task, referred to as the ‚Äùshot pool‚Äù. For the remaining training samples, we remove their labels and utilize them as unlabeled task data, which is used in our data selection strategy utilizing task data. This particular configuration is adopted because we do not have access to unlabeled task data to evaluate the efficacy of TACP. By using this setup, we also simulate the constraints posed by scarce labeled data. Although this approach creates unlabeled task data for TACP, the size is too small, containing only 0.24 million tokens from the four tasks.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Using Efficient DACP methods, we select 10% subset of the financial corpus for each method. We also create another version of ETS-DACP called <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">ETS-DACP-com</span> by using the other two measures with similarity by averaging all three measures for ranking/weighting.
To mitigate overfitting, both the TACP and Efficient DACP methods run for a single epoch, employing the same pre-training configuration as DACP to ensure a fair comparison. We run these experiments with Pythia-1B due to the compute budget. We perform the evaluation ten times using different random seeds and report the mean performance for each of our four financial tasks.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The evaluation results are presented in Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Efficient Domain-adaptive Continual Pre-training ‚Ä£ 4 Results and Analysis ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. While TACP shows significant improvement in model performance compared to the original Pythia-1B, ETS-DACP stands out as the top-performing approach among DACP, TACP, and efficient DACP methods in terms of average task performance.
This enhanced performance cannot be solely attributed to the increased number of tokens, as DACP with the same amount of tokens yields inferior results. The results underscore the efficacy of both task-adaptive and domain continual pre-training LLMs on unlabeled task data, in line with results observed in other model types <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">We can observe the following: 1) ETS-DACP trained on 10% outperforms DACP with 100% of the data; 2) ETS-DACP has the best performance among all three counterparts and is on par with a combination of three metrics - ETS-DACP-com; 3) ETA-DACP-ent trained on 10% corpus is a close second despite not having any access to task data, handily surpassing DACP trained on 100% of the data; and 4) Efficient DACP methods with hard sampling outperform ones with soft sampling.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:184.5pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-122.7pt,52.0pt) scale(0.63858817993497,0.63858817993497) ;">
<table id="S4.T3.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.2.1.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S4.T3.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.2.1.1.1.2.1" class="ltx_text ltx_font_bold">Tokens</span></td>
<td id="S4.T3.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.2.1.1.1.3.1" class="ltx_text ltx_font_bold">FPB</span></td>
<td id="S4.T3.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T3.2.1.1.1.4.1" class="ltx_text ltx_font_bold">FiQA SA</span></td>
<td id="S4.T3.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.2.1.1.1.5.1" class="ltx_text ltx_font_bold">Headline</span></td>
<td id="S4.T3.2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.2.1.1.1.6.1" class="ltx_text ltx_font_bold">NER</span></td>
<td id="S4.T3.2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.1.1.7.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Average</span></td>
<td id="S4.T3.2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.1.1.8.1" class="ltx_text ltx_font_bold" style="background-color:#E6E6E6;">Win Rate (%)</span></td>
</tr>
<tr id="S4.T3.2.1.2.2" class="ltx_tr">
<th id="S4.T3.2.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T3.2.1.2.2.2" class="ltx_td"></td>
<td id="S4.T3.2.1.2.2.3" class="ltx_td ltx_align_center">Acc</td>
<td id="S4.T3.2.1.2.2.4" class="ltx_td ltx_align_center">F1</td>
<td id="S4.T3.2.1.2.2.5" class="ltx_td ltx_align_center">Acc</td>
<td id="S4.T3.2.1.2.2.6" class="ltx_td ltx_align_center">F1</td>
<td id="S4.T3.2.1.2.2.7" class="ltx_td ltx_align_center">F1</td>
<td id="S4.T3.2.1.2.2.8" class="ltx_td ltx_align_center">F1</td>
<td id="S4.T3.2.1.2.2.9" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.2.2.9.1" class="ltx_text" style="background-color:#E6E6E6;">F1</span></td>
<td id="S4.T3.2.1.2.2.10" class="ltx_td"></td>
</tr>
<tr id="S4.T3.2.1.3.3" class="ltx_tr">
<th id="S4.T3.2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.2.1.3.3.1.1" class="ltx_text ltx_font_bold">Pythia 1B</span></th>
<td id="S4.T3.2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T3.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">41.89 (15.8)</td>
<td id="S4.T3.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">52.84 (15.5)</td>
<td id="S4.T3.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.2.1.3.3.5.1" class="ltx_text ltx_framed ltx_framed_underline">59.66</span> (10.3)</td>
<td id="S4.T3.2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">65.32 (13.7)</td>
<td id="S4.T3.2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">45.61 (10.0)</td>
<td id="S4.T3.2.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">48.77 (13.7)</td>
<td id="S4.T3.2.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.3.3.9.1" class="ltx_text" style="background-color:#E6E6E6;">53.14 (7.5)</span></td>
<td id="S4.T3.2.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.3.3.10.1" class="ltx_text" style="background-color:#E6E6E6;">45.5</span></td>
</tr>
<tr id="S4.T3.2.1.4.4" class="ltx_tr">
<th id="S4.T3.2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.2.1.4.4.1.1" class="ltx_text ltx_font_bold">DACP</span></th>
<td id="S4.T3.2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">2.39B (10%)</td>
<td id="S4.T3.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">58.06 (8.6)</td>
<td id="S4.T3.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">64.77 (10.4)</td>
<td id="S4.T3.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">53.83 (16.3)</td>
<td id="S4.T3.2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">59.85 (19.0)</td>
<td id="S4.T3.2.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">41.41 (6.5)</td>
<td id="S4.T3.2.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.2.1.4.4.8.1" class="ltx_text ltx_framed ltx_framed_underline">51.32</span> (7.6)</td>
<td id="S4.T3.2.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.4.4.9.1" class="ltx_text" style="background-color:#E6E6E6;">54.34 (8.9)</span></td>
<td id="S4.T3.2.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.4.4.10.1" class="ltx_text" style="background-color:#E6E6E6;">59.1</span></td>
</tr>
<tr id="S4.T3.2.1.5.5" class="ltx_tr">
<th id="S4.T3.2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.5.5.1.1" class="ltx_text ltx_font_bold">DACP</span></th>
<td id="S4.T3.2.1.5.5.2" class="ltx_td ltx_align_center">23.9B (100%)</td>
<td id="S4.T3.2.1.5.5.3" class="ltx_td ltx_align_center">50.86 (14.5)</td>
<td id="S4.T3.2.1.5.5.4" class="ltx_td ltx_align_center">59.16 (12.1)</td>
<td id="S4.T3.2.1.5.5.5" class="ltx_td ltx_align_center">50.17 (17.0)</td>
<td id="S4.T3.2.1.5.5.6" class="ltx_td ltx_align_center">52.84 (18.1)</td>
<td id="S4.T3.2.1.5.5.7" class="ltx_td ltx_align_center">53.34 (9.4)</td>
<td id="S4.T3.2.1.5.5.8" class="ltx_td ltx_align_center">
<span id="S4.T3.2.1.5.5.8.1" class="ltx_text ltx_font_bold">55.20</span> (5.8)</td>
<td id="S4.T3.2.1.5.5.9" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.5.5.9.1" class="ltx_text" style="background-color:#E6E6E6;">55.14 (2.5)</span></td>
<td id="S4.T3.2.1.5.5.10" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.5.5.10.1" class="ltx_text" style="background-color:#E6E6E6;">52.3</span></td>
</tr>
<tr id="S4.T3.2.1.6.6" class="ltx_tr">
<th id="S4.T3.2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.2.1.6.6.1.1" class="ltx_text ltx_font_bold">TACP</span></th>
<td id="S4.T3.2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">0.24M</td>
<td id="S4.T3.2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">56.94 (.094)</td>
<td id="S4.T3.2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">66.80 (10.5)</td>
<td id="S4.T3.2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">62.43 (3.2)</td>
<td id="S4.T3.2.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.2.1.6.6.6.1" class="ltx_text ltx_framed ltx_framed_underline">72.27</span> (2.2)</td>
<td id="S4.T3.2.1.6.6.7" class="ltx_td ltx_align_center ltx_border_t">38.91 (1.5)</td>
<td id="S4.T3.2.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t">50.55 (11.7)</td>
<td id="S4.T3.2.1.6.6.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.6.6.9.1" class="ltx_text" style="background-color:#E6E6E6;">57.13 (13.2)</span></td>
<td id="S4.T3.2.1.6.6.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.6.6.10.1" class="ltx_text" style="background-color:#E6E6E6;">56.8</span></td>
</tr>
<tr id="S4.T3.2.1.7.7" class="ltx_tr">
<th id="S4.T3.2.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="9"><span id="S4.T3.2.1.7.7.1.1" class="ltx_text ltx_font_bold">Hard Sampling</span></th>
<td id="S4.T3.2.1.7.7.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T3.2.1.8.8" class="ltx_tr">
<th id="S4.T3.2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.2.1.8.8.1.1" class="ltx_text ltx_font_bold">ETS-DACP</span></th>
<td id="S4.T3.2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">2.39B (10%)</td>
<td id="S4.T3.2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">59.93 (6.2)</td>
<td id="S4.T3.2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">67.11 (9.6)</td>
<td id="S4.T3.2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">46.26 (19.6)</td>
<td id="S4.T3.2.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t">50.84 (21.9)</td>
<td id="S4.T3.2.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.2.1.8.8.7.1" class="ltx_text ltx_font_bold">71.56</span> (7.1)</td>
<td id="S4.T3.2.1.8.8.8" class="ltx_td ltx_align_center ltx_border_t">49.52 (8.4)</td>
<td id="S4.T3.2.1.8.8.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;">
<span id="S4.T3.2.1.8.8.9.1" class="ltx_text ltx_font_bold" style="color:#0000FF;background-color:#E6E6E6;">59.76</span><span id="S4.T3.2.1.8.8.9.2" class="ltx_text" style="background-color:#E6E6E6;"> (9.7)</span>
</td>
<td id="S4.T3.2.1.8.8.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.8.8.10.1" class="ltx_text ltx_font_bold" style="color:#0000FF;background-color:#E6E6E6;">63.6</span></td>
</tr>
<tr id="S4.T3.2.1.9.9" class="ltx_tr">
<th id="S4.T3.2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.9.9.1.1" class="ltx_text ltx_font_bold">ETA-DACP-ppl</span></th>
<td id="S4.T3.2.1.9.9.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S4.T3.2.1.9.9.3" class="ltx_td ltx_align_center">
<span id="S4.T3.2.1.9.9.3.1" class="ltx_text ltx_font_bold">62.73</span> (3.5)</td>
<td id="S4.T3.2.1.9.9.4" class="ltx_td ltx_align_center">
<span id="S4.T3.2.1.9.9.4.1" class="ltx_text ltx_font_bold">73.66</span> (1.9)</td>
<td id="S4.T3.2.1.9.9.5" class="ltx_td ltx_align_center">42.12 (22.3)</td>
<td id="S4.T3.2.1.9.9.6" class="ltx_td ltx_align_center">45.86 (24.9)</td>
<td id="S4.T3.2.1.9.9.7" class="ltx_td ltx_align_center">39.11 (2.0)</td>
<td id="S4.T3.2.1.9.9.8" class="ltx_td ltx_align_center">48.69 (8.5)</td>
<td id="S4.T3.2.1.9.9.9" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.9.9.9.1" class="ltx_text" style="background-color:#E6E6E6;">51.83 (13.1)</span></td>
<td id="S4.T3.2.1.9.9.10" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.9.9.10.1" class="ltx_text" style="background-color:#E6E6E6;">40.9</span></td>
</tr>
<tr id="S4.T3.2.1.10.10" class="ltx_tr">
<th id="S4.T3.2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.10.10.1.1" class="ltx_text ltx_font_bold">ETA-DACP-ent</span></th>
<td id="S4.T3.2.1.10.10.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S4.T3.2.1.10.10.3" class="ltx_td ltx_align_center">59.18 (5.5)</td>
<td id="S4.T3.2.1.10.10.4" class="ltx_td ltx_align_center">69.58 (8.4)</td>
<td id="S4.T3.2.1.10.10.5" class="ltx_td ltx_align_center">53.19 (14.4)</td>
<td id="S4.T3.2.1.10.10.6" class="ltx_td ltx_align_center">58.14 (19.1)</td>
<td id="S4.T3.2.1.10.10.7" class="ltx_td ltx_align_center">59.83 (11.1)</td>
<td id="S4.T3.2.1.10.10.8" class="ltx_td ltx_align_center">46.18 (15.7)</td>
<td id="S4.T3.2.1.10.10.9" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.10.10.9.1" class="ltx_text" style="background-color:#E6E6E6;">58.43 (8.3)</span></td>
<td id="S4.T3.2.1.10.10.10" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.10.10.10.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#E6E6E6;">61.4</span></td>
</tr>
<tr id="S4.T3.2.1.11.11" class="ltx_tr">
<th id="S4.T3.2.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.11.11.1.1" class="ltx_text ltx_font_bold">ETS-DACP-com</span></th>
<td id="S4.T3.2.1.11.11.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S4.T3.2.1.11.11.3" class="ltx_td ltx_align_center">55.41 (11.7)</td>
<td id="S4.T3.2.1.11.11.4" class="ltx_td ltx_align_center">62.58 (14.7)</td>
<td id="S4.T3.2.1.11.11.5" class="ltx_td ltx_align_center">
<span id="S4.T3.2.1.11.11.5.1" class="ltx_text ltx_font_bold">62.55</span> (3.6)</td>
<td id="S4.T3.2.1.11.11.6" class="ltx_td ltx_align_center">
<span id="S4.T3.2.1.11.11.6.1" class="ltx_text ltx_font_bold">72.83</span> (1.8)</td>
<td id="S4.T3.2.1.11.11.7" class="ltx_td ltx_align_center">53.91 (11.6)</td>
<td id="S4.T3.2.1.11.11.8" class="ltx_td ltx_align_center">48.34 (15.9)</td>
<td id="S4.T3.2.1.11.11.9" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;">
<span id="S4.T3.2.1.11.11.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#E6E6E6;">59.41</span><span id="S4.T3.2.1.11.11.9.2" class="ltx_text" style="background-color:#E6E6E6;"> (9.3)</span>
</td>
<td id="S4.T3.2.1.11.11.10" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.11.11.10.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#E6E6E6;">61.4</span></td>
</tr>
<tr id="S4.T3.2.1.12.12" class="ltx_tr">
<th id="S4.T3.2.1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="9"><span id="S4.T3.2.1.12.12.1.1" class="ltx_text ltx_font_bold">Soft Sampling</span></th>
<td id="S4.T3.2.1.12.12.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S4.T3.2.1.13.13" class="ltx_tr">
<th id="S4.T3.2.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.2.1.13.13.1.1" class="ltx_text ltx_font_bold">ETS-DACP</span></th>
<td id="S4.T3.2.1.13.13.2" class="ltx_td ltx_align_center ltx_border_t">2.39B (10%)</td>
<td id="S4.T3.2.1.13.13.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.2.1.13.13.3.1" class="ltx_text ltx_framed ltx_framed_underline">61.47</span> (2.6)</td>
<td id="S4.T3.2.1.13.13.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T3.2.1.13.13.4.1" class="ltx_text ltx_framed ltx_framed_underline">72.45</span> (3.4)</td>
<td id="S4.T3.2.1.13.13.5" class="ltx_td ltx_align_center ltx_border_t">43.83 (17.3)</td>
<td id="S4.T3.2.1.13.13.6" class="ltx_td ltx_align_center ltx_border_t">47.08 (18.1)</td>
<td id="S4.T3.2.1.13.13.7" class="ltx_td ltx_align_center ltx_border_t">40.82 (7.9)</td>
<td id="S4.T3.2.1.13.13.8" class="ltx_td ltx_align_center ltx_border_t">46.16 (15.1)</td>
<td id="S4.T3.2.1.13.13.9" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.13.13.9.1" class="ltx_text" style="background-color:#E6E6E6;">51.63 (12.3)</span></td>
<td id="S4.T3.2.1.13.13.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.13.13.10.1" class="ltx_text" style="background-color:#E6E6E6;">34.1</span></td>
</tr>
<tr id="S4.T3.2.1.14.14" class="ltx_tr">
<th id="S4.T3.2.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.14.14.1.1" class="ltx_text ltx_font_bold">ETA-DACP-ppl</span></th>
<td id="S4.T3.2.1.14.14.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S4.T3.2.1.14.14.3" class="ltx_td ltx_align_center">53.90 (14.1)</td>
<td id="S4.T3.2.1.14.14.4" class="ltx_td ltx_align_center">61.44 (18.4)</td>
<td id="S4.T3.2.1.14.14.5" class="ltx_td ltx_align_center">46.04 (15.6)</td>
<td id="S4.T3.2.1.14.14.6" class="ltx_td ltx_align_center">52.44 (13.6)</td>
<td id="S4.T3.2.1.14.14.7" class="ltx_td ltx_align_center">41.00 (5.6)</td>
<td id="S4.T3.2.1.14.14.8" class="ltx_td ltx_align_center">43.80 (13.7)</td>
<td id="S4.T3.2.1.14.14.9" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.14.14.9.1" class="ltx_text" style="background-color:#E6E6E6;">49.67 (8.0)</span></td>
<td id="S4.T3.2.1.14.14.10" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.14.14.10.1" class="ltx_text" style="background-color:#E6E6E6;">20.5</span></td>
</tr>
<tr id="S4.T3.2.1.15.15" class="ltx_tr">
<th id="S4.T3.2.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.15.15.1.1" class="ltx_text ltx_font_bold">ETA-DACP-ent</span></th>
<td id="S4.T3.2.1.15.15.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S4.T3.2.1.15.15.3" class="ltx_td ltx_align_center">59.49 (9.2)</td>
<td id="S4.T3.2.1.15.15.4" class="ltx_td ltx_align_center">68.20 (9.5)</td>
<td id="S4.T3.2.1.15.15.5" class="ltx_td ltx_align_center">48.85 (16.7)</td>
<td id="S4.T3.2.1.15.15.6" class="ltx_td ltx_align_center">57.00 (22.5)</td>
<td id="S4.T3.2.1.15.15.7" class="ltx_td ltx_align_center">
<span id="S4.T3.2.1.15.15.7.1" class="ltx_text ltx_framed ltx_framed_underline">62.06</span> (11.4)</td>
<td id="S4.T3.2.1.15.15.8" class="ltx_td ltx_align_center">38.00 (19.6)</td>
<td id="S4.T3.2.1.15.15.9" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.15.15.9.1" class="ltx_text" style="background-color:#E6E6E6;">56.31 (11.3)</span></td>
<td id="S4.T3.2.1.15.15.10" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.15.15.10.1" class="ltx_text" style="background-color:#E6E6E6;">52.3</span></td>
</tr>
<tr id="S4.T3.2.1.16.16" class="ltx_tr">
<th id="S4.T3.2.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.2.1.16.16.1.1" class="ltx_text ltx_font_bold">ETS-DACP-com</span></th>
<td id="S4.T3.2.1.16.16.2" class="ltx_td ltx_align_center ltx_border_bb">2.29B (10%)</td>
<td id="S4.T3.2.1.16.16.3" class="ltx_td ltx_align_center ltx_border_bb">57.07 (10.5)</td>
<td id="S4.T3.2.1.16.16.4" class="ltx_td ltx_align_center ltx_border_bb">64.41 (11.0)</td>
<td id="S4.T3.2.1.16.16.5" class="ltx_td ltx_align_center ltx_border_bb">59.06 (6.0)</td>
<td id="S4.T3.2.1.16.16.6" class="ltx_td ltx_align_center ltx_border_bb">67.97 (9.2)</td>
<td id="S4.T3.2.1.16.16.7" class="ltx_td ltx_align_center ltx_border_bb">51.22 (12.5)</td>
<td id="S4.T3.2.1.16.16.8" class="ltx_td ltx_align_center ltx_border_bb">47.68 (13.8)</td>
<td id="S4.T3.2.1.16.16.9" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.16.16.9.1" class="ltx_text" style="background-color:#E6E6E6;">57.82 (8.6)</span></td>
<td id="S4.T3.2.1.16.16.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S4.T3.2.1.16.16.10.1" class="ltx_text" style="background-color:#E6E6E6;">52.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.6.2" class="ltx_text" style="font-size:90%;">Effect of TACP and efficient DACP measured in 5-shot setting on financial tasks for Pythia-1B model class. The reported are mean and standard deviation (in parenthesis) of 10 runs. ETA-DACP-ppl is ETA-DACP with perplexity measure, and ETA-DACP-ent is with entropy measure. ETS-DACP-com is task similar DACP with data selection by averaging all three metrics: perplexity, similarity, and entropy. Win rate is percentage of times a model is more accurate than other models in a pair-wise comparison&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. <span id="S4.T3.6.2.1" class="ltx_text ltx_font_bold">Bold</span> indicates the best results and <span id="S4.T3.6.2.2" class="ltx_text ltx_framed ltx_framed_underline">underline</span> indicates the second best per task.</span></figcaption>
</figure>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">These results clearly show that <em id="S4.SS2.p5.1.1" class="ltx_emph ltx_font_italic"><span id="S4.SS2.p5.1.1.1" class="ltx_text ltx_font_bold">not all data is equal</span> for continual pre-training</em>. In fact, all the data used in efficient DACP methods (10%) is a subset of the data in DACP. Since DACP‚Äôs (100%) performance is lower than ETS-DACP/ETA-DACP-ent, adding more data on top of highly similar or high entropy data actually hurts the performance. The difference in results between hard and soft sampling adds more evidence to this observation. While there is variability across tasks, on an average, adding examples from outside the top decile of metrics hurts the performance with the notable exception of ETS-DACP-com which is a combination of all three metrics. Hence, we should carefully curate the data for any domain continual pre-training.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">Note, 10% of domain data (2.39B) translates to less than 1% of the 300 billion tokens the original Pythia was trained on. These results demonstrate that being selective during the data curation process for continual pre-training can have large effects on domain performance at a small cost.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">These results demonstrate the effectiveness of continual pre-training on domains and task (sub-domains). A natural question that arises from this exercise is <em id="S4.SS2.p7.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">whether the LLM is losing its generality by being further tuned on a narrow domain?</em> In short, is the LLM becoming a specialist at the expense of being a generalist? We answer this question by measuring the performance of continually pre-trained LLM variants on out-of-domain tasks which Pythia was evaluated on. Table&nbsp;<a href="#S5.T4" title="Table 4 ‚Ä£ Domain specific large language models. ‚Ä£ 5 Related Work ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the performance on the standard four non-finance tasks. We do not observe any significant change in the performance on the four out-of-domain tasks except for DACP with 100% data. Hence, <em id="S4.SS2.p7.1.2" class="ltx_emph ltx_font_italic">by being selective about the data to use for continual pre-training, we can keep the LLM‚Äôs original capability intact while improving their domain performance.</em></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Domain specific large language models.</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">While the majority of released LLMs are general-purpose models, domain-specific LLMs have emerged as valuable counterparts. Google‚Äôs MedPaLM and MedPaLM-2, trained on a medical domain corpus, achieved state-of-the-art results on medical benchmarks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Bloomberg developed the first financial LLM from scratch by training on a financial corpus&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> while Galactica was developed for scientific domains&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Continual pre-training presents an alternative approach to building domain-specific LLMs from scratch. Wu et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> build medical LLMs through continual pre-training LLaMA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> on medical papers. However, they do not evaluate the model‚Äôs quantitative performance in a non-fine tuning setting. In this work, we measure the model‚Äôs performance in an in-context learning setting, showing the clear benefits of continual pre-training.
</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:211pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-80.1pt,38.9pt) scale(0.730203699495374,0.730203699495374) ;">
<table id="S5.T4.2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.2.1.1.1" class="ltx_tr">
<td id="S5.T4.2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S5.T4.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Tokens</td>
<td id="S5.T4.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T4.2.1.1.1.3.1" class="ltx_text ltx_font_bold">ARC</span></td>
<td id="S5.T4.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T4.2.1.1.1.4.1" class="ltx_text ltx_font_bold">MMLU</span></td>
<td id="S5.T4.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T4.2.1.1.1.5.1" class="ltx_text ltx_font_bold">TruthfulQA</span></td>
<td id="S5.T4.2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T4.2.1.1.1.6.1" class="ltx_text ltx_font_bold">HellaSwag</span></td>
<td id="S5.T4.2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S5.T4.2.1.1.1.7.1" class="ltx_text ltx_font_bold">Average</span></td>
</tr>
<tr id="S5.T4.2.1.2.2" class="ltx_tr">
<td id="S5.T4.2.1.2.2.1" class="ltx_td"></td>
<td id="S5.T4.2.1.2.2.2" class="ltx_td"></td>
<td id="S5.T4.2.1.2.2.3" class="ltx_td ltx_align_center">Acc</td>
<td id="S5.T4.2.1.2.2.4" class="ltx_td ltx_align_center">Acc Norm</td>
<td id="S5.T4.2.1.2.2.5" class="ltx_td ltx_align_center">Acc</td>
<td id="S5.T4.2.1.2.2.6" class="ltx_td ltx_align_center">Acc Norm</td>
<td id="S5.T4.2.1.2.2.7" class="ltx_td ltx_align_center">MC1</td>
<td id="S5.T4.2.1.2.2.8" class="ltx_td ltx_align_center">MC2</td>
<td id="S5.T4.2.1.2.2.9" class="ltx_td ltx_align_center">Acc</td>
<td id="S5.T4.2.1.2.2.10" class="ltx_td ltx_align_center">Acc Norm</td>
<td id="S5.T4.2.1.2.2.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.2.2.11.1" class="ltx_text" style="background-color:#E6E6E6;">Acc</span></td>
<td id="S5.T4.2.1.2.2.12" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.2.2.12.1" class="ltx_text" style="background-color:#E6E6E6;">Acc Norm</span></td>
</tr>
<tr id="S5.T4.2.1.3.3" class="ltx_tr">
<td id="S5.T4.2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.3.3.1.1" class="ltx_text ltx_font_bold">Pythia 1B</span></td>
<td id="S5.T4.2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S5.T4.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">25.94</td>
<td id="S5.T4.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">29.27</td>
<td id="S5.T4.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">26.29</td>
<td id="S5.T4.2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">26.29</td>
<td id="S5.T4.2.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">23.62</td>
<td id="S5.T4.2.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">40.47</td>
<td id="S5.T4.2.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.3.3.9.1" class="ltx_text ltx_font_bold">37.65</span></td>
<td id="S5.T4.2.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.3.3.10.1" class="ltx_text ltx_font_bold">47.83</span></td>
<td id="S5.T4.2.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.3.3.11.1" class="ltx_text" style="background-color:#E6E6E6;">28.38</span></td>
<td id="S5.T4.2.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.3.3.12.1" class="ltx_text ltx_font_bold" style="color:#0000FF;background-color:#E6E6E6;">35.96</span></td>
</tr>
<tr id="S5.T4.2.1.4.4" class="ltx_tr">
<td id="S5.T4.2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.4.4.1.1" class="ltx_text ltx_font_bold">DACP</span></td>
<td id="S5.T4.2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">2.39B (10%)</td>
<td id="S5.T4.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">26.28</td>
<td id="S5.T4.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">29.44</td>
<td id="S5.T4.2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">26.43</td>
<td id="S5.T4.2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">26.43</td>
<td id="S5.T4.2.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">24.48</td>
<td id="S5.T4.2.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">42.26</td>
<td id="S5.T4.2.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">36.83</td>
<td id="S5.T4.2.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t">45.34</td>
<td id="S5.T4.2.1.4.4.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.4.4.11.1" class="ltx_text" style="background-color:#E6E6E6;">28.50</span></td>
<td id="S5.T4.2.1.4.4.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.4.4.12.1" class="ltx_text" style="background-color:#E6E6E6;">35.87</span></td>
</tr>
<tr id="S5.T4.2.1.5.5" class="ltx_tr">
<td id="S5.T4.2.1.5.5.1" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.5.5.1.1" class="ltx_text ltx_font_bold">DACP</span></td>
<td id="S5.T4.2.1.5.5.2" class="ltx_td ltx_align_center">23.9B (100%)</td>
<td id="S5.T4.2.1.5.5.3" class="ltx_td ltx_align_center">24.32</td>
<td id="S5.T4.2.1.5.5.4" class="ltx_td ltx_align_center">27.47</td>
<td id="S5.T4.2.1.5.5.5" class="ltx_td ltx_align_center">26.09</td>
<td id="S5.T4.2.1.5.5.6" class="ltx_td ltx_align_center">26.09</td>
<td id="S5.T4.2.1.5.5.7" class="ltx_td ltx_align_center">24.60</td>
<td id="S5.T4.2.1.5.5.8" class="ltx_td ltx_align_center">42.05</td>
<td id="S5.T4.2.1.5.5.9" class="ltx_td ltx_align_center">35.34</td>
<td id="S5.T4.2.1.5.5.10" class="ltx_td ltx_align_center">42.45</td>
<td id="S5.T4.2.1.5.5.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.5.5.11.1" class="ltx_text" style="background-color:#E6E6E6;">27.59</span></td>
<td id="S5.T4.2.1.5.5.12" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.5.5.12.1" class="ltx_text" style="background-color:#E6E6E6;">34.52</span></td>
</tr>
<tr id="S5.T4.2.1.6.6" class="ltx_tr">
<td id="S5.T4.2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.6.6.1.1" class="ltx_text ltx_font_bold">TACP</span></td>
<td id="S5.T4.2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">0.24M</td>
<td id="S5.T4.2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">25.34</td>
<td id="S5.T4.2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">28.41</td>
<td id="S5.T4.2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">24.93</td>
<td id="S5.T4.2.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">24.93</td>
<td id="S5.T4.2.1.6.6.7" class="ltx_td ltx_align_center ltx_border_t">24.48</td>
<td id="S5.T4.2.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t">41.95</td>
<td id="S5.T4.2.1.6.6.9" class="ltx_td ltx_align_center ltx_border_t">37.03</td>
<td id="S5.T4.2.1.6.6.10" class="ltx_td ltx_align_center ltx_border_t">47.27</td>
<td id="S5.T4.2.1.6.6.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.6.6.11.1" class="ltx_text" style="background-color:#E6E6E6;">27.95</span></td>
<td id="S5.T4.2.1.6.6.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.6.6.12.1" class="ltx_text" style="background-color:#E6E6E6;">35.64</span></td>
</tr>
<tr id="S5.T4.2.1.7.7" class="ltx_tr">
<td id="S5.T4.2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_t" colspan="12"><span id="S5.T4.2.1.7.7.1.1" class="ltx_text ltx_font_bold">Hard Sampling</span></td>
</tr>
<tr id="S5.T4.2.1.8.8" class="ltx_tr">
<td id="S5.T4.2.1.8.8.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.8.8.1.1" class="ltx_text ltx_font_bold">ETS-DACP</span></td>
<td id="S5.T4.2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">2.39B (10%)</td>
<td id="S5.T4.2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">24.74</td>
<td id="S5.T4.2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">28.07</td>
<td id="S5.T4.2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">25.99</td>
<td id="S5.T4.2.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t">25.99</td>
<td id="S5.T4.2.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t">23.26</td>
<td id="S5.T4.2.1.8.8.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.8.8.8.1" class="ltx_text ltx_font_bold">43.85</span></td>
<td id="S5.T4.2.1.8.8.9" class="ltx_td ltx_align_center ltx_border_t">36.31</td>
<td id="S5.T4.2.1.8.8.10" class="ltx_td ltx_align_center ltx_border_t">44.79</td>
<td id="S5.T4.2.1.8.8.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.8.8.11.1" class="ltx_text" style="background-color:#E6E6E6;">27.57</span></td>
<td id="S5.T4.2.1.8.8.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.8.8.12.1" class="ltx_text" style="background-color:#E6E6E6;">35.68</span></td>
</tr>
<tr id="S5.T4.2.1.9.9" class="ltx_tr">
<td id="S5.T4.2.1.9.9.1" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.9.9.1.1" class="ltx_text ltx_font_bold">ETA-DACP-ppl</span></td>
<td id="S5.T4.2.1.9.9.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S5.T4.2.1.9.9.3" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.9.9.3.1" class="ltx_text ltx_font_bold">26.71</span></td>
<td id="S5.T4.2.1.9.9.4" class="ltx_td ltx_align_center">28.41</td>
<td id="S5.T4.2.1.9.9.5" class="ltx_td ltx_align_center">26.31</td>
<td id="S5.T4.2.1.9.9.6" class="ltx_td ltx_align_center">26.31</td>
<td id="S5.T4.2.1.9.9.7" class="ltx_td ltx_align_center">24.97</td>
<td id="S5.T4.2.1.9.9.8" class="ltx_td ltx_align_center">41.42</td>
<td id="S5.T4.2.1.9.9.9" class="ltx_td ltx_align_center">36.70</td>
<td id="S5.T4.2.1.9.9.10" class="ltx_td ltx_align_center">44.89</td>
<td id="S5.T4.2.1.9.9.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.9.9.11.1" class="ltx_text ltx_font_bold" style="color:#0000FF;background-color:#E6E6E6;">28.67</span></td>
<td id="S5.T4.2.1.9.9.12" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.9.9.12.1" class="ltx_text" style="background-color:#E6E6E6;">35.26</span></td>
</tr>
<tr id="S5.T4.2.1.10.10" class="ltx_tr">
<td id="S5.T4.2.1.10.10.1" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.10.10.1.1" class="ltx_text ltx_font_bold">ETA-DACP-ent</span></td>
<td id="S5.T4.2.1.10.10.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S5.T4.2.1.10.10.3" class="ltx_td ltx_align_center">25.34</td>
<td id="S5.T4.2.1.10.10.4" class="ltx_td ltx_align_center">27.99</td>
<td id="S5.T4.2.1.10.10.5" class="ltx_td ltx_align_center">24.60</td>
<td id="S5.T4.2.1.10.10.6" class="ltx_td ltx_align_center">24.60</td>
<td id="S5.T4.2.1.10.10.7" class="ltx_td ltx_align_center">24.11</td>
<td id="S5.T4.2.1.10.10.8" class="ltx_td ltx_align_center">41.38</td>
<td id="S5.T4.2.1.10.10.9" class="ltx_td ltx_align_center">36.92</td>
<td id="S5.T4.2.1.10.10.10" class="ltx_td ltx_align_center">44.98</td>
<td id="S5.T4.2.1.10.10.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.10.10.11.1" class="ltx_text" style="background-color:#E6E6E6;">27.75</span></td>
<td id="S5.T4.2.1.10.10.12" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.10.10.12.1" class="ltx_text" style="background-color:#E6E6E6;">34.74</span></td>
</tr>
<tr id="S5.T4.2.1.11.11" class="ltx_tr">
<td id="S5.T4.2.1.11.11.1" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.11.11.1.1" class="ltx_text ltx_font_bold">ETS-DACP-com</span></td>
<td id="S5.T4.2.1.11.11.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S5.T4.2.1.11.11.3" class="ltx_td ltx_align_center">26.37</td>
<td id="S5.T4.2.1.11.11.4" class="ltx_td ltx_align_center">29.35</td>
<td id="S5.T4.2.1.11.11.5" class="ltx_td ltx_align_center">26.58</td>
<td id="S5.T4.2.1.11.11.6" class="ltx_td ltx_align_center">26.58</td>
<td id="S5.T4.2.1.11.11.7" class="ltx_td ltx_align_center">24.48</td>
<td id="S5.T4.2.1.11.11.8" class="ltx_td ltx_align_center">41.51</td>
<td id="S5.T4.2.1.11.11.9" class="ltx_td ltx_align_center">36.61</td>
<td id="S5.T4.2.1.11.11.10" class="ltx_td ltx_align_center">44.97</td>
<td id="S5.T4.2.1.11.11.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.11.11.11.1" class="ltx_text" style="background-color:#E6E6E6;">28.51</span></td>
<td id="S5.T4.2.1.11.11.12" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.11.11.12.1" class="ltx_text" style="background-color:#E6E6E6;">35.60</span></td>
</tr>
<tr id="S5.T4.2.1.12.12" class="ltx_tr">
<td id="S5.T4.2.1.12.12.1" class="ltx_td ltx_align_center ltx_border_t" colspan="12"><span id="S5.T4.2.1.12.12.1.1" class="ltx_text ltx_font_bold">Soft Sampling</span></td>
</tr>
<tr id="S5.T4.2.1.13.13" class="ltx_tr">
<td id="S5.T4.2.1.13.13.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.13.13.1.1" class="ltx_text ltx_font_bold">ETS-DACP</span></td>
<td id="S5.T4.2.1.13.13.2" class="ltx_td ltx_align_center ltx_border_t">2.39B (10%)</td>
<td id="S5.T4.2.1.13.13.3" class="ltx_td ltx_align_center ltx_border_t">26.45</td>
<td id="S5.T4.2.1.13.13.4" class="ltx_td ltx_align_center ltx_border_t">28.33</td>
<td id="S5.T4.2.1.13.13.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.13.13.5.1" class="ltx_text ltx_font_bold">27.10</span></td>
<td id="S5.T4.2.1.13.13.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T4.2.1.13.13.6.1" class="ltx_text ltx_font_bold">27.10</span></td>
<td id="S5.T4.2.1.13.13.7" class="ltx_td ltx_align_center ltx_border_t">24.60</td>
<td id="S5.T4.2.1.13.13.8" class="ltx_td ltx_align_center ltx_border_t">41.73</td>
<td id="S5.T4.2.1.13.13.9" class="ltx_td ltx_align_center ltx_border_t">36.24</td>
<td id="S5.T4.2.1.13.13.10" class="ltx_td ltx_align_center ltx_border_t">44.49</td>
<td id="S5.T4.2.1.13.13.11" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.13.13.11.1" class="ltx_text" style="background-color:#E6E6E6;">28.60</span></td>
<td id="S5.T4.2.1.13.13.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.13.13.12.1" class="ltx_text" style="background-color:#E6E6E6;">35.41</span></td>
</tr>
<tr id="S5.T4.2.1.14.14" class="ltx_tr">
<td id="S5.T4.2.1.14.14.1" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.14.14.1.1" class="ltx_text ltx_font_bold">ETA-DACP-ppl</span></td>
<td id="S5.T4.2.1.14.14.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S5.T4.2.1.14.14.3" class="ltx_td ltx_align_center">25.85</td>
<td id="S5.T4.2.1.14.14.4" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.14.14.4.1" class="ltx_text ltx_font_bold">29.69</span></td>
<td id="S5.T4.2.1.14.14.5" class="ltx_td ltx_align_center">26.59</td>
<td id="S5.T4.2.1.14.14.6" class="ltx_td ltx_align_center">26.59</td>
<td id="S5.T4.2.1.14.14.7" class="ltx_td ltx_align_center">24.85</td>
<td id="S5.T4.2.1.14.14.8" class="ltx_td ltx_align_center">42.17</td>
<td id="S5.T4.2.1.14.14.9" class="ltx_td ltx_align_center">36.55</td>
<td id="S5.T4.2.1.14.14.10" class="ltx_td ltx_align_center">44.71</td>
<td id="S5.T4.2.1.14.14.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.14.14.11.1" class="ltx_text" style="background-color:#E6E6E6;">28.46</span></td>
<td id="S5.T4.2.1.14.14.12" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.14.14.12.1" class="ltx_text" style="background-color:#E6E6E6;">35.79</span></td>
</tr>
<tr id="S5.T4.2.1.15.15" class="ltx_tr">
<td id="S5.T4.2.1.15.15.1" class="ltx_td ltx_align_center"><span id="S5.T4.2.1.15.15.1.1" class="ltx_text ltx_font_bold">ETA-DACP-ent</span></td>
<td id="S5.T4.2.1.15.15.2" class="ltx_td ltx_align_center">2.39B (10%)</td>
<td id="S5.T4.2.1.15.15.3" class="ltx_td ltx_align_center">25.94</td>
<td id="S5.T4.2.1.15.15.4" class="ltx_td ltx_align_center">29.10</td>
<td id="S5.T4.2.1.15.15.5" class="ltx_td ltx_align_center">25.61</td>
<td id="S5.T4.2.1.15.15.6" class="ltx_td ltx_align_center">25.61</td>
<td id="S5.T4.2.1.15.15.7" class="ltx_td ltx_align_center">24.60</td>
<td id="S5.T4.2.1.15.15.8" class="ltx_td ltx_align_center">41.64</td>
<td id="S5.T4.2.1.15.15.9" class="ltx_td ltx_align_center">36.78</td>
<td id="S5.T4.2.1.15.15.10" class="ltx_td ltx_align_center">45.20</td>
<td id="S5.T4.2.1.15.15.11" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.15.15.11.1" class="ltx_text" style="background-color:#E6E6E6;">28.23</span></td>
<td id="S5.T4.2.1.15.15.12" class="ltx_td ltx_align_center" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.15.15.12.1" class="ltx_text" style="background-color:#E6E6E6;">35.39</span></td>
</tr>
<tr id="S5.T4.2.1.16.16" class="ltx_tr">
<td id="S5.T4.2.1.16.16.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.2.1.16.16.1.1" class="ltx_text ltx_font_bold">ETS-DACP-com</span></td>
<td id="S5.T4.2.1.16.16.2" class="ltx_td ltx_align_center ltx_border_bb">2.39B (10%)</td>
<td id="S5.T4.2.1.16.16.3" class="ltx_td ltx_align_center ltx_border_bb">25.77</td>
<td id="S5.T4.2.1.16.16.4" class="ltx_td ltx_align_center ltx_border_bb">27.47</td>
<td id="S5.T4.2.1.16.16.5" class="ltx_td ltx_align_center ltx_border_bb">27.05</td>
<td id="S5.T4.2.1.16.16.6" class="ltx_td ltx_align_center ltx_border_bb">27.05</td>
<td id="S5.T4.2.1.16.16.7" class="ltx_td ltx_align_center ltx_border_bb">24.24</td>
<td id="S5.T4.2.1.16.16.8" class="ltx_td ltx_align_center ltx_border_bb">41.82</td>
<td id="S5.T4.2.1.16.16.9" class="ltx_td ltx_align_center ltx_border_bb">36.93</td>
<td id="S5.T4.2.1.16.16.10" class="ltx_td ltx_align_center ltx_border_bb">44.62</td>
<td id="S5.T4.2.1.16.16.11" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.16.16.11.1" class="ltx_text" style="background-color:#E6E6E6;">28.50</span></td>
<td id="S5.T4.2.1.16.16.12" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#E6E6E6;"><span id="S5.T4.2.1.16.16.12.1" class="ltx_text" style="background-color:#E6E6E6;">35.24</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.5.2" class="ltx_text" style="font-size:90%;"> Evaluation on standard tasks <span id="S5.T4.5.2.1" class="ltx_text ltx_font_bold">Bold</span> indicates the best value for a column We follow the evaluation practice used to create HuggingFace Open LLM leaderboard.</span></figcaption>
</figure>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Task-adaptive pre-training.</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">Continual pre-training of language models on unlabeled data for a given task has been demonstrated to be beneficial for enhancing end-task performance <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In scenarios involving domain shift, domain-adaptive pre-training bears similarities to task-adaptive pre-training to some extent. Aharoni et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> documented that continual pre-training a model on a similar domain contributes to improved task performance on the target domain. Notably, the work closest to ours is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, which shows that continual pre-training of language models on both unlabeled task data and augmented unlabeled task data, sampled from the in-domain corpus based on similarity. While these works use task data, we also propose a task agnostic method, ETA-DACP, as task similarity is prohibitively expensive for LLMs.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data selection.</h5>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">Data selection in continual pre-training plays a critical role in choosing the most valuable data samples for the training process. Various distributed and linguistic features independent of specific domains or tasks have been shown to be beneficial for data selection and the organization of learning curricula <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. In the context of LLMs, there is limited understanding of how to curate data for pre-training, let alone for continual pre-training. <em id="S5.SS0.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">To best of our knowledge, ours is the first work that attempts to do data selection in the context of LLMs for more effective continual pre-training.</em></p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we demonstrate the efficacy of domain-adaptive continual pre-training for developing domain-specific LLMs. Our results in the finance domain show that domain-adaptive continual pre-training improves the LLMs‚Äô performance on financial tasks. Domain-adaptive continual pre-training enables the LLMs to acquire new knowledge in the financial domain at a much lower cost.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Furthermore, we propose efficient domain-adaptive continual pre-training methods, ETS-DACP and ETA-DACP to enhance the effectiveness of the continual pre-training. By being selective with the training data curation, our methods refine the continual pre-training, yielding even better results with just 10% of the data and cost of vanilla continual pre-training. ETA-DACP with data selection based on task-agnostic measures like entropy works almost at par with the task-aware data selection strategy. This finding can be used to build data selection for continual pre-training even in the absence of task data. We also observe no degradation in performance on open-domain standard tasks, implying that domain-adaptive continual pre-training does not hurt open-domain capabilities.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Our findings place domain continual pre-training as a strong alternative to building domain-specific LLMs from scratch. By being smarter about data selection for continual pre-training, we can surpass vanilla continual pre-training at a fraction of the cost. Overall, our work paves the way for developing domain-specific LLMs at a reduced cost, with implications for a wide range of applications.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Roee Aharoni and Yoav Goldberg.

</span>
<span class="ltx_bibblock">Unsupervised domain clusters in pretrained language models.

</span>
<span class="ltx_bibblock">In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel&nbsp;R. Tetreault,
editors, <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, ACL 2020, Online, July 5-10, 2020</span>, pages
7747‚Äì7763. Association for Computational Linguistics, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
Jennifer&nbsp;Wortman Vaughan.

</span>
<span class="ltx_bibblock">A theory of learning from different domains.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Machine learning</span>, 79:151‚Äì175, 2010.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yoshua Bengio, J√©r√¥me Louradour, Ronan Collobert, and Jason Weston.

</span>
<span class="ltx_bibblock">Curriculum learning.

</span>
<span class="ltx_bibblock">In Andrea&nbsp;Pohoreckyj Danyluk, L√©on Bottou, and Michael&nbsp;L.
Littman, editors, <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th Annual International
Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June
14-18, 2009</span>, volume 382 of <span id="bib.bib3.2.2" class="ltx_text ltx_font_italic">ACM International Conference Proceeding
Series</span>, pages 41‚Äì48. ACM, 2009.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Stella Biderman, Hailey Schoelkopf, Quentin&nbsp;Gregory Anthony, Herbie Bradley,
Kyle O‚ÄôBrien, Eric Hallahan, Mohammad&nbsp;Aflah Khan, Shivanshu Purohit,
USVSN&nbsp;Sai Prashanth, Edward Raff, et&nbsp;al.

</span>
<span class="ltx_bibblock">Pythia: A suite for analyzing large language models across training
and scaling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
2397‚Äì2430. PMLR, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
33:1877‚Äì1901, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Xiang Dai, Sarvnaz Karimi, Ben Hachey, and C√©cile Paris.

</span>
<span class="ltx_bibblock">Using similarity measures to select pretraining data for NER.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio, editors, <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and
Short Papers)</span>, pages 1460‚Äì1470. Association for Computational Linguistics,
2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, Fran√ßois Laviolette, Mario Marchand, and Victor&nbsp;S.
Lempitsky.

</span>
<span class="ltx_bibblock">Domain-adversarial training of neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">J. Mach. Learn. Res.</span>, 17:59:1‚Äì59:35, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Suchin Gururangan, Ana Marasoviƒá, Swabha Swayamdipta, Kyle Lo, Iz&nbsp;Beltagy,
Doug Downey, and Noah&nbsp;A. Smith.

</span>
<span class="ltx_bibblock">Don‚Äôt stop pretraining: Adapt language models to domains and tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span>, pages 8342‚Äì8360, Online, July 2020. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Matthew Honnibal and Ines Montani.

</span>
<span class="ltx_bibblock">spaCy 2: Natural language understanding with Bloom embeddings,
convolutional neural networks and incremental parsing.

</span>
<span class="ltx_bibblock">To appear, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han,
Gyeonghun Kim, and Minjoon Seo.

</span>
<span class="ltx_bibblock">Temporalwiki: A lifelong benchmark for training and evaluating
ever-evolving language models.

</span>
<span class="ltx_bibblock">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11,
2022</span>, pages 6237‚Äì6250. Association for Computational Linguistics, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun
Kim, Stanley&nbsp;Jungkyu Choi, and Minjoon Seo.

</span>
<span class="ltx_bibblock">Towards continual knowledge learning of language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</span>.
OpenReview.net, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei,
Andrew&nbsp;O. Arnold, and Xiang Ren.

</span>
<span class="ltx_bibblock">Lifelong pretraining: Continually adapting language models to
emerging corpora.

</span>
<span class="ltx_bibblock">In Marine Carpuat, Marie-Catherine de&nbsp;Marneffe, and Iv√°n
Vladimir&nbsp;Meza Ru√≠z, editors, <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2022 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United
States, July 10-15, 2022</span>, pages 4764‚Äì4780. Association for Computational
Linguistics, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu.

</span>
<span class="ltx_bibblock">Continual pre-training of language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">The Twelfth International Conference on Learning
Representations</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
Chris Callison-Burch, and Nicholas Carlini.

</span>
<span class="ltx_bibblock">Deduplicating training data makes language models better.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, pages 8424‚Äì8445, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
et&nbsp;al.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2211.09110</span>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Hong Liu, Sang&nbsp;Michael Xie, Zhiyuan Li, and Tengyu Ma.

</span>
<span class="ltx_bibblock">Same pre-training loss, better downstream: Implicit bias matters for
language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
22188‚Äì22214. PMLR, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Lefteris Loukas, Manos Fergadiotis, Ion Androutsopoulos, and Prodromos
Malakasiotis.

</span>
<span class="ltx_bibblock">EDGAR-CORPUS: billions of tokens make the world go round.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2109.14394, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Macedo Maia, Siegfried Handschuh, Andr√© Freitas, Brian Davis, Ross
McDermott, Manel Zarrouk, and Alexandra Balahur.

</span>
<span class="ltx_bibblock">Www‚Äô18 open challenge: Financial opinion mining and question
answering.

</span>
<span class="ltx_bibblock">In Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and
Panagiotis&nbsp;G. Ipeirotis, editors, <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Companion of the The Web Conference
2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27,
2018</span>, pages 1941‚Äì1942. ACM, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Pekka Malo, Ankur Sinha, Pekka&nbsp;J. Korhonen, Jyrki Wallenius, and Pyry Takala.

</span>
<span class="ltx_bibblock">Good debt or bad debt: Detecting semantic orientations in economic
texts.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">J. Assoc. Inf. Sci. Technol.</span>, 65(4):782‚Äì796, 2014.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">Deepspeed: System optimizations enable training deep learning models
with over 100 billion parameters.

</span>
<span class="ltx_bibblock">In Rajesh Gupta, Yan Liu, Jiliang Tang, and B.&nbsp;Aditya Prakash,
editors, <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">KDD ‚Äô20: The 26th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020</span>, pages
3505‚Äì3506. ACM, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sebastian Ruder and Barbara Plank.

</span>
<span class="ltx_bibblock">Learning to select data for transfer learning with bayesian
optimization.

</span>
<span class="ltx_bibblock">In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017</span>, pages
372‚Äì382. Association for Computational Linguistics, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Teven&nbsp;Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic,
Daniel Hesslow, Roman Castagn√©, Alexandra&nbsp;Sasha Luccioni,
Fran√ßois Yvon, Matthias Gall√©, Jonathan Tow, Alexander&nbsp;M. Rush,
Stella Biderman, Albert Webson, Pawan&nbsp;Sasanka Ammanamanchi, Thomas Wang,
Beno√Æt Sagot, Niklas Muennighoff, Albert&nbsp;Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz&nbsp;Beltagy,
Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro&nbsp;Ortiz Suarez, Victor Sanh,
Hugo Lauren√ßon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin
Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham&nbsp;Fikri Aji, Amit
Alfassy, Anna Rogers, Ariel&nbsp;Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris
Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David&nbsp;Ifeoluwa
Adelani, and et&nbsp;al.

</span>
<span class="ltx_bibblock">BLOOM: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2211.05100, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Karan Singhal, Shekoofeh Azizi, Tao Tu, S.&nbsp;Sara Mahdavi, Jason Wei, Hyung&nbsp;Won
Chung, Nathan Scales, Ajay&nbsp;Kumar Tanwani, Heather Cole-Lewis, Stephen
Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal
Sch√§rli, Aakanksha Chowdhery, Philip&nbsp;Andrew Mansfield,
Blaise&nbsp;Ag√ºera y&nbsp;Arcas, Dale&nbsp;R. Webster, Gregory&nbsp;S. Corrado, Yossi
Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin
Rajkomar, Joelle&nbsp;K. Barral, Christopher Semturs, Alan Karthikesalingam, and
Vivek Natarajan.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2212.13138, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le&nbsp;Hou,
Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike
Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip&nbsp;Andrew Mansfield,
Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise&nbsp;Ag√ºera y&nbsp;Arcas,
Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S.&nbsp;Sara Mahdavi,
Joelle&nbsp;K. Barral, Dale&nbsp;R. Webster, Gregory&nbsp;S. Corrado, Yossi Matias,
Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan.

</span>
<span class="ltx_bibblock">Towards expert-level medical question answering with large language
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2305.09617, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ankur Sinha and Tanmay Khandait.

</span>
<span class="ltx_bibblock">Impact of news on the commodity market: Dataset and results.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2009.04202, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony
Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.

</span>
<span class="ltx_bibblock">Galactica: A large language model for science.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2211.09085, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric
Hambro, Faisal Azhar, Aur√©lien Rodriguez, Armand Joulin, Edouard Grave,
and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2302.13971, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Brian MacWhinney, and Chris Dyer.

</span>
<span class="ltx_bibblock">Learning the curriculum with bayesian optimization for task-specific
word representation learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany,
Volume 1: Long Papers</span>. The Association for Computer Linguistics, 2016.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ben Wang and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/kingoflolz/mesh-transformer-jax" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kingoflolz/mesh-transformer-jax</a>, May 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Chaoyi Wu, Xiaoman Zhang, Ya&nbsp;Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">Pmc-llama: Further finetuning llama on medical papers.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2304.14454, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian
Gehrmann, Prabhanjan Kambadur, David&nbsp;S. Rosenberg, and Gideon Mann.

</span>
<span class="ltx_bibblock">Bloomberggpt: A large language model for finance.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2303.17564, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Tongtong Wu, Massimo Caccia, Zhuang Li, Yuan-Fang Li, Guilin Qi, and
Gholamreza Haffari.

</span>
<span class="ltx_bibblock">Pretrained language model in continual learning: A comparative
study.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</span>.
OpenReview.net, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro
Lopez-Lira, and Jimin Huang.

</span>
<span class="ltx_bibblock">PIXIU: A large language model, instruction data and evaluation
benchmark for finance.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2306.05443, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona&nbsp;T. Diab, Xian Li, Xi&nbsp;Victoria Lin, Todor
Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit&nbsp;Singh
Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">OPT: open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2205.01068, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Mingmin Zhao, Shichao Yue, Dina Katabi, Tommi&nbsp;S Jaakkola, and Matt&nbsp;T Bianchi.

</span>
<span class="ltx_bibblock">Learning sleep stages from radio signals: A conditional adversarial
architecture.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4100‚Äì4109. PMLR, 2017.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Benchmark BloombergGPT‚Äôs Performance</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">As BloombergGPT is evaluated using an in-house data split, and the calculation details of reported metrics may not be identical, direct comparisons of their results with ours are not feasible. To adequately assess the efficacy of continual pre-training, we benchmark BloombergGPT‚Äôs performance against the FLARE framework. This involves evaluating OPT-66B and GPT-NeoX-20B‚Äôs performance, as obtained from FLARE, and comparing it to the results reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. This rigorous benchmarking ensures a fair and comprehensive evaluation, providing valuable insights into the effectiveness of our continual pre-training approach in relation to financial LLMs trained from scratch.</p>
</div>
<figure id="A1.T5" class="ltx_table">
<div id="A1.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:121.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.7pt,2.3pt) scale(0.963849345169969,0.963849345169969) ;">
<table id="A1.T5.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T5.2.1.1.1" class="ltx_tr">
<th id="A1.T5.2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="A1.T5.2.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="A1.T5.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" colspan="2">FLARE</th>
<th id="A1.T5.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="2">BloombergGPT</th>
</tr>
<tr id="A1.T5.2.1.2.2" class="ltx_tr">
<th id="A1.T5.2.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t"></th>
<th id="A1.T5.2.1.2.2.2" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="A1.T5.2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">GPT-NeoX</th>
<th id="A1.T5.2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">OPT-66B</th>
<th id="A1.T5.2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">GPT-NeoX</th>
<th id="A1.T5.2.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">OPT-66B</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T5.2.1.3.1" class="ltx_tr">
<th id="A1.T5.2.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">FPB</th>
<td id="A1.T5.2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">F1</td>
<th id="A1.T5.2.1.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">46.75</th>
<td id="A1.T5.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40.00</td>
<th id="A1.T5.2.1.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">44.64</th>
<td id="A1.T5.2.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">48.67</td>
</tr>
<tr id="A1.T5.2.1.4.2" class="ltx_tr">
<th id="A1.T5.2.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">FiQA SA</th>
<td id="A1.T5.2.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">F1</td>
<th id="A1.T5.2.1.4.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">73.86</th>
<td id="A1.T5.2.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r">37.36</td>
<th id="A1.T5.2.1.4.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_row">50.59</th>
<td id="A1.T5.2.1.4.2.6" class="ltx_td ltx_align_center">51.60</td>
</tr>
<tr id="A1.T5.2.1.5.3" class="ltx_tr">
<th id="A1.T5.2.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Headline</th>
<td id="A1.T5.2.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">F1</td>
<th id="A1.T5.2.1.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">62.62</th>
<td id="A1.T5.2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">61.36</td>
<th id="A1.T5.2.1.5.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_row">73.22</th>
<td id="A1.T5.2.1.5.3.6" class="ltx_td ltx_align_center">79.41</td>
</tr>
<tr id="A1.T5.2.1.6.4" class="ltx_tr">
<th id="A1.T5.2.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">NER</th>
<td id="A1.T5.2.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r">F1</td>
<th id="A1.T5.2.1.6.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">47.03</th>
<td id="A1.T5.2.1.6.4.4" class="ltx_td ltx_align_center ltx_border_r">52.24</td>
<th id="A1.T5.2.1.6.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_row">60.98</th>
<td id="A1.T5.2.1.6.4.6" class="ltx_td ltx_align_center">57.49</td>
</tr>
<tr id="A1.T5.2.1.7.5" class="ltx_tr">
<th id="A1.T5.2.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">Average</th>
<td id="A1.T5.2.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">F1</td>
<th id="A1.T5.2.1.7.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">57.57</th>
<td id="A1.T5.2.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">47.74</td>
<th id="A1.T5.2.1.7.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">57.36</th>
<td id="A1.T5.2.1.7.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">59.29</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="A1.T5.4.2" class="ltx_text" style="font-size:90%;">Evaluation results obtained on FLARE benchmark versus BloombergGPT <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for two public models: GPT-NeoX and OPT-66B.</span></figcaption>
</figure>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">Table <a href="#A1.T5" title="Table 5 ‚Ä£ Appendix A Benchmark BloombergGPT‚Äôs Performance ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> reports the comparison results. GPT-NeoX reports similar average task performance under two evaluation frameworks, but its performance on individual tasks varies. For example, the F1 score on FiQA SA obtained by FLARE is 46% higher than BloombergGPT‚Äôs evaluation, whereas F1 scores for Headline and NER are lower. Moreover, OPT-66B reports inferior results based on FLARE than BloombergGPT‚Äôs evaluation on all of the 4 tasks, and the average task performance is 20% lower. These results suggest that BloombergGPT‚Äôs evaluation results are inflated compared with FLARE. The comparison is still inconclusive unless BloombergGPT is benchmarked on FLARE or BloombergGPT‚Äôs evaluation configuration is made public.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Perplexity, Similarity and Diversity</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In this section, we present an in-depth analysis of the distribution of perplexity, similarity, and diversity within our financial corpus. Our findings reveal that all three metrics display a highly skewed distribution. Specifically, as illustrated in the top row of Figure <a href="#A2.F3" title="Figure 3 ‚Ä£ Appendix B Perplexity, Similarity and Diversity ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the similarity metric demonstrates a two-modal pattern, potentially attributable to the presence of two distinct sources within our financial corpus.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">Figure&nbsp;<a href="#A2.F4" title="Figure 4 ‚Ä£ Appendix B Perplexity, Similarity and Diversity ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the Spearman‚Äôs rank correlation of all three metrics. We see that the three metrics exhibit low correlation. This suggests that subsets of data we selected by ranking across these three metrics do not have a high degree of overlap. This inspired us to create the ETS-DACP-com method, which combines the three metrics together to balance the three different dimensions. Figure&nbsp;<a href="#A3.F5" title="Figure 5 ‚Ä£ Appendix C ETS-DACP-com vs ETS-DACP ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the quantile distribution of three metrics for selected subsets for each of the efficient DACP methods with hard sampling.</p>
</div>
<figure id="A2.F3" class="ltx_figure"><img src="/html/2311.08545/assets/x3.png" id="A2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="273" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="A2.F3.3.2" class="ltx_text" style="font-size:90%;">Distribution of perplexity, similarity and diversity. </span></figcaption>
</figure>
<figure id="A2.F4" class="ltx_figure"><img src="/html/2311.08545/assets/x4.png" id="A2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="A2.F4.3.2" class="ltx_text" style="font-size:90%;">Spearman‚Äôs rank correlation heatmap between perplexity, similarity, and entropy measures.</span></figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>ETS-DACP-com vs ETS-DACP</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">ETS-DACP-com effectively strikes a balance between constructing a domain-specific LLM and a task-specific LLM. To demonstrate its efficacy, we utilize the average quantile of similarity, knowledge novelty, and diversity as the sampling weights. By applying these weights, we perform weighted sampling, selecting 10% and 20% of the financial corpus without replacement to construct the training data.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">The average sample quantile for various subsets of the financial corpus is illustrated in Figure <a href="#A3.F5" title="Figure 5 ‚Ä£ Appendix C ETS-DACP-com vs ETS-DACP ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We claim that using a simple average of quantiles for the three metrics achieves a good balance among the three dimensions‚Äîthe average quantile for the three dimensions lies in a similar ballpark for each subset. In contrast, the subset for ETS-DACP exhibits higher perplexity and lower or middle entropy, suggesting that unlabeled task data contains new knowledge but is less diverse. For ETA-DACP-ppl and ETA-DACP-ent, the samples are uniform across the other two dimensions.</p>
</div>
<figure id="A3.F5" class="ltx_figure"><img src="/html/2311.08545/assets/x5.png" id="A3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="A3.F5.4.2" class="ltx_text" style="font-size:90%;color:#000000;">Average sample quantile of subsets of financial corpus used in ETS-DACP-com and ETS-DACP.<span id="A3.F5.4.2.1" class="ltx_text" style="color:#000000;"> </span></span></figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Train and Test Loss of Efficient DACP Methods</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">We show the plots of Finance domain loss (Fin Test) and open domain loss (Pile Loss) for our efficient DACP methods in Figure&nbsp;<a href="#A4.F6" title="Figure 6 ‚Ä£ Appendix D Train and Test Loss of Efficient DACP Methods ‚Ä£ Efficient Continual Pre-training for Building Domain Specific Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. ETS-DACP-com (Hard sampling) has the lowest loss for Fin Test loss as it uses both task knowledge and also uses high entropy/perplexity samples in the the larger financial pile. All methods have similar Fin Test loss for Soft sampling as we sample entire financial corpus space for sampling.</p>
</div>
<figure id="A4.F6" class="ltx_figure"><img src="/html/2311.08545/assets/x6.png" id="A4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="350" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A4.F6.3.2" class="ltx_text" style="font-size:90%;">Loss curves: in domain loss (Fin Test loss) on left and general domain loss (Pile loss) on right for our Efficient DACP class of methods.</span></figcaption>
</figure>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p">ETS-DACP has the highest loss for open domain Pile loss. However, we did not observe any degradation of performance on open domain tasks with ETS-DACP. Surprisingly, there is a tight correlation between losses of ETS-DACP-ent and ETS-DACP-ppl, while ETS-DACP-ppl performs consistently and considerably worse than ETS-DACP-ent on our tasks. These observations suggest that there is no good correlation between actual our task performance and loss curves. Using validation/test loss with unlabeled data is not a good proxy for task performance, atleast in this domain. This is supported by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>‚Äôs observations on low correlation between task performance and pre-training loss.</p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Financial Dataset Curation</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">We describe the two data sources for curating our domain corpus: Financial News CommonCrawl and SEC filings.</p>
</div>
<section id="A5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Financial News CommonCrawl [13.2B words, 83.5%]</h5>

<div id="A5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px1.p1.1" class="ltx_p">We curate an English financial news dataset by pre-processing the publicly available News CommonCrawl dumps hosted on AWS S3<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">‚Ä°</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä°</sup><span class="ltx_tag ltx_tag_note">‚Ä°</span><a href="s3://commoncrawl" title="" class="ltx_ref ltx_url ltx_font_typewriter">s3://commoncrawl</a></span></span></span> spanning from 2016 to 2022. To identify financial news articles from the vast collection of News CommonCrawl dumps, we employ two filtering mechanisms: the domain filter and the URL keyword filter. Firstly, we establish a comprehensive portfolio of web domains corresponding to reputable news outlets that predominantly focus on financial, economic, and business news, such as CNBC. We retain news articles specifically sourced from these financial news domains, which constitute a substantial portion of our financial corpus.</p>
</div>
<div id="A5.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="A5.SS0.SSS0.Px1.p2.1" class="ltx_p">Secondly, to capture financial articles from general news outlets, we observe that many of them designate dedicated sections or subdomains for business, economy, or finance news, like Fox Business. To effectively identify these financial articles, we implement a simple yet effective keyword-based approach that targets financial sections and subdomains within general news outlets. The filtering processes ensure the selection of a financial corpus appropriate for our continual pre-training in the financial domain.</p>
</div>
<figure id="A5.F7" class="ltx_figure"><img src="/html/2311.08545/assets/x7.png" id="A5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A5.F7.3.2" class="ltx_text" style="font-size:90%;">Financial news size by month</span></figcaption>
</figure>
</section>
<section id="A5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SEC Filing [3.3B words, 16.5%]</h5>

<div id="A5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px2.p1.1" class="ltx_p">Public companies in the United States are legally required to submit their financial statements on a regular basis. The Securities and Exchange Commission (SEC) facilitates public access to these filings through the Electronic Data Gathering, Analysis, and Retrieval (EDGAR) System, which has been available since 1993. On average, this system accommodates approximately 40,000 new files per year. To enrich our financial corpus, we include 10-K filings from the period spanning 1993 to 2022. To ensure data accuracy and consistency, these filings are parsed and pre-processed using the package detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Furthermore, we optimize the quality of our corpus by eliminating report sections containing less than 20 words, to remove spurious examples.</p>
</div>
</section>
<section id="A5.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">List of Domains used to Filter Financial News</h5>

<div id="A5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px3.p1.1" class="ltx_p">We use the following keywords to identify subdomains and urls: economy, market, finance, money, wealth, invest, business, industry.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.08544" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.08545" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2311.08545">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.08545" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.08546" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 19:10:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>
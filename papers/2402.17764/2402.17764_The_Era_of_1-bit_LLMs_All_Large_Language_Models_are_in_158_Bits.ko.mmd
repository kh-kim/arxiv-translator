# The Era of 1-bit LLMs:

모든 대용량 언어 모델은 1.58비트

마홍유왕링샤오마레이왕원휘왕샤오한황리동루이왕지롱슈후루웨이

###### Abstract

비트넷[23]과 같은 최근의 연구는 LMM(1-bit Large Language Models)의 새로운 시대를 위한 길을 열어주고 있다. 이 작업에서 LLM의 모든 단일 매개 변수(또는 가중치)가 3진 {-1, 0, 1}인 1비트 LLM 변형, 즉 **BitNet b1.58** 을 소개합니다. 이는 전체-정밀도(즉, FP16 또는 BF16) 트랜스포머 LLM과 동일한 모델 크기 및 트레이닝 토큰이 복잡성 및 엔드-태스크 성능 둘 다 측면에서 매칭되는 한편, 레이턴시, 메모리, 스루풋 및 에너지 소비 측면에서 훨씬 더 비용 효율적이다. 보다 심오하게, 1.58비트 LLM은 고성능 및 비용 효율적인 새로운 세대의 LLM을 훈련시키기 위한 새로운 스케일링 법칙 및 레시피를 정의한다. 또한, 새로운 연산 패러다임을 가능하게 하며, 1비트 LLM에 최적화된 특정 하드웨어를 설계할 수 있는 문을 연다.

Shuming Ma Hongyu Wang Lingxiao Ma Lei Wang Wenhui Wang Shaohan Huang Li Dong Ruiping Wang Jilong Xue Furu Wei [https://aka.ms/GeneralAI](https://aka.ms/GeneralAI)

도 1: 1비트 LLMs(예를 들어, BitNet b1.58)는 모델 성능을 유지하면서 LLMs의 추론 비용(대기 시간, 처리량 및 에너지)을 감소시키기 위한 파레토 솔루션을 제공한다. 비트넷 b1.58의 새로운 계산 패러다임은 1비트 LLM에 최적화된 새로운 하드웨어를 설계하기 위한 동작을 요구한다.

1비트 LLM의 시대

최근 AI 분야는 LMM(Large Language Models)의 규모와 역량이 급성장하고 있다. 이러한 모델은 광범위한 자연어 처리 작업에서 놀라운 성능을 보여주었지만, 그 크기가 증가함에 따라 배치에 대한 문제가 제기되었고 높은 에너지 소비로 인해 환경 및 경제적 영향에 대한 우려가 제기되었다. 이러한 과제들을 해결하기 위한 하나의 접근법은 추론용 저-비트 모델들을 생성하기 위해 사후 트레이닝 양자화를 사용하는 것이다[23, 1, 13, 14]. 이 기술은 가중치 및 활성화의 정밀도를 감소시켜 LLM의 메모리 및 계산 요구 사항을 상당히 감소시킨다. 추세는 16 비트로부터 4 비트 변형들[1, 15]과 같은 하위 비트들로 이동하는 것이었다. 그러나 학습 후 양자화는 산업 LLM에서 널리 사용되지만 차선책이다.

비트넷[23]과 같은 1비트 모델 아키텍처에 대한 최근의 연구는 성능을 유지하면서 LLM의 비용을 줄이는 유망한 방향을 제시한다. 바닐라 LLM은 16비트 부동 값(즉, FP16 또는 BF16)에 있고, 임의의 LLM의 대부분은 행렬 곱셈이다. 따라서 부동소수점 덧셈과 곱셈 연산에서 계산 비용이 크게 발생한다. 반면에, BitNet의 행렬 곱셈은 정수 덧셈만을 포함하므로 LLM의 에너지 비용을 절약할 수 있다. 많은 칩에서 성능을 계산하는 근본적인 한계는 전력이기 때문에 에너지 절약도 더 빠른 계산으로 변환될 수 있다.

계산 외에도, 모델 파라미터들을 DRAM으로부터 온-칩 가속기(예를 들어, SRAM)의 메모리로 전달하는 프로세스는 추론 동안 비용이 많이 들 수 있다. 스루풋을 개선하기 위해 SRAM을 확대하려는 시도가 있었지만, 이것은 DRAM보다 상당히 높은 비용을 도입한다. 전체 정밀도 모델에 비해 1비트 LLM은 용량 및 대역폭 관점에서 훨씬 낮은 메모리 공간을 가지고 있다. 이것은 DRAM으로부터의 가중치 로딩의 비용 및 시간을 상당히 감소시킬 수 있고, 더 빠르고 더 효율적인 추론으로 이어질 수 있다.

이 작업에서 **BitNet b1.58** 이라는 중요 한 1 비트 LLM 변형을 도입 합니다. 여기서 모든 매개 변수는 {-1, 0, 1} 값을 사용 하 여 3진법입니다. 우리는 원래의 1비트 BitNet에 0의 추가 값을 추가하여 이진 시스템에서 1.58비트를 생성했다. 비트넷 b1.58은 행렬 곱셈을 위한 곱셈 연산이 거의 필요하지 않고 고도로 최적화될 수 있는 새로운 계산 패러다임을 포함하여 원래 1비트 비트넷의 모든 이점을 유지한다. 추가적으로, 그것은 원래의 1비트 비트넷과 동일한 에너지 소비를 가지며 FP16 LLM 베이스라인에 비해 메모리 소비, 스루풋 및 레이턴시 측면에서 훨씬 더 효율적이다. 또한 BitNet b1.58은 두 가지 추가 이점을 제공합니다. 첫째, 특징 필터링에 대한 명시적인 지원으로 인해 모델링 성능이 더 강하며, 모델 가중치에 0을 포함함으로써 1비트 LLM의 성능을 크게 향상시킬 수 있다. 두 번째로, 우리의 실험은 비트넷 b1.58이 동일한 구성(예를 들어, 모델 크기, 트레이닝 토큰 등)을 사용할 때, 3B 크기로부터 시작하여, 복잡성과 엔드-태스크 성능 둘 다 측면에서 완전 정밀도(즉, FP16) 기준선을 매칭할 수 있음을 보여준다.

## 2 BitNet b1.58

BitNet b1.58은 _nn.Linear_를 _BitLinear_로 대체하는 Transformer인 BitNet 아키텍처를 기반으로 한다. 1.58비트 가중치 및 8비트 활성화로 처음부터 훈련됩니다. 원래 BitNet과 비교하여 아래에 요약한 몇 가지 수정 사항을 소개합니다.

양자화 함수. 가중치를 -1, 0 또는 +1로 제한하기 위해, 우리는 _absmean_ 양자화 함수를 채택한다. 먼저 가중치 행렬을 자신의 평균 절대값으로 스케일링한 후 각 값을 {-1, 0, +1} 중 가장 가까운 정수로 반올림한다:

\[\widetilde{W}=\mathrm{RoundClip}(\frac{W}{\gamma+\epsilon},-1,1), \tag{1}\] \[\mathrm{RoundClip}(x,a,b)=\max(a,\min(b,\mathrm{round}(x))),\] (2) \[\gamma=\frac{1}{nm}\sum_{ij}|W_{ij}|. \tag{3}\]

활성화에 대한 양자화 함수는 비트넷에서 비선형 함수 이전의 활성화를 범위 \([0,Q_{b}]\)으로 확장하지 않는다는 점을 제외하고는 동일한 구현을 따른다. 대신, 활성화는 모두 영점 양자화를 제거하기 위해 토큰당 \([-Q_{b},Q_{b}]\)으로 조정된다. 이것은 구현 및 시스템 수준 최적화 모두에 대해 더 편리하고 간단하지만 실험에서 성능에 무시할 수 있는 영향을 도입한다.

LLaMA와 유사한 컴포넌트. LLaMA [111, 123]의 아키텍처는 오픈 소스 LLM의 실제 백본이었다. 오픈 소스 커뮤니티를 수용하기 위해 비트넷 b1.58의 디자인은 LLaMA 유사 구성 요소를 채택한다. 구체적으로 RMSNorm[10], SwiGLU[11], 회전 임베딩[12]을 사용하며, 모든 바이어스를 제거한다. 이러한 방식으로, 비트넷 b1.58은 최소한의 노력으로 인기 있는 오픈 소스 소프트웨어(예를 들어, 허깅페이스, vLLM[11], 및 llama.cpp2)에 통합될 수 있다.

각주 2: [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)

## 3 Results

비트넷 b1.58을 다양한 크기의 재생 FP16 LLaMA LLM과 비교했다. 공정하게 비교하기 위해 1,000억 토큰에 대해 RedPajama 데이터 세트 [14]에 대한 모델을 사전 훈련했다. ARC-Easy [10], ARC-Challenge [10], Hellaswag [11], Winogrande [2], PIQA [10], OpenbookQA [12], BoolQ [13]를 포함한 다양한 언어 작업에서 제로 샷 성능을 평가했다. 우리는 또한 위키텍스트2 [12] 및 C4 [14] 데이터 세트에 대한 검증 복잡성을 보고했다.

LLaMA LLM과 BitNet b1.58의 런타임 GPU 메모리와 레이턴시를 비교하였다. 결과는 GPU 장치에서 LLM 추론 레이턴시에 잘 최적화된 FasterTransformer3 코드베이스를 사용하여 측정되었다. Ladder [23]의 2비트 커널은 BitNet b1.58을 위해 통합되었다. 우리는 추론에 대한 주요 비용인 출력 토큰당 시간을 보고했다.

각주 3: [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)

표 1은 비트넷 b1.58 및 LLaMA LLM에 대한 복잡성과 비용을 요약한 것이다. 비트넷 b1.58은 복잡성 측면에서 3B 모델 크기에서 완전 정밀도 LLaMA LLM을 일치시키기 시작하는 반면, GPU 메모리는 2.71배 더 빠르고 3.55배 더 적게 사용함을 보여준다. 특히, 3.9B 모델 크기의 BitNet b1.58은 2.4배 빠르고, 3.32배 적은 메모리를 소모하지만, LLaMA LLM 3B에 비해 상당히 우수한 성능을 보인다.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Models** & **Size** & **Memory (GB)\(\downarrow\)** & **Latency (ms)\(\downarrow\)** & **PPL\(\downarrow\)** \\ \hline LLaMA LLM & 700M & 2.08 (1.00x) & 1.18 (1.00x) & 12.33 \\
**BitNet b1.58** & 700M & 0.80 (2.60x) & 0.96 (1.23x) & 12.87 \\ \hline LLaMA LLM & 1.3B & 3.34 (1.00x) & 1.62 (1.00x) & 11.25 \\
**BitNet b1.58** & 1.3B & 1.14 (2.93x) & 0.97 (1.67x) & 11.29 \\ \hline LLaMA LLM & 3B & 7.89 (1.00x) & 5.07 (1.00x) & 10.04 \\
**BitNet b1.58** & 3B & **2.22 (3.55x)** & **1.87 (2.71x)** & **9.91** \\
**BitNet b1.58** & 3.9B & **2.38 (3.32x)** & **2.11 (2.40x)** & **9.62** \\ \hline \hline \end{tabular}
\end{table}
표 1: BitNet b1.58 및 LLaMA LLM의 비용뿐만 아니라 복잡성.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**Models** & **Size** & **ARCe** & **ARCc** & **HS** & **BQ** & **OQ** & **PQ** & **WGe** & **Avg.** \\ \hline LLaMA LLM & 700M & 54.7 & 23.0 & 37.0 & 60.0 & 20.2 & 68.9 & 54.8 & 45.5 \\
**BitNet b1.58** & 700M & 51.8 & 21.4 & 35.1 & 58.2 & 20.0 & 68.1 & 55.2 & 44.3 \\ \hline LLaMA LLM & 1.3B & 56.9 & 23.5 & 38.5 & 59.1 & 21.6 & 70.0 & 53.9 & 46.2 \\
**BitNet b1.58** & 1.3B & 54.9 & 24.2 & 37.7 & 56.7 & 19.6 & 68.8 & 55.8 & 45.4 \\ \hline LLaMA LLM & 3B & 62.1 & 25.6 & 43.3 & 61.8 & 24.6 & 72.1 & 58.2 & 49.7 \\
**BitNet b1.58** & 3B & **61.4** & **28.3** & **42.9** & **61.5** & **26.6** & **71.5** & **59.3** & **50.2** \\
**BitNet b1.58** & 3.9B & **64.2** & **28.7** & **44.2** & **63.5** & **24.2** & **73.2** & **60.5** & **51.2** \\ \hline \hline \end{tabular}
\end{table}
표 2: 엔드 태스크에서 비트넷 b1.58 및 LLaMA LLM의 제로 샷 정확도.

<표 2>는 엔드 태스크에 대한 제로샷 정확도의 세부 결과를 보고한다. 우리는 평가를 수행하기 위해 _lm-평가-harness4_의 파이프라인을 따랐다. 결과는 비트넷 b1.58과 LLaMA LLM 사이의 성능 격차가 모델 크기가 증가함에 따라 좁아짐을 보여준다. 더 중요한 것은, 비트넷 b1.58은 3B 크기로부터 시작하는 완전 정밀도 베이스라인의 성능을 일치시킬 수 있다는 것이다. 퍼플렉시티의 관찰과 유사하게, 엔드-태스크 결과는 비트넷 b1.58 3.9B가 더 낮은 메모리 및 레이턴시 비용으로 LLaMA LLM 3B보다 성능이 우수함을 보여준다. 이는 비트넷 b1.58이 최신 LLM 모델에 비해 파레토 개선임을 보여준다.

각주 4: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)

메모리 및 잠복기 모델 크기를 7B, 13B 및 70B로 추가로 확장하고 비용을 평가했다. 그림 2는 지연 시간과 메모리의 추세를 보여주며 모델 크기가 확장될수록 속도가 증가한다는 것을 보여준다. 특히 비트넷 b1.58 70B는 LLaMA LLM 기준선보다 4.1배 빠르다. _nn.Linear_의 시간 비용은 모델 크기에 따라 커지기 때문입니다. 임베딩이 완전한 정밀도를 유지하고 더 큰 모델의 경우 메모리 비율이 더 작기 때문에 메모리 소비는 유사한 경향을 따른다. 레이턴시와 메모리 모두 2비트 커널로 측정되었기 때문에 여전히 비용을 더 줄일 수 있는 최적화의 여지가 있다.

또한 BitNet b1.58과 LLaMA LLM의 산술 연산 에너지 소비량을 추정한다. 우리는 행렬 곱셈이 LLM의 비용에 가장 크게 기여하기 때문에 주로 행렬 곱셈 계산에 중점을 둔다. 그림 3은 에너지 비용의 구성을 나타낸 것이다. 비트넷 b1.58의 대부분은 INT8 추가 계산인 반면 LLaMA LLM은 FP16 추가 및 FP16 곱셈으로 구성된다. [12, 2]의 에너지 모델에 따르면 비트넷 b1.58은 7nm 칩에서 행렬 곱셈을 위해 71.4배의 산술 연산 에너지 소비를 절약한다. 우리는 512 토큰이 있는 모델에 대한 종단 간 에너지 비용을 추가로 보고했다. 우리의 결과는 모델 크기 규모에 따라 비트넷 b1.58이 FP16 LLaMA LLM 기준선에 비해 에너지 소비 측면에서 점점 더 효율적이 된다는 것을 보여준다. 이는 _nn.Linear_의 비율이 모델 크기에 따라 증가하는 반면 다른 구성 요소의 비용은 더 큰 모델의 경우 더 작기 때문입니다.

Throughput우리는 LLaMA LLM 70B가 디바이스에서 실행될 수 있도록 파이프라인 병렬성 [1]을 사용하여 두 개의 80GB A100 카드에서 70B 파라미터와 BitNet b1.58 및 LLaMA LLM의 처리량을 비교한다. GPU 메모리 한계에 도달할 때까지 배치 크기를 512의 시퀀스 길이로 늘렸다. 표 3은 BitNet b1.58 70B가 LLaMA LLM의 배치 크기의 최대 11배를 지원할 수 있어 처리량이 8.9배 더 높다는 것을 보여준다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Models** & **Size** & **Max Batch Size** & **Throughput (tokens/s)** \\ \hline LLaMA LLM & 70B & 16 (1.0x) & 333 (1.0x) \\
**BitNet b1.58** & 70B & **176 (11.0x)** & **2977 (8.9x)** \\ \hline \hline \end{tabular}
\end{table}
표 3: BitNet b1.58 70B와 LLaMA LLM 70B의 처리량 비교.

도 2: 모델 크기를 변화시키는 BitNet b1.58의 디코딩 레이턴시(왼쪽) 및 메모리 소비량(오른쪽).

[MISSING_PAGE_FAIL:5]

### LLMs에서 긴 시퀀스의 네이티브 지원

LLM 시대에 긴 시퀀스를 다룰 수 있는 능력이 중요한 요구가 되었다. 긴 시퀀스 추론을 위한 한 가지 주요 과제는 KV 캐시에 의해 도입된 메모리 소비이다. 비트넷 b1.58은 16 비트에서 8 비트로 활성화를 감소시켜 동일한 리소스가 주어지면 컨텍스트 길이를 두 배로 늘릴 수 있기 때문에 긴 시퀀스에 대한 네이티브 지원을 향한 중요한 단계를 나타낸다. 이것은 향후 작업으로 남겨두는 1.58비트 LLM에 대해 4비트 또는 훨씬 더 낮은 비트로 더 손실 없이 압축될 수 있다.

### Edge 및 Mobile의 LLMs

1.58비트 LLM을 사용하면 에지 및 모바일 장치에서 언어 모델의 성능을 크게 향상시킬 수 있다. 이러한 장치는 종종 메모리 및 계산 능력에 의해 제한되며, 이는 LLM의 성능과 규모를 제한할 수 있다. 그러나, 1.58-비트 LLM들의 감소된 메모리 및 에너지 소비는 이들이 이들 디바이스들 상에 배치될 수 있게 하여, 이전에 불가능했던 광범위한 애플리케이션들을 가능하게 한다. 이는 에지 및 모바일 디바이스의 능력을 크게 향상시킬 수 있고 LLM의 새롭고 흥미로운 애플리케이션을 가능하게 할 수 있다. 또한, 1.58비트 LLM은 에지 및 모바일 장치에서 사용되는 주요 프로세서인 CPU 장치에 더 친화적이다. 이것은 비트넷 b1.58이 이들 디바이스에서 효율적으로 실행될 수 있고, 이들의 성능 및 능력을 더욱 향상시킬 수 있음을 의미한다.

### 1비트 LLMs용 새 하드웨어

Groq5와 같은 최근 작업은 LLM을 위한 특정 하드웨어(예: LPU)를 구축하는 유망한 결과와 큰 잠재력을 보여주었다. 한 걸음 더 나아가, 우리는 비트넷[23]에서 가능하게 된 새로운 계산 패러다임을 고려할 때, 1-비트 LLM에 특별히 최적화된 새로운 하드웨어 및 시스템을 설계하기 위한 동작을 구상하고 요구한다.

각주 5: [https://groq.com/](https://groq.com/)

## References

* [BZB\({}^{+}\)19] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: 자연 언어에서 물리적 상식에 대한 추론 _ CoRR_, abs/1911.11641, 2019.
* [CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 보장성을 갖는 대형 언어 모델의 2비트 양자화. _ CoRR_, abs/2307.13304, 2023.
* [CLC\({}^{+}\)19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구합니다. _ CoRR_, abs/1905.10044, 2019.
* [Com23] Together Computer. 레드파자마: 2023년 대규모 언어 모델을 훈련하기 위한 공개 데이터 세트입니다.
* [FAHA23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: 생성 사전 훈련된 트랜스포머에 대한 정확한 양자화. _The 11번째 International Conference on Learning Representations_, 2023.
* [HCB\({}^{+}\)19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, Hyouk Joongong Lee, Jiquan Ngiam, Quoc V. 르, 우용희, 지펑 첸 Gpipe: 파이프라인 병렬성을 이용한 거대 신경망의 효율적인 훈련. _Neural Information Processing Systems의 발전_에서, 페이지 103-112, 2019.
* [Hor14] Mark Horowitz. 1.1 computing의 에너지 문제 (그리고 우리가 그것에 대해 할 수 있는 것). _2014 IEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest of Technical Papers, San Francisco, CA, USA, February 9-13, 2014_, pages 10-14, 2014.
* [KLZ\({}^{+}\)23] Woosuk Kwon, Zhuohan Li, Siyuan Zhang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 페이징된 주의력을 갖는 대용량 언어 모델을 위한 효율적인 메모리 관리 _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_, 2023.

* [LTT\({}^{+}\)23] 지린, 지밍탕, 하오톈탕, 상양, 신규당, 송한. AWQ: LLM 압축 및 가속을 위한 활성화 인식 가중치 양자화. _ CoRR_, abs/2306.00978, 2023.
* [MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal. 갑옷이 전기를 전도할 수 있나요? 오픈 북 질문 답변을 위한 새 데이터 세트입니다. _ CoRR_, abs/1809.02789, 2018.
* [MXBS16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 포인터 센티넬 혼합물 모델, 2016년
* [PKL\({}^{+}\)16] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. LAMBADA 데이터 세트: 광범위한 담화 맥락을 요구하는 단어 예측. _Proceedings of the 54th Annual Meeting of the Association of Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers_. 2016년 컴퓨터 언어학 협회
* [RSR\({}^{+}\)19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계를 탐구합니다. _ CoRR_, abs/1910.10683, 2019.
* [SAL\({}^{+}\)24] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu. 로포르머: 회전 위치 매립을 갖는 향상된 트랜스포머. _ Neurocomputing_, 568:127063, 2024.
* [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Wino-Grande: 규모의 적대적 위노그라드 스키마 도전이다. _The Thirty-Fourth AAAI Conference on Artificial Intelligence_, pages 8732-8740, 2020.
* [Sha20] Noam Shazeer. GLU 변형은 변압기를 개선 합니다. _ CoRR_, abs/2002.05202, 2020.
* [TBMR] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. Stablelm 3b 4e1t.
* [TCS\({}^{+}\)24] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Hadamard incoherence 및 격자 코드북을 사용 하 여 LLM 양자화가 훨씬 더 좋습니다. _ CoRR_, abs/2402.04396, 2024.
* [TLI\({}^{+}\)23] Hugo Touvron, Thibaut Lavril, Gautier Izzard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: 개방형 및 효율적인 기초 언어 모델입니다. _ CoRR_, abs/2302.13971, 2023.
* [TMS\({}^{+}\)23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned chat models. _ CoRR_, abs/2307.09288, 2023.
* [WLG17] Johannes Welbl, Nelson F. Liu, and Matt Gardner. 다중 선택 과학 질문을 크라우드소싱합니다. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, _Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017_, pages 94-106. Association for Computational Linguistics, 2017.
* [WMC\({}^{+}\)23] Lei Wang, Lingxiao Ma, Shijie Cao, Ningxin Zheng, Quanlu Zhang, Jilong Xue, Ziming Miao, Ting Cao, and Yuqing Yang. 사다리: 사용자 정의된 데이터 형식의 효율적인 텐서 컴파일입니다. _OSDI_, 2023.
* [WMD\({}^{+}\)23] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 비트넷: 대규모 언어 모델을 위한 1비트 변환기의 크기 조정 _ CoRR_, abs/2310.11453, 2023.

* [XLS\({}^{+}\)23] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: 대형 언어 모델에 대한 정확하고 효율적인 사후 트레이닝 양자화. _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, 2023.
* [YBS19] Vikas Yadav, Steven Bethard, and Mihai Surdeanu. 빠르고 (그렇지 않음) 더럽음: 멀티-홉 질문 응답을 위한 정당화 문장의 감독되지 않은 선택. 이누이 겐타로, 징장, 빈센트 응, 샤오준 완에서 편집자 _EMNLP-IJCNLP_ 2019.
* [ZHB\({}^{+}\)19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? # 57회 Computational Linguistics Association of the 57th Conference of the Computational Linguistics_, pages 4791-4800, 2019.
* [ZS19] Biao Zhang and Rico Sennrich. 루트 평균 제곱 레이어 정규화. 한나 M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems_, pages 12360-12371, 2019.
* [ZZL22] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. PokeBNN: 경량화 정확도의 바이너리 추구. _IEEE/CVF Conference on Computer Vision and Pattern Recognition_에서, 페이지 12465-12475. IEEE, 2022.

# 프로덕션 언어 모델의 일부 훔치기

 니콜라스 칼리니

Daniel Paleka

크리쉬나무르티 디비조탐

Thomas Steinke

Jonathan Hayase

A 페더 쿠퍼

Katherine Lee

Matthew Jagielski

Milad Nasr

Arthur Conny

Eric Wallace

David Rolnick

Florian Tramer

###### Abstract

본 논문에서는 OpenAI의 ChatGPT나 구글의 PaLM-2와 같은 블랙박스 제작 언어 모델로부터 정확하고 비사소한 정보를 추출하는 첫 번째 모델스틸링 공격을 소개한다. 구체적으로, 본 공격은 전형적인 API 접근이 주어졌을 때 트랜스포머 모델의 _embedding projection layer_ (최대 대칭)를 복구한다. 미화 20달러 미만의 경우, 공격은 OpenAI의 ada 및 babbage 언어 모델의 전체 투영 행렬을 추출한다. 따라서 이러한 블랙박스 모델이 각각 1024 및 2048의 숨겨진 차원을 갖는다는 것을 처음으로 확인한다. 또한 gpt-3.5-터보 모델의 정확한 은닉 차원 크기를 복구하고 전체 투영 행렬을 복구하는 데 2,000달러 미만의 쿼리가 소요될 것으로 추정한다. 우리는 잠재적인 방어와 완화로 결론을 내리고 공격을 확장할 수 있는 가능한 미래 작업의 의미에 대해 논의한다.

머신러닝, ICML

## 1 Introduction

GPT-4, Claude 2 또는 Gemini와 같은 오늘날 가장 인기 있는 대형 언어 모델의 내부 작동에 대해서는 공개적으로 알려져 있는 바가 거의 없다. GPT-4 기술 보고서는 "아키텍처(모델 크기 포함), 하드웨어, 트레이닝 컴퓨트, 데이터세트 구성, 트레이닝 방법, 또는 이와 유사한 것에 대한 어떠한 [...] 세부사항도 포함하지 않는다"(OpenAI 등, 2023). 유사하게, PaLM-2 논문은 "[the] 모델 크기 및 아키텍처의 세부사항들은 외부 출판으로부터 보류된다"(Anil et al., 2023). 이러한 비밀성은 종종 "경쟁 구도"(이러한 모델은 훈련하는 데 비용이 많이 들기 때문)와 "대규모 모델의 안전 영향"(OpenAI 등, 2023)에 기인한다(더 많은 정보가 이용 가능할 때 모델을 공격하는 것이 더 쉽기 때문이다). 그럼에도 불구하고 이러한 모델의 가중치와 내부 세부 정보는 공개적으로 액세스할 수 없지만 모델 자체는 API를 통해 노출된다.

이 문서에서 우리는 질문합니다. _적수가 API에 쿼리를 만들어 프로덕션 언어 모델에 대해 얼마나 많은 정보를 배울 수 있습니까?_ 이것은 _모델 훔치기_ 분야에서 연구된 질문입니다(Tramer et al., 2016): 적대자가 API를 쿼리하여 모델 가중치를 추출하는 능력입니다.

기여.우리는 블랙박스 언어 모델에 적용할 수 있는 공격을 소개하고 변압기 언어 모델의 완전한 _임베딩 프로젝션 레이어_ 를 복구할 수 있습니다. 우리의 공격은 입력 계층에서 시작하여 _상향식_ 방식으로 모델을 재구성하는 이전 접근 방식에서 벗어난다. 대신 공격은 _하향식_ 으로 작동 하 고 모델의 마지막 계층을 직접 추출 합니다. 특히, 언어 모델의 최종 계층이 숨겨진 차원으로부터 (고차원) 로짓 벡터로 투영된다는 사실을 이용한다. 따라서 이 최종 레이어는 순위가 낮으며 모델의 API에 대상 쿼리를 만들어 임베딩 차원 또는 최종 가중치 행렬을 추출할 수 있다.

이 층을 훔치는 것은 여러 가지 이유로 유용하다. 첫째, 변압기 모델의 _폭_ 을 보여주며, 이는 종종 총 매개변수 수와 상관 관계가 있다. 둘째, 모델이 완전한 "블랙박스"인 정도를 약간 감소시키므로 향후 공격에 유용할 수 있다. 셋째, 우리의 공격은 전체 모델의 일부(상대적으로 작은 부분)만 복구하지만 프로덕션 모델의 _모든_ 매개 변수를 훔칠 수 있다는 사실은 놀랍고 이 공격의 확장이 더 많은 정보를 복구할 수 있을 수 있다는 우려를 제기한다. 마지막으로, 모델의 마지막 층(및 따라서 숨겨진 차원)을 복구하는 것은 상이한 모델들 사이의 상대적 크기 차이들과 같은 모델에 대한 더 많은 글로벌 정보를 드러낼 수 있다.

우리의 공격은 효과적이고 효율적이며 API가 전체 로그프로브 또는 "로짓 편향"을 노출하는 프로덕션 모델에 적용할 수 있다. 여기에는 구글의 PaLM-2 및 OpenAI의 GPT-4(Anil 등, 2023; OpenAI 등, 2023)가 포함되었으며 책임 있는 공개 후 두 API 모두 공격을 방지하거나 더 비싸게 만들기 위해 방어를 구현했다. 우리는 평균 제곱 오차 \(10^{-4}\) (불가피한 대칭까지)의 여러 OpenAI 모델의 임베딩 레이어를 추출한다. 우리는 200달러 미만의 비용으로 제한된 형태의 공격을 gpt-3.5에 적용하고 전체 임베딩 계층을 복구하는 대신 임베딩 차원의 크기만 복구한다.

책임 있는 공개.우리는 이 공격에 취약한 것으로 알고 있는 모든 서비스와 공격을 공유했습니다. 또한 다른 환경에서 공격의 변형이 가능할 수 있기 때문에 특정 공격에 취약하지 않더라도 여러 인기 있는 서비스와 공격을 공유했습니다. 우리는 모델의 마지막 레이어의 매개변수를 추출하기 전에 OpenAI로부터 승인을 받았고 OpenAI와 협력하여 접근법의 유효성을 확인한 다음 공격과 관련된 모든 데이터를 삭제했다. 우리의 공격에 대응하여, OpenAI와 구글은 (섹션 8에서 제안하는 것과 같은) 완화 및 방어를 도입하기 위해 API를 수정하여 적들이 이 공격을 수행하는 것을 더 어렵게 만들었다.

## 2 관련 작업

모델 훔치기 공격들(Tramer et al., 2016)은 블랙박스 모델의 기능성을 복구하는 것을 목표로 하고, 두 가지 목적들 중 하나에 대해 최적화한다(Jagielski et al., 2020)).

1. _정확도_: 도난된 모델 \(\hat{f}\)은 특정 데이터 도메인에서 대상 모델 \(f\)의 성능과 일치해야 합니다. 예를 들어, 표적이 이미지 분류기인 경우 도난된 모델이 이미지넷에서 표적의 전체 정확도와 일치하기를 원할 수 있다.
2. _ 충실도_: 도난된 모델 \(\hat{f}\)은 모든 입력에서 대상 모델 \(f\)과 기능적으로 동일해야 합니다. 즉, 유효한 입력 \(p\)에 대해 \(\hat{f}(p)\approx f(p)\를 원한다.

본 논문에서는 고충실도 공격에 초점을 맞춘다. 대부분의 사전 고충실도 공격은 ReLU 활성화로 심층 신경망의 특정 특성을 이용한다. Milli 등(2019)은 먼저 공격자가 대상 2층 ReLU 모델의 _gradients_ 를 계산할 수 있는 경우 거의 비트당 비트 등가 모델을 훔칠 수 있음을 보여주었다. Jagielski et al.(2020)은 공격자가 모델 출력에 대한 쿼리 액세스만 있는 경우 유한한 차이로 기울기에 근사할 수 있음을 관찰했다. 후속 작업은 이러한 공격을 확장하여 더 깊은 ReLU 모델을 효율적으로 추출했다(Carlini et al., 2020; Rolnick and Kording, 2020; Shamir et al., 2023). 불행히도, 이러한 접근법들 중 어느 것도 프로덕션 언어 모델들에 스케일링되지 않는데, 그 이유는 (1) 토큰들을 입력들로서 받아들이고(그래서 유한한 차이들을 수행하는 것은 난치성임), (2) ReLU들 이외의 액티베이션들을 사용하고; (3) 어텐션, 레이어 정규화, 잔차 연결들 등과 같은 아키텍처 컴포넌트들을 포함한다. 현재 공격은 처리할 수 없습니다. (4) 이전에 추출된 모델보다 큰 크기 순서이며, (5) 제한된 정밀도 출력만 노출됩니다.

다른 공격들은 더 제한된 정보를 복구하는 것을 목표로 한다. Wei et al. (2020)은 LLM과 동일한 서버에 동시에 위치한 적이 모든 은닉 계층의 크기를 복구할 수 있음을 보여준다. 다른 사람들은 발표된 벤치마크에 대한 성능을 학술 논문(가오, 2021)의 모델 크기와 연관시켜 모델 크기를 복구하려고 시도했다.

## 3 문제 형식

우리는 어휘 \(\mathcal{X}\)에서 추출한 토큰의 시퀀스를 입력으로 하는 모델을 연구한다. \(\mathcal{P}\left(\mathcal{X}\right)\)는 \(\mathcal{X}\)에 대한 확률 분포의 공간을 나타낸다. 본 연구에서는 \(N\) 토큰의 입력 시퀀스가 주어졌을 때 다음 출력 토큰에 대한 확률 분포를 생성하는 매개변수화된 모델 \(f_{\theta}:\mathcal{X}^{N}\rightarrow\mathcal{P}\left(\mathcal{X}\right)\)을 연구한다. 모델은 다음과 같은 구조를 갖는다:

\[f_{\theta}(p)=\mathsf{softmax}\big{(}\mathbf{W}\cdot g_{\theta}(p)\big{)}, \tag{1}\]

여기서, \(g_{\theta}:\mathcal{X}^{N}\rightarrow\mathbf{R}^{h}\)는 숨겨진 상태를 계산하는 또 다른 매개변수화된 모델이고, \(\mathbf{W}\)는 \(l\times h\) 차원 행렬( _embedding projection matrix_)이고, \(\mathsf{softmax}:\mathbf{R}^{l}\rightarrow[0,1]^{l}\)은 결과 _logits_에 적용되는 소프트맥스 함수이다:

\[\mathsf{softmax}(\mathbf{z})=\left[\frac{e^{\mathbf{z}_{1}}}{\sum_{i=1}^{l} e^{\mathbf{z}_{i}}},\dots,\frac{e^{\mathbf{z}_{l}}}{\sum_{i=1}^{l}e^{\mathbf{z}_{i}}} \right]\enspace.\

숨겨진 차원 크기는 토큰 사전의 크기, 즉 \(h\ll l\)보다 훨씬 작습니다. 예를 들어, LLaMA(Touvron et al., 2023)는 \(h\in\{4096,5120,6656,8192\}\) 및 \(l=32{,}000\)을 선택하고, 점점 더 큰 토큰 크기를 향한 최근 경향이 있다; GPT-4는 예를 들어 \(\approx\)100,000 토큰 어휘를 갖는다.

**위협 모델.** 논문 전체에서 적대자가 모델 매개 변수에 대한 추가 지식이 없다고 가정합니다. 서비스 제공자가 호스트하고 API(query interface) \(\mathcal{O}\)를 통해 사용자에게 제공되는 모델 \(f_{\theta}\)에 대한 액세스를 가정한다. 우리는 \(\mathcal{O}\)가 완벽한 오라클이라고 가정한다: 입력 시퀀스가 주어지면 \(p\), \(y=\mathcal{O}\left(p\right)\)를 생성하며, \((p,y)\로부터 추론할 수 있는 것보다 \(f_{\theta}\)에 대한 다른 정보를 누출하지 않는다. 예를 들어, 적수는 타이밍 부채널들 또는 질의 인터페이스의 구현의 다른 세부사항들을 통해 \(f_{\theta}\)에 관한 어떤 것도 추론할 수 없다.

다양한 오픈 소스 및 독점 LLM은 다양한 기능을 가진 API를 제공하며, 이는 모델 추출 공격 수행 능력과 공격 알고리즘 선택에 영향을 미친다. 우리가 연구하는 다양한 API에 대한 요약과 이에 대한 동기는 표 1에 나와 있다. 로짓 API는 API가 주어진 프롬프트에 대한 응답에서 모든 토큰에 대한 로짓을 제공하는 빨갱이 위협 모델이다. 여기에서 개발하는 공격 기술은 다음 섹션에서 재사용할 수 있으므로 이 장난감 설정으로 시작합니다. 여기서 먼저 더 제한된 정보(예: 상위 몇 개의 토큰에 대해서만 로그 확률)에서 로짓을 재구성한 다음 공격을 실행합니다.

## 4 Logit-Vector API에 대한 추출 공격

이 섹션에서는 적대자가 어휘의 모든 토큰에 대해 소프트맥스 함수로 공급되는 로짓들을 직접 볼 수 있다고 가정한다(나중에 이 가정을 완화할 것이다), 즉,

\[\mathcal{O}\left(p\right)\leftarrow\mathbf{W}\cdot g_{\theta}\left(p\right)\enspace.\] 우리는 변압기의 작은 부분을 고충실도로 추출할 수 있는 새로운 공격 기법을 개발한다. 섹션 4.1은 로짓 API를 사용하여 숨겨진 차원 \(h\)을 식별하는 방법을 보여주고 섹션 4.2는 매트릭스 \(\mathbf{W}\)를 복구할 수 있는 알고리즘을 제시한다.

### Warm-up: 숨겨진 차원 복구

우리는 오라클 \(\mathcal{O}\) (알고리즘 1)에 질의를 함으로써 공격자가 언어 모델의 숨겨진 차원의 크기를 복구할 수 있도록 하는 간단한 공격으로 시작한다. 이 공격을 수행하는 데 사용하는 기술은 최종 임베딩 투영 행렬의 완전한 추출을 수행하기 위해 더 발전하는 공격의 기초가 될 것이다.

```
0 : Oracle LLM \(\mathcal{O}\)
1: \(n\)을 \(h\)보다 큰 적정값으로 초기화
2: 빈 행렬 초기화 \(\mathbf{Q}=\mathbf{0}^{n\times l}\)
3:for\(i=1\) to \(n\)do
4:\(p_{i}\leftarrow\textsc{RandPrefix}()\)\(\triangleright\) 랜덤 프롬프트 선택
5:\(\mathbf{Q}_{i}\leftarrow\mathcal{O}(p_{i})\)
6:endfor
7:\(\lambda_{1}\geq\lambda_{2}\geq\cdots\geq\lambda_{n}\leftarrow\textsc{ SingularValues}(\mathbf{Q})\)
8: count \(\leftarrow\arg\max_{i}\log\lVert\lambda_{i}\rVert-\log\lVert\lambda_{i+1}\rVert\)
9:return count
```

**알고리즘 1** 은닉 차원 추출 공격

**직관.** 많은 수의 다른 랜덤 접두사에 대해 언어 모델을 쿼리한다고 가정합니다. 각 출력 로짓 벡터는 \(l\)-차원 벡터임에도 불구하고, 투영 계층이 \(h\)-차원으로부터 상향 투영되기 때문에 모두 실제로 \(h\)-차원 부분공간에 놓여 있다. 따라서, 모델 "충분"( \(h\)회 이상)을 질의함으로써 결국 새로운 질의가 과거 질의에 선형적으로 의존한다는 것을 관찰할 수 있을 것이다. 그런 다음 이 부분공간의 차원(예: SVD)을 계산하고 이를 모델의 숨겨진 차원으로 보고할 수 있다.

**형식화** 공격은 다음과 같은 간단한 수학적 결과를 기반으로 합니다.

**Lemma 4.1**.: _Let \(\mathbf{Q}\left(p_{1},\ldots p_{n}\right)\in\mathbf{R}^{l\times n}\)는 로짓 벡터 API에서 쿼리 응답의 열 \(\mathcal{O}\left(p_{1}\right),\ldots,\mathcal{O}\left(p_{n}\right)\가 있는 행렬을 나타냅니다. Then_

\[h\geq\text{rank}\left(\mathbf{Q}\left(p_{1},\ldots p_{n}\right)\right).\]

또한, 열이 있는 행렬이 \(g_{\theta}\left(p_{i}\right)\)(\(i=1,...,n\))에 순위 \(h\)가 있고 \(\mathbf{W}\)에 순위 \(h\)가 있는 경우_

\[h=\text{rank}\left(\mathbf{Q}\left(p_{1},\ldots p_{n}\right)\right).\]

증명: \(\mathbf{Q}=\mathbf{W}\cdot\mathbf{H}\)를 가지며, 여기서 \(\mathbf{H}\)는 \(g_{\theta}(p_{i})\)(\(i=1,\ldots,n\))인 \(h\times n\) 행렬이다. 따라서 \(h\geq\text{rank}\left(\mathbf{Q}\right)\). 또한, \(\mathbf{H}\)가 순위 \(h\)를 갖는 경우(두 번째 가정으로) \(h=\text{rank}\left(\mathbf{Q}\right)\).

**가정.** Lemma 4.1에서 열 \(g_{\theta}\left(p_{i}\right)\)이 있는 행렬과 행렬 \(\mathbf{W}\)이 모두 순위 \(h\)를 갖는다고 가정합니다. 이러한 행렬은 \(h\) 행 또는 \(h\) 열을 가지므로 둘 다 최대 \(h\) 순위를 갖는다. 또한 \(\mathbf{W}\)의 모든 \(h\ll l\) 열이 \(\mathbf{R}^{l}\)의 동일한 \((h-1)\)차원 부분공간(로짓의 출력공간)에 놓이기 위해서는 \(g_{\theta}\left(p\right)\)의 분포가 모든 \(p_{i}\)차원 부분공간에서 완전히 지원되어야 한다. 실제로 우리는 이 가정이 모든 더 큰 모델(표 2)과 다른 정규화 계층이 사용될 때(부록 B.1)에 성립한다는 것을 발견했다.

**실제 고려 사항** 행렬 \(\mathbf{Q}\)은 실수에서 계산되지 않고 부동 소수점 수(프로덕션 신경망의 경우 16비트 또는 8비트만큼 정밀도가 낮음)에서 계산되므로 순진하게 순위를 선형 독립 행의 수로 취할 수 없습니다. 대신에, 우리는 \(\mathbf{Q}\)의 실용적인 _numerical rank_를 사용하며, 여기서 특이값 \(\lambda_{1}\geq\lambda_{2}\geq\cdots\geq\lambda_{n}\)을 주문하고 연속된 특이값 사이의 가장 큰 _multiplicative_ gap \(\frac{\lambda_{1}}{i+1}\)을 식별한다. 큰 곱셈 격차는 우리가 큰 "실제" 특이값에서 수치의 부정확성에서 발생하는 작은 특이값으로 전환할 때 발생한다. 그림 2는 이러한 간격을 보여준다. 알고리즘 1은 이 공격을 설명합니다.

**실험.** 이 공격 뒤에 있는 직관을 시각화하기 위해 그림 1은 피티아-1.4b LLM에 대한 공격을 보여줍니다. 여기서는 질의의 수가 증가함에 따라 \(\mathbf{Q}\)의 특이값의 크기를 모형으로 표현한다. 2048개 미만의 쿼리를 보낼 때

\begin{table}
\begin{tabular}{l l} API & Motivation \\ \hline All Logits §4 & Pedagogy \& basis for next attacks \\ Top Logprobs, Logit-bias §5 & Current LLM APIs (e.g., OpenAI) \\ No logprobs, Logit-bias §6 & Potential future constrained APIs \\ \end{tabular}
\end{table}
표 1: API의 요약

그림 1: SVD는 최종 출력 레이어 차원이 은닉 차원보다 클 때 모델의 은닉 차원을 복구할 수 있다. 여기에서 우리는 피티아 1.4B 모델의 숨겨진 차원(2048)을 추출한다. 우리는 2048개 이상의 전체 로짓 벡터를 약간 구함으로써 크기를 정확하게 식별할 수 있다.

은닉 공간의 차원을 식별하는 것은 불가능하다. 이것은 \(n<h\)이고, 따라서 \(n\times l\) 차원 행렬 \(\mathbf{Q}\)은 완전한 랭크와 \(n\) 자명하지 않은 특이값을 갖기 때문이다. 그러나 일단 우리는 \(2048\) 이상의 쿼리를 모델에 만들고, 따라서 \(n) h\), 수치적으로 유의한 특이값의 수는 더 이상 증가하지 않고 정확히 \(2048\)에서 캡핑된다.

그림 2에서 후속 특이값 간의 차이(로그 공간)를 표시한다. 우리가 볼 수 있듯이, 가장 큰 차이는 (거의 정확히) 이 모델의 진정한 숨겨진 차원인 2048번째 특이값에서 발생한다.

이제 우리는 GPT-2(Radford et al., 2019) Small and XL, Pythia(Biderman et al., 2023) 1.4B and 6.9B, LLaMA(Touvron et al., 2023) 7B and 65B와 같은 광범위한 모델에 걸쳐 이 공격의 효능을 분석한다. 결과는 표 2에 나와 있다: 우리의 공격은 6가지 사례 중 5가지 사례에서 0 또는 1의 오류로 임베딩 크기를 거의 완벽하게 복구한다.

우리의 거의 완벽한 추출에는 한 가지 예외가 있다: GPT-2 Small. 이 768 차원 모델에서 공격은 757의 숨겨진 차원을 보고한다. 부록 A에서 이 "실패"는 \(768\) 차원을 가짐에도 불구하고 실제로 \(757\)의 효과적인 숨겨진 차원을 갖는 GPT-2에 의해 발생한다는 것을 보여준다.

### 전체 계층 추출(대칭까지)

최종 은닉 계층에서 출력 로짓으로 매핑되는 최종 출력 투영 행렬 \(\mathbf{W}\)을 복구하기 위해 이전 섹션에서 공격을 확장한다.

**방법:** \(\mathbf{Q}\)를 알고리즘 1에 정의된 대로 둡니다. 이제 \(\mathbf{Q}=\mathbf{U}\cdot\mathbf{\Sigma}\cdot\mathbf{V}^{\top}\)를 SVD로 다시 씁니다. 이전에 우리는 충분히 큰 특이값의 수가 모델의 차원에 해당한다는 것을 보았다. 그러나 행렬 \(\mathbf{U}\)는 실제로 마지막 층의 회전(회전)을 직접적으로 나타낸다는 것이 밝혀졌다! 구체적으로, \(\mathbf{U}\cdot\mathbf{\Sigma}=\mathbf{W}\cdot\mathbf{G}\)의 행렬 \(\mathbf{G}\)에 대해 다음과 같은 정리를 수행할 수 있다.

**Lemma 4.2**.: _logit-API 위협 모델에서 Lemma 4.1:_ (i) _위의 방법은 일부 \(\mathbf{G}\in\mathbb{R}^{h\times h}\)에 대해 \(\tilde{\mathbf{W}}=\mathbf{W}\cdot\mathbf{G}\)를 복구합니다. _ (ii) _ \(g_{\theta}(p)\)가 잔차 연결을 가진 변압기라는 추가 가정으로 \(\mathbf{W}\)를 정확하게 추출할 수 없습니다. _

증거: 부록 C를 참조하십시오.

실험에 앞서 고려된 6개의 모델에 대해, \(h\times h\) 아핀 변환을 허용한 후 추출된 행렬 \(\tilde{\mathbf{W}}=\mathbf{U}\cdot\mathbf{\Sigma}\)과 실제 가중치 행렬 간의 RMS(Root Mean Square)를 비교하여 공격 성공률을 평가한다. 구체적으로는 \(\tilde{\mathbf{W}}\cdot\mathbf{G}\approx\mathbf{W}\)에 대한 최소자승계 \(\tilde{\mathbf{W}}\cdot\mathbf{G}\approx\mathbf{W}\)를 풀어서, \(l\) 방정식과 \(h\) 미지수를 갖는 선형 최소자승 문제로 축소한다. 그런 다음, \(\mathbf{W}\)와 \(\tilde{\mathbf{W}}\cdot\mathbf{G}\)의 RMS를 보고한다.

결과는 표 2에 나와 있다. 기준점으로서, 랜덤하게 초기화된 모델과 실제 가중치 사이의 RMS는 \(2\cdot 10^{-2}\), \(100\)-\(500\times\) 이상으로 재구성 오류보다 높다.

부록 C 및 G에서 우리는 아핀 변환까지의 재구성에 대해 \(h^{2}\)와 대조적으로 _직교_ 변환(약 \(h^{2}/2\) 누락 매개변수에서 재구성이 가능하며 이것이 일부 공식 가정 하에서 엄격하다는 것을 보여준다. 그러나, 우리는 아핀 변환까지의 재구성을 위한 효율적인 알고리즘만을 가지고 있다.

## 5 Logit-Bias API에 대한 추출 공격

위의 공격은 적수가 각 입력에 대한 완전한 로짓 벡터를 직접 관찰할 수 있다는 상당한 가정을 만든다. 실제로 이것은 사실이 아닙니다. 우리가 알고 있는 어떤 프로덕션 모델도 그러한 API를 제공하지 않습니다. 대신 사용자가 로짓 토큰 로그 확률을 얻을 수 있는 방법을 제공 합니다. 이 섹션에서는 이 문제를 해결합니다.

\begin{table}
\begin{tabular}{l r r r} \hline \hline Model & Hidden Dim & Stolen Size & \(\mathbf{W}\) RMS \\ \hline GPT-2 Small (fp32) & \(768\) & \(757\pm 1\) & \(4\cdot 10^{-4}\) \\ GPT-2 XL (fp32) & \(1600\) & \(1599\pm 1\) & \(6\cdot 10^{-4}\) \\ Pythia-1.4 (fp16) & \(2048\) & \(2047\pm 1\) & \(3\cdot 10^{-5}\) \\ Pythia-6.9 (fp16) & \(4096\) & \(4096\pm 1\) & \(4\cdot 10^{-5}\) \\ LLaMA 7B (fp16) & \(4096\) & \(4096\pm 2\) & \(8\cdot 10^{-5}\) \\ LLaMA 65B (fp16) & \(8192\) & \(8192\pm 2\) & \(5\cdot 10^{-5}\) \\ \hline \hline \end{tabular}
\end{table}
표 2: 우리의 공격은 모델 크기를 훔치는 것과 출력 투영 행렬을 재구성하는 것 모두에서 오픈 소스 모델의 범위에 걸쳐 성공한다(불변까지; 우리는 루트 MSE를 보여준다).

그림 2: 우리의 추출 공격은 연속된 특이값 간의 차이에서 스파이크로 시각화된 특이값의 급격한 하락을 찾아 은닉 차원 크기를 복구한다. 2048차원 모델인 피티아-1.4B에서 스파이크는 2047개의 특이값에서 발생한다.

### API 설명

이 섹션에서는 상위 \(K\) 토큰에 대한 로그 확률을 반환하는 API에 대한 공격을 개발하고 사용자가 소프트맥스 이전에 지정된 토큰에 대한 로짓에 추가할 실수 값 바이어스 \(b\in\mathbb{R}^{|\mathcal{X}|}\)("로짓 바이어스")를 지정할 수 있습니다.

\[\mathcal{O}(p,b)\leftarrow\mathsf{TopK}\left(\mathsf{logsoftmax} \left(\mathbf{W}g_{\theta}(p)+b\right)\right)\] \[=\mathsf{TopK}\left(\mathbf{W}g_{\theta}(p)\!+\!b\!-\!\log\! \left(\!\sum_{i}\exp(\mathbf{W}g_{\theta}(p)\!+\!b)_{i}\!\right)\!\cdot\!\mathbf{1}\!\right)\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\

여기서 \(\mathsf{TopK}\left(\mathbf{z}\right)\)는 \(\mathbf{z}\in\mathbb{R}^{l}\)의 가장 높은 항목과 그 인덱스를 반환한다. 많은 API(본 논문 이전)는 최신 모델(OpenAI, 2024; Google, 2024)에 대해 이러한 옵션을 제공했다. 특히 OpenAI API는 최대 \(300\) 토큰에 대한 로짓 수정을 지원하며, 각 토큰에 대한 로짓 바이어스는 \([-100,100]\)(OpenAI, 2023) 범위로 제한된다.

남은 것은 이 API를 통해 별개의 프롬프트 쿼리에 대한 전체 로짓 벡터를 찾을 수 있음을 보여주는 것이다. 이 섹션에서는 이러한 목적을 위한 기술을 개발합니다. 여러 개의 완전한 로짓 벡터를 복구하면 섹션 4.2에서 공격을 수정 없이 실행할 수 있다.

### Evaluation Methodology

실제 공격은 추출 비용을 관리할 수 있도록 유지하고 API의 속도 제한기 또는 기타 필터를 우회하기 위해 모두 _효율적이어야 합니다. 따라서 공격의 효율성을 측정하는 데 사용하는 두 가지 비용 정의로 시작한다.

**토큰 비용:** 공격 중에 상대방이 모델에 보내거나 모델에서 받는 토큰 수입니다. 대부분의 API는 토큰당 사용자를 청구하므로 이 메트릭은 (토큰 비용으로 확장한 후) 공격의 금전적 비용을 나타냅니다.

**쿼리 비용:** 공격의 총 기간입니다. 대부분의 API는 주어진 간격에서 적수가 만들 수 있는 쿼리 수에 제한을 둡니다. 따라서 일부 공격은 더 빠를 수 있지만 비용이 더 많이 듭니다(쿼리당 더 많은 토큰을 전송함으로써).

이 섹션의 나머지 부분에서는 다양한 공격 가정 하에서 여러 공격을 개발하고 _토큰 비용_, _쿼리 비용_ 또는 둘 다에 대해 최적화합니다.

### Top-5 로짓 바이어스 API에 대한 추출 공격

본 논문에서는 다양한 로짓 편향을 갖는 질의 시퀀스를 통해 임의의 접두사 \(p\)에 대한 로짓 벡터를 계산하는 기법을 제안한다.

시작하려면 **API가 상위 \(K\) 로짓값을 반환했다고 가정합니다* *. 그런 다음 로짓 편향에 대해 서로 다른 선택을 순환하고 매번 상위 \(k\) 로짓들을 측정함으로써 임의의 프롬프트 \(p\)에 대한 완전한 로짓 벡터를 복구할 수 있다. 특히 상위 5개 로짓이 있는 API의 경우 쿼리 시퀀스를 보낼 수 있습니다.

\[\mathcal{O}(p,b_{k}\!=\!b_{k+1}\!=\!b_{k+1}\!=\!\ldots\!=\!b_{k+4}\!=\!B)\text{, for }k\in\! \{\!0,\!5,\!10,\ldots\!,|\mathcal{X}|\!\}\!\}\!

with a large enough \(B\). 따라서 각 쿼리는 5개의 서로 다른 토큰 \(\{k,k+1,\ldots,k+4\}\)을 상위 5개로 촉진하여 로짓을 관찰할 수 있습니다. 바이어스 \(B\)를 빼고 이 모든 질의에서 답을 병합함으로써 전체 로짓 벡터를 복구한다.

불행히도 로짓 대신 수익 _logprobs_(모델의 소프트맥스 출력 로그)를 알고 있는 모든 프로덕션 API이기 때문에 이 공격을 직접 사용할 수 없습니다. 이제 문제는 로짓 바이어스 \(B\)를 \(i\)번째 토큰에 적용하고 토큰의 로그프로브를 관찰하면 값을 얻는다는 것이다.

\[y_{i}^{B}=z_{i}+B-\log\big{(}\sum_{j\neq i}\exp(z_{j})+\exp(z_{i}+B)\big{)}\]

따라서 우리는 처리해야 할 추가 편향 의존적 용어를 얻는다. 본 논문에서는 두 가지 접근 방법을 제안한다.

우리의 첫 번째 접근법은 모든 로짓 간의 상대적 차이를 배울 수 있는 공통 "참조" 토큰에 의존한다(이것은 소프트맥스가 로짓으로의 추가 이동 하에서 불변하기 때문에 우리가 기대할 수 있는 최선이다). 프롬프트에 대한 최상위 토큰이 \(R\)이고, 우리는 토큰의 로짓들 \(i\)과 \(R\) 사이의 상대적인 차이를 학습하고자 한다. 토큰 \(i\)에 큰 bias \(B\)를 추가하여 top-5로 밀어 넣은 후, 토큰 \(i\)와 \(R\)의 logprobs를 관찰한다. 우린...

\[y_{R}^{B}-y_{i}^{B}-B=z_{R}+z_{i}\,.\]

우리는 5개의 로그프로브를 관찰할 수 있기 때문에 4개의 토큰을 모두 상위 5(참조 토큰과 함께)에 푸시하는 큰 편향을 추가하여 참조 토큰 \(R\)을 쿼리당 4개의 토큰과 비교할 수 있다. 따라서 일련의 쿼리를 발행합니다.

\[\mathcal{O}(p,b_{i}=b_{i+1}=b_{i+2}=b_{i+3}=B)\]

for \(i\in\{0,4,8,\cdots,|\mathcal{X}|\}\). 이것은 \(0\)로 설정한 자유 매개변수 \(z_{R}\)까지의 로짓들을 복구한다.

**쿼리 비용** 이 공격은 로짓당 \(1/4\) 쿼리 비용에 대해 모델에 대한 각 쿼리(다섯 번째는 참조점으로 사용됨)를 사용하여 4개의 로짓 값을 표시합니다.

부록 E에서 우리는 우리가 받은 각 로그프로브를 원래 로짓에 대한 선형 제약으로 보고 쿼리당 5개의 로짓, 즉 \(1/5\)의 비용을 복구할 수 있는 두 번째 더 정교한 방법을 제시한다.

**토큰 비용** 공격에는 여러 개의 개별 프롬프트에 대한 로짓이 학습되어야 하므로 각 프롬프트는 최소한 하나의 토큰 길이여야 합니다. 따라서 이 공격은 쿼리당 2개 이상의 토큰(입력 1개와 출력 1개) 또는 출력의 각 토큰에 대해 \(1/2\)의 비용이 듭니다. 그러나 실제로 많은 모델(gpt-3.5-turbo와 같은 모델)에는 모든 쿼리와 함께 몇 개의 오버헤드 토큰이 포함됩니다. 이는 로짓당 토큰 비용을 \(\frac{2+\Delta}{4}\)로 증가시키며, 여기서 \(\Delta\)는 오버헤드 토큰의 수이고, gpt-3.5-turbo의 경우 \(\Delta=7\)를 보고한다.

개선된 비용 최적 공격.위의 공격을 일반화하여 쿼리 비용과 토큰 비용을 모두 개선할 수 있습니다. 단일 토큰에 대해 4 또는 5 로짓 값을 나타내는 모델에 쿼리를 발행하는 대신 다중 토큰 쿼리 \(\begin{bmatrix}p_{0}&p_{1}&p_{2}\dots p_{n}\end{bmatrix}\)를 보낸 다음 프롬프트 \(\begin{bmatrix}p_{0}\end{bmatrix}\), \(\begin{bmatrix}p_{0}&p_{1}\end{bmatrix}\), \(\begin{bmatrix}p_{0}&p_{1}&p_{2}\end{bmatrix}\)의 각 접두사에 대한 로그프로브 벡터를 요청할 수 있습니다. OpenAI의 API는 이전에 로그프로브 및 에코 매개 변수를 결합하여 생성된 토큰뿐만 아니라 _prompt_ 토큰에 대한 로그프로브를 제공하여 이 형식의 쿼리를 허용했으며 이후 이 옵션이 제거되었습니다.

이제 _생성된_ 토큰의 로그프로브만 볼 수 있습니다. 그리고 마지막 토큰만 생성되기 때문에 이 더 긴 단일 쿼리에 대해 4개의 로그프로브만 볼 수 있습니다. 그러나 이것은 질의와 토큰 비용을 줄일 수 있는 잠재적인 접근법을 제시한다. 만약 모델이 토큰의 특정 시퀀스를 방출할 수 있는 어떤 방법이 있다면 \(\begin{bmatrix}p_{n+1}&p_{n+2}&\dots&p_{n+m}\end{bmatrix}\) 각각의 생성된 토큰의 로그프로브 벡터를 검사할 수 있다.

이를 위해 토큰 \(x\)과 다른 네 개의 토큰을 수정하고, 모델을 강제로 \(\begin{bmatrix}x&x&\dots&x\end{bmatrix}\)을 방출한다. 5개의 토큰 각각에 대해 \(B\)의 로짓 바이어스를 공급하는 대신, 토큰 \(x\)에 대해 \(B\)의 로짓 바이어스를 공급하고, 나머지 4개의 토큰에 대해 \(B^{\prime}<B\)의 로짓 바이어스를 공급한다. 만약 \(B^{\prime}\)가 충분히 커서 다른 토큰들이 상위 5개의 출력으로 유입된다면, 우리는 여전히 그 토큰들에 대한 로짓들을 배울 수 있을 것이다. 이 모델은 \(B^{\prime}\)가 충분히 작기 때문에 토큰 \(x\)로 항상 초기 프롬프트 \(p_{0}\)를 완료할 수 있다. 그러면 우리는 \(\begin{bmatrix}p_{0}&x&x&dots&x\end{bmatrix}\) 형태의 여러 프롬프트에서 로짓들을 수집할 수 있을 것이다.

**분석.** 이 공격의 쿼리 비용은 \(\frac{1}{4m}\)이며, 여기서 \(m\)는 확장 팩터입니다. 또한 각 쿼리에 \(1+m\) 토큰이 필요하므로 토큰 비용은 \(\frac{1+m}{4m}\)입니다. (또는, API에 \(\Delta\) 토큰의 오버헤드가 있는 경우 \(1+m+\Delta\)입니다. \(m=1\), 즉 확장이 없는 경우 이 공격은 첫 번째 공격으로 줄어듭니다. 분석도 마찬가지로 쿼리 비용 \(\nicefrac{1}}{{4}}}\)과 토큰 비용 \(\nicefrac{1}}{{2}}}\를 제공합니다.

### 상위 1 이진 로짓 바이어스 Api에 대 한 추출 공격

우리의 공격에 비추어 모델 제공자가 위의 API에 제한을 도입하는 것을 생각할 수 있다. 이제 API가 섹션 5.1의 API에서 최상위 로그프로브(\(K=1\)만 반환하고 로짓 편향이 두 값 중 하나만 취하도록 제한되더라도 공격이 가능함을 보여준다.

**API.** 로짓 바이어스 API에 대한 추가 제한 사항(섹션 5.1)에 다음 두 가지를 둡니다. 첫째, \(K=1\)을 설정하고 가장 가능성이 높은 토큰의 로그프로브만 봅니다. 둘째, 각 로짓 바이어스 항목 \(b\)은 \(\{-1,0\}\)로 제한됩니다. 이러한 제약 조건은 이전 섹션의 공격을 완전히 방지합니다. 우리는 이 제약이 어떤 실제적인 구현이 정의하는 것보다 훨씬 더 빡빡하다고 믿는다.

**메서드** 처음에는 토큰이 이미 가장 가능성이 높은 토큰이 아닌 경우 토큰 \(t\)에 대한 정보를 배울 수 없는 것처럼 보일 수 있습니다. 그러나 모델을 두 번, 로짓 치우침이 없는 한 번, 토큰 \(t\)에 대해 로짓 치우침이 \(-1\)인 한 번 쿼리하면 최상위 토큰은 토큰 \(t\)의 로그프로브의 _값_ 에 따라 정확히 얼마나 작은 값으로 \(-1\)의 치우침이 있을 가능성이 약간 더 높습니다. 구체적으로 부록 D에서 로그프로브는 \((\nicefrac{{1}}{{e}}-1)^{-1}(\exp(y_{\text{top}}-y^{\prime}_{\text{top}})-1)\)와 같으며, 여기서 \(y_{\text{top}}\) 및 \(y^{\prime}_{\text{top}}\)는 \(0\) 및 \(-1\)의 로짓 편향으로 쿼리할 때 가장 가능성이 높은 토큰의 로그프로브임을 보여준다.

**분석.** 이 공격에는 추출된 로그프로브당 \(1\) 쿼리 및 토큰이 필요합니다. 그러나, 우리가 평가에서 보여주듯이, 이 공격은 이전에 논의된 공격들보다 훨씬 덜 수치적으로 안정적이며, 따라서 동일한 수준의 정확도에 도달하기 위해 더 많은 질의가 필요할 수 있다.

## 6 Logprob-free API에서 추출

보다 보수적인 API 공급자는 로짓 편향과 로그프로브의 조합에 대한 액세스를 완전히 제거할 수 있다. 실제로 오픈AI에 우리의 공격을 공개한 후, 그들은 로짓 편향이 상위 로그프로브에 영향을 미치는 능력을 제거하여 이전 섹션에서 공격을 방지했다. 이러한 상황을 활용하기 위해 비용 증가에도 불구하고 로짓 바이어스 벡터에 대해 이진 검색을 수행하여 완전한 로짓 벡터를 복구하는 여러 로그프로브 없는 공격을 추가로 개발한다.

**Api:** 일부 API는 로짓 편향 용어에 대한 액세스를 제공하지만 로그프로브에 대한 정보는 제공하지 않습니다. 따라서, 우리는

\[\mathcal{O}(p,b)=\mathsf{ArgMax}\left(\mathsf{logsoftmax}\left(\mathbf{W} \cdot g_{\theta}(p)+b\right)\right).\]

여기서 \(\mathsf{ArgMax}\left(\mathbf{z}\right)\)는 벡터에서 가장 높은 좌표의 인덱스를 반환한다 \(\mathbf{z}\in\mathbb{R}^{I}\). 이 절에서는 \(b=\{i:z\}\) 표기를 사용하여 바이어스가 토큰 \(i\)에 대해 \(z\)로 설정되고 다른 모든 토큰에 대해 \(0\)로 설정됨을 나타낼 것이다. 우리는 또한 로짓 바이어스가 사용되지 않는다는 것을 나타내기 위해 \(b=\{\}\)를 사용한다. 마지막으로, 바이어스가 \([-B,B]\) 범위에 속하도록 제한된다고 가정한다.

**추출할 수 있는 것은 무엇입니까?* * 이 섹션에서 개발된 공격은 \(\varepsilon\)의 추가(\(\infty\)-norm) 오류까지 로짓 벡터를 재구성합니다.

### Warm-up: Basic Logprob-free 공격

**메서드** 로그프로브가 없는 공격에 대한 한 가지 간단한 통찰력을 얻습니다. 온도 0을 사용하여 샘플링하면 로짓 값이 가장 큰 토큰이 생성됩니다. 따라서 각 토큰에 대한 로짓 바이어스를 적절하게 조정함으로써 이진 검색을 통해 모든 토큰의 로짓 값을 복구할 수 있다. 형식적으로 \(p\)를 프롬프트로 하고, 인덱스 \(0\)가 있는 토큰이 \(\mathcal{O}(p,b=\{\})\로 주어진 \(p\)에 대한 응답에서 가장 가능성이 높은 토큰이 되도록 토큰을 다시 표시합니다. 각 토큰 \(i\neq 0\)에 대해 최소값 \(x_{i}\geq 0\)을 찾기 위해 로짓 바이어스 항을 통해 이진 검색을 실행하여 모델이 확률 1로 토큰 \(i\)을 방출한다. 이는 모든 로짓(이전의 모든 공격과 마찬가지로 소프트맥스로 인해 하나의 자유 변수를 잃음)을 복구한다.

```
\(\mathcal{O}\left(p,b=\{i:-\frac{\alpha_{i}+\beta_{i}}{2}\}\right)=0\)then \(\beta_{i}\leftarrow\frac{\alpha_{i}+\beta_{i}}{2}\) else \(\alpha_{i}\leftarrow\frac{\alpha_{i}+\beta_{i}}{2}\)endif  Return \(\frac{\alpha_{i}+\beta_{i}}{2}\)endif \(\frac{\alpha_{i}+\beta_{i}}{2}\)endif \(\frac{\alpha_{i}+\beta_{i}}{2}\)endif\(\frac{\alpha_{i}+\beta_{i}}{2}\)endif\(\frac{\alpha_{i}+\beta_{i}}{2}\)endif\(\frac{\alpha_{i}+\beta_{i}}{2}\
```

**알고리즘 2** 로짓 차이 학습

분석.이 공격은 비효율적이지만 로짓 벡터를 올바르게 추출합니다.

**Lemma 6.1**.: _모든 토큰 \(i\)에 대해 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\geq-B\) 알고리즘 2는 최대 \(\log\left(\frac{B}{\varepsilon}\right)\) API 쿼리에서 최대 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\)에서 떨어져 있는 값을 출력합니다._

증명: API는 추가된 로짓 바이어스가 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\)보다 작은 경우 (재순서화된) 토큰을 반환합니다. 이 가정으로 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\in[-B,0]\)을 알 수 있다. 이 알고리즘은 귀납적 논증에 의해 쉽게 알 수 있듯이 각 반복에서 \(\beta_{i}\geq\mathsf{logit}_{i}-\mathsf{logit}_{0}\geq\alpha_{i}\)를 보장한다. 또한 \(\beta_{i}-\alpha_{i}\)는 각 반복에서 \(2\)만큼 감소하므로 종료 시 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\)의 참값이 \(\varepsilon\)의 간격으로 끼워져 있음을 알 수 있다. 또한, 반복 횟수는 최대 \(\log_{2}\left(\frac{B}{\varepsilon}\right)\)이므로 이 알고리즘의 질의 비용도 마찬가지이다.

접근법의 한계점 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}<-2B\)의 경우 토큰을 샘플링하는 효율적인 방법이 없으므로 \(\log\!rob\) 접근 없이 \(\mathsf{logit}_{i}\)에 대한 정보를 찾을 수 없다. \(-2B\leq\mathsf{logit}_{i}-\mathsf{logit}_{0}\leq-B\)에 대해 가장 큰 로짓 값을 가진 토큰에 음의 로짓 편향을 추가하여 범위를 약간 늘리는 방법이 있지만 대부분의 모델에 대해 사용하려는 프롬프트에 대해 모든 토큰이 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}}_{0})을 만족하므로 자세한 내용은 생략합니다. -B\).

관련 작업.동시 작업(Morris et al., 2023)은 이러한 로짓 추출 방법에 대해 논의했다.

### 개선된 Logprob-free 공격: Hyperrectangle Relaxation Center

여러 토큰의 로짓 편향을 한 번에 수정하여 이전 공격을 개선할 수 있다.

API: 로짓 바이어스 사전에서 \(\mathcal{O}\)가 최대 \(N+1\) 토큰을 수락하는 추가 제약 조건과 함께 이전 섹션과 동일한 API를 사용합니다. 우리는 다시 질의 \(\mathcal{O}(p,b=\{\})\)를 실행하여 가장 가능성이 높은 토큰을 식별하고 인덱스를 \(0\)로 설정한다. 우리의 목표는 \(N\)개의 서로 다른 토큰에 대해 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\)을 근사화하는 것이다. 만약 \(N<l-1\)이면, 우리는 \(N\) 토큰의 다른 배치에 대해 동일한 알고리즘을 \(\frac{l-1}{N}\)번 반복한다.

```
\(T\) 라운드에 대해 \(b_{i}\leftarrow-\frac{\alpha_{i}\leq B\quad\forall i=1,\ldots,N\) \(\mathcal{C}=\{\mathsf{logit}_{i}\leq B\quad\forall i=1,\ldots,N\) \(\mathcal{C}=\{\mathsf{o}(p,b=\0:b_{0},1:b_{1},\ldots,N:b_{N}\)\) 라운드에 대해 \(b_{i}\leftarrow-\frac{\alpha_{i}+\beta_{i}}{2}\) \(k\leftarrow\mathcal{O}(p,b=\0:b_{0},1:b_{1},\ldots,N:b_{N}\)\) 라운드에 대해 \(k\leftarrow\mathcal{O}(p,b=\0:b_{0},1:b_{1},\ldots,
```

**알고리즘 3** 다중 토큰 호출을 사용하여 로짓 차이 학습

메서드.우리의 접근 방식은 여러 토큰에 대해 설정된 로짓 바이어스를 사용하여 API를 병렬로 쿼리합니다. 알고리즘은 _라운드_로 진행되며, 여기서 각 라운드는 여러 토큰에 대해 설정된 로짓 바이어스로 API를 쿼리하는 것을 포함합니다.

로짓 바이어스가 \(i=1,\ldots,l\)에 대해 \(\{i:b_{i}\}\)로 설정되고 프롬프트가 \(p\)일 때 쿼리가 토큰 \(k\)를 출력으로 반환한다고 가정합니다. 그리고 API의 정의에 의해 모든 \(j\neq k\)에 대해 \(\mathsf{logit}_{k}+b_{k}\geq\mathsf{logit}_{j}+b_{j}\)임을 알 수 있다.

이로 인해 로짓에 선형 제약 시스템이 부과됩니다. 모델을 여러 번 질의하고, 그러한 많은 연립방정식을 누적함으로써, 우리는 로짓 값을 더 효율적으로 복구할 수 있다. 이를 위해 집합 \(\mathcal{C}\)에 이러한 모든 선형 제약 조건을 누적하고, 각 라운드가 끝날 때 제약 조건 집합 \(\mathcal{C}\)에 대해 이 값을 최대화/최소화하는 선형 프로그램을 풀어서 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\)에 대해 더 작고 가장 큰 값을 계산한다. 따라서, 각 라운드에서 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\)를 둘러싸는 구간을 유지하고, 해당 라운드에서 이루어진 질의로부터 추가 정보가 주어지면 각 라운드에서의 구간을 정제할 수 있다. 공격에 대한 총 쿼리 예산을 기준으로 \(T\) 라운드를 선택한 후 각 로짓에서 가장 긴밀하게 알려진 경계를 반환합니다.

**Lemma 6.2**.: _모든 \(i=1,\ldots,l\)에 대해 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\in[-B,0]\)를 가정합니다. 그런 다음 알고리즘 3은 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\in[\alpha_{i},\beta_{i}]\)을 각 \(i\)에 대해 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\in[-B,0]\)이 되도록 간격 \([\alpha_{i},\beta_{i}]\를 반환한다. 또한, 알고리즘의 각 라운드는 계산 시간 \(O(N^{3})\)(API 호출에 필요한 계산 제외)로 구현될 수 있다._

증명: 알고리즘 3은 각 라운드에서 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\in[\alpha_{i},\beta_{i}]\)의 불변성을 유지한다. 우리는 귀납을 통해 이것이 참이고 로짓의 참 벡터가 항상 \(\mathcal{C}\)에 있다는 것을 증명할 것이다. 렘마에 명시된 가정에 따르면, 이것은 첫 번째 라운드의 시작 부분에서 분명히 사실이다. 이것이 \(K<T\) 라운드 후에 참이라고 가정하자. 그런 다음, \(K+1\) 라운드에서 추가된 제약 조건은 모두 실제 로짓 벡터에 대한 유효한 제약 조건입니다. API 반환 토큰 \(k\)은 모든 \(j\neq k\에 대해 \(\mathsf{logit}_{k}+b_{k}\geq\mathsf{logit}_{j}+b_{j}\)를 보장하기 때문입니다. 따라서 유도에 의해 알고리즘은 항상 \(\mathsf{logit}_{i}-\mathsf{logit}_{0}\in[\alpha_{i},\beta_{i}]\)를 보장한다.

6.2.1절에서는 모든 \(i\)에 대해 \(\alpha_{i},\beta_{i}\)를 계산하는 LP가 에지 가중치 \(c_{jk}=\min_{\text{rounds}}b_{j}-b_{k}\)를 갖는 그래프에서 모든 쌍 최단 경로 문제로 볼 수 있음을 보여준다. 이를 통해 로짓 차이 구간을 \(O(N^{3})\)로 유지하는 계산 복잡도를 보장한다.

#### 6.2.1 Shortest-path Formulation of the Logprob-free Attack LP

폴리토프 \(\mathcal{C}\)의 초직사각형 이완의 계산 효율을 향상시킬 수 있다. 여기에서 우리는 이 문제를 가중치 그래프에서 최단 경로 문제로 공식화하는 방법을 보여준다. 이를 통해 각 질의 후에 모든 \(i\in\{1,\ldots,N\}\)에 대한 정확한 \([\alpha_{i},\beta_{i}]\)을 빠르게 계산할 수 있다.

**Lemma 6.3**.: _Let \(G=(\{0,1,\ldots,N\},E)\)는 음의 주기 없이 가중된 방향 그래프입니다. Let \(\mathcal{P}\subset\mathds{R}^{n+1}\) is the solution set of the system of linear inequality:_

\[\mathsf{logit}_{i}-\mathsf{logit}_{j}\leq c_{ji}\quad\forall\;j \stackrel{{ c_{ji}}}{{\longrightarrow}}i\quad\in E\]

_Then if \(\mathsf{logit}_{0}=0\) we have_

\[\max_{x\in\mathcal{C}}\mathsf{logit}_{i}=\text{distance in $G$ from $0$ to $i$}.\]

증명: \(G\)에서 \(0\)에서 \(i\)까지의 최소 거리 경로의 모서리를 \(e_{0j_{1}},e_{j_{1}j_{2}},\ldots,e_{j_{m-1}i}\라고 하자. 우리는

\[\mathsf{logit}_{i} \leq\mathsf{logit}_{j_{m-1}}+c_{j_{m-1}i}\leq\ldots\] \[\leq\mathsf{logit}_{0}+\sum_{t=1}^{m-1}c_{j_{t+1}j_{t}}=\sum_{t=1}^{m-1}c_{j_{t+1}j_{t}},\

따라서 최단경로는 \(\mathsf{logit}_{i}\)의 상한이다. 이를 증명하기 위해 \(\mathsf{logit}_{i}\)를 \(0\)에서 \(i\)까지의 거리로 설정하는 것이 모든 부등식을 만족한다고 주장한다. 부등식 \(\mathsf{logit}_{i}-\mathsf{logit}_{j}\leq c_{ji}\)이 위반되었다고 가정합니다. 그리고 \(\mathsf{logit}_{j}+c_{ji}<\mathsf{logit}_{i}\)의 총 무게로 \(0\에서 j\에서 i\로 갈 수 있는데, 이는 \(\mathsf{logit}_{i}\)가 \(0\)에서 \(i\)까지의 거리라는 가정과 모순된다.

이를 우리의 설정에 적용하기 위해, (1) 모든 제약 조건, 심지어 초기 \(\alpha_{i}\leq\mathsf{logit}_{i}\leq\beta_{i}\)이 필요한 형태이고, (2) 그래프는 실제 로짓이 실현 가능한 해를 제공하기 때문에 음의 사이클을 갖지 않는다. (3) \(-\mathsf{logit}_{i}\)의 부등식에 의해 유도된 그래프에 동일한 절차를 적용하여 하한값을 구할 수 있다.

우리는 \(O(N^{3})\) 시간에 Bellman-Ford 알고리즘을 사용하여 \(0\)에서 다른 모든 정점까지의 거리를 구할 수 있다. 만약 \(N=300\)이면, 이것은 기껏해야 \(\mathcal{O}\)의 레이턴시와 비슷하다. 각 단계에서 그래프의 \(N\) 에지만 갱신하기 때문에 Bellman-Ford의 몇 가지 점진적 반복을 수행하는 휴리스틱은 실제로 \([\alpha_{i},\beta_{i}]\)을 높은 정밀도로 얻는다. API 쿼리 수와 토큰 비용은 물론 동일하게 유지됩니다.

### 개선된 Logprob-free 공격: Hyperrectangles에 대한 쿼리 개선

이전 접근법의 주요 문제는 로짓 벡터에 대한 이전이 \([-B,0]\)에 걸쳐 균일한 경우에도 일부 토큰이 다른 토큰보다 더 자주 샘플링된다는 것이다. "초직사각형의 중심화" 로짓 바이어스가 초직사각형을 argmax 좌표에 의해 라벨링된 동등한 크기의 부분들로 분할하지 않기 때문이다. 예를 들어, \(\beta_{i}-\alpha_{i}\ll\beta_{j}-\alpha_{j}\)의 경우, \([\alpha_{i},\beta_{i}]\times[\alpha_{j},\beta_{j}]\)보다 \(j\)가 출력 토큰일 가능성이 훨씬 높다. 따라서 알고리즘 3에서는 다른 로짓의 제약 조건인 \(\mathsf{logit}_{i}\)을 거의 얻지 못하며, 이는 \(\mathcal{C}\)의 이완을 약화시킨다.

우리의 해결책은 출력 토큰 분포가 균일성에 더 가깝도록 토큰을 편향시키는 것이다. 특히, \(\beta_{t}-\alpha_{t}\)( \(0\) 토큰)이 초직사각형보다 먼저 균일성이 주어졌을 때 정확히 \(1/(N+1)\)의 확률을 갖도록 토큰을 편향시키는 것이다. 이를 만족하는 하나의 로짓 바이어스는 다음과 같다:

\[b_{i} =-(1-c)\alpha_{i}-c\beta_{i}\quad\forall i=0,\ldots,N\] \[\text{where}\quad c =\exp(-\log(N+1)/N) \tag{2}\]

우리는 이제 알고리즘 3을 하나의 간단한 수정으로 실행한다: \(b_{i}=-\frac{\alpha+\beta}{2}\)를 \(b=-(1-c)\alpha-c\beta\)로 대체한다. 표 3에서 볼 수 있듯이 수정된 알고리즘은 6.2에서 방법보다 훨씬 우수하다.

모든 출력 토큰의 균형 잡힌 샘플링의 목표는 여러 가지 방법으로 접근할 수 있다. 예를 들어, 위의 식에서 \(c\)를 조정할 수 있습니다. \(\mathcal{O}\)가 이전에 반환되지 않은 바이어스 토큰이 더 가능성이 높거나 \(\mathcal{C}\)(또는 일부 이완)을 동일한 부분으로 분리하는 정확한 로짓 바이어스를 해결할 수 있습니다. 표 3의 이 방법의 쿼리/로짓 메트릭은 놀랍게도 최적에 가깝다.

## 7 Evaluation

우리는 이제 실제 도용 공격의 효과를 연구합니다.

### Logit Validation

이전 섹션에서 개발된 공격들이 제한된 질의 인터페이스에서 전체 로짓 벡터를 효과적으로 복구할 수 있다는 것을 검증하는 것으로 시작한다. 표 3에서 우리는 진정한 로짓 벡터와 복구된 로짓 벡터 사이의 평균 일치 비트 수와 하나의 완전한 로짓 벡터를 복구하는 데 필요한 (상각된) 쿼리 수를 보고한다.

일반적으로 더 강력한 위협 모델 하에서 작동하는 공격은 더 높은 정밀도를 갖는다. 그러나 이론적인 개선이 항상 실용적인 것은 아니다: 실제로 쿼리당 5개의 로그프로브를 학습하는 SSE로부터의 이론적으로 더 강한 공격은 더 많은 쿼리를 필요로 하고 낮은 충실도로 로짓들을 복구한다. 이 공격은 수치적으로 불안정하기 때문에 잠재적으로 잘못 조절된 매트릭스를 필요로 하며, 따라서 로짓 바이어스를 조정한 후 API를 재조회해야 할 수 있기 때문이다. 가장 강력한 로그프로브 없는 공격은 매우 효율적이며, 로짓당 \(3.7\) 쿼리에서 \(18\) 비트의 정밀도를 복구한다. 부록 F에서 우리는 이것이 최적으로부터 얼마나 멀리 떨어져 있는지를 이론적으로 분석하고 그것이 2의 인자 내에 있음을 발견한다.

### 프로덕션 모델 일부 도용

우리는 이제 2024년 1월 1일에 사용할 수 있는 OpenAI의 모델 중 ada, babbage, babbage-002, gpt-3.5-turbo-instruct, gpt-3.5-turbo-1106의 5가지 모델을 중심으로 프로덕션 언어 모델을 훔칠 수 있는 능력을 조사합니다. 우리는 이 모델들이 추출 공격을 시도할 수 있는 사전 허가를 받을 수 있는 유일한 프로덕션 모델이었기 때문에 이 모델들을 선택했습니다.

이전 섹션의 결과를 감안할 때, 우리는 개선된 4-로그프로브 공격(섹션 5.3)을 구현하기로 선택했는데, 이는 가장 질의 효율적인 공격이자 가장 정확한 공격이기 때문이다. 다른 공격 알고리즘으로 전환하면 총 실험 비용이 크게 증가하므로 이러한 절제 연구를 수행하지 않는다.

우리의 숨겨진 차원 도용과 전체 층 도용 공격 모두 이 다섯 모델 모두에게 효과가 있었습니다. 우리가 모델로부터 복구하는 크기는 OpenAI에 의해 확인된 바와 같이 원래 모델의 실제 크기와 완벽하게 일치한다. 처음 세 모델의 경우 (1) 이러한 모델의 크기가 이전에 확인된 적이 없지만 (2) 현재 사용되지 않으므로 크기를 공개하는 것이 해롭지 않기 때문에 표 4에 복구된 크기를 보고한다. OpenAI와의 논의에서 우리는 gpt-3.5-터보 모델의 크기에 대한 공개를 보류하기로 결정했지만, 그들로 우리의 공격이 보고한 숫자가 정확하다는 것을 확인했다.

전층스틸링 공격을 수행할 때, 추출된 가중치는 실제 가중치와 거의 동일함을 확인했으며, 오류 \(<7\cdot 10^{-4}\)는 이전에 논의된 바와 같이 \(h\times h\) 행렬 곱까지였다. 표 4는 \(h\times h\) 변환에 의해 둘을 "정렬"한 후 추출된 가중치 매트릭스와 실제 모델 가중치 사이의 RMS를 보고한다.

## 8 Defenses

기능의 상실에도 불구하고, 다수의 상이한 방식으로 이 공격을 방지하거나 완화하는 것이 가능할 것이다.

### Prevention

**로짓 편향 제거** API에서 로짓 편향 매개 변수를 완전히 제거 하는 것이 가장 간단한 방어일 수 있습니다. 불행히도 이 매개변수의 몇 가지 합법적인 사용 사례가 있다. 예를 들어, 몇몇 작업들은 제어되거나 제약된 생성을 수행하기 위해(Jiang et al., 2023; Yang and Klein, 2021), 세대 이동 및 모방 미세-조정을 위해(Liu et al., 2024; Mitchell et al., 2024), 또는 다른 이유들을 위해(Ren et al., 2023; Lee et al., 2022) 로짓 바이어스를 사용한다.

**로짓 편향을 블록 목록으로 바꿉니다.* * 로짓 편향을 제공하는 대신 모델 개발자는 모델을 내보내는 것이 금지된 토큰의 블록 목록으로 대체할 수 있습니다. 이는 이전 섹션에서 논의된 일부 기능을 지원하지만 여전히 공격을 방지할 수 있습니다.

**아키텍처 변경 사항** API를 수정하는 대신 모델을 변경할 수 있습니다. 우리의 공격은 은닉 차원 \(h\)이 출력 차원 \(l\)보다 작기 때문에만 작동합니다. 이것은 자연적인 건축 방어를 제안합니다. 최종 레이어를 두 개의 레이어로 분할합니다. 하나는 \(h\에서 t\로 가는 레이어이고, 다른 하나는 \(t\에서 l\로 가는 레이어입니다. 여기서 \(t>l\)와 그 사이에 비선형성이 배치됩니다. 마지막 선형 레이어가 크기 때문에(어휘 크기가 2차) 그다지 효율적이지 않습니다.

**아키텍처를 변경 합니다.* * 모델을 학습 한 후 최종 계층에 대 한 숨겨진 차원 \(h \)을 수정할 수도 있습니다. 특히, \(\mathbf{W}\)의 차원은 원래의 행렬과 직교하는 여분의 가중치 벡터들을 연결하여 확장할 수 있다. 우리는 에 대한 특이값을 설정한다

\begin{table}
\begin{tabular}{l c c c} \hline \hline Attack & Logprobs & Bits of precision & Queries per logit \\ \hline logprob-4 (§5.3) & top-5 & \(23.0\) & \(0.25\) \\ logprob-5 (§E) & top-5 & \(11.5\) & \(0.64\) \\ logprob-1 (§5.4) & top-1 & \(6.1\) & \(1.0\) \\ binary search (§6.1) & ✗ & \(7.2\) & \(10.0\) \\ hyperrectangle (§6.2) & ✗ & \(15.7\) & \(5.4\) \\ one-of-n (§6.3) & ✗ & \(18.0\) & \(3.7\) \\ \hline \hline \end{tabular}
\end{table}
표 3: 우리가 개발하는 각 로짓 추정 공격에 대한 로짓 벡터 복구의 평균 오류입니다. 우리의 가장 높은 정밀도와 가장 효율적인 공격은 로짓들을 거의 완벽하게 복구한다; 다른 공격들은 이 수준의 정밀도에 접근하지만 더 높은 질의 비용으로 접근한다.

이러한 가중치는 모델의 예측에 물질적으로 영향을 미치지 않을 만큼 작으면서도 사실적으로 보일 만큼 충분히 크다. 그런 다음, 모델의 전진 패스 동안 랜덤 가우시안 잡음의 벡터를 최종 은닉 벡터 \(g_{\theta}(p)\)에 연결한 후 \(\mathbf{W}\)를 곱한다. 그림 7은 GPT-2를 768차원 대신 1024차원처럼 작게 확장한 예를 보여준다. 이것은 상대방이 모델이 실제보다 넓다고 착각하게 만든다.

### Mitigations

로짓 바이어스 XOR logprobs. 우리의 공격은 적수가 로짓 바이어스를 제공하고 출력 로그프로브를 모두 볼 수 있을 때 \(10\times\) 더 저렴하다. 이것은 자연스러운 완화를 제안합니다. _로짓 편향과 로그프로브를 동시에 사용하는 API에 대한 쿼리를 금지합니다. 이러한 유형의 방어는 보안 및 기계 학습 커뮤니티 모두에서 일반적입니다. 예를 들어, 2023년에 OpenAI는 에코와 로그프로브를 모두 결합하는 기능을 제거했지만 둘 중 하나만 허용됩니다. 이 방어는 유사하게 행동합니다.

노이즈 추가.주어진 쿼리의 출력 로짓에 충분한 양의 노이즈를 추가함으로써, 우리의 공격을 방지할 수 있을 것이다. 그러나 로짓-노이즈는 모델을 덜 유용하게 만들 가능성이 있다. 우리는 부록 H에서 이 방향에 대한 몇 가지 예비 실험을 수행한다.

로짓 편향에 대한 속도 제한.우리의 공격은 각 프롬프트 \(p\)에 대해 최소한 \(h\) 로짓 값을 학습할 수 있어야 합니다. 한 가지 방어는 모델에 로짓 편향 쿼리를 허용하지만 모델이 숨겨진 차원 \(\tilde{h}\) 또는 더 작은 경우 상대방이 학습하는 것을 방지하기 위해 주어진 프롬프트에 대해 \(T=\tilde{h}/5\) 로짓 편향 쿼리만 허용하는 것이다.

불행히도 이것은 몇 가지 중요한 단점을 가지고 있다: 임계값은 \(h\)와 독립적이어야 한다(또는 임계값을 학습하면 \(h\)가 드러난다); 시스템은 API에 대한 모든 사용자 쿼리의 상태를 유지해야 한다; 그리고 Sybil 공격을 방지하려면 상당한 프라이버시 위험을 제시할 수 있는 사용자 쿼리의 전역 풀이 필요하다(Debenedetti et al., 2023).

악성 쿼리를 탐지합니다. 모델 가중치를 유출할 수 있는 쿼리를 방지 하는 대신 악성 행동의 패턴을 _탐지_ 하기 위해 표준 남용 방지 도구를 구현 하는 것이 대체 전략일 수 있습니다. 모델 도용(Juuti et al., 2019; Pal et al., 2021) 및 적대적 예(Chen et al., 2020)를 포함하는 사전 기계 학습 공격에 대한 이러한 형태의 여러 제안이 존재한다.

## 9 Future Work

우리는 전체 프로덕션 변압기 모델을 비트당 도용할 수 있을 것으로 기대하기 때문이 아니라 모델 도용 공격이 학문적 관심사일 뿐만 아니라 오늘날 배포되는 가장 큰 프로덕션 모델에 실질적으로 적용될 수 있음을 확실하게 입증하기를 희망하기 때문에 이 문제를 연구하고자 한다. 우리는 이 공격에 대해 개선할 수 있는 여러 가지 잠재적인 방향을 볼 수 있다.

양자화된 가중치로 대칭을 깨뜨리기.대형 생산 모델은 일반적으로 "양자화"되어 저장되며, 여기서 각 가중치는 단지 4 또는 8비트로 표현된다. 원리적으로, 이 양자화는 적수가 행렬 \(\mathbf{W}\\mathbf{G}\)의 거의 비트당 복사본을 복구하도록 허용할 수 있다: 무한히 많은 행렬 \(\mathbf{W}\\cdot\mathbf{G}\)이 존재하지만, 오직 하나만이 적절하게 이산화될 것이다. 불행하게도, 이 정수 제한 문제는 일반적으로 NP-hard이다(유사한 문제들은 공개 키 암호 시스템들의 전체 클래스의 기초가 된다). 그러나 이것이 모든 경우에 문제가 어렵다는 것을 암시할 필요는 없다.

이 공격을 단일 계층 이상으로 확장합니다. 우리의 공격은 변압기의 단일 계층을 복구합니다. 우리는 모델의 비선형성으로 인해 단일 층을 넘어 확장하는 명백한 방법론을 보지 못한다. 그러나 우리는 이 분야에 대한 추가 연구를 초대합니다.

로짓 편향 가정을 제거합니다. 우리의 모든 공격에는 로짓 편향을 통과시키는 능력이 필요합니다. 구글과 오픈AI를 포함한 모델 제공업체들은 우리가 이 논문 작성을 시작할 때 이 기능을 제공했지만, 이것은 바뀔 수 있다. (실제로, 모델 제공자가 이 공격을 방지하기 위해 방어 구현을 시작함에 따라 이미 있습니다.) 다른 API 매개 변수가 있을 수 있습니다.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{Dimension Extraction} & \multicolumn{3}{c}{Weight Matrix Extraction} \\ \cline{2-7} Model & Size & \# Queries & Cost (USD) & RMS & \# Queries & Cost (USD) \\ \hline OpenAI ada & \(1024\,\checkmark\) & \(<2\cdot 10^{6}\) & \$1 & \(5\cdot 10^{-4}\) & \(<2\cdot 10^{7}\) & \$4 \\ OpenAI babbage & \(2048\,\checkmark\) & \(<4\cdot 10^{6}\) & \$2 & \(7\cdot 10^{-4}\) & \(<4\cdot 10^{7}\) & \$12 \\ OpenAI babbage-002 & \(1536\,\checkmark\) & \(<4\cdot 10^{6}\) & \$2 & \(\dagger\) & \(<4\cdot 10^{6}\)\({}^{\dagger+}\) & \$12 \\ OpenAI gpt-3.5-turbo-instruct & \(\ast\) & \(<4\cdot 10^{7}\) & \$200 & \(\dagger\) & \(<4\cdot 10^{8}\)\({}^{\dagger+}\) & \$2,000\({}^{\dagger+}\) \\ OpenAI gpt-3.5-turbo-1106 & \(\ast\) & \(\checkmark\) & \(<4\cdot 10^{7}\) & \$800 & \(\dagger\) & \(<4\cdot 10^{8}\)\({}^{\dagger+}\) & \$8,000\({}^{\dagger+}\) \\ \hline \hline \end{tabular}

* 추출된 공격 크기가 정확히 맞았습니다. OpenAI와의 논의에서 확인되었습니다.
* 책임 있는 공개의 일부로 OpenAI는 이 번호를 게시하지 않도록 요청했습니다.
* 가중치의 보안을 유지하기 위해 공격이 구현되지 않았습니다.
* 모델의 크기와 추정된 크기 조정 비율을 감안할 때 예상 공격 비용.

\end{table}
표 4: 5가지 블랙박스 모델에 대한 공격 성공률

[MISSING_PAGE_FAIL:11]

Gurnee, W., Horsley, T., Guo, Z. C., Kheirkhah, T. R., Sun, Q., Hathaway, W., Nanda, N., and Bertsimas, D. Universal neurons in gpt2 language models, 2024.
* Hayase et al. (2024) Hayase, J., Borevkovic, E., Carlini, N., Tramer, F., and Nasr, M. 쿼리 기반 적대적 프롬프트 생성. _ arXiv preprint arXiv:2402.12329_, 2024.
* Jagielski et al. (2020) Jagielski, M., Carlini, N., Berthelot, D., Kurakin, A., and Papernot, N. 신경망의 높은 정확도와 높은 충실도 추출. 2020년 USENIX 보안 심포지엄에서.
* Jiang et al. (2023) Jiang, Z., Xu, F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan, J., and Neubig, G. Active retrieval augmented generation. EMNLP, 2023
* Juuti et al. (2019) Juuti, M., Szymler, S., Marchal, S., and Asokan, N. PRADA: DNN 모델 도용 공격으로부터 보호. 2019년 EuroS&P에서.
* Lee et al.(2022) Lee, K. -H., Nachum, O., Yang, M. S., Lee, L., Freeman, D., Guadarrama, S., Fischer, I., Xu, W., Jang, E., Michalewski, H., and Mordatch, I. Multi-game decision transformers. 2022년, 신경 정보 처리 시스템의 발전에서.
* Liu et al.(2024) Liu, A., Han, X., Wang, Y., Tsvetkov, Y., Choi, Y., and Smith, N. A. Tuning language models by proxy, 2024.
* Milli et al.(2019) Milli, S., Schmidt, L., Dragan, A. D., and Hardt, M. 모델 설명으로부터 모델 재구성. 2019년 공정성, 책임 및 투명성에 관한 회의 회보에서.
* Mitchell et al. (2024) Mitchell, E., Rafailov, R., Sharma, A., Finn, C., and Manning, C. D. Small language models using fine-tuning large language models for emulator. 2024년 ICLR에서요
* Morris et al. (2023) Morris, J. X., Zhao, W., Chiu, J. T., Shmatikov, V., and Rush, A. M. Language model inversion _ arXiv preprint arXiv:2311.13647_, 2023.
* OpenAI(2024) OpenAI. 로짓 편향을 사용하여 토큰 확률을 정의합니다. 2023. URL [https://help.openai.com/en/articles/5247780-using-logit-bias-to-define-token-probability](https://help.openai.com/en/articles/5247780-using-logit-bias-to-define-token-probability). Febraury 1, 2024
* OpenAI(2024) OpenAI. 채팅 완료 만들기, 2024. URL [https://platform.openai.com/docs/api-reference/chat/create](https://platform.openai.com/docs/api-reference/chat/create) 2024년 1월 30일에 접속했어요
* OpenAI 등 (2023) OpenAI 등 GPT-4 Technical Report, 2023.
* Pal et al.(2021) Pal, S., Gupta, Y., Kanade, A., and Shevade, S. 모델 추출 공격의 상태 저장 탐지 _ arXiv preprint arXiv:2107.05166_, 2021.
* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language Models is Unsupervised Multitask Learners. 기술 보고서, OpenAI, 2019. URL [https://rb.gy/tm8qh](https://rb.gy/tm8qh).
* Rae et al.(2019) Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., R., Young, S., Hutherford, E., Hennigan, T., Menick, J., Cassirer, A., L. L., Maese, A., Welbl, J., Dathathri, S., Wu, A., Lespiau, J., D., Sutherland, E., Simonyan, K., Paganini, I., Clark, A., de Las Casas, D., Guy, C., Bradbury, J., Johnson, M., Hechtman, B., Weidinger, L., Gabriel, I., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, G. Scaling 언어 모델:
* Ren et al.(2023) Ren, J., Zhao, Y., Vu, T., Liu, P. J., and Lakshminarayanan, B. Self-evaluation improves selective generation in large language models. _ arXiv preprint arXiv:2312.09300_, 2023.
* Rolnick & Kording (2020) Rolnick, D. and Kording, K. 역공학 딥 리루 네트워크 2020년 "머신 러닝에 관한 국제 회의"에서.
* Shamir et al. (2023) Shamir, A., Canales-Martinez, I., Hambitzer, A., Chavez-Saab, J., Rodriguez-Henriquez, F., and Satpute, N. 신경망 모델의 다항 시간 암호 분석 추출 _ arXiv preprint arXiv:2310.08708_, 2023.
* Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and efficient foundation language models. _ arXiv preprint arXiv:2302.13971_, 2023.
* Tramer et al. (2016) Tramer, F., Zhang, F., Juels, A., Reiter, M. K., and Ristenpart, T. 예측 API를 통해 기계 학습 모델을 훔칩니다. 2016년 USENIX 보안 심포지엄에서.
* Veit 등(2016) Veit, A., Wilber, M. J., and Belongie, S. J. Residual networks behave like ensembles of relatively shallow networks. In _Advances in Neural Information Processing Systems_, pp. 550-558, 2016.

Wei, J., Zhang, Y., Zhou, Z., Li, Z., and Al Faruque, M. A. Leaky DNN: Stealing deep-learning model secret with GPU context-switching side-channel. 2020년 IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)_에서.
* Yang & Klein (2021) Yang, K. And Klein, D. FUDGE: Controlled text generation with future discriminators. Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), _ACL_, 2021.
* Zhang & Sennrich (2019) Zhang, B. and Sennrich, R. 루트 평균 제곱 계층 정규화입니다. _ NeurIPS_, 2019.

## 부록 A GPT-2 소형에서 무슨 일이 일어나고 있습니까?

우리의 공격은 모든 모델의 모델 크기를 거의 완벽하게 추출합니다. GPT-2 스몰을 제외하고 757의 추출된 크기가 올바른 768에서 11만큼 떨어져 있습니다. 왜 그럴까요?

그림 3에서 우리는 \(10,000\) 다른 모델 쿼리에 걸쳐 이 모델의 최종 숨겨진 활성화 벡터를 직접 검사하고 결과 활성화 행렬의 SVD를 수행한다. 우리는 GPT-2가 실제로 768개의 잠재적인 숨겨진 뉴런을 가지고 있음에도 불구하고, 단지 \(757\) 다른 활성화 방향이 있다는 것을 안다. 따라서 이 모델은 기술적으로 768차원 모델이지만 실제로는 757(즉, 임베딩 행렬의 순위는 757)차원 모델인 것처럼 행동하며 우리의 공격은 이 유효 크기를 복구했다.

그러나 더 높은 float64 정밀도로 모델을 실행할 때 실제로 모든 차원이 사용되지만 가장 작은 12개 또는 그 정도의 특이값이 동시 작업에 의해 이루어진 관찰인 다른 특이값보다 훨씬 작다는 것을 발견했다(Cancedda, 2024).

## 정규화 계층에 대한 부록 B 계정

### 계층Norm은 우리의 순위 \(h\) 가정에 영향을 주지 않습니다.

공개된 아키텍처 세부 정보를 가진 거의 모든 LLM은 출력 투영을 적용하기 직전에 LayerNorm(Ba 등, 2016) 또는 RMSNorm(Zhang and Sennrich, 2019)을 사용한다 \(\mathbf{W}\)(Biderman, 2024). LayerNorm은 \((h-1)\)-차원 부분공간에 입력을 투사하는 센터링 단계로 시작되며 RMSNorm은 그렇지 않다. 이론적으로 이것은 열 \(g_{\theta}\left(p_{i}\right)\)(\(i=1,...,n\))이 있는 행렬의 순위가 \(h\)(Lemma 4.1)이라는 가정을 깨뜨릴 수 있다. 실제로 우리가 조사한 모든 LLM(Biderman, 2024)은 LayerNorm 바이어스를 활성화했으며, 이는 행렬이 전체 순위 \(h\)를 가졌다는 것을 의미한다(GPT-2 소형 외에: 부록 A를 참조).

### 정규화 계층에 대한 아키텍처 세부 정보 훔치기

#### b.2.1 Theory

LayerNorm과 RMSNorm의 차이(부록 B.1)는 공격자가 모델이 LayerNorm을 사용했는지 아니면 RMSNorm을 사용했는지 추론할 수 있게 할 수 있다. 공격자가 초기 로짓 벡터 API 쿼리 응답 \(\mathcal{O}\left(p_{1}\right)-\mathcal{O}\left(p_{0}\right))\(\mathcal{O}\left(p_{0}\right))\(\mathcal{O}\left(p_{1}\right)-\mathcal{O}\left(p_{n}\right)-\mathcal{O}\left(p_{0}\right))\(\mathcal{O}\left(p_{0}\right))\(\mathcal{O}\left(p_{i}\right)-\mathcal{O}\left(p_{0}\right))\(\mathcal{O}\left(p_{0}\right))\(\mathcal{O}\left(p_{i}\right)-\mathcal{O}\left(p_{0}\right))\(\mathcal{O}\left(p_{0}\right))\(\mathcal{O}\left(p_{i}\right)-\mathcal{O 이 \(g\)항의 뺄셈은 LayerNorm 직후에 발생하고,

그림 3: GPT-2 Small의 최종 은닉 활성화의 특이값이다.

따라서 LayerNorm 바이어스 항을 취소합니다. 따라서 이 뺄셈 수정에 의한 Lemma 4.1 공격을 LayerNorm을 사용하는 모델에 적용하면 결과 '\(h\)' 출력은 1만큼 작아진다(부록 B.1). 이는 RMSNorm이 더 작은 부분 공간에 투영되지 않기 때문에 이 빼기 트릭을 사용하면 '\(h\)' 값이 감소하지 않기 때문에 RMSNorm보다 LayerNorm을 사용한 모델을 의미한다.

#### b.2.2 Results

부록 B.2.1의 방법이 작동하는지 확인하기 위해 GPT-2, 피티아 및 LLAMA 아키텍처가 로짓 출력에서만 LayerNorm 또는 RMSNorm을 사용하는지 여부를 테스트한다. 우리는 이 기술이 32비트 정밀도보다 낮은 모델에서 작동하기 전에 두 가지 조정이 필요하다는 것을 발견했다(항상 32비트 정밀도로 작동했다). i) 로짓 질의에서 \(\mathcal{O}\left(p_{0}\right)\)를 빼지 않고, 대신 모든 질의에 대한 평균 로짓, 즉 \(\frac{1}{n}\sum_{i=1}^{n}\mathcal{O}\left(p_{i}\right)\)을 뺀다. 공통 아핀 부분공간의 여러 점들의 평균은 여전히 그 아핀 부분공간에 있기 때문에, 이것은 부록 B.2.1. ii의 결론을 바꾸지 않는다. 우리는 또한 컴팩트 SVD를 계산하기 위해 64비트 정밀도로 주조하기 전에 더 낮은 정밀도로 이 평균을 계산하는 데 도움이 된다는 것을 발견했다.

결과는 그림 4에 나와 있습니다. 특이값 크기(그림 1과 같이)를 표시하고 **계층Norm을 사용하는 아키텍처에 대해 \(h\) 번째 특이값이 감소하지만 RMSNorm을 사용하는 아키텍처에 대해서는 감소하지 않음** 을 보여 줍니다.

이 공격은 실제 모델에게 실용적인가? 우리는 ada와 babbage에 대해 얻은 로그프로브에 대해 동일한 공격을 수행합니다.2 그림 4(a)-b에서 실제로 LayerNorm을 사용하는 이 두 모델에 대해 \(h\) 번째 특이 값의 감소가 발생한다는 것을 볼 수 있습니다(GPT-3의 아키텍처는 GPT-2에서 거의 완전히 상속됨):

각주 2: 안타깝게도 보안 제약으로 인해 이 공격을 만들기 전에 GPT-3.5 모델에 대한 로그프로브를 삭제했습니다.

최종 스트레스 테스트로서, RMSNorm을 사용하는 모든 개방 언어 모델은 어떠한 바이어스 항도 사용하지 않는다는 것을 발견했다(Biderman, 2024). 따라서 우리는 RMSNorm이 있는 모델에 적용할 때 공격이 거짓 양성을 제공하지 않고 편향이 있음을 확인했다. 우리는 공공 건축 상세를 갖지만 공공 중량 액세스가 없는 모델인 Gopher-7B(Rae et al., 2022)를 선택했다.

그림 4: 모델이 특이값 크기별로 **LayerNorm** 또는 **RMSNorm** 을 사용 하는지 여부를 감지 합니다.

그림 5: API 뒤에 있는 모델(a-b)과 RMSNorm과 바이어스(c)를 모두 사용하는 모델에 대한 LayerNorm 추출 공격을 스트레스 테스트합니다.

RMSNorm을 사용하지만 (예: 출력 로짓에서) 편향도 사용 합니다. 그림 5c에서 우리는 RMSNorm을 사용하는 이 모델에 대해 실제로 \(h\) 번째 특이값이 감소하지 않는다는 것을 보여준다.

## 부록 C 증명 Lemma 4.2

섹션 4.2에서 보조 부호를 복원하는 단계:

**Lemma 4.2**: _Logit-API 위협 모델에서 Lemma 4.1:_ (i) 섹션 4.2의 가정 하에서 _일부 \(\mathbf{G}\in\mathds{R}^{h\times h}\)에 대해 \(\hat{\mathbf{W}}=\mathbf{W}\cdot\mathbf{G}\)를 복구합니다. _ (ii) _ \(g_{\theta}(p)\)가 잔차 연결을 가진 변압기라는 추가 가정에서는 \(\mathbf{W}\)를 정확하게 추출할 수 없습니다. _

우리는 먼저 (i)에 대한 간략한 증명을 제시한다:

증명: (i) 우리는 \(\hat{\mathbf{W}}=\mathbf{W}\cdot\mathbf{G}\), Lemma를 회수할 수 있다. 4.1: 우리는 \(\mathbf{Q}^{\top}=\mathbf{W}\cdot\mathbf{H}\)에 접근하여 일부 \(\mathbf{H}\in\mathds{R}^{h\times n}\)에 접근할 수 있다. 4.2절의 방법에서 \(\mathbf{Q}\)의 컴팩트 SVD를 사용하여 \(\mathbf{W}\cdot\mathbf{H}\cdot\mathbf{V}=\mathbf{U}\cdot\mathbf{\Sigma}\). 우리는 \(\mathbf{G}:=\mathbf{H}\cdot\mathbf{V}\in\mathds{R}^{h\times h}\)를 알고 있으므로 \(\hat{\mathbf{W}}=\mathbf{U}\cdot\mathbf{\Sigma}\)를 취하면 \(\hat{\mathbf{W}}=\mathbf{W}\cdot\mathbf{G}\)가 된다.

Lemma 4.2(ii)를 증명하는 것은 변압기 구조의 복잡성으로 인해 여러 단계가 필요하다: 잔차 연결이 없는 모델(C.1), 잔차 연결이 있는 모델(C.2), RMSNorm이 있는 모델(C.4), LayerNorm(C.5) 및 \(\varepsilon\) 항이 있는 정규화(C.6)에 적용하기 위해 증명을 점진적으로 강화한다.

### 완전히 연결된 계층을 가진 모델에서 Lemma 4.2(ii)의 증명

Lemma 4.2(ii)의 증명: 완만한 웜업으로서, 우리는 (ii) 모델이 그것의 아키텍처에서 정규화 층들(LayerNorm 또는 RMSNorm)을 사용하지 않는다는 추가적인 가정 하에서 증명한다. (ii)를 증명하기 위해 우리는 동일한 API 출력을 초래하는 서로 다른 임베딩 투영 행렬을 갖는 모델 매개변수 \(\theta,\theta^{\prime}\)의 두 세트를 찾을 수 있음을 보여준다.

우리는 \(g_{\theta}\)가 잔류 연결이 아니라 완전 연결(FC) 최종 레이어를 갖는 더 간단한 경우로 시작한다. 이 경우, 임의의 가역적인 \(h\times h\) 행렬 \(\mathbf{S}\)에 대해, \(g_{\theta}\left(p\right)=\mathbf{S}_{g\theta^{\prime}}\left(p\right)\)를 가지며, 여기서 \(\theta^{\prime}\)는 최종 FC 층의 가중치가 \(\mathbf{S}^{-1}\)에 미리 곱해진 것을 제외하고는 \(\theta\)와 동일하다. 따라서, \(g_{\theta}\)가 최종 FC 레이어를 갖는 경우, API \(\mathcal{O}\)의 출력에만 접근할 경우, \(g_{\theta}\)에 작용하는 임베딩 투영 레이어 \(\mathbf{W}\)와 \(g_{\theta^{\prime}\)에 작용하는 임베딩 투영 레이어 \(\mathbf{W}\cdot\mathbf{S}\)를 구별할 수 없다.

### Lemma의 증명 4.2(ii) With Residual Layers

보다 일반적으로, \(g_{\theta}\)가 잔차 계층으로 구성되지만 정규화 계층이 없는 경우 \(g_{\theta}(p)=\sum_{i}L_{i}(p)\), 여기서 \(L_{i}(p)\)는 스킵 연결을 무시하고 모델에서 \(i\)번째 잔차 계층의 출력이다(Ehlage et al., 2021; Veit et al., 2016). 또한 각 \(L_{i}\)가 완전 연결 선형 계층과 선형 입력 계층인 최종 계층을 갖는다고 가정한다(이 가정은 정규화 계층이 없는 변압기의 주의 및 MLP 모듈 모두에 적용된다). 각 \(L_{i}\)에 \(\mathbf{S}^{-1}\)을 미리 곱한 입력 가중치와 출력 FC 가중치에 \(\mathbf{S}\)을 곱하도록 \(\theta^{\prime}\)을 구성하면 선형성에 의해 \(g_{\theta^{\prime}}(p)=\sum_{i}\mathbf{S}L_{i}(p)=\mathbf{S}\cdot g_{\theta}(p)\)가 된다. 마지막으로, 새로운 임베딩 투영행렬 \((\mathbf{S}^{-1})^{\top}\cdot\mathbf{W}^{\top}\)을 이용하여 계산

\[\left((\mathbf{S}^{-1})^{\top}\cdot\mathbff{W}^{\top})^{\top}\cdot g_{\theta^ {\prime}}(p)=\mathbf{W}\cdot g_{\theta}(p), \tag{3}\]

로짓 출력이 동일하므로 \(\mathcal{O}\)와 \(\mathcal{O}^{\prime}\)만 조회해도 이들 변압기를 구분할 수 없음을 보였다.

### 정규화 계층 및 직교 행렬

섹션 C.3-C.6에서 우리는 더 이상 우리의 논증에서 일반적인 가역 행렬 \(\mathbf{S}\)을 사용할 수 없으며, 대신 직교 행렬, 행렬 \(\mathbf{U}\)을 사용하여야 한다. LayerNorm이 있는 모델에서도, 우리는 더 전문적이다(부록 C.5).

**Lemma C.1**: _RMSNorm 작업은 \(x\mapsto\mathbf{W}n(x)+b\)와 같습니다. 여기서 \(\mathbf{W}\)는 대각 행렬입니다._

증명: RMSNorm은 통상적으로 다음과 같이 작성된다.

\[x\mapsto\frac{w\cdot x}{\sqrt{\frac{1}{h}\sum_{i}x_{i}^{2}}}+b \tag{4}\] 여기서 \(w\)는 정규화된 \(x\)에 원소 곱해진다. 분명히 이것은 대각 행렬로 쓰여질 수 있다. 또한, 우리는 이 대각행렬에 \(\sqrt{h}\)를 곱하여 식 (4)의 분모에서 그 인자를 상쇄할 수 있다. 우리는 \(n(x)=x/||x||=x\bigg{/}\sum_{i}\sqrt{x_{i}^{2}}\)이기 때문에 그 결과를 얻는다.

직관적으로 부록 C.2의 증명은 입력 투영 가중치를 행렬 \(\mathbf{S}^{-1}\)에 미리 곱하는 것에 의존하여 모델의 숨겨진 상태에 적용되는 회전 \(\mathbf{S}\)을 취소했다(기계론적 해석 가능성 문헌에서 '보조 스트림'이라고 함)(Elhage et al., 2021). 형식적으로 입력 투영층을 \(\mathbf{M}\)라고 하면, \(\left(\mathbf{MS}^{-1}\right)\left(\mathbf{S}x\right)=\mathbf{M}x\라는 사실을 사용하고 있었다. 그러나 정규화 계층이 있는 모델은 선형 입력 투영 전에 이를 사용하기 때문에 \(\mathbf{S}\)를 은닉 상태에 적용한 결과 동일한 절차를 적용하면 활성화가 생성된다.

\[(\mathbf{MS}^{-1})(\mathbf{W}n(\mathbf{S}x)+b) \tag{5}\]

그러나 일반적으로 \(n\)와 \(\mathbf{S}\)는 통근하지 않기 때문에, 우리는 \(\mathbf{S}\) 변환이 변압기의 출력을 보존한다고 결론지을 수 없다. 우리는 \(\mathbf{S}=\mathbf{U}\) 직교행렬을 취하면 여전히 일반적인 불가능 결과를 얻을 수 있음을 보일 것이다.

이를 위해서는 선형대수학의 간단한 결과가 필요하다.

**Lemma C.2**.: _Let \(x\in\mathds{R}^{h}\). 그런 다음 정규화 맵 \(n(x):=\frac{x}{||x||}\)은 직교 행렬 \(\mathbf{U}\)로 변환됩니다._

Lemma C.2의 증명: \(\frac{\mathbf{U}x}{||x||}=\frac{\mathbf{U}x}{||\mathbf{U}x||}\). 이것은 \(x^{\top}\mathbf{U}^{\top}\mathbf{U}x=x^{T}x\)이므로 true입니다. 따라서 \(||\mathbf{U}x||=||x||\).

### RMSNorm이 있는 모델에서 Lemma 4.2(ii)의 증명

Lemma C.2에서는 직교행렬 \(\mathbf{U}\)이 정규화와 함께 통근하는 것을 보였다. 따라서 모든 레이어 출력 가중치를 \(\mathbf{U}\)에 곱하고 모든 레이어 입력 투영 가중치를 \(\mathbf{W}\mathbf{U}^{\top}\mathbf{W}^{-1}\)에 미리 곱하면 선형 투영 레이어의 효과는 다음과 같다.

\[(\mathbf{M}\mathbff{W}\mathbff{U}^{\top}\mathbf{W}^{-1})(\mathbf{W}n(\mathbf{U} x)+b)=(\mathbf{M}\mathbff{W}\mathbff{U}^{\top}\mathbf{W}^{-1})(\mathbf{W} \mathbff{U}n(x)+b)=\mathbf{M}(\mathbf{W}n(x)+b) \tag{6}\]

이는 원래 모델과 동일합니다. 이 절차를 은닉 상태에 추가된 모든 층(매번 다른 \(W\) 대각 행렬을 사용)에 적용하면 모델 \(g_{\theta^{\prime}}(p)\)을 얻을 수 있으므로 \(g_{\theta^{\prime}}(p)=\mathbf{U}g_{\theta^{\prime}}(p)\)은 다른 임베딩 투영 행렬 \(\mathbf{W}\mathbf{U}^{\top}\)은 원래 모델 \(g_{\theta}(p)\)과 동일한 출력을 얻을 수 있다.

우리는 상수 \(b\in\mathds{R}^{h}\)에 적용되는 모든 어파인 맵이 상수 \(b^{\prime}\in\mathds{R}^{h}\)를 산출하기 때문에 위의 인수에서 \(b\)에 발생하는 일은 무시한다. 그리고 \(g_{\theta^{\prime}}\)에서 \(b\) 대신 \(b^{\prime}\)를 사용할 수 있다.

### LayerNorm이 있는 모델에서 Lemma 4.2(ii)의 증명

LayerNorm 연산은 센터링 연산 \(x\mapsto x-\bar{x}\)과 RMSNorm(즉, 먼저 센터링을 적용한 후 RMSNorm)을 합한 것이다. 따라서 센터링 연산으로 통근하기 위해서는 \(\mathbf{U}\)가 필요하다는 점 외에 부록 C.4와 동일한 논증을 거친다. 센터링 연산은 \(\mathbf{1}^{T}x=0\)에 의해 정의된 \((h-1)\)차원 부분공간을 고정하기 때문에 \(\mathbf{1}\in\mathds{R}^{h}\)는 벡터의 벡터이므로 \(\mathbf{U}\,\mathbf{1}\in\{-\mathbf{1},\mathbf{1}\}\)라는 추가 조건을 부과하기에 충분하다.

### 정규화 모델에서의 Lemma 4.2(ii) 증명 \(\varepsilon\neq 0\)

우리는 이제 LayerNorm의 분모에 있는 \(\varepsilon\)이 0이 아닌 현실적인 모델로 확장한다. \(x\mapsto n(x)\)에 대해 우리가 사용한 유일한 사실은 \(x\mapsto n(\mathbf{U}x)\)이 \(x\mapsto\mathbf{U}n(x)\)와 동일하기 때문이다. 결국 Lemma C.2는 직교성으로 인해 \(||\mathbf{U}x||=||x||\)에 의존했다. 그러나 \(n(x)\)을 \(n^{\prime}(x):=x\bigg{/}\sqrt{\frac{1}{h}||x||^{2}+\varepsilon}\)로 조정하는 것(즉, epsilon을 사용한 정규화)은 \(||x||=||\mathbf{U}x||\), \(n^{\prime}\)이 \(\mathbf{U}\)로 통근하기 때문에 부록 C.4와 부록 C.5의 증명은 \(n\) 대신 \(n^{\prime}\)를 사용할 때 여전히 작동합니다.

따라서, 우리는 모든 공통 모델 아키텍처(조밀한 층으로 끝나는 모든 비잔류 네트워크 및 바이더만(2024)의 모든 변압기)에서 불가능 결과 Lemma 4.2(ii)를 입증했다.

## 부록 D 이진화된 로그프로브 추출의 파생(섹션 5.4)

먼저 우리가 쓸 수 있다는 걸 지켜봐

\[y_{\text{top}} =\text{logit}_{\text{top}}-\log\sum_{i}\exp(\text{logit}_{i})\] \[y^{\prime}_{\text{top}} =\text{logit}_{\text{top}}-\log\big{(}\exp(\text{logit}_{t}-1)+ \sum_{i\neq t}\exp(\text{logit}_{i})\big{}\]

\(\mathbf{N}=\sum_{i}\exp\left(\text{logit}_{i}\right)\)와 \(p=\exp\left(\text{logit}_{t}\right)/\mathbf{N}\)로 한다. 그럼 다시 쓸 수 있어

\[y_{\text{top}} =\text{logit}_{\text{top}}-\log\mathbf{N}\] \[y_{\text{top}} =\text{logit}_{\text{top}}-\log(\mathbf{N}+(1/e-1)p\mathbf{N})\]

둘을 빼면

\[y_{\text{top}}-y^{\prime}_{\text{top}} =\log\left(1+(1/e-1)p\right)\] \[\implies p =\frac{\exp(y_{\text{top}}-y^{\prime}_{\text{top}})-1}{1/e-1}.\

관련 작업.동시 작업(Morris et al., 2023)은 유사하지만 더 약한 2-쿼리 로그프로브 추출을 논의한다. 그들의 공격은 \(\text{logit}_{\text{top}}-\text{logit}_{i}\)보다 큰 로짓 바이어스와 top-2 logprob 액세스가 필요하며, 우리의 공격은 로짓 바이어스가 0이 아닌 것으로 허용되고 top-1 logprob 액세스가 되는 즉시 작동한다.

## 부록 E 상위 \(k\) 로그프로브 API에서 로짓의 효율적인 복구

본문 섹션 5.3에서는 상위 몇 개의 로그프로브만 제공하고 각 쿼리로 로짓 편향을 수용하는 API에 다중 쿼리를 통해 전체 로짓 벡터를 추출하는 간단하고 실용적인 방법을 제시했다. 이 섹션에서는 보다 효율적인 방법을 제시한다.

앞서 제시한 방법은 참조 토큰을 사용한다. 이를 임의의 값(예: 0)으로 설정한 다음 다른 모든 토큰에 대한 로짓값을 이 값과 비교합니다. 이 방법은 수치적으로 안정적이지만 약간 낭비적입니다. API에서 반환 하는 상위 \(K \) 로그 프로브 중 하나는 항상 참조 토큰입니다. 따라서 이 방법을 사용하여 쿼리당 \(K-1\) 로짓만 복구한다.

이 부록에서는 쿼리당 \(K\) 로짓을 최상위 \(K\) 로그프로브 API로 복구할 수 있는 선형 대수적 방법을 제시한다.

**설정:** 복구하려는 알 수 없는 벡터 \(z=\mathbf{W}\cdot g_{\theta}(p)\in\mathds{R}^{\ell}\)(즉, 지정된 프롬프트에 대한 로짓 \(p\))가 있음을 상기합니다. 우리는 동일한 프롬프트 \(\mathcal{O}(p,b)\)를 사용하여 API에 여러 개의 쿼리를 만들 수 있다. 각 질의는 벡터 \(b\in\mathds{R}^{\ell}\)(a.k.a. logit bias)에 의해 지정된다. 우리는 \((i,a_{i}(z,b))\in\mathbf{N}\times\mathbf{R}\)의 양식을 입력받는데, 여기서 \(i\)는 토큰 인덱스이고 \(a_{i}(z,b)\)는 로그프로브이다:

\[a_{i}(z,b)=\log\left(\frac{\exp(z_{i}+b_{i})}{\sum_{j}^{\ell}\exp(z_{j}+b_{j})}\right)=z_{i}+b_{i}-\log\left(\sum_{j}^{\ell}\exp(z_{j}+b_{j})\right) \tag{7}\]

각 쿼리는 여러 개의 답변(즉, \(K\) 가장 큰 \(a_{i}(z,b)\) 값을 수신할 수 있습니다. 기호적 단순화를 위해, 우리는 각각 하나의 답변을 반환하는 다수의 질의와 동일한 방식으로 하나의 질의에 대한 다수의 답변을 나타낸다. 질문 \(b^{1},\cdots,b^{m}\)이 질문되었고 \(m\) 대답 \((i_{1},a_{i_{1}}(z,b^{1}))\leftarrow\mathcal{O}(p,b^{1}),\cdots,(i_{m},a_{i_{m}}(z,b^{m}))\leftarrow\mathcal{O}(p,b^{m})\를 받았다.

우리의 목표는 답으로부터 \(z\)를 계산하는 것이다 \(a_{i}(z,b)\).

### Warmup: Single Logprob API (\(K=1\))

시작점으로 API가 가장 큰 단일 로그프로브(즉, \(K=1\))만 반환한다고 가정합니다. 섹션 5.3의 접근법은 참조 토큰과 다른 토큰의 로그프로브를 동시에 얻을 수 없기 때문에 이 설정에서 작동할 수 없으며, 이는 쿼리당 \(1\) 로짓보다 적게 복구할 수 있음을 의미한다.

이 문제를 극복하기 위한 높은 수준의 아이디어는 참조 토큰에 대한 로짓을 정규화하는 대신 로짓을 로그프로브로 정규화한다는 것이다. 즉, 정규화 \(\sum_{j}\exp(z_{j})=1\)로 로짓들을 복원한다. 이 정규화를 사용하면 더 이상 모든 쿼리에 참조 토큰을 포함할 필요가 없습니다.

토큰 인덱스 \(i\)를 고정하고 모든 \(j\neq i\)에 대해 \(b_{i}=B\) 및 \(b_{j}=0\)를 둡니다. 이 로짓 바이어스를 사용하여 API를 쿼리하고 \(B\)가 토큰 \(i\)가 반환될 만큼 충분히 크다고 가정합니다.

\[(i,a_{i}(z,b))\leftarrow\mathcal{O}(p,b).\]

상기 수학식 7로부터,

\] \[\implies\exp(z_{i}+B)+\sum_{j}^{\ell}\exp(z_{j})\right)\] \[=z_{i}+B-\log\left(\exp(z_{i}+B)+\sum_{j}^{\ell}\exp(z_{j})\right),\] \[\implies\exp(z_{i}+B-a_{i}(z,b)) =\exp(z_{i}+B)-\exp(z_{i}+B-a_{i}(z,b)) =\exp(z_{i}+B)+\sum_{j}^{\ell}\exp(z_{j})\right),\] \[\implies\exp(z_{i}+B-a_{i}(z,b)) =\exp(z_{i}+B)-\exp(z_{i}+B-a_{i}(z,b)) =\exp(z_{i}+B)+\sum_{j}^{\ell}\exp(z_{j

따라서 \(\sum_{j}^{\ell}\exp(z_{j})=1\)를 정규화하면 다음과 같다.

\[z_{i}=-\log\left(\exp(B-a_{i}(z,b))-\exp(B)+1\right). \tag{8}\]

### 복구 \(K\) Logits From \(K\) Logprobs

이전 하위 섹션의 접근 방식은 각 API 쿼리가 상위 \(K\) 로그프로브를 반환하는 설정으로 확장됩니다. 실제로 우리는 \(K=5\)로 작업한다. 우리는 \(K\) 로짓들을 복구할 수 있다. 다시, 참조 토큰을 사용하여 로짓들을 정규화하는 대신 \(\sum_{j}\exp(z_{j})=1\)를 정규화할 것이다. 그러나 이 설정에서는 \(K\)-by\(K\) 연립일차방정식을 풀어야 한다.

토큰 인덱스 \(k\) \(i_{1},\cdots,i_{K}\) 및 let \(b_{i_{k}}=B\)는 \(k\in\{1,\cdots,K\}\) 및 \(b_{j}=0\)는 모든 \(j\notin\{i_{1},\cdots,i_{K}\}\)에 대해 수정합니다. 이 로짓 바이어스를 사용하여 API를 쿼리하고 \(B\)가 충분히 커서 \(i_{1},\cdots,i_{K}\)에 대한 로그프로브가 상위 \(K\) 로그프로브로 반환된다고 가정합니다.

\[(i_{1},a_{i_{1}}(z,b)),(i_{2},a_{i_{2}}(z,b)),\cdots,(i_{K},a_{i_{K}}(z,b)) \leftarrow\mathcal{O}(p,b).\]

\(z\in\mathbb{R}^{\ell}\)를 미지의 로짓으로 하고 \(\mathbf{N}=\sum_{i}\exp(z_{i})\)를 정규화 상수로 한다. 각각의 \(k\in\{1,\cdots,K\}\)에 대하여, 우리는

(e^{B}-1)\sum_{i\in\{i_{1},\cdots,i_{K}\}} \exp(z_{i}+B-\log\left((e^{B}-1)\sum_{i\in\{i_{1},\cdots,i_{K}\}} \exp(z_{i})+\sum_{i}^{\ell}\mathbf{z_{i})\right)\] \[\implies z_{i_{k}+B-\log\left((e^{B}-1)\sum_{i\in\{i_{1},\cdots,i_{K}\}} \exp(z_{i}+B)+\sum_{i}^{\ell}\exp(z_{i})\right)\] \[=z_{i_{k}+B-\log\left((e^{B}-1)\sum_{i\in\{i_{1},\cdots,i_{K}\}} \exp(z_{i})+\mathbf{z_{i})\

따라서 결론을 내릴 수 있습니다.

\[\exp(B-a_{i_{k}}(z,b))\cdot\exp(z_{k})-(e^{B}-1)\sum_{i\in\{i_{1},\cdots,i_{K} \}}\exp(z_{i})=\mathbf{N}.\]

이러한 선형 연립방정식은 행렬 형태로 표현될 수 있다:

\[A\cdot\left(\begin{array}{c}\exp(z_{i_{1}})\\ \exp(z_{i_{2}})\\ \vdots\\ \exp(z_{i_{K}})\end{array}\right)=\left(\begin{array}{c}\mathbf{N}\\ \mathbf{N}\\ \vdots\\\mathbf{N}\end{array}\right)\]

여기서, \(A\)는 엔트리가 있는 \(K\times K\) 행렬

\[A_{k,j}=\begin{cases}\exp(B-a_{i_{k}}(z,b))-(e^{B}-1)&\text{if }j=k\\ -(e^{B}-1)&\text{if }j\neq k.\end{cases}\]

\(A\)는 대각 행렬의 순위원 섭동, 즉, \(\mathbf{1}\)가 전부 벡터라면, 그럼 주목하라.

\[A=\operatorname{diag}_{1\leq k\leq K}(\exp(B-a_{i_{k}}(z,b)))-(e^{B}-1) \mathbf{1}\mathbf{1}^{T},\]

여기서 \(\operatorname{diag}_{1\leq k\leq K}(\exp(B-a_{i_{k}}(z,b)))\)는 대각 행렬을 나타내며, \(k\)-번째 대각 엔트리는 \(\exp(B-a_{i_{k}}(z,b))\)이다. 대각 행렬을 역산하는 것은 쉬우므로 셔먼-모리슨 공식을 사용하여 \(A\)의 역산을 계산할 수 있다:

\[A^{-1} =\operatorname{diag}_{1\leq k\leq K}(\exp(a_{i_{k}}(z,b)-B)))+(e^ {B}-1)\frac{\operatorname{diag}_{1\leq k\leq K}(\exp(a_{i_{k}}(z,b)-B)))\mathbf{1}\mathbf{1}\mathbf{1}^{T}\operatorname{diag}_{1\leq k\leq 5}(\exp(a_{i_{k}}(z,b)-B))) \[=\operatorname{diag}(v)+(e^{B}-1)\frac{vv^{T}}{1-(e^{B}-1) \mathbf{1}^{T}}{1}^{T}}^{T}}(e^{B}-1) \mathbf{R}^{K}\] 여기서 \(v\in\mathbb{R}^{K}\) 따라서

\[=\left(v+\frac{(e^{B}-1)vv^{T}}}{1-(e^{B}-1)\mathbf{1}^{T}}\right)\\vdots\\\mathbf{N}\end{array}\right) \[=\left(v+\frac{(e^{B}-1)\mathbf{1}^{T}}}{1-(e^{B}-1)\mathbf{N}\\mathbf{N}\end{array}\right) \[=\frac{\mathbf{N}}{1-(e^{B}-1)\mathbf{1}^{T}}\right)\cdot\mathbf{N}\end{array}\right) \[=\frac{\mathbf{N}}{1-(e^{B}-1)\mathbf{1}^{T}}\right)\cdot\mathbf{N}\end{array}\right) \[=\frac{\mathbf{N}}

우리가 \(\mathbf{N}=1\)을 정규화하면, 이것은 우리에게 로짓 계산을 위한 공식을 제공한다:

\[z_{i_{k}}=a_{i_{k}}(z,b)-B-\log\left(1-(1-e^{-B})\sum_{j}^{K}\exp(a_{i_{j}}(z,b))\right) \tag{9}\]

\(K=1\)을 설정하면 식 8과 같은 결과가 나온다는 점에 유의하라.

식 9를 사용한 복구는 5.3절의 방법보다 효율적이며, \(K\) 로짓 \(z_{i_{1}},z_{i_{2}},\cdots,z_{i_{K}}\) 로짓만 복구가 아니라 \(z_{i_{1}},z_{i_{K}}\)을 복구한다. 그러나 \(B\)가 크면 수치 안정성이 문제가 될 수 있다. (그리고, \(B\)가 작으면, API가 원하는 토큰을 상단 \(K\)에 배치하여 출력하도록 하기에는 로짓 바이어스가 충분하지 않을 수 있다.) 구체적으로, \(B\to\infty\)로서, 우리는 \((1-e^{-B})\sum_{j}^{K}\exp(a_{i_{j}}(z,b))\to1\)를 갖게 되고, 따라서 식 9의 로그 값은 \(\log(1-1)=-\infty\)로 변하게 된다.

관련 작품.책임공시기간에 출판된 두 작품은 유사한 절차를 사용하며, 수치적인 문제를 서로 다른 방식으로 다룬다. (Chiu, 2024)는 전체 어휘에 대해 낮은 \(B\)로 시작하여 \(B\)를 증가시키고 이전에 나타나지 않은 모든 토큰을 요청하고 모든 토큰이 커버될 때까지 반복한다. (Hayase et al., 2024)는 부록 E.1에서 방법을 사용하고 \(B=-\hat{z}_{i}\)를 설정하며, 여기서 \(\hat{z}_{i}\)는 응용 프로그램에 고유한 \(z_{i}\)의 추정치이다. 이 방법의 변형은 우리 또는 이러한 작업 이전에 논의되었을 수 있지만 추가 참조를 알지 못한다.

### General Method

일반적으로 API가 반환하는 로그프로브 또는 API에 제공되는 로짓 편향을 완전히 제어할 수 없습니다. 따라서 우리는 위의 선형 대수적 접근법을 일반화하여 임의의 로짓 편향과 토큰으로부터 로짓들을 재구성한다.

질문 \(b^{1},\cdots,b^{m}\)이 질문되었고, \(m\) 대답 \((i_{1},a_{i_{1}}(z,b^{1}))\leftarrow\mathcal{O}(p,b^{1}),\ldots,(i_{m},a_{i_{m}}(z,b^{m}))\leftarrow\mathcal{O}(p,b^{m})\를 받았다. (쿼리가 여러 개의 답변을 반환하는 경우, 우리는 이것을 각각 하나의 답변을 반환하는 여러 개의 쿼리와 동일하게 취급할 수 있다.)

앞서와 같이 식 7을 재배열하면 다음과 같은 식을 얻을 수 있다.

\[\forall k\in[m] \exp(a_{i_{k}}(z,b_{i_{k}}^{k}))=\frac{\exp(z_{i_{k}}+b_{i_{k}}^{k})}{\sum_{j}^{\ell}\exp(z_{j}+b_{j}^{k})}.\] \ [\forall k\in[m] \sum_{j}^{\ell}\exp(z_{j}+b_{j}^{k})=\exp(z_{i_{k}}+b_{i_{k}}^{k} -a_{i_{k}}(z,b^{k}))\] \ [\forall k\in[m] \sum_{j}^{\ell}\exp(z_{j})\cdot\exp(b_{j}^{k})=\exp(z_{i_{k}}) \cdot\exp(b_{i_{k}}^{k}-a_{i_{k}}(z,b^{k})) \] \] \ \forall k\in[m] \sum_{j}^{\ell}\left(\exp(b_{j}^{k})-\mbox{1I}[j=i_{k}]\cdot \exp(b_{i_{k}}^{k}-a_{i_{k}(z,b^{k}))\right)\cdot\exp(z_{j})=0.\] \[A\cdot\left(\begin{array}{c}\exp(z_{1})\\\vdots\\\\\exp(z_{\ell})\end{array}\right)=\left(\begin{array}{c}0\\0\\\vdots\\0\end{array}\right),\] \[\mbox{where}\\forall k\in[m]\forall j\in[\ell]\\A_{k,j}=\exp(b_{j}^{k})\cdot\left(1-\mbox{1I}[j=i_{k}]\cdot\exp(-a_{

여기서 \(\mbox{1I}[j=i_{k}]\)는 \(1\) if \(j=i_{k}\) and \(0\) otherwise이다. 만약 \(A\)가 가역적인 경우, 이 선형 시스템은 로짓 \(z\)을 복구하기 위해 풀릴 수 있다. 불행하게도, \(A\)는 되돌릴 수 없다: 실제로, 우리는 모든 로짓들을 동일한 양만큼 이동시키면 정확히 동일한 대답 \(a_{i}(z,b)=a_{i}(z+\mbox{1I},b)\)을 산출하기 때문에 해는 유일할 수 없다는 것을 안다. 즉, \(A\cdot\exp(z)=\mbox{0}\)에 대한 유효한 해의 1차원 공간을 예상한다. 이를 처리하기 위해 \(z_{1}=0\) 또는 등가적으로 \(\exp(z_{1})=1\)라는 제약 조건을 간단히 추가한다. 이것은 시스템에 해당한다.

\[\widehat{A}\cdot\exp(z)=\left(\begin{array}{cc}1&0&\cdots&0\\ &A\end{array}\right)\cdot\left(\begin{array}{c}\exp(z_{1})\\\exp(z_{2})\\\vdots\\\exp(z_{\ell})\end{array}\right)=\left(\begin{array}{c}1\\0\\\vdots\\0\end{array}\right)\]\]

( \(\sum_{i}^{\ell}\exp(z_{i})=1\). 이것은 하나의 \(1\)이 아닌 모든 \(1\)s인 \(\widehat{A}\)의 첫 번째 행에 해당한다.) 이는 증강 행렬이 0이 아닌 행렬식을 갖는 한 해결 가능하다.

\[\det\left(\widehat{A}\right)=\det\left(\begin{array}{cc}1&0&\cdots&0\\ &A\end{array}\right)=\det(A_{1:m,2\ell}). \tag{10}\

여기서 \(A_{1:m,2\ell}\)는 첫 번째 열이 제거된 \(A\)를 나타낸다. 우리는 \(m=\ell-1\)을 설정하고 있습니다. 이것은 우리가 필요로 하는 질의-응답 쌍의 최소 수이다. 만약 우리가 \(m\geq\ell\)를 더 가지고 있다면, 시스템은 과도하게 결정된다. 시스템이 과도하게 결정되는 것은 좋은 일이다; 추가 답은 우리가 로그프로브를 더 정확하게 복구하는 데 도움이 될 수 있다. 과잉 결정된 시스템에 대한 최소 제곱 솔루션은 다음과 같이 주어진다.

\[\widehat{A}^{T}\widehat{A}\cdot\left(\begin{array}{c}\exp(z_{1})\\ \exp(z_{2})\\ \vdots\\\exp(z_{\ell})\end{array}\right)=\widehat{A}^{T}\left(\begin{array}{c}1\\ 0\\ \vdots\\ 0\end{array}\right)\tag{11}\\

이는 로그프로브 API로부터 (정규화된) 로짓들을 복구하기 위한 일반적인 방법을 제공한다.

## 부록 F 우리의 Logprob-Free 공격은 최적에서 얼마나 멀리 있는가?

로그프로브가 없는 API에서, 우리는 로짓들을 복구할 수 있는 공격들을 발생시켰고, 궁극적으로 은닉 차원과 임베딩 행렬을 유사성 변환까지 생성시켰다. 이제 로그프로브가 없는 API 위협 모델에서 모델 도용을 시도하는 _any_ 공격자가 요구하는 최소 쿼리 수에 대한 하한을 제공합니다.

**Lemma F.1**.: _\(\mathsf{logit}\in\mathbb{R}^{l}\)의 항목이 \([-B,0]\)에 대해 i.i.d. 균일하다고 가정합니다. 벡터 \(\mathsf{logit}\)에서 \(\infty\)-norm error \(\varepsilon\)까지 복구하려면 \(\mathcal{O}(p,\cdot)\)에 대한 쿼리 수가 최소한:_

\[\frac{l\log_{2}(B/\varepsilon)}{\log_{2}(l)}.\]

증명: \([-B,0]\)에서 \(\infty\)-norm error \(\varepsilon\)까지의 단일 로짓 값의 정보 내용은 \(\log_{2}(B/\varepsilon)\) 구간에서 \(\varepsilon\)-간격 점 위에 균일한 이전을 가정합니다. 로짓이 독립적이므로, \(\infty\)-norm error \(\varepsilon\)까지 \(l\) logit 값으로 인코딩된 정보는 \(l\log_{2}(100/\varepsilon)\이다.

\(\mathcal{O}\)에 대한 단일 쿼리는 아무리 잘 조작되더라도 출력이 \(l\) 별개의 값 중 하나이기 때문에 최대 \(\log_{2}(l)\) 비트를 산출한다. 필요한 최소 쿼리 수는 적어도 전체 정보 내용을 쿼리당 정보로 나누어 하한 \(l\log_{2}(B/\varepsilon)/\log_{2}(l)\)을 산출한다.

한 번에 최대 \(N\) 토큰에서 바이어싱의 제한은 다음과 같은 하한을 제공합니다.

\[\frac{l\log_{2}(B/\varepsilon)}{\log_{2}(N)}\]

쿼리는 \(\log_{2}(l)/\log_{2}(N)\) 악화 요인입니다. \(N=300\) 및 \(l\ 대략 100{,}000\)의 경우, 이것은 \(2\)의 인자일 뿐이다.

\(B=100\) 및 \(N=300\)의 경우, 따라서 우리는 적어도 필요하다.

\[\frac{\log_{2}(B/\varepsilon)}{\log_{2}(N)}\approx 0.81+0.12\log_{2}(1/\varepsilon)\]

로짓당 쿼리입니다. 우리가 6자리에서 23자리 사이의 정밀도를 원한다면 하한은 로짓당 1.53에서 3.57 쿼리에 해당한다. 표 3에서 가장 좋은 로그프로브 없는 공격은 로짓당 약 1개의 쿼리만 하한보다 더 나쁘다는 것을 알 수 있다.

Lemma F.1의 주요 비현실적 가정은 로짓 값에 대한 이전이 간격에 걸쳐 i.i.d. 균일하다는 것이다. 더 나은 가정은 대부분의 로짓 값이 가벼운 꼬리 단봉 분포에서 나온다는 것이다. 우리는 향후 작업에 앞서 이를 더 잘 활용하는 보다 현실적인 하한과 공격을 남긴다.

## 부록 G 복구 \(\mathbf{W}\) 최대 직교 행렬

이 절에서는 부록 C에서와 같이 단순히 비특이적인 \(h\times h\) 행렬이 아닌 직교적인 \(h\times h\) 행렬까지 \(\mathbf{W}\)를 추출하는 알고리즘을 개략적으로 설명한다. 본 논문에서 고려한 모델에 대해 실제로 이 공격을 수행하지 않으며, 이 알고리즘을 개선하는 것은 향후 작업을 위한 열린 문제로 남긴다.

우리는 몇 가지 간단한 가정을 한다.

1. 선형성에 의해 최종 LayerNorm 가중치 \(\gamma\)를 \(\mathbf{W}\)로 병합한다.3 각주 3: 언임베딩 행렬을 다시 쓰는 이러한 방법에 대한 완전한 설명은 Gurnee 등(2024)의 부록 A.1, ‘Folding LayerNorm’을 참조한다.
2. 최종 LayerNorm 이후에 숨겨진 상태가 구 위에 있을 정도로 수치 정밀도가 높다고 가정한다.
3. 모든 쿼리 \(p\)에 대해 모든 \(g_{\theta}(p)\)를 포함하는 퇴화 하위 차원 하위 공간이 없습니다.
4. RMSNorm/LayerNorm의 \(\varepsilon\)은 \(0\)이라고 가정한다. 이것은 중요한 가정이 아니다.

다시, 질의 출력 행렬 \(\mathbf{Q}=\mathbf{U}\cdot\boldsymbol{\Sigma}\cdot\mathbf{V}^{\top}\)에 소형 SVD를 사용한다. 여기서 \(\mathbf{Q}\in\mathbb{R}^{l\times n}\), \(\mathbf{U}\in\mathbb{R}^{l\times h}\), \(\boldsymbol{\Sigma}\in\mathbb{R}^{h\times h}\), \(\mathbf{V}^{\top}\in\mathbb{R}^{h\times n}\). 점 \(g_{\theta}(p)\)은 \(\mathbb{R}^{h}\), \(\mathbf{U}^{\top}\cdot\mathbf{W}\in\mathbb{R}^{h\times h}\), \(\mathbf{U}^{\top}\cdot\mathbf{W}\cdot g_{\theta}(p)\)는 \(\mathbb{R}^{h}\)의 타원체에 놓여 있다. 앞으로는 \(x_{i}=\mathbf{U}^{\top}\cdot\mathbf{W}\cdot g_{\theta}(p_{i})\)와 \(\mathbf{W}\cdot g_{\theta}(p_{i})\를 모두 알고 있기 때문에 이 점들을 직접 계산할 수 있다는 점에 주목한다. 타원체는 일부 양의 반유한(대칭) 행렬에 대해 \(x^{\top}\mathbf{A}x=1\)에 의해 동등하게 정의됨에 따라 \(\mathbf{A}\in\mathbb{R}^{h\times h}\) 다음과 같은 문장이 함축된다:

**Lemma G.1**.: _모든 \(i\)에 대해 \(x_{i}^{\top}\mathbf{A}x_{i}=1\)이 되도록 양의 반유한 \(\mathbf{A}\in\mathbb{R}^{h\times h}\)이 있습니다._ \(\mathbf{A}\)는 양의 반무한이므로 일부 \(\mathbf{M}\)에 대해 \(\mathbf{A}=\mathbf{M}^{\top}\cdot\mathbf{M}\)을 쓸 수 있다. 다음은 주요 관찰 사항입니다.

**Lemma G.2**.: \(\mathbf{W}=\mathbf{U}\cdot\mathbf{M}^{-1}\cdot\mathbf{O}\) _ 일부 직교 행렬에 대해 \(\mathbf{O}\)._

증명: 우리는 \(g_{\theta}(p_{i})\)가 구 위에 있다는 것을 안다. 방정식 \(x_{i}^{\top}\mathbf{A}x_{i}=1\)은 \(x_{i}^{\top}\mathbf{M}^{\top}\mathbf{M}x_{i}=1\)과 동등하며, 방정식은 \(\|\mathbf{M}x_{i}\|=1\)과 동등하다. 이것은 \(\mathbf{M}x_{i}\)가 구 위에 놓여 있다는 것을 의미한다. \(\mathbf{M}x_{i}=\mathbf{M}\cdot\mathbf{U}^{\top}\cdot\mathbf{W}\cdot g_{ \theta}(p_{i})\이기 때문에, \(\mathbf{M}\cdot\mathbf{U}^{\top}\cdot\mathbf{W}\)는 점 \(g_{\theta}(p_{i})\)에 대한 규범 보존 변환이다. \(g_{\theta}(p_{i})\)가 축퇴된 저차원 부분공간에 있지 않다는 가정에 의해, \(\mathbf{M}\cdot\mathbf{U}^{\top}\cdot\mathbf{W}=:\mathbf{O}\)는 \(\mathds{R}^{h}\)의 정규분포를 보존하는 내형태이므로 직교행렬이 된다. 이것은 청구된 바와 같이 \(\mathbf{W}=\mathbf{U}\cdot\mathbf{M}^{-1}\cdot\mathbf{O}\)를 직접적으로 암시한다.

이는 \(\mathbf{W}\)를 직교행렬까지 추출하기 위해서는 모든 \(i\)에 대해 \(\|\mathbf{M}x_{i}\|=1\)를 만족하는 일부 \(\mathbf{M}\)를 복구하기에 충분하다는 것을 의미한다. \(\mathbf{M}\)을 계산하기 위해, 우리는 실제로 Lemma G.1을 만족하는 양의 반정의 \(\mathbf{A}\)을 복구할 수 있으며, 이는 SVD 또는 Cholesky 분해에 의해 실현 가능한 \(\mathbf{M}\)을 제공할 것이다.

최종 관측은 Lemma G.1(\(x_{i}^{\top}\mathbf{A}x_{i}=1\)의 모든 \(i\))에 대한 시스템이 \(h(h+1)/2\) \(\mathbf{A}\)의 별개의 항목에서 선형이라는 것이다. 따라서 \(x_{i}\)에 대해 \(h(h+1)/2\) 이상의 값을 생성함으로써(그리고 다시 축퇴가 없다고 가정하면), 우리는 큰 연립방정식을 풀 수 있고, 원리적으로 \(\mathbf{A}\), 따라서 \(\mathbf{M}\), 따라서 \(\mathbf{W}\)를 복구할 수 있다. 과잉 결정된 선형 시스템은 최대 하나의 해를 갖기 때문에 임의의 해 \(\mathbf{A}\)는 양의 반무한이 될 것이다. 그러나 우리는 모든 실험에서 \(h^{2}\) 변수에서 이러한 선형 방정식을 효율적으로 푸는 방법을 알지 못하므로 실제로 부록 C에 설명된 대로 임의의 \(h\times h\) 행렬까지 가중치를 재구성한다.

## 부록 H 양자화 및 노이즈

### Quantization

양자화는 모델의 메모리 공간을 줄이고 추론을 빠르게 하기 위한 인기 있는 전략이다. 이러한 이점 외에도 더 낮은 정밀도 수 표현을 사용하면 노이즈도 효과적으로 추가된다. 섹션 8.2에서 언급했듯이 출력 로짓에 노이즈를 추가하면 공격을 방지할 수 있다. 다음의 자연스러운 질문은 양자화가 우리의 공격을 비효율적으로 만들거나 수행하기 더 어렵게 만들기에 충분한 노이즈를 추가하느냐이다.

간단한 테스트를 위해 8-비트 및 4-비트 모두에서 Llama-7B를 양자화하고 기본 16-비트 구현과 기준 공격(섹션 4.1)을 비교한다. 우리는 HuggingFace가 모델 가중치의 out-of-the-box 양자화와 더 낮은 정밀도 추론을 지원하는 비트 및 바이트(Dettmers et al., 2022)를 사용하여 양자화한다(그림 6). 우리는 서로 다른 양자화 수준에서 유의미한 차이를 관찰하지 못했으며, 각 모델을 쿼리하면 동일한 수의 쿼리에서 동일한 임베딩 행렬 차원 \(h\)을 복구할 수 있다. 8-비트 및 4-비트 양자화가 일반적으로 성능에 큰 영향을 미치지 않는 것으로 관찰된다는 점을 감안할 때, 이는 아마도 놀라운 결과일 수 있으며, 양자화로부터의 잡음은 (우리의 공격의 맥락에서) 로짓에 의미 있는 영향을 미치지 않는 것으로 보인다.

### Noise

우리의 공격에 대한 한 가지 자연스러운 방어는 잡음을 추가하여 로짓들을 난독화하는 것이다. 이것은 유틸리티와 취약성 사이의 균형을 자연스럽게 유도합니다. 노이즈가 증가하면 유용한 출력은 감소하지만 추출 난이도는 증가합니다. 우리는 그림 5(c)에서 이 절충안을 경험적으로 측정한다. 동일한 프롬프트의 서로 다른 쿼리 간에 일관되는 로짓에 직접 추가된 노이즈를 고려한다. 이를 시뮬레이션하기 위해 복구된 로짓에 노이즈를 직접 추가하고 추출된 임베딩 행렬을 재계산한다. GPT-2의 경우 실제 임베딩 행렬과 특정 잡음 레벨로 추출된 임베딩 행렬 사이의 RMSE를 측정하며, ada와 babbage의 경우 잡음으로 추출된 가중치와 잡음이 없는 상태에서 추출된 가중치 사이의 RMSE를 측정한다. 우리는 RMSE를 측정하기 전에 모든 임베딩 행렬( \(\ell_{2}\) norm 1)을 정규화한다.

그림 6: (a, b)에서 Llama-7B에 대한 임베딩 행렬 차원 \(h\)을 서로 다른 수준의 정밀도로 복구합니다. 16비트(기본값), 8비트 및 4비트입니다. 우리는 공격과 관련하여 서로 다른 수준의 양자화에서 유의미한 차이를 관찰하지 못한다. (c)에서 추출된 임베딩 사이의 RMSE는 로짓에 추가된 가우시안 잡음의 표준 편차의 함수이다.

그림 7: 왼쪽에는 GPT-2에 대한 공격을 사용하여 추출된 특이값을 플로팅하고, 추정된 은닉 차원은 768에 가깝다. 오른쪽에는 8절에서 설명한 대로 가중치 행렬의 차원을 사후적으로 1024로 확장한다. 이는 적대자가 모델이 실제보다 넓다고 착각하게 만든다.

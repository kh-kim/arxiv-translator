# Open Language Models의 Falcon Series

팔콘 LLM Team1

에브테삼 알마즈루에 함자 알로비들리 압둘라지즈 알샨시 알레산드로 카펠리 루산드라 코요카루 메루앙 데바 에티엔 고핀 다니엘 헤슬로 줄리엔 라우네 쿠엔틴 말레스 다니엘 마조타 바도르딘 명사 세례자 파니에 길헴 페도

###### Abstract

우리는 팔콘 시리즈: 7B, 40B 및 180B 매개변수 인과 디코더 전용 모델을 소개하고 웹 데이터에서 주로 조립된 다양한 고품질 말뭉치에서 훈련한다. 가장 큰 모델인 팔콘-180B는 3조 5천억 개 이상의 텍스트 토큰에 대해 훈련되었으며, 공개적으로 문서화된 가장 큰 사전 훈련 실행이다. Falcon-180B는 PaLM 또는 Chinchilla와 같은 모델보다 훨씬 우수하고 LLaMA 2 또는 Inflection-1과 같은 동시에 개발된 모델에 비해 개선된다. GPT-4 및 PaLM-2-Large와 함께 세계 3대 언어 모델 중 하나인 PaLM-2-Large의 사전 훈련 및 추론 비용을 줄임으로써 PaLM-2-Large의 성능에 가깝다. 우리는 상세한 평가와 팔콘을 사전 훈련하는 데 사용되는 방법 및 사용자 지정 도구를 자세히 보고한다. 특히, 제한된 상호 연결이 있는 클라우드 AWS 인프라의 최대 4,096 A100s에서 이러한 모델을 효율적으로 사전 훈련할 수 있도록 맞춤형 분산 훈련 코드 베이스에 대해 보고한다. 웹 데이터 세트의 600B 토큰 추출물과 팔콘-7/40/180B 모델을 공개하여 개방 과학을 육성하고 대규모 언어 모델의 개방형 생태계 개발을 가속화한다.

각주 1: 알파벳 순으로 나열된 저자들, 부록 A에 상세히 기재된 기여들. falconllm@tii.ae에 대한 대응

[https://huggingface.co/tiiuae/](https://huggingface.co/tiiuae/)

그림 1: **팔콘 시리즈 모델은 경쟁적 성능을 달성하며, 팔콘-180B는 PaLM-2 Large의 1샷 성능과 거의 일치합니다. 1-shot performance of PaLM (Chowdhery et al., 2022), PaLM-2 (Anil et al., 2023), and Falcon-180B, on a set of tasks from Brown et al.(2020). 이러한 평가 결과는 우리 평가의 작은 스냅샷일 뿐이며 GPT-3.5/4, LLaMA-1/2 및 Inflection-1과의 세부 정보 및 비교는 섹션 6을 참조하세요. **소개**

언어 모델의 지속적인 캄브리아기 폭발은 주로 인기 있는 트랜스포머 기반 레시피의 고유한 확장성(scalability)에 의해 촉발되었다. 이러한 확장성은 여러 축에 걸쳐 나타납니다.

* **성능 확장성** (및 예측 가능성)입니다. 컴퓨팅 예산을 체계적으로 사전 트레이닝하는 증가는 일관되고 예측가능한 방식으로 언어 모델링 능력의 개선을 산출한다(Kaplan et al., 2020). Falcon-180B는 Hoffmann 등(2022)의 업데이트된 스케일링 법칙 권장 사항을 따르는 최초의 공개 문서화된 GPT-3 크기 모델이며, 업샘플링 없이 총 사전 훈련 길이가 3,500억 토큰이다.
* 데이터 확장성입니다. 사전 훈련을 효율적으로 확장하고 사전 훈련과 추론 계산을 분리하려면 점점 더 큰 모델이 더 큰 말뭉치에서 더 오래 훈련되어야 한다. 팔콘 시리즈를 유지하기 위해 5조 토큰의 고품질 필터링 및 중복 제거 웹 데이터 세트인 RefinedWeb(Penedo et al., 2023)을 개발했으며, 이는 공개적으로 문서화된 최대 규모이다.
* **하드웨어 확장성.** Transformer 모델 (Vaswani et al., 2017)은 자연적으로 현대 GEMM 최적화 된 하드웨어에 적합 하 여 학습 및 추론이 많은 수의 가속기에 걸쳐 효율적으로 배포 될 수 있습니다 (Narayanan et al., 2021; Pope et al., 2023). Falcon-180B를 사용하여 비용 효율적인 AWS 클라우드 인프라에서 가속기당 50Gbps 인터커넥트만으로 4,096 A100 40GB로 스케일업 사전 훈련을 시연한다.

이러한 펀더멘털을 기반으로, 점점 더 큰 언어 모델은 소위 창발적 능력을 발생시킨다(Wei 등, 2022). 이러한 능력들은 인간 선호도에 더 맞춤화되어, 명령어-추종 또는 채팅 모델들을 구축할 수 있다(Ouyang et al., 2022). 모두 함께 이러한 방법들은 ChatGPT(GPT-3.5/4, OpenAI(2023)), Claude, Bard(PaLM-2, Anil et al. (2023)), 또는 Pi(Inflection-1, Inflection(2023))와 같은 고객 대면 애플리케이션들에서 큰 언어 모델들의 광범위한 배포를 야기한다. 이 논문에서는 주로 팔콘 시리즈 모델의 사전 훈련에만 대해 보고하고 추가 다운스트림 피니튜닝 및 정렬을 향후 작업에 맡긴다.

Falcon 시리즈는 최대 4,096 A100에서 훈련된 3개의 인과적 디코더 전용 모델로 구성된다. 우리는 RefinedWeb(Penedo et al., 2023)에 대한 우리의 작업에서 주로 공급되는 3,500억 토큰의 사전 훈련 데이터 세트를 조립했다. 모델의 아키텍처는 PaLM(Chowdhery 등, 2022)을 기반으로 하지만 각 결정을 독립적으로 검증하여 궁극적으로 세부 사항에 대해 섹션 4를 참조하세요. Falcon 시리즈는 2022년 8월 개발이 시작된 광범위한 커스텀 툴링(예: 사전 훈련 코드 베이스, 데이터 파이프라인)을 활용하여 2022년 12월 모델의 훈련을 시작했다. 심층 평가를 통해 Falcon 시리즈는 규모에 걸쳐 경쟁력이 있으며 Falcon-180B는 PaLM-2 Large의 성능에 근접하여 최상의 개방형 모델 및 최고의 언어 모델 상위 3위에 위치함을 알 수 있다.

이 논문과 팔콘 시리즈를 통해 다음과 같은 기여를 한다.

* **대규모 모델의 사전 훈련에 대한 공개 문서입니다.* * 최신 최신 모델은 거의 문서화되지 않아 해당 분야의 추가 연구 및 진행을 방해합니다. 이러한 작업과 달리 우리는 팔콘 시리즈의 사전 훈련을 광범위하게 문서화한다.
* **데이터 및 모델 열기** 연구를 가속화하고 대규모 언어 모델의 커뮤니티 기반 개선을 사용하도록 설정하기 위해 Falcon-7/40/180B 및 RefinedWeb 데이터 세트의 6000억 토큰 추출물([https://huggingface.co/tiiuae/](https://huggingface.co/tiiuae/)을 공개합니다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **Falcon-7B** & **Falcon-40B** & **Falcon-180B** \\ \hline
**Pretraining** [tokens] & 1,500B & 1,000B & 3,500B \\
**Compute** [PF-days] & 730 & 2,800 & 43,500 \\
**Training** [A100s] & 384 & 384 & 4,096 \\
**가용성** & Apache 2.0 & Apache 2.0 & 책임 있는 사용 라이선스 \\
**Agg**. performance** (Section 6.5 & 60.8 & 67.1 & 70.3 \\
**Closest model** & \textless{}GPT-3 & Chinchilla & PaLM-2 Large \\ \hline \hline \end{tabular}
\end{table}
표 1: **팔콘 시리즈 모델은 대규모 웹 데이터에 의해 사용하도록 설정된 광범위한 기능 및 추론 요구 사항을 포함합니다. Falcon-7B는 소비자 하드웨어 (예: Apple M2)에서 효율적으로 실행할 수 있는 반면 Falcon-180B는 일반적으로 전용 추론 인프라 (예: \(8\times\text{A100}\) 80GB)를 요구 합니다. 전체 팔콘 시리즈에서 꾸준한 제로 샷 성능 향상을 보고합니다.* *

###### Contents

* 1 소개
* 2 State-of-the-art: 언어 모델링에서 프론티어 모델까지
* 3 디자인 철학
* 4 데이터, 아키텍처 및 하이퍼파라미터에 대한 실험 및 동기
	* 4.1 소규모 실험을 위한 설정
	* 4.2 데이터: 웹 대 큐레이트, 코드 및 다국어성이 영어 성능에 미치는 영향
		* 4.2.1 Web data alone can outper outperated corpora
		* 4.2.2 강력한 웹 베이스라인에 대항하여, 큐레이트된 데이터는 심지어 해로울 수 있다
		* 4.2.3 제한 코드 및 다국어 데이터는 영어 성능을 크게 저하시키지 않음
	* 4.3 아키텍처 및 사전 훈련: 인기 있는 레시피 및 추론 최적화 유효성 검사
		* 4.3.1 텐서 병렬 훈련 및 추론을 위한 멀티그룹으로의 멀티쿼리 확장
		* 4.3.2 로터리 위치nal 임베딩은 ALBi에 대해 제한된 에지만을 제공할 수 있음
		* 4.3.3 GLU의 추가 메모리 비용은 비용 효율적인 교육을 위해 가치가 없을 수 있음
		* 4.3.4 작은 수정은 확장성에 도움이 됩니다. 병렬 계층 및 선형 계층에 편향이 없음
		* 4.3.5 Hyperparameters에 대한 유효성 검사 모범 사례: z-loss, weight decay, LR 검색
	* 4.4 추가 실험이 필요: 절단을 만들지 않은 아이디어
	* 4.5 Wrapping-it up: 전체 데이터 세트 및 아키텍처 레시피의 유효성 검사
* 5 구현
	* 5.1 Falcon 데이터 세트: 주로 웹, 큐레이트 및 대화 데이터가 추가됨
		* 5.1.1 Macrodata Refinement 파이프라인 및 RefinedWeb 데이터 세트
		* 5.1.2 Microdata 큐레이션된 말뭉치 및 대화형 마스킹
	* 5.2 효율적인 추론 및 (안정적인) 훈련을 위한 팔콘 아키텍처 및 레시피
		* 5.2.1 Architectural nitpicks: separate layer norms, tied embeddings and scaling-up
		* 5.2.2 Large language model alchemy: preraining을 위한 hyperparameters
	* 5.3 Gigatron을 사용한 클라우드 인프라에 대한 대규모 분산 교육
		* 5.3.1 세밀한 제어를 위한 3D 병렬성과 확장성을 위한 ZeRO의 결합
		* 5.3.2 전용 Triton 커널을 사용한 최신 처리량
		* 5.3.3 단일층으로 구현된 선택적 재전산을 통한 효율적인 메모리 사용
		* 5.3.4 수치 정밀도: bfloat16만 있으면 됩니다.
		* 5.3.5 향상된 유연성 및 신뢰성을 위한 삶의 질 기능
	* 5.4 실행 관리: 대규모 인프라 실행 유지
* 6 결과
	* 6.1 프롬프트 또는 프롬프트 안 함: 코드베이스 간 평가 비교
	* 6.2 자연어 작업 집계에서 PaLM과의 비교
	* 6.3 제한된 작업 세트에 대한 GPT-3.5 및 GPT-4와의 비교
상식, 질의응답, 코드 태스크에 대한 최신 비교 6.5 EleutherAI 평가 Harness를 이용한 다른 모델과의 비교
* 7 제약사항
	* 7.1 본 연구진의 발견 및 삭제의 한계
	* 7.2 팔콘 모델의 한계
* 8 결론
* 기여도
* B Acknowledgements
* C 모델 카드
* Datasheet
* E 문서화되지 않은 모델과의 비교
* F 의사코드 샘플
* F.1 모든 대역폭/레이턴시를 효율적으로 측정하기 위한 측정 계획
* F.2 트리 토큰 깊이를 주의 마스크로 변환:
* F.3 Zero-1 의사코드
* G Prompts

최신 기술: 언어 모델링에서 프론티어 모델까지

우리는 이 절에서 팔콘 시리즈에 인접한 일반적인 경향과 작업에 대한 개요를 제공한다. 개별 기술 구성요소에 대한 자세한 문헌 검토는 관련 섹션을 참조하십시오.

**언어 모델링.** 코퍼스/작업별 접근 방식을 넘어 최초의 대규모 벡터 기반 단어 임베딩 방법(Mikolov et al., 2013; Pennington et al., 2014)은 대규모 비정형 텍스트 코퍼스에서 비지도 학습을 개척했습니다. 딥 리커런트 뉴럴 아키텍처의 통합은 모델이 다의성을 다루고 상황 정보를 통합할 수 있도록 했다(Peters et al., 2018); 전이 학습 패러다임의 등장까지, 피니튜닝을 통해 다운스트림 작업에 특화된 보편적 모델을 활용했다(Howard and Ruder, 2018). 현재 사용되는 첫 번째 원칙의 많은 존재에도 불구하고 초기 스케일링 시도(Jozefowicz et al., 2016)는 부분적으로 반복 접근법의 일반적인 하드웨어에 대한 까다롭고 확장성이 좋지 않기 때문에 혼합된 성공만 있었다.

**트랜스포머 모델.** 주의 기반 Transformer 아키텍처 Vaswani 등 (2017)의 도입은 효율적인 일반주의 모델을 만들기 위한 레시피 수의 폭발을 일으켰습니다. 임베딩 및 분류 집중 인코더 전용 BERT(Kenton and Toutanova, 2019), 인과 디코더 전용 GPT(Radford 등, 2018). 구체적으로, GPT-2(Radford et al., 2019)는 독창적인 소수의 샷 일반화 능력을 대중화한 최초의 일련의 모델로서, 모델이 문맥 내 지시 및 시연으로부터 간단히 임의의 작업을 이해하고 수행할 수 있게 하였다.

**대형 언어 모델.** 앞서 언급한 작업은 현재 모델에 주요 구성 요소를 배치했으며 마지막 구성 요소인 스케일링은 GPT-3(Brown et al., 2020)에 의해 입증되었으며 스케일링 법칙 개요(Kaplan et al., 2020)에 의해 설명되었습니다. 점점 더 많은 양의 계산이 모델을 사전 훈련하는 데 소비됨에 따라 언어 모델링 성능에서 상응하는 이득이 만들어진다. 더 능력 있는 언어 모델에 대한 체계적이고 직접적인 경로에 대한 탄탈화 전망은 "스케일링 광풍"으로 이어진다. 첫째, GPT-3을 쥬라기-1(리버 등, 2021) 또는 판구-알파(젠 등, 2021)로 재현하고 GPT-J(왕과 코마츠자키, 2021), OPT(장 등, 2022) 또는 BLOOM(스코 등, 2022)과 같은 개방적인 노력으로; 둘째, 고퍼(래 등, 2021), MT-NLG(스미스 등, 2022) 또는 PaLM(초우더리 등, 2022)과 함께 스케일링의 한계를 더욱 밀어붙이는 작업으로. 대규모 언어 모델의 개발 및 채택은 사전 훈련 방법의 개선으로 이어진다. 특히, Hoffmann 등(2022)은 Chinchilla를 통해 최적의 스케일링이 실제로 모델 크기 및 사전 트레이닝 데이터세트를 공동으로 증가시켜야 한다는 것을 입증하였다. 실제 세계에서의 배치를 위해, 훈련 및 추론 계산을 분리하고 서빙 비용을 줄이기 위해, 소위 최적인 것을 훨씬 지나도록 훈련하는 것이 바람직할 수도 있다. 이것은 LLaMA 모델들(Touvron et al., 2023, 2023)로 예시되며, 최대 2조 개의 토큰들에 대해 트레이닝된 7B/13B/30B/70B 파라미터 모델들을 포함한다.

**프론티어 모델.** 이 작업과 동시에 "가장 진보된 기존 모델에 현재 존재하는 기능을 초과하고 다양한 작업을 수행할 수 있는 대규모 머신 러닝 모델"의 전환 정의 아래 소위 프론티어 모델이 등장했습니다. 비록 움직이는 목표이지만, 우리는 GPT-4(OpenAI, 2023)와 PaLM-2(Anil et al., 2023)에 대한 최근 작업을 이 범주에 대한 초기 기여로 돌린다. 이들은 상당히 증가된 컴퓨팅 예산과 향상된 기능을 통해 눈에 띕니다. 미등록 모델에 대한 접근법에 대한 자세한 내용은 부록 E를 참조한다.

그림 2: 열린 모델은 사전 훈련 계산에서 닫힌 모델보다 늦지만(\(\sim\)18개월) 격차가 벌어지지 않습니다. 1년 후, GPT-3는 수많은 LLM의 출현과 함께 "스케일링 광풍"을 촉발했다. \(>\)10,000PF-일 범위의 모델은 오픈 소스의 경우 드물게 남아 있다.

Design philosophy

쓴 교훈(서튼, 2019)에서 영감을 받은 우리는 계산을 가장 잘 활용하는 확장 가능한 방법이 궁극적으로 가장 효과적이라고 믿는다. 따라서 팔콘 시리즈 모델을 설계할 때 성능, 데이터 및 하드웨어의 세 축에 걸쳐 주로 **확장성** 에 초점을 맞췄습니다.

**성능 확장성.** 대규모 언어 모델 규모의 지속적인 증가입니다 (그림). 2) 주로 소위 스케일링 법칙들에 의해 동기 부여되어 왔다 : 증가된 사전 트레이닝 컴퓨팅은 언어 모델링 능력들의 상응하는 개선들을 수반한다(Hestness et al., 2017; Kaplan et al., 2020). 모델 개선을 위한 이러한 체계적인 경로는 간헐적인 연구 깨달음이 패러다임 전환 방법을 나타내기를 기다리는 것보다 훨씬 더 신뢰할 수 있는 것으로 입증되었다. 그러나 업스트림과 다운스트림 성능은 단순히 스케일링의 동기가 아니라 강력한 동인이기도 하다. 실제로, 모델링 개입의 영향(예: 아키텍처 수정, 데이터 소싱, 하이퍼파라미터 선택)을 정량화하는 것은 소규모 절제에 의해 제공되는 피드백 루프를 유지하는 데 중요하다. 그러나, 정량화해야 할 _what_ 의 질문은 사소하지 않다: 업스트림 성능만으로는 다운스트림 태스크와 상충될 수 있고(Tay 등, 2021), 심지어 다운스트림 메트릭도 인간의 선호도와 정렬되지 않을 수 있다(Stiennon 등, 2020). 이는 사전 훈련 목표(즉, 주로 웹 기반 말뭉치에서 다음 단어를 예측함)와 일반적인 다운스트림 사용(즉, 도움이 되고, 무해하며, 솔직한 방식으로 사용자의 지시를 따르함) 사이의 근본적인 대조에 의해 훨씬 더 어렵게 된다(Bai 등, 2022). Falcon 모델의 사전 훈련 단계에 초점을 맞추므로 EleutherAI Harness(Gao 등, 2021)를 사용하여 Le Scao 등(2022); Wang 등(2022)의 설정과 유사한 **자연어 작업의 큰 집합에 대한 0/few-shot 일반화** 측정에 평가를 중점을 둡니다. 우리는 다른 최첨단 모델과의 비교를 가능하게 하는 집계체를 구축하지만 원칙적인 비교를 실행하는 데 어려움이 있다. 우리는 평가 설정의 섹션 7 제한 사항에 대해 논의한다.

데이터 확장성 사전 훈련 컴퓨팅 예산의 증가는 더 큰 모델을 사용 하거나 더 긴 훈련을 위해 사용할 수 있습니다. Kaplan et al.(2020)은 먼저 최적의 스케일링이 대부분 모델 크기 구동임을 발견했지만 Hoffmann et al.(2022)은 이 발견을 수정하고 관절 스케일링이 바람직하다는 것을 발견했다. 3을 사용하여 임팩트를 확인한다. 또한, 모델 크기가 커지면 추론 부담이 증가하며, 사전 학습 데이터 세트의 크기가 커지면 추론 비용과 분리된다. 최근의 개방형 모델들은 최대 2조 개의 토큰들의 데이터 세트들에 대해 트레이닝되었다(Touvron et al., 2023); 순진한 반복이 모델을 열화시킬 위험이 있기 때문에(Hernandez et al., 2022; Muennighoff et al., 2023), 이것은 데이터 스케일링의 지속 가능성에 대한 우려로 이어졌다(Villalobos et al., 2022). 이러한 우려는 최첨단 모델을 구축하기 위해 큐레이션된 코포라가 필요하다는 널리 알려진 믿음으로 인해 악화되며, 아르시브 논문, 책 등과 같은 개별 소스의 수동 추가가 필요하다(Brown et al., 2020; Gao et al., 2020). 팔콘 시리즈 모델의 경우 **엄격한 필터링 및 중복 제거를 통해 고품질 웹 데이터 크기 조정** 에 중점을 두어 5,000억 토큰의 영어 웹 데이터 세트를 수집하고 학습 중에 데이터를 반복하지 않도록 합니다. 우리는 Penedo 등(2023)에서 이 작업에 대해 광범위하게 보고하지만 이 문서에서 몇 가지 핵심 요소를 제공한다.

**하드웨어 확장성.** 대규모 교육에는 수천 개의 하드웨어 가속기가 동시에 효율적으로 작동해야 합니다. 이러한 가속기를 최대한 활용하려면 차례로 원칙적인 분산 교육 방법이 필요합니다 (Shoeybi 등, 2019). 효율적으로 가장 잘 실행될 수 있고 대규모 계산을 레버리지할 수 있는 방법은 종종 트랜스포머 아키텍처 자체에 의해 가장 잘 입증된 바와 같이 커뮤니티에서 가장 많은 견인력을 얻는 방법이다(후커, 2021). 또한, 예를 들어 데이터의 영향(Le Scao et al., 2022)에 비해 모델의 작업 성능을 크게 향상시키는 아키텍처 개선 사항을 찾기 어렵다. 따라서 **아키텍처 결정은 작업 성능을 개선하는 것이 아니라 하드웨어 확장성 및 처리량을 개선하는 데 중점을 둡니다. 우리는 또한 추론 확장성(Pope 등, 2023)에 대해 우려하며, (개정) 멀티쿼리 어텐션 스킴(Shazeer, 2019)과 같은 트윗의 채택으로 이어진다.

마지막으로 확장성을 넘어 **비용 효율성** 과 협력 하 고 **검증 된 접근 방식** 에 의존 합니다. 최적화 샤딩(Rajbhandari et al., 2020)과 결합된 3D 병렬화 전략(Narayanan et al., 2021)을 다시 구현하여 제한된 상호 연결로 보다 비용 효율적인 클라우드 AWS 인프라에서 실행할 수 있다. 또한 메모리 절약 방법에 중점을 두어 더 저렴한 40GB A100에서 실행할 수 있다. 데이터 전처리와 파이프라인 사전 훈련을 처음부터 다시 구현하면 알려지지 않은 외부 코드 베이스에 의존하지 않고 광범위하게 검증할 수 있다. 또한 상태 공간 모델(Fu et al., 2022)과 같은 대규모 언어 모델의 고전적인 설계에서 급진적인 이탈을 탐구하지 않는데, 이는 일반적으로 규모에서 입증되지 않았기 때문이다.

데이터, 아키텍처 및 하이퍼파라미터에 대한 실험 및 동기 부여

먼저 권장 관행을 검증하고 흥미로운 조정을 식별하기 위해 1B-3B 매개변수 범위의 모델을 사용한 일련의 소규모 실험에 초점을 맞춘다. 또한 Penedo 등(2023)에 보고된 웹 데이터 파이프라인을 검증하기 위해 광범위한 실험을 수행했다. 각 관심 주제와 함께 우리는 또한 이 방향을 탐구하기 위한 일반적인 관행과 동기를 요약한다.

### 소규모 실험을 위한 설정

**소형 모델.** 이러한 삭제의 경우 제한된 컴퓨팅 비용으로 빠르게 반복할 수 있습니다. 이를 통해 각각 300억/600억 개의 매개변수에 대해 30억 개의 매개변수 모델을 훈련할 수 있다. 우리는 Hoffmann 등(2022)의 최적성 체제를 설명하기 위해 이러한 짧은 훈련 길이를 선택한다. 우리는 참조 아키텍처 및 하이퍼파라미터들을 GPT-3에 대해 설명된 것(Brown et al., 2020)에 기초하고, ALiBi 위치 임베딩을 우리의 베이스라인으로 사용하는 것을 주의한다(Press et al., 2022). 합리적인 리소스(32-64 A100)를 사용하면 이러한 절제 모델을 밤새 또는 며칠 내에 훈련할 수 있어 신속한 반복이 가능하다. 더 작은 모델의 사용에 대한 주의 사항은 더 큰 모델의 일부 동작을 설명하지 못할 수 있다는 점에 주목한다. 예를 들어, Dettmers 등(2022)은 이상치 특징이 6B 척도에서 나타나 양자화에 영향을 미친다는 것을 발견했으며, 데이터 복제 및 암기에 대한 우려는 또한 더 큰 모델에 불균형적으로 영향을 미치는 것으로 나타났다(Carlini 등, 2022; Hernandez 등, 2022).

**전용 집계** 작은 모델이 빠른 반복을 사용 하도록 설정 하지만 제로 샷 기능도 제한 됩니다. 순진한 대량 평가로 인해 대부분의 작업이 무작위 기준선에 가깝고 평가에서 상당한 노이즈가 발생 합니다. 더 파일의 하위 집합에 대해 훈련된 모델을 사용하여 소규모에서 합리적인 성능과 실행 전반에 걸쳐 제한된 변동성을 모두 보여주는 작업을 확인했다. 서로 다른 시드(아키텍처 실험을 위해 이 \(\sigma\)를 사용)를 사용하는 5개의 실행과 서로 다른 데이터 하위 집합과 셔플링(데이터 실험을 위해)을 사용하는 10개의 실행에서 50개 이상의 작업에 대한 분산과 평균 성능을 독립적으로 정량화했다. 이를 바탕으로 Brown et al. (2020), Le Scao et al. (2022), Srivastava et al. (2023)의 평가 셋업(zs-main/data/web)에서 11개의 태스크를 추출하였다. 이 세 부분 집합 간의 차이는 대부분 시간과 팀 간의 다른 관행 때문이다. zs-comp는 일반적으로 보고된 작업을 기반으로 다른 모델과의 비교를 위한 주요 하위 집합이다. 또한, 아키텍처에 대한 The Pile (Gao et al., 2020) (ppl-pile)에 대한 복잡도(데이터 실험의 경우, The Pile에 대한 복잡도는 대부분 콘텐츠보다는 포맷팅의 차이를 나타내는 것으로 나타났다) 및 낮은 분산(zs-small)을 갖는 3개의 NLP 태스크의 제한된 서브세트에 대해 보고한다. 소규모 평가를 위해 EleutherAI 하네스(Gao et al., 2021)와 BigBench(Srivastava et al., 2023)를 모두 사용하지만, 우리의 추론 및 평가 코드베이스는 이 설정과 섹션 6에 보고한 최종 결과 간에 크게 다르기 때문에 결과는 직접 비교할 수 없다. 우리는 표 2에 집합체의 개요를 제시한다.

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline
**Tasks** & **Type** & **Random** & main & comp & data & web & core & pile \\ \hline LAMBADA (Papemo et al., 2016) & Reading Comprehension & 0.0 & \(\checkmark\) & & & & \(\checkmark\) & \\ RACE (Lai et al., 2017) & Reading Comprehension & 25.0 & \(\checkmark\) & & & & & \(\checkmark\) & \\ HellaSwag (Zellers et al., 2019) & Common Sense & 25.0 & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \\ Winogomez (Sakaguchi et al., 2019) & Common Sense & 50.0 & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \\ PIQA (Risk et al., 2020) & Common Sense & 50.0 & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \\ BooIQ (Clark et al., 2019) & Common Sense & 50.0 & \(\checkmark\) & & \(\checkmark\) & & \\ COPA (Gordon et al., 2012) & Common Sense & 50.0 & \(\checkmark\) & & \(\checkmark\) & & \\ Date (Srivastava et al., 2023) & Common Sense & 25.0 & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \\ ARC (Clark et al., 2018) & Question Answering & 25.0 & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) & \\ OpenBookQA (Milwolov et al., 2018) & Question Answering & 25.0 & \(\checkmark\) & & \(\checkmark\) & & \\ SciQ (Johannes Wbelt, 2017) & Question Answering & 25.0 & \(\checkmark\) & & \(\checkmark\) & \(\checkmark\) & \\ The Pile (Gao et al., 2020) & Language Modeling & & & & & & \(\checkmark\) \\ \hline \hline \end{tabular}
\end{table}
표 2: **삭제에 사용되는 작은 모델(30/60B 토큰에서 학습된 1/3B 모델)을 평가하기 위해 제로 샷 성능 및 복잡성을 측정하기 위해 11개 작업에 걸쳐 4개의 집계체를 구축합니다.* * 이 규모에서 랜덤 및 낮은 가변성보다 성능이 우수한 작업을 식별하기 위해 파일 및 랜덤 시드에 대해 15개의 참조 모델을 학습했습니다. 모든 평가는 BigBench(Srivastava et al., 2023)에서 가져온 날짜를 제외하고 EAI Harness(Gao et al., 2021)를 활용한다. 주로 아키텍처 및 하이퍼파라미터 제거, 데이터 혼합 실험을 위한 데이터, 웹 데이터에 대한 소규모 삭제를 위한 웹, 감소된 작업 수에 대한 낮은 분산을 위한 코어를 구축했다. 이 설정은 자연어 능력만 포함합니다. 메인, 데이터 및 웹의 경우 차이는 대부분 실험 당시 개인의 선호도에 기인한다.

### 데이터: 웹 대 큐레이트, 코드 및 다국어성이 영어 성능에 미치는 영향

#### 4.2.1 Web data alone can outperated corpora

주로 웹 데이터에 대한 훈련, 처리 파이프라인의 세부사항 및 광범위한 평가에 대한 우리의 동기는 우리의 전용 RefinedWeb 논문(Penedo et al., 2023)에 자세히 설명되어 있다. 이 섹션에서는 웹 데이터에 노력을 집중하기로 한 결정을 안내하는 주요 삭제 사항만 강조합니다.

**배경.** 더 간단하고 더 얕은 통계 언어 모델 (Shannon, 1951; Mikolov 등, 2013)을 사용 한 이후로 자연 언어 처리는 오랫동안 비구조화된 대규모 텍스트 말뭉치를 활용했습니다. 이러한 말뭉치가 처음에 "문장-와이즈"로 구축되었다면(Chelba et al., 2013), 보다 진보된 아키텍처의 출현은 모델들이 통일된 문서들에 존재하는 긴 컨텍스트 정보를 가장 잘 사용할 수 있게 하였다(Devlin et al., 2018; Radford et al., 2018). Wikipedia 또는 BookCorpus(Zhu 등, 2015)와 같은 단일 도메인 소스로부터 시작하여, 모델들과 함께 스케일링된 데이터 세트들, 및 대규모 웹-스크랩들이 보급되었다(Ortiz Suarez 등, 2019; Raffel 등, 2019). 그러나, 웹 데이터만으로는 수행자 모델을 구축하기에는 부족하다는 것이 널리 알려져 있다(Brown et al., 2020; Gao et al., 2020). 따라서, 대규모 언어 모델들은 대규모 웹 데이터 둘 다를 조합하여 혼합 코포라 상에서 트레이닝하고, 소위 "고품질" 개별 소스들(예를 들어, 책들, 기술 논문들, 소셜 미디어 대화들)을 큐레이팅한다 - 일반적인 사전 트레이닝 믹스들의 개요는 표 3을 참조한다.

그러나 섹션 3의 데이터 확장성 논의에서 설명한 바와 같이 현대 언어 모델을 사전 훈련하는 데 필요한 수조 개의 토큰을 소싱하는 것은 어려울 수 있다. 이것은 큐레이션된 데이터가 웹 데이터보다 근본적으로 낫다는 생각에 도전하게 한다. 특히 Lee et al.(2022)의 작업을 기반으로 Rae et al.(2021)에서 영감을 받은 엄격한 중복제거와 광범위한 필터링이 웹 데이터 단독으로 수행 모델을 훈련시킬 수 있는 방법을 연구한다.

**질문.** 웹 데이터 단독(필터링 및 중복 제거 포함)을 사용하여 자연어 제로 샷 성능으로 측정한 큐레이트된 데이터에 대해 학습된 모델보다 성능이 우수한 모델을 훈련할 수 있습니까?

**방법.** 27/60B 토큰, 관심 있는 데이터 세트 및 데이터 파이프라인의 중간 아티팩트에 대해 1/3B 매개 변수 모델을 학습합니다. 최첨단 웹 데이터 세트의 경우, OSCAR(Ortiz Suarez et al., 2019; Abadji et al., 2022) 및 C4(Raffel et al., 2019)의 두 가지 버전을 고려한다. 큐레이티드 데이터 세트의 경우 가장 인기 있는 사전 집계 데이터 세트인 The Pile(Gao et al., 2020)을 고려하며, 많은 모델도 사전 훈련 데이터를 The Pile의 특정 구성 요소에 기반하도록 선택했다(Smith et al., 2022; Zhang et al., 2022). RW-Raw는 텍스트 추출 직후 가장 적은 양의 필터링으로 파이프라인의 출력에 해당하지만 알려져 있는 성인 콘텐츠에 대한 URL 차단 리스트뿐만 아니라 영어 식별이 적용됨; RW-Filtered는 Rae 등(2021)이 사용한 것과 유사한 첫 번째 라운드의 휴리스틱을 적용하며, 마지막으로 RefinedWeb은 두 단계로 중복 제거를 적용한 최종 웹 데이터 세트에 해당한다. zs-웹 집합체의 모든 모델을 평가하며 구성에 대한 자세한 내용은 표 2를 참조한다.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & **Web** & **Curated web** & **Curated** & & & & & **Total** \\  & & & & Books & Tech. & Code & Conv. & Epochs & \\ \hline
**GPT-3** & 60 \% & 22 \% & 18 \% & 16 \% & 2 \% & 0 \% & 0 \% & 2.4 & 300B \\
**말뚝** & 18 \% & 10 \% & 72 \% & 15 \% & 40 \% & 7 \% & 10 \% & 1.8 & 340B \\
**MT-NLG** & 38 \% & 29 \% & 33 \% & 16 \% & 9 \% & 2 \% & 6 \% & 2 & 270B \\
**고퍼** & 58 \% & 10 \% & 32 \% & 27 \% & 2 \% & 3 \% & 0 \% & \(\sim\)1 & 300B \\
**LaMDA & 25 \% & 0 \% & 75 \% & 0 \% & 25 \% & 0 \% & 50 \% & & 340B \\
**PaLM** & 27 \% & 1 \% & 72 \% & 13 \% & 4 \% & 5 \% & 50 \% & & 780B \\ \hline
**Chinchilla** & 55 \% & 10 \% & 35 \% & 30 \% & 1 \% & 4 \% & 0 \% & 1.1 & 1400B \\
**LLaMA** & 82 \% & 0 \% & 18 \% & 5 \% & 7 \% & 4 \% & 2 \% & 1.7 & 1400B \\\
**Falcon** & 84 \% & 0 \% & 16 \% & 6 \% & 2 \% & 3 \% & 5 \% & 1 & 3500B \\ \hline \hline \end{tabular}
\end{table}
표 3: Hoffmann 등(2022)의 권고에 따라, 사전 트레이닝 데이터 세트는 크기가 증가되어 웹 데이터의 보급의 증가를 야기하였다. 웹 데이터 소스는 CommonCrawl에서 조달된 C4(Raffel et al., 2019)와 같은 대규모 웹 스크래프이다. 큐레이트된 웹 데이터는 타겟팅된 도메인 필터링을 거치게 되는데, 이는 예를 들어 CC-News(Hamborg 등, 2017)를 포함한다. 아르시브, 위키피디아 또는 PubMed와 같은 출처를 기술 큐레이트 데이터로 간주하고 레딧, 해커뉴스 또는 스택오버플로우와 같은 출처를 대화로 간주한다. 전체 데이터 세트의 크기가 아니라 사전 훈련에 사용되는 토큰의 양을 보고한다. LaMDA(Thoppilan et al., 2022)의 경우, 대략적인 카운트만이 논문에 제공됨에 따라 숫자가 대략적으로 추정된다.

**결과.** 이 라운드의 실험에 대한 결과는 표 4에 나와 있습니다. 예상과 일치 하 여 원시 웹 데이터 (RW-Raw)가 제대로 수행 되지 않는다는 것을 발견 합니다. 마찬가지로 OSCAR-22.01은 모든 데이터 세트의 최악의 성능을 제공 합니다. 이는 제작자가 중복 제거를 적용하지 않고 기본적으로 배포하기로 선택했기 때문일 수 있습니다. 반대로 OSCAR-21.09와 C4는 모두 강력한 기준선이다. 특히 더 파일은 웹 데이터보다 더 나은 성능을 제공하지 않을 가능성이 매우 높다는 것을 발견했다.

파이프라인의 후속 단계는 데이터 세트의 품질과 이에 대해 훈련된 모델의 성능을 크게 향상시킨다. 필터링만으로도 더 파일과의 격차를 줄일 수 있는 반면 엄격한 중복 제거를 추가하면 우리가 벤치마킹한 데이터 세트 중 RefinedWeb이 최고의 데이터 세트가 될 수 있다.

우리는 이러한 실험의 두 가지 제한 사항에 주목한다. 첫째, 모델은 작고 제한된 양의 데이터에 대해 학습됩니다. 그러나, 더 큰 모델이 중복에 더 민감하고(Hernandez et al., 2022), 개별 샘플을 기억할 가능성이 더 높기 때문에, 중복 제거로부터의 이득은 모델 스케일에 따라 실제로 증가할 가능성이 있다(Carlini et al., 2022). 둘째, 우리의 평가는 자연어 과제에 중점을 둡니다. 더 파일(The Pile)이 코드를 명시적으로 포함하고 있는 반면, 대규모 웹 스크래프는 일부 부수적인 경우를 제외하고는 대부분 없는 경우가 많기 때문에, 웹 데이터만으로 학습된 모델이 코드 태스크에 대해 더 파일(The Pile)로 학습된 모델과 유리하게 비교될 가능성은 거의 없다.

\begin{table}
\begin{tabular}{|l|} \hline
**Finding.** Challenging beliefs on data quality, filtered and deduplicated web data _alone_ allows \\ models to match the natural language tasks performance of models trained on curated data. \\ \hline \end{tabular}
\end{table}
표 4: 큐레이션은 제로 샷 일반화를 위한 은 총알이 아니다: RefinedWeb에서 훈련된 소규모 모델들은 웹 데이터(C4, OSCAR), 및 큐레이티드 코포라(The Pile)에서 훈련된 모델들보다 성능이 우수하다. Zs-web aggregate에 대한 Zero-shot accuracy (\(\sigma\) = 0.69, _like_\(\pm\)0.69%, _very likely_\(\pm\)1.38% 점수 차이). 동일한 양의 토큰에 대해 동일한 아키텍처 및 하이퍼 매개 변수로 훈련된 모든 모델입니다. 우리는 OSCAR-22.01이 중복 제거가 선택적일 뿐이기 때문에 다른 데이터 세트를 상당히 과소 수행한다는 것을 발견했다. C4는 OSCAR-21.09가 약간 뒤처지는 강력한 기준선이지만, RefinedWeb이 웹 데이터 세트와 큐레이트 데이터 세트 모두에 비해 성능이 우수하다는 것을 발견했다. C4와의 파일 성능 차이는 결정적인 것으로 충분하지 않지만 C4는 모델에 너무 작을 것이다. 필터링과 중복 제거 모두 제로 샷 성능을 개선하는 데 크게 기여합니다.

그림 3: 우세모델 스케일링 \(\blacksquare\)부터 공동모델 및 데이터 스케일링 \(\blackphi\)까지 사전 훈련의 두 가지 시대. Hoffmann et al.(2022) 이전에 모델(\(\blacksquare\))은 주로 Kaplan et al.(2020)의 권장 사항에 따라 고정된 데이터 세트 크기(약 3,000억 토큰)에서 매개변수 수를 조정했다. 이후 (\(\blacklozenge\)) 모델은 모델 크기와 데이터 세트 크기를 공동으로 크기 조정하기 시작하여 확장 가능한 데이터 파이프라인의 필요성을 급격히 증가시켰다. Penedo 등(2023)의 연구에서 CommonCrawl에서 사용할 수 있는 깨끗한 데이터의 추정은 다국어성을 허용하는 경우 영어만 고려할 때 두 배가 될 것이다.

#### 4.2.2 강력한 웹 기준선에 대해 큐레이션된 데이터는 해로울 수 있습니다.

**배경.** 섹션 4.2.1 및 표 3에서 대규모 언어 모델이 대규모 웹 크롤 데이터와 개별 큐레이트 소스를 모두 결합한 사전 훈련 데이터 세트를 사용한다는 점에 주목했습니다. 그러한 소스들은 도메인-특정 모델들을 구축하기 위해 처음 이용되었고(Beltagy 등, 2019), 이들은 또한 예를 들어 대화 모달리티들에 대한 모델들의 표현성을 넓히기 위해 제안되었다(Adiwardana 등, 2020; Thoppilan 등, 2022). 이러한 소스들 중 일부는 또한 강하게 큐레이팅된 데이터와 크롤들의 교차점에 존재할 수 있다 : Laurenson 등(2022)은 예를 들어 인간-선택된 URL들을 사용하여 크롤에 첫 번째 링크들을 시드하도록 제안되었다. 그러나 이러한 맞춤형 소스는 실무자에게 어려움을 제기한다. 첫째, 개별 말뭉치는 중앙 집중식 파이프라인 대신 흩어져 있는 작업이 필요하기 때문에 대규모 웹 크롤링에 비해 확장성이 훨씬 떨어진다. 둘째, 이러한 소스 중 일부의 공급자는 LLM이 데이터 Paresh(2023)에 대해 훈련되는 것을 금지하는 조치를 취하기 시작했으며 적절한 라이선스를 얻는 데 비용이 많이 들 수 있다. 이전 섹션의 연구 결과에 기초하여, 우리는 큐레이션된 데이터와 RefinedWeb과 같은 강력한 웹 기준선을 결합할 때 어떤 일이 일어나는지 궁금해할 수 있다.

**질문.** 강력한 웹 기준선을 대체 하 여 추가 하면 개별 말뭉치의 큐레이션 된 데이터가 모델의 자연 언어 제로 샷 성능에 여전히 도움이 됩니까?

**방법.** 30B 토큰에서 작은 1B 모델을 학습하고 사전 학습 데이터는 웹 데이터와 특정 큐레이션된 범주 간에 분할됩니다. 우리는 대상 범주의 1, 10, 25, 50, 75 및 100%에 대한 교육을 샘플링한다. 우리는 1차원 접근법만을 고려하고, 큐레이트된 데이터의 단일 카테고리와 웹 데이터를 혼합한다. 우리는 표 5에 요약된 바와 같이 책, 대화 및 기술 데이터로 카테고리를 나눈다. 이러한 카테고리를 만드는 개별 코퍼스의 경우 대화 카테고리에 대한 Reddit(Baumgartner et al., 2020)의 데이터로 개선한 The Pile(Gao et al., 2020)에서 영감을 얻는다. 우리의 웹 데이터는 RefinedWeb(Penedo et al., 2023)에서 가져온 것이며, 유사한 파이프라인을 통해 큐레이션된 소스를 처리하며, 공정한 비교를 위해 필터링 및 중복 제거를 적용한다. 자세한 구성은 표 2를 참조하면서 zs 데이터 집합에 대한 성능을 평가한다.

**결과.** 그림 4에서 큐레이션된 데이터의 비율을 증가시키면서 제로 샷 정확도를 표시합니다. 강력한 웹 기준선과 결합하면 큐레이션된 데이터의 추가가 성능을 크게 향상시키지 않는다는 것을 알 수 있습니다. 사실, 선별된 데이터의 과잉은 심지어 성능을 악화시킨다: 책과 기술, 지난 50%의 경우, 우리는 정확도의 의미 있는 저하를 관찰하기 시작한다. 우리는 이것이 웹 데이터의 높은 다양성에 비해 "모드 붕괴"에 의해 야기되었을 가능성이 있다고 믿는다.

흥미롭게도, 대화는 100%로 가장 작은 저하와 함께 전체적으로 적절하게 수행된다. 우리는 이것이 우리의 대화 범주가 세 가지 중 가장 다양하거나 대화가 작업 분배에 더 가깝기 때문일 수 있다고 가정한다. 실제로, 이 범주에는 사람들이 상호 작용하고, 질문에 답하고, 서로에게 지침을 주는 것이 포함될 수 있으며, 이는 문서나 특허와 같은 기술 기반 콘텐츠에서 덜 널리 퍼져 있는 스타일이다.

다시 한 번, 이 절제술은 고려된 작업의 범위에 의해 제한된다. 높은 기술 작업은 도메인 특정 데이터로부터 이익을 얻을 수 있으며 웹 데이터는 코드 작업에 대한 다소 열악한 기준선이 될 가능성이 있다. 또한, 우리의 제로 샷 평가는 모두 짧은 문맥 길이에서 수행되는데, 예를 들어 책이 모델이 장거리 상관 관계를 배우는 데 도움이 될 수 있다.

**찾기.** 강력한 웹 기준선을 대체 하 여 추가 하는 경우 선별 된 데이터 범주는 체계적으로 자연 언어 제로 샷 성능을 향상 시키지 않습니다.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Conversations** & Reddit (Baumgartner et al., 2020), HackerNews, OpenSubtitles (Tiedemann, 2016), Ubuntu IRC, Youtube Subtitles, StackOverflow \\
**Books** & Project Gutenberg Rae et al. (2019) \\
**Technical** & ArXiv, PubMed Central, PubMed Abstracts, USPTO Patents \\ \hline \hline \end{tabular}
\end{table}
표 5: **큐레이션된 데이터를 대화, 책 및 기술적 범주의 세 가지 광범위한 범주로 분할합니다. Gao 등(2020)에서 영감을 받은 개별 구성 요소이지만 자체 데이터 파이프라인을 통해 처리됩니다.**

#### 4.2.3 제한 코드 및 다국어 데이터는 영어 성능을 크게 저하시키지 않음

**질문.** 모델의 영어 성능을 손상시키지 않고 추가 된 사전 훈련 데이터에 제한 된 양 (5-10%)의 코드 및 다국어 데이터를 대체할 수 있습니까?

**다국어.** RNN 및 Transformers의 첫 번째 대규모 배포 중 일부는 기계 번역을 위한 것입니다 (Wu et al., 2016); 주의 메커니즘이 원래 이러한 사용 사례에 대해 도입되었습니다 (Bahdanau et al., 2014). 따라서, 다중 언어 언어 모델들이 그들의 단일 언어 대응물들을 따라 빠르게 번성했다는 것은 놀라운 일이 아니다(Xue et al., 2021). 그러나 다국어 _생성_ 대형 언어 모델은 더 파악하기 어려운 상태로 남아 있습니다. 실제로 Lin et al.(2021)과 Scao et al.(2022b)은 모두 방대한 다중언어성이 영어 성능을 희생시키면서 온다고 보고하여 그들의 단일언어 대응물들을 과소수행하는 다중언어 모델들을 초래하였다(Scao et al., 2022a). 이러한 소위 다국어성의 저주(Conneau et al., 2020)는 실무자들이 다국어 능력을 명시적으로 목표로 하는 PaLM(Chowdhery et al., 2022)조차도 다른 언어를 대량으로 순진하게 추가하는 것에 지치게 하고, 단지 20%의 비영어 데이터에 대해서만 훈련시킨다. 게다가, 다국어 데이터는 거의 이용가능하지 않다(Costa-jussa et al., 2022): CommonCrawl의 문서의 거의 60%가 영어이고, 최상위 언어의 분포는 유럽어로 치우쳐 있다. 특히 중국어는 세계 2위임에도 불구하고 커먼크롤에서 6위 상위 언어에 불과하고, 힌디어는 세계 3위임에도 불구하고 상위 20위권에서도 나타나지 않는다(Eberhard et al., 2023).

제한된 다국어 설정을 실험하기로 한다. 라틴어 알파벳을 사용하는 언어를 고려하고 데이터 파이프라인을 사용하여 커먼크롤(100억 토큰 이상)에서 자명하지 않은 양을 수집할 수 있는 언어에 중점을 둔다. 실험을 위한 분할은 표 6에 나와 있다. 우리는 고정된 10% 다국어 데이터(CommonCrawl의 보급률에 따라 개별 언어 가중치)로 모델을 훈련하고 영어 과제에 대해 평가한다. 각 데이터 분할은 전용 토큰나이저를 사용합니다.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Set** & **Languages** \\ \hline
**영어** & English \\
**Restricted** & English, German, Spanish, French \\
**European** & English, German, Spanish, French, Italian, Dutch, Polish, Portuguese, Czech, \\  & Swedish, Romanian, Danish, Norwegian, Catalan, Slovene, Bulgarian \\ \hline \hline \end{tabular}
\end{table}
표 6: **영어 전용, 제한(유럽에서 가장 많이 사용되는 3개 언어 추가) 및 유럽어의 세 가지 범주로 언어를 분할합니다.* * 라틴어 알파벳이 있는 언어와 CommonCrawl에 충분한 존재만 고려하여 최소 100억 토큰을 수집합니다.

도 4: 강력한 웹 데이터 베이스라인과 비교할 때, 고품질 큐레이팅된 데이터는 제로 샷 성능을 개선하지 않는다. 단일 유형의 큐레이트 데이터에 과도하게 의존하는 것은 성능에 해롭다. 대화, 책, 기술 데이터 코퍼스의 구성은 표 5에 요약되어 있다. 우리는 서로 다른 데이터 혼합을 사용하여 제로 샷 성능을 보고하고 RefinedWeb에서만 훈련된 기준 모델과 비교하고, 우리의 중복 제거 및 필터링된 웹 전용 데이터 세트 Penedo 등(2023)과 비교된다. 음영 처리된 녹색 영역은 데이터 분할에 대한 10개의 실험을 기반으로 한 \(\pm 3\sigma\) 신뢰 구간을 나타낸다.

우리는 표 7에 결과를 제시한다. 우리는 10% 다국어성으로 인한 성능 저하가 매우 제한적이며 독일어, 스페인어, 프랑스어보다 다른 유럽 언어의 추가가 추가적인 저하를 유발하지 않는다는 것을 발견했다. 우리는 HellaSwag에서 대부분의 성능 감소를 보았지만 다른 작업은 큰 영향을 받지 않았다. 우리는 이러한 실험이 매우 제한된 설정에서 적용되며, 다른 다국어 설정(예를 들어, 라틴어 알파벳이 없는 언어, 영어 토큰의 부족 등을 보상하기 위해 다른 언어를 사용하는 등)을 예시하지 않을 수 있다는 점에 주목한다.

**코드.** 대규모 언어 모델은 일반적인 사전 교육 후 finetuning을 통해 강력한 코딩 능력을 입증했습니다 (Chen et al., 2021; Chowdhery et al., 2022; Roziere et al., 2023). 또는 전용 사전 교육 레시피를 통해 강력한 코딩 능력을 입증했습니다 (Li et al., 2023a). 게다가, 다국어 데이터와의 분산에서, 코드 데이터는 풍부하며(Kocetkov 등, 2022), 수조 개의 토큰들이 크레이잉 퍼블릭 리포지토리들로부터 이용가능하다. 코드 태스크는 LLM-파워 어시스턴트(Luo 등, 2023)의 우세한 적용이며, 따라서 팔콘과 함께 그러한 사용 사례를 허용하는 것이 중요하다. 그러나, 언어 능력을 손상시킬 위험을 감수하고 싶지 않다: 따라서 우리는 여기서 다른 모델에 따라 제한된 코드 데이터를 사전 훈련에 추가하는 일반적인 관행을 검증한다(사전 훈련에서의 공통 분수에 대한 표 3 참조).

GitHub에서 상위 30개 프로그래밍 언어를 선택하고 사전 훈련 데이터의 5%를 코드로 대체한다. 코드 데이터 세트에 중복 제거를 적용하지만 데이터를 너무 많이 제거하지 않도록 휴리스틱을 조정합니다. 우리는 영어 과제에 대한 성능만을 고려합니다. 결과는 표 7에 요약되어 있다. 우리는 코드 데이터의 추가가 작업 저하가 거의 또는 전혀 없이 다국어성과 유사한 효과를 갖는다는 것을 발견했다.

**찾기.** 코드 및 다국어 데이터의 작은 부분 (5-10%)은 대규모 언어 모델에 대 한 일반적인 레시피와 일치 하 여 영어 작업에 대 한 제로 샷 성능에 크게 영향을 주지 않습니다.

우리는 절제술의 작은 규모가 여기에서 더 강력한 한계이며 다중 언어성과 코드에 대한 보다 보수적인 선택으로 이어질 가능성이 있다는 점에 주목한다. 용량 증가 덕분에 더 큰 모델이 다국어성을 더 잘 다룰 수 있다고 주장되어 왔다(Shaham et al., 2022). 코드 데이터는 또한 상식 능력을 향상시키기 위해 더 큰 모델에 대해 나타났다(Madan et al., 2022). 보다 넓게는, 다국어 모델에 대해서도 유사한 효과가 관찰되었다(Aghajanyan et al., 2023).

### 아키텍처 및 사전 훈련: 인기 있는 레시피 및 추론 최적화 유효성 검사

#### 4.3.1 멀티쿼리를 텐서 병렬 학습 및 추론을 위한 멀티그룹으로 확장

**배경.** 만장일치로 대형 언어 모델은 Vaswani 등(2017)에 설명된 다중 헤드 주의 방식을 처음 채택했습니다. 각 토큰은 (쿼리, 키 및 값)의 \(n_{\text{head}}\) 삼중항을 생성 하 고 각 헤드의 결과를 합산 하 여 주의 모듈의 최종 출력을 생성 합니다. 그러나, 이 방식은 변경될 수 있다. Shazeer(2019)는 성능 저하가 작지만 모든 주의 헤드 간에 동일한 키와 값을 공유할 수 있음을 발견했다. 이러한 멀티쿼리 주의에서 질의에 대한 헤드의 수는 \(n_{q}=n_{\text{head}}\)으로 유지되지만 키와 값에 대한 헤드는 \(n_{kv}=1\)뿐이다. 이것은 추론 메모리 소비를 상당히 감소시킨다: 자기회귀 생성 동안, 키들 및 값들은 멀티쿼리로 생성을 가속화하기 위해 캐시되고, K,V-캐시 크기는 바닐라 주의에 비해 \(n_{\text{head}}\)으로 분할되어, 공통 모델들에 대한 메모리 소비의 10-100배 감소를 초래한다. 멀티쿼리는 대형 모델들에 대한 추론의 확장성을 향상시킨다(Pope et al., 2023). Chowdhery 등(2022)은 최근에 LLaMA-2(Touvron 등, 2023b)에 의해 특히 채택된 이러한 건축 변형을 대중화시켰다.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Pretraining data** & \multicolumn{2}{c}{**Zero-shot accuracy**} \\  & zs-main\(\uparrow\) & zs-small\(\uparrow\) \\ _Likely_ threshold (1-\(\sigma\)) & \(\pm 1.0\) & \(\pm 0.5\) \\ \hline English-only & **53.7** & **49.2** \\
10\% Restricted & 53.4 & 48.3 \\
10\% European & 53.6 & 48.2 \\ \hline
5\% Code & 53.6 & 48.5 \\ \hline \hline \end{tabular}
\end{table}
표 7: 프리트레이닝 데이터에 코드 또는 다국어 데이터의 작은 부분을 광범위하게 포함시키면 특정 태스크를 제외하고는 성능이 크게 저하되지 않는다. 전반적으로 관찰된 분해는 더 큰 집합체에서 거의 측정할 수 없으며 대부분 zs-작은 집합체에서 HellaSwag에 의해 구동된다. 밑줄 친 값은 _거의_ 1-\(\sigma\) 분해 임계값을 초과했다.

**크기 조정.** 흥미롭게도 멀티쿼리는 더 큰 모델에 대해 불균형적으로 효과적입니다. 총 파라미터 수는 \(N\)으로 모델 크기 \(d_{\text{model}}\), 층수 \(n_{\text{layer}}\), 고정된 \(d_{\text{head}}=d_{\text{model}}/n_{\text{head}}\)을 가정하면 FlashAttention(Dao et al., 2022)을 사용하는 경우가 일반적이다. 효율적인 스케일링을 위해서는 \(n_{\text{layer}}\sim\mathcal{O}(\log(N))\)(Levine et al., 2020); \(N\simeq n_{\text{layer}}(d_{\text{model}})^{2}\)(Kaplan et al., 2020)을 근사화할 수 있으므로, 멀티헤드 어텐션을 갖는 K,V-캐시의 크기는 \(\mathcal{O}(\sqrt{N}\log(n))\으로 스케일링하는 것이 좋다. 반대로 멀티쿼리의 경우 K,V-캐시는 계층당 고정된 \(2d_{\text{head}}\)만을 저장하며, 이는 폭에 따라 증가하지 않으므로 \(\mathcal{O}(\log(N))\)에서 보다 효율적인 스케일링을 수행한다.

**다중 그룹.** 다중 그룹 주의의 한 가지 주의 사항은 GPU 기반 인프라에서 일반적인 것처럼 텐서 병렬화에 의존할 때 효율적으로 병렬화하기 어렵다는 것입니다 (Shoeybi 등, 2019). 각 GPU는 공유 키/값의 복사본을 유지 하 고 개별적으로 다시 계산 한 다음 그래디언트를 공유 하 여 동기화를 유지 하거나 단일 GPU에서 계산 된 다음 필요에 따라 통신 합니다. 우리는 각 텐서 병렬 랭크에 대해 별도의 키/값 쌍을 도입하여 필요한 통신을 단순화하는 것을 제안한다. Shazeer (2019)와 같이 \(n_{q}=n_{\text{head}}\)을 유지하지만, 현재는 \(n_{kv}=\text{TP}\)을 갖는다. 이 스킴은 고정 TP 팩터만 적용하므로 K,V-캐시의 스케일링을 변경하지 않습니다. 팔콘 시리즈의 개발과 동시에 Ainslie et al.(2023)도 이 수정을 제안했으며, 우리는 이 주의의 변형을 그룹화된 쿼리 주의 또는 다중 그룹으로 언급한다. 우리는 통신 감소가 추론 동안뿐만 아니라 훈련 동안에도 적용된다는 점에 주목한다.

**결과.** The Pile(Gao et al., 2020)의 30/60B 토큰에 대해 1B/3B 모델을 학습하고, 멀티쿼리 및 다양한 수준의 멀티그룹 주의를 기울입니다. 중요한 것은 추가 키 및 값 손실로 인한 매개변수 감소를 제어하지 않는다는 것이다. 결과는 표 9에 나와 있다. 멀티쿼리와 멀티그룹 모두 훈련 가능한 매개변수의 감소를 보상하지 않더라도 제로 샷 성능의 큰 감소를 초래하지 않는다.

**레시피 결정.** 최대 모델에 대 한 성능 확장성을 개선 하기 위해 팔콘 시리즈는 모든 모델에 대해 KV=TP를 사용 하는 다중 그룹을 구현 합니다 (각각 팔콘-7/40/180B의 경우 1/8/8).

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline
**Attention scheme** & \(n_{q}\) & \(n_{kv}\) & **K,V-cache for a 2,048 sequence** & & \\  & & & & 7B & 40B & 180B \\ \hline Vanilla & \(n_{head}\) & \(n_{head}\) & \(\mathcal{O}(\sqrt{N}\log(n))\) & 1GB & 4GB & 10GB \\ Multiquery (Shazeer, 2019) & \(n_{head}\) & 1 & \(\mathcal{O}(\log(N))\) & 20MB & 30MB & 40MB \\ Multigroup (Ainslie et al., 2023) & \(n_{head}\) & TP & \(\mathcal{O}(\log(N))\) & N/A & 250MB & 335MB \\ \hline \hline \end{tabular}
\end{table}
표 8: 멀티쿼리/그룹 스킴은 추론을 위한 K,V-캐시의 크기를 상당히 감소시킨다. 팔콘-7B의 경우 TP=1이고 팔콘-40/180B의 경우 TP=8이라고 가정하면 서열 길이가 2,048이다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Model size** & **KV** & **Performance** & & \\  & & zs-main\(\uparrow\) & zs-small\(\uparrow\) & ppl-pile\(\downarrow\) \\ _Very likely_ threshold (2-\(\sigma\)) & \(\pm 2.2\) & \(\pm 0.8\) & \(\pm 0.005\) \\ \hline
1B & 1 & 48.5 & 42.6 & 0.908 \\  & 2 & 48.2 & 42.1 & 0.899 \\  & 4 & 48.9 & 42.4 & 0.908 \\  & 8 & 48.6 & 42.8 & 0.903 \\  & Vanilla & **49.2** & **43.1** & **0.895** \\ \hline
3B & 1 & 52.7 & 48.6 & 0.825 \\  & 8 & **54.6** & **50.1** & 0.819 \\  & Vanilla & 54.4 & 49.8 & **0.807** \\ \hline \hline \end{tabular}
\end{table}
표 9: 파라미터의 감소를 제어하지 않더라도, 멀티쿼리는 제한된 제로 샷 성능 비용에서만 제공된다. 당혹감에 대한 영향은 더 직접적으로 측정할 수 있는 반면, 제로 샷 성능에 대한 영향은 덜 일관적이다. KV=8인 다중군은 바닐라 기준선에 가깝게 일관되게 수행한다. 밑줄 친 값은 _매우 가능성_ 2-\(\sigma\) 분해 임계값을 초과했다.

#### 4.3.2 Rotary positionnal embeddings may only offer a limited edge over ALiBi

**배경.** 기본적으로 주의는 모델에 위치 정보를 제공하지 않습니다. 시퀀스를 단어 백으로만 봅니다. 따라서, 원래의 Transformer 아키텍처는 위치 정보 Vaswani 등(2017)을 인코딩하기 위해 절대 사인파 임베딩을 채택하였다. 그러나 그 이후로 절대 임베딩은 인기가 감소했으며 커뮤니티는 대신 상대 임베딩으로 전환했다. 이러한 시프트는 경험적으로 잘 동기화되어 있지만(Shaw et al., 2018; Scao et al., 2022), 실무자들은 아직 단일 상대 위치 임베딩 상에서 결정화되지 않았다: BLOOM 및 MPT(Scao et al., 2022; MosaicML, 2023)는 ALiBi(Press et al., 2022), GPT-J, PaLM 및 LLaMA(Wang and Komatsuzaki, 2021; Chowdhery et al., 2022; Touvron et al., 2023, 2023)는 Rotary Positional Embeddings(RoPE)(Su et al., 2021). RoPE는 위의 작업에서 더 나은 업스트림 성능을 제공하는 것으로 종종 인용되는 반면 ALiBi는 내장된 외삽 능력으로 인해 이점이 있다. 최근에 소개된 또 다른 대안은 보편적인 상대적 포지셔널 임베딩, URPE(Luo et al., 2022)이며, 이는 전형적인 상대적 포지셔널 임베딩의 표현성에서의 단점을 해결한다. 호기심으로서, 인과 모델의 자기회귀 마스크는 또한 모델에 일부 위치 정보를 제공하므로(Scao et al., 2022; Haviv et al., 2022), 위치 임베딩이 전혀 없는 트레이닝이 제로 샷 성능을 위해 절대 사인파 모델에 필적할 수 있다는 점에 주목한다.

이 작업과 동시에, RoPE(Chen 등, 2023)로 제로 샷 또는 피니튜닝된 길이 외삽을 가능하게 하는 레시피가 등장하여 외삽을 위해 ALiBi로 갭을 메운다.

**결과.** The Pile(Gao 등, 2020)에서 30/60B 토큰에 1/3B 모델을 학습합니다. 우리는 표 10의 결과를 보고한다. URPE가 수용 가능한 성능을 제공하기 위해 융합된 주의 커널에 상당한 수정을 필요로 하기 때문에 RoPE를 능가하는 것에 대한 증거를 찾지 못하지만 더 이상 추구하지 않는다. 1B 규모에서 ALiBi보다 RoPE를 사용할 가능성이 있는 이점을 찾지만, 그 이점은 3B 규모에서 감소하며 명확하게 결론짓기에는 충분하지 않는다. ALiBi의 한 가지 남은 장점은 계산 오버헤드이다. RoPE보다 계산 비용이 훨씬 저렴하지만 사용자 지정 트리톤 커널(섹션 5.3.2)을 사용하면 이러한 오버헤드를 완화할 수 있다.

**레시피 결정.** 다른 인기 있는 대규모 모델과 인라인에서 회전 위치 최종 임베딩을 채택 하 고 사용자 지정 커널을 사용 하 여 오버 헤드를 완화 합니다.

#### 4.3.3 GLU의 추가 메모리 비용은 비용 효율적인 교육을 위해 가치가 없을 수 있습니다.

**배경.** 게이티드 선형 단위(Shazeer, 2020)를 기반으로 하는 활성화는 GeLU(Scao 등, 2022)와 같은 기존 활성화 함수를 능가하는 것으로 널리 알려져 있습니다. 그들은 PaLM 및 LLaMA와 같은 모델에서 채택된 것을 보았다(Chowdhery et al., 2022; Touvron et al., 2023, 2023).

**스케일링.** GLU 활성화는 MLP의 크기를 증가(첫 번째 계층을 두 배로 늘림)시키므로 단순 행렬 곱셈으로 더 많은 계산을 이동하기 때문에 선호되었습니다. 그러나, 이것은 비용이 든다: MLP에 중간 활성화들을 저장하는 데 필요한 메모리가 더 높다. 일반적으로 활성화 함수에 대 한 입력은 백워드에 대해 저장 됩니다 (함수 자체를 다시 계산 하는 것은 무시할 수 있습니다). 게이트 장치의 경우 이 입력이 이제 두 배 커집니다. 전반적으로 SwiGLU는 중간 활성화를 두 배로 늘리고 MLP의 매개변수 수를 50% 증가시키며, 따라서 MLP에 대한 매개변수당 활성화 메모리는 33% 증가한다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Model size** & **Pos. Emb.** & \multicolumn{2}{c}{**Performance**} \\  & & zs-main\(\uparrow\) & zs-small \(\uparrow\) & ppl-pile \(\downarrow\) \\ _Likely_ threshold (1-\(\sigma\)) & \(\pm\)1.1 & \(\pm\)0.4 & \(\pm\)0.002 \\ \hline
1B & ALiBi & 49.2 & 43.1 & 0.895 \\  & URPE & 49.6 & 43.1 & 0.885 \\  & RoPE & **50.0** & **44.2** & **0.883** \\ \hline
3B & ALiBi & **54.4** & 49.8 & 0.807 \\  & RoPE & **54.4** & **50.5** & **0.799** \\ \hline \hline \end{tabular}
\end{table}
표 10: 소규모 URPE 및 RoPE에서 ALiBi보다 더 나을 수 있지만 그 이점은 증가된 크기에서 명확하지 않다. 우리는 ALiBi가 1B 모델에 대한 세 가지 집합체 중 두 개에서 회전식보다 최악일 가능성이 있지만 3B 규모에서 회전식의 성능에 더 가깝다는 것을 발견했다. 밑줄 친 값은 RoPE에서 _거의_ 1-\(\sigma\) 분해 임계값을 초과했다.

**결과.** 표 11에서 The Pile의 30B 토큰에 대해 1B 모델을 학습하면 제로 샷 성능을 위해 SwiGLU를 채택할 때 뚜렷한 이점이 없으며 복잡성에 대한 개선은 평가 설정의 분산으로 인해 실제 개선을 특성화할 가능성이 거의 없는 임계값에 불과합니다. 이것은 SwiGLU가 모든 아키텍처 삭제에 대해 전용 하이퍼파라미터 튜닝을 필요로 할 수 있다는 사실에 기인할 수 있다는 점에 주목하며, 우리는 단순히 GPT-3(Brown 등, 2020)의 것을 채택했다. 또한, 우리는 이미 병렬 주의/MLP 계층을 사용하기 때문에 SwiGLU에서 추가 처리량 이득을 거의 보지 못했다. 우리는 비용을 최적화하기 위해 40GB A100에서 팔콘 시리즈를 훈련시키기 때문에 메모리 소비에 대해 일찍이 우려했는데, 그에 따라 메모리 강도가 증가하고 GeLU와 유사한 성능을 가진 SwiGLU는 우리에게 순 음수가 될 가능성이 높다.

**레시피 결정.** A100-40GB에 대한 훈련의 메모리 사용량에 대한 우려로, 제로 샷에서 명확한 상승이 없기 때문에 SwiGLU를 채택하지 않기로 결정했습니다.

#### 4.3.4 작은 수정은 확장성에 도움이 됩니다. 병렬 계층 및 선형 계층에 편향이 없습니다.

**병렬 주의 및 MLP 블록.**왕 및 Komatsuzaki (2021)는 GPT-J를 교육 하는 동안 병렬 주의 및 MLP 계층을 처음 도입 했습니다. 이러한 증대는 텐서 병렬성과 관련된 통신 비용을 감소시키는데 중요하다: 이러한 간단한 수정은 레이어당 필요한 all_reduce의 수를 두 개에서 한 개로 줄인다. 우리는 Chowdhery 등(2022)에 따라 제로 샷 성능이나 복잡성에서 측정 가능한 열화를 발견하지 못했고 이 관행을 채택했다. 도면을 참조한다. 도 5를 예시한다.

**편향 없음.**Chowdhery 등(2022)은 선형 계층 및 계층 규범에서 편향을 제거하면 안정성이 향상된다는 것을 발견했습니다. 우리는 선형 레이어에서 바이어스를 제거하는 것이 더 나쁜 성능을 초래하지 않는다는 것을 검증한다(표 11 참조): 언어 모델링 손실의 관점이나 최종 제로 샷 성능의 관점 모두에서. 따라서 우리는 팔콘 시리즈의 선형 층에서 편향을 제거한다.

**레시피 결정.** 병렬 주의 및 MLP를 채택 하 고 선형 계층에서 편향을 제거 합니다.

#### 4.3.5 Validating best practices for hyperparameters: z-loss, weight decay, LR search

**Z 손실.** 메시-텐서플로우 코드base2 Shazeer 등(2018)에 처음 도입된 z 손실은 로짓이 0에 가깝게 유지되도록 권장하여 훈련의 안정성을 높이는 것을 목표로 합니다. 보조 손실(z_loss=\(10^{-4}\) log\({}^{2}\)(\(\Sigma_{i}e^{z_{i}}\))로 구현할 수 있으며, 여기서 \(z_{i}\)은 모델의 출력 로그이다. z-loss는 소규모(표 11)에서 과제 수행에 유의한 영향을 미치지 않는다는 점에 유의한다.

각주 2: 독자를 위한 재미 있는 연습으로, 우리는 구글 외에서 이 관행을 대중화한 PaLM(Chowdhery et al., 2022)을 본문에 요약하여 그 인용을 파헤치기를 권장한다.

**레시피 결정.** z-loss를 채택합니다. 이는 대규모 훈련 안정성을 향상 시키고 ablations에서 제로 샷 성능에 영향을 주지 않는다고 주장됩니다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Model size** & **Pos. Emb.** & \multicolumn{2}{c}{**Performance**} \\  & & zs-main\(\uparrow\) & zs-small\(\uparrow\) & ppl-pile\(\downarrow\) \\ _Unlikely_ threshold (0.4-\(\sigma\)) & \(\pm 0.4\) & \(\pm 0.2\) & \(\pm 0.001\) \\ \hline
1B & & **49.2** & 43.1 & 0.895 \\  & SwiGLU & **49.2** & 43.1 & **0.891** \\  & z-loss & 49.0 & **43.6** & 0.895 \\ \hline
3B & & **54.5** & **49.8** & **0.807** \\  & No biases & 54.4 & **49.8** & **0.807** \\ \hline \hline \end{tabular}
\end{table}
표 11: GLU, z-손실 및 편향 제거와 같은 작은 아키텍처 조정은 제로-샷 성능을 향상시킬 것 같지 않다. 그러나, 일부 시나리오에서, 이들은 확장성 및/또는 안정성을 개선하기 위해 제안되었으며, 이는 채택을 보증할 수 있다. 밑줄 친 값은 기준선보다 _불가능_ 0.4-\(\sigma\) 개선 임계값 이상으로 교차했다.

**무게 감소.** Chowdhery 등(2022)에서 무게 감소 일정을 재현하려고 시도했지만 개선을 얻지는 못했습니다. 이는 초기화의 차이 때문인 것으로 의심됩니다. 우리는 체중 감소가 중복 제거되지 않았거나 품질이 낮은 데이터 세트에 불균형적인 영향을 미친다는 것을 발견했다(표 12). 우리는 체중 감량을 위해 AdamW를 사용한다(Loshchilov and Hutter, 2018).

**레시피 결정.** 모든 팔콘 모델에 대해 AdamW와 함께 0.1의 고정 중량 감량을 사용합니다.

**최적 학습 속도.** 실행의 학습 속도를 설정 하는 관행은 나이브 그리드 검색에서 보다 원칙적인 접근법 (Yang et al., 2022; Dinan et al., 2023)에 따라 다릅니다. 우리는 섹션 4.4에서 나중에 일부를 구현하고 재현하는 실패에 대해 더 논의하며, 이 짧은 섹션에서는 순진한 접근법과 이에 대한 검증에 중점을 둔다. 대체로 학습률을 너무 높게 설정하면 훈련 중 실행과 불안정성의 분기를 유발할 위험이 있으며, 반대로 너무 낮게 설정하면 업스트림 및 다운스트림 성능이 테이블에 남아 비효율적인 훈련을 초래한다.

우리는 다음에서 가능한 학습률을 통해 검색할 것을 제안한다. (1) 우리는 4-6개의 대략 대수적으로 간격을 둔 후보 학습률을 선택하고, GPT-3(Brown et al., 2020)에 사용된 것 주위에 고정하고, 더 높은 학습률을 선호한다. (2) 모든 후보 학습률에 대해 긴 5억 토큰 워밍업을 실행한다. (3) 이 시점에서 가장 낮은 손실을 달성한 학습률을 선택하고, 이미 스파이크를 일으킨 모든 학습률을 폐기한다.

이 방법을 소규모로 테스트하고 1B 모델에 대해 7개의 학습률을 선택한 다음 준비 후 얻은 순위를 실제 최종 순위가 달성된 것과 비교한다. 결과는 표 13에 제시되어 있다. 우리는 이 나이브 방법이 웜업이 끝날 때 순위로부터 최상의 학습률을 찾는 데 성공한다는 것을 발견했다. 보다 넓게는 워밍업 종료와 훈련 종료 시 순위가 비교적 안정되어 2, 3위 전환 장소만 있다.

**레시피 결정.** 후보 LR에서 준비 작업 후 손실이 가장 적은 것을 선택 합니다.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Dataset** & **Weight decay** & \begin{tabular}{c} **Performance** \\ **zs-main** \(\uparrow\) \\ \end{tabular} & \begin{tabular}{c} **zs-small** \(\uparrow\) \\ \end{tabular} &
\begin{tabular}{c} **ppl-pile** \(\downarrow\) \\ \end{tabular} \\ _Likely_ threshold (1-\(\sigma\)) & \(\pm\)1.1 & \(\pm\)0.4 & \(\pm\)0.002 \\ \hline RefinedWeb & 0. & **52.1** & 47.9 & 1.07 \\  & 1. & 52.0 & **48.4** & **1.06** \\ \hline The Pile & 0. & 50.3 & 43.7 & 0.877 \\  & 1. & **51.7** & **45.0** & **0.868** \\ \hline \hline \end{tabular}
\end{table}
표 12: 가중치 감쇄는 특히 더 파일과 같은 데이터 세트에 대해 성능을 향상시킬 가능성이 있으며, 이는 적절하게 중복 제거되지 않았을 수 있다. 놀랍게도 체중 감량의 효과는 기본 데이터 세트에 따라 불균형적으로 강력하다.

그림 5: **주의 및 MLP 블록을 병렬화 하면 텐서 병렬 훈련 중에 동기화 지점 하나를 제거할 수 있습니다. 이는 Wang 및 Komatsuzaki (2021)에 의해 GPT-J에 대해 처음 제안 되었습니다. **

### 추가 실험 필요: 절단을 만들지 않은 아이디어입니다.

이 섹션에서는 우리가 실험한 몇 가지 관행과 아이디어를 간략하게 언급하지만 결과를 재현하거나 의미 있는 결과를 얻을 수 없었다. 우리는 이것이 이러한 관행의 기소로 간주되지 않는다는 점에 주목한다: 많은 것들이 인기 있는 모델에 성공적으로 채택되었다.

**대체 교육 목표.** 언어 모델의 가장 큰 값은 일반적으로 인과적 디코더 전용 아키텍처 및 목표로 훈련되었습니다 (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022). Wang 등(2022)은 이러한 모델들이 T5와 같은 마스킹된 인코더-디코더들보다 더 나은 제로-샷 능력들을 나타낸다는 것을 발견하였다(Raffel 등, 2019); 그러나, 그들은 또한 멀티태스크 피니튜닝(Sanh 등, 2021) 후에, 마스킹된 인코더-디코더가 더 잘 수행된다는 것을 발견하여, 상이한 레짐들 및 사용 케이스들이 상이한 아키텍처들 및 객관적인 것을 선호할 수 있다는 것을 강조한다. 전체에 걸쳐, 비인과적 디코더 전용(소위 프리픽스 언어 모델)이 근접 초로서 경쟁적으로 수행되었다. UL2를 사용하여, Tay 등(2022)은 이러한 패러다임이 대신 목적들의 혼합에 대한 훈련에 의해 통일될 수 있음을 발견하였다. 우리는 UL2를 실험했지만 관련 있을 때 다양한 패러다임에 작업을 적용한 후에도 제로 샷 성능의 향상을 얻을 수 없었다. 시간 및 자원 제약으로 인해 Tay 등(2022)이 UL2 목표에 대한 사후 적응이 가능할 뿐만 아니라 효율적임을 보여주었기 때문에 실험을 더 이상 추진하지 않았다.

코드 모델의 경우, 소위 FIM(fill-in-the-middle) 트레이닝(Bavarian 등, 2022)이 이러한 모델에 대한 일반적인 사용 사례를 다루기 때문에 대중적이었다. FIM은 자기회귀 모델링 능력을 거의 내지 전혀 희생시키지 않는다고 주장되며, 우리는 이러한 결과를 광범위하게 확인할 수 있었고, 중간 채우기 비율(0.25-0.5)에 대한 소수의 작업에 대해서만 열화될 가능성이 있음을 보여주었다. 낮은 충전율과 높은 충전율은 제로샷 성능에 가장 낮은 영향을 미쳤다. 그럼에도 불구하고, 당시 광범위한 채택의 부족으로 인해, 우리는 FIM을 건너뛰고 대신에 Falcon-Coder 모델에 대한 적응 단계로서 고려하는 것을 선택한다 - 이는 Code-LLaMA에 대해 동시에 입증되었다(Roziere 등, 2023).

**Principled hyperparameters.** 는 \(\mu\)-parametrization (Yang et al., 2022)을 사용 하 여 hyperparameters를 더 작은 실행에서 더 큰 실행으로 원리적으로 확장 하는 방법으로 실험 했습니다. 불행히도 하이퍼파라미터 추정의 순진한 전략에 대한 개선을 입증할 수 없었다.

**대체 최적화기.** BERT 및 T5에 대해 보고 된 강력한 결과에 고무 된 어댑티브 학습 속도와 체중 감소를 사용 하 여 Adam의 대안인 Amos (Tian and Parikh, 2022)를 실험 했습니다. 우리는 소규모에서도 인과적 디코더 전용 모델에 대한 개선을 얻을 수 없었다.

**대화 마이닝.** RefinedWeb을 개발할 때 웹에서 특정 유형의 데이터를 마이닝하는 아이디어를 실험했습니다. BERT 모델을 기반으로 한 전용 분류기를 학습하면 과적합(over-fitting)이 발생하는 경우가 많아 단순 휴리스틱(예를 들어, 전이 단어의 밀도에 의한 인수 식별, 사용자의 회전을 찾아 대화)을 사용했다. 우리는 상당한 제로 샷 성능 향상을 얻을 수 있었지만 이 데이터는 매우 부족했다. 모든 CommonCrawl을 처리하기 전에 모델의 학습이 시작되었기 때문에 모든 데이터를 효과적으로 표면화하고 표준 웹 데이터보다 우선 순위로 사용할 수 없어 향후 모델에 대한 아이디어를 남길 수 있다.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multicolumn{2}{c}{**Learning rate**} & \multicolumn{2}{c}{**End LR warm-up [0.5GT]**} & \multicolumn{2}{c}{**End of run [27GT]**} & \multicolumn{1}{c}{**Run stability**} \\ Factor & LR & Improv. \(\downarrow\) & Rank & Improv. \(\downarrow\) & Rank & \\ \hline x1 & \(2\times 10^{-4}\) & \(\pm 0.0\)\% & (4) & \(\pm 0.0\)\% & (4) & No spikes \\ x2 & \(4\times 10^{-4}\) & \(-2.0\)\% & (2) & \(-1.7\)\% & (3) & No spikes \\ x5 & \(1\times 10^{-3}\) & \(\mathbf{-2.6}\)\% & (1) & \(\mathbf{-2.5}\)\% & (1) & No spikes \\ x10 & \(2\times 10^{-3}\) & \(-1.4\)\% & (3) & \(-1.9\)\% & (2) & One small spike \\ x20 & \(4\times 10^{-3}\) & \(+1.3\)\% & (5) & \(+1.9\)\% & (5) & Multiple spikes \\ x50 & \(1\times 10^{-2}\) & \(+7.6\)\% & (6) & \(+6.8\)\% & (6) & Multiple large spikes \\ x100 & \(2\times 10^{-2}\) & \multicolumn{4}{c}{Diverging after 0.2GT} & \\ \hline \hline \end{tabular}
\end{table}
표 13: 학습 레이트 워밍업 종료시의 손실 순위는 트레이닝 종료시의 순위를 광범위하게 반영하여, 최적의 학습 레이트를 효율적으로 검색할 수 있게 한다. 이 간단한 휴리스틱은 사용하기 쉽고, 더 큰 실행을 위해 리소스의 일부만을 소비한다.

### Wrapping-it up: 전체 데이터 세트 및 아키텍처 레시피의 유효성 검사

편의상 팔콘 레시피에 대한 전체 개요는 섹션 7을 참조한다. 이제 더 긴 훈련 실행과 최신 모델의 모델과의 비교를 통해 레시피의 성능을 더 큰 규모로 검증할 것이다. 본 논문에서는 (1) 사전 학습 데이터셋, 특히 웹 컴포넌트, (2) The Pile에서 이를 적용한 모델을 학습하여 아키텍처를 독립적으로 검증한다.

**데이터 세트 유효성 검사** 이전 모델의 일반적인 관행을 재현 하기 위해 27B 토큰 및 350B 토큰에 대 한 1B 및 7B 매개 변수 모델을 훈련 합니다. 우리는 GPT-3(Brown et al., 2020)과 ALBi(Press et al., 2022)를 기반으로 하는 우리의 베이스라인 아키텍처를 사용한다. 우리는 The Pile(Gao et al., 2020), RefinedWeb(우리의 웹 데이터세트, Penedo et al., 2023) 및 RefinedWeb과 큐레이티드 소스를 업샘플링 없이 결합한 Falcon 데이터 혼합물에서 훈련한다. 이 마지막 혼합물은 팔콘-180B용으로 설계되었으며 총 3,500B 토큰을 대상으로 합니다. 자세한 내용은 섹션 5.1을 참조하십시오.

표 14에서 데이터 혼합물이 성능을 크게 향상시킨다는 것을 발견했다. 이러한 이득의 대부분은 큐레이트된 데이터 없이 RefinedWeb 단독으로 달성된다. 이것은 웹 데이터만으로도 적절하게 필터링되고 중복 제거될 때 수행 모델들을 훈련시킬 수 있다는 우리의 이전 발견을 강조한다. 우리는 1/7B 모델이 최신 모델의 다른 모델과 유리하게 비교된다는 것을 발견하지만, 우리는 우리의 설정이 약간 더 오래 훈련함으로써 작은 이점을 가지고 있다는 점에 주목한다.

**아키텍처 유효성 검사.** 데이터 세트 유효성 검사의 설정을 따르고 27B 및 350B 토큰에 대한 The Pile에서 아키텍처 유효성 검사 1B 모델을 학습합니다. 이 실험에서 하이퍼파라미터 수정은 포함하지 않는다는 점에 유의하라: 모든 모델은 가중치 감쇠를 사용하고 Brown 등(2020)의 동일한 학습률 - 이 실험은 모델 자체의 아키텍처에만 관한 것이다.

우리는 우리의 아키텍처가 작은 성능 저하를 수반한다는 것을 발견하는데(표 14), 이는 대부분 멀티쿼리로 인한 파라미터의 감소에 기인한다. 우리는 멀티쿼리 모델을 성장시키면 그 격차를 줄일 수 있을 것으로 의심한다. 그럼에도 불구하고, 데이터 개선은 매우 중요한 영향을 미치지만 아키텍처 개선은 대부분 훈련 및 추론 확장성을 개선하는 데 중점을 두고 있으며, 이는 작업 성능의 향상을 초래하지 않는다는 점에 주목하는 것이 흥미롭다.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline
**Scale** & **Dataset** & **Architecture** & \multicolumn{2}{c}{**Performance**} & & \\  & & & zs-main \(\uparrow\) & zs-comp \(\uparrow\) & zs-small \(\uparrow\) & pp1-pile \(\downarrow\) \\ \hline \multirow{4}{*}{1B@27GT} & The Pile & Baseline & 51.7 & 40.3 & 45.0 & 0.868 \\ \cline{2-7}  & Falcon & Baseline & **53.5** & 42.3 & **48.8** & \\  & RefinedWeb & Baseline & 53.2 & **43.4** & 48.4 & \\ \cline{2-7}  & The Pile & Falcon & _51.1_ & _40.0_ & **45.1** & _0.870_ \\ \hline \hline \multirow{4}{*}{1B@350GT} & The Pile & Baseline & 57.8 & 47.1 & 54.0 & 0.763 \\ \cline{2-7}  & RefinedWeb & Baseline & **59.8** & **50.1** & **55.7** & \\ \cline{2-7}  & The Pile & Falcon & _56.6_ & _46.1_ & _52.7_ & _0.775_ \\ \hline
1B@300GT & OpenAI & babbage\({}^{\dagger}\) & & 47.8d & & \\
1B@380GT & The Pile & GPT-Neo\({}^{\dagger}\) & & 44.3d\({}^{\text{a,d}}\) & & \\
1B@300GT & The Pile & BS-A\&S\({}^{\dagger}\) & & 46.1d\({}^{\text{d}}\) & & \\
1B@300GT & The Pile & Pythia\({}^{\dagger}\) & & 45.2d\({}^{\text{a,d}}\) & & \\ \hline \hline
7B@350GT & RefinedWeb & Baseline & **55.3** & & \\
6B@300GT & OpenAI & curie\({}^{\dagger}\) & & 53.7d\({}^{\text{d}}\) & & \\
6B@400GT & The Pile & GPT-J\({}^{\dagger}\) & & 53.5d\({}^{\text{d}}\) & & \\ \hline \hline \end{tabular}
\end{table}
표 14: 우리의 데이터 레시피는 주로 RefinedWeb에 대한 우리의 작업(Penedo 등, 2023)에 기초하여 최신 기술로부터의 The Pile 및 다른 모델들에 대해 상당히 개선된다. 멀티쿼리는 모델을 더 작게 만들기 때문에(그리고 우리는 그 효과를 제어하지 않기 때문에), 우리의 아키텍처는 작은 제로 샷 성능 저하를 수반한다. 우리는 매개변수 수를 제어하면 아키텍처가 기준보다 동등하거나 더 좋아질 것이라고 의심한다. 그럼에도 불구하고, 아키텍처의 개선은 대부분 추론 및 훈련을 위한 하드웨어 확장성의 개선을 제공하는 반면, 데이터 레시피의 개선은 모델의 다운스트림 성능을 크게 향상시킨다. 관련 변경 내용에 대 한 밑줄, 기준선에 대 한 개선에 대 한 **굵은**, 기준선에 대 한 저하에 대 한 _italics_ 입니다. \ ({}^{\dagger}\)는 EleutherAI Harness(Gao et al., 2021)를 사용하여 독립적인 평가를 플래그하고, \({}^{\text{a}}\)은 아키텍처 실행이 더 나은 반면 \({}^{\text{d}}\)은 데이터 실행이 더 나은 것으로 나타낸다.

Implementation

이전 절제 섹션 4의 연구 결과와 문헌의 추가 테스트 및 모범 사례를 기반으로 이제 팔콘 시리즈 모델을 훈련하는 데 사용되는 코드 베이스 및 방법을 설명한다.

### Falcon 데이터 세트: 큐레이트 및 대화 데이터가 추가 된 주로 웹입니다.

계산 예산 30,000-50,000 PF-일의 추정치를 기반으로 **3,000-5,000 billion 토큰** 범위의 사전 훈련 데이터 세트 크기를 대상으로 합니다. 모델 크기의 경우 상위 경계로 Hoffmann 등 (2022)을 사용 하 고 사전 훈련 길이의 경우 하위 경계를 사용 합니다. 이것은 Chinchilla에 대한 데이터셋의 2배 이상이고, GPT-3에 대한 데이터셋의 10배 이상이다; 비록 이러한 크기의 범위가 최근에 LLaMA-2 또는 OLMo와 같은 동시 모델에서 더 일반화되고 있지만(Touvron et al., 2023; Soldaini et al., 2023). 데이터 반복으로 인한 암기 및 저하 우려(Carlini et al., 2022; Hernandez et al., 2022)에서 **원본을 업샘플링하지 않음** 을 선택합니다.

**고급 개요.** 섹션 4.2.1에서 충분히 필터링되고 중복 제거된 웹 데이터가 수행 모델을 제공할 수 있음을 보여주었습니다. 이는 필요한 규모를 달성하기 위해 웹 데이터를 스케일 업하는 데 중점을 둡니다. 데이터 품질의 개선은 다운스트림 성능의 상당한 개선으로 이어지며 데이터 처리가 모델 학습보다 매우 저렴하기 때문에 데이터 처리로 비용을 최적화하는 데 너무 신경 쓰지 않는다.

우리는 Reddit(Baumgartner et al., 2020)의 대화 추가와 함께 The Pile(Gao et al., 2020)에서 영감을 얻은 소량의 큐레이트 데이터를 여전히 포함하는데, 적절하게 처리되면 성능이 저하될 가능성이 낮고(섹션 4.2.2) 모델의 다운스트림 적용 가능성을 넓힐 수 있기 때문이다. 그러나 이러한 소스는 최종 데이터 세트의 13%를 차지하는 업샘플링을 허용하지 않는다는 점을 감안할 때 소수로 남아 있을 수밖에 없다. 코드 및 다국어성과 관련하여 4.2.3절의 결과를 기반으로 8%의 다국어 데이터와 3%의 코드를 포함하는 보수적인 접근법을 취한다. 실험한 것보다 이러한 낮은 분수는 재고 제약으로 인한 것이며, 특히 코드의 경우 파이프라인을 추가로 개선하여 가용성을 크게 확장할 수 있었지만 이는 모델이 훈련을 시작한 이후이므로 혼합을 수정하지 않았다.

최종 팔콘 혼합물은 표 15에 나와 있다. 우리는 선별된 소스의 업샘플링을 허용하지 않고 3,500B 토큰 사전 훈련 데이터 세트를 기반으로 혼합물을 설계했다. 다양한 훈련 길이에도 불구하고, 동일한 혼합물(%)이 팔콘-7B, 40B 및 180B에 사용된다.

\begin{table}
\begin{tabular}{l l c c} \hline \hline
**Corpora** & & \multicolumn{2}{c}{**Pretraining**} \\ Name & Source & Stock & Fraction & Used \\ \hline
**RefinedWeb-English** & Filtered and deduplicated Common- & \(\sim\)5,000B & 76\% & 2,700B \\  & Crawl, Penedo et al. (2023) & \(\sim\)2,000B & 8\% & 400B \\
**RefinedWeb-Euro** & Filtered and deduplicated multilingual (유럽 중심) Common- & 215B & 6\% & 214B \\
**Books** & Project Gutenberg & 170B & 5\% & 168B \\
**대화** & Reddit, StackOverflow, HackerNews, IRC, YouTube Subtitle & \(\sim\\)1,000B & 3\% & 115B \\
**Code** & GitHub & 60B & 2\% & 57B \\
**Technical** & arXiv, PubMed, USPTO, Wikipedia & 100B & 100B & 100B \\ \hline \hline \end{tabular}
\end{table}
표 15: 최종 팔콘 혼합물은 주로 웹 기반(거의 85%)이지만, 모델의 표현력을 넓히기 위해 (업샘플링 없이) 다른 선별된 말뭉치를 포함한다. 개별 큐레이트된 코포라는 The Pile(Gao et al., 2020)에서 영감을 얻었지만, 우리의 데이터 파이프라인과의 고품질 및 호환성을 보장하기 위해 처음부터 재구성되었다. 코드 스톡은 업데이트 된 파이프라인을 기반으로 하는 대략적인 추정치입니다. 팔콘에서 사용 되는 코드 데이터는 허용 된 라이선스 GitHub 리포지토리에서 공급 됩니다. 혼합물은 업샘플링을 피하기 위해 설계되었으며, 처리가 여전히 진행 중이기 때문에 RefinedWeb의 총 재고량은 교육 초기에 알려지지 않았다. 토큰의 수량입니다.

#### 5.1.1 Macrodata Refinement 파이프라인 및 RefinedWeb 데이터 세트

우리의 웹 데이터 처리 파이프라인은 전용 용지 RefinedWeb 용지에 광범위하게 기술되어 있다(Penedo et al., 2023). 이 섹션에서는 주요 구성 요소와 결정 사항만 강조합니다.

스케일-업 프리트레이닝 데이터를 위해, 두 가지 접근법이 가능하다:

* **데이터를 반복합니다.* * 이것이 가장 쉬운 옵션이며 원래 컴퓨터 비전의 표준이었습니다. 그러나, 대부분의 대형 언어 모델들은 훨씬 더 보수적이어서, 일반적으로 2-6회 동안 특정 말뭉치를 업샘플링할 뿐이다(표 3 참조). 이는 크게 암기(Carlini et al., 2022)에 대한 우려와 모델의 품질을 불균형적으로 저하시키는 중복(Lee et al., 2022; Hernandez et al., 2022)에 대한 우려 때문이다. 최근 Muennighoff 등(2023)은 동일한 데이터에 대해 최대 4개의 epoch가 허용될 수 있지만 추가 반복은 열화를 유발하여 이 전략을 피할 수 있다고 주장했다.
* **크기 조정 웹 데이터 처리** 큐레이션된 원본의 크기 조정은 번거롭고 광범위한 수동 작업이 필요하지만 웹 데이터는 거대하고 풍부한 원본입니다. 웹 데이터에 대한 개선은 한 번에 많은 양의 토큰에 영향을 미치기 때문에 높은 레버리지를 가지고 있다: CommonCrawl과 같은 공개 크롤은 50-100조 토큰을 초과하여 90%의 거부율조차도 조 단위의 데이터 세트를 초래할 수 있다. 그러나, 원시 웹 데이터 역시 매우 열악한 품질(Trinh and Le, 2018; Kreutzer et al., 2022)로, 바람직하지 않은 성인 콘텐츠 및 머신 생성 스팸을 다량 포함하고 있다. 대규모 필터링 및 중복 제거를 통해 웹 데이터의 품질을 개선하는 데 집중하기로 결정했습니다.

이러한 접근 방식은 직교적이지만 적대적이지는 않으며, 10-100조 토큰의 사전 훈련 데이터 세트를 사용하여 프론티어 모델로 확장하려면 몇 가지 시대에 걸쳐 대규모 웹 데이터 세트를 반복해야 할 가능성이 높다.

**철학.** RefinedWeb은 다음과 같은 방식으로 이전 웹 데이터 세트와 차별화됩니다.

* **Extreme-scale.** 매크로 데이터 정제 파이프라인은 확장성에 중점을 둡니다. 최대 20,000개의 CPU 코어를 사용하여 RefinedWeb을 생성했습니다. 거의 5조 개의 중복 제거된 토큰을 가진 RefinedWeb 데이터 세트는 문서화된 가장 큰 사전 훈련 데이터 세트이며, 여러 에포크에 의존하지 않고 가능한 이전보다 더 큰 모델의 훈련을 지원한다.
* **Stringent 중복 제거 및 필터링.** Lee 등(2022)에서 영감을 받은 RefinedWeb은 완전히 중복 제거되었습니다. MinHash를 이용한 퍼지 중복제거를 이용하여 먼저 데이터셋을 크게 축소한 후 부분 문자열 중복제거를 추출한다. 필터링 휴리스틱은 또한 텍스트 추출 인공물을 감소시키고 기계 생성 콘텐츠를 제거하기 위해 먼저 사용된다. 섹션 4.2.1에서 이것이 웹 데이터가 큐레이팅된 말뭉치와 일치할 수 있다는 것을 발견했다.
* **중립 필터링.** 언어 식별을 제외하고 Macrodata 정제 파이프라인은 ML 기반 필터링 전략에 의존하지 않습니다. 실제로, 이러한 필터는 데이터 Dodge 등(2021); Welbl 등(2021)에 편향을 쉽게 도입하거나 증폭할 수 있다.

그림 6: 매크로데이터 세분화의 후속 단계는 원래 커먼크롤에 있는 문서의 거의 90%를 제거한다. 특히, 필터링 및 중복 제거는 각각 사용 가능한 데이터의 절반으로 귀결되는데, 문서의 약 50%는 영어가 아닌 것으로, 나머지 문서의 24%는 품질이 불충분한 것으로, 12%는 중복된 것으로 폐기된다. 우리는 각 이전 단계에 대한 제거율(회색)과 전체 유지율(음영)을 보고한다. 도면 Penedo 등(2023).

**개요.** 매크로 데이터 정제 파이프라인은 다음 세 단계로 분할됩니다 (그림 참조). 상세한 제거율에 대한 6): (1) 문서 준비; (2) 필터링; (3) 중복제거.

문서 작성을 위해 계산량이 많은 처리를 수행하기 전에 먼저 성인 사이트의 블록 리스트를 사용하여 URL만 기반으로 문서를 필터링하고 이름을 기반으로 URL을 채점한다. CommonCrawl에서 제공하는 전처리된.NET 파일에는 여전히 원하지 않는 내용(예: 탐색 메뉴)이 포함되어 있음을 발견하여 원시 WARC 파일(HTML 응답)을 trafilatura로 처리하여 자연 텍스트를 추출한다. 마지막으로, 문서의 최상위 언어를 식별하기 위해 CCNet(Wenzek 등, 2020)의 fastText 분류기를 사용한다. 영어의 경우 약 48%의 문서가 남아 있습니다.

필터링 단계에서는 반복된 텍스트(크롤링/텍스트 추출의 인공물일 수 있음)와 길이, 기호 대 단어 비율 등의 이상치인 문서를 제거하기 위해 다수의 휴리스틱을 적용한다. 이러한 휴리스틱은 Rae 등(2021)에 의해 영감을 받았다. 좋아요 카운터나 네비게이션 버튼과 같은 남아 있는 유물을 제거하는 소위 라인별 수정도 소개합니다. 적합한 데이터의 크기는 절반에 반대하여 커먼크롤의 약 23%가 유지된다.

마지막으로, 대규모 중복제거를 두 단계로 적용하는데, 먼저 MinHash(Broder, 1997)를 사용하여 문서 레벨에서 근사 중복을 제거하고, 접미사 배열(Manber and Myers, 1993)을 사용하여 정확한 부분 문자열 매칭을 제거한다. Lee 등(2022)의 중복 제거 설정으로, 이는 CommonCrawl에서 전체 데이터의 약 12%로 감소된, 사용 가능한 데이터의 최종 절반으로 귀결된다.

#### 5.1.2 Microdata 큐레이션된 코퍼스와 대화형 마스킹

섹션 4.2.2에서 대화, 책 또는 기술 출처에서 큐레이팅된 데이터를 추가해도 RefinedWeb과 같은 강력한 웹 기준선 위에 성능이 더 이상 향상되지 않는다는 것을 발견했다. 그러나 이러한 종류의 데이터는 코드와 함께 모델의 표현성을 확장할 수 있으며 평가 설정에서 캡처되지 않은 다양한 종류의 다운스트림 작업에 적용할 수 있다고 믿는다. 따라서, 우리는 또한 (가오 등, 2020)에서 영감을 받은 개별 소스를 사용하여 큐레이트된 데이터의 작은 부분을 추가한다. 또한 Reddit(Baumgartner et al., 2020)의 데이터를 추가하고, 트리형 대화를 효율적으로 포맷하기 위한 새로운 주의 마스킹 전략을 소개한다. 마이크로 데이터 큐레이트된 말뭉치 위에 수정된 필터 설정(예: 책에 대한 문서 길이 임계값 조정)과 중복된 개별 말뭉치를 사용하여 매크로 데이터 정제 파이프라인을 적용한다.

**The Pile의 구성 요소.** 중요한 품질 또는 라이선스 문제가 있는 경우를 제외하고 The Pile의 개별 구성 요소(Gao 등, 2020)를 재사용합니다. 모든 경우에 데이터 파이프라인과 형식이 일치하는지 확인하기 위해 처음부터 다시 구현합니다. 큐레이트된 코퍼라와 더 나은 제어 생성에서 구조화된 정보를 재현하기 위해 많은 특수 토큰을 도입했습니다. *ITITLE*, *ABSTRACT*, *INTRODUCTION* 및 *COMMENT* 입니다. IETEX 파일을 이용할 수 있을 때, Lewkowycz 등(2022)과 유사하게 Markdown으로 변환한다. 매크로데이터 정제(Macrodata Refinement)의 휴리스틱(heuristics)은 각 말뭉치에 대해 수동으로 조정하고 거부 및 수락된 샘플을 수동으로 분석하는 방법이며, 책의 경우 색인, 거부권 또는 내용 표와 같은 관련 없는 내용을 제거하기 위한 새로운 규칙을 소개한다.

**대화 데이터.** 점차로 대형 언어 모델은 사용자와 모델 간의 전후 상호 작용과 함께 "채티" 사용 사례에 배포됩니다 (Adiwardana et al., 2020; Zheng et al., 2023). 이 사용 사례에 대해 하류에 적응된 거친 모델, 우리는 대화 데이터로 사전 훈련을 강화하는 데 중점을 두었다; 우리는 특히 Reddit의 데이터를 추가한다(Baumgartner et al., 2020).

**대화 트리 및 주의 마스킹.** Reddit 또는 HackerNews와 같은 온라인 포럼의 데이터를 사용하는 한 가지 문제는 이 데이터가 사용자 간에 대화가 분기되는 트리로 형식화된다는 것입니다. 과거 모델은 이러한 나무에서 궤적을 샘플링했지만(Thoppilan et al., 2022; Chowdhery et al., 2022), 이는 데이터가 반복되거나 일부 궤적이 누락되어야 함을 의미한다. 대신 주의 마스크를 사용하여 트리 같은 구조를 인코딩하여 나중에 주석이 "측면" 주석을 무시하고 궤적의 주석에만 참석할 수 있음을 발견했다. 대화는 깊이-우선 순서로 시퀀스로 직렬화되고, 이후 주석/발음은 현재와 관련이 없는 경우(즉, 다른 분기 또는 더 깊은 경우) 마스킹된다. 위치 임베딩을 위해, 우리는 자연스럽게 대화를 직렬화하는 트리의 깊이를 사용한다. 부록 F.2에서 깊이 첫 번째 위치를 주의 마스크로 변환하기 위한 의사 코드를 사용하여 설명한다. 또한 이 전략을 사용하여 <EOD> 토큰에만 의존하는 대신 문서를 서로 마스킹한다. 피상적인 실험은 제로 샷 성능에 대한 이점을 찾지 못했다는 점에 유의하라.

### 효율적인 추론 및 (안정적인) 훈련을 위한 Falcon 아키텍처 및 레시피

팔콘 아키텍처의 목표는 훈련 및 추론 효율성을 최대화하는 동시에 모델에 대한 다운스트림 성능 및 위험에 대한 영향을 최소화하는 것이다. 섹션 4.3에서 삭제를 기반으로 한 여러 결정의 개요를 설명했다.

* **아키텍처.** 추론의 확장성을 개선하기 위해 다중 그룹 주의(섹션 4.3.1)를 사용합니다. 멀티쿼리의 확장(Shazeer, 2019); 회전 임베딩(Su et al., 2021); 메모리 사용량이 증가했기 때문에 GLU(Shazeer, 2020)를 사용하지 않고 대신 바닐라 GLU를 사용합니다. 병렬 주의 및 MLP 블록(Wang and Komatsuzaki, 2021)을 사용하고 선형 레이어에서 편향을 제거합니다(Chowdhery et al., 2022).
* **Hyperparameters.** z 손실을 사용 하 여 안정성을 돕습니다 (Shazeer et al., 2018). 고정 0.1 중량 감쇠를 사용 합니다. 준비 작업 중에 학습 속도 로그 그리드 검색을 수행 하 고 준비 작업이 끝날 때 가장 손실이 적은 학습 속도를 선택 합니다.

#### 5.2.1 Architectural nitpicks: separate layer norms, tied embeddings and scaling-up

**계층 규범.** 병렬 주의력을 사용할 때 MLP 및 주의 블록에 대해 별도의 계층 규범(바닐라 Transformer 아키텍처에 가까움)을 갖거나 둘 다에 대해 통합 계층 규범을 사용할 수 있습니다. 계층 규범의 입력에 대한 기울기 계산은 선형이기 때문에 두 개의 계층 규범을 사용하면서 병렬 주의와 MLP의 유리한 통신량을 유지할 수 있다. 또한, 훈련 후, 계층 규범의 가중치와 편향을 후속 선형 계층에 곱함으로써 두 계층 규범을 다시 하나로 병합할 수 있다. 이것은 바닐라 아키텍처에 가깝게 유지하고 두 개의 개별 계층 규범을 사용하도록 유도하지만, 우리는 이것이 다른 인기 있는 포맷으로의 다운스트림 변환을 위해 불필요한 추가 복잡성을 도입한다는 점에 주목한다. Falcon-7B(나중에 훈련됨)의 경우 단일 계층 표준으로 전환했다.

**연결된 임베딩.** 연결 임베딩은 Transformer 모델의 유비쿼터스 관행입니다. 토큰 \(x\)을 \(z^{0}=xW\)으로 변환 하는 임베딩 가중치는 임베딩을 예측 로짓 \(p=z^{u}W^{T}\)으로 다시 변환 하는 것과 동일 합니다. 가장 최근의 LLMs(GPT:3 Brown et al. (2020), PaLM Chowdhery et al. (2022), LLaMA Touvron et al. (2023a))에 의해 여전히 사용되지만, 원래의 동기(Press and Wolf, 2016; Inan et al., 2016)는 완전히 관련되지 않을 수 있다. 특히 가중치 공유는 모델의 크기를 줄이는 데 사용되었지만 1,000억 개 이상의 매개변수를 가진 모델의 경우 임베딩이 매개변수의 중요한 부분이 아니다. 또한, 가중치 공유는 추가 통신을 요구하기 때문에 분산 훈련에 어려움을 제기한다. 의미론적 논증은 남아 있지만 피상적 실험은 1B 매개변수 규모에서 강한 영향을 보여주지 않았다. 그럼에도 불구하고, 팔콘 시리즈의 훈련에 추가적인 위험을 추가하지 않기 위해 우리는 임베딩을 계속 묶는다.

도 7: 트리형 어텐션 마스킹은 데이터를 샘플링하거나 반복하지 않고도 모든 대화 궤적을 실현할 수 있게 한다. 트리는 직렬화된 깊이-우선이며, 여기서 토큰의 위치는 트리 내의 깊이이다. 이를 통해 대화의 초기 전환을 반복하지 않고 트리에서 모든 대화를 동시에 훈련할 수 있습니다. 예를 들어, 그 구절은 _예! 사랑스러워요. 분홍색으로요. 파리를 좋아하세요? 그 자체로 녹색과 인과적 주의를 기울이지만 다른 구절은 볼 수 없다.

**어휘 크기.** 언어 모델에서 어휘의 크기는 256개의 항목을 포함하는 문자 수준 모델(Xue 등, 2022)에서 수백만 개를 포함하는 대규모 다국어 모델(Liang 등, 2023)까지 크게 다를 수 있습니다. 생성 모델의 경우, 관행은 단일 언어 모델의 경우 30-60k 범위의 어휘(Brown et al., 2020; Rae et al., 2021; Touvron et al., 2023a) 또는 다중 언어성으로 더 기울어진 모델의 경우 +100k 범위의 어휘(Scao et al., 2022; Chowdhery et al., 2022)의 두 가지 모드를 중심으로 붕괴되었다. 비록 더 큰 어휘들이 더 나은 다산성을 가질 수 있고, 따라서 텍스트 바이트당 더 빠른 추론을 산출할 수 있지만, 그들은 또한 일부 주의사항들을 수반한다: 확장성 관점에서, 그것들은 불균형한 파이프라인 스테이지들을 초래할 수 있고, 더 많은 저장 공간을 필요로 할 수 있다; 또한, 모델들이 그것들을 최적으로 사용하는지 여부가 불분명하다(Lieber et al., 2021). 우리는 65,024의 어휘 크기에 대해 토큰라이저를 훈련시키고, 16비트 부호없는 정수에 어휘 정보를 저장한다. 이는 다운스트림 적응에 사용할 약 500개의 여분의 값을 남긴다(예를 들어, UL2 Tay 등(2022b)에 대한 패러다임 토큰).

**모델 크기 조정.** 표 16에서 팔콘 모델의 모양 및 하이퍼 매개 변수의 개요를 설명합니다. 계산 예산을 늘릴 때 리소스를 더 큰 모델(매개 변수 수 증가) 또는 더 긴 학습(토큰 수 증가)에 사용할 수 있습니다. Hoffmann 등(2022)은 최적의 스케일링을 위해 관절(즉, 동등) 증가를 추천한다. 그러나 이 발견은 두 가지 방식으로 미묘하게 요약되어야 한다: (1) 매개변수 수를 늘리면 다운스트림 추론 비용이 증가하며, 이는 모델이 널리 배포되는 경우 중요할 수 있다; (2) 반면 데이터가 제한된 경우 크기를 늘리는 것이 다운스트림 성능 증가를 위해 계산을 거래하는 방법이 될 수 있다. Falcon의 경우 Hoffmann 등(2022)을 사전 훈련 길이에 대한 하한으로, 모델 크기에 대한 상한으로 사용하도록 선택한다.

모델 크기 외에도 이러한 매개변수를 형성하는 방법에 대한 질문도 있다. 예를 들어 더 깊거나 얕은 모델을 만들고 주의력을 넓히거나 수를 늘리기 위해 할당해야 한다. Scao 등(2022b)은 짧은 검토를 수행했는데, 이는 모델 깊이가 일반적으로 총 파라미터 카운트로 대수적으로만 스케일링된다는 것을 강조한다(Levine 등, 2020). 그러나 주의력 헤드 크기 주변의 연습은 다양했다. 우리는 로그 깊이 스케일링 추천을 광범위하게 따르고, 플래시어텐션(Dao et al., 2022)으로 성능을 최적화하기 위해 64의 고정된 어텐션 헤드 사이즈를 사용한다. 또한, 다중 그룹의 질의 수를 훈련 시 사용되는 텐서 병렬도의 수와 같도록 고정하고, 2,048의 고정된 시퀀스 길이에 대해 훈련한다. 이 컨텍스트 길이는 사후 적응(Chen et al., 2023)을 통해 효율적으로 증가될 수 있다는 점에 주목한다.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **Falcon-7B** & **Falcon-40B** & **Falcon-180B** \\ \hline
**Data** & 1,500B & 1,000B & 3,500B \\ \hline
**Shape** & & & \\ \(n_{\text{layer}}\) & 32 & 60 & 80 \\ \(d_{\text{model}}\) & 4,544 & 8,192 & 14,848 \\ \(d_{\text{head}}\) & & 64 & \\ \(n_{\text{q}}\) & 71 & 128 & 232 \\ \(n_{\text{kv}}\) & 1 & 8 & 8 \\ \(d_{\text{vocab}}\) & & 65,024 & \\ \(n_{\text{tokens}}\) & & 2,048 & \\ \hline
**Pretraining** & & & \\ Learning rate & \(6\times 10^{-4}\) & \(1.85\times 10^{-4}\) & \(1.25\times 10^{-4}\) \\ Decay & & Cosine, divides by 10 & \\ Ramp-up & 4B & 4B & 4B \\ Batch-size & 2,304 & 1,152 & 2,048 \\ Warm-up & 30B & 100B & 100B \\ Weight decay & & 0.1 & \\ Gradient clipping & 1. & 0.6 & 0.4 \\ Z-loss & & \(1\times 10^{-4}\) & \\ \hline
**Parallelism** & & & \\ TP & 1 & 8 & 8 \\ PP & 2 & 4 & 8 \\ DP & 192 & 12 & 64 \\ \hline \hline \end{tabular}
\end{table}
표 16: **Falcon 모델의 모양, 하이퍼 매개 변수 및 배포 전략의 요약** Falcon-7B는 실험적인 증가된 배치 크기로 Falcon-40/180B 이후에 훈련되었습니다.

#### 5.2.2 Large language model alchemy: preraining을 위한 hyperparameters

우리는 표 16에서 사전 훈련 중에 사용된 하이퍼 매개 변수를 보고하고 아래 몇 가지 결정을 강조한다.

**학습률 검색.** 섹션 4.3.5에서 설명하는 절차는 Falcon-7, 40 및 180B에 대해 각각 \(6\times 10^{-4}\), \(1.85\times 10^{-4}\) 및 \(1.15\times 10^{-4}\)의 학습률을 가져옵니다. 이는 이전 모델에서 보고된 학습률보다 훨씬 높지만 훈련 실행이 (대부분) 안정적이라는 것을 발견했다. 돌이켜보면 스파이크에서 복구하는 것이 비교적 쉽기 때문에 검색 절차가 최적-허용 가능한 절충보다 더 높은 학습률을 초래할 수 있다고 믿는다(섹션 5.4).

**학습 속도 증가** 모든 모델에 대해 40억 토큰 이상의 긴 증가 토큰을 수행 합니다.

**배치 크기 준비.** 배치 크기 준비 주변의 관행은 기본 하드웨어에 따라 놀라울 정도로 광범위하게 분기되었습니다. GPT-3(Brown et al., 2020), BLOOM(Scao et al., 2022) 또는 MT-NLG(Smith et al., 2022)와 같이 GPU에서 훈련된 모델은 일반적으로 100-200억 토큰에 대해 세밀한 준비 작업을 수행했습니다. 한편, TPU에서 훈련된 모델은 Gopher(Rae et al., 2021), Chinchilla(Hoffmann et al., 2022) 또는 PaLM(Chowdhery et al., 2022)과 같이 종종 배치 크기 중간 훈련을 두 배로 늘리거나 훈련을 통해 25/50%로 더 큰 배치 크기 단계를 수행합니다. 일부 최근의 모델들은 또한 배치 사이즈 워밍업을 완전히 건너뛰기로 선출했다(Zhang et al., 2022; Touvron et al., 2023, 20). 소규모에서, 우리는 더 긴 웜업이 다운스트림 성능에 영향을 미치지 않는다는 것을 발견했고, 실제로 일반적으로 더 나은 모델을 제공한다; 이것은 훈련 동안 데이터 병렬성 정도와 클러스터의 전체 크기를 확장할 수 있다는 1,000억 토큰 이상의 긴 웜업 전략을 채택하게 하며, 이는 제한된 처리량 비용을 초래한다. Falcon-7B의 경우 벽 시계 시간 제약으로 인해 300억 토큰 이상의 가속화된 일정을 선택할 수 있습니다.

**Gradient 클리핑.** Falcon-40B의 경우 Gradient 클리핑에 대한 임계값을 0.6으로 설정합니다. Falcon-180B의 경우 처음에는 0.6으로 시작했지만 나중에 훈련 안정성을 개선하기 위해 0.4로 줄였다. 두 경우 모두 기울기 클리핑 임계값을 조정하여 대부분의 훈련 단계가 아닌 이상치 이벤트에만 영향을 미쳤다. Falcon-7B의 경우 1로 설정했다. 이 규모에서의 훈련은 우리가 채택한 큰 배치 크기에도 불구하고 실제로 불안정하지 않다는 것을 발견했다.

**Optimizer.** AdamW(Loshchilov and Hutter, 2017): 가장 일반적으로 사용되는 최적화기이며 일반적으로 그리고 특히 대규모 언어 모델 훈련에 대해 잘 수행하는 것으로 다시 한번 입증되었습니다. 훈련 시 성능을 높이기 위해 Megatron Shoeybi 등(2019)의 융합 최적화 커널을 사용한다. 그러나 옵티마이저를 위한 융합 커널은 특히 옵티마이저 샤딩을 사용할 때 대규모 훈련에서 덜 중요하다는 점에 주목한다.

**탈락 없음.** 대규모 언어 모델은 일반적으로 비교적 고유한 데이터의 단일 에폭에 대해 훈련되므로 일반적으로 드롭아웃을 사용하지 않습니다 (Srivastava 등, 2014). 몇 개의 에포크를 제안하는 데이터 및 최근 논문의 부족은 견딜 수 있을 수 있다(Xue et al., 2023). 미래 모델은 수십 조 개의 토큰을 필요로 하지만 팔콘 시리즈의 경우 중퇴를 사용하지 않았기 때문에 이러한 관행에 분명히 도전할 것이다.

### Gigatron을 사용 하 여 클라우드 인프라에서 대규모 분산 교육

고가의 전용 HPC 자원에 대한 교육보다는 비용 효율성을 개선하기 위해 클라우드 인프라에서 팔콘 시리즈를 교육하기로 결정했다. 팔콘 시리즈는 팔콘-180B에 대해 최대 4,096 A100s로 AWS의 p4d 군집에 대해 훈련되었다. 교육 인프라에 대한 주요 메트릭은 다음과 같습니다.

* **\(8\times\mathbf{A100}\) 40GB를 사용 하는 노드** 전력 또는 세분성 제약 조건이 있는 일부 데이터 센터에서 인기 있는 4x A100 40/80GB를 사용 하는 구성이 텐서 병렬성의 사용 가능한 정도 감소 (8 대신 4)로 인해 처리량이 감소 한다는 것을 발견했습니다. 그러나 A100의 40GB 버전은 가용성 및 비용 효율성을 높였습니다.
* **GPU당 50Gbps 인터커넥트.** 최신 인프라에는 A100당 200Gbps 인터커넥트가 제공되며, 대기 시간이 짧은 InfiniBand로 구동됩니다. 그러나 이러한 구성은 엄청나게 비쌀 수 있습니다. 우리는 팔콘-180B의 크기까지의 모델에 대해 대역폭이 전체 처리량에 제한된 영향만 미친다는 것을 발견했다(대부분 데이터 병렬도에 걸쳐 all_reduce의 크기와 연결됨). 반대로, p4d에서 EFA의 더 높은 지연 시간은 파이프라인 통신, 특히 소규모 모델의 주요 병목 현상이었다.
* **분산 파일 시스템 없음** 전용 파일 시스템에 의존하는 대신 S3에서 직접 데이터를 스트리밍합니다. 분산 파일 시스템은 비용이 많이 들고 유지 관리가 어려우며, 대규모 언어 모델 훈련으로 인해 발생하는 작은 데이터 I/O 볼륨은 사용을 정당화하지 못한다.

따라서 우리의 인프라는 진정한 HPC 시스템과 유연한 클라우드 환경 사이에 존재하며, 특히 멀티스핀 구성이 신뢰할 수 없기 때문에 GPU가 데이터 센터에서 단일 스파인을 공유해야 한다는 것을 발견했다. 비용 효율적이긴 하지만, 한계도 염두에 두어야 합니다. 우리는 완전히 샤딩된 데이터 병렬 처리(Rajbhandari et al., 2020)를 사용한 인기 있는 (그리고 간단한) 훈련 레시피가 이 인프라에 잘 맞지 않는다는 것을 발견했다. 대신, 최적의 성능을 달성하기 위해 3D 병렬성(Narayanan et al., 2021)의 더 미세한 제어가 필요했다.

당시 오픈 소스 프레임워크의 한계로 인해 자체 독점 분산 교육 프레임워크를 구축하기로 결정했습니다. 기가트론은 파이토치를 기반으로 하며, 그 코어에서 메모리 소모를 줄이고 확장성을 향상시키기 위해 ZeRO 옵티마이저 샤딩(Rajbhandari et al., 2020)과 결합된 3D 분산 병렬화 전략(Shoeybi et al., 2019; Narayanan et al., 2021)을 구현한다.

#### 5.3.1 3D parallelism for fine-grained control and ZeRO for scalability

**데이터 병렬성(DP).** 데이터 병렬성(도 8) 는 단연코 가장 일반적으로 사용되는 병렬성의 형태이다. 모든 머신 러닝 프레임워크들은 이제 데이터 샘플들에 걸쳐 모델 트레이닝을 쉽게 병렬화하는 방법들을 제공한다 : Pytorch with Distributed Data Parallel (DDP), jax with pmap, and tensorflow with MirroredStrategy (Paszke et al., 2017; Bradbury et al., 2021; Abadi et al., 2015). 데이터 병렬성은 단순성 덕분에 매력적이며, 분산 기계는 사용자 정의 아키텍처와의 상호 작용 없이 프레임워크 내부에 쉽게 숨겨질 수 있다. 가장 간단한 구현에서, 데이터 병렬화는 트레이닝 절차에 대한 두 가지 수정만을 필요로 한다 : (1) 각각의 디바이스는 고유한 데이터 샘플들에 대해 동작할 필요가 있다; (2) 가중치들을 동기화 상태로 유지하기 위해 디바이스들에 걸쳐 구배들이 감소될 필요가 있다. 이 all_reduce는 일괄 처리 크기에 따라 증가 하 여 결국 데이터 병렬 처리 대역폭 제한을 만듭니다. 또한, 데이터 병렬성은 모든 치료법이 아니다. 모델이 샤딩되지 않기 때문에 더 많은 장치를 추가하더라도 장치당 메모리 공간은 일정하다. 따라서 데이터 병렬성만으로는 단일 장치에 맞는 모델로 제한된다.

**텐서 병렬 처리(TP)** 디바이스 간에 모델을 공유하려면 모델 병렬 처리로 전환해야 합니다. Shoeybi 등(2019)에 의해 가장 인기 있는 형태로 소개된 텐서 병렬성은 주의 블록 내의 선형 층들을 분할하고, 통신 볼륨을 감소시키기 위한 원리적인 방식으로 MLP를 분할한다. 이는 폭방향 모형평행성의 사례로 볼 수 있다. 구체적으로, 이층 MLP의 간단한 경우에 대해, 첫 번째 층 열과 두 번째 행을 평행하게 함으로써, 최종 결과를 생성하기 위해 단지 단일의 모든 감소만이 필요하며-중간 결과의 통신은 필요하지 않다. 그래디언트를 후방으로 전파하기 위해 행렬이 전치되어 행 병렬 계층이 열 병렬 계층으로 바뀌고 그 반대로 효율적인 통신 패턴을 유지한다. 우리는 그림 9에서 열과 행 병렬성을 설명한다. 유사한 접근법을 사용하여 주의 블록을 분할하고 GPU를 가로질러 헤드를 분할할 수 있으며, 여기서 K,Q,V 계산은 열 병렬 및 최종 투영 행 병렬이며 모든 계산은 GPU 간에 서로 독립적이다. 이에 따라, 트랜스포머 내의 전체 블록들이 이러한 방식으로 효율적으로 병렬화될 수 있다. 그러나 텐서 병렬은 높은 대역과 낮은 지연 시간을 갖는 인터커넥트가 모두 유효해야 하므로, 현재 GPU 인프라에서는 단일 노드 내에서 제약되어 노드 간에 효율적으로 사용할 수 없다. 따라서 모델은 일반적으로 최대 8까지의 텐서 병렬 차수로 훈련될 것이며, 이는 여전히 모델의 충분히 작은 샤드들을 생성하기에 불충분할 수 있다.

그림 8: **데이터 병렬 처리는 각 디바이스에 모델 복제본을 만들고 다른 샘플을 병렬로 처리합니다.* * 모델 복제본은 서로 다른 장치에 배치되고 서로 다른 데이터 샘플의 기울기를 병렬로 계산합니다. 그런 다음 최적화 단계가 수행되기 전에 기울기가 감소합니다. **

**파이프라인 병렬 처리.** 모델 병렬 처리는 계층을 다른 가속기에서 실행할 후속 단계로 그룹화하여 깊이별로 수행할 수도 있습니다. 그러나, 순진한 파이프라인 병렬화는 비효율적이거나 즉각적인 바리케이트를 초래할 것이다: 실제로, 스테이지들은 다수의 전방 및 후방으로 동시에 프로세싱하기 위해 배치들을 분할해야 한다. 우리는 현재 일반적으로 1F1B로 지칭되는 Narayanan 등(2021)의 PipeDream-Flush 스케쥴을 채택한다. 우리는 Interleaved-1F1B(Narayanan et al., 2021)와 같은 더 많은 관련된 스케줄이 모델 복제물당 마이크로바치의 수가 일반적으로 64보다 작을 때 유익하다는 것을 발견했다. 배치 크기 램프업이 완료된 후, 더 많은 수의 마이크로바치로 훈련한다; 따라서 인터리빙으로부터 어떠한 속도 향상도 관찰하지 않는다. 통신 볼륨을 감소시키기 위해, 우리는 일명 산포-수집 최적화들을 일관되게 이용하는데, 여기서 다음 스테이지로 전송된 활성화들은 먼저 텐서 평행도 위에 샤딩된 후 다음 스테이지에 일단 수신되면 다시 수집된다(예시의 경우 도 10 참조). 이렇게 하면 기본 상호 연결 토폴로지(게더리스가 실행되는 NVLink를 통한 노드 간 통신이 훨씬 더 빠릅니다)를 최적화하여 노드 간 연결이 레일을 최적화한 경우 더욱 그렇습니다. 이 최적화는 파이프라인 병렬에 대한 통신량을 텐서 병렬 세계 크기의 인자만큼 감소시키며, 이는 우리의 경우 8이다.

**Optimizer sharding.** 모델 가중치 및 기울기는 연속 bfloat16 메모리 버퍼에서 구체화됩니다. (Rae 등, 2021)과 유사하게, 최적화기는 업데이트들을 계산할 그러한 가중치들/구배들의 fp32 버전을 유지한다. 이 설정에서 Adam 최적화기를 사용할 때 지수 평균을 추적해야 하므로 모델 매개 변수당 20바이트가 필요합니다.

\[\underbrace{2_{\text{model\,param}}+2_{\text{model\,grad}}}_{\text{bf float16}}+ \underbrace{4_{\text{opt\,param}}+4_{\text{opt\,grad}}+4_{\text{exp\,avg}}+4_ {\text{exp\,avg}}_{\text{float32}}=20\text{ bytes/param} \tag{1}\]

Falcon-7/40/180B의 전체 상태는 복제본당 140GB, 800GB 및 3,600GB의 메모리를 차지하며, 무게+구배+메모리 상태에서만 복제본당 최소 4, 20 또는 90 A100 40GB가 필요하다.

그림 9: **열 및 행 병렬성을 변경 하면 두 개의 후속 행렬 곱셈 간에 통신이 필요 하지 않으므로 텐서 병렬성이 GPU에 걸쳐 주의 및 MLP 블록을 효율적으로 분할할 수 있습니다. 열 병렬 행렬 곱셈의 경우, 입력은 모든 GPU에 걸쳐 복제(백워드에서의 모든_리듀스)되며, 그 후 독립적으로 그들의 동작을 수행할 수 있다. 결과는 행 병렬 방식으로 다음 행렬 곱셈을 실행하기 위해 GPU에 걸쳐 이미 분할된다. 마지막으로, 각 GPU의 출력은 all_reduce를 통해 합산된다. 두 행렬 곱셈 간의 중간 결과는 전달 되지 않습니다.* *최적화 상태만 메모리 사용량의 80%를 차지 하 고 결국 메모리 사용을 병목 현상 하 여 예를 들어 배치 크기를 증가 하 여 대신 GPU를 더 포화 하는 것을 방지 합니다. 따라서 문헌(라지반다리 등, 2020)에서 일반적으로 ZeRO-1로 지칭되는 관행인 데이터 병렬도에 걸쳐 최적화기 상태를 샤드하고 그림에 예시한다. 11. 각각의 데이터 병렬도 상에 최적화기의 완전한 중복 사본을 저장하는 것이 아니라, 최적화기는 독립적으로 샤드된 DP 시간들이다. 파라미터당 바이트 수를 샤딩하는 옵티마이저 스테이트와 함께 증가된 데이터 병렬성으로 더욱 감소되어, 확장성을 개선한다:

\[\text{Bytes per parameter}=4+\frac{16}{\overline{\text{DP}}} \tag{2}\]

Falcon-7/40/180B의 경우 이제 모델 복제본을 보관하는 데 30, 215, 765GB의 메모리 또는 1, 6, 20 A100 40GB만 필요합니다. 이들은 활성화 메모리를 설명하지 않는다는 점에 유의한다. 이제 최적화기가 분할되었으므로, 우리는 역방향에서 증가된 통신 부담을 직면해야 한다. 먼저, bfloat16 모델 구배 상에 reduce_scatter가 적용된다: 각각의 최적화기 샤드에 대해, 이것은 모든 DP 도에 걸쳐 감소된 관련 구배 샤드를 도출한다. 그 다음, 결과 동기화된 그래디언트 청크는 최적화기에 의해 사용하기 위해 fp32 최적화기 그래디언트 버퍼에 복사된다. 최적화기 단계가 수행되어, 관련 fp32 샤드된 최적화기 가중치들의 업데이트된 사본이 생성된다. 마지막으로, all_gather는 앞서 언급한 fp32 샤드된 최적화기 가중치로부터 모든 작업자에 대한 bfloat16 모델 가중치를 통합하는 데 사용된다. 이 ZeRO-1 워크플로에 대 한 의사 코드는 부록 F.3에 제공 됩니다. 흥미롭게도 총 통신 볼륨 측면에서 이 워크플로는 데이터 병렬 설정 Rajbhandari 등 (2020)에서 일반적으로 사용 되는 all_reduce와 **동등** 합니다.

모든 스케일(10억 매개 변수에서 1,800억 매개 변수)에서 최적화기 샤딩은 기존 데이터 병렬성에 비해 성능 오버헤드가 없음을 발견했다. 실제로, 최적화기 샤딩은 메모리를 자유롭게 하기 때문에 더 큰 마이크로배치 크기를 사용할 수 있게 하여 자원 포화를 개선하고 체계적으로 처리량을 높일 수 있다.

**순서 병렬 처리 없음.** Li 등(2021)은 훈련 중 활성화 메모리를 줄이기 위한 새로운 전략을 제안했습니다. 이러한 활성화가 다른 방식으로 복제되는 텐서 병렬 작업자에 걸쳐 잔차 스트림에서 활성화를 분할할 수 있습니다. 이 샤딩은 비교적 싸다: MLP 이후에 all_reduce를 사용하는 대신에, 그것은 다음 디코더 블록 바로 전에 reduce_scatter 다음에 all_gather로 대체될 수 있다. 그러나 5.3.3에서 논의된 메모리를 절약하기 위한 조치를 구현한 후 서열 병렬성이 필요하지 않음을 관찰한다. 또한 처리량이 약간 감소하여 최종 교육 중에 사용하지 않기로 결정했습니다.

그림 10: **각 순위가 전체 텐서를 중복으로 보내는 대신 분산/수집 최적화에는 각 순위가 텐서의 조각을 보낸 다음 빠른 인트라노드 상호 연결을 활용하여 다시 수집합니다. 이 패턴은 노드 간 통신이 일반적으로 노드 간 통신보다 훨씬 느리기 때문에 더 효율적이다. 각 순위가 이미 전체 텐서를 가지고 있기 때문에 산포는 효과적으로 제거되지 않는다는 점에 유의해야 한다. 인터노드 레일 최적화는 일반적으로 서로 다른 노드의 동일한 순위의 GPU가 최소 매개체로 피어 투 피어를 통신할 수 있도록 하여 이 패턴을 더욱 가속화합니다.* *

#### 5.3.2 전용 Triton 커널을 사용한 최신 처리량

**플래시 주의력.** 메모리 효율적인 주의력 대안은 오랫동안 커뮤니티의 관심을 얻었지만 주의 계획에 상당한 변화가 발생했으며, 특히 스케일 업 시 다운스트림 성능이 저하되는 경우가 많습니다. 메모리 내의 전체 주의 매트릭스의 구체화를 요구하지 않고 주의를 계산하기 위해 정확한 알고리즘이 개발된 것이 (상대적으로) 최근에야 있다(Rabe and Staats, 2021). Dao 등(2022)은 후속적으로 동일한 기술을 활용하는 통합된 커널이 모델 학습을 상당히 빠르게 할 수 있음을 보여주었다. 우리는 Triton(Tillet et al., 2019)에서 사용자 지정 구현을 사용하며, 이는 Dao(2023)를 위해 동시에 개발된 개선사항들 중 일부를 통합한다. 우리는 FlashAttention의 사용이 훈련 동안 향상된 처리량의 주요 드라이버라는 점에 주목한다. 특히, 메모리 절감을 통해 훈련 중 활성화 체크포인팅에 의존하지 않고 FLOPS를 모델 훈련에 직접 기여하는 데 집중할 수 있다. 또한 전용 플래시 어텐션 커널은 순진한 커널보다 절대 측면에서 더 빠릅니다. 흥미로운 사실은 플래시 어텐션이 bfloat16에서 수행되고 fp32가 지상 진리로 수행될 때 전통적인 어텐션보다 훨씬 더 수치적으로 정확하다는 점에 주목한다.

**다른 트리톤 커널.** 또한 트리톤의 계층 규범뿐만 아니라 회전 위치 임베딩을 위해 특수화된 커널을 사용합니다. 회전식 매립의 경우 사용하지 않은 PyTorch 대응물에 비해 특히 큰 개선이 보인다. 우리는 변환이 동일한 위치에 있는 모든 헤드에 대해 동일하기 때문에 값비싼 삼각 함수를 재사용할 수 있으며, 다른 구현에서 일반적인 것처럼 RAM에 캐싱할 필요가 없기 때문에 그 사용을 추가로 단순화할 수 있다는 점에 주목한다.

#### 5.3.3 단일 층으로 구현 된 선택적 재컴퓨팅을 통한 효율적인 메모리 사용

비용 효율성을 위해 A100 40GB를 목표로 하고 있기 때문에 메모리 발자국은 우리에게 중요한 관심사입니다. ZeRO와 FlashAttention은 이미 메모리 가용성을 크게 향상시켰지만 선택적 재컴퓨팅을 통해 추가 메모리를 확보했다. Li 등(2021)은 시퀀스 병렬성 위에, 역전파를 위해 그들의 출력을 메모리에 저장하는 것보다 일부 활성화들을 재계산하도록 제안되었는데, 활성화들로서 일반적으로 평가하기에 싸다. 우리는 이 아이디어를 한 단계 더 나아가 모든 활성화 함수뿐만 아니라 계층 규범도 재계산한다. 우리는 계층 규범의 통계만을 기억하기 위해 저장하며, 이는 재계산을 사소한 것으로 만든다. 전반적으로, 선택적 재컴퓨팅의 이러한 구현은 디코더 블록의 메모리 소비를 팩터 2x만큼 감소시키면서, 추가 계산으로부터 처리량의 저하를 야기하지 않는다.

PyTorch의 Autograd 엔진의 한계로 인해 후속 선형 레이어를 다시 계산하지 않고 레이어 규범만 다시 계산하는 것은 오히려 어렵다. 이를 달성하기 위해 전체 디코더 블록에 대해 단일 사용자 지정 자동 래드 함수를 만듭니다. 이 개념을 _monolayer_ 라고 추가 합니다.

그림 11: **옵티마이저 샤딩은 데이터 병렬도에 걸쳐 큰 옵티마이저 상태를 분할하여 메모리 공간을 줄이고 확장성을 개선합니다. 자유 메모리는 증가된 마이크로배치 크기에 대해 거래될 수 있어, 처리량을 향상시킬 수 있다. 그림은 Rajbhandari 등 (2020)에서 영감을 받았습니다.**

#### 5.3.4 수치 정밀도: bfloat16만 있으면 됩니다.

Brown et al. (2020); Zhang et al. (2022); Scao et al. (2022)은 모두 fp16에 의존할 때 1000억 매개 변수 범위의 안정성 문제 훈련 모델을 보고했으며 대신 bfloat16을 채택하여 다소 더 안정적인 훈련을 초래한다. Rae 등(2021)은 최적화 단계 후에 float32 파라미터를 bfloat16으로 양자화할 때 확률적 라운딩을 사용하는 것에서 상당한 개선을 보고했다. 그러나 전체 옵티마이저 상태가 float32에 저장되는 경우 유사한 개선을 관찰하지 못한다. 대신, 우리는 최적화자 상태가 훈련 역학과 평형을 이루기 전에 확률적 반올림이 초기 단계 동안 도움이 되지만 확률적 반올림이 없는 훈련은 그 직후 동일한 훈련 궤적에 접근한다는 것을 관찰한다.

#### 5.3.5 유연성 및 신뢰성 향상을 위한 수명 품질 기능

**위상 불가지 체크포인트.** 놀랍게도, 우리가 팔콘 시리즈를 개발했을 때 대부분의 분산된 교육 라이브러리는 체크포인트 형식과 배포 토폴로지를 강력하게 연결했습니다. 토폴로지를 변경하려면 체크포인트를 수동으로 새 형식으로 변환해야 합니다. 훈련 동안 클러스터를 성장시킬 계획이었기 때문에, 모델 및 최적화기 체크포인트들이 t5x와 유사한 임의의 토폴로지 구성들 사이에서 판독가능/기록가능하도록 보장한다(Roberts et al., 2022).

**낮은 불일치 데이터 로드.** 다른 소스를 단일 혼합 데이터 집합으로 집계할 때 데이터 집합당 목표 확률/가중치에 따라 각 소스를 무작위로 샘플링하는 것이 일반적입니다. 그러나 이는 예상 목표 확률을 가진 소스로부터의 샘플링만 보장한다. 대신에, 트레이닝의 각각의 서브세트 동안의 유효 가중치는 일정한 것이 바람직하다. 우리는 각 데이터 소스에 대한 정확한 가중치를 포함하도록 보장되는 \(10,000\)의 상대적으로 짧은 길이의 서로 다른 데이터 소스 간의 미리 정의된 샘플링 패턴을 사용한다.

**토폴로지 검색 및 최적화.** 처리량을 최적화하려면 서로 가까운 인스턴스에 통신량이 많은 순위를 배치하는 것이 중요합니다. 기존 HPC 플랫폼과 달리 AWS는 훈련에 할당된 인스턴스에 대한 토폴로지 정보를 노출하지 않습니다. 따라서, 배치를 최적화하기 위해 먼저 모든 노드 쌍 사이의 대역폭을 측정하여 토폴로지를 발견하고, 측정된 토폴로지에 대해 랭크 배치를 최적화한다. 토폴로지 탐색 단계와 관련하여, 나이브 솔루션은 첫 번째 노드가 다른 모든 노드에 대한 대역폭을 먼저 확인하고, 그 다음 두 번째 노드 등을 확인하는 것이다. 이를 위해서는 \(\mathcal{O}(n^{2})\) 순차적인 측정이 필요하다. 대신, F.1에 제공된 의사코드를 병렬로 비교하여 \(\mathcal{O}(n)\) 단계에서만 이를 달성할 수 있다. 토폴로지가 (효율적으로) 발견되면 Gromov-Wasserstein 최적 전송(Memoli, 2011)을 활용하여 배치한다. 측정된 대역폭을 기반으로 소스 거리 행렬을 측정하고, 파이프라인/데이터 병렬 그리드의 거리로부터 타겟 거리 행렬을 정의한다. 그런 다음 파이썬 최적 전송 툴박스(Flamary et al., 2021)를 사용하여 결과적인 문제를 효율적으로 해결한다.

### 관리 실행: 대규모 인프라 실행 유지

**하드웨어 오류.** 수백 개의 노드까지 확장하면 하드웨어 오류가 Falcon-180B에서 점점 더 일반화되고, 매일 4,096 A100에서 11년 이상의 누적 사용과 동일합니다. 또한 클라우드 설정에서 실행이 다시 시작될 때마다 새로운 익명화된 노드 선택을 샘플링합니다. 이전 할당에서 어떤 노드가 의심되었는지 기록을 유지할 수 없습니다. 따라서 결함 노드를 신속하게 식별하고 배제할 수 있는 것이 중요하다. 우리는 대부분의 하드웨어 장애가 결함이 있는 A100, 특히 손상된 메모리 행과 연결된다는 것을 발견했다. 이러한 오류는 항상 Xid 코드와 함께 표시 되는 것은 아니며, 이를 포착 하기 위해 수동 테스트가 필요 합니다. 일반적으로 NaN을 반환 하는 계산을 초래 합니다. 스타트업에서는 이러한 오류를 포착하기 위해 일련의 대규모 행렬 곱셈을 실행하며, 훈련 중에는 범인 노드를 신속하게 식별하기 위해 통신에서 NaN을 추적한다. 또한 시작에 대한 간단한 통신 테스트를 수행하여 통신 프리미티브가 예상대로 작동하는지 확인합니다.

**모니터링.** 평활화가 적용되지 않는 경우에도 많은 웹 기반 모니터링 도구가 메트릭을 샘플링한다는 것을 발견합니다. 이는 스파이크와 같은 중요한 이벤트를 숨길 수 있습니다. 이에 따라 자체 현지 시청자를 배치합니다.

**Spikes.** 훈련 중에 스파이크가 거의 발생하지 않았습니다. Chowdhery 등(2022)과 유사하게 스파이크가 발생했을 때, 우리는 최신 프리 스파이크 체크포인트에서 재개하여 10억 토큰을 건너뛰었다. 40B 모델과 180B 모델을 모두 훈련하는 동안 9개의 스파이크가 발생했다.

Results

**배경.** 자연어 처리 커뮤니티는 모델의 기능을 평가하기 위해 많은 벤치마크를 개발했습니다. 읽기 이해 테스트에서 영감을 받은 작업(예: RACE Lai 등(2017))에서 세계 지식 평가(예: OpenBookQA Mihaylov 등(2018)) 또는 소위 상식 작업(예: HellaSwag 등(젤러 등, 2019))에 이르기까지. GLUE 및 SuperGLUE와 같은 역사적 벤치마크는 또한 단어 중의성 해소 또는 수반성 인식과 같은 능력을 측정하는 많은 언어적으로 동기화된 작업을 집계한다(Wang et al., 2018, 2019). 그러나, 대용량 언어 모델들이 성능이 향상되고 응용 범위가 넓어짐에 따라 평가의 풍경은 이러한 마지막 장르의 작업에서 멀어졌다. 대신, MMLU(Hendrycks et al., 2020) 또는 BigBench(Srivastava et al., 2023)와 같은 최근의 벤치마크는 언어 행동보다는 일반적인 지식과 능력을 포착하려고 시도한다. 코드 평가도 수학 과제(Cobbe et al., 2021)와 함께 점점 더 보편화되었다(Chen et al., 2021). 최근 최신 모델에 대한 기술 보고서에는 수많은 학술 및 전문 시험(OpenAI, 2023a; Anil et al., 2023)도 포함되어 있다.

이러한 주제의 변화와 함께 _how_ 모델을 중심으로 하는 연습도 변경 되었습니다. 0/few-shot 평가의 경우, 표준 설정은 Brown 등(2020)에 의해 대중화되었다: 대부분의 고전적인 NLP 태스크들은 제한된 수의 선택들을 가지므로, 이들은 자유 형태 답변들을 생성하는 대신, 질문이 주어진 선택들의 로그-확률들을 계산함으로써 평가될 수 있다. 이것은 엄격한 가드레일을 모델에 배치하여 이 프레이밍에서 샘플링을 통한 자기회귀 추론에 의존하지 않기 때문에 평가의 구현 및 재생산을 단순화한다. 그러나, 이러한 비탄력성은 항상 원하는 것은 아니다: 예를 들어, 연쇄-생각 프롬프팅(Wei 등, 2022b) 모델이 최종 답변을 제공하기 전에 그것의 추론의 중간 단계들을 기록하게 한다. 이 설정은 또한 모델이 다운스트림에서 사용되는 방법을 가장 잘 설명하지 못할 수 있습니다. 요약과 같은 일반적인 자유 형식 작업은 자기 회귀 생성이 필요합니다. 그러나 이러한 과제에 대해서는 모델의 답변을 평가하는 별도의 과제가 빠르게 발생한다. 예를 들어, Stiennon 등(2020)은 모델 생성 요약에 대한 ROUGE 점수가 반드시 인간의 선호도와 정렬되는 것은 아니라는 것을 발견하였다.

전형적인 대형 언어 모델들이 챗봇/가상 비서로서 배치될 것이다. 이것은 정량화하기에는 매우 어려운 사용 사례이다: 적절한 스타일과 수다스러움, 광범위한 세계 지식, 종종 추론/코드 능력이 필요하다. 이러한 배포는 또한 도움이 되는 것 외에도 이상적으로는 무해하고 정직해야 하며(Bai 등, 2022a), 평가 및 절충안을 더욱 복잡하게 만든다. 특히, 고전적인 NLP 레시피는 더 나은 챗봇으로 잘 번역되지 않는다. 도 12를 참조하면, 우리는 인기 있는 SuperGLUE 벤치마크(Wang et al., 2019) 및 인간 주석자에 의해 평가된 완료가 있는 250개의 프롬프트 세트에서 Falcon-40B의 변형을 평가한다. 인기 있는 FLAN 레시피(Longpre 등, 2023)는 SuperGLUE 상에서 최상의 제로-샷 성능을 갖는 모델을 초래하지만, 그러한 모델은 또한 인간 주석자에 의해 가장 덜 선호되는 것이다. 대신에, 소위 셀프-인스트럭션(Wang et al., 2022b; Taori et al., 2023) 데이터세트에 대해 트레이닝된 모델들은 더 작은 방식으로 SuperGLUE 성능을 개선하지만, 주석자 선호도에 대한 상당한 개선을 초래한다-불행하게도, 인간 등급은 SuperGLUE에 대한 평가보다 수집하는데 더 비싸다!

그림 12: **인간 등급은 NLP 작업 성능과 다를 수 있습니다. 팔콘-40B의 변종은 OpenAI API 및 인간 주석의 기준선에 대한 수평선, 다양한 데이터 세트에서 미세 조정되었다. 별 등급은 250개의 고유한 프롬프트에 걸쳐 15개의 주석자 풀에서 블라인드로 수집되었습니다. **

고가의 인간 선호 데이터와 값싼 NLP 평가들 사이의 중간 지면으로서, 강력한 외부 모델(예를 들어, GPT-4)을 판사로 사용하는 것이 최근에 제안되었다(Chiang and Lee, 2023; Chiang et al., 2023; Zheng et al., 2023)-RLAIF에 의해 광범위하게 영감을 받아 RLHF 내의 인간 주석자들을 모델들 자체로 대체하고자 한다(Bai et al., 2022; Dubois et al., 2023). 그러나 이러한 관행이 얼마나 신뢰할 수 있는지는 아직 불분명하다(Wang et al., 2023). 또한 사전 훈련된 모델이 아닌 다운스트림 사용 사례에 대해 미세 조정된 모델을 평가하는 데 주로 초점을 맞추고 있다.

마지막으로 모델 간의 공정한 비교는 어렵다. 먼저, 태스크 선택은 광범위하게 분기된다 : PaLM 논문들(Chowdhery et al., 2022; Anil et al., 2023)은 전형적으로 Brown et al.(2020)의 셋업을 재현하고; 최근의 기술 보고서들(OpenAI, 2023a; Inflection, 2023)은 다양한 설정들에서 태스크들의 임의의 선택을 보고하고; 그리고 Gopher(Rae et al., 2021), Chinchilla(Hoffmann et al., 2022) 또는 LLaMA(Touvron et al., 2023a, b)와 같은 최신 모델들은 모두 NLP 태스크들의 다양한 선택들을 보고하도록 선출되었다. 특히, 이러한 설정에 대한 표준화가 부족하며, Eleuther AI Evaluation Harness(Gao et al., 2021) 또는 HELM(Liang et al., 2022)과 같은 표준화된 벤치마크가 존재하지만 개방형 액세스 모델 중에서 제한된 채택만 볼 수 있으며 대부분의 논문은 사용된 프롬프트를 포함하여 평가 설정에 대한 세부 정보를 보고하지 않는다. 데이터 포맷팅 또는 토큰화에 대한 상이한 관행은 광범위하게 상이한 작업 점수를 초래할 수 있다(Fourrier 등, 2023); 어떤 면에서는, 일률적 평가는 아직 완전히 존재하지 않는다. 우리는 섹션 6.1에서 이 문제를 구체적인 사례와 함께 추가로 논의한다.

**우리의 평가 설정.** 이 문서에서는 Falcon 시리즈의 사전 훈련된 모델에 초점을 맞추고 있으므로 Brown 등(2020)의 더 고전적인 로그프로브 기반 설정에 평가를 중점을 두기로 선택합니다. 이는 모델 전반에 걸쳐 공정성 문제가 남아 있지만 비교를 단순화한다. 이를 해결하기 위해 (1) PaLM 모델(Chowdhery et al., 2022; Anil et al., 2023), (Brown et al., 2020), (2) GPT-4 논문(OpenAI, 2023a)에 의해 보고된 소수의 샷 태스크 집합에 대한 평가; (3) 상식, 질의 응답 및 코드 태스크에 걸쳐 최신 모델에 대한 평가; (4) EAI Harness(Gao et al., 2021)의 결과를 보고하는 모델에 대해 동일한 프롬프트 및 메트릭과 비교할 수 있다. 우리는 이 평가 설정이 모델의 성능을 광범위하게 먼저 살펴보기 위한 것일 뿐이며, 다운스트림 사용 사례에서 팔콘 시리즈의 사용에 대해 알리기 위해서는 모델의 전용 전문화 후 적절한 심층 도메인별 평가가 필요할 것이다.

### 프롬프트 또는 프롬프트 안 함 코드베이스에서 평가 비교

몇 번의 샷으로 모델을 평가할 때 설정 간에 두 가지 주요 불일치가 발생할 수 있는데, (1) 프롬프트의 스타일과 쓰기의 변화, (2) 정확도를 측정하는 데 사용되는 메트릭의 차이이다.

**prompting.** (1)에 대해 구체적으로 작은 단어 조정을 통해 성능 변경(참/거짓, 오타 대신 복수, 예/아니오 사용)을 유도할 수 있을 뿐만 아니라 작업을 다른 방식으로 프레임화할 수도 있습니다. 예를 들어, Rae 등(2021)은 객관식 질문 응답 태스크의 경우 프롬프트에 옵션을 기록하는 것이 더 큰 모델에는 유익하지만, RACE 상의 더 작은 모델에는 해롭다는 것을 발견하였다(Lai 등, 2017). 모델이 답변 자체를 예측하도록 요청받았는지 또는 그에 대한 문자 키를 요청받았는지에 관계없이. RACE의 경우, 이는 10-20%의 절대적 변화를 설명할 수 있다. 가장 일반적으로 사용되는 작업은 제로/퓨샷 평가가 대중화되기 전에 생성되었기 때문에 작업에 대한 표준 프롬프트 또는 프레이밍과 같은 것은 거의 없다. 또한 대다수의 논문은 그들이 사용한 프롬프트를 보고하지 않아 재현성을 저해한다.

그림 13: **모든 평가에서 프롬프트가 제공된 모델에 따라 가능한 답변의 로그 확률에 따라 순위를 매깁니다. 프롬프트의 회색에서 Rae 등(2021)과 함께 프롬프트(CH)에서 각 후보를 자세히 설명할 수 있는 대체 가능성을 강조하지만, 이는 더 크고 유능한 모델에는 유리하지만 더 작은 모델에는 해롭다는 것을 발견했습니다.**

**메트릭.** 대부분의 작업에서 일반적으로 정확도가 보고되지만 후보 답변을 평가하는 데 사용되는 로그 확률 계산에 대한 연습은 다릅니다. 선택이 여러 개인 태스크의 경우, 각 선택지들을 연속적으로 후보로 사용하여 \(P\)(candidate\(|\)context)를 평가하고, 확률이 가장 높은 선택을 모델의 답변으로 취한다. 답변이 긴 태스크(즉, 단일 토큰에 맞지 않음)의 경우 후보의 각 토큰의 로그 확률을 합산하여 전체 완료의 로그 확률을 추정합니다. 길이가 매우 다른 선택/완성을 비교할 때, 이것은 더 긴 선택들에게 불공평할 수 있다: 그것들은 더 가능성이 희박하다. 이를 보완하기 위해 로그 확률은 후보 답변의 길이-첫 번째 잠재적 발산에 의해 정규화되기도 한다. Brown 등(2020)은 또한 일부 답변들이 프롬프트에서 최종 시퀀스 "Answer:"를 따를 가능성이 더 높아서 심하게 편향된 것을 발견하였다. 따라서, ARC, OpenBookQA, RACE의 경우, 각각의 완성에 대한 소위 무조건 확률인 \(P\)(candidate\(|\)context)\(/P\)(candidate\(|\)Answer)"에 의해 확률을 정규화하는 것을 선택한다. Touvron 등(2023)도 OpenBookQA 및 BoolQ에 대해 이러한 관행을 채택하지만, 로그 확률의 계산에 대한 세부 사항이 거의 제공되지 않기 때문에 다른 논문도 이를 채택했는지 여부가 종종 불분명하다.

**실험.** 프롬프트에서 객관식 옵션 개요의 영향을 탐구합니다 (예시는 그림 13 참조). 표 17의 Falcon-7B/40B/180B에 대해 ARC와 OpenBookQA에서 0-shot으로 무조건적인 확률로 정규화하고, 무조건적인 확률로 정규화하는 것은 표 17의 Falcon-7B/40B/180B의 모델 크기와 작업 모두에 걸쳐 두 관행에 대해 광범위하게 다른 효과를 발견한다. ARC-Easy에서는 무조건 정규화(UN)가 항상 성능을 저하시키는 반면 ARC-Challenge에서는 모든 모델에 대해 성능을 향상시키고 OpenBookQA의 Falcon-180B에 대해서만 성능을 향상시킨다. 프롬프트(CH)의 선택을 요약하면 팔콘-7B의 경우 항상 성능이 저하되지만 팔콘-180B의 경우 항상 성능이 향상되며, 팔콘-40B의 경우 ARC-Easy에서 성능이 약간 저하되고 ARC-챌린지 및 OpenBookQA에서 크게 향상된다. 이러한 발견은 RACE에 대한 Rae 등(2021)의 관찰과 일치하며, 효과가 실제로 매우 중요하다는 것을 발견했다(Falcon-180B에 대한 +10-20% 절대 이득). 두 가지 방법을 모두 결합하면 개별적으로 사용하는 것보다 성능이 향상되지 않습니다.

공정성을 위해 다른 논문과 모델의 평가 설정을 그들의 결과와 비교할 때 일치시키려고 하며, 위의 트릭 중 하나를 사용하는 것이 예를 들어 우리를 불공정한 이점으로 만들지 않도록 한다.

* **PaLM과의 비교.** PaLM(Chowdhery 등, 2022; Anil 등, 2023)과 마찬가지로 Brown 등(2020)의 평가 설정을 재현합니다. 후보들이 하나의 토큰보다 긴 경우, 우리는 로그 확률을 길이별로 정규화한다. 우리는 Brown et al.(2020)에 의해 자세히 설명된 것과 가깝게 유지하며 프롬프트를 크게 조정하지 않는다. 이러한 결과는 6.2절에 자세히 제시되어 있다.
* **GPT-3.5/GPT-4와의 비교** MMLU의 경우 원래 저자가 제안한 형식을 사용 합니다 (Hendrycks 등, 2020). ARC의 경우 GPT-4 논문 (OpenAI, 2023)에 보고 된 대로 프롬프트에서 선택 사항을 명시적으로 개요 합니다. HellaSwag 및 Winogrande의 경우, Gao 등(2021)이 사용하는 프롬프트를 변경하지 않는다. 결과는 섹션 6.3에 보고되어 있습니다.
* **최신 비교입니다.* * 가장 넓은 비교이기 때문에 관행에서 가장 큰 변동이 있는 비교일 가능성이 높습니다. OpenBookQA에서는 무조건 정규화를 사용하고 ARC, OpenBookQA, RACE에서는 개략적인 답변을 사용한다. 우리는 프롬프트의 문구를 가볍게 조정하지만 가오 등(2021)에서 구현된 브라운 등(2020)의 문구에 가깝게 유지한다. 이러한 결과는 섹션 6.4에 보고되어 있다.

\begin{table}
\begin{tabular}{l c c c c c|c c c c|c c c} \hline \hline  & \multicolumn{2}{c}{**ARC-Easy**} & \multicolumn{6}{c}{**ARC-Challenge**} & \multicolumn{6}{c}{**OpenBookQA**} \\  & & UN & CH & BT & & UN & CH & BT & & UN & CH & BT \\ \hline
**팔콘-7B** & **73,7** & 68,1 & 33,3 & 24,7 & 44,5 & **49,5** & 27,4 & 24,8 & **44,6** & 39,2 & 24,4 & 24,0 \\
**팔콘-40B** & **81,2** & 76,6 & 80,5 & 78,8 & 56,7 & 59,0 & **62,1** & 61,3 & 48,0 & 43,2 & **61,2** & 56,4 \\
**Falcon-180B** & 84,7 & 79,5 & **94,4** & 84,6 & 63,7 & 63,9 & **83,5** & 81,6 & 47,6 & 48,8 & **76,4** & 71,8 \\ \hline \hline \end{tabular}
\end{table}
표 17: **다양한 모델 크기에서 다양한 프롬프트 공식 및 메트릭 계산은 최상의 제로 샷 성능을 제공합니다. 특히, 프롬프트에서 객관식 질문 옵션을 명시적으로 설명하는 것은 거의 항상 더 큰 모델의 성능을 향상시키지만 더 작고 덜 능력 있는 모델의 경우 성능이 저하된다는 것을 발견했다. 무조건적 정상화(UN)는 일부 작업은 개선되지만 다른 작업은 개선되지 않는 더 작은 영향을 미친다. 두 가지 트릭을 함께 사용하면 최상의 트릭을 개별적으로 사용하는 것보다 성능이 향상되지 않습니다. 기준선에 대 한 열화, 개선: Gao et al.(2021) 기본 프롬프트 및 길이 정규화 로그프로브 계산 CH 및 UN에 대 한 길이 정규화 로그프로브 계산, BT에 대 한 CH/UN의 최량 **EleutherAI 평가 Harness와의 비교** 가 엄격하게 사용 되며 이에 대 한 프롬프트 공식 또는 로그프로브 계산을 변경 하지 않습니다. 우리는 같은 관행을 채택하는 다른 논문과 비교할 뿐이다. 이것은 직접적인 일대일 비교가 가능하고 다른 모델이 사용하는 설정을 추측할 필요가 없는 가장 공정한 설정이다. 이러한 결과는 섹션 6.5에 보고되어 있다.

마지막으로 모든 설정에 대해 재현성을 위해 부록 G에 사용된 프롬프트를 제공한다.

### 자연어 작업 집계에서 PaLM과의 비교

**설정.** PaLM-2 논문에서 보고 된 1 샷 NLP 작업 벤치마크에서 PaLM (Chowdhery et al., 2022) 및 PaLM-2 (Anil et al., 2023)와 비교 합니다. 이 벤치마크는 GPT-3에 사용되는 태스크들의 세트를 광범위하게 재현한다(Brown et al., 2020). 우리는 평가 프레임워크에서 이러한 작업을 구현하고 검증하지 않았기 때문에 이러한 작업 중 몇 가지(예: 스토리클로즈)가 누락되었음을 주목한다.

**결과.** 우리는 표 18에 자세한 결과를 보고합니다. 작업 전반에 걸쳐 성능을 평균화할 때 Falcon-180B는 PaLM-2 Large의 성능의 99.5%를 복구합니다. PaLM-2 Medium 및 PaLM의 94.8% 및 94.4%를 훨씬 초과합니다. 특히, 팔콘-180B는 HellaSwag, Winogrande 및 PIQA와 같은 일부 고전적인 벤치마크에서 PaLM-2보다 훨씬 더 우수하다. 그러나 팔콘-180B는 두 가지 특정 벤치마크인 RACE와 ANLI에서 뒤쳐진다. 우리는 RACE가 프롬프트와 샘플의 포맷화에 매우 민감하다는 것을 발견했으며, 이는 차이의 일부를 설명할 수 있으며, 또한 ANLI가 일반적으로 훈련 전반에 걸쳐 엇갈린 진행을 나타내는 것을 관찰했으며, 예를 들어 HellaSwag와 같은 부드러운 증가보다는 PaLM-2 Large의 성능 범위를 따라잡을 수 있는 다음 단계 전환 바로 아래에 있다.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Task** & **Subtask** & **PaLM** & \multicolumn{2}{c}{**PaLM-2**} & \multicolumn{2}{c}{**Falcon**} \\  & & **S** & **M** & **L** & **180B** \\ \hline WebQuestions (EM) & 22,6 & 21,8 & 26,9 & 28,2 & **31,9** \\ \hline HellaSwag & 83,6 & 82,0 & 84,0 & 86,8 & **87,5** \\ LAMBADA & 81,8 & 80,7 & 83,7 & **86,9** & 84,4 \\ \hline WSC & 86,3 & 84,6 & **88,1** & 86,9 & 87,5 \\ Winogrande & & 83,7 & 77,9 & 79,2 & 83,0 & **85,1** \\ \hline RACE & Hard & 52,1 & 53,3 & 57,2 & **62,3** & 56,7 \\ \hline PIQA & & 83,9 & 82,2 & 83,2 & 85,0 & **86,1** \\ ARC & Challenge & 60,1 & 59,6 & 64,9 & **69,2** & 67,8 \\  & Easy & 85,0 & 85,6 & 88,0 & **89,7** & 88,8 \\  & Overall & 72,6 & 72,6 & 76,5 & **79,5** & 78,3 \\ OpenBookQA & & 53,6 & 57,4 & 56,2 & 58,5 & **64,2** \\ \hline BoolQ & & 88,7 & 88,1 & 88,6 & **90,9** & 89,0 \\ CB & & 83,9 & 82,1 & 80,4 & 87,5 & **89,3** \\ COPA & & 91,0 & 89,0 & 90,0 & **96,0** & **96,0** \\ RTE & & 78,7 & 78,7 & **81,8** & 79,3 & 80,1 \\ WiC & & 63,2 & 50,6 & 52,0 & **66,8** & 66,1 \\ ReCORD & & 92,8 & 92,1 & 92,4 & **93,8** & 93,2 \\ \hline ANLI & R1 & 52,6 & 53,1 & 58,1 & **73,1** & 60,5 \\  & R2 & 48,7 & 48,8 & 49,5 & **63,4** & 55,5 \\  & R3 & 52,3 & 53,2 & 54,5 & **67,1** & 56,8 \\ \hline \hline Task average & & 73,1 & 71,6 & 73,4 & **77,5** & 77,1 \\ Fraction of PaLM-2 L & & 94,4 & 92,4 & 94,8 & **99,5** \\ \hline \hline \end{tabular}
\end{table}
표 18: **Falcon-180B는 PaLM-2 Medium에 더 가깝게 수행하는 ANLI 및 RACE를 제외하고 대부분의 작업에서 PaLM-2 Large의 성능과 일치합니다. 우리는 이러한 작업이 사용된 프롬프트 형식, 특히 RACE에 매우 민감하다는 것을 발견했으며 공정성을 위해 브라운 등(2020)이 제안한 프롬프트에 가깝게 유지하더라도 추가 수정으로 추가 개선이 해제될 수 있다고 의심한다. 전반적으로, 우리는 팔콘-180B가 PaLM-2 Medium의 94.8%를 훨씬 초과하는 PaLM-2 Large의 성능의 99.5%를 회복한다는 것을 발견했다.*전반적으로, 이들은 팔콘 레시피의 견고성을 나타내는 강력한 점수이다. 데이터 세트 구성과 아키텍처 수정 외에도 PaLM-2 Large와 Falcon-180B 간의 두 가지 차이점이 있습니다. (1) PaLM-2 L은 Falcon-180B(부록 E 참조)와 유사한 양의 토큰과 두 배의 매개 변수를 사용하여 더 큰 컴퓨팅 예산으로 훈련된 것으로 추정되며, (2) PaLM-2 모델은 다운스트림 작업 성능을 향상시키는 것으로 보고된 목표의 혼합물(Tay 등, 2022)을 사용했습니다. Falcon-180B의 사후 적응(예를 들어, PaLM-U 레시피(Tay 등, 2022))은 Falcon-180B를 기본 모델로 하는 PaLM-2 L의 성능을 더 많이 회복하는 데 도움이 될 수 있다.

### 제한된 작업 집합에서 GPT-3.5 및 GPT-4와 비교

**설정.** 자연어 작업인 MMLU, HellaSwag, Winogrande 및 ARC를 중심으로 GPT-4 논문(OpenAI, 2023)에 보고된 GPT-3.5 및 GPT-4와 비교합니다. 우리는 25 샷 정확도 대신 2 샷을 보고하는 ARC를 제외하고 제안된 샷 수와 동일한 수를 사용한다.

**결과.** 결과는 표 19에 나와 있습니다. Falcon-180B가 모든 작업에서 GPT-3.5 이상, GPT-4 이하를 체계적으로 수행한다는 것을 알 수 있습니다. 특히, Falcon-180B는 HellaSwag에서는 GPT-3.5와 GPT-4의 중간 거리에 가까운 반면, Winogrande에서는 GPT-4의 성능과 거의 일치한다. Falcon-180B는 객관식 질의응답 태스크에서 GPT-3.5에 가깝지만 항상 더 높은 성능을 보인다. GPT-4는 팔콘-180B(부록 E 참조)보다 4-5배 더 많은 사전 훈련 컴퓨팅으로 훈련되었으며, 이는 두 모델 간의 성능 차이의 대부분에 기여할 가능성이 있다.

### 상식, 질문 응답 및 코드 작업에 대 한 최신 비교

**설정.** 이 섹션에서는 상식, 질문 응답 및 코드 작업에 대 한 다른 모델과 팔콘 시리즈를 비교 합니다. 최첨단 "before" PaLM-2 Large 및 GPT-4: GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), MT-NLG (Smith et al., 2022), PaLM (Chowdhery et al., 2022), LLaMA-2 (Touvron et al., 2023) 및 Inflection-1 (Inflection, 2023)을 연속적으로 정의한 모델들과 비교한다. 상식 태스크의 경우 PIQA(Bisk et al., 2020), HellaSwag(Zellers et al., 2019), Winogrande(Sakaguchi et al., 2019), BoolQ(Clark et al., 2019) 및 LAMBADA(Paperno et al., 2016)를 포함한다. 질의 응답 데이터 세트의 경우, ARC(Clark et al., 2018), OpenBookQA(Mihaylov et al., 2018), 및 MMLU(Hendrycks et al., 2020)를 포함한다. 공정한 비교를 가능하게 하기 위해, 우리는 선택을 요약하지 않고 ARC를 평가한다(섹션 6.1 참조). OpenBookQA의 경우 성능이 평평하다는 것을 알 수 있기 때문에 선택의 개요를 설명하고, 마지막으로 MMLU의 경우 Hendrycks 등(2020)의 표준 설정을 사용한다. 이 모든 작업에 대해 위에서 설명한 논문의 다른 모델에 대한 결과를 취한다. 코드에 대해서는 Python에서 BigCode Models Leaderboard for HumanEval의 Set-up을 재현하고, Codex(Chen et al., 2021), StarCoder(Li et al., 2023), Code LLaMA(Roziere et al., 2023)와 같은 추가적인 코드 특화 모델을 포함한다. 사전 훈련 직후 팔콘 모델과의 비교에 초점을 맞추기 때문에 위의 모델의 지시 변형을 고려하지 않는다.

**상식.** 우리는 표 20의 결과를 보고합니다. BoolQ를 제외하고 Falcon-180B는 모든 작업에서 최신 모델보다 크게 개선됩니다. 대체로 Falcon-40B가 LLaMA-2 34B보다 약간 낮다는 것을 발견했는데, 이는 Falcon-40B의 경우 2,800PF-일, LLaMA-2 34B의 경우 4,700PF-일(거의 70% 이상)의 사전 훈련 계산이 더 작았기 때문이다. 팔콘-7B는 또한 LLaMA-2 7B를 약간 과소 수행한다; 이번에는 계산의 차이가 더 작다(730PF-일 대비).

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **GPT-3.5** & **GPT-4** & **Falcon-180B** \\ \hline
**HellaSwag** (10-shot) & 85.5 & **95.3** & 89.0 \\
**Winogrande** (5-shot) & 81.6 & **87.5** & 87.1 \\
**ARC Challenge** (25-shot) & 85.2 & **96.3** & 87.8\({}^{*}\) \\
**MMLU** (5-shot) & 70.0 & **86.5** & 70.6 \\ \hline \hline \end{tabular}
\end{table}
표 19: **Falcon-180B는 GPT-3.5와 GPT-4 사이에서 다운스트림 성능을 전달한다. Falcon-180B는 상식 태스크(HellaSwag 및 Winogrande)에서 잘 수행하며, 여기서 GPT-3.5보다 훨씬 앞서 있다. 객관식 질의 응답(ARC 및 MMLU)의 경우, Falcon-180B는 GPT-3.5를 초과하지만 그리 크게는 그렇지 않다.\ ({}^{*}\) ARC Challenge.**970PF-days, 30% 더 많은 25샷 성능이 아닌 2샷을 보고하지만 단일 헤드 치수 64를 가진 멀티쿼리가 팔콘-7B에 대해 매우 공격적인 구성이라고 의심한다. 우리는 LLaMA-2와 Inflection-1이 BoolQ에서 탁월한 성능 덕분에 Inflection-1이 아마도 앞서 있는 비교적 유사한 성능을 달성한다는 것을 발견했다.

**질문 응답.** 표 21에서 Falcon-180B가 질문 응답 작업에서 최신 모델의 다른 모델보다 훨씬 더 우수하다는 것을 다시 찾을 수 있습니다. 우리는 팔콘 시리즈가 MMLU에서 약간(다른 작업과 비교하여) 덜 수행되는 것으로 보인다는 점에 주목한다: 우리는 이것이 MMLU에서 발견되는 질문의 스타일 및 내용과 더 즉시 관련될 수 있는 더 많은 기술적 소스에 비해 사전 훈련 데이터세트에서 웹 데이터의 큰 보급에 기인할 수 있다고 믿는다.

**코드.** 우리는 표 22의 HumanEval에 대한 결과를 보고합니다. Falcon-180B가 자연 언어에 중점을 둔 모델 중에서 성능이 Inflection-1에 의해만 일치한다는 것을 발견합니다. 실제로 3% 코드에 대해서만 훈련되었음에도 불구하고 Falcon-180B는 사전 훈련 후 전용 코드 전문화를 거친 두 모델인 PaLM-코더 및 PaLM-2 S\({}^{*}\)의 성능과 거의 일치합니다. 이것은 팔콘-코더 전문화의 발전에 고무적인 결과이다.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **APQA** & **HellaSwag** & **Winogrande** & **BoolQ** & **LAMBADA** \\  & & & _(10-shot)_ & _(5-shot)_ & & \\ \hline
**GPT-3** & & 81,0 & 78,9 & 70,2 & 60,5 & 76,2 \\
**Gopher** & & 81,8 & 79,2 & 70,1 & 79,4 & 74,5 \\
**친칠라** & & 81,8 & 80,8 & 74,9 & 83,7 & 77,4 \\
**MT-NLG** & & 82,0 & 80,2 & 73 & 78,2 & 76,6 \\
**PaLM** & & 82,3 & 83,4 & 81,1 & 88,0 & 77,9 \\
**LLaMA-2** & 7B & 78,8 & 77,2 & 78,6 & 69,2 & 77,4 \\ & 13B & 80,5 & 80,7 & 82,1 & 72,8 & 81,7 \\ & 34B & 81,9 & 83,3 & 76,7 & 83,7 & \\ & 70B & 82,8 & 85,3 & 87,3 & 80,2 & 85,0 \\
**Inflection-1** & & 84,2 & 84,3 & 85,8 & & 83,3 & **89,7** & 78,5 \\ \hline
**Falcon** & 7B & 80,3 & 76,3 & 78,1 & 67,2 & 72,6 & 73,8 & 74,9 \\  & 40B & 83,0 & 82,7 & 85,3 & 76,0 & 81,8 & 81,9 & 77,3 \\  & 180B & **84,9** & **85,9** & **89,0** & **80,3** & **87,1** & 87,8 & **79,8** \\ \hline \hline \end{tabular}
\end{table}
표 20: **PaLM-2 Large 및 GPT-4 외부에서 Falcon-180B는 상식 작업에 대 한 LLaMA-2 또는 Inflection-1과 같은 다른 최신 모델을 크게 개선 합니다. Falcon-40B는 계산 예산이 상당히 작기 때문에 LLaMA-2 34B에서 약간 수행한다(4,700PF-일에 비해 2,800PF-일, LLaMA-2의 경우 70% 더 많이). 우리는 BoolQ에 대한 Inflection-1의 탁월한 성능에 주목하며, 반대로 최선의 신속한 엔지니어링 노력에도 불구하고 팔콘 시리즈에 대한 BoolQ에 대한 LLaMA-2 논문에서 보고한 성능을 재현할 수 없었으며, 우리가 보고한 결과가 개선될 수 있다. 최선을 위해 대담 하 고 차선을 위해 밑줄을 긋습니다. **

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & & **ARC-Challenge** & **ARC-Easy** & **OpenBookQA** & **MMLU** \\ \hline
**GPT-3** & & 51,4 & 68,8 & 57,6 & & \\
**PaLM** & & 53,0 & 76,6 & 53,4 & 69,3 \\
**LLaMA-2** & 7B & 45,9 & 75,2 & 58,6 & 45,3 \\  & 13B & 49,4 & 77,3 & 57,0 & 54,8 \\  & 34B & 54,5 & 79,4 & 58,2 & 62,6 \\  & 70B & 57,4 & 80,2 & 60,2 & 68,9 \\ \hline
**Falcon** & 7B & 44,5 & 73,6 & 44,6\({}^{*}\) & 28,0 \\  & 40B & 56,7 & 81,2 & 61,2 & 57,0 \\  & 180B & **63,7** & **84,7** & **76,4** & **70,6** \\ \hline \hline \end{tabular}
\end{table}
표 21: **Falcon-180B는 질의 응답 데이터 세트에서 GPT-3, PaLM 및 LLaMA-2보다 크게 개선되는 반면 Falcon-40B는 LLaMA-2 34B와 인라인으로 수행합니다. \ ({}^{*}\): OpenBookQA 상의 Falcon-7B의 경우, 그렇지 않으면 성능이 랜덤에 가깝기 때문에 후보의 개요 없이 정확도를 보고한다(상세 내용은 표 17 참조). 최선을 위해 대담 하 고 차선을 위해 밑줄을 긋습니다. **

### EleutherAI 평가 Harness를 사용 하 여 다른 모델과의 비교

**설정.** 이 최종 비교를 위해 EleutherAI 평가 Harness(Gao 등, 2021)로 수행된 평가 보고서가 있는 모델만 고려합니다. 이러한 결과는 샘플의 전처리, 프롬프트 및 메트릭의 계산이 동일하기 때문에 가장 직접적으로 비교할 수 있다. BigScience group, FairSeq(Artetxe et al., 2021), GPT-Neo-1.3B(Black et al., 2021), GPT-J(Wang and Komatsuzaki, 2021), GPT-NeoX-20B(Black et al., 2022), OPT(Zhang et al., 2022), Pythia(Biderman et al., 2023), CerebrasGPT(Dey et al., 2023), Aleph Alpha(Aleph Alpha, 2023), BLOOM(Scao et al., 2022a)에 의해 API로 평가된 GPT-3에 대한 결과를 보고한다. 우리는 HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), Winogrande (Sakaguchi et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018) 및 OpenBookQA (Milaylov et al., 2018)에 대한 평균 성능 - 이것은 논문 전반에 걸쳐 상이한 보고 관행에 기초하여 우리가 조립할 수 있는 가장 광범위한 태스크 세트였다. 우리는 이러한 모델이 대부분 더 작은 계산 범위(최대 수천 PF-일)에 걸쳐 있으며 이전에 비교했던 다른 최신 모델보다 성능이 현저히 낮다는 점에 주목한다. 본 논문에서 제시한 Falcon 시리즈와 성능 검증을 위해 RefinedWeb 데이터 세트만을 사용한 Penedo et al.(2023)에서 훈련된 더 작은 규모의 Falcon-RefinedWeb 모델에 대한 결과를 보고한다.

**결과.** 우리는 그림 1에 결과를 제시한다. 14. 척도 전반에 걸쳐 팔콘 시리즈가 이 비교 세트에서 다른 모델에 비해 크게 개선된다는 것을 발견했다. 특히, Falcon-40B는 더 작은 컴퓨팅 예산으로 훈련되었음에도 불구하고 GPT-3 175B보다 성능이 우수하다. 사실, Falcon-7B조차도 GPT-3 175B의 성능에 접근한다: 우리는 더 긴 훈련과 덜 공격적인 멀티쿼리 설정으로, 원래의 GPT-3 모델의 성능을 7B 파라미터(또는 더 적음)와 일치시키는 것이 가능해야 한다고 믿는다. 마지막으로, 우리는 또한 RefinedWeb 단독으로 훈련된 더 작은 검증 모델이 동일한 크기의 GPT-3 모델의 성능을 재현하면서 유리하게 비교한다는 점에 주목한다.

한 걸음 물러서서, 이 줄거리에서 더 넓은 경향을 주목하는 것은 흥미롭습니다. 대부분의 오래된 모델 시리즈는 GPT-3 시리즈를 루프라인으로 사용하여 매우 유사한 성능을 달성한다. OPT 모델은 더 작은 크기에서 성능이 저하되었음에도 불구하고 결국 GPT-3 175B의 성능과 일치한다. 두 외부인이 눈에 띄는 것은 첫 번째 오픈 소스 대형 언어 모델인 GPT-Neo-1.3B와 사전 훈련의 문제 때문일 가능성이 큰 BLOOM 시리즈와 다중 언어 사전 훈련 데이터가 많고 임베딩 후 추가 계층 규범을 보수적으로 사용하기 때문일 가능성이 크다(Scao et al., 2022b).

\begin{table}
\begin{tabular}{l l c c} \hline \hline  & \multicolumn{3}{c}{**Specialized for code?**} & **HumanEval** \\ \hline
**PaLM** & & & 26,2 \\
**LLaMA-2** & 7B & 12,2 \\ & 13B & 20,1 \\ & 34B & 22,6 \\ & 70B & 30,5 \\
**Inflection-1** & & **35,4** \\
**Falcon** & 180B & & **35,4** \\ \hline
**Codex** & \begin{tabular}{l} cushman-001 \\ davinci-002 \\ \end{tabular} & \begin{tabular}{l} ✓ \\ \end{tabular} &
\begin{tabular}{l} 33,5 \\ 45,9 \\ 30,4 \\ \end{tabular} \\
**StarCoder** & & ✓ & 30.4 \\
**PaLM-Coder** & & ✓ & 35,9 \\
**PaLM-2 S\({}^{*}\)** & & ✓ & 37.6 \\
**GPT-3.5** & & ✓\({}^{\dagger}\) & 48,1 \\
**GPT-4** & & ✓\({}^{\dagger}\) & **67.0** \\
**Code LLaMA** & 7B & ✓ & 33,5 \\  & 13B & ✓ & 36,0 \\  & 34B & ✓ & 48,8 \\ \hline \hline \end{tabular}
\end{table}
표 22: **Falcon-180B는 HumanEval의 다른 모든 주로 영어 모델보다 성능이 뛰어납니다.* * Morever, 3% 코드에서만 훈련되었음에도 불구하고 코드 전문화를 거친 PaLM-Coder 및 PaLM-2 S\({}^{*}\)의 성능과 거의 일치합니다. \ ({}^{\dagger}\): GPT-3.5 및 GPT-4 모델은 자연어 위에 있는 코드의 많은 부분에서 사전 훈련된다는 점에서 다소 독특하다(OpenAI, 2023b). 최상의 경우 **볼드**, 차선의 경우 밑줄(전문화당), pass@1입니다.

## 7 Limitations

우리는 연구 결과의 이 섹션 제한과 팔콘 시리즈 모델 자체의 한계를 강조하고 모델의 뉘앙스 잠재적 적용과 추가 연구 방향을 강조한다. 일반적으로 이러한 한계는 (1) 팔콘 시리즈에 대한 대부분의 연구는 모델 교육이 시작된 2022년 12월 이전에 수행되었지만 대규모 언어 모델의 세계에서 상당한 발전이 발생했다. (2) 컴퓨팅 자원의 제약으로 인해 일부 방향에 대한 보다 철저한 탐구가 불가능하다.

### 발견 및 삭제의 제한 사항

**크기.** 삭제는 최적성을 위해 훈련된 1B 및 3B 모델로 수행되며, 350B 토큰에 대해 훈련된 1B 및 7B 모델로 최종 아키텍처 및 데이터 세트를 검증했지만, 이는 팔콘 시리즈의 실제 모델의 컴퓨팅 예산보다 10배 미만입니다. 모델 측면에서 규모가 증가함에 따라 다른 작업은 이상치 특징의 출현에 주목했으며, 이는 우리의 설정(Dettmers 등, 2022)에 포착되지 않은 중요한 역학을 주도할 수 있다. 데이터 측면에서, 점점 더 큰 모델들은 암기에 점점 더 민감해지는 것으로 알려져 있다(Carlini et al., 2022). 잠재적으로 치명적인 품질 저하로 이어진다(Hernandez et al., 2022).

**Benchmarks.** 삭제에서 로그프로브 기반 평가만 사용 하 여 제한된 작업 집합에 대 한 제로 샷 성능을 측정 하는 데 중점을 두었습니다. 다운스트림 작업 수행이 실제 인간 선호도와 상충될 수 있을 뿐만 아니라(해당 문제의 자체 경험에 대한 그림 12 참조) 그러나 작업 세트는 코드나 다국어 성능을 포착하지 못합니다. 범용 언어 모델을 평가하는 것은 어렵고 단일 집계된 메트릭은 모델링 또는 데이터 개입에 의해 발생하는 뉘앙스를 잘 해석하지 못하는 경우가 많다. 일반적으로 작업 세트에서 특정 메트릭을 체계적으로 개선하는 한 개입으로 일관된 추세를 경험한 적이 없지만 다른 개입은 경험하지 않았다(예: 기술 데이터를 추가해도 SciQ 또는 PubMedQA에서 성능이 일관되게 향상되지 않음). 그러나 이는 일반적으로 이득을 측정하는 소규모 설정의 한계일 가능성이 높다. MMLU(Hendrycks et al., 2020) 또는 AGileval(Zhong et al., 2023)과 같은 인기 있는 벤치마크에 대한 성능은 전용 도메인 내 데이터로 개선될 수 있다. 특히, 최적의 사전 훈련 데이터 구성에 대한 대규모 삭제의 가치와 중복 제거의 정량적 대 정성적 측면을 더 잘 이해하는 데 있어(즉, 웹 데이터의 중복 제거는 최악의 샘플 중 일부를 필터링하기 때문에 불합리하게 효과적일 수 있다).

전반적으로 섹션 4를 팔콘 시리즈에 대한 일부 결정을 근거 짓고 광범위하게 검증하는 데 도움이 되는 피상적인 실험 세트로 본다. 미래의 실무자들은 이러한 실험과 결과를 더 큰 규모로 재방문하여 더 나은 캡처 성능을 위한 광범위한 벤치마킹을 가능하게 할 것이다.

그림 14: **팔콘 시리즈는 EleutherAI 평가 Harness로 결과를 보고한 다른 모델에 비해 모든 규모에서 강력하게 개선된다. RefinedWeb에서만 훈련된 모델들은 큐레이팅된 데이터를 사용하지 않았음에도 불구하고 GPT-3 시리즈의 성능을 재현한다. HellaSwag, LAMBADA, Winogrande, PIQA, ARC 및 OpenBookQA에 대한 제로 샷 정확도를 집계했습니다. **

### 팔콘 모델의 제한 사항

**디커플링 훈련 및 추론 계산.** 지난 1 년 동안 대규모 언어 모델을 채택 하는 데 극적인 폭발이 발생 하 여 다운스트림 사용이 크게 증가 했습니다. 그 패러다임 하에서 추론 비용이 우세해지고 사전 훈련 비용이 뒤따를 수 있으며, 더 나아가 소규모 오픈 소스 모델은 커뮤니티가 구축하기 쉬워 지역 배포 및 미세 조정까지 가능하게 한다. 스케일 업 사전 트레이닝 시, 추가 컴퓨팅은 더 큰 모델을 향해 또는 더 긴 트레이닝을 향해 소비될 수 있다. 추론보다 매개변수 수 우식증을 증가시키고 모델의 전체 수명 주기에 걸쳐 비용을 증가시키지만, 더 긴 사전 훈련의 경우 그렇지 않으며, 디커플링 훈련 및 추론 계산을 위한 축을 제공한다. LLaMA 시리즈(Touvron et al., 2023, 20)와 같은 최근의 오픈 소스 모델들은 결국 고정된 2,000B 토큰들에 대한 그들의 7-70B 파라미터 모델들을 트레이닝함으로써 이러한 관행을 채택하였다. 우리의 7B가 유사한 아이디어를 따르지만, 우리의 40B 및 180B 모델은 호프만 등(2022)에 의해 제안된 사전 트레이닝 최적성에 더 가깝게 트레이닝된다. 이를 통해 팔콘 시리즈의 모델 배포가 더욱 어려워집니다. 당시, 이 결정은 부분적으로 데이터 가용성에 대한 제약에 의해 동기가 부여되었는데, 우리는 훈련의 한 시대를 엄격하게 고수하기 위해 일찍 선출했고, 교육을 시작했을 때 정제된 웹 토큰의 총 재고량은 알려지지 않았다(데이터 처리 작업은 1월 초에만 종료됨). 그러나 최근의 연구들은, 적어도 10B 파라미터 범위의 모델들에 대해, 최대 4 에포크들이 성능의 최소 저하를 초래할 수 있다고 제안했다(Xue et al., 2023; Muennighoff et al., 2023). 사용 가능한 데이터의 최종 크기(약 5,000B 토큰의 영어 및 1,000B 토큰 코드)를 기반으로 최소 4,000-5,000B 토큰에 대한 미래 모델을 훈련하고 더 큰 데이터에는 10,000-15,000B 토큰으로 기울이는 것이 좋다. 이는 더 큰 모델이 효율적으로 배포되기 쉽기 때문에 사전 훈련 중에 몇 가지 문제를 제기한다는 점에 유의해야 한다. 또한, 라우팅된 언어 모델이라고도 알려진 MoE(layerwise mixture of experts)와 같은 아키텍처 개입을 통해 훈련 및 추론 컴퓨팅 디커플링을 가능하게 하는 가능성을 본다(Clark et al., 2022; Fedus et al., 2022). MoE 모델들에서, 파라미터들은 단지 희소하게 활성화되어, 추론 비용들의 또 다른 8-16 감소를 허용한다.

**코드 성능 및 사전 훈련 데이터.** 사전 훈련 하위 집합에 대한 업샘플링을 허용하지 않았기 때문에 주로 RefinedWeb의 웹 데이터에 대해 Falcon을 훈련했습니다. 또한 사전 훈련에서 코드의 비율에 대해 보수적인 반면 사용 가능한 데이터는 사전 훈련에서 코드 데이터의 10-30%에 도달할 수 있었다. 향후 모델의 경우 코드 데이터를 훨씬 더 널리 사용하고 다운스트림 사용 사례에 대한 공통 사용 사례 또는 관심 있는 데이터 영역을 설명하는 잠재적으로 업샘플링 소스를 만드는 것이 좋다. 코드 데이터는 단일 소스(즉, GitHub)로부터 널리 이용가능하고 웹 데이터-중복제거와 유사한 대규모 프로세싱을 겪을 수 있기 때문에 특히 유망하다(Kocetkov et al., 2022; Allal et al., 2023; Li et al., 2023). 또한, GPT-3.5 및 GPT-4 모델은 자연 언어 및 코드 데이터 모두의 혼합에 대해 훈련된 것으로 보인다(OpenAI, 2023, 20). 이러한 하이브리드 언어/코드 모델은 또한 사용자 질문에 답하거나 다양한 도구와 인터페이스하기 위해 코드를 광범위하게 사용할 수 있는 챗봇으로서 다운스트림 사용 사례에 대해 더 쉽게 적응할 수 있다. 또한 합성 데이터의 사용에서 잠재적인 가능성을 볼 수 있다(Gunasekar 등, 2023; Li 등, 2023); 그러나 이러한 방법 중 다수는 현재 실제 사전 훈련보다 증류에 더 가깝고 다운스트림 적응을 넘어 새로운 부류의 더 큰 모델을 부트스트랩하는 데 사용할 수 있는 방법은 다소 불분명하다.

**더 긴 시퀀스 길이.** 모든 팔콘 모델은 2,048 시퀀스 길이로 사전 훈련되며, 이는 다중 턴 채팅 또는 코드 사용 사례에 대해 제한될 수 있습니다. 회전 임베딩의 사용(Su 등, 2021) 덕분에, 더 긴 시퀀스에 대한 사후 적응이 가능하다 : kaiokenmdenv (2023); Chen 등(2023)은 동시에 보간 및 경량 피니튜닝에 의해 회전 위치 임베딩으로 컨텍스트 길이를 확장하는 것이 가능함을 발견하였다. 긴 컨텍스트 피니튜닝에 대한 자체 실험에서, 우리는 긴 컨텍스트 데이터가 RefinedWeb에서 풍부하다는 것을 발견했다. 토큰의 위쪽 및 13% 이상은 8k 토큰 이상의 문서에서 발생하며, 32k 토큰 이상의 문서에서 토큰의 1.5%가 발생한다. 이 접근법은 LLaMA-2 Long(Xiong 등, 2023)에 대해 더욱 세분화되어, 많은 샷(예를 들어, 섹션 6.3의 MMLU 및 ARC)을 갖는 태스크에 대한 상당한 성능 부스트를 초래하였다.

**미리 훈련된 버전만 사용합니다.* * Falcon 모델의 가볍게 조정된 버전을 사용하여 지시 또는 채팅 모델로 가능성을 시연할 수 있지만 작업은 주로 미리 훈련된 버전과 관련이 있습니다. 팔콘 모델을 배치하기 전에 적절한 가드레일을 배치하고 대상 사용 사례에 특정한 편향/해악 평가를 체계적으로 수행할 것을 강력히 권장한다. 특히, 인간 피드백으로부터의 강화 학습은 모델들을 더 도움이 되게 할 뿐만 아니라 적대적 질의들에 대해 더 견고하게 하는 것과 관련이 있을 수 있다는 점에 주목한다(Ganguli et al., 2022). 또한 PaLM-Coder (Chowdhery et al., 2022) 및 PaLM-2 S\({}^{*}\)(Anil et al., 2023) 또는 Code LLaMA (Roziere et al., 2023)와 유사한 Falcon 모델의 코드 적응 가능성을 본다.

Conclusion

본 논문에서는 최대 3,500B 토큰에 대해 훈련된 Falcon-7/40/180B를 사용하여 사전 훈련된 모델의 Falcon 시리즈를 소개하고 광범위하게 설명했다.

먼저 팔콘 모델의 훈련 데이터셋과 아키텍처를 준비하기 위해 수행한 일부 절제 및 실험에 대해 설명했다. 우리는 적절하게 필터링되고 중복 제거된 웹 데이터가 놀랍도록 강력한 기준선이라는 것을 발견했다; 더 큰 모델에 대한 암기(칼리니 등, 2022; 에르난데스 등, 2022)에 대한 우려로 인해 우리는 어떤 소스도 업샘플링하지 않고 이 웹 데이터에서 주로 훈련하지 않기로 선택했다. 아키텍처 측면에서는 대부분의 중재가 제한적인 영향을 미치는 것으로 나타났으며, K,V-캐시의 크기를 크게 줄임으로써 추론 확장성을 향상시키는 방법으로 멀티그룹 주의(multigroup attention, multiquery의 확장(Shazeer, 2019))를 채택하였다. 팔콘 시리즈의 미래 세대에 대해 사전 훈련 데이터에서 코드의 비율을 크게 늘리고 단계적 과정(예: 훈련의 절반은 최대 8k, 후반은 최대 16k, 다운스트림 적응은 32-256k)에서 더 긴 시퀀스 길이를 가진 훈련에서 가능성을 볼 수 있다.

그런 다음 팔콘 시리즈의 사전 훈련을 위한 최종 전략의 구현을 설명했다. 우리는 Penedo 등(2023)에서 우리의 데이터 파이프라인에 대해 광범위하게 보고한다. 대화 마스킹에 대한 접근 방식과 ZeRO와 결합된 3D 병렬성에 특히 의존하여 비용 효율적인 클라우드 인프라에서 실행하기 위한 분산 교육 전략을 설명했다. 또한 트리톤(Tillet et al., 2019)의 전용 FlashAttention(Dao et al., 2022) 커널과 단일 레이어 전략과 같은 빠른 메모리 효율적인 훈련을 위한 몇 가지 개입에 대해 논의했다. 또한 하이퍼 매개 변수 설정 및 수천 개의 GPU 이상의 실행 관리에 대한 몇 가지 세부 사항에 대해 논의했다.

마지막으로 주요 벤치마크에 대해 팔콘 시리즈에서 얻은 결과 중 일부를 요약했다. 우리는 Falcon-180B를 PaLM-2 Large(Anil et al., 2023)의 성능에 근접하고, GPT-3.5와 GPT-4(OpenAI, 2023a) 사이의 끝에서 발견했다. 팔콘-180B는 현재 이용 가능한 최고의 오픈 소스 모델이며 전체적으로는 최고의 모델 중 하나일 가능성이 높다. 우리는 우리의 평가가 주로 고전적인 자연어 작업에 초점을 맞추고 있으며 전용 피니튜닝 또는 강화 학습을 거친 팔콘의 다운스트림 버전에서 인간의 선호도를 평가하기 위해서는 추가 작업이 필요할 것이라는 점에 주목한다.

대형 언어 모델에 대한 개방형 연구를 촉진하고 이 공간의 기술 개발을 가속화하기 위해 허용 오픈 소스 라이선스에 따라 다음과 같은 인공물을 대중에게 공개한다.

* **Falcon-7/40/180B.** Apache 2.0 라이선스에서 Falcon-7/40B 및 전용 책임 AI 사용 라이선스에서 Falcon-180B를 사용하여 Falcon 시리즈의 모든 모델을 사용할 수 있습니다. 출시 시점에 팔콘-180B는 이용 가능한 가장 강력한 개방형 대형 언어 모델이다.
* **RefinedWeb의 600B 토큰 추출** 웹 데이터 세트의 600B 토큰 추출을 사용할 수 있도록 만들고, 연구자가 대규모 필터링 및 중복 제거된 웹 데이터를 연구하는 데 사용하고, 다른 분획자가 고품질 웹 데이터의 표준으로 채택할 수 있도록 합니다. 또한 이 추출에서 350B 토큰으로 훈련된 오픈 소스 1/7B 모델도 있습니다.
* **자세한 연구.** 이 문서 및 RefinedWeb 문서(Penedo 등, 2023)를 사용하여 팔콘 시리즈에 대한 수많은 결정 및 실험을 자세히 설명합니다.

우리는 큰 언어 모델이 우리 문명의 미래를 위한 기초 기술이라고 믿고, 결국 책임감 있게 공유되어야 한다고 믿는다. 광범위한 아이디어 교환은 우리 역사에서 가속화된 기술 및 경제적 진보의 필수 요소이며, 차례로 이러한 가속은 모두를 상승시킨다. 인공지능 연구와 모델을 오픈소싱함으로써 보다 광범위하고 다양한 커뮤니티를 육성할 수 있으며, 대형 언어 모델의 안전성과 신뢰성을 향상시키기 위한 활발한 협업 노력의 혜택을 누릴 수 있다. 우리는 팔콘 시리즈가 이 비전을 향한 작은 발걸음이 되기를 바란다.

## References

* Abadi et al. (2015) Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, G., Irving, M., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. (2015). TensorFlow: 이기종 시스템에 대한 대규모 기계 학습. Software available from tensorflow.org.
* Abadji 등(2022) Abadji, J., Ortiz Suarez, P., Romary, L., and Sagot, B. (2022). 더 깨끗한 문서 지향 다국어 크롤링 코퍼스를 향합니다. _ arXiv e-prints_, page arXiv:2201.06642.
* Adiwardana 등(2020) Adiwardana, D., Luong, M. - T., So, D. R., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., et al.(2020). 인간과 같은 개방형 도메인 챗봇을 향합니다. _ arXiv preprint arXiv:2001.09977_.
* Aghajanyan 등(2023) Aghajanyan, A., Yu, L., Conneau, A., Hsu, W. - N., Hambardzumyan, K., Zhang, S., Roller, S., Goyal, N., Levy, O., and Zettlemoyer, L. (2023). 생성 혼합-모달 언어 모델에 대한 법칙을 스케일링하는 단계. _ arXiv preprint arXiv:2301.03728_.
* Ainslie et al. (2023) Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sangghai, S. (2023). Gqa: 다중 헤드 검사점에서 일반화된 다중 쿼리 변환기 모델을 학습합니다. _ arXiv preprint arXiv:2305.13245_.
* 알레프 알파(2023) 알레프 알파(2023). 발광: 성능 벤치마크입니다. _ arXiv preprint arXiv:1810.12885_.
* Allal 등(2023) Allal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennighoff, N., Mishra, M., Gu, A., Dey, M., 등(2023). 별을 찾지 마! _DLAC(Deep Learning for Code) Workshop_에서.
* Anil 등(2023) Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., 등(2023). Palm 2 기술 보고서 _ arXiv preprint arXiv:2305.10403_.
* Artetxe 등(2021) Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., 등(2021). 전문가 혼합을 사용 하 여 효율적인 대규모 언어 모델링 _ arXiv preprint arXiv:2112.10684_.
* Bahdanau et al. (2014) Bahdanau, D., Cho, K., and Bengio, Y. (2014). 정렬 및 번역을 공동으로 학습하여 신경망 기계 번역을 수행합니다. _ arXiv preprint arXiv:1409.0473_.
* Bai 등(2022) Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., 등(2022a). 인간의 피드백에서 강화 학습으로 유용하고 무해한 어시스턴트를 교육합니다. _ arXiv preprint arXiv:2204.05862_.
* Bai 등(2022b) Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kermion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., 등(2022b). constitutional ai: ai 피드백으로부터의 무해함_ arXiv preprint arXiv:2212.08073_.
* Baumgartner 등(2020) Baumgartner, J., Zannettou, S., Keegan, B., Squire, M., and Blackburn, J. (2020). Pushshift reddit 데이터 세트입니다.
* Bavarian 등(2022) Bavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey, C., Tworek, J., and Chen, M. (2022). 중간을 채우기 위한 언어 모델의 효율적인 교육 _ arXiv preprint arXiv:2207.14255_.
* Beltagy 등(2019) Beltagy, I., Lo, K., and Cohan, A. (2019). Scibert: 과학 텍스트에 대한 사전 훈련된 언어 모델. _The Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3615-3620.
* Biderman 등(2023) Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O'Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., 등(2023). 피티아: 훈련 및 스케일링 전반에 걸쳐 대규모 언어 모델을 분석하기 위한 제품군입니다. _International Conference on Machine Learning_, pages 2397-2430. PMLR.
* Berman 등(2020)Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. (2020). 피카: 자연어로 육체적 상식에 대해 추론합니다. 30번째 AAAI 인공지능 콘퍼런스.
* Black 등(2022) Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell, K., Phang, J., 등(2022). Gpt-neox-20b: 오픈소스 자기회귀 언어 모델. _Proceedings of BigScience Episodet 5-Workshop on Challengees and Perspective in Creating Large Language Models_, pages 95-136.
* Black 등(2021) Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. (2021). GPT-Neo: Mesh-Tensorflow를 이용한 대규모 자기회귀 언어 모델링 이 소프트웨어를 사용하는 경우 이러한 메타데이터를 사용하여 인용하십시오.
* Bradbury 등(2021) Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., 등(2021). 오토그라드와 xla. _ 천체물리 소스 코드 라이브러리_, 페이지 ascl-2111입니다.
* Broder(1997) Broder, A. Z. (1997). 문서의 유사성과 격리에 관하여. Proceedings _Proceedings. Compression and Complexity of Sequences 1997_, pages 21-29. IEEE.
* Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). 언어 모델은 적은 수의 학습자입니다. _신경 정보 처리 시스템의 발전_ 에서, 볼륨 33, 페이지 1877-1901.
* Carlini 등(2022) Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. (2022). 신경 언어 모델에 걸친 암기 정량화 _학습 표현에 관한 제11회 국제 콘퍼런스_에서.
* Chelba et al. (2013) Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. (2013). 통계적 언어 모델링의 진행률을 측정하기 위한 10억 단어 벤치마크입니다. _ arXiv preprint arXiv:1312.3005_.
* Chen 등(2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., 등(2021). 코드에 대해 학습 된 대규모 언어 모델을 평가 합니다. _ arXiv preprint arXiv:2107.03374_.
* Chen 등(2023) Chen, S., Wong, S., Chen, L., and Tian, Y. (2023). 위치 보간을 통해 대규모 언어 모델의 컨텍스트 창을 확장 합니다. _ arXiv preprint arXiv:2306.15595_.
* Chiang and Lee (2023) Chiang, C.-H. 이현용 (2023). 대규모 언어 모델이 인간 평가의 대안이 될 수 있습니까? _ arXiv preprint arXiv:2305.01937_.
* Chiang 등(2023) Chiang, W. - L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. (2023). 비쿠나: 90%*채팅 품질로 gpt-4를 감동시키는 오픈 소스 챗봇입니다.
* Chowdhery 등(2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., 등(2022). Palm: 경로를 사용하여 언어 모델링을 스케일링합니다. _ arXiv preprint arXiv:2204.02311_.
* Clark 등(2022) Clark, A., De Las Casas, D., Guy, A., Mensch, A., Paganini, M., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T., Borgeaud, S., 등(2022). 라우팅된 언어 모델의 통일된 크기 조정 법칙입니다. _International Conference on Machine Learning_, pages 4057-4086. PMLR.
* Clark et al. (2019) Clark, C., Lee, K., Chang, M. - W., Kwiatkowski, T., Collins, M., and Toutanova, K. (2019). Boolq: 자연스러운 예/아니오 질문의 놀라운 어려움을 탐구한다. _NAACL_에서입니다.
* Clark 등(2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. (2018). 질문에 대한 답을 풀었다고 생각해? try arc, the AI2 reasoning challenge. _ CoRR_, abs/1803.05457.
* Clark 등(2019)Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., 등(2021). 검증자를 학습하여 수학 단어 문제를 해결합니다. _ arXiv preprint arXiv:2110.14168_.
* Conneau 등(2020) Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzman, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). 비지도적 언어 교차 표현 학습 규모 _Computational Linguistics Association의 제58차 연차회의 회보_에서, 페이지 8440-8451이다.
* Costa-jussa 등(2022) Costa-jussa, M. R., Cross, J., Celebi, O., Elbayad, M., Heafield, K., Heffernan, K., Kalbassi, E., Lam, J., Licht, D., Maillard, J., 등(2022). 언어가 남아 있지 않습니다. 인간 중심의 기계 번역을 스케일링합니다. _ arXiv preprint arXiv:2207.04672_.
* 도(2023) 도, T. (2023). 플래시 Attention-2: 더 나은 병렬 처리 및 작업 분할로 더 빠른 주의를 기울입니다. _ arXiv preprint arXiv:2307.08691_.
* Dao 등(2022) Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. (2022). 플래시 주의: io-awareness를 사용 하 여 빠르고 메모리 효율적인 정확한 주의력입니다. _ Advances in Neural Information Processing Systems_, 35:16344-16359.
* Dettmers 등(2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. (2022). Llm.int8(): 스케일에서의 트랜스포머에 대한 8비트 매트릭스 곱셈. _ Advances in Neural Information Processing Systems_, 35:30318-30332.
* Devlin 등(2018) Devlin, J., Chang, M. - W., Lee, K., and Toutanova, K. (2018). Bert: 언어 이해를 위한 깊은 양방향 트랜스포머의 사전 훈련. _ arXiv preprint arXiv:1810.04805_.
* Dey 등(2023) Dey, N., Gosal, G., Khachane, H., Marshall, W., Pathria, R., Tom, M., Hestness, J., 등(2023). Cerebras-gpt: 대뇌 s 웨이퍼 크기 클러스터에서 훈련 된 컴퓨팅 최적 언어 모델을 엽니다. _ arXiv preprint arXiv:2304.03208_.
* Dinan et al.(2023) Dinan, E., Yaida, S., and Zhang, S. (2023). 초기화시 변압기의 효과적인 이론. _ arXiv preprint arXiv:2304.02034_.
* Dodge 등(2021) Dodge, J., Sap, M., Marasovic, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., and Gardner, M. (2021). 대형 웹텍스트 말뭉치 문서화: 거대 클린 크롤링 코퍼스에 대한 사례 연구 _2021년 자연어 처리에서의 경험적 방법에 관한 회의의 진행문_ 에서, 페이지 1286-1305.
* Dubois 등(2023) Dubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). 알파카팜: 인간의 피드백으로부터 학습하는 방법들에 대한 시뮬레이션 프레임워크. _ arXiv preprint arXiv:2305.14387_.
* Eberhard et al. (2023) Eberhard, D. M., Simons, G. F., and Fennig, C. D. (2023). _ 민족어: 세계의 언어들. SIL International, Dallas, TX, USA, 26번째 판.
* Fedus 등(2022) Fedus, W., Zoph, B., and Shazeer, N. (2022). 변압기 전환: 간단 하 고 효율적인 희소성을 사용 하 여 매개 변수 모델을 조정 합니다. _ The Journal of Machine Learning Research_, 23(1):5232-5270.
* Flamary 등(2021) Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V., Sutherland, D. J., Tavenard, R., Tong, A., and Vayer, T. (2021). Pot: Python 최적 전송입니다. _ Journal of Machine Learning Research_, 22(78):1-8.
* Fourrier 등(2023) Fourrier, C., Habib, N., Launay, J., and Wolf, T. (2023). 열린 llm 리더보드는 어떻게 된 거죠? "[https://huggingface.co/blog/evaluating-mmlu-leaderboard](https://huggingface.co/blog/evaluating-mmlu-leaderboard)"."
* Fu 등(2022) Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. (2022). 배고픈 하마: 국가 공간 모델을 이용한 언어 모델링을 지향한다. _학습 표현에 관한 제11회 국제 콘퍼런스_에서.
* Ganguli 등(2022) Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., 등(2022). 해로움을 줄이기 위한 학습 언어 모델: 메서드, 스케일링 행위 및 학습한 교훈입니다. _ arXiv preprint arXiv:2209.07858_.
* Goyal 등(2020) Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. (2020). 파일: 언어 모델링을 위한 다양한 텍스트의 800GB 데이터 세트입니다. _ arXiv preprint arXiv:2101.00027_.
* Gao 등(2021) Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). 소수의 샷 언어 모델 평가를 위한 프레임워크입니다.
* 제1권: 메인 컨퍼런스와 공유 태스크의 진행 상황, 제2권: 시맨틱 평가에 관한 제6회 국제 워크숍의 진행 상황(SemEval 2012)_, 캐나다 몬트리올 394-398 페이지. 계산 언어학을 위한 연관성.
* Gunasekar 등(2023) Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M., Kauffmann, P., de Rosa, G., Saarikivi, O., 등(2023). 교과서만 있으면 됩니다. _ arXiv preprint arXiv:2306.11644_.
* Hamborg 등(2017) Hamborg, F., Meuschke, N., Breitinger, C., and Gipp, B. (2017). news-please: 일반적인 뉴스 크롤러 및 추출기입니다. _제15회 국제 정보 과학 심포지엄의 진행사항_에서, 페이지 218-223.
* Haviv 등(2022) Haviv, A., Ram, O., Press, O., Izsak, P., and Levy, O. (2022). 위치 부호화가 없는 트랜스포머 언어 모델은 여전히 위치 정보를 학습한다. _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 1382-1390.
* Hendrycks 등(2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2020). 대규모 다중 작업 언어 이해 측정 _학습 표현에 대 한 국제 회의_ 에서입니다.
* Hernandez 등(2022) Hernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Henighan, T., Hume, T., 등(2022). 법칙의 크기 조정 및 반복된 데이터로부터 학습의 해석 가능성 _ arXiv preprint arXiv:2205.10487_.
* Hestness 등(2017) Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. (2017). 딥 러닝 스케일링은 경험적으로 예측할 수 있습니다. _ arXiv preprint arXiv:1712.00409_.
* Hoffmann 등(2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., 등(2022). 컴퓨팅 최적화 대용량 언어 모델을 학습합니다. _ arXiv preprint arXiv:2203.15556_.
* Hooker (2021) Hooker, S. (2021). 하드웨어 추첨. _ Communications of the ACM_, 64(12):58-65.
* Howard and Ruder (2018) Howard, J. and Ruder, S. (2018). 텍스트 분류를 위한 범용 언어 모델 미세 조정 _제56회 전산언어학회 연차총회 회보(제1권: 장문)_의 328-339페이지.
* Inan et al.(2016) Inan, H., Khosravi, K., and Socher, R. (2016). 단어 벡터 및 단어 분류기 묶기: 언어 모델링을 위한 손실 프레임워크입니다. _ arXiv preprint arXiv:1611.01462_.
* 변곡(2023) 변곡 1.
* Johannes Welbl et al. (2017) Johannes Welbl, Nelson F. Liu, M. G. (2017). 다중 선택 과학 질문을 크라우드소싱합니다.
* Jozefowicz et al.(2016) Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y. (2016). 언어 모델링의 한계를 탐구합니다. _ arXiv preprint arXiv:1602.02410_.
* kaiokenmdenv(2023) kaiokenmdenv(2023). 맥락을 확장하는 건 힘들지만... 불가능하진 않아 2023-10-02
* Kaplan 등(2020) Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). 신경 언어 모델에 대한 법칙을 스케일링합니다. _ arXiv preprint arXiv:2001.08361_.
* Kaplan et al. (2020)Kenton, J. D. M.-W. C. and Toutanova, L. K. (2019). 버트: 언어 이해를 위한 심층 양방향 변압기 사전 교육입니다. _Proceedings of NAACL-HLT_, pages 4171-4186.
* Koectkov 등(2022) Koectkov, D., Li, R., Jia, L., Mou, C., Jernite, Y., Mitchell, M., Ferrandis, C. M., Hughes, S., Wolf, T., Bahdanau, D., 등(2022). 스택: 허용 가능한 라이선스 소스 코드의 3 tb입니다. _ 머신 러닝 연구 트랜잭션입니다.
* Kreutzer 등(2022) Kreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh, N., Tapo, A. A., Subramani, N., Sokolov, A., Sikasote, C., 등(2022). 한 눈에 품질: 웹으로 크롤링된 다국어 데이터 세트에 대한 감사입니다. _ Transactions of the Association for Computational Linguistics_, 10:50-72.
* Lai 등(2017) Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. (2017). 인종: 시험의 대규모 읽기 이해 데이터 세트. _2017년 자연어 처리에서의 경험적 방법에 관한 회의의 진행문_ 에서, 페이지 785-794.
* Laurencon 등(2022) Laurencon, H., Saulnier, L., Wang, T., Akiki, C., del Moral, A. V., Nguyen, H., Srohberg, J., Sako, M., Lhoest, Q., McMillan-Major, A., Werra, L. V., Mou, C., Ponferrada, E. G., Nguyen, H., Lhoest, Q., McMillan-Major, A., Dupont, G., Biderman, S., Rogers, A., allal, L. B., T., Pistilli, G., Nguyen, O., Nikpoor, S., Masoud, M., Colombo, P., de la Rosa, J., Villegas, P., Thrush, T., Longpre, S., Nagel, S., Weber, L., Munoz, M. R., Zhu, J., Strien, L., Munoz, M. L., Almubarak, K., Chien, V. M., Gonz (2022). 빅사이언스 ROATS 코퍼스: 1.6TB 복합 다국어 데이터 세트. _36번째 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙에 관한 회의_에서.
* Le Scao 등(2022) Le Scao, T., Wang, T., Hesslow, D., Bekman, S., Bari, M. S., Biderman, S., Elsahar, H., Muennighoff, N., Phang, J., Press, O., 등(2022). 100만 gpu 시간이 있다면 어떤 언어 모델을 훈련시킬까요? _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 765-782.
* Lee et al. (2022) Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. (2022). 훈련 데이터를 중복하면 언어 모델이 더 좋아집니다. _Computational Linguistics Association of the 60th Annual Meeting의 Proceedings of the Computational Linguistics(Volume 1: Long Papers)_, pages 8424-8445.
* Levine 등(2020) Levine, Y., Wies, N., Sharir, O., Bata, H., and Shashua, A. (2020). 자체 주의의 깊이 효율성으로 제한됩니다. _ Advances in Neural Information Processing Systems_, 33:22640-22651.
* Lewkowycz 등(2022) Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. (2022). 언어 모델로 정량적 추론 문제를 해결합니다.
* Li 등(2023a) Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Koectkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., 등(2023a). Starcoder: may the source is you! _ arXiv preprint arXiv:2305.06161_.
*Li 등(2021) Li, S., Xue, F., Li, Y., and You, Y. (2021). 서열 병렬성: 4d 병렬성을 가능하게 한다. _ arXiv preprint arXiv:2105.13120_.
* Li 등(2023b) Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y. T. (2023b). ii: phi-1.5 기술 보고서가 필요한 것은 교과서입니다. _ arXiv preprint arXiv:2309.05463_.
* Liang 등(2023) Liang, D., Gonen, H., Mao, Y., Hou, R., Goyal, N., Ghazvininejad, M., Zettlemoyer, L., and Khabsa, M. (2023). Xlm-v: 다국어 마스킹 언어 모델에서의 어휘 병목 현상을 극복. _ arXiv preprint arXiv:2301.10472_.
* Liang 등(2022) Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., 등(2022). 언어 모델에 대한 전체론적 평가. _ arXiv preprint arXiv:2211.09110_.
* Lieber 등(2021) Lieber, O., Sharir, O., Lenz, B., and Shoham, Y. (2021). 쥬라기-1: 기술적 세부사항 및 평가. 기술 보고서, AI21 연구소
* Li 등(2021)Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., Pasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V., O'Horo, B., Wang, J., Zettlemoyer, L., Kozareva, Z., Diab, M., Stoyanov, V., and Li, X. (2021). 다국어 언어 모델을 사용한 샷 학습은 거의 없습니다. _ ArXiv_, abs/2112.10668.
* Longpre 등(2023) Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., 등(2023). 최종 컬렉션: 효과적인 명령 조정을 위한 데이터 및 메서드 디자인 _ arXiv preprint arXiv:2301.13688_.
* Loshchilov and Hutter (2017) Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. _ arXiv preprint arXiv:1711.05101_.
* Loshchilov and Hutter (2018) Loshchilov, I. and Hutter, F. (2018). 비결합 중량 감소 규칙화. _학습 표현에 대 한 국제 회의_ 에서입니다.
* Luo 등(2022) Luo, S., Li, S., Zheng, S., Liu, T. -Y., Wang, L., and He, D. (2022). 트랜스포머는 예상한 만큼 강력하지 않을 수 있습니다. _ arXiv preprint arXiv:2205.13401_.
* Luo 등(2023) Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. (2023). Wizardcoder: Empowering code large language models with evolution-instruct. _ arXiv preprint arXiv:2306.08568_.
* Madaan 등(2022) Madaan, A., Zhou, S., Alon, U., Yang, Y., and Neubig, G. (2022). 코드의 언어 모델은 소수의 상식적인 학습자들이다. _2022 자연어 처리에서의 경험적 방법에 관한 회의의 진행문_에서, 1384-1403페이지.
* Manber and Myers (1993) Manber, U. 및 Myers, G. (1993). 서픽스 배열: 온라인 문자열 검색을 위한 새 메서드입니다. _ Journal on Computing_, 22(5):935-948.
* Memoli (2011) Memoli, F. (2011). Gromov-wasserstein distance and metric approach to object matching. _ Foundations of computational mathematics_, 11:417-487.
* Mihaylov et al. (2018) Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. (2018). 갑옷이 전기를 전도할 수 있나요? 열린 책 질문 답변을 위한 새로운 데이터 세트 _EMNLP_에서.
* Mikolov et al. (2013) Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). 벡터 공간에서 단어 표현의 효율적인 추정 _ arXiv preprint arXiv:1301.3781_.
* Mitchell 등(2019) Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D., and Gebru, T. (2019). 모델 보고를 위한 모델 카드입니다. 공정성, 책임 및 투명성에 대 한 회의 진행률 _220-229 페이지입니다.
* MosaicML(2023) MosaicML(2023). mpt-30b 소개: 오픈 소스 기초 모델용 바를 올립니다. 2023-06-22
* Muennighoff 등(2023) Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., and Raffel, C. (2023). 데이터 제약 언어 모델 크기 조정 _ arXiv preprint arXiv:2305.16264_.
* Narayanan 등(2021a) Narayanan, D., Phanishayee, A., Shi, K., Chen, X., and Zaharia, M. (2021a). 메모리 효율적인 파이프라인 병렬 dnn 훈련입니다. _International Conference on Machine Learning_에서 7937-7947. PMLR 페이지.
* Narayanan 등(2021b) Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., 등(2021b). 메가트론-lm을 이용한 gpu 클러스터에 대한 효율적인 대규모 언어 모델 학습 _High Performance Computing, Networking, Storage and Analysis를 위한 International Conference Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_의 1-15페이지.
* OpenAI(2023a) OpenAI(2023a). Gpt-4 기술 보고서. _ arXiv_, 페이지 2303-08774.
* OpenAI(2023b) OpenAI(2023b). 연구자를 위한 모델 인덱스입니다. 2023-09-26
*16, 만하임. Leibniz-Institut fur Deutsche Sprache.
* Ortiz Suarez 등(2019)Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., 등(2022). 인간 피드백으로 지침을 따르도록 언어 모델을 훈련합니다. _ Advances in Neural Information Processing Systems_, 35:27730-27744.
* Paperno 등(2016) Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N. Q., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. (2016). LAMBADA 데이터 세트: 광범위한 담화 맥락을 요구하는 단어 예측. _제54회 전산언어학회 연차총회 회보(제1권:장문)_에서 독일 베를린 1525-1534쪽. 계산 언어학을 위한 연관성.
* Paresh(2023) Paresh, D. (2023). 스택 오버플로로 인해 대규모 학습 데이터에 대한 요금이 부과됩니다.
* Paszke 등(2017) Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). 파이토치의 자동 분화. _NIPS-W_에서.
* Penedo 등(2023) Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. (2023). Falcon LLM에 대한 RefinedWeb 데이터 세트: 웹 데이터 및 웹 데이터만 사용하여 큐레이션된 말뭉치를 능가합니다. _ arXiv preprint arXiv:2306.01116_.
* Pennington 등(2014) Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: 단어 표현을 위한 전역 벡터. _EMNLP(empirical methods in natural language processing)_ 에 관한 2014 컨퍼런스 진행사항_ 에서, 페이지 1532-1543.
* Peters 등(2018) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. (2018). 심층 문맥화된 단어 표현. _Proceedings of the 2018 Conference of the North American Chapter of the Association of Computational Linguistics: Human Language Technologies, Volume 1(Long Papers)_, pages 2227-2237, New Orleans, Louisiana. 계산 언어학을 위한 연관성.
* Pope 등(2023) Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and Dean, J. (2023). 변환기 추론을 효율적으로 스케일링합니다. _ Proceedings of Machine Learning and Systems_, 5.
* Press et al.(2022) Press, O., Smith, N., and Lewis, M. (2022). 열차 짧음, 테스트 길음: 선형 바이어스를 갖는 주의력은 입력 길이 외삽을 가능하게 한다. _학습 표현에 대 한 국제 회의_ 에서입니다.
* Press and Wolf (2016) Press, O. 및 Wolf, L. (2016). 출력 임베딩을 사용하여 언어 모델을 개선합니다. _ arXiv preprint arXiv:1608.05859_.
* Rabe and Staats (2021) Rabe, M. N. and Staats, C. (2021). Self Attention은 \(o(n^{2})\) 메모리가 필요하지 않습니다. _ arXiv preprint arXiv:2112.05682_.
* Radford 등(2018) Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). 생성 사전 교육을 통해 언어 이해를 향상시킵니다. _ OpenAI Blog_.
* Radford 등(2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., 등(2019). 언어 모델은 감독 되지 않은 다중 작업 학습자입니다. _ OpenAI blog_, 1(8):9.
* Rae 등(2021) Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., 등(2021). Scaling language models: Methods, analysis & insights from training gopher. _ arXiv preprint arXiv:2112.11446_.
* Rae et al. (2019) Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. (2019). 장거리 시퀀스 모델링을 위한 압축 변압기.
* Raffel 등(2019) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2019). 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계를 탐구합니다. _ CoRR_, abs/1910.10683.
* Rajbhandari 등(2020) Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. (2020). 제로: 조 단위 매개변수 모델을 학습하기 위한 메모리 최적화입니다. _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16. IEEE.
* Raffel 등(2019)Roberts, A., Chung, H. W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D., Narang, S., Lester, B., Gaffney, C., Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Salcianu, A., van Zee, M., Austin, J., Goodman, S., Soares, L. B., Hu, H., Tsvyashchenko, S., Chowdhery, A., Bastings, J., Bulian, J., Garcia, X., Ni, J., Chen, A., Kenealy, K., Clark, J. H., Lee, S., Garrette, D., Lee-Thorp, J., Raffel, C., Shazeer, N., Ritter, M., Bosma, M., Passos, A., Maitin-Shepard, J., Fiedel, N., Omernick, M., Seta, B., Sepassi, R., Spiridonov, t5x 및 seqio를 사용하여 모델 및 데이터를 확장합니다. _ arXiv preprint arXiv:2203.17189_.
* Roziere 등(2023) Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., 등(2023). 코드 라마: 코드를 위한 기초 모델을 엽니다. _ arXiv preprint arXiv:2308.12950_.
* Sakaguchi et al. (2019) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. (2019). Winogrande: 스케일에서의 적대적인 winograd 스키마 챌린지. _ arXiv preprint arXiv:1907.10641_.
* Sanh et al. (2021) Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L. A., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., BARI, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N. V., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Biderman, S. R., T. G. O., Gao, L., Bers, T. 다중 작업 프롬프트 교육을 통해 제로 샷 작업 일반화를 사용할 수 있습니다. _ ArXiv_, abs/2110.08207.
* Scao 등(2022) Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M., 등(2022a). Bloom: 176b 매개 변수 오픈 액세스 다국어 언어 모델입니다. _ arXiv preprint arXiv:2211.05100_.
* Scao 등(2022b) Scao, T. L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S., Bari, M. S., Bideman, S., Elsahar, H., Muennighoff, N., Phang, J., 등(2022b). 100만 gpu 시간이 있는 경우 교육할 언어 모델은 무엇입니까? _ arXiv preprint arXiv:2210.15424_.
* Shaham 등(2022) Shaham, U., Elbayad, M., Goswami, V., Levy, O., and Bhosale, S. (2022). 다국어 번역의 간섭을 유발하고 치료합니다. _ arXiv preprint arXiv:2212.07530_.
* Shannon(1951) Shannon, C. E. (1951). 인쇄된 영어의 예측 및 엔트로피. _ Bell system technical journal_, 30(1):50-64.
* Shaw et al. (2018) Shaw, P., Uszkoreit, J., and Vaswani, A. (2018). 상대적 위치 표현들을 갖는 자기-주의 _ arXiv preprint arXiv:1803.02155_.
* Shazeer(2019) Shazeer, N. (2019). 빠른 변환기 디코딩: 쓰기 헤드 하나만 있으면 됩니다. _ arXiv preprint arXiv:1911.02150_.
* Shazeer(2020) Shazeer, N. (2020). Glu 변형은 변압기를 개선 합니다. _ arXiv preprint arXiv:2002.05202_.
* Shazeer 등(2018) Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., Sepassi, R., and Hechtman, B. (2018). Mesh-TensorFlow: 슈퍼컴퓨터를 위한 딥 러닝. _신경 정보 처리 시스템_에서.
* Shoeybi 등(2019) Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019). 메가트론-lm: 모델 병렬 처리를 사용 하 여 수십억 매개 변수 언어 모델을 훈련 합니다. _ arXiv preprint arXiv:1909.08053_.
* Smith 등(2022) Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S., Zerveas, G., Korthikanti, V., 등(2022). 대규모 생성 언어 모델인 메가트론 튜링 nlg 530b를 훈련하기 위해 심층 속도와 메가트론을 사용합니다. _ arXiv preprint arXiv:2201.11990_.
* Soldaini 등(2023) Soldaini, L., Lo, K., Kinney, R., Naik, A., Ravichander, A., Bhagia, A., Groeneveld, D., Schwenk, D., Magnusson, I., and Chandu, K. (2023). 돌마
* Srivastava 등(2023) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., 등(2023). 모방 게임을 넘어 언어 모델의 기능을 정량화하고 외삽합니다. _ 머신 러닝 연구 트랜잭션입니다.
* Srivastava 등(2023)Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: 신경망이 과적합되는 것을 방지하는 간단한 방법. _ Journal of Machine Learning Research_, 15(56):1929-1958.
* Stiennon 등(2020) Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. (2020). 인간의 피드백으로 요약하는 방법을 배웁니다. _ Advances in Neural Information Processing Systems_, 33:3008-3021.
* Su 등(2021) Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. (2021). 로포르머: 회전 위치 매립을 갖는 향상된 트랜스포머. _ arXiv preprint arXiv:2104.09864_.
* Sutton(2019) Sutton, R. (2019). 쓰라린 교훈이지 Incomplete Ideas(blog)_, 13(1).
* Taori 등(2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Stanford alpaca: 명령어 추종 llama 모델. [https://github.com/tatsu-lab/stanford_alpaca] (https://github.com/tatsu-lab/stanford_alpaca).
* Tay 등(2021) Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., Narang, S., Yogatama, D., Vaswani, A., and Metzler, D. (2021). 효율적으로 확장: 사전 훈련 및 미세 조정 변압기의 Insights _ ArXiv_, abs/2109.10686.
* Tay 등(2022a) Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J., Wang, X., Chung, H. W., Bahri, D., Schuster, T., Zheng, S., 등(2022a). Ul2: 언어 학습 패러다임 통일. _학습 표현에 관한 제11회 국제 콘퍼런스_에서.
* Tay 등(2022b) Tay, Y., Wei, J., Chung, H. W., Tran, V. Q., So, D. R., Shakeri, S., Garcia, X., Zheng, H. S., Rao, J., Chowdhery, A., 등(2022b). 0.1% 추가 계산으로 스케일링 법칙을 전송합니다. _ arXiv preprint arXiv:2210.11399_.
* Thoppilan 등(2022) Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., 등(2022). Lamda: 대화 상자 애플리케이션을 위한 언어 모델입니다. _ arXiv preprint arXiv:2201.08239_.
* Tian and Parikh (2022) Tian, R. 및 Parikh, A. P. (2022). Amos: 모델 지향 스케일에 대한 적응적 가중치 감쇄를 갖는 아담 스타일 최적화기. _ arXiv preprint arXiv:2210.11693_.
* Tiedemann (2016) Tiedemann, J. (2016). 대형 영화 자막 코퍼스에서 대체 번역본을 찾습니다. _Proceedings of the Tenth International Conference on Language Resources and Evaluation(LREC'16)_, pages 3518-3522, Portoroz, Slovenia. 유럽 언어 자원 협회(ELRA).
* Tillet et al. (2019) Tillet, P., Kung, H.-T., and Cox, D. (2019). 트리톤: 타일형 신경망 계산을 위한 중간 언어 및 컴파일러입니다. <제3차 ACM SIGPLAN International Workshop on Machine Learning and Programming Languages>의 Proceedings_ 10-19페이지.
* Touvron 등(2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. - A., Lacroix, T., Roziere, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023a). Llama: 오픈하고 효율적인 기초 언어 모델입니다. _ arXiv preprint arXiv:2302.13971_.
* Touvron 등(2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 등(2023b). 라마 2: 기반 및 미세 조정 채팅 모델을 엽니다. _ arXiv preprint arXiv:2307.09288_.
* Trinh and Le (2018) Trinh, T. H. and Le, Q. V. (2018). 상식추론을 위한 간단한 방법. _ arXiv preprint arXiv:1806.02847_.
* Vaswani 등(2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). 관심만 있으면 됩니다. _신경 정보 처리 시스템의 발전_에서, 페이지 5998-6008.
* Villalobos 등(2022) Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A. (2022). 데이터가 부족할까요? Machine Learning에서 스케일링 데이터 세트의 제한에 대한 분석 _ arXiv preprint arXiv:2211.04325_.
* Voss et al. (2018)Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. (2019). 슈퍼글루: 범용 언어 이해 시스템을 위한 보다 엄격한 벤치마크. _ Neural Information Processing Systems_, 32의 진보.
* Wang 등(2018) Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018). 글루: 자연어 이해를 위한 멀티 태스크 벤치마크 및 분석 플랫폼. _학습 표현에 대 한 국제 회의_ 에서입니다.
* Wang and Komatsuzaki (2021) Wang, B. and Komatsuzaki, A. (2021). GPT-J-6B: 60억 Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax] (https://github.com/kingoflolz/mesh-transformer-jax).
* Wang 등(2023) Wang, P., Li, L., Chen, L., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., and Sui, Z. (2023). 대형 언어 모델은 공정한 평가자가 아닙니다. _ arXiv preprint arXiv:2305.17926_.
* Wang 등(2022a) Wang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung, H. W., Beltagy, I., Launay, J., and Raffel, C. (2022a). 제로 샷 일반화에 가장 적합한 언어 모델 아키텍처 및 사전 훈련 목표는 무엇입니까? _ arXiv preprint arXiv:2204.05832_.
* Wang 등(2022b) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. (2022b). 자체 지시: 언어 모델을 자체 생성 지침에 정렬합니다. _ arXiv preprint arXiv:2212.10560_.
* Wei 등(2022a) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., 등(2022a). 대형 언어 모델의 최신 기능입니다. _ 머신 러닝 연구 트랜잭션입니다.
* Wei 등(2022b) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., 등(2022b). 생각의 연결 프롬프트는 대규모 언어 모델에서 추론을 이끌어냅니다. _ Advances in Neural Information Processing Systems_, 35:24824-24837.
* Welbl 등(2021) Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S. (2021). 언어 모델을 해독하는 데 어려움이 있습니다. _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 2447-2469.
* Wenzek 등(2020) Wenzek, G., Lachaux, M. - A., Conneau, A., Chaudhary, V., Guzman, F., Joulin, A., and Grave, E. (2020). Ccnet: 웹 크롤 데이터에서 고품질 단일 언어 데이터 세트를 추출합니다. _제12 언어 자원 및 평가 회의의 진행문_ 에서, 페이지 4003-4012.
* Wu 등(2016) Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., 등(2016). 구글의 신경 기계 번역 시스템: 인간과 기계 번역 사이의 간극을 좁히는 것. _ arXiv preprint arXiv:1609.08144_.
* Xiong 등(2023) Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. (2023). 기초 모델의 효과적인 긴 컨텍스트 스케일링입니다.
* Xue 등(2023) Xue, F., Fu, Y., Zhou, W., Zheng, Z., and You, Y. (2023). 반복하거나 반복하지 않으려면: 토큰 위기에서 llm 크기를 조정하여 Insights합니다. _ arXiv preprint arXiv:2305.13230_.
* Xue 등(2022) Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. (2022). Byt5: 미리 훈련 된 바이트 대 바이트 모델을 사용 하 여 토큰이 없는 미래를 향 합니다. _ Transactions of the Association for Computational Linguistics_, 10:291-306.
* Xue 등(2021) Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. (2021). mt5: 대용량 다국어 사전 훈련된 텍스트-텍스트 변환기. _Proceedings of the 2021 Conference of the North American Chapter of the Association of Computational Linguistics: Human Language Technologies_, pages 483-498.
* Yang et al. (2022) Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W., and Gao, J. (2022). 텐서 프로그램 v: 제로 샷 하이퍼 매개 변수 전송을 통해 큰 신경망을 조정합니다. _ arXiv preprint arXiv:2203.03466_.
* Yang et al.(2021)Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). HellaSwag: 기계가 정말로 당신의 문장을 끝낼 수 있을까요? _Computational Linguistics Association의 제57차 연차총회 회보_ 에서, 이탈리아 플로렌스 4791-4800 페이지이다. 계산 언어학을 위한 연관성.
* Zeng 등(2021) Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z., Jiang, X., Yang, Z., Wang, K., Zhang, X., 등(2021). Pangu-\(\alpha\): 자동 병렬 계산을 사용 하는 대규모 자기 회귀 사전 훈련 된 중국어 모델입니다. _ arXiv preprint arXiv:2104.12369_.
* Zhang 등(2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., 등(2022). Opt: 사전 훈련된 변압기 언어 모델을 엽니다. _ arXiv preprint arXiv:2205.01068_.
* Zheng et al.(2023) Zheng, L., Chiang, W. - L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023). mt-bench 및 챗봇 영역으로 llm-as-a-judge를 판단 합니다. _ arXiv preprint arXiv:2306.05685_.
* Zhong et al.(2023) Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. (2023). Agieval: 기초 모델을 평가하기 위한 인간 중심 벤치마크. _ arXiv preprint arXiv:2304.06364_.
* Zhu 등(2015) Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S. (2015). 책과 영화의 정렬: 영화를 보고 책을 읽음으로써 이야기 같은 시각적 설명을 향합니다. _Proceedings of the IEEE international conference on computer vision_, pages 19-27.

## Appendix A Contributions

* **Engineering & Tooling**.*
* **분산 학습 코드 베이스**.*
침례식 파니에 다니엘 헤슬로우
* **웹 데이터**.*
* Guilherme Penedo, Ruxandra Cojocaru _(다국어 데이터)_, Quentin Malatric _(데이터 품질)_, Alessandro Cappelli, Hamza Alobeidli.
* **큐레이트된 데이터**.*
* Alessandro Cappelli, Etienne Goffinet _(코드 데이터)_, Quentin Malatric, Ruxandra Cojocaru, Abdulaziz Alshamsi _(북 데이터)_.
* Daniel Hesslow, Baptiste Pannier, Guilherme Penedo _(deployment)_.
다니엘 마조타, 에티엔 고핀, 길허메 페네도
* **하드웨어 정확성**.*
* 침례자 파니에, 줄리엔 로네, 다니엘 헤슬로우.
* **Pretraining.** Baptiste Pannier, Julien Launay, Daniel Hesslow.

* **데이터 삭제**.*
* Julien Launay, Ruxandra Cojocaru _(커리큘럼 학습)_, Alessandro Cappelli, Quentin Malatric _(미세 입자 필터)_, Daniel Hesslow.
* **아키텍처 삭제**.*
* Julien Launay, Daniel Hesslow, Etienne Goffinet, Quentin Malatric _(training objectives)_, Baptiste Pannier, Badreddine Noune _(optimizers)_.
* **모델 finetuning**.*
* Quentin Malatric, Alessandro Cappelli, Etienne Goffinet _(code specialization)_, Guilherme Penedo _(human evaluation)_, Baptiste Pannier _(long-context)_, Julien Launay.
줄리엔 로네, 다니엘 헤슬로우, 쿠엔틴 말라트릭
* **종이 쓰기**.*
* Julien Launay, Daniel Hesslow, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei.
* **리더십.** Julien Launay, Ebtesam Almazrouei, Merouane Debbah.

## Appendix B Acknowledgements

AWS 팀, 특히 올리비에 크루챠트가 프로젝트 전반에 걸쳐 지원을 해주셔서 결국 팔콘-180B 훈련을 최대 4,096 A100s까지 확장할 수 있게 해주셔서 감사합니다. 우리는 또한 액셀 마멧, 트라이다오, 댄 푸, 콜린 라펠, 캐서린 리, 토마스 울프, 이즈 벨타기, 더크 그로네벨트가 프로젝트 전반에 걸쳐 통찰력 있는 토론에 감사드린다.

## Appendix C 모델 카드

\begin{table}
\begin{tabular}{p{113.8pt}|p{284.5pt}} \hline \multicolumn{2}{c|}{**Model details**} \\ \hline
**Organization** & The models were created by the Technology Innovation Institute. \\ \hline
**Model date** & Training of the Falcon models started in December and completed in the first half of 2023. \\ \hline
**Model type and information about training** & Falcon are autoregressive Transformer models trained with a causal language modeling objective. Architecture based on PaLM Chowdhery et al. (2022), with an extension of multiquery attention for tensor parallelism (multigroup) and minor tweaks (no SwiGLU, etc.). See Section 4 and Section 5 for details. \\ \hline
**Licence** & Falcon-7B and Falcon-40B are made available under the Apache 2.0 license; Falcon-180B is made available under the Falcon-180B TII license, with restrictions related to responsible use. \\ \hline
**Point of contact** & Falconllm@tii.ae \\ \hline
**Intended use** \\ \hline
**Primary intended uses** & Research on large language models; as a foundation for further specialization for specific use cases (e.g., chatbot, etc.) \\ \hline
**Primary intended users** & NLP researchers and engineers. \\ \hline
**Out-of-scope use cases** & Production use without adequate assessment of risks and mitigation; use cases which may be considered irresponsible or harmful. \\ \hline
**Factors** & \\ \hline
**Relevant factors** & The Falcon models are predominantly trained on English data from a large-scale web corpora representative of the web. Accordingly, they will carry the stereotypes and biases commonly encountered online, and are unlikely to generalize appropriately beyond English or European latin languages. \\ \hline
**Evaluation factors** & We evaluated the toxicity of the underlying pretraining dataset and found it to be in line with common curated pretraining datasets such as The Pile, see Penedo et al. (2023). Note that this only accounts for toxicity under the definition of Perspective API: "content that is rude or disrespectful". Notably, this fails to include concerns about social biases or harmfulness. \\ \hline
**Metrics** \\ \hline
**Model performance measures** & We focus our evaluation on the zero-shot generalization capabilities of our models across a wide range of tasks, leveraging the Eleuther AI language model evaluation harness Gao et al. (2021). \\ \hline
**Variation approaches** & Due to the costs associated with training Falcon we cannot train the models multiple times and measure variability across runs. \\ \hline
**Evaluation data** \\ \hline
**Datasets** & We evaluate zero-shot accuracy on 18 natural language tasks and one Python programming task, detailed in Section 6. \\ \hline
**Motivation** & We selected and aggregated tasks to build comparisons with other models in the literature (see Section 6.1). \\ \hline
**Preprocessing** & We mostly use the default setup of Gao et al. (2021), see Appendix G for the custom prompts we used for some evaluations \\ \hline
**Training data** \\ \hline \multicolumn{2}{c}{**See the dedicated datasheet in Penedo et al. (2023).**} \\ \hline \end{tabular}
\end{table}
표 23: **Mitchell 등 (2019)에 의해 도입된 프레임워크에 따라 Falcon에 대한 모델 카드**.**Datasheet

전용 RefinedWeb paper(Penedo et al., 2023)를 참조한다.

## 부록 E 문서화되지 않은 모델과의 비교

최근 많은 모델이 적절하게 문서화된 연구 논문 대신 불충분한 홍보 기술 보고서를 발표하기로 선택했으며 특히 희박한 평가만 제공하고 모델링 세부 정보는 종종 없다. 이것은 인용 및 귀속에 제대로 적용되지 않는 형식인 백채널을 통해 많은 세부 정보가 유출되거나 알려져 있기 때문에 비교를 어렵게 만든다. Anil 등(2023)의 경우 세 가지 모델 중 가장 큰 모델에 대해 CNBC3에 의해 매개변수 수와 사전 훈련 길이가 보고되었다. OpenAI(2023a)의 경우, 다수의 누출이 발생했으며, 대부분은 SemiAnalysis4에 의해 단편으로 요약되었다.

각주 3: [https://www.cnbc.com/2023/05/16/goles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html](https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html)

각주 4: [https://www.semanalysis.com/p/gpt-4-architecture-infrastructure](https://www.semanalysis.com/p/gpt-4-architecture-infrastructure)

## Appendix F Pseudocode 샘플

### 모든 대역폭/레이턴시를 효율적으로 측정할 수 있는 측정 계획

```
defget_all_comps(n:int): #n:poweroftwo defop(l,d=4,r=l): l=1.reshape(-1,d) l[1::2]=np.roll(l[1::2],r,axis=1) returnl.T.reshape(-1) x=np.array(list(range(n))) comps=[] d=1 whiled<n: forinrange(d): comps.append(op(x,d=d,r=r).copy()) d=2 ret=np.stack(comps) returnret.reshape(ret.shape[0],-1,2)
```

### 트리 토큰 깊이를 주의 마스크로 변환 합니다.

```
defattn_mask_pos_(x,attention_mask): foriinrange(len(x)): attn=False forjinrange(0,len(x)): attn|=(i+1)==j attn&=x[i]<x[j] attention_mask[j,i]=-attn&(i!=j)
```

### Zero-1 pseudo-code

```
defreduce_scatter_grads(buffer): chunk_size=buffer.model_grads.numel()//dp_world_size t=empty_tensor(chunk_size,dtype=bfloat16) reduce_scatter_tensor(t,buffer.model_grad,group=data_parallel_group) buffer.optimizer_grads.copy(t)
```

[MISSING_PAGE_FAIL:54]

\begin{table}
\begin{tabular}{l l} \hline \hline
**Context** & Complete
\begin{tabular}{} \end{tabular} & in the following extracts. \\
**Sample(s)** & Extract: _My wife refused to allow me to come to Hong Kong when the plague was at its height and –” Your wife, Johanne? You are married at last 2” Johanne grinned._ \\  & _”Well, when a man gets to my age, he starts to need a few home comforts. After my_ \\  & _dear mother passed away ten years ago now, I became_ \\  & Completion: \\ \hline \hline \end{tabular}
\end{table}
표 27: **LAMBADA.** 미리 정해진 후보 집합에 대한 제약 없이 전체 모델에 따라 정답이 가장 가능성이 높은지 여부를 평가합니다.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Context** & \begin{tabular}{} \end{tabular} &
\begin{tabular}{} \end{tabular} \\
**Sample(s)** & \begin{tabular}{} \end{tabular} &
\begin{tabular}{} \end{tabular} \\
**Candidates** & \begin{tabular}{} \end{tabular} &
\begin{tabular}{} \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
표 26: **RTE**.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Context** & \begin{tabular}{} \end{tabular} &
\begin{tabular}{} \end{tabular} \\
**Sample(s)** &
\begin{tabular}{} \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
표 28: **OpenBookQA.** 프롬프트에 옵션이 제공되지 않는 경우 후보자는 문자 키 대신 가능한 답변(예: 새 트릭을 배우는 강아지 등)입니다.

\begin{table}
\begin{tabular}{l l} \hline \hline
**샘플(들)** & 기사: _지난 주에 몇 명의 학생들과 졸업 후 무엇을 하고 싶었는지, 어떤 직업 전망을 가지고 있는지 이야기했습니다. 내가 의사가 되기 위해 훈련 중인 학생들을 가르친다는 점에서, 나는 대부분의 사람들이 “외부 도움” 없이는 그들이 원하는 직업을 얻을 수 없을 것이라고 생각하는 것에 놀랐다. “그게 무슨 도움이야?” 나는 그들이 그들을 도와줄 가족이나 친구가 필요할 것이라고 말하기를 기대하며 물었다. “수술” 한 사람이 대답했다. 나는 그 반응에 꽤 놀랐다. 오늘날의 졸업생들은 점점 더 취업에 있어서 다른 사람들보다 앞서기 위해 성형수술을 받을 의향이 있는 것 같다. 한 소녀가 키를 키우기 위해 수술을 고려하고 있다고 말했어요. “그들은 다리를 부러뜨리고, 특별한 연장 나사를 넣고, 뼈가 다시 자라면서 뼈의 양쪽 끝 사이의 간격을 천천히 확장하면, 적어도 5 cm는 더 커질 수 있다!” 그때 나는 충격을 받았다. 나는 키가 작고, 그것을 부정할 수는 없지만, 단지 몇 센티미터의 키가 되기 위해 몇 달간의 고뇌를 겪을 것이라고 생각하지 않는다. 나는 키가 크지 않다는 사실을 숨기려 하지 않기 때문에 두꺼운 밑창이 있는 신발을 신는 것도 귀찮지 않아! 나는 '완벽'을 원하는 경향이 있는 것 같고, 그것은 현실에는 존재하지 않는 이상이다. 아무도 완벽하게 태어나지 않았지만 잡지, TV 쇼와 영화는 날씬하고 키가 크고 아름다운 사람들의 이미지를 표준으로 제시한다. 슬리밍 보조기, 미용 시술, 성형외과에 대한 광고는 신문 페이지를 가득 채우며, 더 나아가 '완벽'은 필수 요건이며, 비용은 어떻든 구매해야 한다는 생각을 낳는다. 내 생각에, 외모보다는 기술이 어떤 사람이 자신이 선택한 직업 질문에서 얼마나 성공적인지를 결정해야 한다: 우리는 저자가 a_로 작업한다는 것을 구절에서 알 수 있다. \\ A. _doctor_ \\ B. _model_ \\ C. _teacher_ \\ D. _reporter_ \\
**Answer: C. teacher** \\
**질문:**_많은 졸업생들이 오늘날 성형 수술로 돌아갑니다._ \\ A. _더 나은 남자/여성을 결혼_ \\ B. _모델이 되기_ \\ C. _구직에서 다른 사람에 비해 이점을 얻_ \\ D. _더 많은 존경자를 유치_ \\
**정답: C. 구직에서 다른 사람보다 이점을 얻습니다* \\
**질문:**_구절에 따라 작성자는 _._ \\ A. 모든 사람은 완벽을 구매해야 한다. 비용이든 B. 졸업생들이 다른 사람들에게 일자리를 찾는 데 도움을 요청하는 것이 옳다. C.  그것은 자신의 진로에 정말로 중요한 기술 대신 외모이다. D.  미디어는 젊은이들의 수술에 대한 오해를 불러일으킨 책임이 있다.
**정답: D 미디어는 젊은이를 수술 목적으로 오해한 책임이 있습니다. \\
**질문:**_대목에 가장 적합한 제목이 무엇입니까?_ \\ A. _젊은 졸업생일수록 기대가 높습니다_ \\ B. _젊은 졸업생일수록 더 나은 직업을 위해 수술을 찾습니다_ \\ C. _젊은 졸업생일수록 화장품 수술에 대한 의견_ \\ D. _젊은 졸업생일수록 직업 탐색에서 다른 상황에 직면합니다_ \\
**Answer:** \\
**Candidates** & **[A., B., C., D.]** \\ \hline \hline \end{tabular}
\end{table}
표 29: **RACE.** Brown et al.(2020) 및 Gao et al.(2021)이 RACE를 구현하는 방식은 샷 수가 기사 수준에 있음을 의미합니다. 각 기사에 대한 3개의 질문은 0샷에서도 항상 제공됩니다.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Context** & **Compare the following sentences.** \\ \hline \hline \multirow{2}{*}{**Sample(s)**} & Extract: _place_ has a similar meaning in the following two sentences. Yes or no? \\  & Sentence 1: _Do you want to come over to my place later?_ \\  & Sentence 2: _A political system with no place for the less prominent groups._ \\  & Answer: \\
**Candidates** & [yes, no] \\ \hline \hline \end{tabular}
\end{table}
표 34: **Winograd. Gao 등 (2021)의 형식을 변경 하지 않고 true/false에 대 한 후보에서 예/아니오로 전환 합니다. **

\begin{table}
\begin{tabular}{l l} \hline \hline
**Context** & **Compare the following questions about an extract by yes, or no.** \\ \hline \multirow{2}{*}{**Sample(s)**} & Extract: _Powdered sugar, also called confectioners’ sugar, icing sugar, and icing cake, is a finely ground sugar produced by milling granulated sugar into a powdered state. It usually contains a small amount of anti-caking agent to prevent clumping and improve flow. Although most often produced in a factory, powdered sugar can also be made by processing ordinary granulated sugar in a coffee grinder, or by crushing it by hand in a mortar and pestle._ \\  & Question: _is confectionary sugar the same as powdered suga, yes or no?_ \\  & Answer: \\
**Candidates** & [yes, no] \\ \hline \hline \end{tabular}
\end{table}
표 30: **BoolQ**.

\begin{table}
\begin{tabular}{l l} \hline \hline
**문맥** & **추출에 의해 만들어진 문이 True, False 또는 Unsurc인지 비교** \\ \multirow{2}{*}{**Sample(s)** } & Extract: _복잡한 언어였습니다. 적어지지 않고 전해 내려왔다. 껍질을 벗겼다고 할 수도 있어요._ \\ & Statement: _the language peeled down._ \\  & 질문: True, False, Unsurc? \\  & Answer: \\
**Candidates** & [True, False, Unsurc] \\ \hline \hline \end{tabular}
\end{table}
표 31: **CB**.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Context** & **Compare the following questions about an extract by yes, or no.** \\ \hline \multirow{2}{*}{**Sample(s)**} & Extract: _Powdered sugar, also called confectioners’ sugar, icing sugar, and icing cake, is a finely ground sugar produced by milling granulated sugar into a powdered state. It usually contains a small amount of anti-caking agent to prevent clumping and improve flow. Although most often produced in a factory, powdered sugar can also be made by processing ordinary granulated sugar in a coffee grinder, or by crushing it by hand in a mortar and pestle._ \\  & Question: _is confectionary sugar the same as powdered suga, yes or no?_ \\  & Answer: \\
**Candidates** & [yes, no] \\ \hline \hline \end{tabular}
\end{table}
표 32: **COPA. Gao 등(2021)의 형식을 변경하지 않고 유지하지만 지침을 추가합니다.**

\begin{table}
\begin{tabular}{l l} \hline \hline
**Candidates** & **[true, false]** \\ \hline \hline \end{tabular}
\end{table}
표 33: **WiC**.
